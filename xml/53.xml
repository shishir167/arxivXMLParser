<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T02:02:56Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|52001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7792</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7792</id><created>2013-10-29</created><authors><author><keyname>Hechtman</keyname><forenames>Blake A.</forenames></author><author><keyname>Sorin</keyname><forenames>Daniel J.</forenames></author></authors><title>Evaluating Cache Coherent Shared Virtual Memory for Heterogeneous
  Multicore Chips</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The trend in industry is towards heterogeneous multicore processors (HMCs),
including chips with CPUs and massively-threaded throughput-oriented processors
(MTTOPs) such as GPUs. Although current homogeneous chips tightly couple the
cores with cache-coherent shared virtual memory (CCSVM), this is not the
communication paradigm used by any current HMC. In this paper, we present a
CCSVM design for a CPU/MTTOP chip, as well as an extension of the pthreads
programming model, called xthreads, for programming this HMC. Our goal is to
evaluate the potential performance benefits of tightly coupling heterogeneous
cores with CCSVM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7794</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7794</id><created>2013-10-29</created><updated>2014-03-05</updated><authors><author><keyname>Zappone</keyname><forenames>Alessio</forenames></author><author><keyname>Cao</keyname><forenames>Pan</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard A.</forenames></author></authors><title>Energy Efficiency Optimization in Relay-Assisted MIMO Systems with
  Perfect and Statistical CSI</title><categories>cs.IT math.IT math.OC</categories><journal-ref>IEEE Transactions on Signal Processing, vol. 62, no. 2, pp. 443 -
  457, January 2014</journal-ref><doi>10.1109/TSP.2013.2292031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A framework for energy-efficient resource allocation in a single-user,
amplify-and-forward relay-assisted MIMO system is devised in this paper.
Previous results in this area have focused on rate maximization or sum power
minimization problems, whereas fewer results are available when bits/Joule
energy efficiency (EE) optimization is the goal. The performance metric to
optimize is the ratio between the system's achievable rate and the total
consumed power. The optimization is carried out with respect to the source and
relay precoding matrices, subject to QoS and power constraints. Such a
challenging non-convex problem is tackled by means of fractional programming
and and alternating maximization algorithms, for various CSI assumptions at the
source and relay. In particular the scenarios of perfect CSI and those of
statistical CSI for either the source-relay or the relay-destination channel
are addressed. Moreover, sufficient conditions for beamforming optimality are
derived, which is useful in simplifying the system design. Numerical results
are provided to corroborate the validity of the theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7795</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7795</id><created>2013-10-29</created><authors><author><keyname>Ren</keyname><forenames>Jimmy SJ.</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Jiawei</forenames></author><author><keyname>Liao</keyname><forenames>Stephen</forenames></author></authors><title>An Unsupervised Feature Learning Approach to Improve Automatic Incident
  Detection</title><categories>cs.LG</categories><comments>The 15th IEEE International Conference on Intelligent Transportation
  Systems (ITSC 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sophisticated automatic incident detection (AID) technology plays a key role
in contemporary transportation systems. Though many papers were devoted to
study incident classification algorithms, few study investigated how to enhance
feature representation of incidents to improve AID performance. In this paper,
we propose to use an unsupervised feature learning algorithm to generate higher
level features to represent incidents. We used real incident data in the
experiments and found that effective feature mapping function can be learnt
from the data crosses the test sites. With the enhanced features, detection
rate (DR), false alarm rate (FAR) and mean time to detect (MTTD) are
significantly improved in all of the three representative cases. This approach
also provides an alternative way to reduce the amount of labeled data, which is
expensive to obtain, required in training better incident classifiers since the
feature learning is unsupervised.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7799</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7799</id><created>2013-10-29</created><updated>2014-01-09</updated><authors><author><keyname>Zhuang</keyname><forenames>Fuxin</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Backhaul Limited Asymmetric Cooperation for MIMO Cellular Networks via
  Semidefinite Relaxation</title><categories>cs.IT math.IT</categories><comments>14 pages, 7 figures. This paper is accepted by IEEE Transactions on
  Signal Processing</comments><doi>10.1109/TSP.2013.2293972</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multicell cooperation has recently attracted tremendous attention because of
its ability to eliminate intercell interference and increase spectral
efficiency. However, the enormous amount of information being exchanged,
including channel state information and user data, over backhaul links may
deteriorate the network performance in a realistic system. This paper adopts a
backhaul cost metric that considers the number of active directional
cooperation links, which gives a first order measurement of the backhaul
loading required in asymmetric Multiple-Input Multiple-Output (MIMO)
cooperation. We focus on a downlink scenario for multi-antenna base stations
and single-antenna mobile stations. The design problem is minimizing the number
of active directional cooperation links and jointly optimizing the beamforming
vectors among the cooperative BSs subject to
signal-to-interference-and-noise-ratio (SINR) constraints at the mobile
station. This problem is non-convex and solving it requires combinatorial
search. A practical algorithm based on smooth approximation and semidefinite
relaxation is proposed to solve the combinatorial problem efficiently. We show
that semidefinite relaxation is tight with probability 1 in our algorithm and
stationary convergence is guaranteed. Simulation results show the saving of
backhaul cost and power consumption is notable compared with several baseline
schemes and its effectiveness is demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7801</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7801</id><created>2013-10-29</created><updated>2014-09-14</updated><authors><author><keyname>Quang-Hung</keyname><forenames>Nguyen</forenames></author><author><keyname>Thoai</keyname><forenames>Nam</forenames></author><author><keyname>Son</keyname><forenames>Nguyen Thanh</forenames></author></authors><title>EPOBF: Energy Efficient Allocation of Virtual Machines in High
  Performance Computing Cloud</title><categories>cs.DC cs.NI cs.PF</categories><comments>10 pages, in Procedings of International Conference on Advanced
  Computing and Applications, Journal of Science and Technology, Vietnamese
  Academy of Science and Technology, ISSN 0866-708X, Vol. 51, No. 4B, 2013</comments><acm-class>C.2.4; C.4; D.4.8</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Cloud computing has become more popular in provision of computing resources
under virtual machine (VM) abstraction for high performance computing (HPC)
users to run their applications. A HPC cloud is such cloud computing
environment. One of challenges of energy efficient resource allocation for VMs
in HPC cloud is tradeoff between minimizing total energy consumption of
physical machines (PMs) and satisfying Quality of Service (e.g. performance).
On one hand, cloud providers want to maximize their profit by reducing the
power cost (e.g. using the smallest number of running PMs). On the other hand,
cloud customers (users) want highest performance for their applications. In
this paper, we focus on the scenario that scheduler does not know global
information about user jobs and user applications in the future. Users will
request shortterm resources at fixed start times and non interrupted durations.
We then propose a new allocation heuristic (named Energy-aware and Performance
per watt oriented Bestfit (EPOBF)) that uses metric of performance per watt to
choose which most energy-efficient PM for mapping each VM (e.g. maximum of MIPS
per Watt). Using information from Feitelson's Parallel Workload Archive to
model HPC jobs, we compare the proposed EPOBF to state of the art heuristics on
heterogeneous PMs (each PM has multicore CPU). Simulations show that the EPOBF
can reduce significant total energy consumption in comparison with state of the
art allocation heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7813</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7813</id><created>2013-10-08</created><authors><author><keyname>Coluccia</keyname><forenames>Giulio</forenames></author><author><keyname>Valsesia</keyname><forenames>Diego</forenames></author><author><keyname>Magli</keyname><forenames>Enrico</forenames></author></authors><title>Smoothness-Constrained Image Recovery from Block-Based Random
  Projections</title><categories>cs.CV cs.IT math.IT</categories><journal-ref>Proceedings of the 15th International Workshop on Multimedia
  Signal Processing (MMSP), Pula (Sardinia), Italy, September 30 - October 2,
  2013, pp. 129-134</journal-ref><doi>10.1109/MMSP.2013.6659276</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of visual quality of images
reconstructed from block-wise random projections. Independent reconstruction of
the blocks can severely affect visual quality, by displaying artifacts along
block borders. We propose a method to enforce smoothness across block borders
by modifying the sensing and reconstruction process so as to employ partially
overlapping blocks. The proposed algorithm accomplishes this by computing a
fast preview from the blocks, whose purpose is twofold. On one hand, it allows
to enforce a set of constraints to drive the reconstruction algorithm towards a
smooth solution, imposing the similarity of block borders. On the other hand,
the preview is used as a predictor of the entire block, allowing to recover the
prediction error, only. The quality improvement over the result of independent
reconstruction can be easily assessed both visually and in terms of PSNR and
SSIM index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7828</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7828</id><created>2013-10-29</created><authors><author><keyname>Baeckstroem</keyname><forenames>Christer</forenames></author><author><keyname>Jonsson</keyname><forenames>Peter</forenames></author><author><keyname>Ordyniak</keyname><forenames>Sebastian</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>A Complete Parameterized Complexity Analysis of Bounded Planning</title><categories>cs.AI cs.DS</categories><comments>The paper is a combined and extended version of the papers &quot;The
  Complexity of Planning Revisited - A Parameterized Analysis&quot; (AAAI 2012,
  arXiv:1208.2566) and &quot;Parameterized Complexity and Kernel Bounds for Hard
  Planning Problems&quot; (CIAC 2013, arXiv:1211.0479)</comments><journal-ref>Proc. AAAI'12, pp. 1735-1741, AAAI Press 2012 and Proc. CIAC'13,
  pp. 13-24, Springer</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The propositional planning problem is a notoriously difficult computational
problem, which remains hard even under strong syntactical and structural
restrictions. Given its difficulty it becomes natural to study planning in the
context of parameterized complexity. In this paper we continue the work
initiated by Downey, Fellows and Stege on the parameterized complexity of
planning with respect to the parameter &quot;length of the solution plan.&quot; We
provide a complete classification of the parameterized complexity of the
planning problem under two of the most prominent syntactical restrictions,
i.e., the so called PUBS restrictions introduced by Baeckstroem and Nebel and
restrictions on the number of preconditions and effects as introduced by
Bylander. We also determine which of the considered fixed-parameter tractable
problems admit a polynomial kernel and which don't.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7829</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7829</id><created>2013-10-29</created><authors><author><keyname>Sougui</keyname><forenames>Ines Benali</forenames></author><author><keyname>Hidri</keyname><forenames>Minyar Sassi</forenames></author><author><keyname>Grissa-Touzi</keyname><forenames>Amel</forenames></author></authors><title>About Summarization in Large Fuzzy Databases</title><categories>cs.DB cs.IR</categories><journal-ref>The 5th International Conference on Advances in Databases,
  Knowledge, and Data Applications (DBKDA), pp. 87-94, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Moved by the need increased for modeling of the fuzzy data, the success of
the systems of exact generation of summary of data, we propose in this paper, a
new approach of generation of summary from fuzzy data called Fuzzy-SaintEtiQ.
This approach is an extension of the SaintEtiQ model to support the fuzzy data.
It presents the following optimizations such as 1) the minimization of the
expert risk; 2) the construction of a more detailed and more precise summaries
hierarchy, and 3) the co-operation with the user by giving him fuzzy summaries
in different hierarchical levels
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7834</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7834</id><created>2013-10-29</created><updated>2014-02-24</updated><authors><author><keyname>Swamy</keyname><forenames>Chaitanya</forenames></author></authors><title>Improved Approximation Algorithms for Matroid and Knapsack Median
  Problems and Applications</title><categories>cs.DS</categories><comments>Some minor typos were fixed; added remark about
  Lagrangian-multiplier-preservation property</comments><acm-class>F.2.2; G.1.6; G.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the {\em matroid median} problem \cite{KrishnaswamyKNSS11},
wherein we are given a set of facilities with opening costs and a matroid on
the facility-set, and clients with demands and connection costs, and we seek to
open an independent set of facilities and assign clients to open facilities so
as to minimize the sum of the facility-opening and client-connection costs. We
give a simple 8-approximation algorithm for this problem based on LP-rounding,
which improves upon the 16-approximation in \cite{KrishnaswamyKNSS11}. Our
techniques illustrate that much of the work involved in the rounding algorithm
of in \cite{KrishnaswamyKNSS11} can be avoided by first converting the LP
solution to a half-integral solution, which can then be rounded to an integer
solution using a simple uncapacitated-facility-location (UFL) style clustering
step.
  We illustrate the power of these ideas by deriving: (a) a 24-approximation
algorithm for matroid median with penalties, which is a vast improvement over
the 360-approximation obtained in \cite{KrishnaswamyKNSS11}; and (b) an
8-approximation for the {\em two-matroid median} problem, a generalization of
matroid median that we introduce involving two matroids. We show that a variety
of seemingly disparate facility-location problems considered in the
literature---data placement problem, mobile facility location, $k$-median
forest, metric uniform minimum-latency UFL---in fact reduce to the matroid
median or two-matroid median problems, and thus obtain improved approximation
guarantees for all these problems. Our techniques also yield an improvement for
the knapsack median problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7839</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7839</id><created>2013-10-29</created><authors><author><keyname>Cheung</keyname><forenames>Kent Tsz Kan</forenames></author><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Achieving maximum energy-efficiency in multi-relay OFDMA cellular
  networks: a fractional programming approach</title><categories>cs.IT math.IT</categories><comments>12 pages, 11 figures, 3 tables, published on IEEE Transactions on
  Communications, vol. 61, no. 7, pp. 2746-2757, Jul. 2013</comments><journal-ref>IEEE Transactions on Communications, vol. 61, no. 7, pp.
  2746-2757, Jul. 2013</journal-ref><doi>10.1109/TCOMM.2013.13.120727</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the joint power and subcarrier allocation problem is solved in
the context of maximizing the energy-efficiency (EE) of a multi-user,
multi-relay orthogonal frequency division multiple access (OFDMA) cellular
network, where the objective function is formulated as the ratio of the
spectral-efficiency (SE) over the total power dissipation. It is proven that
the fractional programming problem considered is quasi-concave so that
Dinkelbach's method may be employed for finding the optimal solution at a low
complexity. This method solves the above-mentioned master problem by solving a
series of parameterized concave secondary problems. These secondary problems
are solved using a dual decomposition approach, where each secondary problem is
further decomposed into a number of similar subproblems. The impact of various
system parameters on the attainable EE and SE of the system employing both EE
maximization (EEM) and SE maximization (SEM) algorithms is characterized. In
particular, it is observed that increasing the number of relays for a range of
cell sizes, although marginally increases the attainable SE, reduces the EE
significantly. It is noted that the highest SE and EE are achieved, when the
relays are placed closer to the BS to take advantage of the resultant
line-of-sight link. Furthermore, increasing both the number of available
subcarriers and the number of active user equipment (UE) increases both the EE
and the total SE of the system as a benefit of the increased frequency and
multi-user diversity, respectively. Finally, it is demonstrated that as
expected, increasing the available power tends to improve the SE, when using
the SEM algorithm. By contrast, given a sufficiently high available power, the
EEM algorithm attains the maximum achievable EE and a suboptimal SE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7840</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7840</id><created>2013-10-29</created><updated>2014-06-10</updated><authors><author><keyname>Mehta</keyname><forenames>Rahul</forenames></author></authors><title>A New Push-Relabel Algorithm for Sparse Networks</title><categories>cs.DS</categories><comments>23 pages. arXiv admin note: substantial text overlap with
  arXiv:1309.2525 - This version includes an extension of the result to the
  O(n) edge case</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new push-relabel algorithm for the maximum flow
problem on flow networks with $n$ vertices and $m$ arcs. Our algorithm computes
a maximum flow in $O(mn)$ time on sparse networks where $m = O(n)$. To our
knowledge, this is the first $O(mn)$ time push-relabel algorithm for the $m =
O(n)$ edge case; previously, it was known that push-relabel implementations
could find a max-flow in $O(mn)$ time when $m = \Omega(n^{1+\epsilon})$ (King,
et. al., SODA `92). This also matches a recent flow decomposition-based
algorithm due to Orlin (STOC `13), which finds a max-flow in $O(mn)$ time on
sparse networks.
  Our main result is improving on the Excess-Scaling algorithm (Ahuja &amp; Orlin,
1989) by reducing the number of nonsaturating pushes to $O(mn)$ across all
scaling phases. This is reached by combining Ahuja and Orlin's algorithm with
Orlin's compact flow networks. A contribution of this paper is demonstrating
that the compact networks technique can be extended to the push-relabel family
of algorithms. We also provide evidence that this approach could be a promising
avenue towards an $O(mn)$-time algorithm for all edge densities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7852</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7852</id><created>2013-10-29</created><authors><author><keyname>Gupta</keyname><forenames>Gaurav</forenames></author><author><keyname>Chaturvedi</keyname><forenames>A. K.</forenames></author></authors><title>Conditional Entropy based User Selection for Multiuser MIMO Systems</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures</comments><journal-ref>IEEE Commun. Lett. 17 8 (2013) 1628-1631</journal-ref><doi>10.1109/LCOMM.2013.070113.131393</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of user subset selection for maximizing the sum rate
of downlink multi-user MIMO systems. The brute-force search for the optimal
user set becomes impractical as the total number of users in a cell increase.
We propose a user selection algorithm based on conditional differential
entropy. We apply the proposed algorithm on Block diagonalization scheme.
Simulation results show that the proposed conditional entropy based algorithm
offers better alternatives than the existing user selection algorithms.
Furthermore, in terms of sum rate, the solution obtained by the proposed
algorithm turns out to be close to the optimal solution with significantly
lower computational complexity than brute-force search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7868</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7868</id><created>2013-10-29</created><authors><author><keyname>Pichara</keyname><forenames>Karim</forenames></author><author><keyname>Protopapas</keyname><forenames>Pavlos</forenames></author></authors><title>Automatic Classification of Variable Stars in Catalogs with missing data</title><categories>astro-ph.IM cs.LG stat.ML</categories><journal-ref>2013 ApJ 777 83</journal-ref><doi>10.1088/0004-637X/777/2/83</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an automatic classification method for astronomical catalogs with
missing data. We use Bayesian networks, a probabilistic graphical model, that
allows us to perform inference to pre- dict missing values given observed data
and dependency relationships between variables. To learn a Bayesian network
from incomplete data, we use an iterative algorithm that utilises sampling
methods and expectation maximization to estimate the distributions and
probabilistic dependencies of variables from data with missing values. To test
our model we use three catalogs with missing data (SAGE, 2MASS and UBVI) and
one complete catalog (MACHO). We examine how classification accuracy changes
when information from missing data catalogs is included, how our method
compares to traditional missing data approaches and at what computational cost.
Integrating these catalogs with missing data we find that classification of
variable objects improves by few percent and by 15% for quasar detection while
keeping the computational cost the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7890</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7890</id><created>2013-10-26</created><authors><author><keyname>Verma</keyname><forenames>Adarsh Kumar</forenames></author><author><keyname>Kumar</keyname><forenames>Prashant</forenames></author></authors><title>List Sort: A New Approach for Sorting List to Reduce Execution Time</title><categories>cs.DS</categories><comments>7 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we are proposing a new sorting algorithm, List Sort algorithm,
is based on the dynamic memory allocation. In this research study we have also
shown the comparison of various efficient sorting techniques with List sort.
Due the dynamic nature of the List sort, it becomes much more fast than some
conventional comparison sorting techniques and comparable to Quick Sort and
Merge Sort. List sort takes the advantage of the data which is already sorted
either in ascending order or in descending order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7898</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7898</id><created>2013-10-29</created><authors><author><keyname>Spirakis</keyname><forenames>Paul G.</forenames></author><author><keyname>Akrida</keyname><forenames>Eleni Ch.</forenames></author></authors><title>Moving in temporal graphs with very sparse random availability of edges</title><categories>cs.DS</categories><comments>30 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this work we consider temporal graphs, i.e. graphs, each edge of which is
assigned a set of discrete time-labels drawn from a set of integers. The labels
of an edge indicate the discrete moments in time at which the edge is
available. We also consider temporal paths in a temporal graph, i.e. paths
whose edges are assigned a strictly increasing sequence of labels. Furthermore,
we assume the uniform case (UNI-CASE), in which every edge of a graph is
assigned exactly one time label from a set of integers and the time labels
assigned to the edges of the graph are chosen randomly and independently, with
the selection following the uniform distribution. We call uniform random
temporal graphs the graphs that satisfy the UNI-CASE. We begin by deriving the
expected number of temporal paths of a given length in the uniform random
temporal clique. We define the term temporal distance of two vertices, which is
the arrival time, i.e. the time-label of the last edge, of the temporal path
that connects those vertices, which has the smallest arrival time amongst all
temporal paths that connect those vertices. We then propose and study two
statistical properties of temporal graphs. One is the maximum expected temporal
distance which is, as the term indicates, the maximum of all expected temporal
distances in the graph. The other one is the temporal diameter which, loosely
speaking, is the expectation of the maximum temporal distance in the graph. We
derive the maximum expected temporal distance of a uniform random temporal star
graph as well as an upper bound on both the maximum expected temporal distance
and the temporal diameter of the normalized version of the uniform random
temporal clique, in which the largest time-label available equals the number of
vertices. Finally, we provide an algorithm that solves an optimization problem
on a specific type of temporal (multi)graphs of two vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7903</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7903</id><created>2013-10-29</created><updated>2013-11-06</updated><authors><author><keyname>Banin</keyname><forenames>Matan</forenames></author><author><keyname>Tsaban</keyname><forenames>Boaz</forenames></author></authors><title>A reduction of semigroup DLP to classic DLP</title><categories>cs.CR cs.CC quant-ph</categories><comments>Improved discussion of the nonperiodic case</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a polynomial-time reduction of the discrete logarithm problem in
any periodic (a.k.a. torsion) semigroup (SGDLP) to the same problem in a
subgroup of the same semigroup. It follows that SGDLP can be solved in
polynomial time by quantum computers, and that SGDLP has subexponential
algorithms whenever the classic DLP in the corresponding groups has
subexponential algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7911</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7911</id><created>2013-10-29</created><updated>2013-12-09</updated><authors><author><keyname>Iljazovic</keyname><forenames>Zvonko</forenames><affiliation>University of Zagreb</affiliation></author></authors><title>Compact manifolds with computable boundaries</title><categories>cs.LO math.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (December
  11, 2013) lmcs:891</journal-ref><doi>10.2168/LMCS-9(4:19)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate conditions under which a co-computably enumerable closed set
in a computable metric space is computable and prove that in each locally
computable computable metric space each co-computably enumerable compact
manifold with computable boundary is computable. In fact, we examine the notion
of a semi-computable compact set and we prove a more general result: in any
computable metric space each semi-computable compact manifold with computable
boundary is computable. In particular, each semi-computable compact
(boundaryless) manifold is computable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7919</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7919</id><created>2013-10-29</created><authors><author><keyname>Selen</keyname><forenames>Jori</forenames></author><author><keyname>Nazarathy</keyname><forenames>Yoni</forenames></author><author><keyname>Andrew</keyname><forenames>Lachlan L. H.</forenames></author><author><keyname>Vu</keyname><forenames>Hai L.</forenames></author></authors><title>The age of information in gossip networks</title><categories>math.PR cs.NI</categories><comments>15 pages, 8 figures</comments><journal-ref>In: Analytical and Stochastic Modeling Techniques and
  Applications, pages 364-379, Springer Berlin Heidelberg, 2013</journal-ref><doi>10.1007/978-3-642-39408-9_26</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce models of gossip based communication networks in which each node
is simultaneously a sensor, a relay and a user of information. We model the
status of ages of information between nodes as a discrete time Markov chain. In
this setting a gossip transmission policy is a decision made at each node
regarding what type of information to relay at any given time (if any). When
transmission policies are based on random decisions, we are able to analyze the
age of information in certain illustrative structured examples either by means
of an explicit analysis, an algorithm or asymptotic approximations. Our key
contribution is presenting this class of models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7935</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7935</id><created>2013-10-29</created><updated>2014-04-10</updated><authors><author><keyname>Courtois</keyname><forenames>Nicolas T.</forenames></author><author><keyname>Grajek</keyname><forenames>Marek</forenames></author><author><keyname>Naik</keyname><forenames>Rahul</forenames></author></authors><title>The Unreasonable Fundamental Incertitudes Behind Bitcoin Mining</title><categories>cs.CR cs.CE cs.SI</categories><comments>45 pages, colour figures in jpg, not published elsewhere than arxiv</comments><acm-class>E.3; D.4.6; K.4.1; K.4.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bitcoin is a &quot;crypto currency&quot;, a decentralized electronic payment scheme
based on cryptography which has recently gained excessive popularity.
Scientific research on bitcoin is less abundant. A paper at Financial
Cryptography 2012 conference explains that it is a system which &quot;uses no fancy
cryptography&quot;, and is &quot;by no means perfect&quot;. It depends on a well-known
cryptographic standard SHA-256. In this paper we revisit the cryptographic
process which allows one to make money by producing bitcoins. We reformulate
this problem as a Constrained Input Small Output (CISO) hashing problem and
reduce the problem to a pure block cipher problem. We estimate the speed of
this process and we show that the cost of this process is less than it seems
and it depends on a certain cryptographic constant which we estimated to be at
most 1.86. These optimizations enable bitcoin miners to save tens of millions
of dollars per year in electricity bills. Miners who set up mining operations
face many economic incertitudes such as high volatility. In this paper we point
out that there are fundamental incertitudes which depend very strongly on the
bitcoin specification. The energy efficiency of bitcoin miners have already
been improved by a factor of about 10,000, and we claim that further
improvements are inevitable. Better technology is bound to be invented, would
it be quantum miners. More importantly, the specification is likely to change.
A major change have been proposed in May 2013 at Bitcoin conference in San
Diego by Dan Kaminsky. However, any sort of change could be flatly rejected by
the community which have heavily invested in mining with the current
technology. Another question is the reward halving scheme in bitcoin. The
current bitcoin specification mandates a strong 4-year cyclic property. We find
this property totally unreasonable and harmful and explain why and how it needs
to be changed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7950</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7950</id><created>2013-09-09</created><authors><author><keyname>Jones</keyname><forenames>Austin</forenames></author><author><keyname>Schwager</keyname><forenames>Mac</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Technical Report: Distribution Temporal Logic: Combining Correctness
  with Quality of Estimation</title><categories>cs.SY cs.AI cs.LO</categories><comments>More expanded version of &quot;Distribution Temporal Logic: Combining
  Correctness with Quality of Estimation&quot; to appear in IEEE CDC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new temporal logic called Distribution Temporal Logic (DTL)
defined over predicates of belief states and hidden states of partially
observable systems. DTL can express properties involving uncertainty and
likelihood that cannot be described by existing logics. A co-safe formulation
of DTL is defined and algorithmic procedures are given for monitoring
executions of a partially observable Markov decision process with respect to
such formulae. A simulation case study of a rescue robotics application
outlines our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7951</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7951</id><created>2013-10-29</created><authors><author><keyname>Morvan</keyname><forenames>Gildas</forenames></author><author><keyname>Veremme</keyname><forenames>Alexandre</forenames></author><author><keyname>Dupont</keyname><forenames>Daniel</forenames></author></authors><title>IRM4MLS: the influence reaction model for multi-level simulation</title><categories>cs.MA</categories><journal-ref>Multi-Agent-Based Simulation XI, LNCS 6532, 2011, pp 16-27</journal-ref><doi>10.1007/978-3-642-18345-4_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a meta-model called IRM4MLS, that aims to be a generic ground
to specify and execute multi-level agent-based models is presented. It relies
on the influence/reaction principle and more specifically on IRM4S. Simulation
models for IRM4MLS are defined. The capabilities and possible extensions of the
meta-model are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7957</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7957</id><created>2013-09-27</created><authors><author><keyname>Zhang</keyname><forenames>Zhu</forenames></author><author><keyname>Zeng</keyname><forenames>Daniel</forenames></author><author><keyname>Abbasi</keyname><forenames>Ahmed</forenames></author><author><keyname>Peng</keyname><forenames>Jing</forenames></author></authors><title>A Random Walk Model for Item Recommendation in Folksonomies</title><categories>cs.IR</categories><journal-ref>Zhang, Z., Zeng, D., Abbasi, A., and Peng, J. &quot;A Random Walk Model
  for Item Recommendation in Folksonomies,&quot; In Proceedings of the 21st Annual
  Workshop on Information Technologies and Systems, Shanghai, China, December
  3-4, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social tagging, as a novel approach to information organization and
discovery, has been widely adopted in many Web2.0 applications. The tags
provide a new type of information that can be exploited by recommender systems.
Nevertheless, the sparsity of ternary &lt;user, tag, item&gt; interaction data limits
the performance of tag-based collaborative filtering. This paper proposes a
random-walk-based algorithm to deal with the sparsity problem in social tagging
data, which captures the potential transitive associations between users and
items through their interaction with tags. In particular, two smoothing
strategies are presented from both the user-centric and item-centric
perspectives. Experiments on real-world data sets empirically demonstrate the
efficacy of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7961</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7961</id><created>2013-09-16</created><authors><author><keyname>Khaze</keyname><forenames>Seyyed Reza</forenames></author><author><keyname>maleki</keyname><forenames>Isa</forenames></author><author><keyname>Hojjatkhah</keyname><forenames>Sohrab</forenames></author><author><keyname>Bagherinia</keyname><forenames>Ali</forenames></author></authors><title>Evaluation the efficiency of artificial bee colony and the firefly
  algorithm in solving the continuous optimization problem</title><categories>cs.NE cs.AI</categories><journal-ref>International Journal on Computational Sciences &amp; Applications
  (IJCSA) Vol.3, No.4, August 2013</journal-ref><doi>10.5121/ijcsa.2013.3403</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now the Meta-Heuristic algorithms have been used vastly in solving the
problem of continuous optimization. In this paper the Artificial Bee Colony
(ABC) algorithm and the Firefly Algorithm (FA) are valuated. And for presenting
the efficiency of the algorithms and also for more analysis of them, the
continuous optimization problems which are of the type of the problems of vast
limit of answer and the close optimized points are tested. So, in this paper
the efficiency of the ABC algorithm and FA are presented for solving the
continuous optimization problems and also the said algorithms are studied from
the accuracy in reaching the optimized solution and the resulting time and the
reliability of the optimized answer points of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7962</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7962</id><created>2013-09-14</created><authors><author><keyname>Mitrevski</keyname><forenames>Pece</forenames></author><author><keyname>Kostoska</keyname><forenames>Olivera</forenames></author><author><keyname>Angeleski</keyname><forenames>Marjan</forenames></author></authors><title>E-Business Implications for Productivity and Competitiveness</title><categories>cs.OH</categories><journal-ref>Annals of the &quot;Constantin Brancusi&quot; University of Targu - Jiu,
  Economy Series, Issue 1/2009, ISSN 1844-7007, pp. 253-262</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information and Communication Technology (ICT) affects to a great extent the
output and productivity growth. Evidence suggests that investment growth in ICT
has rapidly accelerated the TFP (total factor productivity) growth within the
European Union. Such progress is particularly essential for the sectors which
themselves produce new technology, but it is dispersing to other sectors, as
well. Nevertheless, decrease in ICT investment does not necessarily decline the
ICT contribution to output and productivity growth. These variations come out
from the problems related to the particular phenomenon proper assessment, but
predominantly from the companies' special requirements, as well as the
necessary adjustments of labour employed. Hence, this paper aims at estimating
the huge distinction in terms of ICT and TFB contributions to labour
productivity growth among some of the European member states, as well as the
factors which might stand behind the particular findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7965</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7965</id><created>2013-09-06</created><authors><author><keyname>Wac</keyname><forenames>Katarzyna</forenames></author></authors><title>Smartphone as a Personal, Pervasive Health Informatics Services
  Platform: Literature Review</title><categories>cs.CY cs.HC</categories><journal-ref>Yearb Med Inform. 2012;7(1):83-93</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objectives: The article provides an overview of current trends in personal
sensor, signal and imaging informatics, that are based on emerging mobile
computing and communications technologies enclosed in a smartphone and enabling
the provision of personal, pervasive health informatics services.
  Methods: The article reviews examples of these trends from the PubMed and
Google scholar literature search engines, which, by no means claim to be
complete, as the field is evolving and some recent advances may not be
documented yet.
  Results: There exist critical technological advances in the surveyed
smartphone technologies, employed in provision and improvement of diagnosis,
acute and chronic treatment and rehabilitation health services, as well as in
education and training of healthcare practitioners. However, the most emerging
trend relates to a routine application of these technologies in a
prevention/wellness sector, helping its users in self-care to stay healthy.
  Conclusions: Smartphone-based personal health informatics services exist, but
still have a long way to go to become an everyday, personalized
healthcare-provisioning tool in the medical field and in a clinical practice.
Key main challenge for their widespread adoption involve lack of user
acceptance striving from variable credibility and reliability of applications
and solutions as they a) lack evidence-based approach; b) have low levels of
medical professional involvement in their design and content; c) are provided
in an unreliable way, influencing negatively its usability; and, in some cases,
d) being industry-driven, hence exposing bias in information provided, for
example towards particular types of treatment or intervention procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7981</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7981</id><created>2013-10-29</created><authors><author><keyname>Veloz</keyname><forenames>Tomas</forenames></author><author><keyname>Gabora</keyname><forenames>Liane</forenames></author><author><keyname>Eyjolfson</keyname><forenames>Mark</forenames></author><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author></authors><title>Toward a Formal Model of the Shifting Relationship between Concepts and
  Contexts during Associative Thought</title><categories>q-bio.NC cs.AI</categories><comments>11 pages; 2 figures</comments><journal-ref>(2011). Lecture Notes in Computer Science 7052: Proceedings Fifth
  International Symposium on Quantum Interaction. June 27-29, Aberdeen, UK.
  Berlin: Springer</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quantum inspired State Context Property (SCOP) theory of concepts is
unique amongst theories of concepts in offering a means of incorporating that
for each concept in each different context there are an unlimited number of
exemplars, or states, of varying degrees of typicality. Working with data from
a study in which participants were asked to rate the typicality of exemplars of
a concept for different contexts, and introducing an exemplar typicality
threshold, we built a SCOP model of how states of a concept arise differently
in associative versus analytic (or divergent and convergent) modes of thought.
Introducing measures of state robustness and context relevance, we show that by
varying the threshold, the relevance of different contexts changes, and
seemingly atypical states can become typical. The formalism provides a pivotal
step toward a formal explanation of creative thought proesses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7991</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7991</id><created>2013-10-29</created><updated>2014-07-28</updated><authors><author><keyname>Agarwal</keyname><forenames>Alekh</forenames></author><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Netrapalli</keyname><forenames>Praneeth</forenames></author></authors><title>Learning Sparsely Used Overcomplete Dictionaries via Alternating
  Minimization</title><categories>cs.LG math.OC stat.ML</categories><comments>Local linear convergence now holds under RIP and also more general
  restricted eigenvalue conditions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of sparse coding, where each sample consists of a
sparse linear combination of a set of dictionary atoms, and the task is to
learn both the dictionary elements and the mixing coefficients. Alternating
minimization is a popular heuristic for sparse coding, where the dictionary and
the coefficients are estimated in alternate steps, keeping the other fixed.
Typically, the coefficients are estimated via $\ell_1$ minimization, keeping
the dictionary fixed, and the dictionary is estimated through least squares,
keeping the coefficients fixed. In this paper, we establish local linear
convergence for this variant of alternating minimization and establish that the
basin of attraction for the global optimum (corresponding to the true
dictionary and the coefficients) is $\order{1/s^2}$, where $s$ is the sparsity
level in each sample and the dictionary satisfies RIP. Combined with the recent
results of approximate dictionary estimation, this yields provable guarantees
for exact recovery of both the dictionary elements and the coefficients, when
the dictionary elements are incoherent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7994</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7994</id><created>2013-10-29</created><authors><author><keyname>Ding</keyname><forenames>Weicong</forenames></author><author><keyname>Ishwar</keyname><forenames>Prakash</forenames></author><author><keyname>Rohban</keyname><forenames>Mohammad H.</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Necessary and Sufficient Conditions for Novel Word Detection in
  Separable Topic Models</title><categories>cs.LG cs.IR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The simplicial condition and other stronger conditions that imply it have
recently played a central role in developing polynomial time algorithms with
provable asymptotic consistency and sample complexity guarantees for topic
estimation in separable topic models. Of these algorithms, those that rely
solely on the simplicial condition are impractical while the practical ones
need stronger conditions. In this paper, we demonstrate, for the first time,
that the simplicial condition is a fundamental, algorithm-independent,
information-theoretic necessary condition for consistent separable topic
estimation. Furthermore, under solely the simplicial condition, we present a
practical quadratic-complexity algorithm based on random projections which
consistently detects all novel words of all topics using only up to
second-order empirical word moments. This algorithm is amenable to distributed
implementation making it attractive for 'big-data' scenarios involving a
network of large distributed databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8004</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8004</id><created>2013-10-29</created><authors><author><keyname>Wang</keyname><forenames>Boyu</forenames></author><author><keyname>Pineau</keyname><forenames>Joelle</forenames></author></authors><title>Online Ensemble Learning for Imbalanced Data Streams</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While both cost-sensitive learning and online learning have been studied
extensively, the effort in simultaneously dealing with these two issues is
limited. Aiming at this challenge task, a novel learning framework is proposed
in this paper. The key idea is based on the fusion of online ensemble
algorithms and the state of the art batch mode cost-sensitive bagging/boosting
algorithms. Within this framework, two separately developed research areas are
bridged together, and a batch of theoretically sound online cost-sensitive
bagging and online cost-sensitive boosting algorithms are first proposed.
Unlike other online cost-sensitive learning algorithms lacking theoretical
analysis of asymptotic properties, the convergence of the proposed algorithms
is guaranteed under certain conditions, and the experimental evidence with
benchmark data sets also validates the effectiveness and efficiency of the
proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8038</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8038</id><created>2013-10-30</created><updated>2013-11-14</updated><authors><author><keyname>Li</keyname><forenames>Angsheng</forenames></author><author><keyname>Pan</keyname><forenames>Yicheng</forenames></author><author><keyname>Li</keyname><forenames>Jiankou</forenames></author></authors><title>Community Structures Are Definable in Networks: A Structural Theory of
  Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We found that neither randomness in the ER model nor the preferential
attachment in the PA model is the mechanism of community structures of
networks, that community structures are universal in real networks, that
community structures are definable in networks, that communities are
interpretable in networks, and that homophyly is the mechanism of community
structures and a structural theory of networks. We proposed the notions of
entropy- and conductance-community structures. It was shown that the two
definitions of the entropy- and conductance-community structures and the notion
of modularity proposed by physicists are all equivalent in defining community
structures of networks, that neither randomness in the ER model nor
preferential attachment in the PA model is the mechanism of community
structures of networks, and that the existence of community structures is a
universal phenomenon in real networks. This poses a fundamental question: What
are the mechanisms of community structures of real networks? To answer this
question, we proposed a homophyly model of networks. It was shown that networks
of our model satisfy a series of new topological, probabilistic and
combinatorial principles, including a fundamental principle, a community
structure principle, a degree priority principle, a widths principle, an
inclusion and infection principle, a king node principle and a predicting
principle etc. The new principles provide a firm foundation for a structural
theory of networks. Our homophyly model demonstrates that homophyly is the
underlying mechanism of community structures of networks, that nodes of the
same community share common features, that power law and small world property
are never obstacles of the existence of community structures in networks, that
community structures are {\it definable} in networks, and that (natural)
communities are {\it interpretable}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8040</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8040</id><created>2013-10-30</created><authors><author><keyname>Li</keyname><forenames>Angsheng</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Pan</keyname><forenames>Yicheng</forenames></author><author><keyname>Li</keyname><forenames>Xuechen</forenames></author></authors><title>Homophyly and Randomness Resist Cascading Failure in Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The universal properties of power law and small world phenomenon of networks
seem unavoidably obstacles for security of networking systems. Existing models
never give secure networks. We found that the essence of security is the
security against cascading failures of attacks and that nature solves the
security by mechanisms. We proposed a model of networks by the natural
mechanisms of homophyly, randomness and preferential attachment. It was shown
that homophyly creates a community structure, that homophyly and randomness
introduce ordering in the networks, and that homophyly creates inclusiveness
and introduces rules of infections. These principles allow us to provably
guarantee the security of the networks against any attacks. Our results show
that security can be achieved provably by structures, that there is a tradeoff
between the roles of structures and of thresholds in security engineering, and
that power law and small world property are never obstacles for security of
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8057</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8057</id><created>2013-10-30</created><updated>2014-02-06</updated><authors><author><keyname>Yousefbeiki</keyname><forenames>Mohsen</forenames></author><author><keyname>Najibi</keyname><forenames>Halima</forenames></author><author><keyname>Perruisseau-Carrier</keyname><forenames>Julien</forenames></author></authors><title>User Effects in Beam-Space MIMO</title><categories>cs.IT math.IT</categories><comments>4 pages, 7 figures, 2 tables</comments><journal-ref>IEEE Antennas and Wireless Propagation Letters, vol. 12, pp.
  1716-1719, 2013</journal-ref><doi>10.1109/LAWP.2013.2296560</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance and design of the novel single-RF-chain beam-space MIMO
antenna concept is evaluated for the first time in the presence of the user.
First, the variations of different performance parameters are evaluated when
placing a beam-space MIMO antenna in close proximity to the user body in
several typical operating scenarios. In addition to the typical degradation of
conventional antennas in terms of radiation efficiency and impedance matching,
it is observed that the user body corrupts the power balance and the
orthogonality of the beam-space MIMO basis. However, capacity analyses show
that throughput reduction mainly stems from the absorption in user body tissues
rather than from the power imbalance and the correlation of the basis. These
results confirm that the beam-space MIMO concept, so far only demonstrated in
the absence of external perturbation, still performs very well in typical human
body interaction scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8059</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8059</id><created>2013-10-30</created><authors><author><keyname>Slimani</keyname><forenames>Thabet</forenames></author></authors><title>Description and Evaluation of Semantic Similarity Measures Approaches</title><categories>cs.CL</categories><comments>10 pages</comments><journal-ref>International Journal of Computer Applications 80(10):25-33,
  October 2013. Published by Foundation of Computer Science, New York, USA</journal-ref><doi>10.5120/13897-1851</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, semantic similarity measure has a great interest in Semantic
Web and Natural Language Processing (NLP). Several similarity measures have
been developed, being given the existence of a structured knowledge
representation offered by ontologies and corpus which enable semantic
interpretation of terms. Semantic similarity measures compute the similarity
between concepts/terms included in knowledge sources in order to perform
estimations. This paper discusses the existing semantic similarity methods
based on structure, information content and feature approaches. Additionally,
we present a critical evaluation of several categories of semantic similarity
approaches based on two standard benchmarks. The aim of this paper is to give
an efficient evaluation of all these measures which help researcher and
practitioners to select the measure that best fit for their requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8062</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8062</id><created>2013-10-30</created><authors><author><keyname>Lee</keyname><forenames>Cheng-Wei</forenames></author><author><keyname>Lu</keyname><forenames>Hsueh-I</forenames></author></authors><title>Replacement Paths via Row Minima of Concise Matrices</title><categories>cs.DS</categories><comments>23 pages, 1 table, 9 figures, accepted to SIAM Journal on Discrete
  Mathematics</comments><journal-ref>SIAM Journal on Discrete Mathematics 28(1):206-225, 2014</journal-ref><doi>10.1137/120897146</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix $M$ is {\em $k$-concise} if the finite entries of each column of $M$
consist of $k$ or less intervals of identical numbers. We give an $O(n+m)$-time
algorithm to compute the row minima of any $O(1)$-concise $n\times m$ matrix.
Our algorithm yields the first $O(n+m)$-time reductions from the
replacement-paths problem on an $n$-node $m$-edge undirected graph
(respectively, directed acyclic graph) to the single-source shortest-paths
problem on an $O(n)$-node $O(m)$-edge undirected graph (respectively, directed
acyclic graph). That is, we prove that the replacement-paths problem is no
harder than the single-source shortest-paths problem on undirected graphs and
directed acyclic graphs. Moreover, our linear-time reductions lead to the first
$O(n+m)$-time algorithms for the replacement-paths problem on the following
classes of $n$-node $m$-edge graphs (1) undirected graphs in the word-RAM model
of computation, (2) undirected planar graphs, (3) undirected minor-closed
graphs, and (4) directed acyclic graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8063</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8063</id><created>2013-10-30</created><authors><author><keyname>Kumar</keyname><forenames>Ashish</forenames></author><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author></authors><title>Some Efficient Solutions to Yao's Millionaire Problem</title><categories>cs.CR</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present three simple and efficient protocol constructions to solve Yao's
Millionaire Problem when the parties involved are non-colluding and
semi-honest. The first construction uses a partially homomorphic Encryption
Scheme and is a 4-round scheme using 2 encryptions, 2 homomorphic circuit
evaluations (subtraction and XOR) and a single decryption. The second
construction uses an untrusted third party and achieves a communication
overhead linear in input bit-size with the help of an order preserving
function.Moreover, the second construction does not require an apriori input
bound and can work on inputs of different bit-sizes. The third construction
does not use a third party and, even though, it has a quadratic communication
overhead, it is a fairly simple construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8067</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8067</id><created>2013-10-30</created><updated>2014-03-10</updated><authors><author><keyname>Tervo</keyname><forenames>Valtteri</forenames></author><author><keyname>T&#xf6;lli</keyname><forenames>Antti</forenames></author><author><keyname>Karjalainen</keyname><forenames>Juha</forenames></author><author><keyname>Matsumoto</keyname><forenames>Tad</forenames></author></authors><title>Convergence Constrained Multiuser Transmitter-Receiver Optimization in
  Single Carrier FDMA</title><categories>cs.IT math.IT</categories><comments>Under review in IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convergence constrained power allocation (CCPA) in single carrier multiuser
(MU) single-input multiple-output (SIMO) systems with turbo equalization is
considered in this paper. In order to exploit full benefit of the iterative
receiver, its convergence properties need to be considered also at the
transmitter side. The proposed scheme can guarantee that the desired quality of
service (QoS) is achieved after sufficient amount of iterations. We propose two
different successive convex approximations for solving the non-convex power
minimization problem subject to user specific QoS constraints. The results of
extrinsic information transfer (EXIT) chart analysis demonstrate that the
proposed CCPA scheme can achieve the design objective. Numerical results show
that the proposed schemes can achieve superior performance in terms of power
consumption as compared to linear receivers with and without precoding as well
as to the iterative receiver without precoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8089</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8089</id><created>2013-10-30</created><updated>2015-03-12</updated><authors><author><keyname>Allili</keyname><forenames>Madjid</forenames></author><author><keyname>Kaczynski</keyname><forenames>Tomasz</forenames></author><author><keyname>Landi</keyname><forenames>Claudia</forenames></author></authors><title>Reducing complexes in multidimensional persistent homology theory</title><categories>cs.CG</categories><comments>Construction of indexing map on vertices has been added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Discrete Morse Theory of Forman appeared to be useful for providing
filtration-preserving reductions of complexes in the study of persistent
homology. So far, the algorithms computing discrete Morse matchings have only
been used for one-dimensional filtrations. This paper is perhaps the first
attempt in the direction of extending such algorithms to multidimensional
filtrations. Initial framework related to Morse matchings for the
multidimensional setting is proposed, and a matching algorithm given by King,
Knudson, and Mramor is extended in this direction. The correctness of the
algorithm is proved, and its complexity analyzed. The algorithm is used for
establishing a reduction of a simplicial complex to a smaller but not
necessarily optimal cellular complex. First experiments with filtrations of
triangular meshes are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8097</identifier>
 <datestamp>2014-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8097</id><created>2013-10-30</created><updated>2014-08-14</updated><authors><author><keyname>Schr&#xf6;cker</keyname><forenames>Hans-Peter</forenames></author><author><keyname>Weber</keyname><forenames>Matthias J.</forenames></author></authors><title>Guaranteed Collision Detection With Toleranced Motions</title><categories>cs.CG cs.RO</categories><comments>Accepted for publication in Computer Aided Geometric Design</comments><msc-class>65D18, 70B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for guaranteed collision detection with toleranced
motions. The basic idea is to consider the motion as a curve in the
12-dimensional space of affine displacements, endowed with an object-oriented
Euclidean metric, and cover it with balls. The associated orbits of points,
lines, planes and polygons have particularly simple shapes that lend themselves
well to exact and fast collision queries. We present formulas for elementary
collision tests with these orbit shapes and we suggest an algorithm, based on
motion subdivision and computation of bounding balls, that can give a
no-collision guarantee. It allows a robust and efficient implementation and
parallelization. At hand of several examples we explore the asymptotic behavior
of the algorithm and compare different implementation strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8107</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8107</id><created>2013-10-30</created><updated>2014-02-01</updated><authors><author><keyname>Kutyniok</keyname><forenames>Gitta</forenames></author><author><keyname>Okoudjou</keyname><forenames>Kasso A.</forenames></author><author><keyname>Philipp</keyname><forenames>Friedrich</forenames></author></authors><title>Scalable Frames and Convex Geometry</title><categories>math.NA cs.IT math.FA math.IT</categories><comments>14 pages, to appear in Contemporary Math</comments><msc-class>42C15, 52B11, 15A03, 65F08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently introduced and characterized scalable frames can be considered
as those frames which allow for perfect preconditioning in the sense that the
frame vectors can be rescaled to yield a tight frame. In this paper we define
$m$-scalability, a refinement of scalability based on the number of non-zero
weights used in the rescaling process, and study the connection between this
notion and elements from convex geometry. Finally, we provide results on the
topology of scalable frames. In particular, we prove that the set of scalable
frames with &quot;small&quot; redundancy is nowhere dense in the set of frames.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8111</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8111</id><created>2013-10-30</created><authors><author><keyname>Elmir</keyname><forenames>Abir</forenames></author><author><keyname>Elmir</keyname><forenames>Badr</forenames></author><author><keyname>Bounabat</keyname><forenames>Bouchaib</forenames></author></authors><title>Towards an Assessment-oriented Model for External Information System
  Quality Characterization</title><categories>cs.SE</categories><comments>8 pages, 5 figures, 4 tables, Keywords: Information system quality
  management, External quality characteristics, Quality assessment, RatQual
  Model. arXiv admin note: text overlap with arXiv:1110.1277 by other authors
  without attribution</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 4, No 2, July 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information System Quality (ISQ) management discipline requires a set of
assessment mechanisms to evaluate external quality characteristics that are
influenced by the environmental parameters and impacted by the ecosystem
factors. The present paper suggests a new assessment oriented model that takes
into consideration all facets of each external quality feature. The proposed
model, named RatQual, gives a hierarchical categorization for quality. RatQual
is designed to quantify dependent-environment qualities by considering
internal, external and in use aspects. This model is supported by a tool that
automates the assessment process. This tool gives assistance in quality
evolution planning and serves for periodical monitoring operations used to
enhance and improve information system quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8115</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8115</id><created>2013-10-30</created><authors><author><keyname>Congedo</keyname><forenames>Marco</forenames></author><author><keyname>Barachant</keyname><forenames>Alexandre</forenames></author><author><keyname>Andreev</keyname><forenames>Anton</forenames></author></authors><title>A New Generation of Brain-Computer Interface Based on Riemannian
  Geometry</title><categories>cs.HC math.DG</categories><comments>33 pages, 9 Figures, 17 equations/algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the cumulated experience over the past 25 years in the field of
Brain-Computer Interface (BCI) we can now envision a new generation of BCI.
Such BCIs will not require training; instead they will be smartly initialized
using remote massive databases and will adapt to the user fast and effectively
in the first minute of use. They will be reliable, robust and will maintain
good performances within and across sessions. A general classification
framework based on recent advances in Riemannian geometry and possessing these
characteristics is presented. It applies equally well to BCI based on
event-related potentials (ERP), sensorimotor (mu) rhythms and steady-state
evoked potential (SSEP). The framework is very simple, both algorithmically and
computationally. Due to its simplicity, its ability to learn rapidly (with
little training data) and its good across-subject and across-session
generalization, this strategy a very good candidate for building a new
generation of BCIs, thus we hereby propose it as a benchmark method for the
field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8120</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8120</id><created>2013-10-30</created><authors><author><keyname>Angiulli</keyname><forenames>Fabrizio</forenames></author><author><keyname>Ben-Eliyahu-Zohary</keyname><forenames>Rachel</forenames></author><author><keyname>Fassetti</keyname><forenames>Fabio</forenames></author><author><keyname>Palopoli</keyname><forenames>Luigi</forenames></author></authors><title>On the Tractability of Minimal Model Computation for Some CNF Theories</title><categories>cs.AI cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing algorithms capable of efficiently constructing minimal models of
CNFs is an important task in AI. This paper provides new results along this
research line and presents new algorithms for performing minimal model finding
and checking over positive propositional CNFs and model minimization over
propositional CNFs. An algorithmic schema, called the Generalized Elimination
Algorithm (GEA) is presented, that computes a minimal model of any positive
CNF. The schema generalizes the Elimination Algorithm (EA) [BP97], which
computes a minimal model of positive head-cycle-free (HCF) CNF theories. While
the EA always runs in polynomial time in the size of the input HCF CNF, the
complexity of the GEA depends on the complexity of the specific eliminating
operator invoked therein, which may in general turn out to be exponential.
Therefore, a specific eliminating operator is defined by which the GEA
computes, in polynomial time, a minimal model for a class of CNF that strictly
includes head-elementary-set-free (HEF) CNF theories [GLL06], which form, in
their turn, a strict superset of HCF theories. Furthermore, in order to deal
with the high complexity associated with recognizing HEF theories, an
&quot;incomplete&quot; variant of the GEA (called IGEA) is proposed: the resulting
schema, once instantiated with an appropriate elimination operator, always
constructs a model of the input CNF, which is guaranteed to be minimal if the
input theory is HEF. In the light of the above results, the main contribution
of this work is the enlargement of the tractability frontier for the minimal
model finding and checking and the model minimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8121</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8121</id><created>2013-10-27</created><updated>2015-01-08</updated><authors><author><keyname>Jaffer</keyname><forenames>Aubrey</forenames></author></authors><title>Easy Accurate Reading and Writing of Floating-Point Numbers</title><categories>cs.NA</categories><comments>7 pages, 6 figures</comments><msc-class>65G04</msc-class><acm-class>G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Presented here are algorithms for converting between (decimal)
scientific-notation and (binary) IEEE-754 double-precision floating-point
numbers. By employing a rounding integer quotient operation these algorithms
are much simpler than those previously published. The values are stable under
repeated conversions between the formats. Unlike Java-1.6, the scientific
representations generated use only the minimum number of mantissa digits needed
to convert back to the original binary values.
  Implemented in Java these algorithms execute as fast or faster than Java's
native conversions over nearly all of the IEEE-754 double-precision range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8125</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8125</id><created>2013-08-23</created><authors><author><keyname>Qazi</keyname><forenames>Sameer</forenames></author><author><keyname>Moors</keyname><forenames>Tim</forenames></author></authors><title>Finding Alternate Paths in the Internet:A Survey of Techniques for
  End-to-End Path Discovery</title><categories>cs.NI</categories><comments>13 pages, 10 figures</comments><acm-class>B.8.0</acm-class><journal-ref>International Journal of Current Engineering and Technology, Vol
  2, No 4, ISSN 2277 - 4106, December 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet provides physical path diversity between a large number of
hosts, making it possible for networks to use alternative paths when one path
fails to deliver the required Quality of Service. However, for various reasons,
many established protocols (e.g. de facto Internet inter-domain routing
protocol, Border-Gateway Protocol - BGP) do not fully exploit such alternate
paths. This paper surveys research into techniques for discovering end-to-end
alternate paths, including those based on monitoring path performance, choosing
paths that are maximally disjoint, and in routing across multiple paths. It
surveys proposals for making BGP better able to exploit multiple paths and how
multi-homing can create alternate paths. It also describes how alternate paths
can be realized through detour routing (application layer mechanisms) and
routing deflections (network layer mechanisms). It also discusses Fast Re-Route
techniques for construction of backup routes. It concludes by surveying open
research issues into the discovery and use of alternate paths in the Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8135</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8135</id><created>2013-10-29</created><updated>2015-01-15</updated><authors><author><keyname>Chaudhury</keyname><forenames>Kunal N.</forenames></author><author><keyname>Khoo</keyname><forenames>Yuehaw</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author></authors><title>Large-Scale Sensor Network Localization via Rigid Subnetwork
  Registration</title><categories>cs.NI cs.IT cs.SY math.IT math.OC</categories><comments>5 pages, 8 figures, 1 table. To appear in Proc. IEEE International
  Conference on Acoustics, Speech, and Signal Processing, April 19-24, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe an algorithm for sensor network localization (SNL)
that proceeds by dividing the whole network into smaller subnetworks, then
localizes them in parallel using some fast and accurate algorithm, and finally
registers the localized subnetworks in a global coordinate system. We
demonstrate that this divide-and-conquer algorithm can be used to leverage
existing high-precision SNL algorithms to large-scale networks, which could
otherwise only be applied to small-to-medium sized networks. The main
contribution of this paper concerns the final registration phase. In
particular, we consider a least-squares formulation of the registration problem
(both with and without anchor constraints) and demonstrate how this otherwise
non-convex problem can be relaxed into a tractable convex program. We provide
some preliminary simulation results for large-scale SNL demonstrating that the
proposed registration algorithm (together with an accurate localization scheme)
offers a good tradeoff between run time and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8146</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8146</id><created>2013-10-23</created><authors><author><keyname>Linusson</keyname><forenames>Svante</forenames></author><author><keyname>Ryd</keyname><forenames>Gustav</forenames></author></authors><title>Dynamic adjustment: an electoral method for relaxed double
  proportionality</title><categories>math.OC cs.SI physics.soc-ph</categories><comments>16 pages, to appear in Annals of Operation Research</comments><msc-class>91B12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an electoral system for distributing seats in a parliament. It
gives proportionality for the political parties and close to proportionality
for constituencies. The system suggested here is a version of the system used
in Sweden and other Nordic countries with permanent seats in each constituency
and adjustment seats to give proportionality on the national level. In the
national election of 2010 the current Swedish system failed to give
proportionality between parties. We examine here one possible cure for this
unwanted behavior. The main difference compared to the current Swedish system
is that the number of adjustment seats is not fixed, but rather dynamically
determined to be as low as possible and still insure proportionality between
parties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8148</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8148</id><created>2013-10-30</created><updated>2014-01-20</updated><authors><author><keyname>Blumensath</keyname><forenames>Achim</forenames><affiliation>Universit&#xe9; Paris Diderot</affiliation></author><author><keyname>Courcelle</keyname><forenames>Bruno</forenames><affiliation>Universit&#xe9; Bordeaux 1</affiliation></author></authors><title>Monadic second-order definable graph orderings</title><categories>cs.LO math.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (January
  21, 2014) lmcs:793</journal-ref><doi>10.2168/LMCS-10(1:2)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the question of whether, for a given class of finite graphs, one can
define, for each graph of the class, a linear ordering in monadic second-order
logic, possibly with the help of monadic parameters. We consider two variants
of monadic second-order logic: one where we can only quantify over sets of
vertices and one where we can also quantify over sets of edges. For several
special cases, we present combinatorial characterisations of when such a linear
ordering is definable. In some cases, for instance for graph classes that omit
a fixed graph as a minor, the presented conditions are necessary and
sufficient; in other cases, they are only necessary. Other graph classes we
consider include complete bipartite graphs, split graphs, chordal graphs, and
cographs. We prove that orderability is decidable for the so called
HR-equational classes of graphs, which are described by equation systems and
generalize the context-free languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8156</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8156</id><created>2013-10-30</created><updated>2013-12-17</updated><authors><author><keyname>Hetzl</keyname><forenames>Stefan</forenames><affiliation>INRIA Saclay -- Ile-de-France</affiliation></author><author><keyname>burger</keyname><forenames>Lutz Stra&#xdf;</forenames><affiliation>INRIA Saclay -- Ile-de-France</affiliation></author></authors><title>Herbrand-Confluence</title><categories>cs.LO</categories><comments>25 pages, final version, accepted for publication at LMCS, special
  issue for CSL 2012</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (December
  18, 2013) lmcs:727</journal-ref><doi>10.2168/LMCS-9(4:24)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider cut-elimination in the sequent calculus for classical first-order
logic. It is well known that this system, in its most general form, is neither
confluent nor strongly normalizing. In this work we take a coarser (and
mathematically more realistic) look at cut-free proofs. We analyze which
witnesses they choose for which quantifiers, or in other words: we only
consider the Herbrand-disjunction of a cut-free proof. Our main theorem is a
confluence result for a natural class of proofs: all (possibly infinitely many)
normal forms of the non-erasing reduction lead to the same
Herbrand-disjunction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8182</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8182</id><created>2013-10-30</created><updated>2013-12-04</updated><authors><author><keyname>B&#xe8;s</keyname><forenames>Alexis</forenames><affiliation>LACL, University of Paris-Est Cr&#xe9;teil</affiliation></author></authors><title>Expansions of MSO by cardinality relations</title><categories>cs.LO math.LO</categories><comments>to appear in LMCS</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (December
  5, 2013) lmcs:747</journal-ref><doi>10.2168/LMCS-9(4:18)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study expansions of the Weak Monadic Second Order theory of (N,&lt;) by
cardinality relations, which are predicates R(X1,...,Xn) whose truth value
depends only on the cardinality of the sets X1, ...,Xn. We first provide a
(definable) criterion for definability of a cardinality relation in (N,&lt;), and
use it to prove that for every cardinality relation R which is not definable in
(N,&lt;), there exists a unary cardinality relation which is definable in (N,&lt;,R)
and not in (N,&lt;). These results resemble Muchnik and Michaux-Villemaire
theorems for Presburger Arithmetic. We prove then that + and x are definable in
(N,&lt;,R) for every cardinality relation R which is not definable in (N,&lt;). This
implies undecidability of the WMSO theory of (N,&lt;,R). We also consider the
related satisfiability problem for the class of finite orderings, namely the
question whether an MSO sentence in the language {&lt;,R} admits a finite model M
where &lt; is interpreted as a linear ordering, and R as the restriction of some
(fixed) cardinality relation to the domain of M. We prove that this problem is
undecidable for every cardinality relation R which is not definable in (N,&lt;).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8185</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8185</id><created>2013-10-30</created><authors><author><keyname>Jarynowski</keyname><forenames>Amdrzej</forenames></author><author><keyname>Buda</keyname><forenames>Andrzej</forenames></author></authors><title>Dynamics of popstar record sales on phonographic market -- stochastic
  model</title><categories>stat.AP cs.SY math.DS physics.soc-ph</categories><comments>Summer Solstice 2013 International Conference on Discrete Models of
  Complex Systems, Warsaw, Poland</comments><journal-ref>Acta Physica Polonica B (PS) No 2, Vol. 7 2014</journal-ref><doi>10.5506/APhysPolBSupp.7.317</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate weekly record sales of the world's most popular 30 artists
(2003-2013). Time series of sales have non-trivial kind of memory
(anticorrelations, strong seasonality and constant autocorrelation decay within
120 weeks). Amount of artists record sales are usually the highest in the first
week after premiere of their brand new records and then decrease to fluctuate
around zero till next album release. We model such a behavior by discrete
mean-reverting geometric jump diffusion (MRGJD) and Markov regime switching
mechanism (MRS) between the base and the promotion regimes. We can built up the
evidence through such a toy model that quantifies linear and nonlinear
dynamical components (with stationary and nonstationary parameters set), and
measure local divergence of the system with collective behavior phenomena. We
find special kind of disagreement between model and data for Christmas time due
to unusual shopping behavior. Analogies to earthquakes, product life-cycles,
and energy markets will also be discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8186</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8186</id><created>2013-10-30</created><authors><author><keyname>Bruhn</keyname><forenames>Henning</forenames></author><author><keyname>Schaudt</keyname><forenames>Oliver</forenames></author></authors><title>Claw-free t-perfect graphs can be recognised in polynomial time</title><categories>cs.DM cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is called t-perfect if its stable set polytope is defined by
non-negativity, edge and odd-cycle inequalities. We show that it can be decided
in polynomial time whether a given claw-free graph is t-perfect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8187</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8187</id><created>2013-08-31</created><authors><author><keyname>Bo</keyname><forenames>Cheng</forenames></author><author><keyname>Li</keyname><forenames>Xiang-Yang</forenames></author><author><keyname>Jung</keyname><forenames>Taeho</forenames></author><author><keyname>Mao</keyname><forenames>Xufei</forenames></author></authors><title>SmartLoc: Sensing Landmarks Silently for Smartphone Based Metropolitan
  Localization</title><categories>cs.NI cs.CY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present \emph{SmartLoc}, a localization system to estimate the location
and the traveling distance by leveraging the lower-power inertial sensors
embedded in smartphones as a supplementary to GPS. To minimize the negative
impact of sensor noises, \emph{SmartLoc} exploits the intermittent strong GPS
signals and uses the linear regression to build a prediction model which is
based on the trace estimated from inertial sensors and the one computed from
the GPS. Furthermore, we utilize landmarks (e.g., bridge, traffic lights)
detected automatically and special driving patterns (e.g., turning, uphill, and
downhill) from inertial sensory data to improve the localization accuracy when
the GPS signal is weak. Our evaluations of \emph{SmartLoc} in the city
demonstrates its technique viability and significant localization accuracy
improvement compared with GPS and other approaches: the error is approximately
20m for 90% of time while the known mean error of GPS is 42.22m.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8200</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8200</id><created>2013-10-28</created><updated>2013-12-25</updated><authors><author><keyname>Kuusisto</keyname><forenames>Antti</forenames><affiliation>University of Wroc&#x142;aw</affiliation></author><author><keyname>Meyers</keyname><forenames>Jeremy</forenames><affiliation>Stanford University</affiliation></author><author><keyname>Virtema</keyname><forenames>Jonni</forenames><affiliation>University of Tampere</affiliation></author></authors><title>Undecidable First-Order Theories of Affine Geometries</title><categories>cs.LO math.LO</categories><comments>23 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:1208.4930</comments><proxy>Logical Methods In Computer Science</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (December
  30, 2013) lmcs:728</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tarski initiated a logic-based approach to formal geometry that studies
first-order structures with a ternary betweenness relation \beta, and a
quaternary equidistance relation \equiv. Tarski established, inter alia, that
the first-order (FO) theory of (R^2,\beta,\equiv) is decidable. Aiello and van
Benthem (2002) conjectured that the FO-theory of expansions of (R^2,\beta) with
unary predicates is decidable. We refute this conjecture by showing that for
all n&gt;1, the FO-theory of the class of expansions of (R^2,\beta) with just one
unary predicate is not even arithmetical. We also define a natural and
comprehensive class C of geometric structures (T,\beta), and show that for each
structure (T,\beta) in C, the FO-theory of the class of expansions of (T,\beta)
with a single unary predicate is undecidable. We then consider classes of
expansions of structures (T,\beta) with a restricted unary predicate, for
example a finite predicate, and establish a variety of related undecidability
results. In addition to decidability questions, we briefly study the
expressivities of universal MSO and weak universal MSO over expansions of
(R^n,\beta). While the logics are incomparable in general, over expansions of
(R^n,\beta), formulae of weak universal MSO translate into equivalent formulae
of universal MSO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8204</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8204</id><created>2013-10-30</created><authors><author><keyname>Rakic</keyname><forenames>Gordana</forenames></author><author><keyname>Jerinic</keyname><forenames>Ljubomir</forenames></author><author><keyname>Budimac</keyname><forenames>Zoran</forenames></author><author><keyname>Ivanovic</keyname><forenames>Mirjana</forenames></author></authors><title>Sequencing and navigation through learning content</title><categories>cs.CY</categories><comments>conference paper</comments><journal-ref>Proc. of the 16th International Multiconference Information
  Society - IS 2013, Volume A, 2013, Slovenia, pp. 261-264</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to describe a basic idea to introduce formal
modeling in development and presentation of e-learning systems. By introducing
formalism, reasoning on e-learning systems and processes through them can be
more easily understood. In particular, we propose to use Harel automata
(statecharts) in modeling sequencing and navigation through learning content
and learning process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8211</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8211</id><created>2013-10-30</created><updated>2013-11-27</updated><authors><author><keyname>Erwan</keyname><forenames>Le Merrer</forenames></author><author><keyname>Yizhong</keyname><forenames>Liang</forenames></author><author><keyname>Gilles</keyname><forenames>Tr&#xe9;dan</forenames></author></authors><title>(Re)partitioning for stream-enabled computation</title><categories>cs.DC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partitioning an input graph over a set of workers is a complex operation.
Objectives are twofold: split the work evenly, so that every worker gets an
equal share, and minimize edge cut to achieve a good work locality (i.e.
workers can work independently). Partitioning a graph accessible from memory is
a notorious NP-complete problem. Motivated by the regain of interest for the
stream processing paradigm (where nodes and edges arrive as a flow to the
datacenter), we propose in this paper a stream-enabled graph partitioning
system that constantly seeks an optimum between those two objectives. We first
expose the hardness of partitioning using classic and static methods; we then
exhibit the cut versus load balancing tradeoff, from an application point of
view. With this tradeoff in mind, our approach translates the online
partitioning problem into a standard optimization problem. A greedy algorithm
handles the stream of incoming graph updates while optimizations are triggered
on demand to improve upon the greedy decisions. Using simulations, we show that
this approach is very efficient, turning a basic optimization strategy such as
hill climbing into an online partitioning solution that compares favorably to
literature's recent stream partitioning solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8220</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8220</id><created>2013-10-30</created><authors><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author></authors><title>Prediction of highly cited papers</title><categories>physics.soc-ph cs.DL cs.SI</categories><comments>6 pages, 3 figures, 2 tables</comments><journal-ref>Europhys. Lett. 105, 28002 (2014)</journal-ref><doi>10.1209/0295-5075/105/28002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an article written five years ago [arXiv:0809.0522], we described a method
for predicting which scientific papers will be highly cited in the future, even
if they are currently not highly cited. Applying the method to real citation
data we made predictions about papers we believed would end up being well
cited. Here we revisit those predictions, five years on, to see how well we
did. Among the over 2000 papers in our original data set, we examine the fifty
that, by the measures of our previous study, were predicted to do best and we
find that they have indeed received substantially more citations in the
intervening years than other papers, even after controlling for the number of
prior citations. On average these top fifty papers have received 23 times as
many citations in the last five years as the average paper in the data set as a
whole, and 15 times as many as the average paper in a randomly drawn control
group that started out with the same number of citations. Applying our
prediction technique to current data, we also make new predictions of papers
that we believe will be well cited in the next few years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8224</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8224</id><created>2013-10-30</created><updated>2014-03-27</updated><authors><author><keyname>Clough</keyname><forenames>James R.</forenames></author><author><keyname>Gollings</keyname><forenames>Jamie</forenames></author><author><keyname>Loach</keyname><forenames>Tamar V.</forenames></author><author><keyname>Evans</keyname><forenames>Tim S.</forenames></author></authors><title>Transitive Reduction of Citation Networks</title><categories>physics.soc-ph cs.DL cs.SI</categories><comments>17 pages, 13 figures, data available</comments><report-no>Imperial/TP/13/TSE/3</report-no><journal-ref>Journal of Complex Networks 3 (2015) 189-203</journal-ref><doi>10.1093/comnet/cnu039</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many complex networks the vertices are ordered in time, and edges
represent causal connections. We propose methods of analysing such directed
acyclic graphs taking into account the constraints of causality and
highlighting the causal structure. We illustrate our approach using citation
networks formed from academic papers, patents, and US Supreme Court verdicts.
We show how transitive reduction reveals fundamental differences in the
citation practices of different areas, how it highlights particularly
interesting work, and how it can correct for the effect that the age of a
document has on its citation count. Finally, we transitively reduce null models
of citation networks with similar degree distributions and show the difference
in degree distributions after transitive reduction to illustrate the lack of
causal structure in such models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8226</identifier>
 <datestamp>2014-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8226</id><created>2013-10-30</created><authors><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author><author><keyname>Larsen</keyname><forenames>Birger</forenames></author><author><keyname>Schaer</keyname><forenames>Philipp</forenames></author><author><keyname>Mutschke</keyname><forenames>Peter</forenames></author></authors><title>Bibliometric-enhanced Information Retrieval</title><categories>cs.IR cs.DL physics.soc-ph</categories><comments>6 pages, accepted workshop proposal for ECIR 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bibliometric techniques are not yet widely used to enhance retrieval
processes in digital libraries, although they offer value-added effects for
users. In this workshop we will explore how statistical modelling of
scholarship, such as Bradfordizing or network analysis of coauthorship network,
can improve retrieval services for specific communities, as well as for large,
cross-domain collections. This workshop aims to raise awareness of the missing
link between information retrieval (IR) and bibliometrics/scientometrics and to
create a common ground for the incorporation of bibliometric-enhanced services
into retrieval at the digital library interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8232</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8232</id><created>2013-10-30</created><authors><author><keyname>Sena</keyname><forenames>Alexandre</forenames></author><author><keyname>Nascimento</keyname><forenames>Aline</forenames></author><author><keyname>Boeres</keyname><forenames>Cristina</forenames></author><author><keyname>Rebello</keyname><forenames>Vinod E. F.</forenames></author><author><keyname>Bulc&#xe3;o</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Improving Memory Hierarchy Utilisation for Stencil Computations on
  Multicore Machines</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although modern supercomputers are composed of multicore machines, one can
find scientists that still execute their legacy applications which were
developed to monocore cluster where memory hierarchy is dedicated to a sole
core. The main objective of this paper is to propose and evaluate an algorithm
that identify an efficient blocksize to be applied on MPI stencil computations
on multicore machines. Under the light of an extensive experimental analysis,
this work shows the benefits of identifying blocksizes that will dividing data
on the various cores and suggest a methodology that explore the memory
hierarchy available in modern machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8243</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8243</id><created>2013-10-30</created><authors><author><keyname>Agarwal</keyname><forenames>Alekh</forenames></author><author><keyname>Bottou</keyname><forenames>Leon</forenames></author><author><keyname>Dudik</keyname><forenames>Miroslav</forenames></author><author><keyname>Langford</keyname><forenames>John</forenames></author></authors><title>Para-active learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training examples are not all equally informative. Active learning strategies
leverage this observation in order to massively reduce the number of examples
that need to be labeled. We leverage the same observation to build a generic
strategy for parallelizing learning algorithms. This strategy is effective
because the search for informative examples is highly parallelizable and
because we show that its performance does not deteriorate when the sifting
process relies on a slightly outdated model. Parallel active learning is
particularly attractive to train nonlinear models with non-linear
representations because there are few practical parallel learning algorithms
for such models. We report preliminary experiments using both kernel SVMs and
SGD-trained neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8245</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8245</id><created>2013-10-30</created><authors><author><keyname>Mamageishvili</keyname><forenames>Akaki</forenames></author><author><keyname>Mihalak</keyname><forenames>Matus</forenames></author><author><keyname>Muller</keyname><forenames>Dominik</forenames></author></authors><title>Tree Nash Equilibria in the Network Creation Game</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the network creation game with n vertices, every vertex (a player) buys a
set of adjacent edges, each at a fixed amount {\alpha} &gt; 0. It has been
conjectured that for {\alpha} &gt;= n, every Nash equilibrium is a tree, and has
been confirmed for every {\alpha} &gt;= 273n. We improve upon this bound and show
that this is true for every {\alpha} &gt;= 65n. To show this, we provide new and
improved results on the local structure of Nash equilibria. Technically, we
show that if there is a cycle in a Nash equilibrium, then {\alpha} &lt; 65n.
Proving this, we only consider relatively simple strategy changes of the
players involved in the cycle. We further show that this simple approach cannot
be used to show the desired upper bound {\alpha} &lt; n (for which a cycle may
exist), but conjecture that a slightly worse bound {\alpha} &lt; 1.3n can be
achieved with this approach. Towards this conjecture, we show that if a Nash
equilibrium has a cycle of length at most 10, then indeed {\alpha} &lt; 1.3n. We
further provide experimental evidence suggesting that when the girth of a Nash
equilibrium is increasing, the upper bound on {\alpha} obtained by the simple
strategy changes is not increasing. To the end, we investigate the approach for
a coalitional variant of Nash equilibrium, where coalitions of two players
cannot collectively improve, and show that if {\alpha} &gt;= 41n, then every such
Nash equilibrium is a tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8258</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8258</id><created>2013-10-30</created><authors><author><keyname>Domingues</keyname><forenames>Guilherme de Melo Baptista</forenames></author><author><keyname>Silva</keyname><forenames>Edmundo Albuquerque de Souza e</forenames></author><author><keyname>Le&#xe3;o</keyname><forenames>Rosa Maria Meri</forenames></author><author><keyname>Mensch&#xe9;</keyname><forenames>Daniel Sadoc</forenames></author></authors><title>Enabling Information Centric Networks through Opportunistic Search,
  Routing and Caching</title><categories>cs.NI</categories><comments>31st Brazilian Symposium on Computer Networks and Distributed
  Systems, SBRC' 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content dissemination networks are pervasive in todays Internet. Examples of
content dissemination networks include peer-to-peer networks (P2P), content
distribution networks (CDN) and information centric networks (ICN). In this
paper, we propose a new system design for information centric networks which
leverages opportunistic searching, routing and caching. Our system design is
based on an hierarchical tiered structure. Random walks are used to find
content inside each tier, and gateways across tiers are used to direct requests
towards servers placed in the top tier, which are accessed in case content
replicas are not found in lower tiers. Then, we propose a model to analyze the
system in consideration. The model yields metrics such as mean time to find a
content and the load experienced by custodians as a function of the network
topology. Using the model, we identify trade-offs between these two metrics,
and numerically show how to find the optimal time to live of the random walks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8267</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8267</id><created>2013-10-30</created><authors><author><keyname>Agrawal</keyname><forenames>Vidit</forenames></author><author><keyname>Kang</keyname><forenames>Shivpal Singh</forenames></author><author><keyname>Sinha</keyname><forenames>Sudeshna</forenames></author></authors><title>Realization of Morphing Logic Gates in a Repressilator with Quorum
  Sensing Feedback</title><categories>physics.bio-ph cs.ET nlin.AO q-bio.MN</categories><journal-ref>Physics Letters A 378/16-17 (2014) 1099-1103</journal-ref><doi>10.1016/j.physleta.2014.02.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate how a genetic ring oscillator network with quorum sensing
feedback can operate as a robust logic gate. Specifically we show how a range
of logic functions, namely AND/NAND, OR/NOR and XOR/XNOR, can be realized by
the system, thus yielding a versatile unit that can morph between different
logic operations. We further demonstrate the capacity of this system to yield
complementary logic operations in parallel. Our results then indicate the
computing potential of this biological system, and may lead to bio-inspired
computing devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8278</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8278</id><created>2013-10-30</created><authors><author><keyname>Gao</keyname><forenames>Sicun</forenames></author><author><keyname>Kong</keyname><forenames>Soonho</forenames></author><author><keyname>Clarke</keyname><forenames>Edmund</forenames></author></authors><title>Satisfiability Modulo ODEs</title><categories>cs.LO cs.SY</categories><comments>Published in FMCAD 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study SMT problems over the reals containing ordinary differential
equations. They are important for formal verification of realistic hybrid
systems and embedded software. We develop delta-complete algorithms for SMT
formulas that are purely existentially quantified, as well as exists-forall
formulas whose universal quantification is restricted to the time variables. We
demonstrate scalability of the algorithms, as implemented in our open-source
solver dReal, on SMT benchmarks with several hundred nonlinear ODEs and
variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8293</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8293</id><created>2013-10-30</created><authors><author><keyname>Li</keyname><forenames>Angsheng</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Pan</keyname><forenames>Yicheng</forenames></author></authors><title>Dimensions, Structures and Security of Networks</title><categories>cs.SI physics.soc-ph</categories><comments>arXiv admin note: text overlap with arXiv:1310.8040</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main issues in modern network science is the phenomenon of
cascading failures of a small number of attacks. Here we define the dimension
of a network to be the maximal number of functions or features of nodes of the
network. It was shown that there exist linear networks which are provably
secure, where a network is linear, if it has dimension one, that the high
dimensions of networks are the mechanisms of overlapping communities, that
overlapping communities are obstacles for network security, and that there
exists an algorithm to reduce high dimensional networks to low dimensional ones
which simultaneously preserves all the network properties and significantly
amplifies security of networks. Our results explore that dimension is a
fundamental measure of networks, that there exist linear networks which are
provably secure, that high dimensional networks are insecure, and that security
of networks can be amplified by reducing dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8294</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8294</id><created>2013-10-30</created><authors><author><keyname>Li</keyname><forenames>Angsheng</forenames></author><author><keyname>Li</keyname><forenames>Jiankou</forenames></author><author><keyname>Pan</keyname><forenames>Yicheng</forenames></author></authors><title>Community Structures Are Definable in Networks, and Universal in Real
  World</title><categories>cs.SI physics.soc-ph</categories><comments>arXiv admin note: substantial text overlap with arXiv:1310.8038</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detecting is one of the main approaches to understanding networks
\cite{For2010}.
  However it has been a longstanding challenge to give a definition for
community structures of networks. Here we found that community structures are
definable in networks, and are universal in real world. We proposed the notions
of entropy- and conductance-community structure ratios. It was shown that the
definitions of the modularity proposed in \cite{NG2004}, and our entropy- and
conductance-community structures are equivalent in defining community
structures of networks, that randomness in the ER model \cite{ER1960} and
preferential attachment in the PA \cite{Bar1999} model are not mechanisms of
community structures of networks, and that the existence of community
structures is a universal phenomenon in real networks. Our results demonstrate
that community structure is a universal phenomenon in the real world that is
definable, solving the challenge of definition of community structures in
networks. This progress provides a foundation for a structural theory of
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8295</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8295</id><created>2013-10-30</created><authors><author><keyname>Li</keyname><forenames>Angsheng</forenames></author><author><keyname>Li</keyname><forenames>Jiankou</forenames></author><author><keyname>Pan</keyname><forenames>Yicheng</forenames></author></authors><title>Homophyly Networks -- A Structural Theory of Networks</title><categories>cs.SI physics.soc-ph</categories><comments>arXiv admin note: substantial text overlap with arXiv:1310.8038</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A grand challenge in network science is apparently the missing of a
structural theory of networks. The authors have showed that the existence of
community structures is a universal phenomenon in real networks, and that
neither randomness nor preferential attachment is a mechanism of community
structures of network \footnote{A. Li, J. Li, and Y. Pan, Community structures
are definable in networks, and universal in the real world, To appear.}. This
poses a fundamental question: What are the mechanisms of community structures
of real networks? Here we found that homophyly is the mechanism of community
structures and a structural theory of networks. We proposed a homophyly model.
It was shown that networks of our model satisfy a series of new topological,
probabilistic and combinatorial principles, including a fundamental principle,
a community structure principle, a degree priority principle, a widths
principle, an inclusion and infection principle, a king node principle, and a
predicting principle etc, leading to a structural theory of networks. Our model
demonstrates that homophyly is the underlying mechanism of community structures
of networks, that nodes of the same community share common features, that power
law and small world property are never obstacles of the existence of community
structures in networks, and that community structures are definable in
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8313</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8313</id><created>2013-10-30</created><updated>2014-01-10</updated><authors><author><keyname>Bonomo</keyname><forenames>Flavia</forenames></author><author><keyname>Schaudt</keyname><forenames>Oliver</forenames></author><author><keyname>Stein</keyname><forenames>Maya</forenames></author><author><keyname>Valencia-Pabon</keyname><forenames>Mario</forenames></author></authors><title>b-coloring is NP-hard on co-bipartite graphs and polytime solvable on
  tree-cographs</title><categories>cs.CC cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A b-coloring of a graph is a proper coloring such that every color class
contains a vertex that is adjacent to all other color classes. The b-chromatic
number of a graph G, denoted by \chi_b(G), is the maximum number t such that G
admits a b-coloring with t colors. A graph G is called b-continuous if it
admits a b-coloring with t colors, for every t = \chi(G),\ldots,\chi_b(G), and
b-monotonic if \chi_b(H_1) \geq \chi_b(H_2) for every induced subgraph H_1 of
G, and every induced subgraph H_2 of H_1.
  We investigate the b-chromatic number of graphs with stability number two.
These are exactly the complements of triangle-free graphs, thus including all
complements of bipartite graphs. The main results of this work are the
following:
  - We characterize the b-colorings of a graph with stability number two in
terms of matchings with no augmenting paths of length one or three. We derive
that graphs with stability number two are b-continuous and b-monotonic.
  - We prove that it is NP-complete to decide whether the b-chromatic number of
co-bipartite graph is at most a given threshold.
  - We describe a polynomial time dynamic programming algorithm to compute the
b-chromatic number of co-trees.
  - Extending several previous results, we show that there is a polynomial time
dynamic programming algorithm for computing the b-chromatic number of
tree-cographs. Moreover, we show that tree-cographs are b-continuous and
b-monotonic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8317</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8317</id><created>2013-10-30</created><authors><author><keyname>Hofmann</keyname><forenames>Martin</forenames></author><author><keyname>Ramyaa</keyname><forenames>Ramyaa</forenames></author></authors><title>Power of Nondetreministic JAGs on Cayley graphs</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Immerman-Szelepcsenyi Theorem uses an algorithm for co-st- connectivity
based on inductive counting to prove that NLOGSPACE is closed un- der
complementation. We want to investigate whether counting is necessary for this
theorem to hold. Concretely, we show that Nondeterministic Jumping Graph
Autmata (ND-JAGs) (pebble automata on graphs), on several families of Cayley
graphs, are equal in power to nondeterministic logspace Turing machines that
are given such graphs as a linear encoding. In particular, it follows that
ND-JAGs can solve co-st-connectivity on those graphs. This came as a surprise
since Cook and Rackoff showed that deterministic JAGs cannot solve
st-connectivity on many Cayley graphs due to their high self-similarity (every
neighbourhood looks the same). Thus, our results show that on these graphs,
nondeterminism provably adds computational power. The families of Cayley graphs
we consider include Cayley graphs of abelian groups and of all finite simple
groups irrespective of how they are presented and graphs corresponding to
groups generated by various product constructions, in- cluding iterated ones.
We remark that assessing the precise power of nondeterministic JAGs and in par-
ticular whether they can solve co-st-connectivity on arbitrary graphs is left
as an open problem by Edmonds, Poon and Achlioptas. Our results suggest a
positive answer to this question and in particular considerably limit the
search space for a potential counterexample.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8320</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8320</id><created>2013-10-30</created><authors><author><keyname>Zhao</keyname><forenames>Zheng</forenames></author><author><keyname>Liu</keyname><forenames>Jun</forenames></author></authors><title>Safe and Efficient Screening For Sparse Support Vector Machine</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Screening is an effective technique for speeding up the training process of a
sparse learning model by removing the features that are guaranteed to be
inactive the process. In this paper, we present a efficient screening technique
for sparse support vector machine based on variational inequality. The
technique is both efficient and safe.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8342</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8342</id><created>2013-10-30</created><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Vandendorpe</keyname><forenames>Luc</forenames></author></authors><title>On the Optimum Energy Efficiency for Flat-fading Channels with
  Rate-dependent Circuit Power</title><categories>cs.OH</categories><comments>12 pages, 7 figures, to appear in IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the optimum energy efficiency (EE) and the
corresponding spectral efficiency (SE) for a communication link operating over
a flat-fading channel. The EE is evaluated by the total energy consumption for
transmitting per message bit. Three channel cases are considered, namely static
channel with channel state information available at transmitter (CSIT),
fast-varying (FV) channel with channel distribution information available at
transmitter (CDIT), and FV channel with CSIT. A general circuit power model is
considered. For all the three channel cases, the tradeoff between the EE and SE
is studied. It is shown that the EE improves strictly as the SE increases from
0 to the optimum SE, and then strictly degrades as the SE increases beyond the
optimum SE. The impact of {\kappa}, {\rho} and other system parameters on the
optimum EE and corresponding SE is investigated to obtain insight.Some of the
important and interesting results for all the channel cases include: (1) when
{\kappa} increases the SE corresponding to the optimum EE should keep unchanged
if {\phi}(R) = R, but reduced if {\phi}(R) is strictly convex of R; (2) when
the rate-independent circuit power {\rho} increases, the SE corresponding to
the optimum EE has to be increased. A polynomial-complexity algorithm is
developed with the bisection method to find the optimum SE. The insight is
corroborated and the optimum EE for the three cases are compared by simulation
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8347</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8347</id><created>2013-10-30</created><updated>2013-11-06</updated><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>Quantum Imaging of High-Dimensional Hilbert Spaces with Radon Transform</title><categories>quant-ph cs.IT math.IT</categories><comments>24 pages, 4 figures, 1 table, v2: minor notation change in Sec. 4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-dimensional Hilbert spaces possess large information encoding and
transmission capabilities. Characterizing exactly the real potential of
high-dimensional entangled systems is a cornerstone of tomography and quantum
imaging. The accuracy of the measurement apparatus and devices used in quantum
imaging is physically limited, which allows no further improvements to be made.
To extend the possibilities, we introduce a post-processing method for quantum
imaging that is based on the Radon transform and the projection-slice theorem.
The proposed solution leads to an enhanced precision and a deeper
parameterization of the information conveying capabilities of high-dimensional
Hilbert spaces. We demonstrate the method for the analysis of high-dimensional
position-momentum photonic entanglement. We show that the entropic separability
bound in terms of standard deviations is violated considerably more strongly in
comparison to the standard setting and current data processing. The results
indicate that the possibilities of the quantum imaging of high-dimensional
Hilbert spaces can be extended by applying appropriate calculations in the
post-processing phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8364</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8364</id><created>2013-10-30</created><updated>2014-09-30</updated><authors><author><keyname>Sun</keyname><forenames>Jiajun</forenames></author></authors><title>Incentive Mechanisms for Mobile Crowd Sensing: Current States and
  Challenges of Work</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the author due to some bad writings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile crowd sensing (MCS) is a new paradigm which leverages the ubiquity of
sensor-equipped mobile devices such as smartphones, music players, and
in-vehicle sensors at the edge of the Internet, to collect data. The new
paradigm will fuel the evolution of the Internet of Things to three changes as
follows: First, the terminal devices at the edge of the Internet change from
PCs to mobile phones. Second, the interactive mode extends from the virtual
space to the real physical world. Thirdly, the forwarding manner of sensing
data are undergoing the transition from the priori to the opportunistic. To
better meet the demands of MCS applications at a societal scale, incentive
mechanisms are indispensable. In this paper, we will first overview three
categories of MCS applications, and then propose a new architecture for MCS
applications. Based on the architecture, we discuss various research challenges
about incentive mechanism designs for MCS applications, followed by potential
future work discussions. Finally, we present potential future works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8369</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8369</id><created>2013-10-30</created><authors><author><keyname>Tuxanidy</keyname><forenames>Aleksandr</forenames></author><author><keyname>Wang</keyname><forenames>Qiang</forenames></author></authors><title>On the inverses of some classes of permutations of finite fields</title><categories>math.NT cs.IT math.IT</categories><msc-class>11T06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the compositional inverses of some general classes of permutation
polynomials over finite fields. We show that we can write these inverses in
terms of the inverses of two other polynomials bijecting subspaces of the
finite field, where one of these is a linearized polynomial. In some cases we
are able to explicitly obtain these inverses, thus obtaining the compositional
inverse of the permutation in question. In addition we show how to compute a
linearized polynomial inducing the inverse map over subspaces on which a
prescribed linearized polynomial induces a bijection. We also obtain the
explicit compositional inverses of two classes of permutation polynomials
generalizing those whose compositional inverses were recently obtained in [22]
and [24], respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8374</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8374</id><created>2013-10-30</created><updated>2014-05-19</updated><authors><author><keyname>Chen</keyname><forenames>Yin</forenames></author><author><keyname>Shen</keyname><forenames>Yulong</forenames></author><author><keyname>Zhu</keyname><forenames>Jinxiao</forenames></author><author><keyname>Jiang</keyname><forenames>Xiaohong</forenames></author></authors><title>Capacity and Delay-Throughput Tradeoff in ICMNs with Poisson Meeting
  Process</title><categories>cs.NI</categories><comments>17 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intermittently connected mobile networks (ICMNs) serve as an important
network model for many critical applications. This paper focuses on a
continuous ICMN model where the pair-wise meeting process between network nodes
follows a homogeneous and independent Poisson process. This ICMN model is known
to serve as good approximations to a class of important ICMNs with mobility
models like random waypoint and random direction, so it is widely adopted in
the performance study of ICMNs. This paper studies the throughput capacity and
delay-throughput tradeoff in the considered ICMNs with Poisson meeting process.
For the concerned ICMN, we first derive an exact expression of its throughput
capacity based on the pairwise meeting rate therein and analyze the expected
end-to-end packet delay under a routing algorithm that can achieve the
throughput capacity. We then explore the inherent tradeoff between delay and
throughput and establish a necessary condition for such tradeoff that holds
under any routing algorithm in the ICMN. To illustrate the applicability of the
theoretical results, case studies are further conducted for the random waypoint
and random direction mobility models. Finally, simulation and numerical results
are provided to verify the efficiency of our theoretical capacity/delay results
and to illustrate our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8378</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8378</id><created>2013-10-31</created><authors><author><keyname>Fox</keyname><forenames>Jacob</forenames></author></authors><title>Stanley-Wilf limits are typically exponential</title><categories>math.CO cs.DM</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a permutation $\pi$, let $S_{n}(\pi)$ be the number of permutations on
$n$ letters avoiding $\pi$. Marcus and Tardos proved the celebrated
Stanley-Wilf conjecture that $L(\pi)= \lim_{n \to \infty} S_n(\pi)^{1/n}$
exists and is finite. Backed by numerical evidence, it has been conjectured by
many researchers over the years that $L(\pi)=\Theta(k^2)$ for every permutation
$\pi$ on $k$ letters. We disprove this conjecture, showing that
$L(\pi)=2^{k^{\Theta(1)}}$ for almost all permutations $\pi$ on $k$ letters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8381</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8381</id><created>2013-10-31</created><authors><author><keyname>Cohen</keyname><forenames>Edith</forenames></author><author><keyname>Fiat</keyname><forenames>Amos</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Roditty</keyname><forenames>Liam</forenames></author></authors><title>A Labeling Approach to Incremental Cycle Detection</title><categories>cs.DS cs.DC</categories><comments>15 pages, one figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the \emph{incremental cycle detection} problem arcs are added to a
directed acyclic graph and the algorithm has to report if the new arc closes a
cycle. One seeks to minimize the total time to process the entire sequence of
arc insertions, or until a cycle appears.
  In a recent breakthrough, Bender, Fineman, Gilbert and Tarjan
\cite{BeFiGiTa11} presented two different algorithms, with time complexity
$O(n^2 \log n)$ and $O(m \cdot \min \{m^{1/2}, n^{2/3} \})$, respectively.
  In this paper we introduce a new technique for incremental cycle detection
that allows us to obtain both bounds (up to a logarithmic factor). Furthermore,
our approach seems more amiable for distributed implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8383</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8383</id><created>2013-10-31</created><authors><author><keyname>Gnimpieba</keyname><forenames>Etienne Z.</forenames></author><author><keyname>Jennewein</keyname><forenames>Douglas</forenames></author><author><keyname>Fuhrman</keyname><forenames>Luke</forenames></author><author><keyname>Lushbough</keyname><forenames>Carol M.</forenames></author></authors><title>Bioinformatics Knowledge Transmission (training, learning, and
  teaching): overview and flexible comparison of computer based training
  approaches</title><categories>cs.CY q-bio.OT</categories><comments>Proc. IKE'13, World Academy of Science</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The merger of computer science, mathematics, and life sciences has brought
about the discipline known as bioinformatics. However, the transmission (e.g.
training, learning, and teaching) of this knowledge becomes an important issue.
Many tools have been developed to help the bioinformatics community with that
transmission challenge. When selecting the best of these tools, called here
BKTMS (Bioinformatics Knowledge Transmission Management Systems), there may be
confusion. What makes a good BKTMS? How can we make this choice efficiently?
These questions remain unanswered for many users (e.g. learner, teacher and
student, trainer and trainee, administrator). This paper provides a critical
review of 32 existing BKTMS and a flexible comparison. This review and
evaluation will be used to gain insight into the tools, systems, and
capabilities that will be added to or excluded from a new proposed model for
the next generation of BKTMS, involving multidisciplinary, web semantic tools
(e.g. web services, workflow) and standards like LOM, or SCORM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8387</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8387</id><created>2013-10-31</created><updated>2014-03-20</updated><authors><author><keyname>Lee</keyname><forenames>Sang Hoon</forenames></author><author><keyname>Cucuringu</keyname><forenames>Mihai</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author></authors><title>Density-based and transport-based core-periphery structures in networks</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI q-bio.OT</categories><comments>14 pages, 7 figures, 5 tables</comments><journal-ref>Phys. Rev. E 89, 032810 (2014)</journal-ref><doi>10.1103/PhysRevE.89.032810</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks often possess mesoscale structures, and studying them can yield
insights into both structure and function. It is most common to study community
structure, but numerous other types of mesoscale structures also exist. In this
paper, we examine core-periphery structures based on both density and
transport. In such structures, core network components are well-connected both
among themselves and to peripheral components, which are not well-connected to
anything. We examine core-periphery structures in a wide range of examples of
transportation, social, and financial networks---including road networks in
large urban areas, a rabbit warren, a dolphin social network, a European
interbank network, and a migration network between counties in the United
States. We illustrate that a recently developed transport-based notion of node
coreness is very useful for characterizing transportation networks. We also
generalize this notion to examine core versus peripheral edges, and we show
that the resulting diagnostic is also useful for transportation networks. To
examine the properties of transportation networks further, we develop a family
of generative models of roadlike networks. We illustrate the effect of the
dimensionality of the embedding space on transportation networks, and we
demonstrate that the correlations between different measures of coreness can be
very different for different types of networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8388</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8388</id><created>2013-10-31</created><authors><author><keyname>Li</keyname><forenames>Angsheng</forenames></author><author><keyname>Pan</keyname><forenames>Yicheng</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author></authors><title>Provable Security of Networks</title><categories>cs.SI physics.soc-ph</categories><comments>arXiv admin note: text overlap with arXiv:1310.8038, arXiv:1310.8040</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a definition of {\it security} and a definition of {\it
robustness} of networks against the cascading failure models of deliberate
attacks and random errors respectively, and investigate the principles of the
security and robustness of networks. We propose a {\it security model} such
that networks constructed by the model are provably secure against any attacks
of small sizes under the cascading failure models, and simultaneously follow a
power law, and have the small world property with a navigating algorithm of
time complex $O(\log n)$. It is shown that for any network $G$ constructed from
the security model, $G$ satisfies some remarkable topological properties,
including: (i) the {\it small community phenomenon}, that is, $G$ is rich in
communities of the form $X$ of size poly logarithmic in $\log n$ with
conductance bounded by $O(\frac{1}{|X|^{\beta}})$ for some constant $\beta$,
(ii) small diameter property, with diameter $O(\log n)$ allowing a navigation
by a $O(\log n)$ time algorithm to find a path for arbitrarily given two nodes,
and (iii) power law distribution, and satisfies some probabilistic and
combinatorial principles, including the {\it degree priority theorem}, and {\it
infection-inclusion theorem}. By using these principles, we show that a network
$G$ constructed from the security model is secure for any attacks of small
scales under both the uniform threshold and random threshold cascading failure
models. Our security theorems show that networks constructed from the security
model are provably secure against any attacks of small sizes, for which natural
selections of {\it homophyly, randomness} and {\it preferential attachment} are
the underlying mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8390</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8390</id><created>2013-10-31</created><authors><author><keyname>Ma</keyname><forenames>Li</forenames></author></authors><title>Harnack's inequality and Green functions on locally finite graphs</title><categories>math.DG cs.IT math.AP math.CA math.IT math.MG</categories><comments>10 pages</comments><msc-class>53</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the gradient estimate for positive solutions of
Schrodinger equations on locally finite graph. Then we derive Harnack's
inequality for positive solutions of the Schrodinger equations. We also set up
some results about Green functions of the Laplacian equation on locally finite
graph. Interesting properties of Schrodinger equation are derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8392</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8392</id><created>2013-10-31</created><authors><author><keyname>Thomas</keyname><forenames>Geethu</forenames></author><author><keyname>Jose</keyname><forenames>Prem</forenames><suffix>V</suffix></author><author><keyname>Afsar</keyname><forenames>P.</forenames></author></authors><title>Cloud computing security using encryption technique</title><categories>cs.DC cs.CR</categories><comments>7 Pages, 3 Figures. arXiv admin note: text overlap with
  arXiv:1303.4814 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Computing has been envisioned as the next generation architecture of IT
Enterprise. The Cloud computing concept offers dynamically scalable resources
provisioned as a service over the Internet. Economic benefits are the main
driver for the Cloud, since it promises the reduction of capital expenditure
and operational expenditure. In order for this to become reality, however,
there are still some challenges to be solved. Most important among these are
security and trust issues,since the users data has to be released to the Cloud
and thus leaves the protection sphere of the data owner.In contrast to
traditional solutions, where the IT services are under proper physical,logical
and personnel controls, Cloud Computing moves the application software and
databases to the large data centers, where the management of the data and
services may not be fully trustworthy. This unique attribute, however, poses
many new security challenges which have not been well understood. Security is
to save data from danger and vulnerability. There are so many dangers and
vulnerabilities to be handled. Various security issues and some of their
solution are explained and are concentrating mainly on public cloud security
issues and their solutions. Data should always be encrypted when stored(using
separate symmetric encryption keys)and transmitted. If this is implemented
appropriately, even if another tenant can access the data, all that will appear
is gibberish. So a method is proposed such that we are encrypting the whole
data along with the cryptographic key.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8396</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8396</id><created>2013-10-31</created><updated>2014-10-31</updated><authors><author><keyname>Pasta</keyname><forenames>Mohammad Qasim</forenames></author><author><keyname>Jan</keyname><forenames>Zohaib</forenames></author><author><keyname>Sallaberry</keyname><forenames>Arnaud</forenames></author><author><keyname>Zaidi</keyname><forenames>Faraz</forenames></author></authors><title>Tunable and Growing Network Generation Model with Community Structures</title><categories>cs.SI physics.soc-ph</categories><comments>Social Computing and Its Applications, SCA 13, Karlsruhe : Germany
  (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen a growing interest in the modeling and simulation of
social networks to understand several social phenomena. Two important classes
of networks, small world and scale free networks have gained a lot of research
interest. Another important characteristic of social networks is the presence
of community structures. Many social processes such as information diffusion
and disease epidemics depend on the presence of community structures making it
an important property for network generation models to be incorporated. In this
paper, we present a tunable and growing network generation model with small
world and scale free properties as well as the presence of community
structures. The major contribution of this model is that the communities thus
created satisfy three important structural properties: connectivity within each
community follows power-law, communities have high clustering coefficient and
hierarchical community structures are present in the networks generated using
the proposed model. Furthermore, the model is highly robust and capable of
producing networks with a number of different topological characteristics
varying clustering coefficient and inter-cluster edges. Our simulation results
show that the model produces small world and scale free networks along with the
presence of communities depicting real world societies and social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8397</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8397</id><created>2013-10-31</created><authors><author><keyname>Auger</keyname><forenames>Anne</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Hansen</keyname><forenames>Nikolaus</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author></authors><title>Linear Convergence on Positively Homogeneous Functions of a Comparison
  Based Step-Size Adaptive Randomized Search: the (1+1) ES with Generalized
  One-fifth Success Rule</title><categories>cs.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of unconstraint numerical optimization, this paper
investigates the global linear convergence of a simple probabilistic
derivative-free optimization algorithm (DFO). The algorithm samples a candidate
solution from a standard multivariate normal distribution scaled by a step-size
and centered in the current solution. This solution is accepted if it has a
better objective function value than the current one. Crucial to the algorithm
is the adaptation of the step-size that is done in order to maintain a certain
probability of success. The algorithm, already proposed in the 60's, is a
generalization of the well-known Rechenberg's $(1+1)$ Evolution Strategy (ES)
with one-fifth success rule which was also proposed by Devroye under the name
compound random search or by Schumer and Steiglitz under the name step-size
adaptive random search. In addition to be derivative-free, the algorithm is
function-value-free: it exploits the objective function only through
comparisons. It belongs to the class of comparison-based step-size adaptive
randomized search (CB-SARS). For the convergence analysis, we follow the
methodology developed in a companion paper for investigating linear convergence
of CB-SARS: by exploiting invariance properties of the algorithm, we turn the
study of global linear convergence on scaling-invariant functions into the
study of the stability of an underlying normalized Markov chain (MC). We hence
prove global linear convergence by studying the stability (irreducibility,
recurrence, positivity, geometric ergodicity) of the normalized MC associated
to the $(1+1)$-ES. More precisely, we prove that starting from any initial
solution and any step-size, linear convergence with probability one and in
expectation occurs. Our proof holds on unimodal functions that are the
composite of strictly increasing functions by positively homogeneous functions
with degree $\alpha$ (assumed also to be continuously differentiable). This
function class includes composite of norm functions but also non-quasi convex
functions. Because of the composition by a strictly increasing function, it
includes non continuous functions. We find that a sufficient condition for
global linear convergence is the step-size increase on linear functions, a
condition typically satisfied for standard parameter choices. While introduced
more than 40 years ago, we provide here the first proof of global linear
convergence for the $(1+1)$-ES with generalized one-fifth success rule and the
first proof of linear convergence for a CB-SARS on such a class of functions
that includes non-quasi convex and non-continuous functions. Our proof also
holds on functions where linear convergence of some CB-SARS was previously
proven, namely convex-quadratic functions (including the well-know sphere
function).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8403</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8403</id><created>2013-10-31</created><updated>2014-04-28</updated><authors><author><keyname>Banerjee</keyname><forenames>Sandip</forenames></author><author><keyname>Banik</keyname><forenames>Aritra</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Bhargab B.</forenames></author><author><keyname>Bishnu</keyname><forenames>Arijit</forenames></author><author><keyname>Chatterjee</keyname><forenames>Soumyottam</forenames></author></authors><title>An Existential Proof of the Conjecture on Packing Anchored Rectangles</title><categories>cs.DM math.CO</categories><comments>This paper has been withdrawn as a bug has been discovered in the
  proof of Claim 5</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P_{n}$ be a set of $n$ points, including the origin, in the unit square
$U = [0,1]^2$. We consider the problem of constructing $n$ axis-parallel and
mutually disjoint rectangles inside $U$ such that the bottom-left corner of
each rectangle coincides with a point in $P_{n}$ and the total area covered by
the rectangles is maximized \cite{ibmpuzzle}, \cite{Winkler2007},
\cite{Winkler2010a}, \cite{Winkler2010b}. The longstanding conjecture has been
that at least half of $U$ can be covered when such rectangles are properly
placed. In this paper, we give an existential proof of the conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8408</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8408</id><created>2013-10-31</created><updated>2013-11-10</updated><authors><author><keyname>Valmari</keyname><forenames>Antti</forenames><affiliation>Tampere University of Technology</affiliation></author></authors><title>All Linear-Time Congruences for Familiar Operators</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (November
  12, 2013) lmcs:858</journal-ref><doi>10.2168/LMCS-9(4:11)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The detailed behaviour of a system is often represented as a labelled
transition system (LTS) and the abstract behaviour as a stuttering-insensitive
semantic congruence. Numerous congruences have been presented in the
literature. On the other hand, there have not been many results proving the
absence of more congruences. This publication fully analyses the linear-time
(in a well-defined sense) region with respect to action prefix, hiding,
relational renaming, and parallel composition. It contains 40 congruences. They
are built from the alphabet, two kinds of traces, two kinds of divergence
traces, five kinds of failures, and four kinds of infinite traces. In the case
of finite LTSs, infinite traces lose their role and the number of congruences
drops to 20. The publication concentrates on the hardest and most novel part of
the result, that is, proving the absence of more congruences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8418</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8418</id><created>2013-10-31</created><updated>2015-03-16</updated><authors><author><keyname>Mahajan</keyname><forenames>Dhruv</forenames></author><author><keyname>Agrawal</keyname><forenames>Nikunj</forenames></author><author><keyname>Keerthi</keyname><forenames>S. Sathiya</forenames></author><author><keyname>Sundararajan</keyname><forenames>S.</forenames></author><author><keyname>Bottou</keyname><forenames>Leon</forenames></author></authors><title>An efficient distributed learning algorithm based on effective local
  functional approximations</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scalable machine learning over big data is an important problem that is
receiving a lot of attention in recent years. On popular distributed
environments such as Hadoop running on a cluster of commodity machines,
communication costs are substantial and algorithms need to be designed suitably
considering those costs. In this paper we give a novel approach to the
distributed training of linear classifiers (involving smooth losses and L2
regularization) that is designed to reduce the total communication costs. At
each iteration, the nodes minimize locally formed approximate objective
functions; then the resulting minimizers are combined to form a descent
direction to move. Our approach gives a lot of freedom in the formation of the
approximate objective function as well as in the choice of methods to solve
them. The method is shown to have $O(log(1/\epsilon))$ time convergence. The
method can be viewed as an iterative parameter mixing method. A special
instantiation yields a parallel stochastic gradient descent method with strong
convergence. When communication times between nodes are large, our method is
much faster than the Terascale method (Agarwal et al., 2011), which is a state
of the art distributed solver based on the statistical query model (Chuet al.,
2006) that computes function and gradient values in a distributed fashion. We
also evaluate against other recent distributed methods and demonstrate superior
performance of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8426</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8426</id><created>2013-10-31</created><updated>2013-11-19</updated><authors><author><keyname>El&#xe7;i</keyname><forenames>Eren Metin</forenames></author><author><keyname>Weigel</keyname><forenames>Martin</forenames></author></authors><title>Dynamic connectivity algorithms for Monte Carlo simulations of the
  random-cluster model</title><categories>physics.comp-ph cond-mat.stat-mech cs.DS</categories><comments>Contribution to the &quot;XXV IUPAP Conference on Computational Physics&quot;
  proceedings; Corrected equation 3 and error in the maximal number of edge
  levels</comments><journal-ref>J. Phys. Conf. Ser. 510, 012013 (2014)</journal-ref><doi>10.1088/1742-6596/510/1/012013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review Sweeny's algorithm for Monte Carlo simulations of the random
cluster model. Straightforward implementations suffer from the problem of
computational critical slowing down, where the computational effort per edge
operation scales with a power of the system size. By using a tailored dynamic
connectivity algorithm we are able to perform all operations with a
poly-logarithmic computational effort. This approach is shown to be efficient
in keeping online connectivity information and is of use for a number of
applications also beyond cluster-update simulations, for instance in monitoring
droplet shape transitions. As the handling of the relevant data structures is
non-trivial, we provide a Python module with a full implementation for future
reference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8428</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8428</id><created>2013-10-31</created><updated>2013-11-16</updated><authors><author><keyname>Su</keyname><forenames>Hongyu</forenames></author><author><keyname>Rousu</keyname><forenames>Juho</forenames></author></authors><title>Multilabel Classification through Random Graph Ensembles</title><categories>cs.LG</categories><comments>15 Pages, 1 Figures</comments><journal-ref>JMLR: Workshop and Conference Proceedings 29:404--418, 2013</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present new methods for multilabel classification, relying on ensemble
learning on a collection of random output graphs imposed on the multilabel and
a kernel-based structured output learner as the base classifier. For ensemble
learning, differences among the output graphs provide the required base
classifier diversity and lead to improved performance in the increasing size of
the ensemble. We study different methods of forming the ensemble prediction,
including majority voting and two methods that perform inferences over the
graph structures before or after combining the base models into the ensemble.
We compare the methods against the state-of-the-art machine learning approaches
on a set of heterogeneous multilabel benchmark problems, including multilabel
AdaBoost, convex multitask feature learning, as well as single target learning
approaches represented by Bagging and SVM. In our experiments, the random graph
ensembles are very competitive and robust, ranking first or second on most of
the datasets. Overall, our results show that random graph ensembles are viable
alternatives to flat multilabel and multitask learners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8441</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8441</id><created>2013-10-31</created><authors><author><keyname>Steffen</keyname><forenames>Eckhard</forenames></author></authors><title>Edge-colorings and circular flow numbers on regular graphs</title><categories>math.CO cs.DM</categories><comments>9 pages; 1 figure</comments><msc-class>05C21, 05C15</msc-class><journal-ref>Journal Graph Theory 79 (2015) 1-7</journal-ref><doi>10.1002/jgt.21809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper characterizes $(2t+1)$-regular graphs with circular flow number $2
+ \frac{2}{2t-1}$. For $t=1$ this is Tutte's characterization of cubic graphs
with flow number 4. The class of cubic graphs is the only class of odd regular
graphs where a flow number separates the class 1 graphs from the class 2
graphs. We finally state some conjectures and relate them to existing
flow-conjectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8455</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8455</id><created>2013-10-31</created><authors><author><keyname>Korporal</keyname><forenames>Anja</forenames></author><author><keyname>Regensburger</keyname><forenames>Georg</forenames></author></authors><title>Composing and Factoring Generalized Green's Operators and Ordinary
  Boundary Problems</title><categories>cs.SC cs.MS cs.NA math.CA</categories><comments>19 pages</comments><msc-class>68W30 (Primary), 34B05, 15A09 (Secondary)</msc-class><journal-ref>AADIOS 2012, LNCS 8372, pp. 116-134, 2014</journal-ref><doi>10.1007/978-3-642-54479-8_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider solution operators of linear ordinary boundary problems with &quot;too
many&quot; boundary conditions, which are not always solvable. These generalized
Green's operators are a certain kind of generalized inverses of differential
operators. We answer the question when the product of two generalized Green's
operators is again a generalized Green's operator for the product of the
corresponding differential operators and which boundary problem it solves.
Moreover, we show that---provided a factorization of the underlying
differential operator---a generalized boundary problem can be factored into
lower order problems corresponding to a factorization of the respective Green's
operators. We illustrate our results by examples using the Maple package
IntDiffOp, where the presented algorithms are implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8456</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8456</id><created>2013-10-31</created><authors><author><keyname>Aupy</keyname><forenames>Guillaume</forenames></author><author><keyname>Benoit</keyname><forenames>Anne</forenames></author><author><keyname>H&#xe9;rault</keyname><forenames>Thomas</forenames></author><author><keyname>Robert</keyname><forenames>Yves</forenames></author><author><keyname>Dongarra</keyname><forenames>Jack</forenames></author></authors><title>Optimal Checkpointing Period: Time vs. Energy</title><categories>cs.DC</categories><comments>To be published in PMBS'13 (satellite workshop of SC'13). This work
  was supported in part by ANR RESCUE</comments><report-no>Inria RR-8387</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short paper deals with parallel scientific applications using
non-blocking and periodic coordinated checkpointing to enforce resilience. We
provide a model and detailed formulas for total execution time and consumed
energy. We characterize the optimal period for both objectives, and we assess
the range of time/energy trade-offs to be made by instantiating the model with
a set of realistic scenarios for Exascale systems. We give a particular
emphasis to I/O transfers, because the relative cost of communication is
expected to dramatically increase, both in terms of latency and consumed
energy, for future Exascale platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8462</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8462</id><created>2013-10-31</created><authors><author><keyname>B</keyname><forenames>Radhakrishnan</forenames></author><author><keyname>G</keyname><forenames>Shineraj</forenames></author><author><keyname>M</keyname><forenames>Anver Muhammed K.</forenames></author></authors><title>Application of Data Mining In Marketing</title><categories>cs.DB cs.CY</categories><comments>06 Pages, 02 Figures, 01 Table, Volume 2, Issue 5</comments><report-no>IJCSN-2013-2-5-47</report-no><journal-ref>IJCSN - International Journal of Computer Science and Network -
  October 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important problems in modern finance is finding efficient
ways to summarize and visualize the stock market data to give individuals or
institutions useful information about the market behavior for investment
decisions. The enormous amount of valuable data generated by the stock market
has attracted researchers to explore this problem domain using different
methodologies. Potential significant benefits of solving these problems
motivated extensive research for years. The research in data mining has gained
a high attraction due to the importance of its applications and the increasing
generation information. This paper provides an overview of application of data
mining techniques such as decision tree. Also, this paper reveals progressive
applications in addition to existing gap and less considered area and
determines the future works for researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8467</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8467</id><created>2013-10-31</created><authors><author><keyname>Rao</keyname><forenames>G. Srinivas</forenames></author><author><keyname>Ramana</keyname><forenames>A. V.</forenames></author></authors><title>Reinforcement Learning Framework for Opportunistic Routing in WSNs</title><categories>cs.NI cs.LG</categories><comments>05 Pages,07 Figures, 01 Table, IJCSN, Volume 2, Issue 5</comments><report-no>IJCSN-2013-2-5-22</report-no><journal-ref>IJCSN - International Journal of Computer Science and Network -
  October 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Routing packets opportunistically is an essential part of multihop ad hoc
wireless sensor networks. The existing routing techniques are not adaptive
opportunistic. In this paper we have proposed an adaptive opportunistic routing
scheme that routes packets opportunistically in order to ensure that packet
loss is avoided. Learning and routing are combined in the framework that
explores the optimal routing possibilities. In this paper we implemented this
Reinforced learning framework using a customer simulator. The experimental
results revealed that the scheme is able to exploit the opportunistic to
optimize routing of packets even though the network structure is unknown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8468</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8468</id><created>2013-10-31</created><authors><author><keyname>Kundu</keyname><forenames>Ankit</forenames></author><author><keyname>Roy</keyname><forenames>Pradosh K.</forenames></author></authors><title>Sparse Signal Recovery from Nonadaptive Linear Measurements</title><categories>cs.IT math.IT</categories><comments>5 Pages, 4 Figures. arXiv admin note: text overlap with
  arXiv:1106.6224 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of Compressed Sensing, the emerging sampling paradigm 'that goes
against the common wisdom', asserts that 'one can recover signals in Rn from
far fewer samples or measurements, if the signal has a sparse representation in
some orthonormal basis', from m = O(klogn), k&lt;&lt; n nonadaptive measurements .
The accuracy of the recovered signal is 'as good as that attainable with direct
knowledge of the k most important coefficients and its locations'. Moreover, a
good approximation to those important coefficients is extracted from the
measurements by solving a L1 minimization problem viz. Basis Pursuit. 'The
nonadaptive measurements have the character of random linear combinations of
the basis/frame elements'.
  The theory has implications which are far reaching and immediately leads to a
number of applications in Data Compression,Channel Coding and Data Acquisition.
'The last of these applications suggest that CS could have an enormous impact
in areas where conventional hardware design has significant limitations',
leading to 'efficient and revolutionary methods of data acquisition and storage
in future'.
  The paper reviews fundamental mathematical ideas pertaining to compressed
sensing viz. sparsity, incoherence, reduced isometry property and basis
pursuit, exemplified by the sparse recovery of a speech signal and convergence
of the L1- minimization algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8478</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8478</id><created>2013-10-31</created><updated>2014-04-14</updated><authors><author><keyname>Paolucci</keyname><forenames>Pier Stanislao</forenames></author><author><keyname>Ammendola</keyname><forenames>Roberto</forenames></author><author><keyname>Biagioni</keyname><forenames>Andrea</forenames></author><author><keyname>Frezza</keyname><forenames>Ottorino</forenames></author><author><keyname>Cicero</keyname><forenames>Francesca Lo</forenames></author><author><keyname>Lonardo</keyname><forenames>Alessandro</forenames></author><author><keyname>Pastorelli</keyname><forenames>Elena</forenames></author><author><keyname>Simula</keyname><forenames>Francesco</forenames></author><author><keyname>Tosoratto</keyname><forenames>Laura</forenames></author><author><keyname>Vicini</keyname><forenames>Piero</forenames></author></authors><title>Distributed simulation of polychronous and plastic spiking neural
  networks: strong and weak scaling of a representative mini-application
  benchmark executed on a small-scale commodity cluster</title><categories>cs.DC q-bio.NC</categories><comments>Added detailed profiling of computational and communication
  components. Improved speed and size of simulated networks. 15 pages, 5
  figures, 3 tables</comments><acm-class>C.2.4; C.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a natively distributed mini-application benchmark representative
of plastic spiking neural network simulators. It can be used to measure
performances of existing computing platforms and to drive the development of
future parallel/distributed computing systems dedicated to the simulation of
plastic spiking networks. The mini-application is designed to generate spiking
behaviors and synaptic connectivity that do not change when the number of
hardware processing nodes is varied, simplifying the quantitative study of
scalability on commodity and custom architectures. Here, we present the strong
and weak scaling and the profiling of the computational/communication
components of the DPSNN-STDP benchmark (Distributed Simulation of Polychronous
Spiking Neural Network with synaptic Spike-Timing Dependent Plasticity). In
this first test, we used the benchmark to exercise a small-scale cluster of
commodity processors (varying the number of used physical cores from 1 to 128).
The cluster was interconnected through a commodity network. Bidimensional grids
of columns composed of Izhikevich neurons projected synapses locally and toward
first, second and third neighboring columns. The size of the simulated network
varied from 6.6 Giga synapses down to 200 K synapses. The code demonstrated to
be fast and scalable: 10 wall clock seconds were required to simulate one
second of activity and plasticity (per Hertz of average firing rate) of a
network composed by 3.2 G synapses running on 128 hardware cores clocked @ 2.4
GHz. The mini-application has been designed to be easily interfaced with
standard and custom software and hardware communication interfaces. It has been
designed from its foundation to be natively distributed and parallel, and
should not pose major obstacles against distribution and parallelization on
several platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8486</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8486</id><created>2013-10-31</created><authors><author><keyname>Aupy</keyname><forenames>Guillaume</forenames></author><author><keyname>Benoit</keyname><forenames>Anne</forenames></author><author><keyname>H&#xe9;rault</keyname><forenames>Thomas</forenames></author><author><keyname>Robert</keyname><forenames>Yves</forenames></author><author><keyname>Vivien</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Zaidouni</keyname><forenames>Dounia</forenames></author></authors><title>On the Combination of Silent Error Detection and Checkpointing</title><categories>cs.DC</categories><comments>This work was accepted to be published in PRDC'13. Work supported by
  ANR Rescue</comments><report-no>INRIA RR-8319</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revisit traditional checkpointing and rollback recovery
strategies, with a focus on silent data corruption errors. Contrarily to
fail-stop failures, such latent errors cannot be detected immediately, and a
mechanism to detect them must be provided. We consider two models: (i) errors
are detected after some delays following a probability distribution (typically,
an Exponential distribution); (ii) errors are detected through some
verification mechanism. In both cases, we compute the optimal period in order
to minimize the waste, i.e., the fraction of time where nodes do not perform
useful computations. In practice, only a fixed number of checkpoints can be
kept in memory, and the first model may lead to an irrecoverable failure. In
this case, we compute the minimum period required for an acceptable risk. For
the second model, there is no risk of irrecoverable failure, owing to the
verification mechanism, but the corresponding overhead is included in the
waste. Finally, both models are instantiated using realistic scenarios and
application/architecture parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8487</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8487</id><created>2013-10-31</created><updated>2014-07-07</updated><authors><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author><author><keyname>Kubin</keyname><forenames>Gernot</forenames></author></authors><title>Information Loss and Anti-Aliasing Filters in Multirate Systems</title><categories>cs.IT math.IT</categories><comments>12 pages; a shorter version of this paper was published at the 2014
  International Zurich Seminar on Communications</comments><journal-ref>Proc. Int. Zurich Seminar on Communications, 2014, pp. 148 - 151</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates the information loss in a decimation system, i.e., in
a downsampler preceded by an anti-aliasing filter. It is shown that, without a
specific signal model in mind, the anti-aliasing filter cannot reduce
information loss, while, e.g., for a simple signal-plus-noise model it can. For
the Gaussian case, the optimal anti-aliasing filter is shown to coincide with
the one obtained from energetic considerations. For a non-Gaussian signal
corrupted by Gaussian noise, the Gaussian assumption yields an upper bound on
the information loss, justifying filter design principles based on second-order
statistics from an information-theoretic point-of-view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8489</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8489</id><created>2013-10-31</created><authors><author><keyname>Maina</keyname><forenames>Tirus Muya</forenames></author></authors><title>Instant messaging an effective way of communication in workplace</title><categories>cs.CY</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modern workplace is inherently collaborative, and this collaboration
relies on effective communication among coworkers. Instant messaging is the
multitasking tools of choice most people chatting over IM do other things at
the same time.The use of IM in workplace is less intrusive than the use of
phone, more immediate than email and has added advantage due to the ability to
detect presence.In order for institution to maximize increased business
productivity using instant messaging its imperative that organizations define
and publish ICT policies, guidelines and regulations.Overall IM boosts business
performance by making operations faster, more agile, and more efficient with
very little additional cost thus Organizations that deploy IM would reap
significant Return on Investment.Institutions should adopt IM meetings which
are be more efficient and less prone to straying off topic, because of the
relative effort of typing versus talking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8494</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8494</id><created>2013-10-31</created><authors><author><keyname>Mittal</keyname><forenames>Sparsh</forenames></author></authors><title>Using Cache-coloring to Mitigate Inter-set Write Variation in
  Non-volatile Caches</title><categories>cs.AR</categories><comments>STT-RAM cache</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, researchers have explored use of non-volatile devices such
as STT-RAM (spin torque transfer RAM) for designing on-chip caches, since they
provide high density and consume low leakage power. A common limitation of all
non-volatile devices is their limited write endurance. Further, since existing
cache management policies are write-variation unaware, excessive writes to a
few blocks may lead to a quick failure of the whole cache. We propose an
architectural technique for wear-leveling of non-volatile last level caches
(LLCs). Our technique uses cache-coloring approach which adds a
software-controlled mapping layer between groups of physical pages and cache
sets. Periodically the mapping is altered to ensure that write-traffic can be
spread uniformly to different sets of the cache to achieve wear-leveling.
Simulations performed with an x86-64 simulator and SPEC2006 benchmarks show
that our technique reduces the worst-case writes to cache blocks and thus
improves the cache lifetime by 4.07X.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8499</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8499</id><created>2013-10-31</created><updated>2014-05-20</updated><authors><author><keyname>Gregor</keyname><forenames>Karol</forenames></author><author><keyname>Danihelka</keyname><forenames>Ivo</forenames></author><author><keyname>Mnih</keyname><forenames>Andriy</forenames></author><author><keyname>Blundell</keyname><forenames>Charles</forenames></author><author><keyname>Wierstra</keyname><forenames>Daan</forenames></author></authors><title>Deep AutoRegressive Networks</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the 31st International Conference on
  Machine Learning (ICML), Beijing, China, 2014</comments><journal-ref>Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, Daan
  Wierstra. Deep AutoRegressive Networks. In Proceedings of the 31st
  International Conference on Machine Learning (ICML), JMLR: W&amp;CP volume 32,
  2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a deep, generative autoencoder capable of learning hierarchies
of distributed representations from data. Successive deep stochastic hidden
layers are equipped with autoregressive connections, which enable the model to
be sampled from quickly and exactly via ancestral sampling. We derive an
efficient approximate parameter estimation method based on the minimum
description length (MDL) principle, which can be seen as maximising a
variational lower bound on the log-likelihood, with a feedforward neural
network implementing approximate inference. We demonstrate state-of-the-art
generative performance on a number of classic data sets: several UCI data sets,
MNIST and Atari 2600 games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8508</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8508</id><created>2013-10-31</created><updated>2013-12-10</updated><authors><author><keyname>Samoilenko</keyname><forenames>Anna</forenames></author><author><keyname>Yasseri</keyname><forenames>Taha</forenames></author></authors><title>The distorted mirror of Wikipedia: a quantitative analysis of Wikipedia
  coverage of academics</title><categories>physics.soc-ph cs.CY cs.DL cs.SI physics.data-an</categories><comments>To appear in EPJ Data Science. To have the Additional Files and
  Datasets e-mail the corresponding author</comments><journal-ref>EPJ Data Science 2014, 3:1</journal-ref><doi>10.1140/epjds20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Activity of modern scholarship creates online footprints galore. Along with
traditional metrics of research quality, such as citation counts, online images
of researchers and institutions increasingly matter in evaluating academic
impact, decisions about grant allocation, and promotion. We examined 400
biographical Wikipedia articles on academics from four scientific fields to
test if being featured in the world's largest online encyclopedia is correlated
with higher academic notability (assessed through citation counts). We found no
statistically significant correlation between Wikipedia articles metrics
(length, number of edits, number of incoming links from other articles, etc.)
and academic notability of the mentioned researchers. We also did not find any
evidence that the scientists with better WP representation are necessarily more
prominent in their fields. In addition, we inspected the Wikipedia coverage of
notable scientists sampled from Thomson Reuters list of &quot;highly cited
researchers&quot;. In each of the examined fields, Wikipedia failed in covering
notable scholars properly. Both findings imply that Wikipedia might be
producing an inaccurate image of academics on the front end of science. By
shedding light on how public perception of academic progress is formed, this
study alerts that a subjective element might have been introduced into the
hitherto structured system of academic evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8509</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8509</id><created>2013-10-31</created><updated>2014-05-17</updated><authors><author><keyname>De la Cruz</keyname><forenames>Javier</forenames></author><author><keyname>Gutierrez</keyname><forenames>Ismael</forenames></author><author><keyname>Robinson</keyname><forenames>Jorge</forenames></author></authors><title>Construction of extremal or optimal codes with an automorphism of order
  29</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to the results about
  the [60, 30, 12]-code are already known</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we construct a new optimal code with parameters [120, 60, 20]
of type II with an automorphism of order 29. Furthermore we classify all
extremal codes with length 60 of type I with an automorphism of this order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8511</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8511</id><created>2013-10-31</created><updated>2015-03-06</updated><authors><author><keyname>D&#x119;bowski</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>A Preadapted Universal Switch Distribution for Testing Hilberg's
  Conjecture</title><categories>cs.IT cs.CL math.IT</categories><comments>17 pages, 3 figures</comments><msc-class>68P30, 94A45</msc-class><acm-class>E.4</acm-class><journal-ref>IEEE Transactions on Information Theory, 61(10):5708-5715, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hilberg's conjecture about natural language states that the mutual
information between two adjacent long blocks of text grows like a power of the
block length. The exponent in this statement can be upper bounded using the
pointwise mutual information estimate computed for a carefully chosen code. The
bound is the better, the lower the compression rate is but there is a
requirement that the code be universal. So as to improve a received upper bound
for Hilberg's exponent, in this paper, we introduce two novel universal codes,
called the plain switch distribution and the preadapted switch distribution.
Generally speaking, switch distributions are certain mixtures of adaptive
Markov chains of varying orders with some additional communication to avoid so
called catch-up phenomenon. The advantage of these distributions is that they
both achieve a low compression rate and are guaranteed to be universal. Using
the switch distributions we obtain that a sample of a text in English is
non-Markovian with Hilberg's exponent being $\le 0.83$, which improves over the
previous bound $\le 0.94$ obtained using the Lempel-Ziv code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8532</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8532</id><created>2013-10-29</created><authors><author><keyname>Rezki</keyname><forenames>Zouheir</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>On the Capacity of Multiple Access and Broadcast Fading Channels with
  Full Channel State Information at Low SNR</title><categories>cs.IT math.IT</categories><comments>12 pages, 12 figures, To appear in IEEE Transactions on Wireless
  Communications, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the throughput capacity region of the Gaussian multi-access (MAC)
fading channel with perfect channel state information (CSI) at the receiver and
at the transmitters, at low power regime. We show that it has a
multidimensional rectangle structure and thus is simply characterized by single
user capacity points. More specifically, we show that at low power regime, the
boundary surface of the capacity region shrinks to a single point corresponding
to the sum rate maximizer and that the coordinates of this point coincide with
single user capacity bounds. Inspired by this result, we propose an on-off
scheme, compute its achievable rate, and show that this scheme achieves single
user capacity bounds of the MAC channel for a wide class of fading channels at
asymptotically low power regime. We argue that this class of fading encompasses
all known wireless channels for which the capacity region of the MAC channel
has even a simpler expression in terms of users' average power constraints
only. Using the duality of Gaussian MAC and broadcast channels (BC), we deduce
a simple characterization of the BC capacity region at low power regime and
show that for a class of fading channels (including Rayleigh fading),
time-sharing is asymptotically optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8540</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8540</id><created>2013-10-31</created><authors><author><keyname>Naik</keyname><forenames>Gaurang</forenames></author><author><keyname>Singhal</keyname><forenames>Sudesh</forenames></author><author><keyname>Kumar</keyname><forenames>Animesh</forenames></author><author><keyname>Karandikar</keyname><forenames>Abhay</forenames></author></authors><title>Quantitative Assessment of TV White Space in India</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Licensed but unutilized television (TV) band spectrum is called as TV white
space in the literature. Ultra high frequency (UHF) TV band spectrum has very
good wireless radio propagation characteristics. The amount of TV white space
in the UHF TV band in India is of interest. Comprehensive quantitative
assessment and estimates for the TV white space in the 470-590MHz band for four
zones of India (all except north) are presented in this work. This is the first
effort in India to estimate TV white spaces in a comprehensive manner. The
average available TV white space per unit area in these four zones is
calculated using two methods: (i) the primary (licensed) user and secondary
(unlicensed) user point of view; and, (ii) the regulations of Federal
Communications Commission in the United States. By both methods, the average
available TV white space in the UHF TV band is shown to be more than 100MHz! A
TV transmitter frequency-reassignment algorithm is also described. Based on
spatial-reuse ideas, a TV channel allocation scheme is presented which results
in insignicant interference to the TV receivers while using the least number of
TV channels for transmission across the four zones. Based on this reassignment,
it is found that four TV band channels (or 32MHz) are sufficient to provide the
existing UHF TV band coverage in India.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8568</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8568</id><created>2013-10-31</created><authors><author><keyname>Southern</keyname><forenames>Mary</forenames></author><author><keyname>Nadathur</keyname><forenames>Gopalan</forenames></author></authors><title>Translating Specifications in a Dependently Typed Lambda Calculus into a
  Predicate Logic Form</title><categories>cs.LO</categories><comments>14 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dependently typed lambda calculi such as the Edinburgh Logical Framework (LF)
are a popular means for encoding rule-based specifications concerning formal
syntactic objects. In these frameworks, relations over terms representing
formal objects are naturally captured by making use of the dependent structure
of types. We consider here the meaning-preserving translation of specifications
written in this style into a predicate logic over simply typed {\lambda}-terms.
Such a translation can provide the basis for efficient implementation and
sophisticated capabilities for reasoning about specifications. We start with a
previously described translation of LF specifications to formulas in the logic
of higher-order hereditary Harrop (hohh) formulas. We show how this translation
can be improved by recognizing and eliminating redundant type checking
information contained in it. This benefits both the clarity of translated
formulas, and reduces the effort which must be spent on type checking during
execution. To allow this translation to be used to execute LF specifications,
we describe an inverse transformation from hohh terms to LF expressions; thus
computations can be carried out using the translated form and the results can
then be exported back into LF. Execution based on LF specifications may also
involve some forms of type reconstruction. We discuss the possibility of
supporting such a capability using the translation under some reasonable
restrictions on the structure of specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8573</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8573</id><created>2013-10-31</created><updated>2013-11-12</updated><authors><author><keyname>Ricaud</keyname><forenames>Benjamin</forenames></author><author><keyname>Stempfel</keyname><forenames>Guillaume</forenames></author><author><keyname>Torr&#xe9;sani</keyname><forenames>Bruno</forenames></author><author><keyname>Wiesmeyr</keyname><forenames>Christoph</forenames></author><author><keyname>Lachambre</keyname><forenames>H&#xe9;l&#xe8;ne</forenames></author><author><keyname>Onchis</keyname><forenames>Darian</forenames></author></authors><title>An optimally concentrated Gabor transform for localized time-frequency
  components</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gabor analysis is one of the most common instances of time-frequency signal
analysis. Choosing a suitable window for the Gabor transform of a signal is
often a challenge for practical applications, in particular in audio signal
processing. Many time-frequency (TF) patterns of different shapes may be
present in a signal and they can not all be sparsely represented in the same
spectrogram. We propose several algorithms, which provide optimal windows for a
user-selected TF pattern with respect to different concentration criteria. We
base our optimization algorithm on $l^p$-norms as measure of TF spreading. For
a given number of sampling points in the TF plane we also propose optimal
lattices to be used with the obtained windows. We illustrate the potentiality
of the method on selected numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8583</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8583</id><created>2013-10-31</created><authors><author><keyname>Shatabda</keyname><forenames>Swakkhar</forenames></author><author><keyname>Newton</keyname><forenames>M. A. Hakim</forenames></author><author><keyname>Pham</keyname><forenames>Duc Nghia</forenames></author><author><keyname>Sattar</keyname><forenames>Abdul</forenames></author></authors><title>A Hybrid Local Search for Simplified Protein Structure Prediction</title><categories>cs.CE cs.AI</categories><journal-ref>Proceedings of the International Conference on Bioinformatics
  Models, Methods and Algorithms, Barcelona, Spain, 11 - 14 February, 2013.
  SciTePress 2013 ISBN 978-989-8565-35-8 pages:158-163</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protein structure prediction based on Hydrophobic-Polar energy model
essentially becomes searching for a conformation having a compact hydrophobic
core at the center. The hydrophobic core minimizes the interaction energy
between the amino acids of the given protein. Local search algorithms can
quickly find very good conformations by moving repeatedly from the current
solution to its &quot;best&quot; neighbor. However, once such a compact hydrophobic core
is found, the search stagnates and spends enormous effort in quest of an
alternative core. In this paper, we attempt to restructure segments of a
conformation with such compact core. We select one large segment or a number of
small segments and apply exhaustive local search. We also apply a mix of
heuristics so that one heuristic can help escape local minima of another. We
evaluated our algorithm by using Face Centered Cubic (FCC) Lattice on a set of
standard benchmark proteins and obtain significantly better results than that
of the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8585</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8585</id><created>2013-10-30</created><authors><author><keyname>Steiner</keyname><forenames>Ingmar</forenames><affiliation>MMCI, DFKI, Allgemeine Linguistik</affiliation></author><author><keyname>Richmond</keyname><forenames>Korin</forenames><affiliation>CSTR</affiliation></author><author><keyname>Ouni</keyname><forenames>Slim</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Speech animation using electromagnetic articulography as motion capture
  data</title><categories>cs.HC q-bio.QM</categories><proxy>ccsd</proxy><journal-ref>AVSP - 12th International Conference on Auditory-Visual Speech
  Processing - 2013 (2013) 55-60</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electromagnetic articulography (EMA) captures the position and orientation of
a number of markers, attached to the articulators, during speech. As such, it
performs the same function for speech that conventional motion capture does for
full-body movements acquired with optical modalities, a long-time staple
technique of the animation industry. In this paper, EMA data is processed from
a motion-capture perspective and applied to the visualization of an existing
multimodal corpus of articulatory data, creating a kinematic 3D model of the
tongue and teeth by adapting a conventional motion capture based animation
paradigm. This is accomplished using off-the-shelf, open-source software. Such
an animated model can then be easily integrated into multimedia applications as
a digital asset, allowing the analysis of speech production in an intuitive and
accessible manner. The processing of the EMA data, its co-registration with 3D
data from vocal tract magnetic resonance imaging (MRI) and dental scans, and
the modeling workflow are presented in detail, and several issues discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8588</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8588</id><created>2013-09-22</created><authors><author><keyname>Said</keyname><forenames>Tkatek</forenames></author><author><keyname>Otman</keyname><forenames>Abdoun</forenames></author><author><keyname>Jaafar</keyname><forenames>Abouchabaka</forenames></author><author><keyname>Najat</keyname><forenames>Rafalia</forenames></author></authors><title>A Meta-heuristically Approach of the Spatial Assignment Problem of Human
  Resources in Multi-sites Enterprise</title><categories>cs.AI</categories><journal-ref>International Journal of Computer Applications (0975 - 8887)
  Volume 78 - Number 7 September 2013</journal-ref><doi>10.5120/13500-1248</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this work is to present a meta-heuristically approach of the
spatial assignment problem of human resources in multi-sites enterprise.
Usually, this problem consists to move employees from one site to another based
on one or more criteria. Our goal in this new approach is to improve the
quality of service and performance of all sites with maximizing an objective
function under some managers imposed constraints. The formulation presented
here of this problem coincides perfectly with a Combinatorial Optimization
Problem (COP) which is in the most cases NP-hard to solve optimally. To avoid
this difficulty, we have opted to use a meta-heuristic popular method, which is
the genetic algorithm, to solve this problem in concrete cases. The results
obtained have shown the effectiveness of our approach, which remains until now
very costly in time. But the reduction of the time can be obtained by different
ways that we plan to do in the next work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8591</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8591</id><created>2013-10-31</created><updated>2014-07-20</updated><authors><author><keyname>Afsar</keyname><forenames>M. Mehdi</forenames></author></authors><title>Effective Data Aggregation Scheme for Large-scale Wireless Sensor
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy preservation is one of the most important challenges in wireless
sensor networks. In most applications, sensor networks consist of hundreds or
thousands nodes that are dispersed in a wide field. Hierarchical architectures
and data aggregation methods are increasingly gaining more popularity in such
large-scale networks. In this paper, we propose a novel adaptive
Energy-Efficient Multi-layered Architecture (EEMA) protocol for large-scale
sensor networks, wherein both hierarchical architecture and data aggregation
are efficiently utilized. EEMA divides the network into some layers as well as
each layer into some clusters, where the data are gathered in the first layer
and are recursively aggregated in upper layers to reach the base station. Many
criteria are wisely employed to elect head nodes, including the residual
energy, centrality, and proximity to bottom-layer heads. The routing delay is
mathematically analyzed. Performance evaluation is performed via simulations
which confirms the effectiveness of the proposed EEMA protocol in terms of the
network lifetime and reduced routing delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8599</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8599</id><created>2013-10-31</created><updated>2015-07-13</updated><authors><author><keyname>Wolff</keyname><forenames>J. Gerard</forenames></author></authors><title>Information Compression, Intelligence, Computing, and Mathematics</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents evidence for the idea that much of artificial
intelligence, human perception and cognition, mainstream computing, and
mathematics, may be understood as compression of information via the matching
and unification of patterns. This is the basis for the &quot;SP theory of
intelligence&quot;, outlined in the paper and fully described elsewhere. Relevant
evidence may be seen: in empirical support for the SP theory; in some
advantages of information compression (IC) in terms of biology and engineering;
in our use of shorthands and ordinary words in language; in how we merge
successive views of any one thing; in visual recognition; in binocular vision;
in visual adaptation; in how we learn lexical and grammatical structures in
language; and in perceptual constancies. IC via the matching and unification of
patterns may be seen in both computing and mathematics: in IC via equations; in
the matching and unification of names; in the reduction or removal of
redundancy from unary numbers; in the workings of Post's Canonical System and
the transition function in the Universal Turing Machine; in the way computers
retrieve information from memory; in systems like Prolog; and in the
query-by-example technique for information retrieval. The chunking-with-codes
technique for IC may be seen in the use of named functions to avoid repetition
of computer code. The schema-plus-correction technique may be seen in functions
with parameters and in the use of classes in object-oriented programming. And
the run-length coding technique may be seen in multiplication, in division, and
in several other devices in mathematics and computing. The SP theory resolves
the apparent paradox of &quot;decompression by compression&quot;. And computing and
cognition as IC is compatible with the uses of redundancy in such things as
backup copies to safeguard data and understanding speech in a noisy
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8613</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8613</id><created>2013-10-31</created><updated>2013-11-05</updated><authors><author><keyname>Lis&#xfd;</keyname><forenames>Viliam</forenames></author><author><keyname>Kova&#x159;&#xed;k</keyname><forenames>Vojt&#x11b;ch</forenames></author><author><keyname>Lanctot</keyname><forenames>Marc</forenames></author><author><keyname>Bo&#x161;ansk&#xfd;</keyname><forenames>Branislav</forenames></author></authors><title>Convergence of Monte Carlo Tree Search in Simultaneous Move Games</title><categories>cs.GT</categories><comments>NIPS 2013 paper including appendix</comments><journal-ref>Advances in Neural Information Processing Systems 26, pp
  2112-2120, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study Monte Carlo tree search (MCTS) in zero-sum extensive-form games with
perfect information and simultaneous moves. We present a general template of
MCTS algorithms for these games, which can be instantiated by various selection
methods. We formally prove that if a selection method is $\epsilon$-Hannan
consistent in a matrix game and satisfies additional requirements on
exploration, then the MCTS algorithm eventually converges to an approximate
Nash equilibrium (NE) of the extensive-form game. We empirically evaluate this
claim using regret matching and Exp3 as the selection methods on randomly
generated games and empirically selected worst case games. We confirm the
formal result and show that additional MCTS variants also converge to
approximate NE on the evaluated games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8615</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8615</id><created>2013-10-31</created><authors><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Richard</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Sayed</keyname><forenames>Ali</forenames></author></authors><title>Diffusion LMS for clustered multitask networks</title><categories>cs.SY cs.IT cs.MA math.IT</categories><comments>5 pages, 6 figures, submitted to ICASSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research works on distributed adaptive networks have intensively
studied the case where the nodes estimate a common parameter vector
collaboratively. However, there are many applications that are
multitask-oriented in the sense that there are multiple parameter vectors that
need to be inferred simultaneously. In this paper, we employ diffusion
strategies to develop distributed algorithms that address clustered multitask
problems by minimizing an appropriate mean-square error criterion with
$\ell_2$-regularization. Some results on the mean-square stability and
convergence of the algorithm are also provided. Simulations are conducted to
illustrate the theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8620</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8620</id><created>2013-10-31</created><updated>2014-09-30</updated><authors><author><keyname>Andreasson</keyname><forenames>Martin</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author><author><keyname>Sandberg</keyname><forenames>Henrik</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Distributed Control of Networked Dynamical Systems: Static Feedback,
  Integral Action and Consensus</title><categories>math.DS cs.SY</categories><comments>Automatic Control, IEEE Transactions on, July 2014</comments><doi>10.1109/TAC.2014.2309281</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes distributed control protocols for first- and second-order
networked dynamical systems. We propose a class of nonlinear consensus
controllers where the input of each agent can be written as a product of a
nonlinear gain, and a sum of nonlinear interaction functions. By using integral
Lyapunov functions, we prove the stability of the proposed control protocols,
and explicitly characterize the equilibrium set. We also propose a distributed
proportional-integral (PI) controller for networked dynamical systems. The PI
controllers successfully attenuate constant disturbances in the network. We
prove that agents with single-integrator dynamics are stable for any integral
gain, and give an explicit tight upper bound on the integral gain for when the
system is stable for agents with double-integrator dynamics. Throughout the
paper we highlight some possible applications of the proposed controllers by
realistic simulations of autonomous satellites, power systems and building
temperature control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8631</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8631</id><created>2013-10-31</created><authors><author><keyname>Fischer</keyname><forenames>Felix</forenames></author><author><keyname>Klimm</keyname><forenames>Max</forenames></author></authors><title>Optimal Impartial Selection</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of selecting a member of a set of agents based on
impartial nominations by agents from that set. The problem was studied
previously by Alon et al. and Holzman and Moulin and has important applications
in situations where representatives are selected from within a group or where
publishing or funding decisions are made based on a process of peer review. Our
main result concerns a randomized mechanism that in expectation awards the
prize to an agent with at least half the maximum number of nominations. Subject
to impartiality, this is best possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.8635</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.8635</id><created>2013-10-31</created><updated>2014-04-23</updated><authors><author><keyname>Rowland</keyname><forenames>Eric</forenames></author><author><keyname>Yassawi</keyname><forenames>Reem</forenames></author></authors><title>Automatic congruences for diagonals of rational functions</title><categories>math.NT cs.SC math.CO</categories><comments>42 pages, many figures; final version (minor changes)</comments><msc-class>05A15, 11A07, 11B50, 11B85</msc-class><journal-ref>Journal de Th\'eorie des Nombres de Bordeaux 27 (2015) 245-288</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we use the framework of automatic sequences to study
combinatorial sequences modulo prime powers. Given a sequence whose generating
function is the diagonal of a rational power series, we provide a method, based
on work of Denef and Lipshitz, for computing a finite automaton for the
sequence modulo $p^\alpha$, for all but finitely many primes $p$. This method
gives completely automatic proofs of known results, establishes a number of new
theorems for well-known sequences, and allows us to resolve some conjectures
regarding the Ap\'ery numbers. We also give a second method, which applies to
an algebraic sequence modulo $p^\alpha$ for all primes $p$, but is
significantly slower. Finally, we show that a broad range of multidimensional
sequences possess Lucas products modulo $p$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0035</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0035</id><created>2013-10-31</created><authors><author><keyname>Mousavi</keyname><forenames>Ali</forenames></author><author><keyname>Maleki</keyname><forenames>Arian</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Parameterless Optimal Approximate Message Passing</title><categories>cs.IT math.IT math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iterative thresholding algorithms are well-suited for high-dimensional
problems in sparse recovery and compressive sensing. The performance of this
class of algorithms depends heavily on the tuning of certain threshold
parameters. In particular, both the final reconstruction error and the
convergence rate of the algorithm crucially rely on how the threshold parameter
is set at each step of the algorithm. In this paper, we propose a
parameter-free approximate message passing (AMP) algorithm that sets the
threshold parameter at each iteration in a fully automatic way without either
having an information about the signal to be reconstructed or needing any
tuning from the user. We show that the proposed method attains both the minimum
reconstruction error and the highest convergence rate. Our method is based on
applying the Stein unbiased risk estimate (SURE) along with a modified gradient
descent to find the optimal threshold in each iteration. Motivated by the
connections between AMP and LASSO, it could be employed to find the solution of
the LASSO for the optimal regularization parameter. To the best of our
knowledge, this is the first work concerning parameter tuning that obtains the
fastest convergence rate with theoretical guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0041</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0041</id><created>2013-10-31</created><updated>2014-04-30</updated><authors><author><keyname>Mittal</keyname><forenames>Sparsh</forenames></author></authors><title>A Technique for Write-endurance aware Management of Resistive RAM Last
  Level Caches</title><categories>cs.AR</categories><comments>This paper has been withdrawn by the author for revising the
  experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to increasing cache sizes and large leakage consumption of SRAM device,
conventional SRAM caches contribute significantly to the processor power
consumption. Recently researchers have used non-volatile memory devices to
design caches, since they provide high density, comparable read latency and low
leakage power dissipation. However, their high write latency may increase the
execution time and hence, leakage energy consumption. Also, since their write
endurance is small, a conventional energy saving technique may further
aggravate the problem of write-variations, thus reducing their lifetime. In
this paper, we present a cache energy saving technique for non-volatile caches,
which also attempts to improve their lifetime by making writes equally
distributed to the cache. Our technique uses dynamic cache reconfiguration to
adjust the cache size to meet program requirement and turns off the remaining
cache to save energy. Microarchitectural simulations performed using an x86-64
simulator, SPEC2006 benchmarks and a resistive-RAM LLC (last level cache) show
that over an 8MB baseline cache, our technique saves 17.55% memory subsystem
(last level cache + main memory) energy and improves the lifetime by 1.33X.
Over the same resistive-RAM baseline, an SRAM of similar area with no cache
reconfiguration leads to an energy loss of 186.13%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0044</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0044</id><created>2013-10-31</created><authors><author><keyname>Omar</keyname><forenames>Rasha Salah</forenames></author><author><keyname>El-Mahdy</keyname><forenames>Ahmed</forenames></author><author><keyname>Rohou</keyname><forenames>Erven</forenames></author></authors><title>Thread-Based Obfuscation through Control-Flow Mangling</title><categories>cs.CR cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing use of cloud computing and remote execution have made program
security especially important. Code obfuscation has been proposed to make the
understanding of programs more complicated to attackers. In this paper, we
exploit multi-core processing to substantially increase the complexity of
programs, making reverse engineering more complicated. We propose a novel
method that automatically partitions any serial thread into an arbitrary number
of parallel threads, at the basic-block level. The method generates new
control-flow graphs, preserving the blocks' serial successor relations and
guaranteeing that one basic-block is active at a time using guards. The method
generates m^n different combinations for m threads and n basic-blocks,
significantly complicating the execution state. We provide a correctness proof
for the algorithm and implement the algorithm in the LLVM compilation
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0053</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0053</id><created>2013-10-31</created><updated>2013-11-20</updated><authors><author><keyname>Landecker</keyname><forenames>Will</forenames></author><author><keyname>Chartrand</keyname><forenames>Rick</forenames></author><author><keyname>DeDeo</keyname><forenames>Simon</forenames></author></authors><title>Robust Compressed Sensing and Sparse Coding with the Difference Map</title><categories>cs.CV physics.data-an stat.ML</categories><comments>8 pages; Revised comparison to DM-ECME algorithm in Section 2.1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In compressed sensing, we wish to reconstruct a sparse signal $x$ from
observed data $y$. In sparse coding, on the other hand, we wish to find a
representation of an observed signal $y$ as a sparse linear combination, with
coefficients $x$, of elements from an overcomplete dictionary. While many
algorithms are competitive at both problems when $x$ is very sparse, it can be
challenging to recover $x$ when it is less sparse. We present the Difference
Map, which excels at sparse recovery when sparseness is lower and noise is
higher. The Difference Map out-performs the state of the art with
reconstruction from random measurements and natural image reconstruction via
sparse coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0058</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0058</id><created>2013-10-31</created><authors><author><keyname>Gable</keyname><forenames>Ian</forenames></author><author><keyname>Chester</keyname><forenames>Michael</forenames></author><author><keyname>Armstrong</keyname><forenames>Patrick</forenames></author><author><keyname>Berghaus</keyname><forenames>Frank</forenames></author><author><keyname>Charbonneau</keyname><forenames>Andre</forenames></author><author><keyname>Leavett-Brown</keyname><forenames>Colin</forenames></author><author><keyname>Paterson</keyname><forenames>Michael</forenames></author><author><keyname>Prior</keyname><forenames>Robert</forenames></author><author><keyname>Sobie</keyname><forenames>Randall</forenames></author><author><keyname>Taylor</keyname><forenames>Ryan</forenames></author></authors><title>Dynamic web cache publishing for IaaS clouds using Shoal</title><categories>cs.DC</categories><comments>Conference paper at the 2013 Computing in HEP (CHEP) Conference,
  Amsterdam</comments><doi>10.1088/1742-6596/513/3/032035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have developed a highly scalable application, called Shoal, for tracking
and utilizing a distributed set of HTTP web caches. Squid servers advertise
their existence to the Shoal server via AMQP messaging by running Shoal Agent.
The Shoal server provides a simple REST interface that allows clients to
determine their closest Squid cache. Our goal is to dynamically instantiate
Squid caches on IaaS clouds in response to client demand. Shoal provides the
VMs on IaaS clouds with the location of the nearest dynamically instantiated
Squid Cache. In this paper, we describe the design and performance of Shoal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0059</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0059</id><created>2013-10-31</created><authors><author><keyname>Wen</keyname><forenames>Jian</forenames></author><author><keyname>Borkar</keyname><forenames>Vinayak R.</forenames></author><author><keyname>Carey</keyname><forenames>Michael J.</forenames></author><author><keyname>Tsotras</keyname><forenames>Vassilis J.</forenames></author></authors><title>Revisiting Aggregation for Data Intensive Applications: A Performance
  Study</title><categories>cs.DB</categories><comments>25 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aggregation has been an important operation since the early days of
relational databases. Today's Big Data applications bring further challenges
when processing aggregation queries, demanding adaptive aggregation algorithms
that can process large volumes of data relative to a potentially limited memory
budget (especially in multiuser settings). Despite its importance, the design
and evaluation of aggregation algorithms has not received the same attention
that other basic operators, such as joins, have received in the literature. As
a result, when considering which aggregation algorithm(s) to implement in a new
parallel Big Data processing platform (AsterixDB), we faced a lack of &quot;off the
shelf&quot; answers that we could simply read about and then implement based on
prior performance studies.
  In this paper we revisit the engineering of efficient local aggregation
algorithms for use in Big Data platforms. We discuss the salient implementation
details of several candidate algorithms and present an in-depth experimental
performance study to guide future Big Data engine developers. We show that the
efficient implementation of the aggregation operator for a Big Data platform is
non-trivial and that many factors, including memory usage, spilling strategy,
and I/O and CPU cost, should be considered. Further, we introduce precise cost
models that can help in choosing an appropriate algorithm based on input
parameters including memory budget, grouping key cardinality, and data skew.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0090</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0090</id><created>2013-11-01</created><authors><author><keyname>Uddin</keyname><forenames>Shahadat</forenames></author><author><keyname>Piraveenan</keyname><forenames>Mahendra</forenames></author><author><keyname>Khan</keyname><forenames>Arif</forenames></author><author><keyname>Amiri</keyname><forenames>Babak</forenames></author></authors><title>Conceptual quantification of the dynamicity of longitudinal social
  networks</title><categories>cs.SI physics.soc-ph</categories><comments>SocialCom 2013 conference</comments><doi>10.1109/SocialCom.2013.131</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A longitudinal social network evolves over time through the creation and/ or
deletion of links among a set of actors (e.g. individuals or organizations).
Longitudinal social networks are studied by network science and social science
researchers to understand networke volution, trend propagation, friendship and
belief formation, diffusion of innovation, the spread of deviant behavior and
more. In the current literature, there are different approaches and methods
(e.g. Sampsons approach and the markov model) to study the dynamics of
longitudinal social networks. These approaches and methods have mainly been
utilised to explore evolutionary changes of longitudinal social networks from
one state to another and to explain the underlying reasons for these changes.
However, they cannot quantify the level of dynamicity of the over time network
changes and the contribution of individual network members (i.e. actors) to
these changes. In this study, we first develop a set of measures to quantify
different aspects of the dynamicity of a longitudinal social network. We then
apply these measures, in order to conduct empirical investigations, to two
different longitudinal social networks. Finally, we discuss the implications of
the application of these measures and possible future research directions of
this study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0095</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0095</id><created>2013-11-01</created><authors><author><keyname>Takeda</keyname><forenames>Koujin</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author></authors><title>Reconstruction algorithm in compressed sensing based on maximum a
  posteriori estimation</title><categories>cs.IT cond-mat.dis-nn math.IT</categories><comments>11 pages, 5 figures, proceedings of ELC International Meeting on
  &quot;Inference, Computation, and Spin Glasses&quot; (ICSG2013), Sapporo, Japan</comments><journal-ref>J. Phys. Conf. Ser. 473 (2013) 012003</journal-ref><doi>10.1088/1742-6596/473/1/012003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a systematic method for constructing a sparse data reconstruction
algorithm in compressed sensing at a relatively low computational cost for
general observation matrix. It is known that the cost of l1-norm minimization
using a standard linear programming algorithm is O(N^3). We show that this cost
can be reduced to O(N^2) by applying the approach of posterior maximization.
Furthermore, in principle, the algorithm from our approach is expected to
achieve the widest successful reconstruction region, which is evaluated from
theoretical argument. We also discuss the relation between the belief
propagation-based reconstruction algorithm introduced in preceding works and
our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0100</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0100</id><created>2013-11-01</created><updated>2014-03-03</updated><authors><author><keyname>Li</keyname><forenames>Cheuk Ting</forenames></author><author><keyname>Gamal</keyname><forenames>Abbas El</forenames></author></authors><title>An Efficient Feedback Coding Scheme with Low Error Probability for
  Discrete Memoryless Channels</title><categories>cs.IT math.IT</categories><comments>16 pages, 6 figures</comments><journal-ref>IEEE Trans. Info. Theory, vol.61, no.6, pp.2953-2963, June 2015</journal-ref><doi>10.1109/TIT.2015.2428234</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing feedback communication schemes are either specialized to particular
channels (Schalkwijk--Kailath, Horstein), or apply to general channels but have
high coding complexity (block feedback schemes) or are difficult to analyze
(posterior matching). This paper introduces a feedback coding scheme that
combines features from previous schemes in addition to a new randomization
technique. We show that our scheme achieves the capacity for all discrete
memoryless channels and establish a bound on its error exponent that approaches
the sphere packing bound as the rate approaches the capacity. These benefits
are attained with only O(n log n) coding complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0110</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0110</id><created>2013-11-01</created><updated>2015-07-15</updated><authors><author><keyname>Hwang</keyname><forenames>Duckdong</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author><author><keyname>Nam</keyname><forenames>Sung Sik</forenames></author><author><keyname>Lee</keyname><forenames>Tae-Jin</forenames></author></authors><title>Opportunistic Multiuser Two-Way Amplify-and-Forward Relaying with a
  Multi Antenna Relay</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to crucial errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the opportunistic multiuser diversity in the multiuser two-way
amplify-and-forward (AF) relay channel. The relay, equipped with multiple
antennas and a simple zero-forcing beam-forming scheme, selects a set of two
way relaying user pairs to enhance the degree of freedom (DoF) and consequently
the sum throughput of the system. The proposed channel aligned pair scheduling
(CAPS) algorithm reduces the inter-pair interference and keeps the signal to
interference plus noise power ratio (SINR) of user pairs relatively
interference free in average sense when the number of user pairs become very
large. For ideal situations, where the number of user pairs grows faster than
the system signal to noise ratio (SNR), the DoF of $M$ per channel use can be
achieved when $M$ is the relay antenna size. With a limited number of pairs,
the system is overloaded and the sum rates saturate at high signal to noise
ratio (SNR) though modifications of CAPS can improve the performance to a
certain amount. The performance of CAPS can be further enhanced by
semi-orthogonal channel aligned pair scheduling (SCAPS) algorithm, which not
only aligns the pair channels but also forms semi-orthogonal inter-pair
channels. Simulation results show that we provide a set of approaches based on
(S)CAPS and modified (S)CAPS, which provides system performance benefit
depending on the SNR and the number of user pairs in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0117</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0117</id><created>2013-11-01</created><updated>2015-05-05</updated><authors><author><keyname>Boissonnat</keyname><forenames>Jean-Daniel</forenames><affiliation>INRIA Sophia Antipolis / INRIA Saclay - Ile de France</affiliation></author><author><keyname>Dyer</keyname><forenames>Ramsay</forenames><affiliation>MPII</affiliation></author><author><keyname>Ghosh</keyname><forenames>Arijit</forenames><affiliation>MPII</affiliation></author></authors><title>Delaunay triangulation of manifolds</title><categories>cs.CG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for producing Delaunay triangulations of manifolds.
The algorithm can accommodate abstract manifolds that are not presented as
submanifolds of Euclidean space. Given a set of sample points and an atlas on a
compact manifold, a manifold Delaunay complex is produced provided the
transition functions are bi-Lipschitz with a constant close to 1, and the
sample points meet a local density requirement; no smoothness assumptions are
required. If the transition functions are smooth, the output is a triangulation
of the manifold.
  The output complex is naturally endowed with a piecewise flat metric which,
when the original manifold is Riemannian, is a close approximation of the
original Riemannian metric. In this case the ouput complex is also a Delaunay
triangulation of its vertices with respect to this piecewise flat metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0119</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0119</id><created>2013-11-01</created><authors><author><keyname>Eynard</keyname><forenames>Davide</forenames></author><author><keyname>Kovnatsky</keyname><forenames>Artiom</forenames></author><author><keyname>Bronstein</keyname><forenames>Michael M.</forenames></author></authors><title>Structure-preserving color transformations using Laplacian commutativity</title><categories>cs.CV cs.GR math.SP</categories><comments>11 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mappings between color spaces are ubiquitous in image processing problems
such as gamut mapping, decolorization, and image optimization for color-blind
people. Simple color transformations often result in information loss and
ambiguities (for example, when mapping from RGB to grayscale), and one wishes
to find an image-specific transformation that would preserve as much as
possible the structure of the original image in the target color space. In this
paper, we propose Laplacian colormaps, a generic framework for
structure-preserving color transformations between images. We use the image
Laplacian to capture the structural information, and show that if the color
transformation between two images preserves the structure, the respective
Laplacians have similar eigenvectors, or in other words, are approximately
jointly diagonalizable. Employing the relation between joint diagonalizability
and commutativity of matrices, we use Laplacians commutativity as a criterion
of color mapping quality and minimize it w.r.t. the parameters of a color
transformation to achieve optimal structure preservation. We show numerous
applications of our approach, including color-to-gray conversion, gamut
mapping, multispectral image fusion, and image optimization for color deficient
viewers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0121</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0121</id><created>2013-11-01</created><updated>2014-05-21</updated><authors><author><keyname>Song</keyname><forenames>Chao-Bing</forenames></author><author><keyname>Xia</keyname><forenames>Shu-Tao</forenames></author><author><keyname>Liu</keyname><forenames>Xin-Ji</forenames></author></authors><title>Subspace Thresholding Pursuit: A Reconstruction Algorithm for Compressed
  Sensing</title><categories>cs.IT math.IT</categories><comments>12 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new iterative greedy algorithm for reconstructions of sparse
signals with or without noisy perturbations in compressed sensing. The proposed
algorithm, called \emph{subspace thresholding pursuit} (STP) in this paper, is
a simple combination of subspace pursuit and iterative hard thresholding.
Firstly, STP has the theoretical guarantee comparable to that of $\ell_1$
minimization in terms of restricted isometry property. Secondly, with a tuned
parameter, on the one hand, when reconstructing Gaussian signals, it can
outperform other state-of-the-art reconstruction algorithms greatly; on the
other hand, when reconstructing constant amplitude signals with random signs,
it can outperform other state-of-the-art iterative greedy algorithms and even
outperform $\ell_1$ minimization if the undersampling ratio is not very large.
In addition, we propose a simple but effective method to improve the empirical
performance further if the undersampling ratio is large. Finally, it is showed
that other iterative greedy algorithms can improve their empirical performance
by borrowing the idea of STP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0124</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0124</id><created>2013-11-01</created><authors><author><keyname>Suksmono</keyname><forenames>Andriyan B.</forenames></author></authors><title>Reconstruction of Complex-Valued Fractional Brownian Motion Fields Based
  on Compressive Sampling and Its Application to PSF Interpolation in Weak
  Lensing Survey</title><categories>cs.CV astro-ph.CO</categories><comments>33 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new reconstruction method of complex-valued fractional Brownian motion
(CV-fBm) field based on Compressive Sampling (CS) is proposed. The decay
property of Fourier coefficients magnitude of the fBm signals/ fields indicates
that fBms are compressible. Therefore, a few numbers of samples will be
sufficient for a CS based method to reconstruct the full field. The
effectiveness of the proposed method is showed by simulating, random sampling,
and reconstructing CV-fBm fields. Performance evaluation shows advantages of
the proposed method over boxcar filtering and thin plate methods. It is also
found that the reconstruction performance depends on both of the fBm's Hurst
parameter and the number of samples, which in fact is consistent with the CS
reconstruction theory. In contrast to other fBm or fractal interpolation
methods, the proposed CS based method does not require the knowledge of fractal
parameters in the reconstruction process; the inherent sparsity is just
sufficient for the CS to do the reconstruction. Potential applicability of the
proposed method in weak gravitational lensing survey, particularly for
interpolating non-smooth PSF (Point Spread Function) distribution representing
distortion by a turbulent field is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0151</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0151</id><created>2013-11-01</created><updated>2014-02-04</updated><authors><author><keyname>Mishra</keyname><forenames>Dheerendra</forenames></author></authors><title>A Study On ID-based Authentication Schemes for Telecare Medical
  Information System</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The smart card based authentication schemes are designed and developed to
ensure secure and authorized communication between remote user and the server.
In recent times, many smart card based authentication schemes for the telecare
medical information systems (TMIS) have been presented. In this article, we
briefly discuss some of the recently published smart card based authentication
schemes for TMIS and try to show why efficient login and password change phases
are required. In other word, the study demonstrates how inefficient password
change phase leads to denial of server attack and how inefficient login phase
increase the communication and computation overhead and decrease the
performance of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0156</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0156</id><created>2013-11-01</created><updated>2014-04-15</updated><authors><author><keyname>Zeng</keyname><forenames>Jinshan</forenames></author><author><keyname>Lin</keyname><forenames>Shaobo</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author></authors><title>$L_{1/2}$ Regularization: Convergence of Iterative Half Thresholding
  Algorithm</title><categories>cs.NA</categories><comments>12 pages, 5 figures</comments><doi>10.1109/TSP.2014.2309076</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In recent studies on sparse modeling, the nonconvex regularization approaches
(particularly, $L_{q}$ regularization with $q\in(0,1)$) have been demonstrated
to possess capability of gaining much benefit in sparsity-inducing and
efficiency. As compared with the convex regularization approaches (say, $L_{1}$
regularization), however, the convergence issue of the corresponding algorithms
are more difficult to tackle. In this paper, we deal with this difficult issue
for a specific but typical nonconvex regularization scheme, the $L_{1/2}$
regularization, which has been successfully used to many applications. More
specifically, we study the convergence of the iterative \textit{half}
thresholding algorithm (the \textit{half} algorithm for short), one of the most
efficient and important algorithms for solution to the $L_{1/2}$
regularization. As the main result, we show that under certain conditions, the
\textit{half} algorithm converges to a local minimizer of the $L_{1/2}$
regularization, with an eventually linear convergence rate. The established
result provides a theoretical guarantee for a wide range of applications of the
\textit{half} algorithm. We provide also a set of simulations to support the
correctness of theoretical assertions and compare the time efficiency of the
\textit{half} algorithm with other known typical algorithms for $L_{1/2}$
regularization like the iteratively reweighted least squares (IRLS) algorithm
and the iteratively reweighted $l_{1}$ minimization (IRL1) algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0162</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0162</id><created>2013-11-01</created><authors><author><keyname>D'Hondt</keyname><forenames>Olivier</forenames></author><author><keyname>Guillaso</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Hellwich</keyname><forenames>Olaf</forenames></author></authors><title>Iterative Bilateral Filtering of Polarimetric SAR Data</title><categories>cs.CV</categories><comments>Available:
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6509975</comments><journal-ref>Selected Topics in Applied Earth Observations and Remote Sensing,
  IEEE Journal of (Volume:6, Issue: 3 ) 2013</journal-ref><doi>10.1109/JSTARS.2013.2256881</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce an iterative speckle filtering method for
polarimetric SAR (PolSAR) images based on the bilateral filter. To locally
adapt to the spatial structure of images, this filter relies on pixel
similarities in both spatial and radiometric domains. To deal with polarimetric
data, we study the use of similarities based on a statistical distance called
Kullback-Leibler divergence as well as two geodesic distances on Riemannian
manifolds. To cope with speckle, we propose to progressively refine the result
thanks to an iterative scheme. Experiments are run over synthetic and
experimental data. First, simulations are generated to study the effects of
filtering parameters in terms of polarimetric reconstruction error, edge
preservation and smoothing of homogeneous areas. Comparison with other methods
shows that our approach compares well to other state of the art methods in the
extraction of polarimetric information and shows superior performance for edge
restoration and noise smoothing. The filter is then applied to experimental
data sets from ESAR and FSAR sensors (DLR) at L-band and S-band, respectively.
These last experiments show the ability of the filter to restore structures
such as buildings and roads and to preserve boundaries between regions while
achieving a high amount of smoothing in homogeneous areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0170</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0170</id><created>2013-11-01</created><authors><author><keyname>Mittal</keyname><forenames>Sparsh</forenames></author></authors><title>A Technique for Efficiently Managing SRAM-NVM Hybrid Cache</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a SRAM-PCM hybrid cache design, along with a cache
replacement policy, named dead fast block (DFB) to manage the hybrid cache.
This design aims to leverage the best features of both SRAM and PCM devices.
Compared to a PCM-only cache, the hybrid cache with DFB policy provides
superior results on all relevant evaluation metrics, viz. cache lifetime,
performance and energy efficiency. Also, use of DFB policy for managing the
hybrid cache provides better results compared to LRU replacement policy on all
the evaluation metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0172</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0172</id><created>2013-11-01</created><authors><author><keyname>Holenstein</keyname><forenames>Thomas</forenames></author></authors><title>The PFR Conjecture Holds for Two Opposing Special Cases</title><categories>cs.DM math.CO</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $A \subseteq F_2^n$ be a set with $|2A| = K|A|$. We prove that if (1) for
at least a fraction $1-K^{-9}$ of all $s \in 2A$, the set $(A+s) \cap A$ has
size at most $L\cdot|A|/K$, or (2) for at least a fraction $K^{-L}$ of all $s
\in 2A$, the set $(A+s) \cap A$ has size at least $|A|\cdot(1- K^{-1/L})$, then
there is a subset $B \subseteq A$ of size $|A|/K^{O_L(1)}$ such that
$\mathrm{span}(B) \leq K^{O_L(1)}\cdot|A|$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0181</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0181</id><created>2013-11-01</created><updated>2015-05-26</updated><authors><author><keyname>Moulin</keyname><forenames>Pierre</forenames></author></authors><title>The Log-Volume of Optimal Codes for Memoryless Channels, Asymptotically
  Within A Few Nats</title><categories>cs.IT math.IT</categories><comments>73 pages, submitted for publication. Compared with the first version,
  this one no longer includes abstract alphabets and cost constraints. The
  capacity-achieving distribution is no longer required to be unique, and tight
  coding bounds are given for the binary symmetric channel</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shannon's analysis of the fundamental capacity limits for memoryless
communication channels has been refined over time. In this paper, the maximum
volume $M_\avg^*(n,\epsilon)$ of length-$n$ codes subject to an average
decoding error probability $\epsilon$ is shown to satisfy the following tight
asymptotic lower and upper bounds as $n \to \infty$: \[ \underline{A}_\epsilon
+ o(1) \le \log M_\avg^*(n,\epsilon) - [nC - \sqrt{nV_\epsilon}
\,Q^{-1}(\epsilon) + \frac{1}{2} \log n] \le \overline{A}_\epsilon + o(1) \]
where $C$ is the Shannon capacity, $V_\epsilon$ the $\epsilon$-channel
dispersion, or second-order coding rate, $Q$ the tail probability of the normal
distribution, and the constants $\underline{A}_\epsilon$ and
$\overline{A}_\epsilon$ are explicitly identified. This expression holds under
mild regularity assumptions on the channel, including nonsingularity. The gap
$\overline{A}_\epsilon - \underline{A}_\epsilon$ is one nat for weakly
symmetric channels in the Cover-Thomas sense, and typically a few nats for
other symmetric channels, for the binary symmetric channel, and for the $Z$
channel. The derivation is based on strong large-deviations analysis and
refined central limit asymptotics. A random coding scheme that achieves the
lower bound is presented. The codewords are drawn from a capacity-achieving
input distribution modified by an $O(1/\sqrt{n})$ correction term.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0195</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0195</id><created>2013-11-01</created><updated>2014-10-15</updated><authors><author><keyname>Bunte</keyname><forenames>Christoph</forenames></author><author><keyname>Lapidoth</keyname><forenames>Amos</forenames></author></authors><title>On the Listsize Capacity with Feedback</title><categories>cs.IT math.IT</categories><comments>17 pages. Fixed some typos; minor changes in the presentation.
  Published in the IEEE Transactions on Information Theory. Presented in part
  at the 2013 IEEE Information Theory Workshop</comments><doi>10.1109/TIT.2014.2355815</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The listsize capacity of a discrete memoryless channel is the largest
transmission rate for which the expectation---or, more generally, the $\rho$-th
moment---of the number of messages that could have produced the output of the
channel approaches one as the blocklength tends to infinity. We show that for
channels with feedback this rate is upper-bounded by the maximum of Gallager's
$E_0$ function divided by $\rho$, and that equality holds when the zero-error
capacity of the channel is positive. To establish this inequality we prove that
feedback does not increase the cutoff rate. Relationships to other notions of
channel capacity are explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0198</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0198</id><created>2013-11-01</created><authors><author><keyname>Zhao</keyname><forenames>Dengji</forenames></author><author><keyname>Zhang</keyname><forenames>Dongmo</forenames></author><author><keyname>Perrussel</keyname><forenames>Laurent</forenames></author></authors><title>Decomposing Truthful and Competitive Online Double Auctions</title><categories>cs.GT</categories><comments>19 pages, 5 figures, unpublished</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study online double auctions, where multiple sellers and
multiple buyers arrive and depart dynamically to exchange one commodity. We
show that there is no deterministic online double auction that is truthful and
competitive for maximising social welfare in an adversarial model. However,
given the prior information that sellers are patient and the demand is not more
than the supply, a deterministic and truthful greedy mechanism is actually
2-competitive, i.e. it guarantees that the social welfare of its allocation is
at least half of the optimal one achievable offline. Moreover, if the number of
incoming buyers is predictable, we demonstrate that an online double auction
can be reduced to an online one-sided auction, and the truthfulness and
competitiveness of the reduced online double auction follow that of the online
one-sided auction. Notably, by using the reduction, we find a truthful
mechanism that is almost 1-competitive, when buyers arrive randomly. Finally,
we argue that these mechanisms also have a promising applicability in more
general settings without assuming that sellers are patient, by decomposing a
market into multiple sub-markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0202</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0202</id><created>2013-10-16</created><authors><author><keyname>Amancio</keyname><forenames>D. R.</forenames></author><author><keyname>Comin</keyname><forenames>C. H.</forenames></author><author><keyname>Casanova</keyname><forenames>D.</forenames></author><author><keyname>Travieso</keyname><forenames>G.</forenames></author><author><keyname>Bruno</keyname><forenames>O. M.</forenames></author><author><keyname>Rodrigues</keyname><forenames>F. A.</forenames></author><author><keyname>Costa</keyname><forenames>L. da F.</forenames></author></authors><title>A systematic comparison of supervised classifiers</title><categories>cs.LG</categories><journal-ref>PLoS ONE 9 (4): e94137, 2014</journal-ref><doi>10.1371/journal.pone.0094137</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pattern recognition techniques have been employed in a myriad of industrial,
medical, commercial and academic applications. To tackle such a diversity of
data, many techniques have been devised. However, despite the long tradition of
pattern recognition research, there is no technique that yields the best
classification in all scenarios. Therefore, the consideration of as many as
possible techniques presents itself as an fundamental practice in applications
aiming at high accuracy. Typical works comparing methods either emphasize the
performance of a given algorithm in validation tests or systematically compare
various algorithms, assuming that the practical use of these methods is done by
experts. In many occasions, however, researchers have to deal with their
practical classification tasks without an in-depth knowledge about the
underlying mechanisms behind parameters. Actually, the adequate choice of
classifiers and parameters alike in such practical circumstances constitutes a
long-standing problem and is the subject of the current paper. We carried out a
study on the performance of nine well-known classifiers implemented by the Weka
framework and compared the dependence of the accuracy with their configuration
parameter configurations. The analysis of performance with default parameters
revealed that the k-nearest neighbors method exceeds by a large margin the
other methods when high dimensional datasets are considered. When other
configuration of parameters were allowed, we found that it is possible to
improve the quality of SVM in more than 20% even if parameters are set
randomly. Taken together, the investigation conducted in this paper suggests
that, apart from the SVM implementation, Weka's default configuration of
parameters provides an performance close the one achieved with the optimal
configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0222</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0222</id><created>2013-11-01</created><updated>2013-11-05</updated><authors><author><keyname>Audiffren</keyname><forenames>Julien</forenames><affiliation>LIF</affiliation></author><author><keyname>Kadri</keyname><forenames>Hachem</forenames><affiliation>LIF</affiliation></author></authors><title>Online Learning with Multiple Operator-valued Kernels</title><categories>cs.LG stat.ML</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning a vector-valued function f in an online
learning setting. The function f is assumed to lie in a reproducing Hilbert
space of operator-valued kernels. We describe two online algorithms for
learning f while taking into account the output structure. A first contribution
is an algorithm, ONORMA, that extends the standard kernel-based online learning
algorithm NORMA from scalar-valued to operator-valued setting. We report a
cumulative error bound that holds both for classification and regression. We
then define a second algorithm, MONORMA, which addresses the limitation of
pre-defining the output structure in ONORMA by learning sequentially a linear
combination of operator-valued kernels. Our experiments show that the proposed
algorithms achieve good performance results with low computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0224</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0224</id><created>2013-11-01</created><updated>2014-03-12</updated><authors><author><keyname>Oum</keyname><forenames>Sang-il</forenames></author><author><keyname>S&#xe6;ther</keyname><forenames>Sigve Hortemo</forenames></author><author><keyname>Vatshelle</keyname><forenames>Martin</forenames></author></authors><title>Faster Algorithms For Vertex Partitioning Problems Parameterized by
  Clique-width</title><categories>cs.DM cs.DS math.CO</categories><comments>13 pages, 5 figures</comments><journal-ref>Theoret. Comput. Sci. 535(May 2014), pp. 16-24</journal-ref><doi>10.1016/j.tcs.2014.03.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many NP-hard problems, such as Dominating Set, are FPT parameterized by
clique-width. For graphs of clique-width $k$ given with a $k$-expression,
Dominating Set can be solved in $4^k n^{O(1)}$ time. However, no FPT algorithm
is known for computing an optimal $k$-expression. For a graph of clique-width
$k$, if we rely on known algorithms to compute a $(2^{3k}-1)$-expression via
rank-width and then solving Dominating Set using the $(2^{3k}-1)$-expression,
the above algorithm will only give a runtime of $4^{2^{3k}} n^{O(1)}$. There
have been results which overcome this exponential jump; the best known
algorithm can solve Dominating Set in time $2^{O(k^2)} n^{O(1)}$ by avoiding
constructing a $k$-expression [Bui-Xuan, Telle, and Vatshelle. Fast dynamic
programming for locally checkable vertex subset and vertex partitioning
problems. Theoret. Comput. Sci., 2013. doi:10.1016/j.tcs.2013.01.009]. We
improve this to $2^{O(k\log k)}n^{O(1)}$. Indeed, we show that for a graph of
clique-width $k$, a large class of domination and partitioning problems
(LC-VSP), including Dominating Set, can be solved in $2^{O(k\log{k})}
n^{O(1)}$. Our main tool is a variant of rank-width using the rank of a $0$-$1$
matrix over the rational field instead of the binary field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0228</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0228</id><created>2013-11-01</created><authors><author><keyname>Abrahamsson</keyname><forenames>Pekka</forenames></author><author><keyname>Kautz</keyname><forenames>Karlheinz</forenames></author><author><keyname>Sieppi</keyname><forenames>Heikki</forenames></author><author><keyname>Lappalainen</keyname><forenames>Jouni</forenames></author></authors><title>Improving Software Developer's Competence: Is the Personal Software
  Process Working?</title><categories>cs.SE</categories><comments>Citation to the paper: Abrahamsson, P., Kautz, K., Sieppi, H., &amp;
  Lappalainen, J. (2002). Improving Software Developer's Competence: Is the
  Personal Software Process Working?. In Proceedings of the 4th International
  Conference on Product Focused Software Process Improvement (PROFES 2002),
  Workshop on empirical studies in software engineering, Rovaniemi, Finland</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging agile software development methods are people oriented development
approaches to be used by the software industry. The personal software process
(PSP) is an accepted method for improving the capabilities of a single software
engineer. Five original hypotheses regarding the impact of the PSP to
individual performance are tested. Data is obtained from 58 computer science
students in three university courses on the master level, which were held in
two different educational institutions in Finland and Denmark. Statistical data
treatment shows that the use of PSP did not improve size and time estimation
skills but that the productivity did not decrease and the resulting product
quality was improved. The implications of these findings are briefly addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0243</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0243</id><created>2013-11-01</created><updated>2013-11-15</updated><authors><author><keyname>Eyal</keyname><forenames>Ittay</forenames></author><author><keyname>Sirer</keyname><forenames>Emin Gun</forenames></author></authors><title>Majority is not Enough: Bitcoin Mining is Vulnerable</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bitcoin cryptocurrency records its transactions in a public log called
the blockchain. Its security rests critically on the distributed protocol that
maintains the blockchain, run by participants called miners. Conventional
wisdom asserts that the protocol is incentive-compatible and secure against
colluding minority groups, i.e., it incentivizes miners to follow the protocol
as prescribed.
  We show that the Bitcoin protocol is not incentive-compatible. We present an
attack with which colluding miners obtain a revenue larger than their fair
share. This attack can have significant consequences for Bitcoin: Rational
miners will prefer to join the selfish miners, and the colluding group will
increase in size until it becomes a majority. At this point, the Bitcoin system
ceases to be a decentralized currency.
  Selfish mining is feasible for any group size of colluding miners. We propose
a practical modification to the Bitcoin protocol that protects against selfish
mining pools that command less than 1/4 of the resources. This threshold is
lower than the wrongly assumed 1/2 bound, but better than the current reality
where a group of any size can compromise the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0244</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0244</id><created>2013-11-01</created><authors><author><keyname>Aksaray</keyname><forenames>Derya</forenames></author><author><keyname>Yazicioglu</keyname><forenames>A. Yasin</forenames></author><author><keyname>Feron</keyname><forenames>Eric</forenames></author><author><keyname>Mavris</keyname><forenames>Dimitri N.</forenames></author></authors><title>A Message Passing Strategy for Decentralized Connectivity Maintenance in
  Agent Removal</title><categories>cs.SY cs.MA</categories><comments>9 pages, 9 figures</comments><doi>10.2514/1.G001230</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multi-agent system, agents coordinate to achieve global tasks through
local communications. Coordination usually requires sufficient information
flow, which is usually depicted by the connectivity of the communication
network. In a networked system, removal of some agents may cause a
disconnection. In order to maintain connectivity in agent removal, one can
design a robust network topology that tolerates a finite number of agent
losses, and/or develop a control strategy that recovers connectivity. This
paper proposes a decentralized control scheme based on a sequence of
replacements, each of which occurs between an agent and one of its immediate
neighbors. The replacements always end with an agent, whose relocation does not
cause a disconnection. We show that such an agent can be reached by a local
rule utilizing only some local information available in agents' immediate
neighborhoods. As such, the proposed message passing strategy guarantees the
connectivity maintenance in arbitrary agent removal. Furthermore, we
significantly improve the optimality of the proposed scheme by incorporating
$\delta$-criticality (i.e. the criticality of an agent in its
$\delta$-neighborhood).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0251</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0251</id><created>2013-10-29</created><updated>2014-11-03</updated><authors><author><keyname>Mao</keyname><forenames>Andrew</forenames></author><author><keyname>Soufiani</keyname><forenames>Hossein Azari</forenames></author><author><keyname>Chen</keyname><forenames>Yiling</forenames></author><author><keyname>Parkes</keyname><forenames>David C.</forenames></author></authors><title>Capturing Variation and Uncertainty in Human Judgment</title><categories>cs.IR cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The well-studied problem of statistical rank aggregation has been applied to
comparing sports teams, information retrieval, and most recently to data
generated by human judgment. Such human-generated rankings may be substantially
different from traditional statistical ranking data. In this work, we show that
a recently proposed generalized random utility model reveals distinctive
patterns in human judgment across three different domains, and provides a
succinct representation of variance in both population preferences and
imperfect perception. In contrast, we also show that classical statistical
ranking models fail to capture important features from human-generated input.
Our work motivates the use of more flexible ranking models for representing and
describing the collective preferences or decision-making of human participants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0257</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0257</id><created>2013-11-01</created><authors><author><keyname>Adams</keyname><forenames>Michael D.</forenames></author><author><keyname>Hitefield</keyname><forenames>Seth D.</forenames></author><author><keyname>Hoy</keyname><forenames>Bruce</forenames></author><author><keyname>Fowler</keyname><forenames>Michael C.</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>Application of Cybernetics and Control Theory for a New Paradigm in
  Cybersecurity</title><categories>cs.CR</categories><comments>12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A significant limitation of current cyber security research and techniques is
its reactive and applied nature. This leads to a continuous 'cyber cycle' of
attackers scanning networks, developing exploits and attacking systems, with
defenders detecting attacks, analyzing exploits and patching systems. This
reactive nature leaves sensitive systems highly vulnerable to attack due to
un-patched systems and undetected exploits. Some current research attempts to
address this major limitation by introducing systems that implement moving
target defense. However, these ideas are typically based on the intuition that
a moving target defense will make it much harder for attackers to find and scan
vulnerable systems, and not on theoretical mathematical foundations. The
continuing lack of fundamental science and principles for developing more
secure systems has drawn increased interest into establishing a 'science of
cyber security'. This paper introduces the concept of using cybernetics, an
interdisciplinary approach of control theory, systems theory, information
theory and game theory applied to regulatory systems, as a foundational
approach for developing cyber security principles. It explores potential
applications of cybernetics to cyber security from a defensive perspective,
while suggesting the potential use for offensive applications. Additionally,
this paper introduces the fundamental principles for building non-stationary
systems, which is a more general solution than moving target defenses. Lastly,
the paper discusses related works concerning the limitations of moving target
defense and one implementation based on non-stationary principles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0258</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0258</id><created>2013-11-01</created><authors><author><keyname>McCoy</keyname><forenames>Michael B.</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author><author><keyname>Dinh</keyname><forenames>Quoc Tran</forenames></author><author><keyname>Asaei</keyname><forenames>Afsaneh</forenames></author><author><keyname>Baldassarre</keyname><forenames>Luca</forenames></author></authors><title>Convexity in source separation: Models, geometry, and algorithms</title><categories>cs.IT math.IT</categories><msc-class>Primary: 94A12, 90C25, Secondary: 94A08, 90C22</msc-class><doi>10.1109/MSP.2013.2296605</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Source separation or demixing is the process of extracting multiple
components entangled within a signal. Contemporary signal processing presents a
host of difficult source separation problems, from interference cancellation to
background subtraction, blind deconvolution, and even dictionary learning.
Despite the recent progress in each of these applications, advances in
high-throughput sensor technology place demixing algorithms under pressure to
accommodate extremely high-dimensional signals, separate an ever larger number
of sources, and cope with more sophisticated signal and mixing models. These
difficulties are exacerbated by the need for real-time action in automated
decision-making systems.
  Recent advances in convex optimization provide a simple framework for
efficiently solving numerous difficult demixing problems. This article provides
an overview of the emerging field, explains the theory that governs the
underlying procedures, and surveys algorithms that solve them efficiently. We
aim to equip practitioners with a toolkit for constructing their own demixing
algorithms that work, as well as concrete intuition for why they work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0262</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0262</id><created>2013-10-30</created><authors><author><keyname>Zhang</keyname><forenames>Suofei</forenames></author><author><keyname>Sun</keyname><forenames>Zhixin</forenames></author><author><keyname>Cheng</keyname><forenames>Xu</forenames></author><author><keyname>Wu</keyname><forenames>Zhenyang</forenames></author></authors><title>Tracking Deformable Parts via Dynamic Conditional Random Fields</title><categories>cs.CV cs.MM</categories><comments>4 pages, 5 figures, the manuscript has been submitted to IEEE Signal
  Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the success of many advanced tracking methods in this area, tracking
targets with drastic variation of appearance such as deformation, view change
and partial occlusion in video sequences is still a challenge in practical
applications. In this letter, we take these serious tracking problems into
account simultaneously, proposing a dynamic graph based model to track object
and its deformable parts at multiple resolutions. The method introduces well
learned structural object detection models into object tracking applications as
prior knowledge to deal with deformation and view change. Meanwhile, it
explicitly formulates partial occlusion by integrating spatial potentials and
temporal potentials with an unparameterized occlusion handling mechanism in the
dynamic conditional random field framework. Empirical results demonstrate that
the method outperforms state-of-the-art trackers on different challenging video
sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0269</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0269</id><created>2013-11-01</created><updated>2014-01-22</updated><authors><author><keyname>Abdurachmanov</keyname><forenames>David</forenames></author><author><keyname>Elmer</keyname><forenames>Peter</forenames></author><author><keyname>Eulisse</keyname><forenames>Giulio</forenames></author><author><keyname>Muzaffar</keyname><forenames>Shahzad</forenames></author></authors><title>Initial explorations of ARM processors for scientific computing</title><categories>physics.comp-ph cs.DC cs.NA hep-ex</categories><comments>Submitted to proceedings of the 15th International Workshop on
  Advanced Computing and Analysis Techniques in Physics Research (ACAT2013),
  Beijing. arXiv admin note: text overlap with arXiv:1311.1001</comments><doi>10.1088/1742-6596/523/1/012009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power efficiency is becoming an ever more important metric for both high
performance and high throughput computing. Over the course of next decade it is
expected that flops/watt will be a major driver for the evolution of computer
architecture. Servers with large numbers of ARM processors, already ubiquitous
in mobile computing, are a promising alternative to traditional x86-64
computing. We present the results of our initial investigations into the use of
ARM processors for scientific computing applications. In particular we report
the results from our work with a current generation ARMv7 development board to
explore ARM-specific issues regarding the software development environment,
operating system, performance benchmarks and issues for porting High Energy
Physics software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0272</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0272</id><created>2013-11-01</created><updated>2014-01-22</updated><authors><author><keyname>Arya</keyname><forenames>Kapil</forenames></author><author><keyname>Cooperman</keyname><forenames>Gene</forenames></author><author><keyname>Dotti</keyname><forenames>Andrea</forenames></author><author><keyname>Elmer</keyname><forenames>Peter</forenames></author></authors><title>Use of checkpoint-restart for complex HEP software on traditional
  architectures and Intel MIC</title><categories>physics.comp-ph cs.DC cs.NA hep-ex</categories><comments>Submitted to proceedings of the 15th International Workshop on
  Advanced Computing and Analysis Techniques in Physics Research (ACAT2013),
  Beijing</comments><doi>10.1088/1742-6596/523/1/012015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Process checkpoint-restart is a technology with great potential for use in
HEP workflows. Use cases include debugging, reducing the startup time of
applications both in offline batch jobs and the High Level Trigger, permitting
job preemption in environments where spare CPU cycles are being used
opportunistically and efficient scheduling of a mix of multicore and
single-threaded jobs. We report on tests of checkpoint-restart technology using
CMS software, Geant4-MT (multi-threaded Geant4), and the DMTCP (Distributed
Multithreaded Checkpointing) package. We analyze both single- and
multi-threaded applications and test on both standard Intel x86 architectures
and on Intel MIC. The tests with multi-threaded applications on Intel MIC are
used to consider scalability and performance. These are considered an indicator
of what the future may hold for many-core computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0274</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0274</id><created>2013-11-01</created><authors><author><keyname>Javanmard</keyname><forenames>Adel</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>Nearly Optimal Sample Size in Hypothesis Testing for High-Dimensional
  Regression</title><categories>math.ST cs.IT cs.LG math.IT stat.ME stat.TH</categories><comments>21 pages, short version appears in Annual Allerton Conference on
  Communication, Control and Computing, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of fitting the parameters of a high-dimensional
linear regression model. In the regime where the number of parameters $p$ is
comparable to or exceeds the sample size $n$, a successful approach uses an
$\ell_1$-penalized least squares estimator, known as Lasso. Unfortunately,
unlike for linear estimators (e.g., ordinary least squares), no
well-established method exists to compute confidence intervals or p-values on
the basis of the Lasso estimator. Very recently, a line of work
\cite{javanmard2013hypothesis, confidenceJM, GBR-hypothesis} has addressed this
problem by constructing a debiased version of the Lasso estimator. In this
paper, we study this approach for random design model, under the assumption
that a good estimator exists for the precision matrix of the design. Our
analysis improves over the state of the art in that it establishes nearly
optimal \emph{average} testing power if the sample size $n$ asymptotically
dominates $s_0 (\log p)^2$, with $s_0$ being the sparsity level (number of
non-zero coefficients). Earlier work obtains provable guarantees only for much
larger sample size, namely it requires $n$ to asymptotically dominate $(s_0
\log p)^2$.
  In particular, for random designs with a sparse precision matrix we show that
an estimator thereof having the required properties can be computed
efficiently. Finally, we evaluate this approach on synthetic data and compare
it with earlier proposals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0293</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0293</id><created>2013-11-01</created><authors><author><keyname>Liu</keyname><forenames>David</forenames></author></authors><title>Pebbling Arguments for Tree Evaluation</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Tree Evaluation Problem was introduced by Cook et al. in 2010 as a
candidate for separating P from L and NL. The most general space lower bounds
known for the Tree Evaluation Problem require a semantic restriction on the
branching programs and use a connection to well-known pebble games to generate
a bottleneck argument. These bounds are met by corresponding upper bounds
generated by natural implementations of optimal pebbling algorithms. In this
paper we extend these ideas to a variety of restricted families of both
deterministic and non-deterministic branching programs, proving tight lower
bounds under these restricted models. We also survey and unify known lower
bounds in our &quot;pebbling argument&quot; framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0314</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0314</id><created>2013-11-01</created><updated>2014-03-31</updated><authors><author><keyname>Chen</keyname><forenames>Guangliang</forenames></author><author><keyname>Divekar</keyname><forenames>Atul</forenames></author><author><keyname>Needell</keyname><forenames>Deanna</forenames></author></authors><title>Guaranteed sparse signal recovery with highly coherent sensing matrices</title><categories>math.NA cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1302.3918</comments><msc-class>68U10, 94A08, 65T99, 65D99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing is a methodology for the reconstruction of sparse or
compressible signals using far fewer samples than required by the Nyquist
criterion. However, many of the results in compressive sensing concern random
sampling matrices such as Gaussian and Bernoulli matrices. In common physically
feasible signal acquisition and reconstruction scenarios such as
super-resolution of images, the sensing matrix has a non-random structure with
highly correlated columns. Here we present a compressive sensing type recovery
algorithm, called Partial Inversion (PartInv), that overcomes the correlations
among the columns. We provide theoretical justification as well as empirical
comparisons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0320</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0320</id><created>2013-11-01</created><updated>2013-12-04</updated><authors><author><keyname>Wang</keyname><forenames>Zhi Jie</forenames></author><author><keyname>Yao</keyname><forenames>Bin</forenames></author><author><keyname>Guo</keyname><forenames>Minyi</forenames></author></authors><title>Explicit and Implicit Constrained-Space Probabilistic Threshold Range
  Queries for Moving Objects</title><categories>cs.DB</categories><comments>12 pages</comments><acm-class>H.2.8; H.3.3; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the constrained-space probabilistic threshold range query
(CSPTRQ) for moving objects. We differentiate two kinds of CSPTRQs: implicit
and explicit ones. Specifically, for each moving object $o$, we assume $o$
cannot be located in some specific areas, we model its location as a closed
region, $u$, together with a probability density function, and model a query
range, $R$, as an arbitrary polygon. An implicit CSPTRQ can be reduced to a
search (over all the $u$) that returns a set of objects, which have
probabilities higher than a probability threshold $p_t$ to be located in $R$,
where $0\leq p_t\leq 1$. In contrast, an explicit CSPTRQ returns a set of
tuples in form of ($o$, $p$) such that $p\geq p_t$, where $p$ is the
probability of $o$ being located in $R$. A straightforward adaptation of
existing method is inefficient due to its weak pruning/validating capability.
In order to efficiently process such queries, we propose targeted solutions, in
which three main ideas are incorporated: (1) swapping the order of geometric
operations based on the computation duality; (2) pruning unrelated objects in
the early stages using the location unreachability; and (3) computing the
probability using the multi-step mechanism. Extensive experimental results
demonstrate the efficiency and effectiveness of the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0324</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0324</id><created>2013-11-01</created><authors><author><keyname>Ilic</keyname><forenames>Velimir M.</forenames></author><author><keyname>Stankovic</keyname><forenames>Miomir S.</forenames></author></authors><title>An axiomatic characterization of generalized entropies under analyticity
  condition</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the characterization of the Nath, R\'enyi and
Havrda-Charv\'at-Tsallis entropies under the assumption that they are analytic
function with respect to the distribution dimension, unlike the the previous
characterizations, which supposes that they are expandable maximized for
uniform distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0330</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0330</id><created>2013-11-01</created><authors><author><keyname>Becher</keyname><forenames>Ver&#xf3;nica</forenames></author><author><keyname>Grigorieff</keyname><forenames>Serge</forenames></author></authors><title>Borel and Hausdorff Hierarchies in Topological Spaces of Choquet Games
  and Their Effectivization</title><categories>cs.LO math.LO</categories><journal-ref>Math. Struct. in Comp. Science 25 (2014) 1490-1519</journal-ref><doi>10.1017/S096012951300025X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What parts of classical descriptive set theory done in Polish spaces still
hold for more general topological spaces, possibly T0 or T1, but not T2 (i.e.
not Hausdorff)? This question has been addressed by Victor Selivanov in a
series of papers centered on algebraic domains. And recently it has been
considered by Matthew de Brecht for quasi-Polish spaces, a framework that
contains both countably based continuous domains and Polish spaces. In this
paper we present alternative unifying topological spaces, that we call
approximation spaces. They are exactly the spaces for which player Nonempty has
a stationary strategy in the Choquet game. A natural proper subclass of
approximation spaces coincides with the class of quasi-Polish spaces. We study
the Borel and Hausdorff difference hierarchies in approximation spaces,
revisiting the work done for the other topological spaces. We also consider the
problem of effectivization of these results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0331</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0331</id><created>2013-11-01</created><authors><author><keyname>Becher</keyname><forenames>Ver&#xf3;nica</forenames></author><author><keyname>Grigorieff</keyname><forenames>Serge</forenames></author></authors><title>Wadge Hardness in Scott Spaces and Its Effectivization</title><categories>cs.LO math.LO</categories><journal-ref>Math. Struct. in Comp. Science 25 (2014) 1520-1545</journal-ref><doi>10.1017/S0960129513000248</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove some results on the Wadge order on the space of sets of natural
numbers endowed with Scott topology, and more generally, on omega-continuous
domains. Using alternating decreasing chains we characterize the property of
Wadge hardness for the classes of the Hausdorff difference hierarchy (iterated
differences of open sets). A similar characterization holds for Wadge
one-to-one and finite-to-one completeness. We consider the same questions for
the effectivization of the Wadge relation. We also show that for the space of
sets of natural numbers endowed with the Scott topology, in each class of the
Hausdorff difference hierarchy there are two strictly increasing chains of
Wadge degrees of sets properly in that class. The length of these chains is the
rank of the considered class, and each element in one chain is incomparable
with all the elements in the other chain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0335</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0335</id><created>2013-11-01</created><authors><author><keyname>Becher</keyname><forenames>Ver&#xf3;nica</forenames></author><author><keyname>Heiber</keyname><forenames>Pablo Ariel</forenames></author><author><keyname>Slaman</keyname><forenames>Theodore A.</forenames></author></authors><title>Normal Numbers and the Borel Hierarchy</title><categories>cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the set of absolutely normal numbers is $\mathbf
\Pi^0_3$-complete in the Borel hierarchy of subsets of real numbers. Similarly,
the set of absolutely normal numbers is $\Pi^0_3$-complete in the effective
Borel hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0339</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0339</id><created>2013-11-02</created><authors><author><keyname>Gupta</keyname><forenames>Sonali</forenames></author><author><keyname>Bhatia</keyname><forenames>Komal Kumar</forenames></author></authors><title>A Novel Term Weighing Scheme Towards Efficient Crawl of Textual
  Databases</title><categories>cs.IR</categories><comments>12 Pages. IJCEA, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hidden Web is the vast repository of informational databases available
only through search form interfaces, accessible by therein typing a set of
keywords in the search forms. Typically, a Hidden Web crawler is employed to
autonomously discover and download pages from the Hidden Web. Traditional
hidden web crawlers do not provide the search engines with an optimal search
experience because of the excessive number of search requests posed through the
form interface so as to exhaustively crawl and retrieve the contents of the
target hidden web database. Here in our work, we provide a framework to
investigate the problem of optimal search and curtail it by proposing an
effective query term selection approach based on the frequency &amp; distribution
of terms in the document database. The paper focuses on developing a
term-weighing scheme called VarDF (acronym for variable document frequency)
that can ease the identification of optimal terms to be used as queries on the
interface for maximizing the achieved coverage of the crawler which in turn
will facilitate the search engine to have a diversified and expanded index. We
experimentally evaluate the effectiveness of our approach on a manually created
database of documents in the area of Information Retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0347</identifier>
 <datestamp>2014-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0347</id><created>2013-11-02</created><updated>2014-06-24</updated><authors><author><keyname>Jedari</keyname><forenames>Behrouz</forenames></author><author><keyname>Xia</keyname><forenames>Feng</forenames></author></authors><title>A Survey on Routing and Data Dissemination in Opportunistic Mobile
  Social Networks</title><categories>cs.NI cs.SI</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in figures 4 and 5 25 pages, 12 figures, 12 tables. arXiv admin note:
  previous versions contained much plagiarized material</comments><doi>10.1109/SURV.2014.022714.00153</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opportunistic mobile social networks (MSNs) are modern paradigms of delay
tolerant networks that consist of mobile users with social characteristics. The
users in MSNs communicate with each other to share data objects. In this
setting, humans are the carriers of mobile devices, hence their social features
such as movement patterns, similarities, and interests can be exploited to
design efficient data forwarding algorithms. In this paper, an overview of
routing and data dissemination issues in the context of opportunistic MSNs is
presented, with focus on (1) MSN characteristics, (2) human mobility models,
(3) dynamic community detection methods, and (4) routing and data dissemination
protocols. Firstly, characteristics of MSNs which lead to the exposure of
patterns of interaction among mobile users are examined. Secondly, properties
of human mobility models are discussed and recently proposed mobility models
are surveyed. Thirdly, community detection and evolution analysis algorithms
are investigated. Then, a comparative review of state-of-the-art routing and
data dissemination algorithms for MSNs is presented, with special attention
paid to critical issues like context-awareness and user selfishness. Based on
the literature review, some important open issues are finally discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0350</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0350</id><created>2013-11-02</created><authors><author><keyname>Slimani</keyname><forenames>Thabet</forenames></author><author><keyname>Lazzez</keyname><forenames>Amor</forenames></author></authors><title>Sequential Mining: Patterns and Algorithms Analysis</title><categories>cs.DB</categories><comments>10 pages</comments><journal-ref>International Journal of Computer and Electronics Research, Volume
  2, Issue 5, October 2013, pp 639-647</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents and analysis the common existing sequential pattern
mining algorithms. It presents a classifying study of sequential pattern-mining
algorithms into five extensive classes. First, on the basis of Apriori-based
algorithm, second on Breadth First Search-based strategy, third on Depth First
Search strategy, fourth on sequential closed-pattern algorithm and five on the
basis of incremental pattern mining algorithms. At the end, a comparative
analysis is done on the basis of important key features supported by various
algorithms. This study gives an enhancement in the understanding of the
approaches of sequential pattern mining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0351</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0351</id><created>2013-11-02</created><updated>2013-11-04</updated><authors><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Zhao</keyname><forenames>Hong</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Rough matroids based on coverings</title><categories>cs.AI</categories><comments>15pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The introduction of covering-based rough sets has made a substantial
contribution to the classical rough sets. However, many vital problems in rough
sets, including attribution reduction, are NP-hard and therefore the algorithms
for solving them are usually greedy. Matroid, as a generalization of linear
independence in vector spaces, it has a variety of applications in many fields
such as algorithm design and combinatorial optimization. An excellent
introduction to the topic of rough matroids is due to Zhu and Wang. On the
basis of their work, we study the rough matroids based on coverings in this
paper. First, we investigate some properties of the definable sets with respect
to a covering. Specifically, it is interesting that the set of all definable
sets with respect to a covering, equipped with the binary relation of inclusion
$\subseteq$, constructs a lattice. Second, we propose the rough matroids based
on coverings, which are a generalization of the rough matroids based on
relations. Finally, some properties of rough matroids based on coverings are
explored. Moreover, an equivalent formulation of rough matroids based on
coverings is presented. These interesting and important results exhibit many
potential connections between rough sets and matroids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0352</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0352</id><created>2013-11-02</created><authors><author><keyname>Cabibihan</keyname><forenames>John-John</forenames></author><author><keyname>Javed</keyname><forenames>Hifza</forenames></author><author><keyname>Ang</keyname><forenames>Marcelo</forenames><suffix>Jr.</suffix></author><author><keyname>Aljunied</keyname><forenames>Sharifah Mariam</forenames></author></authors><title>Why robots? A survey on the roles and benefits of social robots in the
  therapy of children with autism</title><categories>cs.RO cs.CY cs.HC</categories><journal-ref>International Journal of Social Robotics, 2013, 5(4), 593-618</journal-ref><doi>10.1007/s12369-013-0202-2</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper reviews the use of socially interactive robots to assist in the
therapy of children with autism. The extent to which the robots were successful
in helping the children in their social, emotional, and communication deficits
was investigated. Child-robot interactions were scrutinized with respect to the
different target behaviors that are to be elicited from a child during therapy.
These behaviors were thoroughly examined with respect to a childs development
needs. Most importantly, experimental data from the surveyed works were
extracted and analyzed in terms of the target behaviors and how each robot was
used during a therapy session to achieve these behaviors. The study concludes
by categorizing the different therapeutic roles that these robots were observed
to play, and highlights the important design features that enable them to
achieve high levels of effectiveness in autism therapy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0355</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0355</id><created>2013-11-02</created><updated>2015-10-23</updated><authors><author><keyname>Hendrickx</keyname><forenames>Julien M.</forenames></author><author><keyname>Olshevsky</keyname><forenames>Alex</forenames></author></authors><title>On symmetric continuum opinion dynamics</title><categories>math.DS cs.MA cs.SY</categories><comments>28 pages, 3 figures, 4 files</comments><msc-class>93D20, 91C20, 93A14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the asymptotic behavior of some common opinion
dynamic models in a continuum of agents. We show that as long as the
interactions among the agents are symmetric, the distribution of the agents'
opinion converges. We also investigate whether convergence occurs in a stronger
sense than merely in distribution, namely, whether the opinion of almost every
agent converges. We show that while this is not the case in general, it becomes
true under plausible assumptions on inter-agent interactions, namely that
agents with similar opinions exert a non-negligible pull on each other, or that
the interactions are entirely determined by their opinions via a smooth
function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0358</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0358</id><created>2013-11-02</created><updated>2015-02-05</updated><authors><author><keyname>Chang</keyname><forenames>Hsien-Chih</forenames></author><author><keyname>Lu</keyname><forenames>Hsueh-I</forenames></author></authors><title>A Faster Algorithm to Recognize Even-Hole-Free Graphs</title><categories>cs.DS</categories><comments>18 pages, 7 figures, to appear in Journal of Combinatorial Theory,
  Series B. A preliminary version of this paper appeared in SODA 2012</comments><doi>10.1016/j.jctb.2015.02.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of determining whether an $n$-node graph $G$ has an even
hole, i.e., an induced simple cycle consisting of an even number of nodes.
Conforti, Cornu\'ejols, Kapoor, and Vu\v{s}kovi\'c gave the first
polynomial-time algorithm for the problem, which runs in $O(n^{40})$ time.
Later, Chudnovsky, Kawarabayashi, and Seymour reduced the running time to
$O(n^{31})$. The best previously known algorithm for the problem, due to da
Silva and Vu\v{s}kovi\'c, runs in $O(n^{19})$ time. In this paper, we solve the
problem in $O(n^{11})$ time. Moreover, if $G$ has even holes, our algorithm
also outputs an even hole of $G$ in $O(n^{11})$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0359</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0359</id><created>2013-11-02</created><authors><author><keyname>P&#x142;aczek</keyname><forenames>Bart&#x142;omiej</forenames></author><author><keyname>Berna&#x15b;</keyname><forenames>Marcin</forenames></author></authors><title>Optimizing data collection for object tracking in wireless sensor
  networks</title><categories>cs.NI</categories><comments>10 pages, 6 figures</comments><journal-ref>Communications in Computer and Information Science, vol. 370, pp.
  485-494 (2013)</journal-ref><doi>10.1007/978-3-642-38865-1_49</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper some modifications are proposed to optimize an algorithm of
object tracking in wireless sensor network (WSN). The task under consideration
is to control movement of a mobile sink, which has to reach a target in the
shortest possible time. Utilization of the WSN resources is optimized by
transferring only selected data readings (target locations) to the mobile sink.
Simulations were performed to evaluate the proposed modifications against
state-of-the-art methods. The obtained results show that the presented tracking
algorithm allows for substantial reduction of data collection costs with no
significant increase in the amount of time that it takes to catch the target.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0362</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0362</id><created>2013-11-02</created><authors><author><keyname>Samra</keyname><forenames>Sameh</forenames></author><author><keyname>El-Mahdy</keyname><forenames>Ahmed</forenames></author><author><keyname>Wada</keyname><forenames>Yasutaka</forenames></author></authors><title>A Linear-Time and Space Algorithm for Optimal Traffic Signal Durations
  at an Intersection</title><categories>cs.DS</categories><comments>New Dynamic programming Traffic Control Algorithm 5 pages, 5 figures</comments><doi>10.1109/TITS.2014.2336657</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding an optimal solution of signal traffic control durations is a
computationally intensive task. It is typically O(T3) in time, and O(T2) in
space, where T is the length of the control interval in discrete time steps. In
this paper, we propose a linear time and space algorithm for the same problem.
The algorithm provides for an efficient dynamic programming formulation of the
state space, the prunes non-optimal states, early on. The paper proves the
correctness of the algorithm and provides an initial experimental validation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0363</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0363</id><created>2013-11-02</created><updated>2014-01-21</updated><authors><author><keyname>David</keyname><forenames>Ren&#xe9;</forenames><affiliation>LAMA - Equipe LIMD - Universit&#xe9; de Savoie, Le Bourget du Lac</affiliation></author><author><keyname>Nour</keyname><forenames>Karim</forenames><affiliation>LAMA - Equipe LIMD - Universit&#xe9; de Savoie, Le Bourget du Lac</affiliation></author></authors><title>About the range property for H</title><categories>math.LO cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (January
  21, 2014) lmcs:849</journal-ref><doi>10.2168/LMCS-10(1:3)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, A. Polonsky has shown that the range property fails for H. We give
here some conditions on a closed term that imply that its range has an infinite
cardinality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0366</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0366</id><created>2013-11-02</created><authors><author><keyname>Haviv</keyname><forenames>Ishay</forenames></author><author><keyname>Regev</keyname><forenames>Oded</forenames></author></authors><title>On the Lattice Isomorphism Problem</title><categories>cs.DS cs.CC cs.DM math.CO</categories><comments>23 pages, SODA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Lattice Isomorphism Problem (LIP), in which given two lattices
L_1 and L_2 the goal is to decide whether there exists an orthogonal linear
transformation mapping L_1 to L_2. Our main result is an algorithm for this
problem running in time n^{O(n)} times a polynomial in the input size, where n
is the rank of the input lattices. A crucial component is a new generalized
isolation lemma, which can isolate n linearly independent vectors in a given
subset of Z^n and might be useful elsewhere. We also prove that LIP lies in the
complexity class SZK.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0376</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0376</id><created>2013-11-02</created><updated>2014-01-22</updated><authors><author><keyname>Chazal</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Fasy</keyname><forenames>Brittany Terese</forenames></author><author><keyname>Lecci</keyname><forenames>Fabrizio</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>On the Bootstrap for Persistence Diagrams and Landscapes</title><categories>math.AT cs.CG stat.AP</categories><journal-ref>Modeling and Analysis of Information Systems, 20(6), 96-105</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Persistent homology probes topological properties from point clouds and
functions. By looking at multiple scales simultaneously, one can record the
births and deaths of topological features as the scale varies. In this paper we
use a statistical technique, the empirical bootstrap, to separate topological
signal from topological noise. In particular, we derive confidence sets for
persistence diagrams and confidence bands for persistence landscapes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0378</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0378</id><created>2013-11-02</created><authors><author><keyname>Teodoro</keyname><forenames>George</forenames></author><author><keyname>Kurc</keyname><forenames>Tahsin</forenames></author><author><keyname>Kong</keyname><forenames>Jun</forenames></author><author><keyname>Cooper</keyname><forenames>Lee</forenames></author><author><keyname>Saltz</keyname><forenames>Joel</forenames></author></authors><title>Comparative Performance Analysis of Intel Xeon Phi, GPU, and CPU</title><categories>cs.DC cs.PF</categories><comments>11 pages, 2 figures</comments><acm-class>C.4; D.1.3; D.2.6</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We investigate and characterize the performance of an important class of
operations on GPUs and Many Integrated Core (MIC) architectures. Our work is
motivated by applications that analyze low-dimensional spatial datasets
captured by high resolution sensors, such as image datasets obtained from whole
slide tissue specimens using microscopy image scanners. We identify the data
access and computation patterns of operations in object segmentation and
feature computation categories. We systematically implement and evaluate the
performance of these core operations on modern CPUs, GPUs, and MIC systems for
a microscopy image analysis application. Our results show that (1) the data
access pattern and parallelization strategy employed by the operations strongly
affect their performance. While the performance on a MIC of operations that
perform regular data access is comparable or sometimes better than that on a
GPU; (2) GPUs are significantly more efficient than MICs for operations and
algorithms that irregularly access data. This is a result of the low
performance of the latter when it comes to random data access; (3) adequate
coordinated execution on MICs and CPUs using a performance aware task
scheduling strategy improves about 1.29x over a first-come-first-served
strategy. The example application attained an efficiency of 84% in an execution
with of 192 nodes (3072 CPU cores and 192 MICs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0380</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0380</id><created>2013-11-02</created><updated>2013-11-11</updated><authors><author><keyname>Amerio</keyname><forenames>S.</forenames></author><author><keyname>Bastieri</keyname><forenames>D.</forenames></author><author><keyname>Corvo</keyname><forenames>M.</forenames></author><author><keyname>Gianelle</keyname><forenames>A.</forenames></author><author><keyname>Ketchum</keyname><forenames>W.</forenames></author><author><keyname>Liu</keyname><forenames>T.</forenames></author><author><keyname>Lonardo</keyname><forenames>A.</forenames></author><author><keyname>Lucchesi</keyname><forenames>D.</forenames></author><author><keyname>Poprocki</keyname><forenames>S.</forenames></author><author><keyname>Rivera</keyname><forenames>R.</forenames></author><author><keyname>Tosoratto</keyname><forenames>L.</forenames></author><author><keyname>Vicini</keyname><forenames>P.</forenames></author><author><keyname>Wittich</keyname><forenames>P.</forenames></author></authors><title>Many-core applications to online track reconstruction in HEP experiments</title><categories>physics.ins-det cs.DC</categories><comments>Proceedings for the 20th International Conference on Computing in
  High Energy and Nuclear Physics (CHEP); missing acks added</comments><doi>10.1088/1742-6596/513/1/012002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interest in parallel architectures applied to real time selections is growing
in High Energy Physics (HEP) experiments. In this paper we describe performance
measurements of Graphic Processing Units (GPUs) and Intel Many Integrated Core
architecture (MIC) when applied to a typical HEP online task: the selection of
events based on the trajectories of charged particles. We use as benchmark a
scaled-up version of the algorithm used at CDF experiment at Tevatron for
online track reconstruction - the SVT algorithm - as a realistic test-case for
low-latency trigger systems using new computing architectures for LHC
experiment. We examine the complexity/performance trade-off in porting existing
serial algorithms to many-core devices. Measurements of both data processing
and data transfer latency are shown, considering different I/O strategies
to/from the parallel devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0388</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0388</id><created>2013-11-02</created><authors><author><keyname>Bhattacharjee</keyname><forenames>Tapomayukh</forenames></author><author><keyname>Oh</keyname><forenames>Yonghwan</forenames></author><author><keyname>Oh</keyname><forenames>Sang-Rok</forenames></author></authors><title>Non-linear Task-Space Disturbance Observer for Position Regulation of
  Redundant Robot Arms against Perturbations in 3D Environments</title><categories>cs.RO</categories><comments>This paper summarizes our work on the formulation of a Non-linear
  Task-Space Disturbance Observer for Redundant Robot Arms. This work was done
  at the Interaction and Robotics Research Center in Korea Institute of Science
  and Technology (KIST), South-Korea during 2010-2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many day-to-day activities require the dexterous manipulation of a redundant
humanoid arm in complex 3D environments. However, position regulation of such
robot arm systems becomes very difficult in presence of non-linear
uncertainties in the system. Also, perturbations exist due to various unwanted
interactions with obstacles for clumsy environments in which obstacle avoidance
is not possible, and this makes position regulation even more difficult. This
report proposes a non-linear task-space disturbance observer by virtue of which
position regulation of such robotic systems can be achieved in spite of such
perturbations and uncertainties. Simulations are conducted using a 7-DOF
redundant robot arm system to show the effectiveness of the proposed method.
These results are then compared with the case of a conventional mass-damper
based task-space disturbance observer to show the enhancement in performance
using the developed concept. This proposed method is then applied to a
controller which exhibits human-like motion characteristics for reaching a
target. Arbitrary perturbations in the form of interactions with obstacles are
introduced in its path. Results show that the robot end-effector successfully
continues to move in its path of a human-like quasi-straight trajectory even if
the joint trajectories deviated by a considerable amount due to the
perturbations. These results are also compared with that of the unperturbed
motion of the robot which further prove the significance of the developed
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0390</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0390</id><created>2013-11-02</created><authors><author><keyname>Scott</keyname><forenames>Michael James</forenames></author><author><keyname>Ghinea</keyname><forenames>Gheorghita</forenames></author></authors><title>Educating Programmers: A Reflection on Barriers to Deliberate Practice</title><categories>cs.CY</categories><comments>Paper Presented at 2nd Annual HEA STEM Conference (Birmingham, UK,
  Apr. 17--18, '13), 6 Pages, 1 Table</comments><journal-ref>(2013) Proceedings of the 2nd Annual HEA STEM Conference, 028P</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programming is a craft which often demands that learners engage in a
significantly high level of individual practice and experimentation in order to
acquire basic competencies. However, practice behaviours can be undermined
during the early stages of instruction. This is often the result of seemingly
trivial misconceptions that, when left unchecked, create cognitive-affective
barriers. These interact with learners' self-beliefs, potentially inducing
affective states that inhibit practice. This paper questions how to design a
learning environment that can address this issue. It is proposed that
analytical and adaptable approaches, which could include soft scaffolding,
ongoing detailed informative feedback and a focus on self-enhancement alongside
skill development, can help overcome such barriers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0391</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0391</id><created>2013-11-02</created><authors><author><keyname>Zhang</keyname><forenames>Peng</forenames></author><author><keyname>Gan</keyname><forenames>Lu</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author><author><keyname>Ling</keyname><forenames>Cong</forenames></author></authors><title>Deterministic Sequences for Compressive MIMO Channel Estimation</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures, EUSIPCO 2013, accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of pilot design for compressive
multiple-input multiple-output (MIMO) channel estimation. In particular, we are
interested in estimating the channels for multiple transmitters simultaneously
when the pilot sequences are shorter than the combined channels. Existing works
on this topic demonstrated that tools from compressed sensing theory can yield
accurate multichannel estimation provided that each pilot sequence is randomly
generated. Here, we propose constructing the pilot sequence for each
transmitter from a small set of deterministic sequences. We derive a
theoretical lower bound on the length of the pilot sequences that guarantees
the multichannel estimation with high probability. Simulation results are
provided to demonstrate the performance of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0396</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0396</id><created>2013-11-02</created><authors><author><keyname>Luo</keyname><forenames>Biao</forenames></author><author><keyname>Wu</keyname><forenames>Huai-Ning</forenames></author><author><keyname>Huang</keyname><forenames>Tingwen</forenames></author><author><keyname>Liu</keyname><forenames>Derong</forenames></author></authors><title>Data-based approximate policy iteration for nonlinear continuous-time
  optimal control design</title><categories>cs.SY math.OC stat.ML</categories><comments>22 pages, 21 figures, submitted for Peer Review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the model-free nonlinear optimal problem with
generalized cost functional, and a data-based reinforcement learning technique
is developed. It is known that the nonlinear optimal control problem relies on
the solution of the Hamilton-Jacobi-Bellman (HJB) equation, which is a
nonlinear partial differential equation that is generally impossible to be
solved analytically. Even worse, most of practical systems are too complicated
to establish their accurate mathematical model. To overcome these difficulties,
we propose a data-based approximate policy iteration (API) method by using real
system data rather than system model. Firstly, a model-free policy iteration
algorithm is derived for constrained optimal control problem and its
convergence is proved, which can learn the solution of HJB equation and optimal
control policy without requiring any knowledge of system mathematical model.
The implementation of the algorithm is based on the thought of actor-critic
structure, where actor and critic neural networks (NNs) are employed to
approximate the control policy and cost function, respectively. To update the
weights of actor and critic NNs, a least-square approach is developed based on
the method of weighted residuals. The whole data-based API method includes two
parts, where the first part is implemented online to collect real system
information, and the second part is conducting offline policy iteration to
learn the solution of HJB equation and the control policy. Then, the data-based
API algorithm is simplified for solving unconstrained optimal control problem
of nonlinear and linear systems. Finally, we test the efficiency of the
data-based API control design method on a simple nonlinear system, and further
apply it to a rotational/translational actuator system. The simulation results
demonstrate the effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0402</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0402</id><created>2013-11-02</created><authors><author><keyname>Tang</keyname><forenames>Yu-Hang</forenames></author><author><keyname>Karniadakis</keyname><forenames>George Em</forenames></author></authors><title>Accelerating Dissipative Particle Dynamics Simulations on GPUs:
  Algorithms, Numerics and Applications</title><categories>cs.DC physics.comp-ph</categories><journal-ref>Computer Physics Communications, 2014, 185(11), 2809 - 2822</journal-ref><doi>10.1016/j.cpc.2014.06.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a scalable dissipative particle dynamics simulation code, fully
implemented on the Graphics Processing Units (GPUs) using a hybrid CUDA/MPI
programming model, which achieves 10-30 times speedup on a single GPU over 16
CPU cores and almost linear weak scaling across a thousand nodes. A unified
framework is developed within which the efficient generation of the neighbor
list and maintaining particle data locality are addressed. Our algorithm
generates strictly ordered neighbor lists in parallel, while the construction
is deterministic and makes no use of atomic operations or sorting. Such
neighbor list leads to optimal data loading efficiency when combined with a
two-level particle reordering scheme. A faster in situ generation scheme for
Gaussian random numbers is proposed using precomputed binary signatures. We
designed custom transcendental functions that are fast and accurate for
evaluating the pairwise interaction. The correctness and accuracy of the code
is verified through a set of test cases simulating Poiseuille flow and
spontaneous vesicle formation. Computer benchmarks demonstrate the speedup of
our implementation over the CPU implementation as well as strong and weak
scalability. A large-scale simulation of spontaneous vesicle formation
consisting of 128 million particles was conducted to further illustrate the
practicality of our code in real-world applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0404</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0404</id><created>2013-11-02</created><authors><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Wang</keyname><forenames>Xianbin</forenames></author><author><keyname>Shen</keyname><forenames>Weiming</forenames></author></authors><title>Physical-Layer Security with Multiuser Scheduling in Cognitive Radio
  Networks</title><categories>cs.IT math.IT</categories><comments>12 pages. IEEE Transactions on Communications, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a cognitive radio network that consists of one
cognitive base station (CBS) and multiple cognitive users (CUs) in the presence
of multiple eavesdroppers, where CUs transmit their data packets to CBS under a
primary user's quality of service (QoS) constraint while the eavesdroppers
attempt to intercept the cognitive transmissions from CUs to CBS. We
investigate the physical-layer security against eavesdropping attacks in the
cognitive radio network and propose the user scheduling scheme to achieve
multiuser diversity for improving the security level of cognitive transmissions
with a primary QoS constraint. Specifically, a cognitive user (CU) that
satisfies the primary QoS requirement and maximizes the achievable secrecy rate
of cognitive transmissions is scheduled to transmit its data packet. For the
comparison purpose, we also examine the traditional multiuser scheduling and
the artificial noise schemes. We analyze the achievable secrecy rate and
intercept probability of the traditional and proposed multiuser scheduling
schemes as well as the artificial noise scheme in Rayleigh fading environments.
Numerical results show that given a primary QoS constraint, the proposed
multiuser scheduling scheme generally outperforms the traditional multiuser
scheduling and the artificial noise schemes in terms of the achievable secrecy
rate and intercept probability. In addition, we derive the diversity order of
the proposed multiuser scheduling scheme through an asymptotic intercept
probability analysis and prove that the full diversity is obtained by using the
proposed multiuser scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0407</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0407</id><created>2013-11-02</created><authors><author><keyname>Bruna</keyname><forenames>Joan</forenames></author><author><keyname>Mallat</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Audio Texture Synthesis with Scattering Moments</title><categories>stat.AP cs.SD</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an audio texture synthesis algorithm based on scattering
moments. A scattering transform is computed by iteratively decomposing a signal
with complex wavelet filter banks and computing their amplitude envelop.
Scattering moments provide general representations of stationary processes
computed as expected values of scattering coefficients. They are estimated with
low variance estimators from single realizations. Audio signals having
prescribed scattering moments are synthesized with a gradient descent
algorithms. Audio synthesis examples show that scattering representation
provide good synthesis of audio textures with much fewer coefficients than the
state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0413</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0413</id><created>2013-11-02</created><authors><author><keyname>Dodig-Crnkovic</keyname><forenames>Gordana</forenames></author></authors><title>Information, Computation, Cognition. Agency-based Hierarchies of Levels</title><categories>cs.AI</categories><comments>5 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Nature can be seen as informational structure with computational dynamics
(info-computationalism), where an (info-computational) agent is needed for the
potential information of the world to actualize. Starting from the definition
of information as the difference in one physical system that makes a difference
in another physical system, which combines Bateson and Hewitt definitions, the
argument is advanced for natural computation as a computational model of the
dynamics of the physical world where information processing is constantly going
on, on a variety of levels of organization. This setting helps elucidating the
relationships between computation, information, agency and cognition, within
the common conceptual framework, which has special relevance for biology and
robotics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0423</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0423</id><created>2013-11-02</created><authors><author><keyname>Deni&#x163;iu</keyname><forenames>Andreea</forenames></author><author><keyname>Petra</keyname><forenames>Stefania</forenames></author><author><keyname>Schn&#xf6;rr</keyname><forenames>Claudius</forenames></author><author><keyname>Schn&#xf6;rr</keyname><forenames>Christoph</forenames></author></authors><title>Phase Transitions and Cosparse Tomographic Recovery of Compound Solid
  Bodies from Few Projections</title><categories>math.NA cs.IT math.IT</categories><msc-class>65F22, 68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study unique recovery of cosparse signals from limited-angle tomographic
measurements of two- and three-dimensional domains. Admissible signals belong
to the union of subspaces defined by all cosupports of maximal cardinality
$\ell$ with respect to the discrete gradient operator. We relate $\ell$ both to
the number of measurements and to a nullspace condition with respect to the
measurement matrix, so as to achieve unique recovery by linear programming.
These results are supported by comprehensive numerical experiments that show a
high correlation of performance in practice and theoretical predictions.
Despite poor properties of the measurement matrix from the viewpoint of
compressed sensing, the class of uniquely recoverable signals basically seems
large enough to cover practical applications, like contactless quality
inspection of compound solid bodies composed of few materials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0433</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0433</id><created>2013-11-03</created><authors><author><keyname>Chen</keyname><forenames>Chiao-En</forenames></author><author><keyname>Yang</keyname><forenames>Chia-Hsiang</forenames></author></authors><title>An Iterative Geometric Mean Decomposition Algorithm for MIMO
  Communications Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an iterative geometric mean decomposition (IGMD)
algorithm for multiple-input-multiple-output (MIMO) wireless communications. In
contrast to the existing GMD algorithms, the proposed IGMD does not require the
explicit computation of the geometric mean of positive singular values of the
channel matrix and hence is more suitable for hardware implementation. The
proposed IGMD has a regular structure and can be easily adapted to solve
problems with different dimensions. We show that the proposed IGMD is
guaranteed to converge to the perfect GMD under certain sufficient condition.
Three different constructions of the proposed algorithm are proposed and
compared through computer simulations. Numerical results show that the proposed
algorithm quickly attains comparable performance to that of the true GMD within
only a few iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0438</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0438</id><created>2013-11-03</created><updated>2014-04-29</updated><authors><author><keyname>Saha</keyname><forenames>Snehanshu</forenames></author><author><keyname>Routh</keyname><forenames>Swati</forenames></author><author><keyname>Goswami</keyname><forenames>Bidisha</forenames></author></authors><title>Modeling Vanilla Option prices: A simulation study by an implicit method</title><categories>cs.CE</categories><comments>An expository report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Option contracts can be valued by using the Black-Scholes equation, a partial
differential equation with initial conditions. An exact solution for European
style options is known. The computation time and the error need to be minimized
simultaneously. In this paper, the authors have solved the Black-Scholes
equation by employing a reasonably accurate implicit method. Options with known
analytic solutions have been evaluated. Furthermore, an overall second order
accurate space and time discretization is proposed in this paper Keywords:
Computational finance, implicit methods, finite differences, call/put options.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0442</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0442</id><created>2013-11-03</created><updated>2014-04-03</updated><authors><author><keyname>Krivulin</keyname><forenames>Nikolai</forenames></author></authors><title>Extremal properties of tropical eigenvalues and solutions to tropical
  optimization problems</title><categories>math.OC cs.SY</categories><comments>22 pages, presented at ILAS 2013 Conference (Providence, RI, 2013),
  major revision</comments><msc-class>65K10 (Primary), 15A80, 65K05, 90C48, 90B35 (Secondary)</msc-class><journal-ref>Linear Algebra and its Applications, 2015. Vol. 468, P. 211-232</journal-ref><doi>10.1016/j.laa.2014.06.044</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An unconstrained optimization problem is formulated in terms of tropical
mathematics to minimize a functional that is defined on a vector set by a
matrix and calculated through multiplicative conjugate transposition. For some
particular cases, the minimum in the problem is known to be equal to the
tropical spectral radius of the matrix. We examine the problem in the common
setting of a general idempotent semifield. A complete direct solution in a
compact vector form is obtained to this problem under fairly general
conditions. The result is extended to solve new tropical optimization problems
with more general objective functions and inequality constraints. Applications
to real-world problems that arise in project scheduling are presented. To
illustrate the results obtained, numerical examples are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0456</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0456</id><created>2013-11-03</created><updated>2014-05-20</updated><authors><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author><author><keyname>Bliss</keyname><forenames>Daniel W.</forenames></author><author><keyname>Rangarajan</keyname><forenames>Sampath</forenames></author><author><keyname>Wichman</keyname><forenames>Risto</forenames></author></authors><title>In-Band Full-Duplex Wireless: Challenges and Opportunities</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In-band full-duplex (IBFD) operation has emerged as an attractive solution
for increasing the throughput of wireless communication systems and networks.
With IBFD, a wireless terminal is allowed to transmit and receive
simultaneously in the same frequency band. This tutorial paper reviews the main
concepts of IBFD wireless. Because one the biggest practical impediments to
IBFD operation is the presence of self-interference, i.e., the interference
caused by an IBFD node's own transmissions to its desired receptions, this
tutorial surveys a wide range of IBFD self-interference mitigation techniques.
Also discussed are numerous other research challenges and opportunities in the
design and analysis of IBFD wireless systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0459</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0459</id><created>2013-11-03</created><authors><author><keyname>Douik</keyname><forenames>Ahmed</forenames></author><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author></authors><title>A Lossy Graph Model for Decoding Delay Reduction in Instantly Decodable
  Network Coding</title><categories>cs.IT math.IT</categories><doi>10.1109/WCL.2014.022814.140067</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the broadcast decoding delay performance of
generalized instantly decodable network coding (G-IDNC) in the lossy feedback
scenario. The problem is formulated as a maximum weight clique problem over the
G-IDNC graph in [1]. In order to further minimize the decoding delay, we
introduce in this paper the lossy G-IDNC graph (LG-IDNC). Whereas the G-IDNC
graph represents only doubtless combinable packets, the LG-IDNC graph
represents also uncertain packet combinations when the expected decoding delay
of the encoded packet is lower than the individual expected decoding delay of
each packet encoded in it. Since the maximum weight clique problem is known to
be NP-hard, we use the heuristic introduced in [2] to discover the maximum
weight clique in the LG-IDNC graph and finally we compare the decoding delay
performance of LG-IDNC and G-IDNC graphs through extensive simulations.
Numerical results show that our new LG-IDNC graph formulation outperforms the
G-IDNC graph formulation in all situations and achieves significant improvement
in the decoding delay especially when the feedback erasure probability is
higher than the packet erasure probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0460</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0460</id><created>2013-11-03</created><authors><author><keyname>Zhang</keyname><forenames>Xiaoge</forenames></author><author><keyname>Liu</keyname><forenames>Qi</forenames></author><author><keyname>Hu</keyname><forenames>Yong</forenames></author><author><keyname>Chan</keyname><forenames>Felix T. S.</forenames></author><author><keyname>Mahadevan</keyname><forenames>Sankaran</forenames></author><author><keyname>Zhang</keyname><forenames>Zili</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>An Adaptive Amoeba Algorithm for Shortest Path Tree Computation in
  Dynamic Graphs</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an adaptive amoeba algorithm to address the shortest path
tree (SPT) problem in dynamic graphs. In dynamic graphs, the edge weight
updates consists of three categories: edge weight increases, edge weight
decreases, the mixture of them. Existing work on this problem solve this issue
through analyzing the nodes influenced by the edge weight updates and recompute
these affected vertices. However, when the network becomes big, the process
will become complex. The proposed method can overcome the disadvantages of the
existing approaches. The most important feature of this algorithm is its
adaptivity. When the edge weight changes, the proposed algorithm can recognize
the affected vertices and reconstruct them spontaneously. To evaluate the
proposed adaptive amoeba algorithm, we compare it with the Label Setting
algorithm and Bellman-Ford algorithm. The comparison results demonstrate the
effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0461</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0461</id><created>2013-11-03</created><authors><author><keyname>Kaipa</keyname><forenames>Krishna</forenames></author></authors><title>An asymptotic formula in q for the number of [n,k] q-ary MDS codes</title><categories>cs.IT math.AG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We obtain an asymptotic formula in q for the number of MDS codes of length n
and dimension k over a finite field with q elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0466</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0466</id><created>2013-11-03</created><authors><author><keyname>Gopalan</keyname><forenames>Aditya</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author></authors><title>Thompson Sampling for Complex Bandit Problems</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider stochastic multi-armed bandit problems with complex actions over
a set of basic arms, where the decision maker plays a complex action rather
than a basic arm in each round. The reward of the complex action is some
function of the basic arms' rewards, and the feedback observed may not
necessarily be the reward per-arm. For instance, when the complex actions are
subsets of the arms, we may only observe the maximum reward over the chosen
subset. Thus, feedback across complex actions may be coupled due to the nature
of the reward function. We prove a frequentist regret bound for Thompson
sampling in a very general setting involving parameter, action and observation
spaces and a likelihood function over them. The bound holds for
discretely-supported priors over the parameter space and without additional
structural properties such as closed-form posteriors, conjugate prior structure
or independence across arms. The regret bound scales logarithmically with time
but, more importantly, with an improved constant that non-trivially captures
the coupling across complex actions due to the structure of the rewards. As
applications, we derive improved regret bounds for classes of complex bandit
problems involving selecting subsets of arms, including the first nontrivial
regret bounds for nonlinear MAX reward feedback from subsets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0468</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0468</id><created>2013-11-03</created><authors><author><keyname>Gopalan</keyname><forenames>Aditya</forenames></author></authors><title>Thompson Sampling for Online Learning with Linear Experts</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we present a version of the Thompson sampling algorithm for the
problem of online linear generalization with full information (i.e., the
experts setting), studied by Kalai and Vempala, 2005. The algorithm uses a
Gaussian prior and time-varying Gaussian likelihoods, and we show that it
essentially reduces to Kalai and Vempala's Follow-the-Perturbed-Leader
strategy, with exponentially distributed noise replaced by Gaussian noise. This
implies sqrt(T) regret bounds for Thompson sampling (with time-varying
likelihood) for online learning with full information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0484</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0484</id><created>2013-11-03</created><updated>2013-11-16</updated><authors><author><keyname>Zehavi</keyname><forenames>Meirav</forenames></author></authors><title>Deterministic Parameterized Algorithms for Matching and Packing Problems</title><categories>cs.DS</categories><comments>Consideration for replacement: A better bound on the running time of
  WSP-Alg (see Section 5)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present three deterministic parameterized algorithms for well-studied
packing and matching problems, namely, Weighted q-Dimensional p-Matching
((q,p)-WDM) and Weighted q-Set p-Packing ((q,p)-WSP). More specifically, we
present an O*(2.85043^{(q-1)p}) time deterministic algorithm for (q,p)-WDM, an
O*(8.04143^p) time deterministic algorithm for the unweighted version of
(3,p)-WDM, and an O*((0.56201\cdot 2.85043^q)^p) time deterministic algorithm
for (q,p)-WSP. Our algorithms significantly improve the previously best known
O* running times in solving (q,p)-WDM and (q,p)-WSP, and the previously best
known deterministic O* running times in solving the unweighted versions of
these problems. Moreover, we present kernels of size O(e^qq(p-1)^q) for
(q,p)-WDM and (q,p)-WSP, improving the previously best known kernels of size
O(q!q(p-1)^q) for these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0486</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0486</id><created>2013-11-03</created><authors><author><keyname>S.</keyname><forenames>Vineeth B.</forenames></author><author><keyname>Mukherji</keyname><forenames>Utpal</forenames></author></authors><title>On the optimal tradeoff of average service cost rate, average utility
  rate, and average delay for the state dependent M/M/1 queue</title><categories>cs.PF cs.NI math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimal tradeoff between average service cost rate, average utility rate,
and average delay is addressed for a state dependent M/M/1 queueing model, with
controllable queue length dependent service rates and arrival rates. For a
model with a constant arrival rate $\lambda$ for all queue lengths, we obtain
an asymptotic characterization of the minimum average delay, when the average
service cost rate is a small positive quantity, $V$, more than the minimum
average service cost rate required for queue stability. We show that depending
on the value of the arrival rate $\lambda$, the assumed service cost rate
function, and the possible values of the service rates, the minimum average
delay either: a) increases only to a finite value, b) increases without bound
as $\log\frac{1}{V}$, c) increases without bound as $\frac{1}{V}$, or d)
increases without bound as $\frac{1}{\sqrt{V}}$, when $V \downarrow 0$. We then
extend our analysis to (i) a complementary problem, where the tradeoff of
average utility rate and average delay is analysed for a M/M/1 queueing model,
with controllable queue length dependent arrival rates, but a constant service
rate $\mu$ for all queue lengths, and (ii) a M/M/1 queueing model, with
controllable queue length dependent service rates and arrival rates, for which
we obtain an asymptotic characterization of the minimum average delay under
constraints on both the average service cost rate as well as the average
utility rate. The results that we obtain are useful in obtaining intuition as
well guidance for the derivation of similar asymptotic lower bounds, such as
the Berry-Gallager asymptotic lower bound, for discrete time queueing models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0505</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0505</id><created>2013-11-03</created><authors><author><keyname>Tran</keyname><forenames>Dang-Hoan</forenames></author></authors><title>Automated Change Detection and Reactive Clustering in Multivariate
  Streaming Data</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many automated systems need the capability of automatic change detection
without the given detection threshold. This paper presents an automated change
detection algorithm in streaming multivariate data. Two overlapping windows are
used to quantify the changes. While a window is used as the reference window
from which the clustering is created, the other called the current window
captures the newly incoming data points. A newly incoming data point can be
considered a change point if it is not a member of any cluster. As our
clustering-based change detector does not require detection threshold, it is an
automated detector. Based on this change detector, we propose a reactive
clustering algorithm for streaming data. Our empirical results show that, our
clustering-based change detector works well with multivariate streaming data.
The detection accuracy depends on the number of clusters in the reference
window, the window width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0512</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0512</id><created>2013-11-03</created><updated>2014-05-12</updated><authors><author><keyname>Candr&#xe1;kov&#xe1;</keyname><forenames>Barbora</forenames></author><author><keyname>Luko&#x165;ka</keyname><forenames>Robert</forenames></author></authors><title>Avoiding 5-circuits in a 2-factor of cubic graphs</title><categories>math.CO cs.DM</categories><comments>22 pages, 3 (8) figures. Submitted</comments><msc-class>05C70 (Primary), 05C38 (Secondary)</msc-class><journal-ref>SIAM Journal on Discrete Mathematics 29 (2015) 1387-1405</journal-ref><doi>10.1137/130942966</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that every bridgeless cubic graph $G$ on $n$ vertices other than the
Petersen graph has a 2-factor with at most $2(n-2)/15$ circuits of length $5$.
An infinite family of graphs attains this bound. We also show that $G$ has a
2-factor with at most $n/5.8\overline{3}$ odd circuits. This improves the
previously known bound of $n/5.41$ [Luko\v{t}ka, M\'a\v{c}ajov\'a, Maz\'ak,
\v{S}koviera: Small snarks with large oddness, arXiv:1212.3641 [cs.DM] ].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0527</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0527</id><created>2013-11-03</created><authors><author><keyname>Kyriakou</keyname><forenames>Harris</forenames></author><author><keyname>Nickerson</keyname><forenames>Jeffrey V.</forenames></author></authors><title>Idea Inheritance, Originality, and Collective Innovation</title><categories>cs.HC cs.CY</categories><comments>Workshop on Information in Networks, 2013</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In order to create new products, inventors search and combine previous ideas.
Few studies have examined the characteristics of search that lead to new
products; most have focused on patent citations, which are often retrospective
and may not reflect the usefulness of inventions.
  Through the analysis of collaborations in an online virtual community, the
impact of originality on popularity and practicality is tested. These tests in
turn are based on a method for measuring the distance between 3D shapes. In
sum, this paper presents a new method for gauging innovation, and suggests ways
of further understanding the role technology plays in encouraging creativity.
From an organization perspective, this work provides insights into the creative
process, and in particular the open innovation process, in which thousands of
individuals together evolve designs, without belonging to the same corporate
structure, without claiming IP rights, without exchanging money.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0529</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0529</id><created>2013-11-03</created><authors><author><keyname>Kyriakou</keyname><forenames>Harris</forenames></author><author><keyname>Englehardt</keyname><forenames>Steven</forenames></author><author><keyname>Nickerson</keyname><forenames>Jeffrey V.</forenames></author></authors><title>Networks of Innovation in 3D Printing</title><categories>cs.HC cs.SI</categories><comments>Workshop on Information in Networks, 2012</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Innovation inside companies is difficult to see. But an emerging online
community of inventors who publicly post 3D CAD drawings of their work provide
a way to observe - and perhaps amplify - innovation. In this paper we analyze
the network structure of Thingiverse, a website oriented toward 3D printing.
This form of printing blurs the line between creating information and
manufacturing objects: drawings can be sent to devices that build 3D objects
out of many materials, including resin, ceramics, and metal. As an exploratory
study, we analyzed the structure of Thingiverse links. Our results suggest that
analysis of remix network structure may provide ways of tracing innovation
processes and detecting the emergence of new ideas, combination of disparate
ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0534</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0534</id><created>2013-11-03</created><authors><author><keyname>Peterson</keyname><forenames>John W.</forenames></author></authors><title>Accurate curve fits of IAPWS data for high-pressure, high-temperature
  single-phase liquid water based on the stiffened gas equation of state</title><categories>cs.CE</categories><comments>17 pages, 7 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a series of optimal (in the sense of least-squares) curve fits for
the stiffened gas equation of state for single-phase liquid water. At high
pressures and (subcritical) temperatures, the parameters produced by these
curve fits are found to have very small relative errors: less than $1\%$ in the
pressure model, and less than $2\%$ in the temperature model. At low pressures
and temperatures, especially near the liquid-vapor transition line, the error
in the curve fits increases rapidly. The smallest pressure value for which
curve fits are reported in the present work is 25 MPa, high enough to ensure
that the fluid remains a single-phase liquid up to the maximum subcritical
temperature of approximately 647K.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0536</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0536</id><created>2013-11-03</created><updated>2014-01-01</updated><authors><author><keyname>Bikakis</keyname><forenames>Nikos</forenames></author><author><keyname>Tsinaraki</keyname><forenames>Chrisa</forenames></author><author><keyname>Stavrakantonakis</keyname><forenames>Ioannis</forenames></author><author><keyname>Gioldasis</keyname><forenames>Nektarios</forenames></author><author><keyname>Christodoulakis</keyname><forenames>Stavros</forenames></author></authors><title>The SPARQL2XQuery Interoperability Framework. Utilizing Schema Mapping,
  Schema Transformation and Query Translation to Integrate XML and the Semantic
  Web</title><categories>cs.DB</categories><comments>To appear in World Wide Web Journal (WWWJ), Springer 2013</comments><acm-class>H.2.1; H.2.3; H.2.5; H.2.4; H.3.4; D.2.12; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Web of Data is an open environment consisting of a great number of large
inter-linked RDF datasets from various domains. In this environment,
organizations and companies adopt the Linked Data practices utilizing Semantic
Web (SW) technologies, in order to publish their data and offer SPARQL
endpoints (i.e., SPARQL-based search services). On the other hand, the dominant
standard for information exchange in the Web today is XML. The SW and XML
worlds and their developed infrastructures are based on different data models,
semantics and query languages. Thus, it is crucial to develop interoperability
mechanisms that allow the Web of Data users to access XML datasets, using
SPARQL, from their own working environments. It is unrealistic to expect that
all the existing legacy data (e.g., Relational, XML, etc.) will be transformed
into SW data. Therefore, publishing legacy data as Linked Data and providing
SPARQL endpoints over them has become a major research challenge. In this
direction, we introduce the SPARQL2XQuery Framework which creates an
interoperable environment, where SPARQL queries are automatically translated to
XQuery queries, in order to access XML data across the Web. The SPARQL2XQuery
Framework provides a mapping model for the expression of OWL-RDF/S to XML
Schema mappings as well as a method for SPARQL to XQuery translation. To this
end, our Framework supports both manual and automatic mapping specification
between ontologies and XML Schemas. In the automatic mapping specification
scenario, the SPARQL2XQuery exploits the XS2OWL component which transforms XML
Schemas into OWL ontologies. Finally, extensive experiments have been conducted
in order to evaluate the schema transformation, mapping generation, query
translation and query evaluation efficiency, using both real and synthetic
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0541</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0541</id><created>2013-11-03</created><authors><author><keyname>Bialkowski</keyname><forenames>Joshua</forenames></author><author><keyname>Otte</keyname><forenames>Michael</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author></authors><title>Free-configuration Biased Sampling for Motion Planning: Errata</title><categories>cs.RO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document contains improved and updated proofs of convergence for the
sampling method presented in our paper &quot;Free-configuration Biased Sampling for
Motion Planning&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0546</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0546</id><created>2013-11-03</created><authors><author><keyname>Estevez-Rams</keyname><forenames>E.</forenames></author><author><keyname>Serrano</keyname><forenames>R. Lora</forenames></author><author><keyname>Fern&#xe1;ndez</keyname><forenames>B. Arag&#xf3;n</forenames></author><author><keyname>Reyes</keyname><forenames>I. Brito</forenames></author></authors><title>On the non-randomness of maximum Lempel Ziv complexity sequences of
  finite size</title><categories>nlin.CD cs.IT math.IT</categories><journal-ref>CHAOS 23, 023118 (2013)</journal-ref><doi>10.1063/1.4808251</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random sequences attain the highest entropy rate. The estimation of entropy
rate for an ergodic source can be done using the Lempel Ziv complexity measure
yet, the exact entropy rate value is only reached in the infinite limit. We
prove that typical random sequences of finite length fall short of the maximum
Lempel-Ziv complexity, contrary to common belief. We discuss that, for a finite
length, maximum Lempel-Ziv sequences can be built from a well defined
generating algorithm, which makes them of low Kolmogorov-Chaitin complexity,
quite the opposite to randomness. It will be discussed that Lempel-Ziv measure
is, in this sense, less general than Kolmogorov-Chaitin complexity, as it can
be fooled by an intelligent enough agent. The latter will be shown to be the
case for the binary expansion of certain irrational numbers. Maximum Lempel-Ziv
sequences induce a normalization that gives good estimates of entropy rate for
several sources, while keeping bounded values for all sequence length, making
it an alternative to other normalization schemes in use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0558</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0558</id><created>2013-11-03</created><authors><author><keyname>Pak</keyname><forenames>Igor</forenames></author><author><keyname>Wilson</keyname><forenames>Stedman</forenames></author></authors><title>A Quantitative Steinitz Theorem for Plane Triangulations</title><categories>math.CO cs.CG cs.DM</categories><comments>25 pages, 6 postscript figures</comments><msc-class>05C62 (Primary), 52B10, 68R10 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new proof of Steinitz's classical theorem in the case of plane
triangulations, which allows us to obtain a new general bound on the grid size
of the simplicial polytope realizing a given triangulation, subexponential in a
number of special cases.
  Formally, we prove that every plane triangulation $G$ with $n$ vertices can
be embedded in $\mathbb{R}^2$ in such a way that it is the vertical projection
of a convex polyhedral surface. We show that the vertices of this surface may
be placed in a $4n^3 \times 8n^5 \times \zeta(n)$ integer grid, where $\zeta(n)
\leq (500 n^8)^{\tau(G)}$ and $\tau(G)$ denotes the shedding diameter of $G$, a
quantity defined in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0573</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0573</id><created>2013-11-03</created><updated>2013-12-16</updated><authors><author><keyname>Robinson</keyname><forenames>Rebecca</forenames></author><author><keyname>Farr</keyname><forenames>Graham</forenames></author></authors><title>Graphs with no 7-wheel subdivision</title><categories>cs.DM math.CO</categories><comments>97 pages, 47 figures</comments><report-no>Technical report number 2012/270, Clayton School of Information
  Technology, Monash University</report-no><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The subgraph homeomorphism problem, SHP($H$), has been shown to be
polynomial-time solvable for any fixed pattern graph $H$, but practical
algorithms have been developed only for a few specific pattern graphs. Among
these are the wheels with four, five, and six spokes. This paper examines the
subgraph homeomorphism problem where the pattern graph is a wheel with seven
spokes, and gives a result that describes graphs with no $W_{7}$-subdivision,
showing how they can be built up, using certain operations, from smaller
`pieces' that meet certain conditions. We also discuss algorithmic aspects of
the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0574</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0574</id><created>2013-11-03</created><authors><author><keyname>Robinson</keyname><forenames>Rebecca</forenames></author><author><keyname>Farr</keyname><forenames>Graham</forenames></author></authors><title>Search strategies for developing characterizations of graphs without
  small wheel subdivisions</title><categories>cs.DM math.CO</categories><comments>22 pages, 4 figures</comments><report-no>Technical report 2009/241, Clayton School of Information Technology,
  Monash University</report-no><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Practical algorithms for solving the Subgraph Homeomorphism Problem are known
for only a few small pattern graphs: among these are the wheel graphs with
four, five, six, and seven spokes. The length and difficulty of the proofs
leading to these algorithms increase greatly as the size of the pattern graph
increases. Proving a result for the wheel with six spokes requires extensive
case analysis on many small graphs, and even more such analysis is needed for
the wheel with seven spokes. This paper describes algorithms and programs used
to automate the generation and testing of the graphs that arise as cases in
these proofs. The main algorithm given may be useful in a more general context,
for developing other characterizations of SHP-related properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0576</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0576</id><created>2013-11-03</created><updated>2015-02-18</updated><authors><author><keyname>Wang</keyname><forenames>Xing</forenames></author><author><keyname>Liang</keyname><forenames>Jie</forenames></author></authors><title>Approximate Message Passing-based Compressed Sensing Reconstruction with
  Generalized Elastic Net Prior</title><categories>cs.IT math.IT</categories><comments>32 pages, 3 figures and 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the compressed sensing reconstruction problem with
generalized elastic net prior (GENP), where a sparse signal is sampled via a
noisy underdetermined linear observation system, and an additional initial
estimation of the signal (the GENP) is available during the reconstruction. We
first incorporate the GENP into the LASSO and the approximate message passing
(AMP) frameworks, denoted by GENP-LASSO and GENP-AMP respectively. We then
investigate the parameter selection, state evolution, and noise-sensitivity
analysis of GENP-AMP. We show that, thanks to the GENP, there is no phase
transition boundary in the proposed frameworks, i.e., the reconstruction error
is bounded in the entire plane. The error is also smaller than those of the
standard AMP and scalar denoising. A practical parameterless version of the
GENP-AMP is also developed, which does not need to know the sparsity of the
unknown signal and the variance of the GENP. Simulation results are presented
to verify the efficiency of the proposed schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0590</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0590</id><created>2013-11-04</created><authors><author><keyname>Jeong</keyname><forenames>Hwancheol</forenames></author><author><keyname>Lee</keyname><forenames>Weonjong</forenames></author><author><keyname>Pak</keyname><forenames>Jeonghwan</forenames></author><author><keyname>Choi</keyname><forenames>Kwang-jong</forenames></author><author><keyname>Park</keyname><forenames>Sang-Hyun</forenames></author><author><keyname>Yoo</keyname><forenames>Jun-sik</forenames></author><author><keyname>Kim</keyname><forenames>Joo Hwan</forenames></author><author><keyname>Lee</keyname><forenames>Joungjin</forenames></author><author><keyname>Lee</keyname><forenames>Young Woo</forenames></author></authors><title>Performance of Kepler GTX Titan GPUs and Xeon Phi System</title><categories>physics.comp-ph cs.DC hep-lat</categories><comments>7 pages, 6 figures, 3 tables, Contribution to proceedings of the 31st
  International Symposium on Lattice Field Theory (Lattice 2013), July 29 -
  August 3, 2013</comments><journal-ref>PoS (LATTICE 2013) 423</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NVIDIA's new architecture, Kepler improves GPU's performance significantly
with the new streaming multiprocessor SMX. Along with the performance, NVIDIA
has also introduced many new technologies such as direct parallelism, hyper-Q
and GPU Direct with RDMA. Apart from other usual GPUs, NVIDIA also released
another Kepler 'GeForce' GPU named GTX Titan. GeForce GTX Titan is not only
good for gaming but also good for high performance computing with CUDA.
Nevertheless, it is remarkably cheaper than Kepler Tesla GPUs. We investigate
the performance of GTX Titan and find out how to optimize a CUDA code
appropriately for it. Meanwhile, Intel has launched its new many integrated
core (MIC) system, Xeon Phi. A Xeon Phi coprocessor could provide similar
performance with NVIDIA Kepler GPUs theoretically but, in reality, it turns out
that its performance is significantly inferior to GTX Titan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0598</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0598</id><created>2013-11-04</created><authors><author><keyname>Kamberaj</keyname><forenames>Hiqmet</forenames></author></authors><title>Q-Gaussian Swarm Quantum Particle Intelligence on Predicting Global
  Minimum of Potential Energy Function</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a newly developed -Gaussian Swarm Quantum-like Particle
Optimization (q-GSQPO) algorithm to determine the global minimum of the
potential energy function. Swarm Quantum-like Particle Optimization (SQPO)
algorithms have been derived using different attractive potential fields to
represent swarm particles moving in a quantum environment, where the one which
uses a harmonic oscillator potential as attractive field is considered as an
improved version. In this paper, we propose a new SQPO that uses -Gaussian
probability density function for the attractive potential field (q-GSQPO)
rather than Gaussian one (GSQPO) which corresponds to harmonic potential. The
performance of the q-GSQPO is compared against the GSQPO. The new algorithm
outperforms the GSQPO on most of the time in convergence to the global optimum
by increasing the efficiency of sampling the phase space and avoiding the
premature convergence to local minima. Moreover, the computational efforts were
comparable for both algorithms. We tested the algorithm to determine the lowest
energy configurations of a particle moving in a 2, 5, 10, and 50 dimensional
spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0602</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0602</id><created>2013-11-04</created><updated>2013-11-04</updated><authors><author><keyname>Timmaraju</keyname><forenames>Aditya Srinivas</forenames></author><author><keyname>Deshmukh</keyname><forenames>Aniket Anand</forenames></author><author><keyname>Khan</keyname><forenames>Mohammed Amir</forenames></author><author><keyname>Khan</keyname><forenames>Zafar Ali</forenames></author></authors><title>Input-Output Logic based Fault-Tolerant Design Technique for SRAM-based
  FPGAs</title><categories>cs.AR</categories><comments>7 pages</comments><acm-class>B.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effects of radiation on electronic circuits used in extra-terrestrial
applications and radiation prone environments need to be corrected. Since FPGAs
offer flexibility, the effects of radiation on them need to be studied and
robust methods of fault tolerance need to be devised. In this paper a new
fault-tolerant design strategy has been presented. This strategy exploits the
relation between changes in inputs and the expected change in output.
Essentially, it predicts whether or not a change in the output is expected and
thereby calculates the error. As a result this strategy reduces hardware and
time redundancy required by existing strategies like Duplication with
Comparison (DWC) and Triple Modular Redundancy (TMR). The design arising from
this strategy has been simulated and its robustness to fault-injection has been
verified. Simulations for a 16 bit multiplier show that the new design strategy
performs better than the state-of-the-art on critical factors such as hardware
redundancy, time redundancy and power consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0603</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0603</id><created>2013-11-04</created><updated>2013-12-02</updated><authors><author><keyname>Junosza-Szaniawski</keyname><forenames>Konstanty</forenames></author><author><keyname>Rz&#x105;&#x17c;ewski</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>An Exact Algorithm for the Generalized List $T$-Coloring Problem</title><categories>cs.DM cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalized list $T$-coloring is a common generalization of many graph
coloring models, including classical coloring, $L(p,q)$-labeling, channel
assignment and $T$-coloring. Every vertex from the input graph has a list of
permitted labels. Moreover, every edge has a set of forbidden differences. We
ask for such a labeling of vertices of the input graph with natural numbers, in
which every vertex gets a label from its list of permitted labels and the
difference of labels of the endpoints of each edge does not belong to the set
of forbidden differences of this edge. In this paper we present an exact
algorithm solving this problem, running in time $\mathcal{O}^*((\tau+2)^n)$,
where $\tau$ is the maximum forbidden difference over all edges of the input
graph and $n$ is the number of its vertices. Moreover, we show how to improve
this bound if the input graph has some special structure, e.g. a bounded
maximum degree, no big induced stars or a perfect matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0607</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0607</id><created>2013-11-04</created><updated>2014-02-19</updated><authors><author><keyname>Mlynarski</keyname><forenames>Wiktor</forenames></author></authors><title>Efficient coding of spectrotemporal binaural sounds leads to emergence
  of the auditory space representation</title><categories>q-bio.NC cs.SD</categories><comments>22 pages, 9 figures</comments><doi>10.3389/fncom.2014.00026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To date a number of studies have shown that receptive field shapes of early
sensory neurons can be reproduced by optimizing coding efficiency of natural
stimulus ensembles. A still unresolved question is whether the efficient coding
hypothesis explains formation of neurons which explicitly represent
environmental features of different functional importance. This paper proposes
that the spatial selectivity of higher auditory neurons emerges as a direct
consequence of learning efficient codes for natural binaural sounds. Firstly,
it is demonstrated that a linear efficient coding transform - Independent
Component Analysis (ICA) trained on spectrograms of naturalistic simulated
binaural sounds extracts spatial information present in the signal. A simple
hierarchical ICA extension allowing for decoding of sound position is proposed.
Furthermore, it is shown that units revealing spatial selectivity can be
learned from a binaural recording of a natural auditory scene. In both cases a
relatively small subpopulation of learned spectrogram features suffices to
perform accurate sound localization. Representation of the auditory space is
therefore learned in a purely unsupervised way by maximizing the coding
efficiency and without any task-specific constraints. This results imply that
efficient coding is a useful strategy for learning structures which allow for
making behaviorally vital inferences about the environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0632</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0632</id><created>2013-11-04</created><authors><author><keyname>Bourreau</keyname><forenames>Pierre</forenames></author></authors><title>On the effect of the IO-substitution on the Parikh image of semilinear
  AFLs</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Back in the 80s, the class of mildly context-sensitive formalisms was
introduced so as to capture the syntax of natural languages. While the
languages generated by such formalisms are constrained by the constant-growth
property, the most well-known and used mildly-context sensitive formalisms,
like tree-adjoining grammars or multiple context-free grammars, generate
languages which verify the stronger property of being semilinear. In (Bourreau
et al., 2012), the operation of IO-ubstitution was created so as to exhibit
mildly context-sensitive classes of languages which are not semilinear,
although they verify the constant-growth property. In the present article, we
extend the notion of semilinearity, and characterise the Parikh image of the
IO-MCFLs (i.e. languages which belong to the closure of MCFLs under
IO-subsitution) as universally-linear. Based on this result and on the work of
Fischer on macro-grammars, we then show IO-MCFLs are not closed under inverse
homomorphism, which proves that the family of IO-MCFLs is not an abstract
family of languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0636</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0636</id><created>2013-11-04</created><authors><author><keyname>Mahajan</keyname><forenames>Dhruv</forenames></author><author><keyname>Keerthi</keyname><forenames>S. Sathiya</forenames></author><author><keyname>Sundararajan</keyname><forenames>S.</forenames></author><author><keyname>Bottou</keyname><forenames>Leon</forenames></author></authors><title>A Parallel SGD method with Strong Convergence</title><categories>cs.LG cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel parallel stochastic gradient descent (SGD) method
that is obtained by applying parallel sets of SGD iterations (each set
operating on one node using the data residing in it) for finding the direction
in each iteration of a batch descent method. The method has strong convergence
properties. Experiments on datasets with high dimensional feature spaces show
the value of this method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0646</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0646</id><created>2013-11-04</created><authors><author><keyname>Bj&#xf6;rklund</keyname><forenames>Tomas</forenames></author><author><keyname>Magli</keyname><forenames>Enrico</forenames></author></authors><title>A Parallel Compressive Imaging Architecture for One-Shot Acquisition</title><categories>cs.CV astro-ph.IM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A limitation of many compressive imaging architectures lies in the sequential
nature of the sensing process, which leads to long sensing times. In this paper
we present a novel architecture that uses fewer detectors than the number of
reconstructed pixels and is able to acquire the image in a single acquisition.
This paves the way for the development of video architectures that acquire
several frames per second. We specifically address the diffraction problem,
showing that deconvolution normally used to recover diffraction blur can be
replaced by convolution of the sensing matrix, and how measurements of a 0/1
physical sensing matrix can be converted to -1/1 compressive sensing matrix
without any extra acquisitions. Simulations of our architecture show that the
image quality is comparable to that of a classic Compressive Imaging camera,
whereas the proposed architecture avoids long acquisition times due to
sequential sensing. This one-shot procedure also allows to employ a fixed
sensing matrix instead of a complex device such as a Digital Micro Mirror array
or Spatial Light Modulator. It also enables imaging at bandwidths where these
are not efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0653</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0653</id><created>2013-11-04</created><authors><author><keyname>Flener</keyname><forenames>Pierre</forenames></author><author><keyname>Pearson</keyname><forenames>Justin</forenames></author></authors><title>Automatic Airspace Sectorisation: A Survey</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Airspace sectorisation provides a partition of a given airspace into sectors,
subject to geometric constraints and workload constraints, so that some cost
metric is minimised. We survey the algorithmic aspects of methods for automatic
airspace sectorisation, for an intended readership of experts on air traffic
management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0663</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0663</id><created>2013-11-04</created><authors><author><keyname>Huang</keyname><forenames>Ai-Ju</forenames></author><author><keyname>Wang</keyname><forenames>Hao-Chuan</forenames></author><author><keyname>Yuan</keyname><forenames>Chien Wen</forenames></author></authors><title>De-Virtualizing Social Events: Understanding the Gap between Online and
  Offline Participation for Event Invitations</title><categories>cs.HC cs.CY</categories><comments>Proc. 17th ACM Conference on Computer Supported Cooperative Work and
  Social Computing (CSCW), 2014</comments><acm-class>H.5.3; H.5.m</acm-class><doi>10.1145/2531602.2531606</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One growing use of computer-based communication media is for gathering people
to initiate or sustain social events. Although the use of computer-mediated
communication and social network sites such as Facebook for event promotion is
becoming popular, online participation in an event does not always translate to
offline attendance. In this paper, we report on an interview study of 31
participants that examines how people handle online event invitations and what
influences their online and offline participation. The results show that
people's event participation is shaped by their social perceptions of the
event's nature (e.g., public or private), their relationships to others (e.g.,
the strength of their connections to other invitees), and the medium used to
communicate event information (e.g., targeted invitation via email or spam
communication via Facebook event page). By exploring how people decide whether
to participate online or offline, the results illuminate the sophisticated
nature of the mechanisms that affect participation and have design implications
that can bridge virtual and real attendance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0667</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0667</id><created>2013-11-04</created><authors><author><keyname>van Hoek</keyname><forenames>Wilko</forenames></author></authors><title>Developing a Visual Interactive Search History Exploration System</title><categories>cs.IR cs.HC</categories><comments>KNOWeSCAPE 2013, 2 pages, 1 figure</comments><msc-class>68P20</msc-class><acm-class>H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As users advance in their search within a system, different queries are
conducted and various results are examined by them. These objects form an
implicit individual library representing the acquired knowledge. In our
research we aim to supply the user with visualizations of the search history
and interaction methods to organize the history. The fundamental question is
what role search history exploration can play in the users search process. In
this paper we want to introduce Ideas of a prototypical system for search
history exploration and discuss methods to address the questions mentioned
above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0680</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0680</id><created>2013-11-04</created><updated>2013-12-28</updated><authors><author><keyname>Hawelka</keyname><forenames>Bartosz</forenames></author><author><keyname>Sitko</keyname><forenames>Izabela</forenames></author><author><keyname>Beinat</keyname><forenames>Euro</forenames></author><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Kazakopoulos</keyname><forenames>Pavlos</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>Geo-located Twitter as the proxy for global mobility patterns</title><categories>cs.SI physics.soc-ph</categories><comments>17 pages, 13 figures</comments><doi>10.1080/15230406.2014.890072</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the advent of a pervasive presence of location sharing services
researchers gained an unprecedented access to the direct records of human
activity in space and time. This paper analyses geo-located Twitter messages in
order to uncover global patterns of human mobility. Based on a dataset of
almost a billion tweets recorded in 2012 we estimate volumes of international
travelers in respect to their country of residence. We examine mobility
profiles of different nations looking at the characteristics such as mobility
rate, radius of gyration, diversity of destinations and a balance of the
inflows and outflows. The temporal patterns disclose the universal seasons of
increased international mobility and the peculiar national nature of overseen
travels. Our analysis of the community structure of the Twitter mobility
network, obtained with the iterative network partitioning, reveals spatially
cohesive regions that follow the regional division of the world. Finally, we
validate our result with the global tourism statistics and mobility models
provided by other authors, and argue that Twitter is a viable source to
understand and quantify global mobility patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0681</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0681</id><created>2013-11-04</created><authors><author><keyname>Gil</keyname><forenames>A.</forenames></author><author><keyname>Segura</keyname><forenames>J.</forenames></author><author><keyname>Temme</keyname><forenames>N. M.</forenames></author></authors><title>Computation of the Marcum Q-function</title><categories>cs.MS math.CA</categories><comments>Accepted for publication in ACM Trans. Math. Softw</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methods and an algorithm for computing the generalized Marcum $Q-$function
($Q_{\mu}(x,y)$) and the complementary function ($P_{\mu}(x,y)$) are described.
These functions appear in problems of different technical and scientific areas
such as, for example, radar detection and communications, statistics and
probability theory, where they are called the non-central chi-square or the non
central gamma cumulative distribution functions.
  The algorithm for computing the Marcum functions combines different methods
of evaluation in different regions: series expansions, integral
representations, asymptotic expansions, and use of three-term homogeneous
recurrence relations. A relative accuracy close to $10^{-12}$ can be obtained
in the parameter region $(x,y,\mu) \in [0,\,A]\times [0,\,A]\times [1,\,A]$,
$A=200$, while for larger parameters the accuracy decreases (close to
$10^{-11}$ for $A=1000$ and close to $5\times 10^{-11}$ for $A=10000$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0701</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0701</id><created>2013-11-04</created><updated>2014-03-05</updated><authors><author><keyname>Bayer</keyname><forenames>Justin</forenames></author><author><keyname>Osendorfer</keyname><forenames>Christian</forenames></author><author><keyname>Korhammer</keyname><forenames>Daniela</forenames></author><author><keyname>Chen</keyname><forenames>Nutan</forenames></author><author><keyname>Urban</keyname><forenames>Sebastian</forenames></author><author><keyname>van der Smagt</keyname><forenames>Patrick</forenames></author></authors><title>On Fast Dropout and its Applicability to Recurrent Networks</title><categories>stat.ML cs.LG cs.NE</categories><comments>The experiments for the Penn Treebank corpus were erroneous and have
  been stripped from this version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent Neural Networks (RNNs) are rich models for the processing of
sequential data. Recent work on advancing the state of the art has been focused
on the optimization or modelling of RNNs, mostly motivated by adressing the
problems of the vanishing and exploding gradients. The control of overfitting
has seen considerably less attention. This paper contributes to that by
analyzing fast dropout, a recent regularization method for generalized linear
models and neural networks from a back-propagation inspired perspective. We
show that fast dropout implements a quadratic form of an adaptive,
per-parameter regularizer, which rewards large weights in the light of
underfitting, penalizes them for overconfident predictions and vanishes at
minima of an unregularized training loss. The derivatives of that regularizer
are exclusively based on the training error signal. One consequence of this is
the absense of a global weight attractor, which is particularly appealing for
RNNs, since the dynamics are not biased towards a certain regime. We positively
test the hypothesis that this improves the performance of RNNs on four musical
data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0707</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0707</id><created>2013-11-04</created><updated>2014-02-14</updated><authors><author><keyname>Br&#xfc;mmer</keyname><forenames>Niko</forenames></author><author><keyname>Garcia-Romero</keyname><forenames>Daniel</forenames></author></authors><title>Generative Modelling for Unsupervised Score Calibration</title><categories>stat.ML cs.LG</categories><comments>Accepted for ICASSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Score calibration enables automatic speaker recognizers to make
cost-effective accept / reject decisions. Traditional calibration requires
supervised data, which is an expensive resource. We propose a 2-component GMM
for unsupervised calibration and demonstrate good performance relative to a
supervised baseline on NIST SRE'10 and SRE'12. A Bayesian analysis demonstrates
that the uncertainty associated with the unsupervised calibration parameter
estimates is surprisingly small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0709</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0709</id><created>2013-11-04</created><authors><author><keyname>Wang</keyname><forenames>Yanshan</forenames></author></authors><title>A novel soft keyboard for touchscreen phones: QWERT</title><categories>cs.HC</categories><journal-ref>Int. J. of Human Factors and Ergonomics, 2013 Vol.2, No.4, pp.246
  - 261</journal-ref><doi>10.1504/IJHFE.2013.059374</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The popularity of touchscreen phones has been growing around the world since
the iPhones and Android phones were released. More and more mobile phones with
large touchscreen have been produced, however, the phones with small size
displays are still in the majority of touch phones. The foremost interface on
touch smartphones is the information input module using soft keyboards.
Traditional input methods on touch phones have either too small key buttons
(such as QWERTY) or too many functions (such as 3$\times$4 keyboard), which are
inconvenient to use. Moreover, the conventional soft keyboards only use tapping
to input texts while current touch smartphones allow various gestures on the
touchscreen, such as sliding. In this paper, a novel soft keyboard called QWERT
is proposed for touchscreen-based smartphones. The users can interact with
phones via finger gestures of tapping or sliding when input text by using the
QWERT. In doing so, the interactions between users and smartphones will be
faster and easier. An experiment carried out on inexperienced human subjects
shows that they can learn very fast due to their familiarities with QWERTY. A
simulation experiment based on a cognitive architecture, ACT-R, was also
conducted to predict the movement time (MT) of experienced human subjects. The
simulation results show that the MT using QWERT outperforms other default
keyboards. These outcomes imply that the novel QWERT is a viable option for
touch smartphone users. Based on the novel design, an application is released
on Android systems. This application is expected to give better user experience
for customers who use touch smartphones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0710</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0710</id><created>2013-11-04</created><authors><author><keyname>Cabrer</keyname><forenames>L. M.</forenames></author><author><keyname>Craig</keyname><forenames>A. P. K.</forenames></author><author><keyname>Priestley</keyname><forenames>H. A.</forenames></author></authors><title>Product representation for default bilattices: an application of natural
  duality theory</title><categories>math.RA cs.LO</categories><msc-class>Primary: 06D50, Secondary: 08C20, 03G25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bilattices (that is, sets with two lattice structures) provide an algebraic
tool to model simultaneously the validity of, and knowledge about, sentences in
an appropriate language. In particular, certain bilattices have been used to
model situations in which information is prioritised and so can be viewed
hierarchically. These default bilattices are not interlaced: the lattice
operations of one lattice structure do not preserve the order of the other one.
The well-known product representation theorem for interlaced bilattices does
not extend to bilattices which fail to be interlaced and the lack of a product
representation has been a handicap to understanding the structure of default
bilattices. In this paper we study, from an algebraic perspective, a hierarchy
of varieties of default bilattices, allowing for different levels of default.
We develop natural dualities for these varieties and thereby obtain a concrete
representation for the algebras in each variety. This leads on to a form of
product representation that generalises the product representation as this
applies to distributive bilattices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0713</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0713</id><created>2013-11-04</created><authors><author><keyname>Gandhi</keyname><forenames>Rajiv</forenames></author><author><keyname>Kortsarz</keyname><forenames>G.</forenames></author></authors><title>Edge covering with budget constrains</title><categories>cs.DS</categories><comments>17 pages</comments><msc-class>68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study two related problems: finding a set of k vertices and minimum number
of edges (kmin) and finding a graph with at least m' edges and minimum number
of vertices (mvms).
  Goldschmidt and Hochbaum \cite{GH97} show that the mvms problem is NP-hard
and they give a 3-approximation algorithm for the problem. We improve
\cite{GH97} by giving a ratio of 2. A 2(1+\epsilon)-approximation for the
problem follows from the work of Carnes and Shmoys \cite{CS08}. We improve the
approximation ratio to 2. algorithm for the problem. We show that the natural
LP for \kmin has an integrality gap of 2-o(1). We improve the NP-completeness
of \cite{GH97} by proving the pronlem are APX-hard unless a well-known instance
of the dense k-subgraph admits a constant ratio. The best approximation
guarantee known for this instance of dense k-subgraph is O(n^{2/9})
\cite{BCCFV}. We show that for any constant \rho&gt;1, an approximation guarantee
of \rho for the \kmin problem implies a \rho(1+o(1)) approximation for \mwms.
Finally, we define we give an exact algorithm for the density version of kmin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0716</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0716</id><created>2013-10-30</created><authors><author><keyname>Laufer</keyname><forenames>Michael Swan</forenames></author></authors><title>Artificial Intelligence in Humans</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, I put forward that in many instances, thinking mechanisms are
equivalent to artificial intelligence modules programmed into the human mind.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0728</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0728</id><created>2013-11-04</created><authors><author><keyname>El-Nashar</keyname><forenames>Alaa I.</forenames></author><author><keyname>Masaki</keyname><forenames>Nakamura</forenames></author></authors><title>To parallelize or not to parallelize, bugs issue</title><categories>cs.PL cs.DC</categories><journal-ref>International Journal of Intelligent Computing and Information
  Science, Egypt, IJICIS, Vol.10, No. 2, JULY 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Program correctness is one of the most difficult challenges in parallel
programming. Message Passing Interface MPI is widely used in writing parallel
applications. Since MPI is not a compiled language, the programmer will be
enfaced with several programming bugs.This paper presents the most common
programming bugs arise in MPI programs to help the programmer to compromise
between the advantage of parallelism and the extra effort needed to detect and
fix such bugs. An algebraic specification of an MPI-like programming language,
called Simple MPI (SMPI), to be used in writing MPI programs specification has
also been proposed. In addition, both nondeterminacy and deadlocks arise in
SMPI programs have been verified using Maud system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0731</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0731</id><created>2013-11-04</created><authors><author><keyname>Elnashar</keyname><forenames>Alaa I.</forenames></author></authors><title>To parallelize or not to parallelize, control and data flow issue</title><categories>cs.PL cs.DC</categories><journal-ref>International Journal of Intelligent Computing and Information
  Science IJICS, Egypt, v. 9, no. 2, July 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New trends towards multiple core processors imply using standard programming
models to develop efficient, reliable and portable programs for distributed
memory multiprocessors and workstation PC clusters. Message passing using MPI
is widely used to write efficient, reliable and portable applications. Control
and data flow analysis concepts, techniques and tools are needed to understand
and analyze MPI programs. If our point of interest is the program control and
data flow analysis, to decide to parallelize or not to parallelize our
applications, there is a question to be answered, &quot; Can the existing concepts,
techniques and tools used to analyze sequential programs also be used to
analyze parallel ones written in MPI?&quot;. In this paper we'll try to answer this
question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0750</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0750</id><created>2013-11-04</created><updated>2015-07-09</updated><authors><author><keyname>Schmidt</keyname><forenames>Jens M.</forenames></author></authors><title>The Mondshein Sequence</title><categories>cs.DS cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Canonical orderings [STOC'88, FOCS'92] have been used as a key tool in graph
drawing, graph encoding and visibility representations for the last decades. We
study a far-reaching generalization of canonical orderings to non-planar graphs
that was published by Lee Mondshein in a PhD-thesis at M.I.T. as early as 1971.
  Mondshein proposed to order the vertices of a graph in a sequence such that,
for any i, the vertices from 1 to i induce essentially a 2-connected graph
while the remaining vertices from i+1 to n induce a connected graph.
Mondshein's sequence generalizes canonical orderings and became later and
independently known under the name non-separating ear decomposition.
Surprisingly, this fundamental link between canonical orderings and
non-separating ear decomposition has not been established before. Currently,
the fastest known algorithm for computing a Mondshein sequence achieves a
running time of O(nm); the main open problem in Mondshein's and follow-up work
is to improve this running time to subquadratic time.
  After putting Mondshein's work into context, we present an algorithm that
computes a Mondshein sequence in optimal time and space O(m). This improves the
previous best running time by a factor of n. We illustrate the impact of this
result by deducing linear-time algorithms for five other problems, for four out
of which the previous best running times have been quadratic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0758</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0758</id><created>2013-11-04</created><authors><author><keyname>Morvan</keyname><forenames>Gildas</forenames></author><author><keyname>Veremme</keyname><forenames>Alexandre</forenames></author><author><keyname>Dupont</keyname><forenames>Daniel</forenames></author></authors><title>Observation of large-scale multi-agent based simulations</title><categories>cs.MA</categories><journal-ref>Multi-Agent-Based Simulation XII, LNCS 7124, 2012, pp 103-112</journal-ref><doi>10.1007/978-3-642-28400-7_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computational cost of large-scale multi-agent based simulations (MABS)
can be extremely important, especially if simulations have to be monitored for
validation purposes. In this paper, two methods, based on self-observation and
statistical survey theory, are introduced in order to optimize the computation
of observations in MABS. An empirical comparison of the computational cost of
these methods is performed on a toy problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0766</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0766</id><created>2013-11-04</created><authors><author><keyname>Devadoss</keyname><forenames>Satyan L.</forenames></author><author><keyname>Huang</keyname><forenames>Daoji</forenames></author><author><keyname>Spadacene</keyname><forenames>Dominic</forenames></author></authors><title>Polyhedral Covers of Tree Space</title><categories>math.CO cs.DM q-bio.PE</categories><comments>8 pages, 9 figures</comments><journal-ref>SIAM Journal of Discrete Mathematics 28 (2014) 1508 - 1514</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The phylogenetic tree space, introduced by Billera, Holmes, and Vogtmann, is
a cone over a simplicial complex. In this short article, we construct this
complex from local gluings of classical polytopes, the associahedron and the
permutohedron. Its homotopy is also reinterpreted and calculated based on
polytope data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0768</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0768</id><created>2013-11-04</created><updated>2014-01-06</updated><authors><author><keyname>Jeannet</keyname><forenames>Bertrand</forenames></author><author><keyname>Schrammel</keyname><forenames>Peter</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Sriram</forenames></author></authors><title>Abstract Acceleration of General Linear Loops</title><categories>cs.PL</categories><comments>Extended version with appendices of paper accepted to POPL'2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present abstract acceleration techniques for computing loop invariants for
numerical programs with linear assignments and conditionals. Whereas abstract
interpretation techniques typically over-approximate the set of reachable
states iteratively, abstract acceleration captures the effect of the loop with
a single, non-iterative transfer function applied to the initial states at the
loop head. In contrast to previous acceleration techniques, our approach
applies to any linear loop without restrictions. Its novelty lies in the use of
the Jordan normal form decomposition of the loop body to derive symbolic
expressions for the entries of the matrix modeling the effect of n&gt;=0
iterations of the loop. The entries of such a matrix depend on $n$ through
complex polynomial, exponential and trigonometric functions. Therefore, we
introduces an abstract domain for matrices that captures the linear inequality
relations between these complex expressions. This results in an abstract matrix
for describing the fixpoint semantics of the loop.
  Our approach integrates smoothly into standard abstract interpreters and can
handle programs with nested loops and loops containing conditional branches. We
evaluate it over small but complex loops that are commonly found in control
software, comparing it with other tools for computing linear loop invariants.
The loops in our benchmarks typically exhibit polynomial, exponential and
oscillatory behaviors that present challenges to existing approaches. Our
approach finds non-trivial invariants to prove useful bounds on the values of
variables for such loops, clearly outperforming the existing approaches in
terms of precision while exhibiting good performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0776</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0776</id><created>2013-11-04</created><updated>2015-12-06</updated><authors><author><keyname>Kairouz</keyname><forenames>Peter</forenames></author><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author></authors><title>The Composition Theorem for Differential Privacy</title><categories>cs.DS cs.CR cs.IT math.IT</categories><comments>32 pages 4 figures, Added a new section on private multi-party
  computation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequential querying of differentially private mechanisms degrades the overall
privacy level. In this paper, we answer the fundamental question of
characterizing the level of overall privacy degradation as a function of the
number of queries and the privacy levels maintained by each privatization
mechanism. Our solution is complete: we prove an upper bound on the overall
privacy level and construct a sequence of privatization mechanisms that
achieves this bound. The key innovation is the introduction of an operational
interpretation of differential privacy (involving hypothesis testing) and the
use of new data processing inequalities. Our result improves over the
state-of-the-art, and has immediate applications in several problems studied in
the literature including differentially private multi-party computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0787</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0787</id><created>2013-11-04</created><authors><author><keyname>Barcelo</keyname><forenames>Jaume</forenames></author><author><keyname>Faridi</keyname><forenames>Azadeh</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author><author><keyname>Martorell</keyname><forenames>Gabriel</forenames></author><author><keyname>Malone</keyname><forenames>David</forenames></author></authors><title>On the Distributed Construction of a Collision-Free Schedule in WLANs</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless local area networks (WLANs), a media access protocol arbitrates
access to the channel. In current IEEE 802.11 WLANs, carrier sense multiple
access with collision avoidance (CSMA/CA) is used. Carrier sense multiple
access with enhanced collision avoidance (CSMA/ECA) is a subtle variant of the
well-known CSMA/CA algorithm that offers substantial performance benefits.
CSMA/ECA significantly reduces the collision probability and, under certain
conditions, leads to a completely collision-free schedule. The only difference
between CSMA/CA and CSMA/ECA is that the latter uses a deterministic backoff
after successful transmissions. This deterministic backoff is a constant and is
the same for all the stations.
  The first part of the paper is of tutorial nature, offering an introduction
to the basic operation of CSMA/ECA and describing the benefits of this approach
in a qualitative manner. The second part of the paper surveys related
contributions, briefly summarizing the main challenges and potential solutions,
and also introducing variants and derivatives of CSMA/ECA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0790</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0790</id><created>2013-11-04</created><updated>2014-02-17</updated><authors><author><keyname>Miller</keyname><forenames>Nicholas C.</forenames></author><author><keyname>Baczewski</keyname><forenames>Andrew D.</forenames></author><author><keyname>Albrecht</keyname><forenames>John D.</forenames></author><author><keyname>Shanker</keyname><forenames>Balasubramaniam</forenames></author></authors><title>A Discontinuous Galerkin Time Domain Framework for Periodic Structures
  Subject To Oblique Excitation</title><categories>cs.CE</categories><comments>Submitted to IEEE TAP on August 5th, 2013. Revision submitted on
  February 3rd, 2014</comments><doi>10.1109/TAP.2014.2324012</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A nodal Discontinuous Galerkin (DG) method is derived for the analysis of
time-domain (TD) scattering from doubly periodic PEC/dielectric structures
under oblique interrogation. Field transformations are employed to elaborate a
formalism that is free from any issues with causality that are common when
applying spatial periodic boundary conditions simultaneously with incident
fields at arbitrary angles of incidence. An upwind numerical flux is derived
for the transformed variables, which retains the same form as it does in the
original Maxwell problem for domains without explicitly imposed periodicity.
This, in conjunction with the amenability of the DG framework to non-conformal
meshes, provides a natural means of accurately solving the first order TD
Maxwell equations for a number of periodic systems of engineering interest.
Results are presented that substantiate the accuracy and utility of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0798</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0798</id><created>2013-11-04</created><authors><author><keyname>Blin</keyname><forenames>L&#xe9;lia</forenames><affiliation>LIP6</affiliation></author><author><keyname>Dolev</keyname><forenames>Shlomi</forenames><affiliation>LIP6</affiliation></author><author><keyname>Potop-Butucaru</keyname><forenames>Maria Gradinariu</forenames><affiliation>LIP6</affiliation></author><author><keyname>Rovedakis</keyname><forenames>Stephane</forenames><affiliation>CEDRIC</affiliation></author></authors><title>Fast Self-Stabilizing Minimum Spanning Tree Construction Using Compact
  Nearest Common Ancestor Labeling Scheme</title><categories>cs.DC cs.DS cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel self-stabilizing algorithm for minimum spanning tree (MST)
construction. The space complexity of our solution is $O(\log^2n)$ bits and it
converges in $O(n^2)$ rounds. Thus, this algorithm improves the convergence
time of previously known self-stabilizing asynchronous MST algorithms by a
multiplicative factor $\Theta(n)$, to the price of increasing the best known
space complexity by a factor $O(\log n)$. The main ingredient used in our
algorithm is the design, for the first time in self-stabilizing settings, of a
labeling scheme for computing the nearest common ancestor with only
$O(\log^2n)$ bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0800</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0800</id><created>2013-11-04</created><authors><author><keyname>Hillel</keyname><forenames>Eshcar</forenames></author><author><keyname>Karnin</keyname><forenames>Zohar</forenames></author><author><keyname>Koren</keyname><forenames>Tomer</forenames></author><author><keyname>Lempel</keyname><forenames>Ronny</forenames></author><author><keyname>Somekh</keyname><forenames>Oren</forenames></author></authors><title>Distributed Exploration in Multi-Armed Bandits</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study exploration in Multi-Armed Bandits in a setting where $k$ players
collaborate in order to identify an $\epsilon$-optimal arm. Our motivation
comes from recent employment of bandit algorithms in computationally intensive,
large-scale applications. Our results demonstrate a non-trivial tradeoff
between the number of arm pulls required by each of the players, and the amount
of communication between them. In particular, our main result shows that by
allowing the $k$ players to communicate only once, they are able to learn
$\sqrt{k}$ times faster than a single player. That is, distributing learning to
$k$ players gives rise to a factor $\sqrt{k}$ parallel speed-up. We complement
this result with a lower bound showing this is in general the best possible. On
the other extreme, we present an algorithm that achieves the ideal factor $k$
speed-up in learning performance, with communication only logarithmic in
$1/\epsilon$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0801</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0801</id><created>2013-11-04</created><updated>2014-07-22</updated><authors><author><keyname>Hogg</keyname><forenames>Tad</forenames></author></authors><title>Using Surface-Motions for Locomotion of Microscopic Robots in Viscous
  Fluids</title><categories>cs.RO physics.bio-ph</categories><comments>14 figures and two Quicktime animations of the locomotion methods
  described in the paper, each showing one period of the motion over a time of
  0.5 milliseconds; version 2 has minor clarifications and corrected typos</comments><journal-ref>J. of Micro-Bio Robotics 9(3) 61-77 (2014)</journal-ref><doi>10.1007/s12213-014-0074-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microscopic robots could perform tasks with high spatial precision, such as
acting in biological tissues on the scale of individual cells, provided they
can reach precise locations. This paper evaluates the feasibility of in vivo
locomotion for micron-size robots. Two appealing methods rely only on surface
motions: steady tangential motion and small amplitude oscillations. These
methods contrast with common microorganism propulsion based on flagella or
cilia, which are more likely to damage nearby cells if used by robots made of
stiff materials. The power potentially available to robots in tissue supports
speeds ranging from one to hundreds of microns per second, over the range of
viscosities found in biological tissue. We discuss design trade-offs among
propulsion method, speed, power, shear forces and robot shape, and relate those
choices to robot task requirements. This study shows that realizing such
locomotion requires substantial improvements in fabrication capabilities and
material properties over current technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0803</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0803</id><created>2013-10-31</created><updated>2014-09-20</updated><authors><author><keyname>Makowski</keyname><forenames>Marcin</forenames></author><author><keyname>Piotrowski</keyname><forenames>Edward W.</forenames></author></authors><title>When &quot;I cut, you choose&quot; method implies intransitivity</title><categories>cs.GT</categories><journal-ref>Physica A: Statistical Mechanics and its Applications (2014) 415C
  pp. 189-193</journal-ref><doi>10.1016/j.physa.2014.05.074</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a common belief that humans and many animals follow transitive
inference (choosing A over C on the basis of knowing that A is better than B
and B is better than C). Transitivity seems to be the essence of rational
choice. We present a theoretical model of a repeated game in which the players
make a choice between three goods (e.g. food). The rules of the game refer to
the simple procedure of fair division among two players, known as the &quot;I cut,
you choose&quot; mechanism which has been widely discussed in the literature. In
this game one of the players has to make intransitive choices in order to
achieve the optimal result (for him/her and his/her co-player). The point is
that an intransitive choice can be rational. Previously, an increase in the
significance of intransitive strategies was achieved by referring to models of
quantum games. We show that \textit{relevant intransitive strategies} also
appear in the classic description of decision algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0804</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0804</id><created>2013-11-04</created><authors><author><keyname>Barreiro</keyname><forenames>Enrique Wulff</forenames><affiliation>ICMAN</affiliation></author></authors><title>Innovation \'educative en sciences de l'information</title><categories>cs.CY</categories><comments>35 pag</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concerning its development in the virtual classroom, the web 2.0 educational
innovation means the use and the production of textbooks and the
personalisation of the classnotes. The controversy, that is a precondition of
awareness, organized around the assignment of knowledge to a central authority
vs its grant to individuals who need it to share their plans with others, would
meet the present dynamism of e-learning. To introduce these training strategies
with scientific information in marine sciences, an online course was
transformed into an opportunity for evaluating and living open access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0805</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0805</id><created>2013-11-04</created><updated>2013-11-16</updated><authors><author><keyname>Ivanov</keyname><forenames>Todor</forenames></author><author><keyname>Korfiatis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Zicari</keyname><forenames>Roberto V.</forenames></author></authors><title>On the inequality of the 3V's of Big Data Architectural Paradigms: A
  case for heterogeneity</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The well-known 3V architectural paradigm for Big Data introduced by Laney
(2011), provides a simplified framework for defining the architecture of a big
data platform to be deployed in various scenarios tackling processing of
massive datasets. While additional components such as Variability and Veracity
have been discussed as an extension to the 3V model, the basic components
(volume, variety, velocity) provide a quantitative framework while variability
and veracity target a more qualitative approach. In this paper we argue why the
basic 3V's are not equal due to the different requirements that need to be
covered in case higher demands for a particular &quot;V&quot;. Similar to other
conjectures such as the CAP theorem 3V based architectures differ on their
implementation. We call this paradigm heterogeneity and we provide a taxonomy
of the existing tools (as of 2013) covering the Hadoop ecosystem from the
perspective of heterogeneity. This paper contributes on the understanding of
the Hadoop ecosystem from the perspective of different workloads and aims to
help researchers and practitioners on the design of scalable platforms
targeting different operational needs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0806</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0806</id><created>2013-11-04</created><authors><author><keyname>Fiard</keyname><forenames>Gaelle</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Selmi</keyname><forenames>Sonia-Yuki</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Promayon</keyname><forenames>Emmanuel</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Vadcard</keyname><forenames>Lucile</forenames><affiliation>LSE - UPMF</affiliation></author><author><keyname>Descotes</keyname><forenames>Jean-Luc</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Troccaz</keyname><forenames>Jocelyne</forenames><affiliation>TIMC-IMAG</affiliation></author></authors><title>Initial validation of a virtual-reality learning environment for
  prostate biopsies: realism matters!</title><categories>cs.CY</categories><proxy>ccsd</proxy><journal-ref>Journal of Endourology and Part B, Videourology (2013) epub ahead
  of print</journal-ref><doi>10.1089/end.2013.0454</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  : Introduction-objectives: A virtual-reality learning environment dedicated
to prostate biopsies was designed to overcome the limitations of current
classical teaching methods. The aim of this study was to validate reliability,
face, content and construct of the simulator. Materials and methods: The
simulator is composed of a) a laptop computer, b) a haptic device with a stylus
that mimics the ultrasound probe, c) a clinical case database including three
dimensional (3D) ultrasound volumes and patient data and d) a learning
environment with a set of progressive exercises including a randomized 12-core
biopsy procedure. Both visual (3D biopsy mapping) and numerical (score)
feedback are given to the user. The simulator evaluation was conducted in an
academic urology department on 7 experts and 14 novices who each performed a
virtual biopsy procedure and completed a face and content validity
questionnaire. Results: The overall realism of the biopsy procedure was rated
at a median of 9/10 by non-experts (7.1-9.8). Experts rated the usefulness of
the simulator for the initial training of urologists at 8.2/10 (7.9-8.3), but
reported the range of motion and force feedback as significantly less realistic
than novices (p=0.01 and 0.03 respectively). Pearson's r correlation
coefficient between correctly placed biopsies on the right and left side of the
prostate for each user was 0.79 (p&lt;0.001). The 7 experts had a median score of
64% (59-73), and the 14 novices a median score of 52% (43-67), without reaching
statistical significance (p=0,19). Conclusion: The newly designed virtual
reality learning environment proved its versatility and its reliability, face
and content were validated. Demonstrating the construct validity will require
improvements to the realism and scoring system used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0810</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0810</id><created>2013-11-04</created><updated>2014-01-19</updated><authors><author><keyname>Bouchaud</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Borghesi</keyname><forenames>Christian</forenames></author><author><keyname>Jensen</keyname><forenames>Pablo</forenames></author></authors><title>On the emergence of an &quot;intention field&quot; for socially cohesive agents</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>10 pages, 3 figures</comments><doi>10.1088/1742-5468/2014/03/P03010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We argue that when a social convergence mechanism exists and is strong
enough, one should expect the emergence of a well defined &quot;field&quot;, i.e. a
slowly evolving, local quantity around which individual attributes fluctuate in
a finite range. This condensation phenomenon is well illustrated by the
Deffuant-Weisbuch opinion model for which we provide a natural extension to
allow for spatial heterogeneities. We show analytically and numerically that
the resulting dynamics of the emergent field is a noisy diffusion equation that
has a slow dynamics. This random diffusion equation reproduces the long-ranged,
logarithmic decrease of the correlation of spatial voting patterns empirically
found in [1, 2]. Interestingly enough, we find that when the social cohesion
mechanism becomes too weak, cultural cohesion breaks down completely, in the
sense that the distribution of intentions/opinions becomes infinitely broad. No
emerging field exists in this case. All these analytical findings are confirmed
by numerical simulations of an agent-based model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0819</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0819</id><created>2013-11-04</created><authors><author><keyname>Such</keyname><forenames>Ondrej</forenames></author><author><keyname>Skvarek</keyname><forenames>Ondrej</forenames></author><author><keyname>Klimo</keyname><forenames>Martin</forenames></author></authors><title>Phoneme discrimination using neurons with symmetric nonlinear response
  over a spectral range</title><categories>cs.SD</categories><comments>5 pages, ICASSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the ability of a very simple feed-forward neural network to
discriminate phonemes based on just relative power spectrum. The network
consists of two neurons with symmetric nonlinear response over a spectral
range. The output of the neurons is subsequently fed to a comparator. We show
that often this is enough to achieve complete separation of data. We compare
the performance of found discriminants with that of more general neurons. Our
conclusion is that not much is gained in passing to real-valued weights. More
likely higher number of neurons and preprocessing of input will yield better
discrimination results. The networks considered are directly amenable to
hardware (neuromorphic) designs. Other advantages include interpretability,
guarantees of performance on unseen data and low Kolmogorov complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0822</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0822</id><created>2013-11-04</created><authors><author><keyname>Nunes</keyname><forenames>C. A. J.</forenames></author><author><keyname>Estevez-Rams</keyname><forenames>E.</forenames></author><author><keyname>Fern&#xe1;ndez</keyname><forenames>B. Arag&#xf3;n</forenames></author><author><keyname>Serrano</keyname><forenames>R. Lora</forenames></author></authors><title>Properties of maximum Lempel-Ziv complexity strings</title><categories>nlin.CD cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The properties of maximum Lempel-Ziv complexity strings are studied for the
binary case. A comparison between MLZs and random strings is carried out. The
length profile of both type of sequences show different distribution functions.
The non-stationary character of the MLZs are discussed. The issue of
sensitiveness to noise is also addressed. An empirical ansatz is found that
fits well to the Lempel-Ziv complexity of the MLZs for all lengths up to $10^6$
symbols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0830</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0830</id><created>2013-11-04</created><updated>2013-11-05</updated><authors><author><keyname>Oymak</keyname><forenames>Samet</forenames></author><author><keyname>Thrampoulidis</keyname><forenames>Christos</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>The Squared-Error of Generalized LASSO: A Precise Analysis</title><categories>cs.IT math.IT math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating an unknown signal $x_0$ from noisy
linear observations $y = Ax_0 + z\in R^m$. In many practical instances, $x_0$
has a certain structure that can be captured by a structure inducing convex
function $f(\cdot)$. For example, $\ell_1$ norm can be used to encourage a
sparse solution. To estimate $x_0$ with the aid of $f(\cdot)$, we consider the
well-known LASSO method and provide sharp characterization of its performance.
We assume the entries of the measurement matrix $A$ and the noise vector $z$
have zero-mean normal distributions with variances $1$ and $\sigma^2$
respectively. For the LASSO estimator $x^*$, we attempt to calculate the
Normalized Square Error (NSE) defined as $\frac{\|x^*-x_0\|_2^2}{\sigma^2}$ as
a function of the noise level $\sigma$, the number of observations $m$ and the
structure of the signal. We show that, the structure of the signal $x_0$ and
choice of the function $f(\cdot)$ enter the error formulae through the summary
parameters $D(cone)$ and $D(\lambda)$, which are defined as the Gaussian
squared-distances to the subdifferential cone and to the $\lambda$-scaled
subdifferential, respectively. The first LASSO estimator assumes a-priori
knowledge of $f(x_0)$ and is given by $\arg\min_{x}\{{\|y-Ax\|_2}~\text{subject
to}~f(x)\leq f(x_0)\}$. We prove that its worst case NSE is achieved when
$\sigma\rightarrow 0$ and concentrates around $\frac{D(cone)}{m-D(cone)}$.
Secondly, we consider $\arg\min_{x}\{\|y-Ax\|_2+\lambda f(x)\}$, for some
$\lambda\geq 0$. This time the NSE formula depends on the choice of $\lambda$
and is given by $\frac{D(\lambda)}{m-D(\lambda)}$. We then establish a mapping
between this and the third estimator $\arg\min_{x}\{\frac{1}{2}\|y-Ax\|_2^2+
\lambda f(x)\}$. Finally, for a number of important structured signal classes,
we translate our abstract formulae to closed-form upper bounds on the NSE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0833</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0833</id><created>2013-11-04</created><authors><author><keyname>Liu</keyname><forenames>Zitao</forenames></author></authors><title>A Comparative Study on Linguistic Feature Selection in Sentiment
  Polarity Classification</title><categories>cs.CL</categories><comments>arXiv admin note: text overlap with arXiv:cs/0205070 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sentiment polarity classification is perhaps the most widely studied topic.
It classifies an opinionated document as expressing a positive or negative
opinion. In this paper, using movie review dataset, we perform a comparative
study with different single kind linguistic features and the combinations of
these features. We find that the classic topic-based classifier(Naive Bayes and
Support Vector Machine) do not perform as well on sentiment polarity
classification. And we find that with some combination of different linguistic
features, the classification accuracy can be boosted a lot. We give some
reasonable explanations about these boosting outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0841</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0841</id><created>2013-11-04</created><updated>2013-11-05</updated><authors><author><keyname>Dobos</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>Sz&#xfc;le</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>Bodn&#xe1;r</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Hanyecz</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Seb&#x151;k</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Kondor</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>Kallus</keyname><forenames>Zs&#xf3;fia</forenames></author><author><keyname>St&#xe9;ger</keyname><forenames>J&#xf3;zsef</forenames></author><author><keyname>Csabai</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>Vattay</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>A multi-terabyte relational database for geo-tagged social network data</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite their relatively low sampling factor, the freely available, randomly
sampled status streams of Twitter are very useful sources of geographically
embedded social network data. To statistically analyze the information Twitter
provides via these streams, we have collected a year's worth of data and built
a multi-terabyte relational database from it. The database is designed for fast
data loading and to support a wide range of studies focusing on the statistics
and geographic features of social networks, as well as on the linguistic
analysis of tweets. In this paper we present the method of data collection, the
database design, the data loading procedure and special treatment of geo-tagged
and multi-lingual data. We also provide some SQL recipes for computing network
statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0842</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0842</id><created>2013-11-04</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Mayukh</forenames></author><author><keyname>Ranjan</keyname><forenames>Om</forenames></author></authors><title>An Intuitive Design Approach For Implementing Real Time Audio Effects</title><categories>cs.SD</categories><comments>12 pages,11 figures,4 tables,24 equations,9 references</comments><journal-ref>International Journal of Advances in Engineering &amp; Technology
  (IJAET), Volume 6 Issue 5(29), pp. 2216-2227, Nov. 2013</journal-ref><doi>10.7323/ijaet/v6_iss5</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Audio effect implementation on random musical signal is a basic application
of digital signal processors. In this paper, the compatibility features of
MATLAB R2008a with Code Composer Studio 3.3 has been exploited to develop
Simulink models which when emulated on TMS320C6713 DSK generate real time audio
effects. Each design has been done by two different asynchronous scheduling
techniques: (i) Idle task Scheduling and (ii) DSP/BIOS task Scheduling. A basic
COCOMO analysis has been done for the generated code to justify the industrial
viability of this design approach.
  KEYWORDS: Musical signal processing, Real time Audio effects, Echo, Stress
Generation, Reverberation, Reverberated Chorus, Real Time Scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0843</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0843</id><created>2013-11-04</created><authors><author><keyname>Hasan</keyname><forenames>Monowar</forenames><affiliation>Department of Electrical and Computer Engineering, University of Manitoba, Winnipeg, Canada</affiliation></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames><affiliation>Department of Electrical and Computer Engineering, University of Manitoba, Winnipeg, Canada</affiliation></author></authors><title>Resource Allocation for Network-Integrated Device-to-Device
  Communications Using Smart Relays</title><categories>cs.NI</categories><journal-ref>International Workshop on Device-to-Device (D2D) Communication
  With and Without Infrastructure (GC13 WS - D2D) in conjunction with IEEE
  Globecom'13, Atlanta, USA, 9-13 December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With increasing number of autonomous heterogeneous devices in future mobile
networks, an efficient resource allocation scheme is required to maximize
network throughput and achieve higher spectral efficiency. In this paper,
performance of network-integrated device-to-device (D2D) communication is
investigated where D2D traffic is carried through relay nodes. An optimization
problem is formulated for allocating radio resources to maximize end-to-end
rate as well as conversing QoS requirements for cellular and D2D user equipment
under total power constraint. Numerical results show that there is a distance
threshold beyond which relay-assisted D2D communication significantly improves
network performance when compared to direct communication between D2D peers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0849</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0849</id><created>2013-11-04</created><updated>2014-07-08</updated><authors><author><keyname>De Biasi</keyname><forenames>Marzio</forenames></author><author><keyname>Yakaryilmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Unary languages recognized by two-way one-counter automata</title><categories>cs.FL cs.CC</categories><comments>14 pages. An improved version accepted to CIAA2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A two-way deterministic finite state automaton with one counter (2D1CA) is a
fundamental computational model that has been examined in many different
aspects since sixties, but we know little about its power in the case of unary
languages. Up to our knowledge, the only known unary nonregular languages
recognized by 2D1CAs are those formed by strings having exponential length,
where the exponents form some trivial unary regular language. In this paper, we
present some non-trivial subsets of these languages. By using the input head as
a second counter, we present simulations of two-way deterministic finite
automata with linearly bounded counters and linear--space Turing machines. We
also show how a fixed-size quantum register can help to simplify some of these
languages. Finally, we compare unary 2D1CAs with two--counter machines and
provide some insights about the limits of their computational power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0864</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0864</id><created>2013-11-04</created><authors><author><keyname>Elnashar</keyname><forenames>Alaa Ismail</forenames></author><author><keyname>Aljahdali</keyname><forenames>Sultan</forenames></author><author><keyname>Sadhan</keyname><forenames>Mosaid Al</forenames></author></authors><title>Two automated techniques for analyzing and debugging Mpi-based programs</title><categories>cs.PL cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1311.0731</comments><report-no>978-1-880843-83-3/ISCA CAINE/November 2011</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Message Passing Interface (MPI) is the most commonly used paradigm in writing
parallel programs since it can be employed not only within a single processing
node but also across several connected ones. Data flow analysis concepts,
techniques and tools are needed to understand and analyze MPI-based programs to
detect bugs arise in these programs. In this paper we propose two automated
techniques to analyze and debug MPI-based programs source codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0897</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0897</id><created>2013-11-04</created><authors><author><keyname>Shuman</keyname><forenames>David I</forenames></author><author><keyname>Wiesmeyr</keyname><forenames>Christoph</forenames></author><author><keyname>Holighaus</keyname><forenames>Nicki</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Spectrum-Adapted Tight Graph Wavelet and Vertex-Frequency Frames</title><categories>math.FA cs.IT cs.SI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of designing spectral graph filters for the
construction of dictionaries of atoms that can be used to efficiently represent
signals residing on weighted graphs. While the filters used in previous
spectral graph wavelet constructions are only adapted to the length of the
spectrum, the filters proposed in this paper are adapted to the distribution of
graph Laplacian eigenvalues, and therefore lead to atoms with better
discriminatory power. Our approach is to first characterize a family of systems
of uniformly translated kernels in the graph spectral domain that give rise to
tight frames of atoms generated via generalized translation on the graph. We
then warp the uniform translates with a function that approximates the
cumulative spectral density function of the graph Laplacian eigenvalues. We use
this approach to construct computationally efficient, spectrum-adapted, tight
vertex-frequency and graph wavelet frames. We give numerous examples of the
resulting spectrum-adapted graph filters, and also present an illustrative
example of vertex-frequency analysis using the proposed construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0902</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0902</id><created>2013-11-04</created><authors><author><keyname>Aurzada</keyname><forenames>Frank</forenames></author><author><keyname>L&#xe9;vesque</keyname><forenames>Martin</forenames></author><author><keyname>Maier</keyname><forenames>Martin</forenames></author><author><keyname>Reisslein</keyname><forenames>Martin</forenames></author></authors><title>FiWi Access Networks Based on Next-Generation PON and Gigabit-Class WLAN
  Technologies: A Capacity and Delay Analysis (Extended Version)</title><categories>cs.IT cs.NI math.IT</categories><comments>Technical Report, School of Electrical, Computer, and Energy Eng.
  Arizona State University, Tempe</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current Gigabit-class passive optical networks (PONs) evolve into
next-generation PONs, whereby high-speed 10+ Gb/s time division multiplexing
(TDM) and long-reach wavelength-broadcasting/routing wavelength division
multiplexing (WDM) PONs are promising near-term candidates. On the other hand,
next-generation wireless local area networks (WLANs) based on frame aggregation
techniques will leverage physical layer enhancements, giving rise to
Gigabit-class very high throughput (VHT) WLANs. In this paper, we develop an
analytical framework for evaluating the capacity and delay performance of a
wide range of routing algorithms in converged fiber-wireless (FiWi) broadband
access networks based on different next-generation PONs and a Gigabit-class
multi-radio multi-channel WLAN-mesh front-end. Our framework is very flexible
and incorporates arbitrary frame size distributions, traffic matrices,
optical/wireless propagation delays, data rates, and fiber faults. We verify
the accuracy of our probabilistic analysis by means of simulation for the
wireless and wireless-optical-wireless operation modes of various FiWi network
architectures under peer-to-peer, upstream, uniform, and nonuniform traffic
scenarios. The results indicate that our proposed optimized FiWi routing
algorithm (OFRA) outperforms minimum (wireless) hop and delay routing in terms
of throughput for balanced and unbalanced traffic loads, at the expense of a
slightly increased mean delay at small to medium traffic loads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0909</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0909</id><created>2013-11-04</created><authors><author><keyname>Aurzada</keyname><forenames>Frank</forenames></author><author><keyname>Scheutzow</keyname><forenames>Michael</forenames></author><author><keyname>Reisslein</keyname><forenames>Martin</forenames></author><author><keyname>Ghazisaidi</keyname><forenames>Navid</forenames></author><author><keyname>Maier</keyname><forenames>Martin</forenames></author></authors><title>Capacity and Delay Analysis of Next-Generation Passive Optical Networks
  (NG-PONs) - Extended Version</title><categories>cs.IT cs.NI math.IT</categories><comments>Technical Report, School of Electrical, Computer, and Energy Eng.,
  Arizona State University, Tempe, AZ</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building on the Ethernet Passive Optical Network (EPON) and Gigabit PON
(GPON) standards, Next-Generation (NG) PONs (i) provide increased data rates,
split ratios, wavelengths counts, and fiber lengths, as well as (ii) allow for
all-optical integration of access and metro networks. In this paper we provide
a comprehensive probabilistic analysis of the capacity (maximum mean packet
throughput) and packet delay of subnetworks that can be used to form NG-PONs.
Our analysis can cover a wide range of NG-PONs through taking the minimum
capacity of the subnetworks making up the NG-PON and weighing the packet delays
of the subnetworks. Our numerical and simulation results indicate that our
analysis quite accurately characterizes the throughput-delay performance of
EPON/GPON tree networks, including networks upgraded with higher data rates and
wavelength counts. Our analysis also characterizes the trade-offs and
bottlenecks when integrating EPON/GPON tree networks across a metro area with a
ring, a Passive Star Coupler (PSC), or an Arrayed Waveguide Grating (AWG) for
uniform and non-uniform traffic. To the best of our knowledge, the presented
analysis is the first to consider multiple PONs interconnected via a metro
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0913</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0913</id><created>2013-11-04</created><updated>2016-02-04</updated><authors><author><keyname>Kalai</keyname><forenames>Gil</forenames></author><author><keyname>Meir</keyname><forenames>Reshef</forenames></author><author><keyname>Tennenholtz</keyname><forenames>Moshe</forenames></author></authors><title>Bidding Games and Efficient Allocations</title><categories>cs.GT</categories><comments>A preliminary version of this paper appeared in the 16th ACM
  Conference on Economics and Computation (EC-2015)</comments><acm-class>I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bidding games are extensive form games, where in each turn players bid in
order to determine who will play next. Zero-sum bidding games (also known as
Richman games) have been extensively studied, focusing on the fraction of the
initial budget that can guaranty the victory of each player [Lazarus et al.'99,
Develin and Payne'10].
  We extend the theory of bidding games to general-sum two player games,
showing the existence of pure subgame-perfect Nash equilibria (PSPE), and
studying their properties.
  We show that if the underlying game has the form of a binary tree (only two
actions available to the players in each node), then there exists a natural
PSPE with the following highly desirable properties: (a) players' utility is
weakly monotone in their budget; (b) a Pareto-efficient outcome is reached for
any initial budget; and (c) for any Pareto-efficient outcome there is an
initial budget s.t. this outcome is attained. In particular, we can assign the
budget so as to implement the outcome with maximum social welfare, maximum
Egalitarian welfare, etc.
  We show implications of this result for combinatorial bargaining. In
particular, we show that the PSPE above is fair, in the sense that a player
with a fraction of X% of the total budget prefers her allocation to X% of the
possible allocations.
  In addition, we discuss the computational challenges of bidding games, and
provide a polynomial-time algorithm to compute the PSPE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0914</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0914</id><created>2013-11-04</created><authors><author><keyname>Hsieh</keyname><forenames>Cho-Jui</forenames></author><author><keyname>Si</keyname><forenames>Si</forenames></author><author><keyname>Dhillon</keyname><forenames>Inderjit S.</forenames></author></authors><title>A Divide-and-Conquer Solver for Kernel Support Vector Machines</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The kernel support vector machine (SVM) is one of the most widely used
classification methods; however, the amount of computation required becomes the
bottleneck when facing millions of samples. In this paper, we propose and
analyze a novel divide-and-conquer solver for kernel SVMs (DC-SVM). In the
division step, we partition the kernel SVM problem into smaller subproblems by
clustering the data, so that each subproblem can be solved independently and
efficiently. We show theoretically that the support vectors identified by the
subproblem solution are likely to be support vectors of the entire kernel SVM
problem, provided that the problem is partitioned appropriately by kernel
clustering. In the conquer step, the local solutions from the subproblems are
used to initialize a global coordinate descent solver, which converges quickly
as suggested by our analysis. By extending this idea, we develop a multilevel
Divide-and-Conquer SVM algorithm with adaptive clustering and early prediction
strategy, which outperforms state-of-the-art methods in terms of training
speed, testing accuracy, and memory usage. As an example, on the covtype
dataset with half-a-million samples, DC-SVM is 7 times faster than LIBSVM in
obtaining the exact SVM solution (to within $10^{-6}$ relative error) which
achieves 96.15% prediction accuracy. Moreover, with our proposed early
prediction strategy, DC-SVM achieves about 96% accuracy in only 12 minutes,
which is more than 100 times faster than LIBSVM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0924</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0924</id><created>2013-11-04</created><authors><author><keyname>Babaioff</keyname><forenames>Moshe</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author><author><keyname>Leme</keyname><forenames>Renato Paes</forenames></author></authors><title>On the Efficiency of the Walrasian Mechanism</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Central results in economics guarantee the existence of efficient equilibria
for various classes of markets. An underlying assumption in early work is that
agents are price-takers, i.e., agents honestly report their true demand in
response to prices. A line of research in economics, initiated by Hurwicz
(1972), is devoted to understanding how such markets perform when agents are
strategic about their demands. This is captured by the \emph{Walrasian
Mechanism} that proceeds by collecting reported demands, finding clearing
prices in the \emph{reported} market via an ascending price t\^{a}tonnement
procedure, and returns the resulting allocation. Similar mechanisms are used,
for example, in the daily opening of the New York Stock Exchange and the call
market for copper and gold in London.
  In practice, it is commonly observed that agents in such markets reduce their
demand leading to behaviors resembling bargaining and to inefficient outcomes.
We ask how inefficient the equilibria can be. Our main result is that the
welfare of every pure Nash equilibrium of the Walrasian mechanism is at least
one quarter of the optimal welfare, when players have gross substitute
valuations and do not overbid. Previous analysis of the Walrasian mechanism
have resorted to large market assumptions to show convergence to efficiency in
the limit. Our result shows that approximate efficiency is guaranteed
regardless of the size of the market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0928</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0928</id><created>2013-11-04</created><authors><author><keyname>Aloupis</keyname><forenames>Greg</forenames></author><author><keyname>Iacono</keyname><forenames>John</forenames></author><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author><author><keyname>&#xd6;zkan</keyname><forenames>&#xd6;zg&#xfc;r</forenames></author></authors><title>The Complexity of Order Type Isomorphism</title><categories>cs.CG</categories><comments>Preliminary version of paper to appear at ACM-SIAM Symposium on
  Discrete Algorithms (SODA14)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The order type of a point set in $R^d$ maps each $(d{+}1)$-tuple of points to
its orientation (e.g., clockwise or counterclockwise in $R^2$). Two point sets
$X$ and $Y$ have the same order type if there exists a mapping $f$ from $X$ to
$Y$ for which every $(d{+}1)$-tuple $(a_1,a_2,\ldots,a_{d+1})$ of $X$ and the
corresponding tuple $(f(a_1),f(a_2),\ldots,f(a_{d+1}))$ in $Y$ have the same
orientation. In this paper we investigate the complexity of determining whether
two point sets have the same order type. We provide an $O(n^d)$ algorithm for
this task, thereby improving upon the $O(n^{\lfloor{3d/2}\rfloor})$ algorithm
of Goodman and Pollack (1983). The algorithm uses only order type queries and
also works for abstract order types (or acyclic oriented matroids). Our
algorithm is optimal, both in the abstract setting and for realizable points
sets if the algorithm only uses order type queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0942</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0942</id><created>2013-11-04</created><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Resource Allocation for Cost Minimization in Limited Feedback MU-MIMO
  Systems with Delay Guarantee</title><categories>cs.IT math.IT</categories><comments>20 pages, 3 figures, and 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design a resource allocation framework for the
delay-sensitive Multi-User MIMO (MU-MIMO) broadcast system with limited
feedback. Considering the scarcity and interrelation of the transmit power and
feedback bandwidth, it is imperative to optimize the two resources in a joint
and efficient manner while meeting the delay-QoS requirement. Based on the
effective bandwidth theory, we first obtain a closed-form expression of average
violation probability with respect to a given delay requirement as a function
of transmit power and codebook size of feedback channel. By minimizing the
total resource cost, we derive an optimal joint resource allocation scheme,
which can flexibly adjust the transmit power and feedback bandwidth according
to the characteristics of the system. Moreover, through asymptotic analysis,
some simple resource allocation schemes are presented. Finally, the theoretical
claims are validated by numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0944</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0944</id><created>2013-11-04</created><authors><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Connectivity for matroids based on rough sets</title><categories>cs.AI</categories><comments>16 pages, 8figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In mathematics and computer science, connectivity is one of the basic
concepts of matroid theory: it asks for the minimum number of elements which
need to be removed to disconnect the remaining nodes from each other. It is
closely related to the theory of network flow problems. The connectivity of a
matroid is an important measure of its robustness as a network. Therefore, it
is very necessary to investigate the conditions under which a matroid is
connected. In this paper, the connectivity for matroids is studied through
relation-based rough sets. First, a symmetric and transitive relation is
introduced from a general matroid and its properties are explored from the
viewpoint of matroids. Moreover, through the relation introduced by a general
matroid, an undirected graph is generalized. Specifically, the connection of
the graph can be investigated by the relation-based rough sets. Second, we
study the connectivity for matroids by means of relation-based rough sets and
some conditions under which a general matroid is connected are presented.
Finally, it is easy to prove that the connectivity for a general matroid with
some special properties and its induced undirected graph is equivalent. These
results show an important application of relation-based rough sets to matroids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0950</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0950</id><created>2013-11-04</created><updated>2013-11-07</updated><authors><author><keyname>Mishra</keyname><forenames>Kumar Vijay</forenames></author><author><keyname>Cho</keyname><forenames>Myung</forenames></author><author><keyname>Kruger</keyname><forenames>Anton</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author></authors><title>Off-The-Grid Spectral Compressed Sensing With Prior Information</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research in off-the-grid compressed sensing (CS) has demonstrated
that, under certain conditions, one can successfully recover a spectrally
sparse signal from a few time-domain samples even though the dictionary is
continuous. In this paper, we extend off-the-grid CS to applications where some
prior information about spectrally sparse signal is known. We specifically
consider cases where a few contributing frequencies or poles, but not their
amplitudes or phases, are known a priori. Our results show that equipping
off-the-grid CS with the known-poles algorithm can increase the probability of
recovering all the frequency components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0951</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0951</id><created>2013-11-04</created><authors><author><keyname>Su</keyname><forenames>Kai</forenames></author><author><keyname>Westphal</keyname><forenames>Cedric</forenames></author></authors><title>On the Benefit of Information Centric Networks for Traffic Engineering</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current Internet performs traffic engineering (TE) by estimating traffic
matrices on a regular schedule, and allocating flows based upon weights
computed from these matrices. This means the allocation is based upon a guess
of the traffic in the network based on its history. Information-Centric
Networks on the other hand provide a finer-grained description of the traffic:
a content between a client and a server is uniquely identified by its name, and
the network can therefore learn the size of different content items, and
perform traffic engineering and resource allocation accordingly. We claim that
Information-Centric Networks can therefore provide a better handle to perform
traffic engineering, resulting in significant performance gain.
  We present a mechanism to perform such resource allocation. We see that our
traffic engineering method only requires knowledge of the flow size (which, in
ICN, can be learned from previous data transfers) and outperforms a min-MLU
allocation in terms of response time. We also see that our method identifies
the traffic allocation patterns similar to that of min-MLU without having
access to the traffic matrix ahead of time. We show a very significant gain in
response time where min MLU is almost 50% slower than our ICN-based TE method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0955</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0955</id><created>2013-11-04</created><updated>2013-12-06</updated><authors><author><keyname>d'Eon</keyname><forenames>Eugene</forenames></author></authors><title>A Dual-Beam Method-of-Images 3D Searchlight BSSRDF</title><categories>cs.GR</categories><comments>added clarifying text and 1 figure to illustrate the method</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel BSSRDF for rendering translucent materials. Angular
effects lacking in previous BSSRDF models are incorporated by using a dual-beam
formulation. We employ a Placzek's Lemma interpretation of the method of images
and discard diffusion theory. Instead, we derive a plane-parallel
transformation of the BSSRDF to form the associated BRDF and optimize the image
confiurations such that the BRDF is close to the known analytic solutions for
the associated albedo problem. This ensures reciprocity, accurate colors, and
provides an automatic level-of-detail transition for translucent objects that
appear at various distances in an image. Despite optimizing the subsurface
fluence in a plane-parallel setting, we find that this also leads to fairly
accurate fluence distributions throughout the volume in the original 3D
searchlight problem. Our method-of-images modifications can also improve the
accuracy of previous BSSRDFs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0959</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0959</id><created>2013-11-04</created><authors><author><keyname>Bhattacharjee</keyname><forenames>Tapomayukh</forenames></author><author><keyname>Oh</keyname><forenames>Yonghwan</forenames></author><author><keyname>Oh</keyname><forenames>Sang-Rok</forenames></author></authors><title>Validation of a Control Algorithm for Human-like Reaching Motion using
  7-DOF Arm and 19-DOF Hand-Arm Systems</title><categories>cs.RO</categories><comments>This paper summarizes our work on the validation of a control
  algorithm for human-like reaching motion. This work was done at the
  Interaction and Robotics Research Center in Korea Institute of Science and
  Technology (KIST), South-Korea during 2010-2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical report gives an overview of our work on control algorithms
dealing with redundant robot systems for achieving human-like motion
characteristics. Previously, we developed a novel control law to exhibit
human-motion characteristics in redundant robot arm systems as well as
arm-trunk systems for reaching tasks [1], [2]. This newly developed method
nullifies the need for the computation of pseudo-inverse of Jacobian while the
formulation and optimization of any artificial performance index is not
necessary. The time-varying properties of the muscle stiffness and damping as
well as the low-pass filter characteristics of human muscles have been modeled
by the proposed control law to generate human-motion characteristics for
reaching motion like quasi-straight line trajectory of the end-effector and
symmetric bell shaped velocity profile. This report focuses on the experiments
performed using a 7-DOF redundant robot-arm system which proved the
effectiveness of this algorithm in imitating human-like motion characteristics.
In addition, we extended this algorithm to a 19-DOF Hand-Arm System for a
reach-to-grasp task. Simulations using the 19-DOF Hand-Arm System show the
effectiveness of the proposed scheme for effective human-like hand-arm
coordination in reach-to-grasp tasks for pinch and envelope grasps on objects
of different shapes such as a box, a cylinder, and a sphere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0960</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0960</id><created>2013-11-04</created><updated>2014-03-25</updated><authors><author><keyname>Chon</keyname><forenames>Yong Chol</forenames></author></authors><title>The abstract Cauchy problem for the non-stationary bulk queue M(t)|M[k,
  B]|1</title><categories>math.GM cs.PF</categories><comments>8 pages</comments><report-no>KISU-MATH-2013-E-R-036</report-no><msc-class>60K20, 60K25, 68M20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derived state probability equations describing the queue M(t)|M[k, B]|1
and formulated as an abstract Cauchy problem to investigate by means of the
semi-group theory of bounded linear operators in functional analysis. For the
abstract Cauchy problem of this queue, we determined the eigenfunctions of
maximal operator and showed some properties of the Dirichlet operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0966</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0966</id><created>2013-11-04</created><updated>2013-12-09</updated><authors><author><keyname>Neftci</keyname><forenames>Emre</forenames></author><author><keyname>Das</keyname><forenames>Srinjoy</forenames></author><author><keyname>Pedroni</keyname><forenames>Bruno</forenames></author><author><keyname>Kreutz-Delgado</keyname><forenames>Kenneth</forenames></author><author><keyname>Cauwenberghs</keyname><forenames>Gert</forenames></author></authors><title>Event-Driven Contrastive Divergence for Spiking Neuromorphic Systems</title><categories>cs.NE q-bio.NC</categories><comments>(Under review)</comments><doi>10.3389/fnins.2013.00272</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Restricted Boltzmann Machines (RBMs) and Deep Belief Networks have been
demonstrated to perform efficiently in a variety of applications, such as
dimensionality reduction, feature learning, and classification. Their
implementation on neuromorphic hardware platforms emulating large-scale
networks of spiking neurons can have significant advantages from the
perspectives of scalability, power dissipation and real-time interfacing with
the environment. However the traditional RBM architecture and the commonly used
training algorithm known as Contrastive Divergence (CD) are based on discrete
updates and exact arithmetics which do not directly map onto a dynamical neural
substrate. Here, we present an event-driven variation of CD to train a RBM
constructed with Integrate &amp; Fire (I&amp;F) neurons, that is constrained by the
limitations of existing and near future neuromorphic hardware platforms. Our
strategy is based on neural sampling, which allows us to synthesize a spiking
neural network that samples from a target Boltzmann distribution. The recurrent
activity of the network replaces the discrete steps of the CD algorithm, while
Spike Time Dependent Plasticity (STDP) carries out the weight updates in an
online, asynchronous fashion. We demonstrate our approach by training an RBM
composed of leaky I&amp;F neurons with STDP synapses to learn a generative model of
the MNIST hand-written digit dataset, and by testing it in recognition,
generation and cue integration tasks. Our results contribute to a machine
learning-driven approach for synthesizing networks of spiking neurons capable
of carrying out practical, high-level functionality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.0989</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.0989</id><created>2013-11-05</created><updated>2014-05-23</updated><authors><author><keyname>Zhang</keyname><forenames>Teng</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Large Margin Distribution Machine</title><categories>cs.LG</categories><comments>In: Proceedings of the 20th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining (KDD'14), New York, NY, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support vector machine (SVM) has been one of the most popular learning
algorithms, with the central idea of maximizing the minimum margin, i.e., the
smallest distance from the instances to the classification boundary. Recent
theoretical results, however, disclosed that maximizing the minimum margin does
not necessarily lead to better generalization performances, and instead, the
margin distribution has been proven to be more crucial. In this paper, we
propose the Large margin Distribution Machine (LDM), which tries to achieve a
better generalization performance by optimizing the margin distribution. We
characterize the margin distribution by the first- and second-order statistics,
i.e., the margin mean and variance. The LDM is a general learning approach
which can be used in any place where SVM can be applied, and its superiority is
verified both theoretically and empirically in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1001</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1001</id><created>2013-11-05</created><updated>2014-01-21</updated><authors><author><keyname>Abdurachmanov</keyname><forenames>David</forenames></author><author><keyname>Arya</keyname><forenames>Kapil</forenames></author><author><keyname>Bendavid</keyname><forenames>Josh</forenames></author><author><keyname>Boccali</keyname><forenames>Tommaso</forenames></author><author><keyname>Cooperman</keyname><forenames>Gene</forenames></author><author><keyname>Dotti</keyname><forenames>Andrea</forenames></author><author><keyname>Elmer</keyname><forenames>Peter</forenames></author><author><keyname>Eulisse</keyname><forenames>Giulio</forenames></author><author><keyname>Giacomini</keyname><forenames>Francesco</forenames></author><author><keyname>Jones</keyname><forenames>Christopher D.</forenames></author><author><keyname>Manzali</keyname><forenames>Matteo</forenames></author><author><keyname>Muzaffar</keyname><forenames>Shahzad</forenames></author></authors><title>Explorations of the viability of ARM and Xeon Phi for physics processing</title><categories>physics.comp-ph cs.DC hep-ex</categories><comments>Submitted to proceedings of the 20th International Conference on
  Computing in High Energy and Nuclear Physics (CHEP13), Amsterdam</comments><doi>10.1088/1742-6596/513/5/052008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on our investigations into the viability of the ARM processor and
the Intel Xeon Phi co-processor for scientific computing. We describe our
experience porting software to these processors and running benchmarks using
real physics applications to explore the potential of these processors for
production physics processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1006</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1006</id><created>2013-11-05</created><updated>2014-03-17</updated><authors><author><keyname>Holm</keyname><forenames>Marcus</forenames></author><author><keyname>Engblom</keyname><forenames>Stefan</forenames></author><author><keyname>Goude</keyname><forenames>Anders</forenames></author><author><keyname>Holmgren</keyname><forenames>Sverker</forenames></author></authors><title>Dynamic autotuning of adaptive fast multipole methods on hybrid
  multicore CPU &amp; GPU systems</title><categories>cs.DC</categories><journal-ref>SIAM J. Sci. Comput. 36(4):C376-C399 (2014)</journal-ref><doi>10.1137/130943595</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss an implementation of adaptive fast multipole methods targeting
hybrid multicore CPU- and GPU-systems. From previous experiences with the
computational profile of our version of the fast multipole algorithm, suitable
parts are off-loaded to the GPU, while the remaining parts are threaded and
executed concurrently by the CPU. The parameters defining the algorithm affects
the performance and by measuring this effect we are able to dynamically balance
the algorithm towards optimal performance. Our setup uses the dynamic nature of
the computations and is therefore of general character.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1013</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1013</id><created>2013-11-05</created><updated>2014-02-19</updated><authors><author><keyname>Zetterberg</keyname><forenames>Per</forenames></author></authors><title>Interference Alignment (IA) and Coordinated Multi-Point (CoMP) with
  IEEE802.11ac feedback compression: testbed results</title><categories>cs.IT math.IT</categories><comments>To appear in ICASSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have implemented interference alignment (IA) and joint transmission
coordinated multipoint (CoMP) on a wireless testbed using the feedback
compression scheme of the new 802.11ac standard. The performance as a function
of the frequency domain granularity is assessed. Realistic throughput gains are
obtained by probing each spatial modulation stream with ten different coding
and modulation schemes. The gain of IA and CoMP over TDMA MIMO is found to be
26% and 71%, respectively under stationary conditions. In our dense indoor
office deployment, the frequency domain granularity of the feedback can be
reduced down to every 8th subcarrier (2.5MHz), without sacrificing performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1018</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1018</id><created>2013-11-05</created><authors><author><keyname>Xu</keyname><forenames>Chen</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Resource Management for Device-to-Device Underlay Communication</title><categories>cs.GT cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1211.2065</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-to-Device (D2D) communication is a technology component for LTE-A. The
existing researches allow D2D as an underlay to the cellular network to
increase the spectral efficiency. In this book, D2D communication underlaying
cellular networks is studied. Some physical-layer techniques and cross-layer
optimization methods on resource management and interference avoidance are
proposed and discussed. WINNER II channel models is applied to be the signal
and interference model and simulation results show that the performance of D2D
link is closely related to the distance between D2D transmitter and receiver
and that between interference source and the receiver. Besides, by power
control, D2D SINR degrades, which will naturally contribute to low interference
to cellular communication. A simple mode selection method of D2D communication
is introduced. Based on path-loss (PL) mode selection criterion, D2D gives
better performance than traditional cellular system. When D2D pair is farther
away from the BS, a better results can be obtained. Game theory, which offers a
wide variety of analytical tools to study the complex interactions of players
and predict their choices, can be used for power and radio resource management
in D2D communication. A reverse iterative combinatorial auction is formulated
as a mechanism to allocate the spectrum resources for D2D communications with
multiple user pairs sharing the same channel. In addition, a game theoretic
approach is developed to implement joint scheduling, power control and channel
allocation for D2D communication. Finally, joint power and spectrum resource
allocation method is studied under consideration of battery lifetime, which is
an important application of D2D communication on increasing user's energy
efficiency. The simulation results show that all these methods have beneficial
effects on improving the system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1025</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1025</id><created>2013-11-05</created><authors><author><keyname>Miozzo</keyname><forenames>Marco</forenames></author><author><keyname>Zordan</keyname><forenames>Davide</forenames></author><author><keyname>Dini</keyname><forenames>Paolo</forenames></author><author><keyname>Rossi</keyname><forenames>Michele</forenames></author></authors><title>SolarStat: Modeling Photovoltaic Sources through Stochastic Markov
  Processes</title><categories>cs.OH</categories><comments>Submitted to IEEE EnergyCon 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a methodology and a tool to derive simple but yet
accurate stochastic Markov processes for the description of the energy
scavenged by outdoor solar sources. In particular, we target photovoltaic
panels with small form factors, as those exploited by embedded communication
devices such as wireless sensor nodes or, concerning modern cellular system
technology, by small-cells. Our models are especially useful for the
theoretical investigation and the simulation of energetically self-sufficient
communication systems including these devices. The Markov models that we derive
in this paper are obtained from extensive solar radiation databases, that are
widely available online. Basically, from hourly radiance patterns, we derive
the corresponding amount of energy (current and voltage) that is accumulated
over time, and we finally use it to represent the scavenged energy in terms of
its relevant statistics. Toward this end, two clustering approaches for the raw
radiance data are described and the resulting Markov models are compared
against the empirical distributions. Our results indicate that Markov models
with just two states provide a rough characterization of the real data traces.
While these could be sufficiently accurate for certain applications, slightly
increasing the number of states to, e.g., eight, allows the representation of
the real energy inflow process with an excellent level of accuracy in terms of
first and second order statistics. Our tool has been developed using Matlab(TM)
and is available under the GPL license at[1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1029</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1029</id><created>2013-11-05</created><updated>2015-12-11</updated><authors><author><keyname>Michel</keyname><forenames>Pascal</forenames></author></authors><title>Problems in number theory from busy beaver competition</title><categories>math.LO cs.CC cs.LO</categories><comments>35 pages</comments><proxy>LMCS</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By introducing the busy beaver competition of Turing machines, in 1962, Rado
defined noncomputable functions on positive integers. The study of these
functions and variants leads to many mathematical challenges. This article
takes up the following one: How can a small Turing machine manage to produce
very big numbers? It provides the following answer: mostly by simulating
Collatz-like functions, that are generalizations of the famous 3x+1 function.
These functions, like the 3x+1 function, lead to new unsolved problems in
number theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1040</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1040</id><created>2013-11-05</created><authors><author><keyname>Gong</keyname><forenames>Xiao-Feng</forenames></author><author><keyname>Wang</keyname><forenames>Cheng-Yuan</forenames></author><author><keyname>Hao</keyname><forenames>Ya-Na</forenames></author><author><keyname>Lin</keyname><forenames>Qiu-Hua</forenames></author></authors><title>Fifth-order canonical polyadic decomposition with partial symmetry via
  joint diagonalization for combined independent component analysis and
  canonical / Parallel factor analysis</title><categories>stat.ML cs.LG</categories><comments>5 pages, 4 figures, submitted to ICASSP2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been a trend to combine independent component analysis
and canonical / parallel factor analysis (ICA-CPA) for an enhanced robustness
for the computation of CPA, and ICA-CPA could be further converted into the
problem of canonical polyadic decomposition (CPD) of a 5th-order partially
symmetric tensor, by calculating the 4th-order cumulant of a trilinear mixture.
In this study, we propose a new 5th-order CPD algorithm constrained with
partial symmetry using joint diagonalization. As the main steps involved in the
proposed algorithm undergo no updating iterations for the loading matrices, it
is much faster than the existing algorithm based on alternating least squares
and enhanced line search, and therefore could be used as a nice initialization
for the latter. Simulation results are given to examine the performance of the
proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1043</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1043</id><created>2013-11-05</created><updated>2013-12-13</updated><authors><author><keyname>Lang</keyname><forenames>Martin</forenames><affiliation>RWTH Aachen University</affiliation></author><author><keyname>L&#xf6;ding</keyname><forenames>Christof</forenames><affiliation>RWTH Aachen University</affiliation></author></authors><title>Modeling and Verification of Infinite Systems with Resources</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (December
  17, 2013) lmcs:1162</journal-ref><doi>10.2168/LMCS-9(4:22)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider formal verification of recursive programs with resource
consumption. We introduce prefix replacement systems with non-negative integer
counters which can be incremented and reset to zero as a formal model for such
programs. In these systems, we investigate bounds on the resource consumption
for reachability questions. Motivated by this question, we introduce relational
structures with resources and a quantitative first-order logic over these
structures. We define resource automatic structures as a subclass of these
structures and provide an effective method to compute the semantics of the
logic on this subclass. Subsequently, we use this framework to solve the
bounded reachability problem for resource prefix replacement systems. We
achieve this result by extending the well-known saturation method to annotated
prefix replacement systems. Finally, we provide a connection to the study of
the logic cost-WMSO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1047</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1047</id><created>2013-11-05</created><updated>2014-02-12</updated><authors><author><keyname>Alameda-Pineda</keyname><forenames>Xavier</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author></authors><title>A Geometric Approach to Sound Source Localization from Time-Delay
  Estimates</title><categories>cs.SD</categories><comments>13 pages, 2 figures, 3 table, journal</comments><journal-ref>IEEE/ACM Transactions on Audio, Speech, and Language Processing,
  22(6), 1082-1095, 2014</journal-ref><doi>10.1109/TASLP.2014.2317989</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of sound-source localization from time-delay
estimates using arbitrarily-shaped non-coplanar microphone arrays. A novel
geometric formulation is proposed, together with a thorough algebraic analysis
and a global optimization solver. The proposed model is thoroughly described
and evaluated. The geometric analysis, stemming from the direct acoustic
propagation model, leads to necessary and sufficient conditions for a set of
time delays to correspond to a unique position in the source space. Such sets
of time delays are referred to as feasible sets. We formally prove that every
feasible set corresponds to exactly one position in the source space, whose
value can be recovered using a closed-form localization mapping. Therefore we
seek for the optimal feasible set of time delays given, as input, the received
microphone signals. This time delay estimation problem is naturally cast into a
programming task, constrained by the feasibility conditions derived from the
geometric analysis. A global branch-and-bound optimization technique is
proposed to solve the problem at hand, hence estimating the best set of
feasible time delays and, subsequently, localizing the sound source. Extensive
experiments with both simulated and real data are reported; we compare our
methodology to four state-of-the-art techniques. This comparison clearly shows
that the proposed method combined with the branch-and-bound algorithm
outperforms existing methods. These in-depth geometric understanding, practical
algorithms, and encouraging results, open several opportunities for future
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1053</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1053</id><created>2013-11-05</created><updated>2013-11-26</updated><authors><author><keyname>Christiansen</keyname><forenames>Mark M.</forenames></author><author><keyname>Duffy</keyname><forenames>Ken R.</forenames></author><author><keyname>Calmon</keyname><forenames>Flavio du Pin</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Guessing a password over a wireless channel (on the effect of noise
  non-uniformity)</title><categories>cs.IT math.IT</categories><comments>Asilomar Conference on Signals, Systems &amp; Computers, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A string is sent over a noisy channel that erases some of its characters.
Knowing the statistical properties of the string's source and which characters
were erased, a listener that is equipped with an ability to test the veracity
of a string, one string at a time, wishes to fill in the missing pieces. Here
we characterize the influence of the stochastic properties of both the string's
source and the noise on the channel on the distribution of the number of
attempts required to identify the string, its guesswork. In particular, we
establish that the average noise on the channel is not a determining factor for
the average guesswork and illustrate simple settings where one recipient with,
on average, a better channel than another recipient, has higher average
guesswork. These results stand in contrast to those for the capacity of wiretap
channels and suggest the use of techniques such as friendly jamming with
pseudo-random sequences to exploit this guesswork behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1082</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1082</id><created>2013-10-24</created><authors><author><keyname>Kenekayoro</keyname><forenames>Patrick</forenames></author><author><keyname>Buckley</keyname><forenames>Kevan</forenames></author><author><keyname>Thelwall</keyname><forenames>Mike</forenames></author></authors><title>Motivation for hyperlink creation using inter-page relationships</title><categories>cs.DL cs.IR cs.SI</categories><comments>The 14th. Conference of the International Society for Scientometrics
  and. Informetrics (ISSI), Vienna, Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using raw hyperlink counts for webometrics research has been shown to be
unreliable and researchers have looked for alternatives. One alternative is
classifying hyperlinks in a website based on the motivation behind the
hyperlink creation. The method used for this type of classification involves
manually visiting a webpage and then classifying individual links on the
webpage. This is time consuming, making it infeasible for large scale studies.
This paper speeds up the classification of hyperlinks in UK academic websites
by using a machine learning technique, decision tree induction, to group web
pages found in UK academic websites into one of eight categories and then infer
the motivation for the creation of a hyperlink in a webpage based on the
linking pattern of the category the webpage belongs to.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1083</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1083</id><created>2013-09-26</created><authors><author><keyname>Chadha</keyname><forenames>Ankit</forenames></author><author><keyname>Satam</keyname><forenames>Neha</forenames></author></authors><title>An Efficient Method for Image and Audio Steganography using Least
  Significant Bit (LSB) Substitution</title><categories>cs.MM cs.CR</categories><journal-ref>International Journal of Computer Applications 77(13):37-45,
  September 2013</journal-ref><doi>10.5120/13547-1342</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to improve the data hiding in all types of multimedia data formats
such as image and audio and to make hidden message imperceptible, a novel
method for steganography is introduced in this paper. It is based on Least
Significant Bit (LSB) manipulation and inclusion of redundant noise as secret
key in the message. This method is applied to data hiding in images. For data
hiding in audio, Discrete Cosine Transform (DCT) and Discrete Wavelet Transform
(DWT) both are used. All the results displayed prove to be time-efficient and
effective. Also the algorithm is tested for various numbers of bits. For those
values of bits, Mean Square Error (MSE) and Peak-Signal-to-Noise-Ratio (PSNR)
are calculated and plotted. Experimental results show that the stego-image is
visually indistinguishable from the original cover-image when n&lt;=4, because of
better PSNR which is achieved by this technique. The final results obtained
after steganography process does not reveal presence of any hidden message,
thus qualifying the criteria of imperceptible message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1084</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1084</id><created>2013-08-27</created><authors><author><keyname>Monti</keyname><forenames>Massimo</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Tschudin</keyname><forenames>Christian</forenames></author><author><keyname>Luise</keyname><forenames>Marco</forenames></author></authors><title>A Chemistry-Inspired Framework for Achieving Consensus in Wireless
  Sensor Networks</title><categories>cs.DC</categories><comments>12 pages, 10 figures, submitted to IEEE Sensors Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to show how simple interaction mechanisms, inspired
by chemical systems, can provide the basic tools to design and analyze a
mathematical model for achieving consensus in wireless sensor networks,
characterized by balanced directed graphs. The convergence and stability of the
model are first proven by using new mathematical tools, which are borrowed
directly from chemical theory, and then validated by means of simulation
results, for different network topologies and number of sensors. The underlying
chemical theory is also used to derive simple interaction rules that may
account for practical issues, such as the estimation of the number of neighbors
and the robustness against perturbations. Finally, the proposed chemical
solution is validated under real-world conditions by means of a four-node
hardware implementation where the exchange of information among nodes takes
place in a distributed manner (with no need for any admission control and
synchronism procedure), simply relying on the transmission of a pulse whose
rate is proportional to the state of each sensor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1089</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1089</id><created>2013-11-05</created><authors><author><keyname>Barua</keyname><forenames>Dibakar</forenames></author><author><keyname>Jain</keyname><forenames>Pranshu</forenames></author><author><keyname>Gupta</keyname><forenames>Jitesh</forenames></author><author><keyname>Gadre</keyname><forenames>Dhananjay V.</forenames></author></authors><title>Road Accident Prevention Unit: An prototyping approach towards
  mitigating an omnipresent threat</title><categories>cs.OH</categories><comments>5 pages, 10 figures, 1 table. http://www.ieeexplore.ieee.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An intelligent multisensor front end based on the ARM Cortex M3. It deduces a
driver's configuration, ascertains his ability to drive safely and contacts
near ones with location data for urgent disaster mitigation. Prevention
measures are undertaken through external display modules and provision for
being vehicle-powered through external voltage regulated supplies. The proof of
concept for this paper is an ALL INDIA THIRD PRIZE WINNER at the Texas
Instruments Analog Design Contest 2012-13 National Finals and this paper is due
for digital publication in IEEE Xplore. All documentation is property of Texas
Instruments, the Texas Instruments Analog Design Contest 2012-13 and IEEE
Xplore.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1090</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1090</id><created>2013-11-05</created><authors><author><keyname>Crespin</keyname><forenames>Daniel</forenames></author></authors><title>Polyhedrons and Perceptrons Are Functionally Equivalent</title><categories>cs.NE</categories><comments>17 pages, 0 figures</comments><msc-class>68T01</msc-class><acm-class>C.1.3; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mathematical definitions of polyhedrons and perceptron networks are
discussed. The formalization of polyhedrons is done in a rather traditional
way. For networks, previously proposed systems are developed. Perceptron
networks in disjunctive normal form (DNF) and conjunctive normal forms (CNF)
are introduced. The main theme is that single output perceptron neural networks
and characteristic functions of polyhedrons are one and the same class of
functions. A rigorous formulation and proof that three layers suffice is
obtained. The various constructions and results are among several steps
required for algorithms that replace incremental and statistical learning with
more efficient, direct and exact geometric methods for calculation of
perceptron architecture and weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1098</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1098</id><created>2013-11-05</created><updated>2014-05-21</updated><authors><author><keyname>He</keyname><forenames>Niao</forenames></author><author><keyname>Juditsky</keyname><forenames>Anatoli</forenames></author><author><keyname>Nemirovski</keyname><forenames>Arkadi</forenames></author></authors><title>Mirror Prox Algorithm for Multi-Term Composite Minimization and
  Semi-Separable Problems</title><categories>math.OC cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper, we develop a composite version of Mirror Prox algorithm for
solving convex-concave saddle point problems and monotone variational
inequalities of special structure, allowing to cover saddle point/variational
analogies of what is usually called &quot;composite minimization&quot; (minimizing a sum
of an easy-to-handle nonsmooth and a general-type smooth convex functions &quot;as
if&quot; there were no nonsmooth component at all). We demonstrate that the
composite Mirror Prox inherits the favourable (and unimprovable already in the
large-scale bilinear saddle point case) $O(1/\epsilon)$ efficiency estimate of
its prototype. We demonstrate that the proposed approach can be naturally
applied to Lasso-type problems with several penalizing terms (e.g. acting
together $\ell_1$ and nuclear norm regularization) and to problems of the
structure considered in the alternating directions methods, implying in both
cases methods with the $O(\epsilon^{-1})$ complexity bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1108</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1108</id><created>2013-11-05</created><updated>2015-09-15</updated><authors><author><keyname>Raymond</keyname><forenames>Jean-Florent</forenames></author><author><keyname>Sau</keyname><forenames>Ignasi</forenames></author><author><keyname>Thilikos</keyname><forenames>Dimitrios M.</forenames></author></authors><title>An edge variant of the Erd\H{o}s-P\'osa property</title><categories>math.CO cs.DM</categories><comments>17 pages, 2 figures</comments><msc-class>05C70</msc-class><acm-class>G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For every $r\in \mathbb{N}$, we denote by $\theta_{r}$ the multigraph with
two vertices and $r$ parallel edges. Given a graph $G$, we say that a subgraph
$H$ of $G$ is a model of $\theta_{r}$ in $G$ if $H$ contains $\theta_{r}$ as a
contraction. We prove that the following edge variant of the Erd{\H
o}s-P{\'o}sa property holds for every $r\geq 2$: if $G$ is a graph and $k$ is a
positive integer, then either $G$ contains a packing of $k$ mutually
edge-disjoint models of $\theta_{r}$, or it contains a set $S$ of $f_r(k)$
edges such that $G\setminus S$ has no $\theta_{r}$-model, for both $f_r(k) =
O(k^2r^3 \mathrm{polylog}~kr)$ and $f_r(k) = O(k^4r^2 \mathrm{polylog}~kr).$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1132</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1132</id><created>2013-11-05</created><authors><author><keyname>Ketabdar</keyname><forenames>Hamed</forenames></author><author><keyname>Qureshi</keyname><forenames>Jalaluddin</forenames></author><author><keyname>Hui</keyname><forenames>Pan</forenames></author></authors><title>Motion and audio analysis in mobile devices for remote monitoring of
  physical activities and user authentication</title><categories>cs.HC cs.CV</categories><journal-ref>Journal of Location Based Services, Volume 5, Issue 3-4, 2011, pp.
  180-200, Special Issue: The social and behavioural implications of
  location-based services</journal-ref><doi>10.1080/17489725.2011.644331</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this article we propose the use of accelerometer embedded by default in
smartphone as a cost-effective, reliable and efficient way to provide remote
physical activity monitoring for the elderly and people requiring healthcare
service. Mobile phones are regularly carried by users during their day-to-day
work routine, physical movement information can be captured by the mobile phone
accelerometer, processed and sent to a remote server for monitoring. The
acceleration pattern can deliver information related to the pattern of physical
activities the user is engaged in. We further show how this technique can be
extended to provide implicit real-time security by analysing unexpected
movements captured by the phone accelerometer, and automatically locking the
phone in such situation to prevent unauthorised access. This technique is also
shown to provide implicit continuous user authentication, by capturing regular
user movements such as walking, and requesting for re-authentication whenever
it detects a non-regular movement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1158</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1158</id><created>2013-11-05</created><updated>2015-11-30</updated><authors><author><keyname>Gordinowicz</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Planar graph is on fire</title><categories>math.CO cs.DM</categories><msc-class>05C15</msc-class><journal-ref>Theoretical Computer Science, 593(2015), 160-164</journal-ref><doi>10.1016/j.tcs.2015.06.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be any connected graph on $n$ vertices, $n \ge 2.$ Let $k$ be any
positive integer. Suppose that a fire breaks out on some vertex of $G.$ Then in
each turn $k$ firefighters can protect vertices of $G$ --- each can protect one
vertex not yet on fire; Next a fire spreads to all unprotected neighbours.
  The \emph{$k$-surviving} rate of G, denoted by $\rho_k(G),$ is the expected
fraction of vertices that can be saved from the fire by $k$ firefighters,
provided that the starting vertex is chosen uniformly at random. In this paper,
it is shown that for any planar graph $G$ we have $\rho_3(G) \ge \frac{2}{21}.$
Moreover, 3 firefighters are needed for the first step only; after that it is
enough to have 2 firefighters per each round. This result significantly
improves known solutions to a problem of Cai and Wang (there was no positive
bound known for surviving rate of general planar graph with only 3
firefighters). The proof is done using the separator theorem for planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1162</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1162</id><created>2013-11-05</created><authors><author><keyname>Wagner</keyname><forenames>Claudia</forenames></author><author><keyname>Singer</keyname><forenames>Philipp</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author></authors><title>Semantic Stability in Social Tagging Streams</title><categories>cs.CY cs.IR cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One potential disadvantage of social tagging systems is that due to the lack
of a centralized vocabulary, a crowd of users may never manage to reach a
consensus on the description of resources (e.g., books, users or songs) on the
Web. Yet, previous research has provided interesting evidence that the tag
distributions of resources may become semantically stable over time as more and
more users tag them. At the same time, previous work has raised an array of new
questions such as: (i) How can we assess the semantic stability of social
tagging systems in a robust and methodical way? (ii) Does semantic
stabilization of tags vary across different social tagging systems and
ultimately, (iii) what are the factors that can explain semantic stabilization
in such systems? In this work we tackle these questions by (i) presenting a
novel and robust method which overcomes a number of limitations in existing
methods, (ii) empirically investigating semantic stabilization processes in a
wide range of social tagging systems with distinct domains and properties and
(iii) detecting potential causes for semantic stabilization, specifically
imitation behavior, shared background knowledge and intrinsic properties of
natural language. Our results show that tagging streams which are generated by
a combination of imitation dynamics and shared background knowledge exhibit
faster and higher semantic stability than tagging streams which are generated
via imitation dynamics or natural language streams alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1163</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1163</id><created>2013-09-05</created><updated>2014-10-11</updated><authors><author><keyname>Hou</keyname><forenames>Thomas Y.</forenames></author><author><keyname>Shi</keyname><forenames>Zuoqiang</forenames></author></authors><title>Sparse Time-Frequency decomposition by dictionary learning</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a time-frequency analysis method to obtain
instantaneous frequencies and the corresponding decomposition by solving an
optimization problem. In this optimization problem, the basis to decompose the
signal is not known. Instead, it is adapted to the signal and is determined as
part of the optimization problem. In this sense, this optimization problem can
be seen as a dictionary learning problem. This dictionary learning problem is
solved by using the Augmented Lagrangian Multiplier method (ALM) iteratively.
We further accelerate the convergence of the ALM method in each iteration by
using the fast wavelet transform. We apply our method to decompose several
signals, including signals with poor scale separation, signals with outliers
and polluted by noise and a real signal. The results show that this method can
give accurate recovery of both the instantaneous frequencies and the intrinsic
mode functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1169</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1169</id><created>2013-11-05</created><authors><author><keyname>Kondor</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>Csabai</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>Dobos</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>Sz&#xfc;le</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>Barankai</keyname><forenames>Norbert</forenames></author><author><keyname>Hanyecz</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Seb&#x151;k</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Kallus</keyname><forenames>Zs&#xf3;fia</forenames></author><author><keyname>Vattay</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>Using Robust PCA to estimate regional characteristics of language use
  from geo-tagged Twitter messages</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal component analysis (PCA) and related techniques have been
successfully employed in natural language processing. Text mining applications
in the age of the online social media (OSM) face new challenges due to
properties specific to these use cases (e.g. spelling issues specific to texts
posted by users, the presence of spammers and bots, service announcements,
etc.). In this paper, we employ a Robust PCA technique to separate typical
outliers and highly localized topics from the low-dimensional structure present
in language use in online social networks. Our focus is on identifying
geospatial features among the messages posted by the users of the Twitter
microblogging service. Using a dataset which consists of over 200 million
geolocated tweets collected over the course of a year, we investigate whether
the information present in word usage frequencies can be used to identify
regional features of language use and topics of interest. Using the PCA pursuit
method, we are able to identify important low-dimensional features, which
constitute smoothly varying functions of the geographic location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1181</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1181</id><created>2013-11-04</created><authors><author><keyname>Hilal</keyname><forenames>I.</forenames></author><author><keyname>Afifi</keyname><forenames>N.</forenames></author><author><keyname>Hilali</keyname><forenames>R. Filali</forenames></author><author><keyname>Ouzzif</keyname><forenames>M.</forenames></author></authors><title>Toward a New Approach for Modeling Dependability of Data Warehouse
  System</title><categories>cs.DB cs.SE</categories><comments>10 pages, (IJCSIS) International Journal of Computer Science and
  Information Security, Vol. 11, No. 6, June 2013</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The sustainability of any Data Warehouse System (DWS) is closely correlated
with user satisfaction. Therefore, analysts, designers and developers focused
more on achieving all its functionality, without considering others kinds of
requirement such as dependability s aspects. Moreover, these latter are often
considered as properties of the system that will must be checked and corrected
once the project is completed. The practice of &quot;fix it later&quot; can cause the
obsolescence of the entire Data Warehouse System. Therefore, it requires the
adoption of a methodology that will ensure the integration of aspects of
dependability since the early stages of project DWS. In this paper, we first
define the concepts related to dependability of DWS. Then we present our
approach inspired from the MDA (Model Driven Architecture) approach to model
dependability s aspects namely: availability, reliability, maintainability and
security, taking into account their interaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1187</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1187</id><created>2013-11-05</created><updated>2013-11-05</updated><authors><author><keyname>Fouladgar</keyname><forenames>Ali Mohammad</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Constrained Codes for Joint Energy and Information Transfer</title><categories>cs.IT math.IT</categories><comments>27 pages, 14 figures, Submitted Submitted in IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In various wireless systems, such as sensor RFID networks and body area
networks with implantable devices, the transmitted signals are simultaneously
used both for information transmission and for energy transfer. In order to
satisfy the conflicting requirements on information and energy transfer, this
paper proposes the use of constrained run-length limited (RLL) codes in lieu of
conventional unconstrained (i.e., random-like) capacity-achieving codes. The
receiver's energy utilization requirements are modeled stochastically, and
constraints are imposed on the probabilities of battery underflow and overflow
at the receiver. It is demonstrated that the codewords' structure afforded by
the use of constrained codes enables the transmission strategy to be better
adjusted to the receiver's energy utilization pattern, as compared to classical
unstructured codes. As a result, constrained codes allow a wider range of
trade-offs between the rate of information transmission and the performance of
energy transfer to be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1189</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1189</id><created>2013-11-05</created><authors><author><keyname>Titsias</keyname><forenames>Michalis K.</forenames></author><author><keyname>Yau</keyname><forenames>Christopher</forenames></author><author><keyname>Holmes</keyname><forenames>Christopher C.</forenames></author></authors><title>Statistical Inference in Hidden Markov Models using $k$-segment
  Constraints</title><categories>stat.ME cs.LG stat.ML</categories><comments>37 pages</comments><doi>10.1080/01621459.2014.998762</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hidden Markov models (HMMs) are one of the most widely used statistical
methods for analyzing sequence data. However, the reporting of output from HMMs
has largely been restricted to the presentation of the most-probable (MAP)
hidden state sequence, found via the Viterbi algorithm, or the sequence of most
probable marginals using the forward-backward (F-B) algorithm. In this article,
we expand the amount of information we could obtain from the posterior
distribution of an HMM by introducing linear-time dynamic programming
algorithms that, we collectively call $k$-segment algorithms, that allow us to
i) find MAP sequences, ii) compute posterior probabilities and iii) simulate
sample paths conditional on a user specified number of segments, i.e.
contiguous runs in a hidden state, possibly of a particular type. We illustrate
the utility of these methods using simulated and real examples and highlight
the application of prospective and retrospective use of these methods for
fitting HMMs or exploring existing model fits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1194</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1194</id><created>2013-11-05</created><authors><author><keyname>Mohammad</keyname><forenames>Saif M.</forenames></author><author><keyname>Kiritchenko</keyname><forenames>Svetlana</forenames></author><author><keyname>Martin</keyname><forenames>Joel</forenames></author></authors><title>Identifying Purpose Behind Electoral Tweets</title><categories>cs.CL</categories><journal-ref>In Proceedings of the KDD Workshop on Issues of Sentiment
  Discovery and Opinion Mining (WISDOM-2013), August 2013, Chicago, USA</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tweets pertaining to a single event, such as a national election, can number
in the hundreds of millions. Automatically analyzing them is beneficial in many
downstream natural language applications such as question answering and
summarization. In this paper, we propose a new task: identifying the purpose
behind electoral tweets--why do people post election-oriented tweets? We show
that identifying purpose is correlated with the related phenomenon of sentiment
and emotion detection, but yet significantly different. Detecting purpose has a
number of applications including detecting the mood of the electorate,
estimating the popularity of policies, identifying key issues of contention,
and predicting the course of events. We create a large dataset of electoral
tweets and annotate a few thousand tweets for purpose. We develop a system that
automatically classifies electoral tweets as per their purpose, obtaining an
accuracy of 43.56% on an 11-class task and an accuracy of 73.91% on a 3-class
task (both accuracies well above the most-frequent-class baseline). Finally, we
show that resources developed for emotion detection are also helpful for
detecting purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1195</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1195</id><created>2013-11-05</created><authors><author><keyname>Xu</keyname><forenames>Kunjie</forenames></author></authors><title>Performance Modeling of BitTorrent Peer-to-Peer File Sharing Networks</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BitTorrent is undoubtedly the most popular P2P file sharing application on
today's Internet. The widespread popularity of BitTorrent has attracted a great
deal of attention from networking researchers who conducted various performance
studies on it. This paper presents a comprehensive survey of analytical
performance modeling techniques for BitTorrent networks. The performance models
examined in this study include deterministic models, Markov chain models, fluid
flow models, and queuing network models. These models evaluate the performance
metrics of BitTorrent networks at different regimes with various realistic
factors considered. Furthermore, a comparative analysis is conducted on those
modeling techniques in the aspects of complexity, accuracy, extensibility, and
scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1197</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1197</id><created>2013-11-05</created><authors><author><keyname>Sridhar</keyname><forenames>M. Bhanu</forenames></author><author><keyname>Srinivas</keyname><forenames>Y.</forenames></author><author><keyname>Prasad</keyname><forenames>M. H. M. Krishna</forenames></author></authors><title>Software Reuse in Cardiology Related Medical Database Using K-Means
  Clustering Technique</title><categories>cs.SE</categories><comments>5 pages. arXiv admin note: text overlap with arXiv:1212.0312</comments><journal-ref>Journal of Software Engineering and Applications, 2012, 5, 682-686</journal-ref><doi>10.4236/jsea.2012.59081</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software technology based on reuse is identified as a process of designing
software for the reuse purpose. The software reuse is a process in which the
existing software is used to build new software. A metric is a quantitative
indicator of an attribute of an item or thing. Reusability is the likelihood
for a segment of source code that can be used again to add new functionalities
with slight or no modification. A lot of research has been projected using
reusability in reducing code, domain, requirements, design etc., but very
little work is reported using software reuse in medical domain. An attempt is
made to bridge the gap in this direction, using the concepts of clustering and
classifying the data based on the distance measures. In this paper cardiologic
database is considered for study. The developed model will be useful for
Doctors or Paramedics to find out the patients level in the cardiologic
disease, deduce the medicines required in seconds and propose them to the
patient. In order to measure the reusability K means clustering algorithm is
used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1213</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1213</id><created>2013-11-05</created><authors><author><keyname>Varshney</keyname><forenames>Lav R.</forenames></author><author><keyname>Pinel</keyname><forenames>Florian</forenames></author><author><keyname>Varshney</keyname><forenames>Kush R.</forenames></author><author><keyname>Bhattacharjya</keyname><forenames>Debarun</forenames></author><author><keyname>Schoergendorfer</keyname><forenames>Angela</forenames></author><author><keyname>Chee</keyname><forenames>Yi-Min</forenames></author></authors><title>A Big Data Approach to Computational Creativity</title><categories>cs.CY cs.AI cs.HC physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational creativity is an emerging branch of artificial intelligence
that places computers in the center of the creative process. Broadly,
creativity involves a generative step to produce many ideas and a selective
step to determine the ones that are the best. Many previous attempts at
computational creativity, however, have not been able to achieve a valid
selective step. This work shows how bringing data sources from the creative
domain and from hedonic psychophysics together with big data analytics
techniques can overcome this shortcoming to yield a system that can produce
novel and high-quality creative artifacts. Our data-driven approach is
demonstrated through a computational creativity system for culinary recipes and
menus we developed and deployed, which can operate either autonomously or
semi-autonomously with human interaction. We also comment on the volume,
velocity, variety, and veracity of data in computational creativity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1223</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1223</id><created>2013-11-05</created><authors><author><keyname>Dammavalam</keyname><forenames>Srinivasa Rao</forenames></author><author><keyname>Maddala</keyname><forenames>Seetha</forenames></author><author><keyname>Prasad</keyname><forenames>M. H. M. Krishna</forenames></author></authors><title>Quality Assessment of Pixel-Level ImageFusion Using Fuzzy Logic</title><categories>cs.CV</categories><comments>13 pages. arXiv admin note: substantial text overlap with
  arXiv:1212.0318</comments><journal-ref>International Journal on Soft Computing ( IJSC) Vol.3, No.1,
  February 2012</journal-ref><doi>10.5121/ijsc.2012.3102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image fusion is to reduce uncertainty and minimize redundancy in the output
while maximizing relevant information from two or more images of a scene into a
single composite image that is more informative and is more suitable for visual
perception or processing tasks like medical imaging, remote sensing, concealed
weapon detection, weather forecasting, biometrics etc. Image fusion combines
registered images to produce a high quality fused image with spatial and
spectral information. The fused image with more information will improve the
performance of image analysis algorithms used in different applications. In
this paper, we proposed a fuzzy logic method to fuse images from different
sensors, in order to enhance the quality and compared proposed method with two
other methods i.e. image fusion using wavelet transform and weighted average
discrete wavelet transform based image fusion using genetic algorithm (here
onwards abbreviated as GA) along with quality evaluation parameters image
quality index (IQI), mutual information measure (MIM), root mean square error
(RMSE), peak signal to noise ratio (PSNR), fusion factor (FF), fusion symmetry
(FS) and fusion index (FI) and entropy. The results obtained from proposed
fuzzy based image fusion approach improves quality of fused image as compared
to earlier reported methods, wavelet transform based image fusion and weighted
average discrete wavelet transform based image fusion using genetic algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1229</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1229</id><created>2013-11-05</created><updated>2014-04-03</updated><authors><author><keyname>Chekanov</keyname><forenames>S. V.</forenames></author><author><keyname>May</keyname><forenames>E.</forenames></author><author><keyname>Strand</keyname><forenames>K.</forenames></author><author><keyname>Van Gemmeren</keyname><forenames>P.</forenames></author></authors><title>ProMC: Input-output data format for HEP applications using varint
  encoding</title><categories>physics.comp-ph cs.OH hep-ex hep-ph</categories><comments>17 pages, 2 figures</comments><report-no>ANL-HEP-PR-13-41</report-no><journal-ref>Computer Physics Communications 185 (2014), 2629-2635 p</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new data format for Monte Carlo (MC) events, or any structural data,
including experimental data, is discussed. The format is designed to store data
in a compact binary form using variable-size integer encoding as implemented in
the Google's Protocol Buffers package. This approach is implemented in the
ProMC library which produces smaller file sizes for MC records compared to the
existing input-output libraries used in high-energy physics (HEP). Other
important features of the proposed format are a separation of abstract data
layouts from concrete programming implementations, self-description and random
access. Data stored in ProMC files can be written, read and manipulated in a
number of programming languages, such C++, JAVA, FORTRAN and PYTHON.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1240</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1240</id><created>2013-11-05</created><updated>2014-10-09</updated><authors><author><keyname>Elmahdy</keyname><forenames>Adel M.</forenames></author><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author><author><keyname>Seddik</keyname><forenames>Karim G.</forenames></author></authors><title>Generalized Instantly Decodable Network Coding for Relay-Assisted
  Networks</title><categories>cs.NI</categories><comments>5 pages, IEEE PIMRC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of minimizing the frame completion
delay for Instantly Decodable Network Coding (IDNC) in relay-assisted wireless
multicast networks. We first propose a packet recovery algorithm in the single
relay topology which employs generalized IDNC instead of strict IDNC previously
proposed in the literature for the same relay-assisted topology. This use of
generalized IDNC is supported by showing that it is a super-set of the strict
IDNC scheme, and thus can generate coding combinations that are at least as
efficient as strict IDNC in reducing the average completion delay. We then
extend our study to the multiple relay topology and propose a joint generalized
IDNC and relay selection algorithm. This proposed algorithm benefits from the
reception diversity of the multiple relays to further reduce the average
completion delay in the network. Simulation results show that our proposed
solutions achieve much better performance compared to previous solutions in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1247</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1247</id><created>2013-11-05</created><authors><author><keyname>Kang</keyname><forenames>Jeon-Hyung</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author></authors><title>LA-CTR: A Limited Attention Collaborative Topic Regression for Social
  Media</title><categories>cs.IR cs.SI physics.soc-ph</categories><comments>The Twenty-Seventh AAAI Conference on Artificial Intelligence (AAAI
  2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic models can learn users' preferences from the history of their
item adoptions on a social media site, and in turn, recommend new items to
users based on learned preferences. However, current models ignore
psychological factors that play an important role in shaping online social
behavior. One such factor is attention, the mechanism that integrates
perceptual and cognitive features to select the items the user will consciously
process and may eventually adopt. Recent research has shown that people have
finite attention, which constrains their online interactions, and that they
divide their limited attention non-uniformly over other people. We propose a
collaborative topic regression model that incorporates limited, non-uniformly
divided attention. We show that the proposed model is able to learn more
accurate user preferences than state-of-art models, which do not take human
cognitive factors into account. Specifically we analyze voting on news items on
the social news aggregator and show that our model is better able to predict
held out votes than alternate models. Our study demonstrates that
psycho-socially motivated models are better able to describe and predict
observed behavior than models which only consider latent social structure and
content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1249</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1249</id><created>2013-11-05</created><authors><author><keyname>Gog</keyname><forenames>Simon</forenames></author><author><keyname>Beller</keyname><forenames>Timo</forenames></author><author><keyname>Moffat</keyname><forenames>Alistair</forenames></author><author><keyname>Petri</keyname><forenames>Matthias</forenames></author></authors><title>From Theory to Practice: Plug and Play with Succinct Data Structures</title><categories>cs.DS</categories><comments>10 pages, 4 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Engineering efficient implementations of compact and succinct structures is a
time-consuming and challenging task, since there is no standard library of
easy-to- use, highly optimized, and composable components. One consequence is
that measuring the practical impact of new theoretical proposals is a difficult
task, since older base- line implementations may not rely on the same basic
components, and reimplementing from scratch can be very time-consuming. In this
paper we present a framework for experimentation with succinct data structures,
providing a large set of configurable components, together with tests,
benchmarks, and tools to analyze resource requirements. We demonstrate the
functionality of the framework by recomposing succinct solutions for document
retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1259</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1259</id><created>2013-11-05</created><authors><author><keyname>Yap</keyname><forenames>Han Lun</forenames></author><author><keyname>Pribi&#x107;</keyname><forenames>Radmila</forenames></author></authors><title>Multi-target Radar Detection within a Sparsity Framework</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional radar detection schemes are typically studied for single target
scenarios and they can be non-optimal when there are multiple targets in the
scene. In this paper, we develop a framework to discuss multi-target detection
schemes with sparse reconstruction techniques that is based on the
Neyman-Pearson criterion. We will describe an initial result in this framework
concerning false alarm probability with LASSO as the sparse reconstruction
technique. Then, several simulations validating this result will be discussed.
Finally, we describe several research avenues to further pursue this framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1264</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1264</id><created>2013-11-05</created><updated>2014-03-19</updated><authors><author><keyname>Song</keyname><forenames>Yangbo</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Dynamic Network Formation with Incomplete Information</title><categories>cs.SI cs.GT physics.soc-ph</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  How do networks form and what is their ultimate topology? Most of the
literature that addresses these questions assumes complete information: agents
know in advance the value of linking to other agents, even with agents they
have never met and with whom they have had no previous interaction (direct or
indirect). This paper addresses the same questions under what seems to us to be
the much more natural assumption of incomplete information: agents do not know
in advance -- but must learn -- the value of linking to agents they have never
met. We show that the assumption of incomplete information has profound
implications for the process of network formation and the topology of networks
that ultimately form. Under complete information, the networks that form and
are stable typically have a star, wheel or core-periphery form, with high-value
agents in the core. Under incomplete information, the presence of positive
externalities (the value of indirect links) implies that a much wider
collection of network topologies can emerge and be stable. Moreover, even when
the topologies that emerge are the same, the locations of agents can be very
different. For instance, when information is incomplete, it is possible for a
hub-and-spokes network with a low-value agent in the center to form and endure
permanently: an agent can achieve a central position purely as the result of
chance rather than as the result of merit. Perhaps even more strikingly: when
information is incomplete, a connected network could form and persist even if,
when information were complete, no links would ever form, so that the final
form would be a totally disconnected network. All of this can occur even in
settings where agents eventually learn everything so that information, although
initially incomplete, eventually becomes complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1266</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1266</id><created>2013-11-05</created><updated>2014-06-30</updated><authors><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author><author><keyname>Oliveira</keyname><forenames>Osvaldo N.</forenames><suffix>Jr.</suffix></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author></authors><title>Topological-collaborative approach for disambiguating authors' names in
  collaborative networks</title><categories>cs.SI physics.soc-ph</categories><comments>To appear in Scientometrics, 2014</comments><journal-ref>Scientometrics 102 (1), 465--485, 2015</journal-ref><doi>10.1007/s11192-014-1381-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concepts and methods of complex networks have been employed to uncover
patterns in a myriad of complex systems. Unfortunately, the relevance and
significance of these patterns strongly depends on the reliability of the data
sets. In the study of collaboration networks, for instance, unavoidable noise
pervading author's collaboration datasets arises when authors share the same
name. To address this problem, we derive a hybrid approach based on authors'
collaboration patterns and on topological features of collaborative networks.
Our results show that the combination of strategies, in most cases, performs
better than the traditional approach which disregards topological features. We
also show that the main factor for improving the discriminability of homonymous
authors is the average distance between authors. Finally, we show that it is
possible to predict the weighting associated to each strategy compounding the
hybrid system by examining the discrimination obtained from the traditional
analysis of collaboration patterns. Once the methodology devised here is
generic, our approach is potentially useful to classify many other networked
systems governed by complex interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1279</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1279</id><created>2013-11-05</created><authors><author><keyname>Huang</keyname><forenames>Sheng</forenames></author><author><keyname>Yang</keyname><forenames>Dan</forenames></author><author><keyname>Yang</keyname><forenames>Fei</forenames></author><author><keyname>Ge</keyname><forenames>Yongxin</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaohong</forenames></author><author><keyname>Lu</keyname><forenames>Jiwen</forenames></author></authors><title>Face Recognition via Globality-Locality Preserving Projections</title><categories>cs.CV</categories><comments>18 pages, 17 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present an improved Locality Preserving Projections (LPP) method, named
Gloablity-Locality Preserving Projections (GLPP), to preserve both the global
and local geometric structures of data. In our approach, an additional
constraint of the geometry of classes is imposed to the objective function of
conventional LPP for respecting some more global manifold structures. Moreover,
we formulate a two-dimensional extension of GLPP (2D-GLPP) as an example to
show how to extend GLPP with some other statistical techniques. We apply our
works to face recognition on four popular face databases, namely ORL, Yale,
FERET and LFW-A databases, and extensive experimental results demonstrate that
the considered global manifold information can significantly improve the
performance of LPP and the proposed face recognition methods outperform the
state-of-the-arts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1288</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1288</id><created>2013-11-06</created><authors><author><keyname>Wang</keyname><forenames>Zhengdao</forenames></author></authors><title>Performance of Uplink Multiuser Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>5 pages, no figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the performance of uplink transmission in a large-scale (massive)
MIMO system, where all the transmitters have single antennas and the receiver
(base station) has a large number of antennas. Specifically, we analyze
achievable degrees of freedom of the system without assuming channel state
information at the receiver. Also, we quantify the amount of power saving that
is possible with increasing number of receive antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1291</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1291</id><created>2013-11-06</created><authors><author><keyname>Raviteja</keyname><forenames>P.</forenames></author><author><keyname>Narasimhan</keyname><forenames>T. Lakshmi</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>Multiuser SM-MIMO versus Massive MIMO: Uplink Performance Comparison</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose algorithms for signal detection in large-scale
multiuser {\em spatial modulation multiple-input multiple-output (SM-MIMO)}
systems. In large-scale SM-MIMO, each user is equipped with multiple transmit
antennas (e.g., 2 or 4 antennas) but only one transmit RF chain, and the base
station (BS) is equipped with tens to hundreds of (e.g., 128) receive antennas.
In SM-MIMO, in a given channel use, each user activates any one of its multiple
transmit antennas and the index of the activated antenna conveys information
bits in addition to the information bits conveyed through conventional
modulation symbols (e.g., QAM). We propose two different algorithms for
detection of large-scale SM-MIMO signals at the BS; one is based on {\em
message passing} and the other is based on {\em local search}. The proposed
algorithms are shown to achieve very good performance and scale well. Also, for
the same spectral efficiency, multiuser SM-MIMO outperforms conventional
multiuser MIMO (recently being referred to as massive MIMO) by several dBs; for
e.g., with 16 users, 128 antennas at the BS and 4 bpcu per user, SM-MIMO with 4
transmit antennas per user and 4-QAM outperforms massive MIMO with 1 transmit
antenna per user and 16-QAM by about 4 to 5 dB at $10^{-3}$ uncoded BER. The
SNR advantage of SM-MIMO over massive MIMO can be attributed to the following
reasons: (i) because of the spatial index bits, SM-MIMO can use a lower-order
QAM alphabet compared to that in massive MIMO to achieve the same spectral
efficiency, and (ii) for the same spectral efficiency and QAM size, massive
MIMO will need more spatial streams per user which leads to increased spatial
interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1294</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1294</id><created>2013-11-06</created><updated>2014-02-27</updated><authors><author><keyname>Hussain</keyname><forenames>Shaista</forenames></author><author><keyname>Basu</keyname><forenames>Arindam</forenames></author><author><keyname>Wang</keyname><forenames>R.</forenames></author><author><keyname>Hamilton</keyname><forenames>Tara Julia</forenames></author></authors><title>Delay Learning Architectures for Memory and Classification</title><categories>cs.NE q-bio.NC</categories><comments>27 pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a neuromorphic spiking neural network, the DELTRON, that can
remember and store patterns by changing the delays of every connection as
opposed to modifying the weights. The advantage of this architecture over
traditional weight based ones is simpler hardware implementation without
multipliers or digital-analog converters (DACs) as well as being suited to
time-based computing. The name is derived due to similarity in the learning
rule with an earlier architecture called Tempotron. The DELTRON can remember
more patterns than other delay-based networks by modifying a few delays to
remember the most 'salient' or synchronous part of every spike pattern. We
present simulations of memory capacity and classification ability of the
DELTRON for different random spatio-temporal spike patterns. The memory
capacity for noisy spike patterns and missing spikes are also shown. Finally,
we present SPICE simulation results of the core circuits involved in a
reconfigurable mixed signal implementation of this architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1298</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1298</id><created>2013-11-06</created><authors><author><keyname>Adamaszek</keyname><forenames>Anna</forenames></author><author><keyname>Popa</keyname><forenames>Alexandru</forenames></author></authors><title>Algorithmic and Hardness Results for the Colorful Components Problems</title><categories>cs.DS</categories><comments>18 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the colorful components framework, motivated by
applications emerging from comparative genomics. The general goal is to remove
a collection of edges from an undirected vertex-colored graph $G$ such that in
the resulting graph $G'$ all the connected components are colorful (i.e., any
two vertices of the same color belong to different connected components). We
want $G'$ to optimize an objective function, the selection of this function
being specific to each problem in the framework.
  We analyze three objective functions, and thus, three different problems,
which are believed to be relevant for the biological applications: minimizing
the number of singleton vertices, maximizing the number of edges in the
transitive closure, and minimizing the number of connected components.
  Our main result is a polynomial time algorithm for the first problem. This
result disproves the conjecture of Zheng et al. that the problem is $ NP$-hard
(assuming $P \neq NP$). Then, we show that the second problem is $ APX$-hard,
thus proving and strengthening the conjecture of Zheng et al. that the problem
is $ NP$-hard. Finally, we show that the third problem does not admit
polynomial time approximation within a factor of $|V|^{1/14 - \epsilon}$ for
any $\epsilon &gt; 0$, assuming $P \neq NP$ (or within a factor of $|V|^{1/2 -
\epsilon}$, assuming $ZPP \neq NP$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1309</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1309</id><created>2013-11-06</created><authors><author><keyname>Briat</keyname><forenames>Corentin</forenames></author></authors><title>Convex lifted conditions for robust stability analysis and stabilization
  of linear discrete-time switched systems</title><categories>math.OC cs.SY</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stability analysis of discrete-time switched systems under minimum dwell-time
is studied using a new type of LMI conditions. These conditions are convex in
the matrices of the system and shown to be equivalent to the nonconvex
conditions proposed by Geromel and Colaneri. The convexification of the
conditions is performed by a lifting process which introduces a moderate number
of additional decision variables. The convexity of the conditions can be
exploited to extend the results to uncertain systems, control design and
$\ell_2$-gain computation without introducing additional conservatism. Several
examples are presented to show the effectiveness of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1312</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1312</id><created>2013-11-06</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Kumagai</keyname><forenames>Shinya</forenames></author><author><keyname>Mehbodniya</keyname><forenames>Abolfazl</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>Two are Better Than One: Adaptive Sparse System Identification using
  Affine Combination of Two Sparse Adaptive Filters</title><categories>cs.IT math.IT</categories><comments>5 pages, 8 figures, submitted for VTC2014-spring</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Sparse system identification problems often exist in many applications, such
as echo interference cancellation, sparse channel estimation, and adaptive
beamforming. One of popular adaptive sparse system identification (ASSI)
methods is adopting only one sparse least mean square (LMS) filter. However,
the adoption of only one sparse LMS filter cannot simultaneously achieve fast
convergence speed and small steady-state mean state deviation (MSD). Unlike the
conventional method, we propose an improved ASSI method using affine
combination of two sparse LMS filters to simultaneously achieving fast
convergence and low steady-state MSD. First, problem formulation and standard
affine combination of LMS filters are introduced. Then an approximate optimum
affine combiner is adopted for the proposed filter according to stochastic
gradient search method. Later, to verify the proposed filter for ASSI, computer
simulations are provided to confirm effectiveness of the proposed filter which
can achieve better estimation performance than the conventional one and
standard affine combination of LMS filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1314</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1314</id><created>2013-11-06</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>kumagai</keyname><forenames>Shinya</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>Variable is Better Than Invariable: Stable Sparse VSS-NLMS Algorithms
  with Application to Estimating MIMO Channels</title><categories>cs.IT math.IT</categories><comments>5 pages, 9 figures, submitted for VTC2014-Spring</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To estimate multiple-input multiple-output (MIMO) channels, invariable
step-size normalized least mean square (ISSNLMS) algorithm was applied to
adaptive channel estimation (ACE). Since the MIMO channel is often described by
sparse channel model due to broadband signal transmission, such sparsity can be
exploited by adaptive sparse channel estimation (ASCE) methods using sparse
ISS-NLMS algorithms. It is well known that step-size is a critical parameter
which controls three aspects: algorithm stability, estimation performance and
computational cost. The previous approaches can exploit channel sparsity but
their step-sizes are keeping invariant which unable balances well the three
aspects and easily cause either estimation performance loss or instability. In
this paper, we propose two stable sparse variable step-size NLMS (VSS-NLMS)
algorithms to improve the accuracy of MIMO channel estimators. First, ASCE for
estimating MIMO channels is formulated in MIMO systems. Second, different
sparse penalties are introduced to VSS-NLMS algorithm for ASCE. In addition,
difference between sparse ISS-NLMS algorithms and sparse VSS-NLMS ones are
explained. At last, to verify the effectiveness of the proposed algorithms for
ASCE, several selected simulation results are shown to prove that the proposed
sparse VSS-NLMS algorithms can achieve better estimation performance than the
conventional methods via mean square error (MSE) and bit error rate (BER)
metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1315</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1315</id><created>2013-11-06</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Kumagai</keyname><forenames>Shinya</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>Variable Earns Profit: Improved Adaptive Channel Estimation using Sparse
  VSS-NLMS Algorithms</title><categories>cs.IT math.IT</categories><comments>6 pages, 9 figures, submitted for ICC2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate channel estimation is essential for broadband wireless
communications. As wireless channels often exhibit sparse structure, the
adaptive sparse channel estimation algorithms based on normalized least mean
square (NLMS) have been proposed, e.g., the zero-attracting NLMS (ZA-NLMS)
algorithm and reweighted zero-attracting NLMS (RZA-NLMS). In these NLMS-based
algorithms, the step size used to iteratively update the channel estimate is a
critical parameter to control the estimation accuracy and the convergence speed
(so the computational cost). However, invariable step-size (ISS) is usually
used in conventional algorithms, which leads to provide performance loss or/and
low convergence speed as well as high computational cost. To solve these
problems, based on the observation that large step size is preferred for fast
convergence while small step size is preferred for accurate estimation, we
propose to replace the ISS by variable step size (VSS) in conventional
NLMS-based algorithms to improve the adaptive sparse channel estimation in
terms of bit error rate (BER) and mean square error (MSE) metrics. The proposed
VSS-ZA-NLMS and VSS-RZA-NLMS algorithms adopt VSS, which can be adaptive to the
estimation error in each iteration, i.e., large step size is used in the case
of large estimation error to accelerate the convergence speed, while small step
size is used when the estimation error is small to improve the steady-state
estimation accuracy. Simulation results are provided to validate the
effectiveness of the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1322</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1322</id><created>2013-11-06</created><updated>2016-01-02</updated><authors><author><keyname>Milani</keyname><forenames>Fredrik</forenames></author><author><keyname>Dumas</keyname><forenames>Marlon</forenames></author><author><keyname>Ahmed</keyname><forenames>Naved</forenames></author><author><keyname>Matulevi&#x10d;ius</keyname><forenames>Raimundas</forenames></author></authors><title>Modelling Families of Business Process Variants: A Decomposition Driven
  Method</title><categories>cs.SE</categories><comments>Extended version of Fredrik Milani et al. &quot;Decomposition Driven
  Consolidation of Process Models&quot;, In Proceedings of the International
  Conference on Advanced Information Systems Engineering (CAiSE), Springer,
  2013, pp. 193-207</comments><journal-ref>Information Systems 56:55-72, 2016</journal-ref><doi>10.1016/j.is.2015.09.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Business processes usually do not exist as singular entities that can be
managed in isolation, but rather as families of business process variants. When
modelling such families of variants, analysts are confronted with the choice
between modelling each variant separately, or modelling multiple or all
variants in a single model. Modelling each variant separately leads to a
proliferation of models that share common parts, resulting in redundancies and
inconsistencies. Meanwhile, modelling all variants together leads to less but
more complex models, thus hindering on comprehensibility. This paper introduces
a method for modelling families of process variants that addresses this
trade-off. The key tenet of the method is to alternate between steps of
decomposition (breaking down processes into sub-processes) and deciding which
parts should be modelled together and which ones should be modelled separately.
We have applied the method to two case studies: one concerning the
consolidation of ex-isting process models, and another dealing with green-field
process discovery. In both cases, the method produced fewer models with respect
to the baseline and reduced duplicity by up to 50% without significant impact
on complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1323</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1323</id><created>2013-11-06</created><authors><author><keyname>Oza</keyname><forenames>Nilay</forenames></author><author><keyname>Fagerholm</keyname><forenames>Fabian</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>How Does Kanban Impact Communication and Collaboration in Software
  Engineering Teams?</title><categories>cs.SE</categories><comments>5 pages. The final publication is available at
  http://ieeexplore.ieee.org. DOI: 10.1109/CHASE.2013.6614747,
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6614747</comments><journal-ref>Proceedings of the 6th International Workshop on Cooperative and
  Human Aspects of Software Engineering (CHASE 2013), pages 125-128, San
  Francisco, United States, May 25 2013</journal-ref><doi>10.1109/CHASE.2013.6614747!</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Highly iterative development processes such as Kanban have gained significant
importance in industry. However, the impact of such processes on team
collaboration and communication is widely unknown. In this paper, we analyze
how the Kanban process aids software team's behaviours -- in particular,
communication and collaboration. The team under study developed a mobile
payment software product in six iterations over seven weeks. The data were
collected by a questionnaire, repeated at the end of each iteration. The
results indicate that Kanban has a positive effect at the beginning to get the
team working together to identify and coordinate the work. Later phases, when
the team members have established good rapport among them, the importance for
facilitating team collaboration could not be shown. Results also indicate that
Kanban helps team members to collectively identify and surface the missing
tasks to keep the pace of the development harmonized across the whole team,
resulting into increased collaboration. Besides presenting the study and the
results, the article gives an outlook on future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1329</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1329</id><created>2013-11-06</created><authors><author><keyname>Fukui</keyname><forenames>Hironori</forenames></author><author><keyname>Yomo</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Physical Layer Network Coding: A Cautionary Story with Interference and
  Spatial Reservation</title><categories>cs.IT math.IT</categories><comments>6 pages, 11 figures, Proc. of IEEE CoCoNet Workshop in conjunction
  with IEEE ICC 2013</comments><journal-ref>Communications Workshops (ICC), 2013 IEEE International Conference
  on, 266-270</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical layer network coding (PLNC) has the potential to improve throughput
of multi-hop networks. However, most of the works are focused on the simple,
three-node model with two-way relaying, not taking into account the fact that
there can be other neighboring nodes that can cause/receive interference. The
way to deal with this problem in distributed wireless networks is usage of
MAC-layer mechanisms that make a spatial reservation of the shared wireless
medium, similar to the well-known RTS/CTS in IEEE 802.11 wireless networks. In
this paper, we investigate two-way relaying in presence of interfering nodes
and usage of spatial reservation mechanisms. Specifically, we introduce a
reserved area in order to protect the nodes involved in two-way relaying from
the interference caused by neighboring nodes. We analytically derive the
end-to-end rate achieved by PLNC considering the impact of interference and
reserved area. A relevant performance measure is data rate per unit area, in
order to reflect the fact that any spatial reservation blocks another data
exchange in the reserved area. The numerical results carry a cautionary message
that the gains brought by PLNC over one-way relaying may be vanishing when the
two-way relaying is considered in a broader context of a larger wireless
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1334</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1334</id><created>2013-11-06</created><authors><author><keyname>Fagerholm</keyname><forenames>Fabian</forenames></author><author><keyname>Johnson</keyname><forenames>Patrik</forenames></author><author><keyname>Guinea</keyname><forenames>Alejandro S&#xe1;nchez</forenames></author><author><keyname>Borenstein</keyname><forenames>Jay</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Onboarding in Open Source Software Projects: A Preliminary Analysis</title><categories>cs.SE</categories><comments>6 pages. The final publication is available at
  http://ieeexplore.ieee.org. DOI: 10.1109/ICGSEW.2013.8,
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6613445
  Proceedings of the 8th International Conference on Global Software
  Engineering (ICGSE 2013), Compendium Proceedings (VirtuES Workshop), Bari,
  Italy, August 2013</comments><doi>10.1109/ICGSEW.2013.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, many software projects are partially or completely open-source
based. There is an increasing need for companies to participate in open-source
software (OSS) projects, e.g., in order to benefit from open source ecosystems.
OSS projects introduce particular challenges that have to be understood in
order to gain the benefits. One such challenge is getting newcomers onboard
into the projects effectively. Similar challenges may be present in other
self-organised, virtual team environments. In this paper we present preliminary
observations and results of in-progress research that studies the process of
onboarding into virtual OSS teams. The study is based on a program created and
conceived at Stanford University in conjunction with Facebook's Education
Modernization program. It involves the collaboration of more than a dozen
international universities and nine open source projects. More than 120
students participated in 2013. The students have been introduced to and
supported by mentors experienced in the participating OSS projects. Our
findings indicate that mentoring is an important factor for effective
onboarding in OSS projects, promoting cohesion within distributed teams and
maintaining an appropriate pace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1338</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1338</id><created>2013-11-06</created><authors><author><keyname>Koeppen</keyname><forenames>Mario</forenames></author><author><keyname>Ohnishi</keyname><forenames>Kei</forenames></author></authors><title>Significance Relations for the Benchmarking of Meta-Heuristic Algorithms</title><categories>cs.DS cs.PF</categories><comments>6 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The experimental analysis of meta-heuristic algorithm performance is usually
based on comparing average performance metric values over a set of algorithm
instances. When algorithms getting tight in performance gains, the additional
consideration of significance of a metric improvement comes into play. However,
from this moment the comparison changes from an absolute to a relative mode.
Here the implications of this paradigm shift are investigated. Significance
relations are formally established. Based on this, a trade-off between
increasing cycle-freeness of the relation and small maximum sets can be
identified, allowing for the selection of a proper significance level and
resulting ranking of a set of algorithms. The procedure is exemplified on the
CEC'05 benchmark of real parameter single objective optimization problems. The
significance relation here is based on awarding ranking points for relative
performance gains, similar to the Borda count voting method or the Wilcoxon
signed rank test. In the particular CEC'05 case, five ranks for algorithm
performance can be clearly identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1339</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1339</id><created>2013-11-06</created><updated>2014-08-24</updated><authors><author><keyname>Kova&#x10d;evi&#x107;</keyname><forenames>Mladen</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Zero-Error Capacity of a Class of Timing Channels</title><categories>cs.IT cs.DM math.IT</categories><comments>5 pages (double-column), 3 figures. v3: Section IV.1 from v2 is
  replaced with Remark 1, and Section IV.2 is removed. Accepted for publication
  in IEEE Transactions on Information Theory</comments><msc-class>94B25, 94A40, 94A24, 68R05, 65Q30</msc-class><journal-ref>IEEE Trans. Inform. Theory, vol. 60, no. 11, pp. 6796-6800, Nov.
  2014</journal-ref><doi>10.1109/TIT.2014.2352613</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the problem of zero-error communication through timing channels
that can be interpreted as discrete-time queues with bounded waiting times. The
channel model includes the following assumptions: 1) Time is slotted, 2) at
most $ N $ &quot;particles&quot; are sent in each time slot, 3) every particle is delayed
in the channel for a number of slots chosen randomly from the set $ \{0, 1,
\ldots, K\} $, and 4) the particles are identical. It is shown that the
zero-error capacity of this channel is $ \log r $, where $ r $ is the unique
positive real root of the polynomial $ x^{K+1} - x^{K} - N $.
Capacity-achieving codes are explicitly constructed, and a linear-time decoding
algorithm for these codes devised. In the particular case $ N = 1 $, $ K = 1 $,
the capacity is equal to $ \log \phi $, where $ \phi = (1 + \sqrt{5}) / 2 $ is
the golden ratio, and the constructed codes give another interpretation of the
Fibonacci sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1343</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1343</id><created>2013-11-06</created><authors><author><keyname>Cordy</keyname><forenames>Maxime</forenames></author><author><keyname>Heymans</keyname><forenames>Patrick</forenames></author><author><keyname>Schobbens</keyname><forenames>Pierre-Yves</forenames></author><author><keyname>Sharifloo</keyname><forenames>Amir Molzam</forenames></author><author><keyname>Ghezzi</keyname><forenames>Carlo</forenames></author><author><keyname>Legay</keyname><forenames>Axel</forenames></author></authors><title>Verification for Reliable Product Lines</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many product lines are critical, and therefore reliability is a vital part of
their requirements. Reliability is a probabilistic property. We therefore
propose a model for feature-aware discrete-time Markov chains as a basis for
verifying probabilistic properties of product lines, including reliability. We
compare three verification techniques: The enumerative technique uses PRISM, a
state-of-the-art symbolic probabilistic model checker, on each product. The
parametric technique exploits our recent advances in parametric model checking.
Finally, we propose a new bounded technique that performs a single bounded
verification for the whole product line, and thus takes advantage of the common
behaviours of the product line. Experimental results confirm the advantages of
the last two techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1354</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1354</id><created>2013-11-06</created><updated>2015-07-16</updated><authors><author><keyname>Melchior</keyname><forenames>Jan</forenames></author><author><keyname>Fischer</keyname><forenames>Asja</forenames></author><author><keyname>Wiskott</keyname><forenames>Laurenz</forenames></author></authors><title>How to Center Binary Deep Boltzmann Machines</title><categories>stat.ML cs.LG</categories><comments>Author list in meta data corrected - 57 pages, 17 figures, 13 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work analyzes centered binary Restricted Boltzmann Machines (RBMs) and
binary Deep Boltzmann Machines (DBMs), where centering is done by subtracting
offset values from visible and hidden variables. We show analytically that (i)
centering results in a different but equivalent parameterization for artificial
neural networks in general, (ii) the expected performance of centered binary
RBMs/DBMs is invariant under simultaneous flip of data and offsets, for any
offset value in the range of zero to one, (iii) centering can be reformulated
as a different update rule for normal binary RBMs/DBMs, and (iv) using the
enhanced gradient is equivalent to setting the offset values to the average
over model and data mean. Furthermore, numerical simulations suggest that (i)
optimal generative performance is achieved by subtracting mean values from
visible as well as hidden variables, (ii) centered RBMs/DBMs reach
significantly higher log-likelihood values than normal binary RBMs/DBMs, (iii)
centering variants whose offsets depend on the model mean, like the enhanced
gradient, suffer from severe divergence problems, (iv) learning is stabilized
if an exponentially moving average over the batch means is used for the offset
values instead of the current batch mean, which also prevents the enhanced
gradient from diverging, (v) centered RBMs/DBMs reach higher LL values than
normal RBMs/DBMs while having a smaller norm of the weight matrix, (vi)
centering leads to an update direction that is closer to the natural gradient
and that the natural gradient is extremly efficient for training RBMs, (vii)
centering dispense the need for greedy layer-wise pre-training of DBMs, (viii)
furthermore we show that pre-training often even worsen the results
independently whether centering is used or not, and (ix) centering is also
beneficial for auto encoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1358</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1358</id><created>2013-11-06</created><updated>2013-11-21</updated><authors><author><keyname>Peric</keyname><forenames>Zoran H.</forenames></author><author><keyname>Velimirovic</keyname><forenames>Lazar</forenames></author><author><keyname>Stankovic</keyname><forenames>Miomir</forenames></author><author><keyname>Jovanovic</keyname><forenames>Aleksandra</forenames></author><author><keyname>Antic</keyname><forenames>Dragan</forenames></author></authors><title>Scalar Compandor Design Based on Optimal Compressor Function
  Approximating by Spline Functions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the approximation of the optimal compressor function using the
first-degree spline functions and quadratic spline functions is done.
Coefficients on which we form approximative spline functions are determined by
solving equation systems that are formed from treshold conditions. For Gaussian
source at the input of the quantizer, using the obtained approximate spline
functions a companding quantizer designing is done. On the basis of the
comparison with the SQNR of the optimal compandor it can be noticed that the
proposed companding quantizer based on approximate spline functions achieved
SQNR arbitrary close to that of the optimal compandor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1363</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1363</id><created>2013-11-06</created><updated>2015-06-25</updated><authors><author><keyname>Cambareri</keyname><forenames>Valerio</forenames></author><author><keyname>Mangia</keyname><forenames>Mauro</forenames></author><author><keyname>Pareschi</keyname><forenames>Fabio</forenames></author><author><keyname>Rovatti</keyname><forenames>Riccardo</forenames></author><author><keyname>Setti</keyname><forenames>Gianluca</forenames></author></authors><title>On Known-Plaintext Attacks to a Compressed Sensing-based Encryption: A
  Quantitative Analysis</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Information Forensics and Security, accepted for
  publication. Article in press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the linearity of its encoding, compressed sensing may be used to
provide a limited form of data protection when random encoding matrices are
used to produce sets of low-dimensional measurements (ciphertexts). In this
paper we quantify by theoretical means the resistance of the least complex form
of this kind of encoding against known-plaintext attacks. For both standard
compressed sensing with antipodal random matrices and recent multiclass
encryption schemes based on it, we show how the number of candidate encoding
matrices that match a typical plaintext-ciphertext pair is so large that the
search for the true encoding matrix inconclusive. Such results on the practical
ineffectiveness of known-plaintext attacks underlie the fact that even
closely-related signal recovery under encoding matrix uncertainty is doomed to
fail.
  Practical attacks are then exemplified by applying compressed sensing with
antipodal random matrices as a multiclass encryption scheme to signals such as
images and electrocardiographic tracks, showing that the extracted information
on the true encoding matrix from a plaintext-ciphertext pair leads to no
significant signal recovery quality increase. This theoretical and empirical
evidence clarifies that, although not perfectly secure, both standard
compressed sensing and multiclass encryption schemes feature a noteworthy level
of security against known-plaintext attacks, therefore increasing its appeal as
a negligible-cost encryption method for resource-limited sensing applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1372</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1372</id><created>2013-11-06</created><authors><author><keyname>Romano</keyname><forenames>Gianmarco</forenames></author><author><keyname>Ciuonzo</keyname><forenames>Domenico</forenames></author></authors><title>Minimum-Variance Importance-Sampling Bernoulli Estimator for Fast
  Simulation of Linear Block Codes over Binary Symmetric Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the choice of the Bernoulli distribution as biased distribution
for importance sampling (IS) Monte-Carlo (MC) simulation of linear block codes
over binary symmetric channels (BSCs) is studied. Based on the analytical
derivation of the optimal IS Bernoulli distribution, with explicit calculation
of the variance of the corresponding IS estimator, two novel algorithms for
fast-simulation of linear block codes are proposed. For sufficiently high
signal-to-noise ratios (SNRs) one of the proposed algorithm is SNR-invariant,
i.e. the IS estimator does not depend on the cross-over probability of the
channel. Also, the proposed algorithms are shown to be suitable for the
estimation of the error-correcting capability of the code and the decoder.
Finally, the effectiveness of the algorithms is confirmed through simulation
results in comparison to standard Monte Carlo method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1378</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1378</id><created>2013-11-06</created><authors><author><keyname>Khairnar</keyname><forenames>Mrs. Vaishali D.</forenames></author><author><keyname>Kotecha</keyname><forenames>Dr. Ketan</forenames></author></authors><title>Simulation-Based Performance Evaluation of Routing Protocols in
  Vehicular Ad-hoc Network</title><categories>cs.NI</categories><comments>14 pages, 19 figures. arXiv admin note: text overlap with
  arXiv:1210.3047, arXiv:1204.1207 by other authors</comments><journal-ref>&quot;International Journal of Scientific and Research Publications
  (IJSRP)&quot;, Volume 3, Issue 10, October 2013 Edition</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Vehicular Ad-hoc Network (VANET) is a collection of wireless vehicle nodes
forming a temporary network without using any centralized Road Side Unit (RSU).
VANET protocols have to face high challenges due to dynamically changing
topologies and symmetric links of networks. A suitable and effective routing
mechanism helps to extend the successful deployment of vehicular ad-hoc
networks. An attempt has been made to compare the performance of two On-demand
reactive routing protocols namely AODV and DSR which works on gateway discovery
algorithms and a geographical routing protocol namely GPSR which works on an
algorithm constantly geographical based updates network topology information
available to all nodes in VANETs for different scenarios. Comparison is made on
the basis of different metrics like throughput, packet loss, packet delivery
ratio and end-to-end delay using SUMO and NS2 simulator. In this paper we have
taken different types of scenarios for simulation and then analysed the
performance results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1395</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1395</id><created>2013-11-06</created><updated>2013-12-09</updated><authors><author><keyname>Kurz</keyname><forenames>Alexander</forenames><affiliation>University of Leicester</affiliation></author><author><keyname>Petri&#x15f;an</keyname><forenames>Daniela Luan</forenames><affiliation>University of Leicester</affiliation></author><author><keyname>Severi</keyname><forenames>Paula</forenames><affiliation>University of Leicester</affiliation></author><author><keyname>de Vries</keyname><forenames>Fer-Jan</forenames><affiliation>University of Leicester</affiliation></author></authors><title>Nominal Coalgebraic Data Types with Applications to Lambda Calculus</title><categories>cs.LO</categories><comments>52 pages, accepted for publication in LMCS</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (December
  11, 2013) lmcs:865</journal-ref><doi>10.2168/LMCS-9(4:20)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate final coalgebras in nominal sets. This allows us to define
types of infinite data with binding for which all constructions automatically
respect alpha equivalence. We give applications to the infinitary lambda
calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1406</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1406</id><created>2013-11-04</created><authors><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author><author><keyname>Ahipa&#x15f;ao&#x11f;lu</keyname><forenames>Selin Damla</forenames></author><author><keyname>Cheung</keyname><forenames>Ngai-Man</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author></authors><title>TOP-SPIN: TOPic discovery via Sparse Principal component INterference</title><categories>cs.CV cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel topic discovery algorithm for unlabeled images based on
the bag-of-words (BoW) framework. We first extract a dictionary of visual words
and subsequently for each image compute a visual word occurrence histogram. We
view these histograms as rows of a large matrix from which we extract sparse
principal components (PCs). Each PC identifies a sparse combination of visual
words which co-occur frequently in some images but seldom appear in others.
Each sparse PC corresponds to a topic, and images whose interference with the
PC is high belong to that topic, revealing the common parts possessed by the
images. We propose to solve the associated sparse PCA problems using an
Alternating Maximization (AM) method, which we modify for purpose of
efficiently extracting multiple PCs in a deflation scheme. Our approach attacks
the maximization problem in sparse PCA directly and is scalable to
high-dimensional data. Experiments on automatic topic discovery and category
prediction demonstrate encouraging performance of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1411</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1411</id><created>2013-11-06</created><updated>2014-01-24</updated><authors><author><keyname>Hou</keyname><forenames>Jie</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>Effective Secrecy: Reliability, Confusion and Stealth</title><categories>cs.IT cs.CR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A security measure called effective security is defined that includes strong
secrecy and stealth communication. Effective secrecy ensures that a message
cannot be deciphered and that the presence of meaningful communication is
hidden. To measure stealth we use resolvability and relate this to binary
hypothesis testing. Results are developed for wire-tap channels and broadcast
channels with confidential messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1419</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1419</id><created>2013-11-06</created><authors><author><keyname>Yu</keyname><forenames>Shuang</forenames></author><author><keyname>Qiao</keyname><forenames>Fei</forenames></author><author><keyname>Luo</keyname><forenames>Li</forenames></author><author><keyname>Yang</keyname><forenames>Huazhong</forenames></author></authors><title>Increasing Compression Ratio of Low Complexity Compressive Sensing Video
  Encoder with Application-Aware Configurable Mechanism</title><categories>cs.MM</categories><comments>5 pages with 6figures and 1 table,conference</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  With the development of embedded video acquisition nodes and wireless video
surveillance systems, traditional video coding methods could not meet the needs
of less computing complexity any more, as well as the urgent power consumption.
So, a low-complexity compressive sensing video encoder framework with
application-aware configurable mechanism is proposed in this paper, where novel
encoding methods are exploited based on the practical purposes of the real
applications to reduce the coding complexity effectively and improve the
compression ratio (CR). Moreover, the group of processing (GOP) size and the
measurement matrix size can be configured on the encoder side according to the
post-analysis requirements of an application example of object tracking to
increase the CR of encoder as best as possible. Simulations show the proposed
framework of encoder could achieve 60X of CR when the tracking successful rate
(SR) is still keeping above 90%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1422</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1422</id><created>2013-11-06</created><updated>2013-11-12</updated><authors><author><keyname>Zhao</keyname><forenames>Feng</forenames></author></authors><title>Structural Learning for Template-free Protein Folding</title><categories>cs.LG cs.CE q-bio.QM</categories><comments>138 pages, 7 chapters, 18 figures and 28 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The thesis is aimed to solve the template-free protein folding problem by
tackling two important components: efficient sampling in vast conformation
space, and design of knowledge-based potentials with high accuracy. We have
proposed the first-order and second-order CRF-Sampler to sample structures from
the continuous local dihedral angles space by modeling the lower and higher
order conditional dependency between neighboring dihedral angles given the
primary sequence information. A framework combining the Conditional Random
Fields and the energy function is introduced to guide the local conformation
sampling using long range constraints with the energy function.
  The relationship between the sequence profile and the local dihedral angle
distribution is nonlinear. Hence we proposed the CNF-Folder to model this
complex relationship by applying a novel machine learning model Conditional
Neural Fields which utilizes the structural graphical model with the neural
network. CRF-Samplers and CNF-Folder perform very well in CASP8 and CASP9.
  Further, a novel pairwise distance statistical potential (EPAD) is designed
to capture the dependency of the energy profile on the positions of the
interacting amino acids as well as the types of those amino acids, opposing the
common assumption that this energy profile depends only on the types of amino
acids. EPAD has also been successfully applied in the CASP 10 Free Modeling
experiment with CNF-Folder, especially outstanding on some uncommon structured
targets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1423</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1423</id><created>2013-11-06</created><authors><author><keyname>F&#xfc;gger</keyname><forenames>Matthias</forenames></author><author><keyname>Nowak</keyname><forenames>Thomas</forenames></author><author><keyname>Schmid</keyname><forenames>Ulrich</forenames></author></authors><title>Unfaithful Glitch Propagation in Existing Binary Circuit Models</title><categories>cs.OH</categories><comments>23 pages, 15 figures</comments><journal-ref>Proc. 19th IEEE International Symposium on Asynchronous Circuits
  and Systems (ASYNC 2013), IEEE Press, New York City, 2013, pp. 191-199</journal-ref><doi>10.1109/ASYNC.2013.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that no existing continuous-time, binary value-domain model for
digital circuits is able to correctly capture glitch propagation. Prominent
examples of such models are based on pure delay channels (P), inertial delay
channels (I), or the elaborate PID channels proposed by Bellido-D\'iaz et al.
We accomplish our goal by considering the solvability/non-solvability border of
a simple problem called Short-Pulse Filtration (SPF), which is closely related
to arbitration and synchronization. On one hand, we prove that SPF is solvable
in bounded time in any such model that provides channels with non-constant
delay, like I and PID. This is in opposition to the impossibility of solving
bounded SPF in real (physical) circuit models. On the other hand, for binary
circuit models with constant-delay channels, we prove that SPF cannot be solved
even in unbounded time; again in opposition to physical circuit models.
Consequently, indeed none of the binary value-domain models proposed so far
(and that we are aware of) faithfully captures glitch propagation of real
circuits. We finally show that these modeling mismatches do not hold for the
weaker eventual SPF problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1426</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1426</id><created>2013-11-06</created><updated>2014-08-22</updated><authors><author><keyname>Jaffe</keyname><forenames>Klaus</forenames></author></authors><title>Is modern science evolving in the wrong direction?</title><categories>physics.soc-ph cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present politically correct consensus is that increased exchange of
scientific insight, knowledge, practitioners and skills at the global level
brings significant benefits to all. The quantifiable scientometric changes
during the last decade, however, suggest that many areas of knowledge are
evolving in the opposite direction. Despite an increase during the last decade
of the numbers of journals and academic articles published, increases in the
number of citations the published articles receive, and increases in the number
of countries participating; important parts of the academic activity are
becoming more nationalistic. In addition, international collaboration is
decreasing in several subject areas, and in several geographic regions. For
example, countries in Asia are becoming scientifically more isolated; and
academics working in the humanities in all the regions of the world are very
nationalistic and are becoming more so. The precise consequences of this
dynamics are difficult to predict, but it certainly will have reverberations
beyond academia. The tendency of the humanities to become more provincial will
certainly not help in reducing international conflicts arising from poor
understanding of cultural differences and of diverging sociopolitical world
views. More and better data on these trends should give us a better
understanding for eventually improving academic policies worldwide.
  Keywords: International Collaboration, National, Science, Humanities, Policy
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1435</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1435</id><created>2013-11-06</created><authors><author><keyname>Yerima</keyname><forenames>Suleiman Y.</forenames></author></authors><title>Implementation and Evaluation of Measurement-Based Admission Control
  Schemes Within a Converged Networks QoS Management Framework</title><categories>cs.NI</categories><comments>16 pages, 8 figures, Journal paper</comments><journal-ref>International Journal of Computer Networks and Communications,
  IJCNC, Vol. 3, No. 4, July 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Policy-based network management (PBNM) paradigms provide an effective tool
for end-to-end resource management in converged next generation networks by
enabling unified, adaptive and scalable solutions that integrate and
co-ordinate diverse resource management mechanisms associated with
heterogeneous access technologies. In our project, a PBNM framework for
end-to-end QoS management in converged networks is being developed. The
framework consists of distributed functional entities managed within a
policy-based infrastructure to provide QoS and resource management in converged
networks. Within any QoS control framework, an effective admission control
scheme is essential for maintaining the QoS of flows present in the network.
Measurement based admission control (MBAC) and parameter based admission
control (PBAC) are two commonly used approaches. This paper presents the
implementation and analysis of various measurement-based admission control
schemes developed within a Java-based prototype of our policy-based framework.
The evaluation is made with real traffic flows on a Linux-based experimental
testbed where the current prototype is deployed. Our results show that unlike
with classic MBAC or PBAC only schemes, a hybrid approach that combines both
methods can simultaneously result in improved admission control and network
utilization efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1436</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1436</id><created>2013-11-06</created><authors><author><keyname>Yerima</keyname><forenames>Suleiman Y.</forenames></author><author><keyname>Parr</keyname><forenames>Gerard P.</forenames></author><author><keyname>McClean</keyname><forenames>Sally I.</forenames></author><author><keyname>Morrow</keyname><forenames>Philip J.</forenames></author></authors><title>Adaptive Measurement-Based Policy-Driven QoS Management with
  Fuzzy-Rule-based Resource Allocation</title><categories>cs.NI</categories><comments>26 pages, 17 figures</comments><journal-ref>Future Internet EISSN 1999-5903</journal-ref><doi>10.3390/fi4030646</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Fixed and wireless networks are increasingly converging towards common
connectivity with IP-based core networks. Providing effective end-to-end
resource and QoS management in such complex heterogeneous converged network
scenarios requires unified, adaptive and scalable solutions to integrate and
co-ordinate diverse QoS mechanisms of different access technologies with
IP-based QoS. Policy-Based Network Management (PBNM) is one approach that could
be employed to address this challenge. Hence, a policy-based framework for
end-to-end QoS management in converged networks, CNQF (Converged Networks QoS
Management Framework) has been proposed within our project. In this paper, the
CNQF architecture, a Java implementation of its prototype and experimental
validation of key elements are discussed. We then present a fuzzy-based CNQF
resource management approach and study the performance of our implementation
with real traffic flows on an experimental testbed. The results demonstrate the
efficacy of our resource-adaptive approach for practical PBNM systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1442</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1442</id><created>2013-11-06</created><updated>2014-04-01</updated><authors><author><keyname>Micheli</keyname><forenames>Giacomo</forenames></author><author><keyname>Schiavina</keyname><forenames>Michele</forenames></author></authors><title>A general construction for monoid-based knapsack protocols</title><categories>cs.CR</categories><comments>18 pages, to appear on Advances in Mathematics of Communications</comments><msc-class>94A60, 11T71</msc-class><journal-ref>Advances in Mathematics of Communications, Volume 8, Issue 3,
  (2014), Pages: 343 - 358</journal-ref><doi>10.3934/amc.2014.8.343</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a generalized version of the knapsack protocol proposed by D.
Naccache and J. Stern at the Proceedings of Eurocrypt (1997). Our new framework
will allow the construction of other knapsack protocols having similar security
features. We will outline a very concrete example of a new protocol using
extension fields of a finite field of small characteristic instead of the prime
field Z/pZ, but more efficient in terms of computational costs for
asymptotically equal information rate and similar key size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1446</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1446</id><created>2013-11-06</created><authors><author><keyname>Kumarasamy</keyname><forenames>Saravanan</forenames></author><author><keyname>B</keyname><forenames>Hemalatha</forenames></author><author><keyname>P</keyname><forenames>Hashini</forenames></author></authors><title>Cluster Based Cost Efficient Intrusion Detection System For Manet</title><categories>cs.CR cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile ad-hoc networks are temporary wireless networks. Network resources are
abnormally consumed by intruders. Anomaly and signature based techniques are
used for intrusion detection. Classification techniques are used in anomaly
based techniques. Intrusion detection techniques are used for the network
attack detection process. Two types of intrusion detection systems are
available. They are anomaly detection and signature based detection model. The
anomaly detection model uses the historical transactions with attack labels.
The signature database is used in the signature based IDS schemes.
  The mobile ad-hoc networks are infrastructure less environment. The intrusion
detection applications are placed in a set of nodes under the mobile ad-hoc
network environment. The nodes are grouped into clusters. The leader nodes are
assigned for the clusters. The leader node is assigned for the intrusion
detection process. Leader nodes are used to initiate the intrusion detection
process. Resource sharing and lifetime management factors are considered in the
leader election process. The system optimizes the leader election and intrusion
detection process.
  The system is designed to handle leader election and intrusion detection
process. The clustering scheme is optimized with coverage and traffic level.
Cost and resource utilization is controlled under the clusters. Node mobility
is managed by the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1484</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1484</id><created>2013-11-06</created><authors><author><keyname>Kallus</keyname><forenames>Zsofia</forenames></author><author><keyname>Barankai</keyname><forenames>Norbert</forenames></author><author><keyname>Kondor</keyname><forenames>Daniel</forenames></author><author><keyname>Dobos</keyname><forenames>Laszlo</forenames></author><author><keyname>Hanyecz</keyname><forenames>Tamas</forenames></author><author><keyname>Szule</keyname><forenames>Janos</forenames></author><author><keyname>Steger</keyname><forenames>Jozsef</forenames></author><author><keyname>Sebok</keyname><forenames>Tamas</forenames></author><author><keyname>Vattay</keyname><forenames>Gabor</forenames></author><author><keyname>Csabai</keyname><forenames>Istvan</forenames></author></authors><title>Regional properties of global communication as reflected in aggregated
  Twitter data</title><categories>physics.soc-ph cs.SI</categories><comments>13 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter is a popular public conversation platform with world-wide audience
and diverse forms of connections between users. In this paper we introduce the
concept of aggregated regional Twitter networks in order to characterize
communication between geopolitical regions. We present the study of a follower
and a mention graph created from an extensive data set collected during the
second half of the year of $2012$. With a k-shell decomposition the global
core-periphery structure is revealed and by means of a modified Regional-SIR
model we also consider basic information spreading properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1490</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1490</id><created>2013-11-06</created><authors><author><keyname>Wang</keyname><forenames>Ye</forenames></author><author><keyname>Rane</keyname><forenames>Shantanu</forenames></author><author><keyname>Ishwar</keyname><forenames>Prakash</forenames></author></authors><title>On Unconditionally Secure Multiparty Computation for Realizing
  Correlated Equilibria in Games</title><categories>cs.CR cs.GT cs.IT math.IT</categories><comments>4 pages, to appear at GlobalSIP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In game theory, a trusted mediator acting on behalf of the players can enable
the attainment of correlated equilibria, which may provide better payoffs than
those available from the Nash equilibria alone. We explore the approach of
replacing the trusted mediator with an unconditionally secure sampling protocol
that jointly generates the players' actions. We characterize the joint
distributions that can be securely sampled by malicious players via protocols
using error-free communication. This class of distributions depends on whether
players may speak simultaneously (&quot;cheap talk&quot;) or must speak in turn (&quot;polite
talk&quot;). In applying sampling protocols toward attaining correlated equilibria
with rational players, we observe that security against malicious parties may
be much stronger than necessary. We propose the concept of secure sampling by
rational players, and show that many more distributions are feasible given
certain utility functions. However, the payoffs attainable via secure sampling
by malicious players are a dominant subset of the rationally attainable
payoffs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1532</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1532</id><created>2013-11-06</created><authors><author><keyname>Robinson</keyname><forenames>Michael</forenames></author></authors><title>Analyzing wireless communication network vulnerability with homological
  invariants</title><categories>cs.NI math.AT</categories><comments>Submitted to ICASSP 2014</comments><msc-class>94C15, 55N30</msc-class><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article explains how sheaves and homology theory can be applied to
simplicial complex models of wireless communication networks to study their
vulnerability to jamming. It develops two classes of invariants (one local and
one global) for studying which nodes and links present more of a liability to
the network's performance when under attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1539</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1539</id><created>2013-11-06</created><authors><author><keyname>Grefenstette</keyname><forenames>Edward</forenames></author></authors><title>Category-Theoretic Quantitative Compositional Distributional Models of
  Natural Language Semantics</title><categories>cs.CL cs.LG math.CT math.LO</categories><comments>DPhil Thesis, University of Oxford, Submitted and accepted in 2013</comments><msc-class>68T50 (primary) 03B65, 18C50 (secondary)</msc-class><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis is about the problem of compositionality in distributional
semantics. Distributional semantics presupposes that the meanings of words are
a function of their occurrences in textual contexts. It models words as
distributions over these contexts and represents them as vectors in high
dimensional spaces. The problem of compositionality for such models concerns
itself with how to produce representations for larger units of text by
composing the representations of smaller units of text.
  This thesis focuses on a particular approach to this compositionality
problem, namely using the categorical framework developed by Coecke, Sadrzadeh,
and Clark, which combines syntactic analysis formalisms with distributional
semantic representations of meaning to produce syntactically motivated
composition operations. This thesis shows how this approach can be
theoretically extended and practically implemented to produce concrete
compositional distributional models of natural language semantics. It
furthermore demonstrates that such models can perform on par with, or better
than, other competing approaches in the field of natural language processing.
  There are three principal contributions to computational linguistics in this
thesis. The first is to extend the DisCoCat framework on the syntactic front
and semantic front, incorporating a number of syntactic analysis formalisms and
providing learning procedures allowing for the generation of concrete
compositional distributional models. The second contribution is to evaluate the
models developed from the procedures presented here, showing that they
outperform other compositional distributional models present in the literature.
The third contribution is to show how using category theory to solve linguistic
problems forms a sound basis for research, illustrated by examples of work on
this topic, that also suggest directions for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1565</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1565</id><created>2013-11-06</created><updated>2013-11-09</updated><authors><author><keyname>Shrestha</keyname><forenames>Anish Prasad</forenames></author><author><keyname>Kwak</keyname><forenames>Kyung Sup</forenames></author></authors><title>On Maximal Ratio Diversity with Weighting Errors for Physical Layer
  Security</title><categories>cs.IT math.IT</categories><comments>It requires some major corrections in equations and numerical results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we introduce the performance of maximal ratio combining (MRC)
with weighting errors for physical layer security. We assume both legitimate
user and eavesdropper each equipped with multiple antennas employ non ideal
MRC. The non ideal MRC is designed in terms of power correlation between the
estimated and actual fadings. We derive new closedform and generalized
expressions for secrecy outage probability. Next, we investigate the asymptotic
behavior of secrecy outage probability for high signal-to-noise ratio in the
main channel between legitimate user and transmitter. The asymptotic analysis
provides the insights about actual diversity provided by MRC with weighting
errors. We substantiate our claims with the analytic results and numerical
evaluations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1567</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1567</id><created>2013-11-06</created><updated>2014-04-29</updated><authors><author><keyname>Jeong</keyname><forenames>Seongah</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Haimovich</keyname><forenames>Alexander</forenames></author><author><keyname>Kang</keyname><forenames>Joonhyuk</forenames></author></authors><title>Beamforming Design for Joint Localization and Data Transmission in
  Distributed Antenna System</title><categories>cs.IT math.IT</categories><comments>15 pages, 9 figures, and 1 table, accepted in IEEE Transactions on
  Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distributed antenna system is studied whose goal is to provide data
communication and positioning functionalities to Mobile Stations (MSs). Each MS
receives data from a number of Base Stations (BSs), and uses the received
signal not only to extract the information but also to determine its location.
This is done based on Time of Arrival (TOA) or Time Difference of Arrival
(TDOA) measurements, depending on the assumed synchronization conditions. The
problem of minimizing the overall power expenditure of the BSs under data
throughput and localization accuracy requirements is formulated with respect to
the beamforming vectors used at the BSs. The analysis covers both
frequency-flat and frequency-selective channels, and accounts also for
robustness constraints in the presence of parameter uncertainty. The proposed
algorithmic solutions are based on rank-relaxation and Difference-of-Convex
(DC) programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1568</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1568</id><created>2013-11-06</created><authors><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Jurado</keyname><forenames>Isabel</forenames></author></authors><title>Stability of Sequence-Based Control with Random Delays and Dropouts</title><categories>math.OC cs.SY</categories><comments>7 pages, 4 Figures, to be published in IEEE Transactions on Automatic
  Control</comments><msc-class>93E15, 93C10</msc-class><doi>10.1109/TAC.2013.2286911</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study networked control of non-linear systems where system states and
tentative plant input sequences are transmitted over unreliable communication
channels. The sequences are calculated recursively by using a pre-designed
nominally stabilizing state-feedback control mapping to plant state
predictions. The controller does not require receipt acknowledgments or
knowledge of delay or dropout distributions. For the i.i.d. case, in which case
the numbers of consecutive dropouts are geometrically distributed, we show how
the resulting closed loop system can be modeled as a Markov non-linear jump
system and establish sufficient conditions for stochastic stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1587</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1587</id><created>2013-11-07</created><authors><author><keyname>Gandzha</keyname><forenames>T. V.</forenames></author><author><keyname>Panov</keyname><forenames>S. A</forenames></author></authors><title>Tasks and architecture of documentation subsystem in multi-level
  modeling environment MARS</title><categories>cs.OH</categories><comments>in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article describes the automated documentation system designed to generate
reports on research conducted by computer complex technical objects and systems
in multi-level modeling environment {\guillemotleft}MARS{\guillemotright}. We
defined the purposes, tasks and abilities of documentation system and examined
the types and structure of documents, and gave an example of its practical use
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1602</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1602</id><created>2013-11-07</created><authors><author><keyname>Li</keyname><forenames>Jianwen</forenames></author><author><keyname>Pu</keyname><forenames>Geguang</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Yao</keyname><forenames>Yinbo</forenames></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames></author><author><keyname>he</keyname><forenames>Jifeng</forenames></author></authors><title>Polsat: A Portfolio LTL Satisfiability Solver</title><categories>cs.LO</categories><comments>11 pages, 1 table, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a portfolio LTL-satisfiability solver, called
Polsat. To achieve fast satisfiability checking for LTL formulas, the tool
integrates four representative LTL solvers: pltl, TRP++, NuSMV, and Aalta. The
idea of Polsat is to run the component solvers in parallel to get best overall
performance; once one of the solvers terminates, it stops all other solvers.
Remarkably, the Polsat solver utilizes the power of modern multi-core compute
clusters. The empirical experiments show that Polsat takes advantages of it.
Further, Polsat is also a testing plat- form for all LTL solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1610</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1610</id><created>2013-11-07</created><authors><author><keyname>Ferraioli</keyname><forenames>Diodato</forenames></author><author><keyname>Goldberg</keyname><forenames>Paul W.</forenames></author><author><keyname>Ventre</keyname><forenames>Carmine</forenames></author></authors><title>Decentralized Dynamics for Finite Opinion Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Game theory studies situations in which strategic players can modify the
state of a given system, due to the absence of a central authority. Solution
concepts, such as Nash equilibrium, are defined to predict the outcome of such
situations. In multi-player settings, it has been pointed out that to be
realistic, a solution concept should be obtainable via processes that are
decentralized and reasonably simple. Accordingly we look at the computation of
solution concepts by means of decentralized dynamics. These are algorithms in
which players move in turns to improve their own utility and the hope is that
the system reaches an &quot;equilibrium&quot; quickly.
  We study these dynamics for the class of opinion games, recently introduced
by Bindel et al. [Bindel et al., FOCS2011]. These are games, important in
economics and sociology, that model the formation of an opinion in a social
network. We study best-response dynamics and show upper and lower bounds on the
convergence to Nash equilibria. We also study a noisy version of best-response
dynamics, called logit dynamics, and prove a host of results about its
convergence rate as the noise in the system varies. To get these results, we
use a variety of techniques developed to bound the mixing time of Markov
chains, including coupling, spectral characterizations and bottleneck ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1616</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1616</id><created>2013-11-07</created><updated>2014-04-28</updated><authors><author><keyname>Bun</keyname><forenames>Mark</forenames></author><author><keyname>Thaler</keyname><forenames>Justin</forenames></author></authors><title>Hardness Amplification and the Approximate Degree of Constant-Depth
  Circuits</title><categories>cs.CC</categories><comments>47 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a generic form of hardness amplification for the approximability
of constant-depth Boolean circuits by polynomials. Specifically, we show that
if a Boolean circuit cannot be pointwise approximated by low-degree polynomials
to within constant error in a certain one-sided sense, then an OR of disjoint
copies of that circuit cannot be pointwise approximated even with very high
error. As our main application, we show that for every sequence of degrees
$d(n)$, there is an explicit depth-three circuit $F: \{-1,1\}^n \to \{-1,1\}$
of polynomial-size such that any degree-$d$ polynomial cannot pointwise
approximate $F$ to error better than
$1-\exp\left(-\tilde{\Omega}(nd^{-3/2})\right)$. As a consequence of our main
result, we obtain an $\exp\left(-\tilde{\Omega}(n^{2/5})\right)$ upper bound on
the the discrepancy of a function in AC$^0$, and an
$\exp\left(\tilde{\Omega}(n^{2/5})\right)$ lower bound on the threshold weight
of AC$^0$, improving over the previous best results of
$\exp\left(-\Omega(n^{1/3})\right)$ and $\exp\left(\Omega(n^{1/3})\right)$
respectively.
  Our techniques also yield a new lower bound of
$\Omega\left(n^{1/2}/\log^{(d-2)/2}(n)\right)$ on the approximate degree of the
AND-OR tree of depth $d$, which is tight up to polylogarithmic factors for any
constant $d$, as well as new bounds for read-once DNF formulas. In turn, these
results imply new lower bounds on the communication and circuit complexity of
these classes, and demonstrate strong limitations on existing PAC learning
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1618</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1618</id><created>2013-11-07</created><authors><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Fagerholm</keyname><forenames>Fabian</forenames></author><author><keyname>Kettunen</keyname><forenames>Petri</forenames></author><author><keyname>Pagels</keyname><forenames>Max</forenames></author><author><keyname>Partanen</keyname><forenames>Jari</forenames></author></authors><title>Experiences and Insights from Applying GQM+Strategies in a Systems
  Product Development Organisation</title><categories>cs.SE</categories><comments>8 pages. Proceedings of the 39th EUROMICRO Conference on Software
  Engineering and Advanced Applications (SEAA 2013), Santander, Spain,
  September 2013, The final publication is available at
  http://ieeexplore.ieee.org.
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6619491</comments><doi>10.1109/SEAA.2013.62</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aligning software-related activities with corporate strategies and goals is
increasingly important for several reasons such as increasing the customer
satisfaction in software-based products and services. Several approaches have
been proposed to create such an alignment. GQM+Strategies is an approach that
applies measurement principles to link goals and strategies on different levels
of an organisation. In this paper, we describe experiences from applying
GQM+Strategies to elicit, link, and align the goals of an integrated systems
product development organisation across multiple organisational levels. We
provide insights into how GQM+Strategies was applied during a five- month
period. The paper presents the enacted application process and main lessons
learnt. In addition, related approaches are described and an outlook on future
work is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1626</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1626</id><created>2013-11-07</created><updated>2013-11-07</updated><authors><author><keyname>Yelbay</keyname><forenames>Belma</forenames></author><author><keyname>Birbil</keyname><forenames>S. Ilker</forenames></author><author><keyname>Bulbul</keyname><forenames>Kerem</forenames></author><author><keyname>Jamil</keyname><forenames>Hasan M.</forenames></author></authors><title>Trade-offs Computing Minimum Hub Cover toward Optimized Graph Query
  Processing</title><categories>cs.DB cs.DS</categories><comments>12 pages, 6 figures and 2 algorithms</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  As techniques for graph query processing mature, the need for optimization is
increasingly becoming an imperative. Indices are one of the key ingredients
toward efficient query processing strategies via cost-based optimization. Due
to the apparent absence of a common representation model, it is difficult to
make a focused effort toward developing access structures, metrics to evaluate
query costs, and choose alternatives. In this context, recent interests in
covering-based graph matching appears to be a promising direction of research.
In this paper, our goal is to formally introduce a new graph representation
model, called Minimum Hub Cover, and demonstrate that this representation
offers interesting strategic advantages, facilitates construction of candidate
graphs from graph fragments, and helps leverage indices in novel ways for query
optimization. However, similar to other covering problems, minimum hub cover is
NP-hard, and thus is a natural candidate for optimization. We claim that
computing the minimum hub cover leads to substantial cost reduction for graph
query processing. We present a computational characterization of minimum hub
cover based on integer programming to substantiate our claim and investigate
its computational cost on various graph types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1632</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1632</id><created>2013-11-07</created><updated>2013-12-05</updated><authors><author><keyname>Herre</keyname><forenames>Heinrich</forenames></author></authors><title>Persistence, Change, and the Integration of Objects and Processes in the
  Framework of the General Formal Ontology</title><categories>cs.AI</categories><comments>13 pages; minor revisions (compared to version 1), mainly wording and
  typos</comments><acm-class>I.2.4; H.1.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discuss various problems, associated to temporal phenomena.
These problems include persistence and change, the integration of objects and
processes, and truth-makers for temporal propositions. We propose an approach
which interprets persistence as a phenomenon emanating from the activity of the
mind, and which, additionally, postulates that persistence, finally, rests on
personal identity. The General Formal Ontology (GFO) is a top level ontology
being developed at the University of Leipzig. Top level ontologies can be
roughly divided into 3D-ontologies, and 4D-ontologies. GFO is the only top
level ontology, used in applications, which is a 4D-ontology admitting
additionally 3D objects. Objects and processes are integrated in a natural way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1640</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1640</id><created>2013-11-07</created><updated>2014-07-15</updated><authors><author><keyname>Bernabeu</keyname><forenames>Miguel O.</forenames></author><author><keyname>Jones</keyname><forenames>Martin</forenames></author><author><keyname>Nielsen</keyname><forenames>Jens H.</forenames></author><author><keyname>Kr&#xfc;ger</keyname><forenames>Timm</forenames></author><author><keyname>Nash</keyname><forenames>Rupert W.</forenames></author><author><keyname>Groen</keyname><forenames>Derek</forenames></author><author><keyname>Schmieschek</keyname><forenames>Sebastian</forenames></author><author><keyname>Hetherington</keyname><forenames>James</forenames></author><author><keyname>Gerhardt</keyname><forenames>Holger</forenames></author><author><keyname>Franco</keyname><forenames>Claudio A.</forenames></author><author><keyname>Coveney</keyname><forenames>Peter V.</forenames></author></authors><title>Computer simulations reveal complex distribution of haemodynamic forces
  in a mouse retina model of angiogenesis</title><categories>cs.CE physics.bio-ph q-bio.QM</categories><comments>34 pages, 12 figures, accepted for publication at the Journal of the
  Royal Society Interface</comments><msc-class>92-08</msc-class><acm-class>I.6.8; J.3</acm-class><journal-ref>J. R. Soc. Interface 6 October 2014 vol. 11 no. 99 20140543</journal-ref><doi>10.1098/rsif.2014.0543</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is currently limited understanding of the role played by haemodynamic
forces on the processes governing vascular development. One of many obstacles
to be overcome is being able to measure those forces, at the required
resolution level, on vessels only a few micrometres thick. In the current
paper, we present an in silico method for the computation of the haemodynamic
forces experienced by murine retinal vasculature (a widely used vascular
development animal model) beyond what is measurable experimentally. Our results
show that it is possible to reconstruct high-resolution three-dimensional
geometrical models directly from samples of retinal vasculature and that the
lattice-Boltzmann algorithm can be used to obtain accurate estimates of the
haemodynamics in these domains. We generate flow models from samples obtained
at postnatal days (P) 5 and 6. Our simulations show important differences
between the flow patterns recovered in both cases, including observations of
regression occurring in areas where wall shear stress gradients exist. We
propose two possible mechanisms to account for the observed increase in
velocity and wall shear stress between P5 and P6: i) the measured reduction in
typical vessel diameter between both time points, ii) the reduction in network
density triggered by the pruning process. The methodology developed herein is
applicable to other biomedical domains where microvasculature can be imaged but
experimental flow measurements are unavailable or difficult to obtain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1642</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1642</id><created>2013-11-07</created><authors><author><keyname>Ehler</keyname><forenames>Martin</forenames></author><author><keyname>Fornasier</keyname><forenames>Massimo</forenames></author><author><keyname>Sigl</keyname><forenames>Juliane</forenames></author></authors><title>Quasi-Linear Compressed Sensing</title><categories>math.NA cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by significant real-life applications, in particular, sparse phase
retrieval and sparse pulsation frequency detection in Asteroseismology, we
investigate a general framework for compressed sensing, where the measurements
are quasi-linear. We formulate natural generalizations of the well-known
Restricted Isometry Property (RIP) towards nonlinear measurements, which allow
us to prove both unique identifiability of sparse signals as well as the
convergence of recovery algorithms to compute them efficiently. We show that
for certain randomized quasi-linear measurements, including Lipschitz
perturbations of classical RIP matrices and phase retrieval from random
projections, the proposed restricted isometry properties hold with high
probability. We analyze a generalized Orthogonal Least Squares (OLS) under the
assumption that magnitudes of signal entries to be recovered decay fast. Greed
is good again, as we show that this algorithm performs efficiently in phase
retrieval and asteroseismology. For situations where the decay assumption on
the signal does not necessarily hold, we propose two alternative algorithms,
which are natural generalizations of the well-known iterative hard and
soft-thresholding. While these algorithms are rarely successful for the
mentioned applications, we show their strong recovery guarantees for
quasi-linear measurements which are Lipschitz perturbations of RIP matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1644</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1644</id><created>2013-11-07</created><authors><author><keyname>Dubiner</keyname><forenames>Moshe</forenames></author><author><keyname>Gavish</keyname><forenames>Matan</forenames></author><author><keyname>Singer</keyname><forenames>Yoram</forenames></author></authors><title>The Maximum Entropy Relaxation Path</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relaxed maximum entropy problem is concerned with finding a probability
distribution on a finite set that minimizes the relative entropy to a given
prior distribution, while satisfying relaxed max-norm constraints with respect
to a third observed multinomial distribution. We study the entire relaxation
path for this problem in detail. We show existence and a geometric description
of the relaxation path. Specifically, we show that the maximum entropy
relaxation path admits a planar geometric description as an increasing,
piecewise linear function in the inverse relaxation parameter. We derive fast
algorithms for tracking the path. In various realistic settings, our algorithms
require $O(n\log(n))$ operations for probability distributions on $n$ points,
making it possible to handle large problems. Once the path has been recovered,
we show that given a validation set, the family of admissible models is reduced
from an infinite family to a small, discrete set. We demonstrate the merits of
our approach in experiments with synthetic data and discuss its potential for
the estimation of compact n-gram language models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1666</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1666</id><created>2013-11-07</created><updated>2014-09-19</updated><authors><author><keyname>Vlasov</keyname><forenames>Alexander Yu.</forenames></author></authors><title>Quantum Circuits and Spin(3n) Groups</title><categories>quant-ph cs.CC math-ph math.MP</categories><comments>v1. REVTeX 4-1, 2 columns, 10 pages, no figures, v3. extended,
  LaTeX2e, 1 col., 23+2 pages, v4. typos, accepted for publication</comments><journal-ref>Q. Inf. Comp., vol. 15, no.3/4, pp. 235-259, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All quantum gates with one and two qubits may be described by elements of
$Spin$ groups due to isomorphisms $Spin(3) \simeq SU(2)$ and $Spin(6) \simeq
SU(4)$. However, the group of $n$-qubit gates $SU(2^n)$ for $n &gt; 2$ has bigger
dimension than $Spin(3n)$. A quantum circuit with one- and two-qubit gates may
be used for construction of arbitrary unitary transformation $SU(2^n)$.
Analogously, the `$Spin(3n)$ circuits' are introduced in this work as products
of elements associated with one- and two-qubit gates with respect to the
above-mentioned isomorphisms.
  The matrix tensor product implementation of the $Spin(3n)$ group together
with relevant models by usual quantum circuits with $2n$ qubits are
investigated in such a framework. A certain resemblance with well-known sets of
non-universal quantum gates e.g., matchgates, noninteracting-fermion quantum
circuits) related with $Spin(2n)$ may be found in presented approach. Finally,
a possibility of the classical simulation of such circuits in polynomial time
is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1667</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1667</id><created>2013-11-07</created><authors><author><keyname>Yavits</keyname><forenames>Leonid</forenames></author><author><keyname>Morad</keyname><forenames>Amir</forenames></author><author><keyname>Ginosar</keyname><forenames>Ran</forenames></author></authors><title>3D Cache Hierarchy Optimization</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3D integration has the potential to improve the scalability and performance
of Chip Multiprocessors (CMP). A closed form analytical solution for optimizing
3D CMP cache hierarchy is developed. It allows optimal partitioning of the
cache hierarchy levels into 3D silicon layers and optimal allocation of area
among cache hierarchy levels under constrained area and power budgets. The
optimization framework is extended by incorporating the impact of multithreaded
data sharing on the private cache miss rate. An analytical model for cache
access time as a function of cache size and a number of 3D partitions is
proposed and verified using CACTI simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1694</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1694</id><created>2013-11-07</created><authors><author><keyname>Chadha</keyname><forenames>Ankit</forenames></author><author><keyname>Satam</keyname><forenames>Neha</forenames></author><author><keyname>Wali</keyname><forenames>Vibha</forenames></author></authors><title>Biometric Signature Processing &amp; Recognition Using Radial Basis Function
  Network</title><categories>cs.CV</categories><comments>CiiT International Journal of Biometrics and Bioinformatics September
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic recognition of signature is a challenging problem which has
received much attention during recent years due to its many applications in
different fields. Signature has been used for long time for verification and
authentication purpose. Earlier methods were manual but nowadays they are
getting digitized. This paper provides an efficient method to signature
recognition using Radial Basis Function Network. The network is trained with
sample images in database. Feature extraction is performed before using them
for training. For testing purpose, an image is made to undergo
rotation-translation-scaling correction and then given to network. The network
successfully identifies the original image and gives correct output for stored
database images also. The method provides recognition rate of approximately 80%
for 200 samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1698</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1698</id><created>2013-11-05</created><authors><author><keyname>Lamersdorf</keyname><forenames>Ansgar</forenames><affiliation>University of Kaiserslautern</affiliation></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames><affiliation>Fraunhofer IESE</affiliation></author><author><keyname>Rombach</keyname><forenames>Dieter</forenames><affiliation>University of Kaiserslautern</affiliation><affiliation>Fraunhofer IESE</affiliation></author></authors><title>A Survey on the State of the Practice in Distributed Software
  Development: Criteria for Task Allocation</title><categories>cs.SE</categories><comments>9 pages, 1 figure. The final publication is available at
  http://ieeexplore.ieee.org. Link:
  http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5196918</comments><acm-class>D.2; K.6.1; K.6.3</acm-class><journal-ref>Proceedings of the 4th IEEE International Conference on Global
  Software Engineering (ICGSE 2009), pp. 41-50, 2009</journal-ref><doi>10.1109/ICGSE.2009.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The allocation of tasks can be seen as a success-critical management activity
in distributed development projects. However, such task allocation is still one
of the major challenges in global software development due to an insufficient
understanding of the criteria that influence task allocation decisions. This
article presents a qualitative study aimed at identifying and understanding
such criteria that are used in practice. Based on interviews with managers from
selected software development organizations, criteria currently applied in
industry are identified. One important result is, for instance, that the
sourcing strategy and the type of software to be developed have a significant
effect on the applied criteria. The article presents the goals, design, and
results of the study as well as an overview of related and future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1700</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1700</id><created>2013-11-07</created><authors><author><keyname>Chadha</keyname><forenames>Ankit</forenames></author><author><keyname>Satam</keyname><forenames>Neha</forenames></author><author><keyname>Sood</keyname><forenames>Rakshak</forenames></author><author><keyname>Bade</keyname><forenames>Dattatray</forenames></author></authors><title>Image Steganography using Karhunen-Loeve Transform and Least Bit
  Substitution</title><categories>cs.MM</categories><journal-ref>International Journal of Computer Applications 79(9):31-37,
  October 2013</journal-ref><doi>10.5120/13771-1628 10.5120/13771-1628 10.5120/13771-1628</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As communication channels are increasing in number, reliability of faithful
communication is reducing. Hacking and tempering of data are two major issues
for which security should be provided by channel. This raises the importance of
steganography. In this paper, a novel method to encode the message information
inside a carrier image has been described. It uses Karhunen-Lo\`eve Transform
for compression of data and Least Bit Substitution for data encryption.
Compression removes redundancy and thus also provides encoding to a level. It
is taken further by means of Least Bit Substitution. The algorithm used for
this purpose uses pixel matrix which serves as a best tool to work on. Three
different sets of images were used with three different numbers of bits to be
substituted by message information. The experimental results show that
algorithm is time efficient and provides high data capacity. Further, it can
decrypt the original data effectively. Parameters such as carrier error and
message error were calculated for each set and were compared for performance
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1704</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1704</id><created>2013-11-07</created><updated>2014-05-20</updated><authors><author><keyname>Gopalan</keyname><forenames>Prem</forenames></author><author><keyname>Hofman</keyname><forenames>Jake M.</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author></authors><title>Scalable Recommendation with Poisson Factorization</title><categories>cs.IR cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a Bayesian Poisson matrix factorization model for forming
recommendations from sparse user behavior data. These data are large user/item
matrices where each user has provided feedback on only a small subset of items,
either explicitly (e.g., through star ratings) or implicitly (e.g., through
views or purchases). In contrast to traditional matrix factorization
approaches, Poisson factorization implicitly models each user's limited
attention to consume items. Moreover, because of the mathematical form of the
Poisson likelihood, the model needs only to explicitly consider the observed
entries in the matrix, leading to both scalable computation and good predictive
performance. We develop a variational inference algorithm for approximate
posterior inference that scales up to massive data sets. This is an efficient
algorithm that iterates over the observed entries and adjusts an approximate
posterior over the user/item representations. We apply our method to large
real-world user data containing users rating movies, users listening to songs,
and users reading scientific papers. In all these settings, Bayesian Poisson
factorization outperforms state-of-the-art matrix factorization methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1707</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1707</id><created>2013-11-07</created><authors><author><keyname>Gagarin</keyname><forenames>Andrei</forenames></author><author><keyname>Zverovich</keyname><forenames>Vadim</forenames></author></authors><title>The probabilistic approach to limited packings in graphs</title><categories>cs.DM math.CO</categories><comments>10 pages</comments><msc-class>05C70, 68W20, 05C85, 68R10, 90B15, 05C69</msc-class><journal-ref>Discrete Appl. Math. 184 (2015), pp.146-153</journal-ref><doi>10.1016/j.dam.2014.11.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider (closed neighbourhood) packings and their generalization in
graphs. A vertex set X in a graph G is a k-limited packing if for any vertex
$v\in V(G)$, $\left|N[v] \cap X\right| \le k$, where N[v] is the closed
neighbourhood of v. The k-limited packing number $L_k(G)$ of a graph G is the
largest size of a k-limited packing in G. Limited packing problems can be
considered as secure facility location problems in networks.
  In this paper, we develop a new probabilistic approach to limited packings in
graphs, resulting in lower bounds for the k-limited packing number and a
randomized algorithm to find k-limited packings satisfying the bounds. In
particular, we prove that for any graph G of order n with maximum vertex degree
$\Delta$, $$L_k(G) \ge {kn \over (k+1)\sqrt[k]{\pmatrix{\Delta \cr k} (\Delta
+1)}}.$$ The problem of finding a maximum size k-limited packing is known to be
NP-complete even in split or bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1712</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1712</id><created>2013-11-07</created><authors><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author><author><keyname>Wang</keyname><forenames>Li</forenames></author><author><keyname>Lv</keyname><forenames>Tiejun</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Approximate Bayesian Probabilistic-Data-Association-Aided Iterative
  Detection for MIMO Systems Using Arbitrary M-ary Modulation</title><categories>cs.IT math.IT</categories><comments>13 pages, 14 figures, 1 table, published in IEEE Transactions on
  Vehicular Technology, vol. 62, no. 3, pp. 1228-1240, March, 2013</comments><journal-ref>IEEE Transactions on Vehicular Technology, vol. 62, no. 3, pp.
  1228-1240, March, 2013</journal-ref><doi>10.1109/TVT.2012.2227863</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the issue of designing an iterative-detection-and-decoding
(IDD)-aided receiver, relying on the low-complexity probabilistic data
association (PDA) method, is addressed for turbo-coded
multiple-input-multiple-output (MIMO) systems using general M-ary modulations.
We demonstrate that the classic candidate-search-aided bit-based extrinsic
log-likelihood ratio (LLR) calculation method is not applicable to the family
of PDA-based detectors. Additionally, we reveal that, in contrast to the
interpretation in the existing literature, the output symbol probabilities of
existing PDA algorithms are not the true a posteriori probabilities (APPs) but,
rather, the normalized symbol likelihoods. Therefore, the classic relationship,
where the extrinsic LLRs are given by subtracting the a priori LLRs from the a
posteriori LLRs, does not hold for the existing PDA-based detectors. Motivated
by these revelations, we conceive a new approximate Bayesian-theorem-based
logarithmic-domain PDA (AB-Log-PDA) method and unveil the technique of
calculating bit-based extrinsic LLRs for the AB-Log-PDA, which facilitates the
employment of the AB-Log-PDA in a simplified IDD receiver structure.
Additionally, we demonstrate that we may dispense with inner iterations within
the AB-Log-PDA in the context of IDD receivers. Our complexity analysis and
numerical results recorded for Nakagami-m fading channels demonstrate that the
proposed AB-Log-PDA-based IDD scheme is capable of achieving a performance
comparable with that of the optimal maximum a posteriori (MAP)-detector-based
IDD receiver, while imposing significantly lower computational complexity in
the scenarios considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1714</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1714</id><created>2013-11-07</created><updated>2014-11-21</updated><authors><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author></authors><title>KaHIP v0.7 -- Karlsruhe High Quality Partitioning -- User Guide</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper serves as a user guide to the graph partitioning framework KaHIP
(Karlsruhe High Quality Partitioning). We give a rough overview of the
techniques used within the framework and describe the user interface as well as
the file formats used. Moreover, we provide a short description of the current
library functions provided within the framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1721</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1721</id><created>2013-11-07</created><authors><author><keyname>Adamek</keyname><forenames>Jiri</forenames></author><author><keyname>Sousa</keyname><forenames>Lurdes</forenames></author><author><keyname>Velebil</keyname><forenames>Jiri</forenames></author></authors><title>Kan injectivity in order-enriched categories</title><categories>cs.LO math.CT</categories><comments>23 pages</comments><doi>10.1017/S0960129514000024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous lattices were characterised by Martin Escardo as precisely the
objects that are Kan-injective w.r.t. a certain class of morphisms. We study
Kan-injectivity in general categories enriched in posets. For every class H of
morphisms we study the subcategory of all objects Kan-injective w.r.t. H and
all morphisms preserving Kan-extensions. For categories such as Top_0 and Pos
we prove that whenever H is a set of morphisms, the above subcategory is
monadic, and the monad it creates is a Kock-Zoeberlein monad. However, this
does not generalise to proper classes: we present a class of continuous
mappings in Top_0 for which Kan-injectivity does not yield a monadic category.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1722</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1722</id><created>2013-11-07</created><authors><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author><author><keyname>Sangiorgi</keyname><forenames>Davide</forenames></author><author><keyname>Alberti</keyname><forenames>Michele</forenames></author></authors><title>On Coinductive Equivalences for Higher-Order Probabilistic Functional
  Programs (Long Version)</title><categories>cs.PL cs.LO</categories><comments>47 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study bisimulation and context equivalence in a probabilistic
$\lambda$-calculus. The contributions of this paper are threefold. Firstly we
show a technique for proving congruence of probabilistic applicative
bisimilarity. While the technique follows Howe's method, some of the
technicalities are quite different, relying on non-trivial &quot;disentangling&quot;
properties for sets of real numbers. Secondly we show that, while bisimilarity
is in general strictly finer than context equivalence, coincidence between the
two relations is attained on pure $\lambda$-terms. The resulting equality is
that induced by Levy-Longo trees, generally accepted as the finest extensional
equivalence on pure $\lambda$-terms under a lazy regime. Finally, we derive a
coinductive characterisation of context equivalence on the whole probabilistic
language, via an extension in which terms akin to distributions may appear in
redex position. Another motivation for the extension is that its operational
semantics allows us to experiment with a different congruence technique, namely
that of logical bisimilarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1723</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1723</id><created>2013-11-07</created><updated>2015-01-09</updated><authors><author><keyname>Mattern</keyname><forenames>Christopher</forenames></author></authors><title>On Probability Estimation via Relative Frequencies and Discount</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probability estimation is an elementary building block of every statistical
data compression algorithm. In practice probability estimation is often based
on relative letter frequencies which get scaled down, when their sum is too
large. Such algorithms are attractive in terms of memory requirements, running
time and practical performance. However, there still is a lack of theoretical
understanding. In this work we formulate a typical probability estimation
algorithm based on relative frequencies and frequency discount, Algorithm RFD.
Our main contribution is its theoretical analysis. We show that the code length
it requires above an arbitrary piecewise stationary model with bounded and
unbounded letter probabilities is small. This theoretically confirms the
recency effect of periodic frequency discount, which has often been observed
empirically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1725</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1725</id><created>2013-11-07</created><authors><author><keyname>Gray</keyname><forenames>Norman</forenames></author><author><keyname>Labrosse</keyname><forenames>Nicolas</forenames></author><author><keyname>Honeychurch</keyname><forenames>Sarah</forenames></author><author><keyname>Draper</keyname><forenames>Steve</forenames></author><author><keyname>Barr</keyname><forenames>Niall</forenames></author></authors><title>Tagging and Linking Lecture Audio Recordings: Goals and Practice</title><categories>physics.ed-ph cs.CY cs.HC</categories><comments>10 pages, 1 figure; draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Making and distributing audio recordings of lectures is cheap and technically
straightforward, and these recordings represent an underexploited teaching
resource. We explore the reasons why such recordings are not more used; we
believe the barriers inhibiting such use should be easily overcome. Students
can listen to a lecture they missed, or re-listen to a lecture at revision
time, but their interaction is limited by the affordances of the replaying
technology. Listening to lecture audio is generally solitary, linear, and
disjoint from other available media.
  In this paper, we describe a tool we are developing at the University of
Glasgow, which enriches students' interactions with lecture audio. We describe
our experiments with this tool in session 2012--13. Fewer students used the
tool than we expected would naturally do so, and we discuss some possible
explanations for this.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1729</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1729</id><created>2013-11-07</created><authors><author><keyname>G</keyname><forenames>Nikita Nahar</forenames></author><author><keyname>wora</keyname><forenames>Pujita K</forenames></author><author><keyname>Kumaresh</keyname><forenames>Sakthi</forenames></author></authors><title>Managing Requirement Elicitation Issues Using Step-Wise Refinement Model</title><categories>cs.SE</categories><comments>7 pages, 5 figures, 1 table</comments><journal-ref>IJASCSE, volume 2, issue 5,2013 page number 27-33</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a Step-wise Refinement model is proposed to elicit
requirements in a more effective manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1731</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1731</id><created>2013-11-07</created><updated>2013-11-07</updated><authors><author><keyname>Airoldi</keyname><forenames>Edoardo M</forenames></author><author><keyname>Costa</keyname><forenames>Thiago B</forenames></author><author><keyname>Chan</keyname><forenames>Stanley H</forenames></author></authors><title>Stochastic blockmodel approximation of a graphon: Theory and consistent
  estimation</title><categories>stat.ME cs.LG cs.SI physics.data-an stat.ML</categories><comments>20 pages, 4 figures, 2 algorithms. Neural Information Processing
  Systems (NIPS), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-parametric approaches for analyzing network data based on exchangeable
graph models (ExGM) have recently gained interest. The key object that defines
an ExGM is often referred to as a graphon. This non-parametric perspective on
network modeling poses challenging questions on how to make inference on the
graphon underlying observed network data. In this paper, we propose a
computationally efficient procedure to estimate a graphon from a set of
observed networks generated from it. This procedure is based on a stochastic
blockmodel approximation (SBA) of the graphon. We show that, by approximating
the graphon with a stochastic block model, the graphon can be consistently
estimated, that is, the estimation error vanishes as the size of the graph
approaches infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1741</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1741</id><created>2013-11-07</created><updated>2013-11-14</updated><authors><author><keyname>Ammendola</keyname><forenames>Roberto</forenames></author><author><keyname>Biagioni</keyname><forenames>Andrea</forenames></author><author><keyname>Frezza</keyname><forenames>Ottorino</forenames></author><author><keyname>Cicero</keyname><forenames>Francesca Lo</forenames></author><author><keyname>Paolucci</keyname><forenames>Pier Stanislao</forenames></author><author><keyname>Lonardo</keyname><forenames>Alessandro</forenames></author><author><keyname>Rossetti</keyname><forenames>Davide</forenames></author><author><keyname>Simula</keyname><forenames>Francesco</forenames></author><author><keyname>Tosoratto</keyname><forenames>Laura</forenames></author><author><keyname>Vicini</keyname><forenames>Piero</forenames></author></authors><title>Architectural improvements and 28 nm FPGA implementation of the APEnet+
  3D Torus network for hybrid HPC systems</title><categories>cs.AR cs.DC physics.comp-ph</categories><comments>Proceedings for the 20th International Conference on Computing in
  High Energy and Nuclear Physics (CHEP)</comments><doi>10.1088/1742-6596/513/5/052002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern Graphics Processing Units (GPUs) are now considered accelerators for
general purpose computation. A tight interaction between the GPU and the
interconnection network is the strategy to express the full potential on
capability computing of a multi-GPU system on large HPC clusters; that is the
reason why an efficient and scalable interconnect is a key technology to
finally deliver GPUs for scientific HPC. In this paper we show the latest
architectural and performance improvement of the APEnet+ network fabric, a
FPGA-based PCIe board with 6 fully bidirectional off-board links with 34 Gbps
of raw bandwidth per direction, and X8 Gen2 bandwidth towards the host PC. The
board implements a Remote Direct Memory Access (RDMA) protocol that leverages
upon peer-to-peer (P2P) capabilities of Fermi- and Kepler-class NVIDIA GPUs to
obtain real zero-copy, low-latency GPU-to-GPU transfers. Finally, we report on
the development activities for 2013 focusing on the adoption of the latest
generation 28 nm FPGAs and the preliminary tests performed on this new
platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1753</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1753</id><created>2013-11-07</created><authors><author><keyname>Andreassen</keyname><forenames>R.</forenames></author><author><keyname>Meadows</keyname><forenames>B. T.</forenames></author><author><keyname>de Silva</keyname><forenames>M.</forenames></author><author><keyname>Sokoloff</keyname><forenames>M. D.</forenames></author><author><keyname>Tomko</keyname><forenames>K.</forenames></author></authors><title>GooFit: A library for massively parallelising maximum-likelihood fits</title><categories>cs.DC cs.MS</categories><comments>Presented at the CHEP 2013 conference</comments><doi>10.1088/1742-6596/513/5/052003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fitting complicated models to large datasets is a bottleneck of many
analyses. We present GooFit, a library and tool for constructing
arbitrarily-complex probability density functions (PDFs) to be evaluated on
nVidia GPUs or on multicore CPUs using OpenMP. The massive parallelisation of
dividing up event calculations between hundreds of processors can achieve
speedups of factors 200-300 in real-world problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1757</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1757</id><created>2013-11-07</created><updated>2015-05-13</updated><authors><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author><author><keyname>Lin</keyname><forenames>Xin</forenames></author><author><keyname>Asztalos</keyname><forenames>Andrea</forenames></author><author><keyname>Sreenivasan</keyname><forenames>Sameet</forenames></author></authors><title>Failure dynamics of the global risk network</title><categories>cs.CY cs.SI physics.soc-ph</categories><journal-ref>Nature Scientific Reports 5:10998, June 18, 2015</journal-ref><doi>10.1038/srep10998</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Risks threatening modern societies form an intricately interconnected network
that often underlies crisis situations. Yet, little is known about how risk
materializations in distinct domains influence each other. Here we present an
approach in which expert assessments of risks likelihoods and influence
underlie a quantitative model of the global risk network dynamics. The modeled
risks range from environmental to economic and technological and include
difficult to quantify risks, such as geo-political or social. Using the maximum
likelihood estimation, we find the optimal model parameters and demonstrate
that the model including network effects significantly outperforms the others,
uncovering full value of the expert collected data. We analyze the model
dynamics and study its resilience and stability. Our findings include such risk
properties as contagion potential, persistence, roles in cascades of failures
and the identity of risks most detrimental to system stability. The model
provides quantitative means for measuring the adverse effects of risk
interdependence and the materialization of risks in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1759</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1759</id><created>2013-11-07</created><updated>2014-05-30</updated><authors><author><keyname>S&#xe1;nchez-Garc&#xed;a</keyname><forenames>Rub&#xe9;n J.</forenames></author><author><keyname>Cozzo</keyname><forenames>Emanuele</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author></authors><title>Dimensionality reduction and spectral properties of multilayer networks</title><categories>physics.soc-ph cs.SI</categories><comments>minor changes; pre-published version</comments><journal-ref>Phys. Rev. E 89, 052815 (2014)</journal-ref><doi>10.1103/PhysRevE.89.052815</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network representations are useful for describing the structure of a large
variety of complex systems. Although most studies of real-world networks
suppose that nodes are connected by only a single type of edge, most natural
and engineered systems include multiple subsystems and layers of connectivity.
This new paradigm has attracted a great deal of attention and one fundamental
challenge is to characterize multilayer networks both structurally and
dynamically. One way to address this question is to study the spectral
properties of such networks. Here, we apply the framework of graph quotients,
which occurs naturally in this context, and the associated eigenvalue
interlacing results, to the adjacency and Laplacian matrices of undirected
multilayer networks. Specifically, we describe relationships between the
eigenvalue spectra of multilayer networks and their two most natural quotients,
the network of layers and the aggregate network, and show the dynamical
implications of working with either of the two simplified representations. Our
work thus contributes in particular to the study of dynamical processes whose
critical properties are determined by the spectral properties of the underlying
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1761</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1761</id><created>2013-11-07</created><authors><author><keyname>Levine</keyname><forenames>Sergey</forenames></author></authors><title>Exploring Deep and Recurrent Architectures for Optimal Control</title><categories>cs.LG cs.AI cs.NE cs.RO cs.SY</categories><comments>Appears in the Neural Information Processing Systems (NIPS 2013)
  Workshop on Deep Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sophisticated multilayer neural networks have achieved state of the art
results on multiple supervised tasks. However, successful applications of such
multilayer networks to control have so far been limited largely to the
perception portion of the control pipeline. In this paper, we explore the
application of deep and recurrent neural networks to a continuous,
high-dimensional locomotion task, where the network is used to represent a
control policy that maps the state of the system (represented by joint angles)
directly to the torques at each joint. By using a recent reinforcement learning
algorithm called guided policy search, we can successfully train neural network
controllers with thousands of parameters, allowing us to compare a variety of
architectures. We discuss the differences between the locomotion control task
and previous supervised perception tasks, present experimental results
comparing various architectures, and discuss future directions in the
application of techniques from deep learning to the problem of optimal control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1762</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1762</id><created>2013-11-07</created><authors><author><keyname>Cole</keyname><forenames>Richard</forenames></author><author><keyname>Kopelowitz</keyname><forenames>Tsvi</forenames></author><author><keyname>Lewenstein</keyname><forenames>Moshe</forenames></author></authors><title>Suffix Trays and Suffix Trists: Structures for Faster Text Indexing</title><categories>cs.DS</categories><comments>Results from this paper have appeared as an extended abstract in
  ICALP 2006</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suffix trees and suffix arrays are two of the most widely used data
structures for text indexing. Each uses linear space and can be constructed in
linear time for polynomially sized alphabets. However, when it comes to
answering queries with worst-case deterministic time bounds, the prior does so
in $O(m\log|\Sigma|)$ time, where $m$ is the query size, $|\Sigma|$ is the
alphabet size, and the latter does so in $O(m+\log n)$ time, where $n$ is the
text size. If one wants to output all appearances of the query, an additive
cost of $O(occ)$ time is sufficient, where $occ$ is the size of the output.
  We propose a novel way of combining the two into, what we call, a {\em suffix
tray}. The space and construction time remain linear and the query time
improves to $O(m+\log|\Sigma|)$ for integer alphabets from a linear range, i.e.
$\Sigma \subset \{1,\cdots, cn\}$, for an arbitrary constant $c$. The
construction and query are deterministic. Here also an additive $O(occ)$ time
is sufficient if one desires to output all appearances of the query.
  We also consider the online version of indexing, where the text arrives
online, one character at a time, and indexing queries are answered in tandem.
In this variant we create a cross between a suffix tree and a suffix list (a
dynamic variant of suffix array) to be called a {\em suffix trist}; it supports
queries in $O(m+\log|\Sigma|)$ time. The suffix trist also uses linear space.
Furthermore, if there exists an online construction for a linear-space suffix
tree such that the cost of adding a character is worst-case deterministic
$f(n,|\Sigma|)$ ($n$ is the size of the current text), then one can further
update the suffix trist in $O(f(n,|\Sigma|)+\log |\Sigma|)$ time. The best
currently known worst-case deterministic bound for $f(n,|\Sigma|)$ is $O(\log
n)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1764</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1764</id><created>2013-11-07</created><authors><author><keyname>Touzi</keyname><forenames>Amel Grissa</forenames></author><author><keyname>Massoud</keyname><forenames>Hela Ben</forenames></author><author><keyname>Ayadi</keyname><forenames>Alaya</forenames></author></authors><title>Automatic ontology generation for data mining using fca and clustering</title><categories>cs.DB cs.AI</categories><comments>10pages, 8 figures KEOD 2013, accepted but not enregistrement De:
  KEOD Secretariat [keod.secretariat@insticc.org] Envoy\'e: mardi 21 mai 2013
  10:47 We are happy to inform you that the regular paper you have submitted to
  KEOD, with number 34, entitled &quot;Automatic Ontology Generation for Data Mining
  Using FCA and Clustering&quot;, has been accepted as a Short Paper. arXiv admin
  note: text overlap with arXiv:1310.7829 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the increased need for formalized representations of the domain
of Data Mining, the success of using Formal Concept Analysis (FCA) and Ontology
in several Computer Science fields, we present in this paper a new approach for
automatic generation of Fuzzy Ontology of Data Mining (FODM), through the
fusion of conceptual clustering, fuzzy logic, and FCA. In our approach, we
propose to generate ontology taking in consideration another degree of
granularity into the process of generation. Indeed, we suggest to define an
ontology between classes resulting from a preliminary classification on the
data. We prove that this approach optimize the definition of the ontology,
offered a better interpretation of the data and optimized both the space memory
and the execution time for exploiting this data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1780</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1780</id><created>2013-11-07</created><updated>2014-09-01</updated><authors><author><keyname>Gulcehre</keyname><forenames>Caglar</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Pascanu</keyname><forenames>Razvan</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks</title><categories>cs.NE cs.LG stat.ML</categories><comments>ECML/PKDD 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose and investigate a novel nonlinear unit, called $L_p$
unit, for deep neural networks. The proposed $L_p$ unit receives signals from
several projections of a subset of units in the layer below and computes a
normalized $L_p$ norm. We notice two interesting interpretations of the $L_p$
unit. First, the proposed unit can be understood as a generalization of a
number of conventional pooling operators such as average, root-mean-square and
max pooling widely used in, for instance, convolutional neural networks (CNN),
HMAX models and neocognitrons. Furthermore, the $L_p$ unit is, to a certain
degree, similar to the recently proposed maxout unit (Goodfellow et al., 2013)
which achieved the state-of-the-art object recognition results on a number of
benchmark datasets. Secondly, we provide a geometrical interpretation of the
activation function based on which we argue that the $L_p$ unit is more
efficient at representing complex, nonlinear separating boundaries. Each $L_p$
unit defines a superelliptic boundary, with its exact shape defined by the
order $p$. We claim that this makes it possible to model arbitrarily shaped,
curved boundaries more efficiently by combining a few $L_p$ units of different
orders. This insight justifies the need for learning different orders for each
unit in the model. We empirically evaluate the proposed $L_p$ units on a number
of datasets and show that multilayer perceptrons (MLP) consisting of the $L_p$
units achieve the state-of-the-art results on a number of benchmark datasets.
Furthermore, we evaluate the proposed $L_p$ unit on the recently proposed deep
recurrent neural networks (RNN).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1793</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1793</id><created>2013-11-07</created><updated>2013-11-15</updated><authors><author><keyname>Desmontils</keyname><forenames>Emmanuel</forenames><affiliation>LINA</affiliation></author></authors><title>Une repr\'esentation en graphe pour l'enseignement de XML</title><categories>cs.OH</categories><comments>22 pages, in French</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, XML is a format widely used. In the context of computer science
teaching, it is necessary to introduce students to this format and, especially,
at its eco-system. We have developed a model to support the teaching of XML. We
propose to represent an XML schema as a graph highlighting the structural
characteristics of the valide documents. We present in this report different
graphic elements of the model and the improvements it brings to data modeling
in XML.---XML est un format actuellement tr\`es utilis\'e. Dans le cadre des
formations en informatique, il est indispensable d'initier les \'etudiants \`a
ce format et, surtout, \`a tout son \'eco-syst\`eme. Nous avons donc mis au
point un mod\`ele permettant d'appuyer l'enseignement de XML. Ce mod\`ele
propose de repr\'esenter un sch\'ema XML sous la forme d'un graphe mettant en
valeur les caract\'eristiques structurelles des documents valides. Nous
pr\'esentons dans ce rapport les diff\'erents \'el\'ements graphique du
mod\`ele et les am\'eliorations qu'il apporte \`a la mod\'elisation de
donn\'ees en XML.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1803</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1803</id><created>2013-11-07</created><authors><author><keyname>Chen</keyname><forenames>Bo</forenames></author><author><keyname>Li</keyname><forenames>Song-Song</forenames></author><author><keyname>Zhang</keyname><forenames>Yu-Zhong</forenames></author></authors><title>Strong Stability of Nash Equilibria in Load Balancing Games</title><categories>cs.GT cs.DM math.OC</categories><comments>17 pages and 4 figures</comments><doi>10.1007/s11425-014-4814-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study strong stability of Nash equilibria in load balancing games of m (m
&gt;= 2) identical servers, in which every job chooses one of the m servers and
each job wishes to minimize its cost, given by the workload of the server it
chooses.
  A Nash equilibrium (NE) is a strategy profile that is resilient to unilateral
deviations. Finding an NE in such a game is simple. However, an NE assignment
is not stable against coordinated deviations of several jobs, while a strong
Nash equilibrium (SNE) is. We study how well an NE approximates an SNE.
  Given any job assignment in a load balancing game, the improvement ratio (IR)
of a deviation of a job is defined as the ratio between the pre- and
post-deviation costs. An NE is said to be a r-approximate SNE (r &gt;= 1) if there
is no coalition of jobs such that each job of the coalition will have an IR
more than r from coordinated deviations of the coalition.
  While it is already known that NEs are the same as SNEs in the 2-server load
balancing game, we prove that, in the m-server load balancing game for any
given m &gt;= 3, any NE is a (5/4)-approximate SNE, which together with the lower
bound already established in the literature yields a tight approximation bound.
This closes the final gap in the literature on the study of approximation of
general NEs to SNEs in load balancing games. To establish our upper bound, we
make a novel use of a graph-theoretic tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1838</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1838</id><created>2013-11-07</created><updated>2014-04-15</updated><authors><author><keyname>Nieuwenhuis</keyname><forenames>Claudia</forenames></author><author><keyname>Toeppe</keyname><forenames>Eno</forenames></author><author><keyname>Gorelick</keyname><forenames>Lena</forenames></author><author><keyname>Veksler</keyname><forenames>Olga</forenames></author><author><keyname>Boykov</keyname><forenames>Yuri</forenames></author></authors><title>Efficient Regularization of Squared Curvature</title><categories>cs.CV</categories><comments>8 pages, 12 figures, to appear at IEEE conference on Computer Vision
  and Pattern Recognition (CVPR), June 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Curvature has received increased attention as an important alternative to
length based regularization in computer vision. In contrast to length, it
preserves elongated structures and fine details. Existing approaches are either
inefficient, or have low angular resolution and yield results with strong block
artifacts. We derive a new model for computing squared curvature based on
integral geometry. The model counts responses of straight line triple cliques.
The corresponding energy decomposes into submodular and supermodular pairwise
potentials. We show that this energy can be efficiently minimized even for high
angular resolutions using the trust region framework. Our results confirm that
we obtain accurate and visually pleasing solutions without strong artifacts at
reasonable run times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1839</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1839</id><created>2013-11-07</created><updated>2014-02-18</updated><authors><author><keyname>Kuindersma</keyname><forenames>Scott</forenames></author><author><keyname>Permenter</keyname><forenames>Frank</forenames></author><author><keyname>Tedrake</keyname><forenames>Russ</forenames></author></authors><title>An Efficiently Solvable Quadratic Program for Stabilizing Dynamic
  Locomotion</title><categories>cs.RO</categories><comments>6 pages, published at ICRA 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We describe a whole-body dynamic walking controller implemented as a convex
quadratic program. The controller solves an optimal control problem using an
approximate value function derived from a simple walking model while respecting
the dynamic, input, and contact constraints of the full robot dynamics. By
exploiting sparsity and temporal structure in the optimization with a custom
active-set algorithm, we surpass the performance of the best available
off-the-shelf solvers and achieve 1kHz control rates for a 34-DOF humanoid. We
describe applications to balancing and walking tasks using the simulated Atlas
robot in the DARPA Virtual Robotics Challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1851</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1851</id><created>2013-11-07</created><updated>2014-04-01</updated><authors><author><keyname>Wang</keyname><forenames>Guoming</forenames></author></authors><title>Quantum Algorithms for Approximating the Effective Resistances in
  Electrical Networks</title><categories>quant-ph cs.DS</categories><comments>24 pages, 2 figure. Lower bound added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of electrical network has many applications in algorithm design
and analysis. It is an important task to compute the basic quantities about
electrical networks, such as electrical flows and effective resistances, as
quickly as possible. Classically, to compute these quantities, one basically
need to solve a Laplacian linear system, and the best known algorithms take
$\tilde{O}(m)$ time, where $m$ is the number of edges.
  In this paper, we present two quantum algorithms for approximating the
effective resistance between any two vertices in an electrical network. Both of
them have time complexity polynomial in $\log{n}$, $d$, $c$, $1/\phi$ and
$1/\epsilon$, where $n$ is the number of vertices, $d$ is the maximum degree of
the vertices, $c$ is the ratio of the largest to the smallest edge resistance,
$\phi$ is the expansion of the network, and $\epsilon$ is the relative error.
In particular, when $d$ and $c$ are small and $\phi$ is large, our algorithms
run very fast. In contrast, it is unknown whether classical algorithms can
solve this case very fast. Furthermore, we prove that the polynomial dependence
on the inverse expansion (i.e. $1/\phi$) is necessary. As a consequence, our
algorithms cannot be significantly improved. Finally, as a by-product, our
second algorithm also produces a quantum state approximately proportional to
the electrical flow between any two vertices, which might be of independent
interest.
  Our algorithms are based on using quantum tools to analyze the algebraic
properties of graph-related matrices. While one of them relies on inverting the
Laplacian matrix, the other relies on projecting onto the kernel of the
weighted incidence matrix. It is hopeful that more quantum algorithms can be
developed in similar way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1856</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1856</id><created>2013-11-07</created><updated>2014-04-15</updated><authors><author><keyname>Gorelick</keyname><forenames>Lena</forenames></author><author><keyname>Boykov</keyname><forenames>Yuri</forenames></author><author><keyname>Veksler</keyname><forenames>Olga</forenames></author><author><keyname>Ayed</keyname><forenames>Ismail Ben</forenames></author><author><keyname>Delong</keyname><forenames>Andrew</forenames></author></authors><title>Submodularization for Quadratic Pseudo-Boolean Optimization</title><categories>cs.CV</categories><comments>8 pages, 5 figures, to appear at IEEE conference on Computer Vision
  and Pattern Recognition (CVPR), June 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many computer vision problems require optimization of binary non-submodular
energies. We propose a general optimization framework based on local submodular
approximations (LSA). Unlike standard LP relaxation methods that linearize the
whole energy globally, our approach iteratively approximates the energies
locally. On the other hand, unlike standard local optimization methods (e.g.
gradient descent or projection techniques) we use non-linear submodular
approximations and optimize them without leaving the domain of integer
solutions. We discuss two specific LSA algorithms based on &quot;trust region&quot; and
&quot;auxiliary function&quot; principles, LSA-TR and LSA-AUX. These methods obtain
state-of-the-art results on a wide range of applications outperforming many
standard techniques such as LBP, QPBO, and TRWS. While our paper is focused on
pairwise energies, our ideas extend to higher-order problems. The code is
available online (http://vision.csd.uwo.ca/code/).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1859</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1859</id><created>2013-11-07</created><authors><author><keyname>Joeris</keyname><forenames>Benson</forenames></author><author><keyname>Lindzey</keyname><forenames>Nathan</forenames></author><author><keyname>McConnell</keyname><forenames>Ross M.</forenames></author><author><keyname>Osheim</keyname><forenames>Nissa</forenames></author></authors><title>Simple DFS on the Complement of a Graph and on Partially Complemented
  Digraphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A complementation operation on a vertex of a digraph changes all outgoing
arcs into non-arcs, and outgoing non-arcs into arcs. A partially complemented
digraph $\widetilde{G}$ is a digraph obtained from a sequence of vertex
complement operations on $G$. Dahlhaus et al. showed that, given an
adjacency-list representation of $\widetilde{G}$, depth-first search (DFS) on
$G$ can be performed in $O(n + \widetilde{m})$ time, where $n$ is the number of
vertices and $\widetilde{m}$ is the number of edges in $\widetilde{G}$. To
achieve this bound, their algorithm makes use of a somewhat complicated
stack-like data structure to simulate the recursion stack, instead of
implementing it directly as a recursive algorithm. We give a recursive
$O(n+\widetilde{m})$ algorithm that uses no complicated data-structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1869</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1869</id><created>2013-11-07</created><authors><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>Optimization, Learning, and Games with Predictable Sequences</title><categories>cs.LG cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide several applications of Optimistic Mirror Descent, an online
learning algorithm based on the idea of predictable sequences. First, we
recover the Mirror Prox algorithm for offline optimization, prove an extension
to Holder-smooth functions, and apply the results to saddle-point type
problems. Next, we prove that a version of Optimistic Mirror Descent (which has
a close relation to the Exponential Weights algorithm) can be used by two
strongly-uncoupled players in a finite zero-sum matrix game to converge to the
minimax equilibrium at the rate of O((log T)/T). This addresses a question of
Daskalakis et al 2011. Further, we consider a partial information version of
the problem. We then apply the results to convex programming and exhibit a
simple algorithm for the approximate Max Flow problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1884</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1884</id><created>2013-11-08</created><authors><author><keyname>Menon</keyname><forenames>Vijay</forenames></author><author><keyname>Jha</keyname><forenames>Saurabh</forenames></author></authors><title>A Parallel Simulated Annealing Approach for the Mirrored Traveling
  Tournament Problem</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Traveling Tournament Problem (TTP) is a benchmark problem in sports
scheduling and has been extensively studied in recent years. The Mirrored
Traveling Tournament Problem (mTTP) is variation of the TTP that represents
certain types of sports scheduling problems where the main objective is to
minimize the total distance traveled by all the participating teams. In this
paper we test a parallel simulated annealing approach for solving the mTTP
using OpenMP on shared memory systems and we found that this approach is
superior especially with respect to the number of solution instances that are
probed per second. We also see that there is significant speed up of 1.5x -
2.2x in terms of number of solutions explored per unit time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1885</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1885</id><created>2013-11-08</created><authors><author><keyname>Pakmehr</keyname><forenames>Mehrdad</forenames></author><author><keyname>Wang</keyname><forenames>Timothy</forenames></author><author><keyname>Jobredeaux</keyname><forenames>Romain</forenames></author><author><keyname>Vivies</keyname><forenames>Martin</forenames></author><author><keyname>Feron</keyname><forenames>Eric</forenames></author></authors><title>Verifiable Control System Development for Gas Turbine Engines</title><categories>cs.SY math.OC</categories><comments>20 pages, 10 figures; this manuscript has been submitted to the ASME
  Turbo Expo 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A control software verification framework for gas turbine engines is
developed. A stability proof is presented for gain scheduled closed-loop engine
system based on global linearization and linear matrix inequality (LMI)
techniques. Using convex optimization tools, a single quadratic Lyapunov
function is computed for multiple linearizations near equilibrium points of the
closed-loop system. With the computed stability matrices, ellipsoid invariant
sets are constructed, which are used efficiently for DGEN turbofan engine
control code stability analysis. Then a verifiable linear gain scheduled
controller for DGEN engine is developed based on formal methods, and tested on
the engine virtual test bench. Simulation results show that the developed
verifiable gain scheduled controller is capable of regulating the engine in a
stable fashion with proper tracking performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1887</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1887</id><created>2013-11-08</created><authors><author><keyname>Song</keyname><forenames>Linqi</forenames></author><author><keyname>Xiao</keyname><forenames>Yuanzhang</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Demand Side Management in Smart Grids using a Repeated Game Framework</title><categories>cs.SY cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Demand side management (DSM) is a key solution for reducing the peak-time
power consumption in smart grids. To provide incentives for consumers to shift
their consumption to off-peak times, the utility company charges consumers
differential pricing for using power at different times of the day. Consumers
take into account these differential prices when deciding when and how much
power to consume daily. Importantly, while consumers enjoy lower billing costs
when shifting their power usage to off-peak times, they also incur discomfort
costs due to the altering of their power consumption patterns. Existing works
propose stationary strategies for the myopic consumers to minimize their
short-term billing and discomfort costs. In contrast, we model the interaction
emerging among self-interested, foresighted consumers as a repeated energy
scheduling game and prove that the stationary strategies are suboptimal in
terms of long-term total billing and discomfort costs. Subsequently, we propose
a novel framework for determining optimal nonstationary DSM strategies, in
which consumers can choose different daily power consumption patterns depending
on their preferences, routines, and needs. As a direct consequence of the
nonstationary DSM policy, different subsets of consumers are allowed to use
power in peak times at a low price. The subset of consumers that are selected
daily to have their joint discomfort and billing costs minimized is determined
based on the consumers' power consumption preferences as well as on the past
history of which consumers have shifted their usage previously. Importantly, we
show that the proposed strategies are incentive-compatible. Simulations confirm
that, given the same peak-to-average ratio, the proposed strategy can reduce
the total cost (billing and discomfort costs) by up to 50% compared to existing
DSM strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1888</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1888</id><created>2013-11-08</created><updated>2015-01-29</updated><authors><author><keyname>Selig</keyname><forenames>Marco</forenames></author><author><keyname>En&#xdf;lin</keyname><forenames>Torsten</forenames></author></authors><title>D$^3$PO - Denoising, Deconvolving, and Decomposing Photon Observations</title><categories>astro-ph.IM cs.IT math.IT physics.data-an stat.CO</categories><comments>22 pages, 8 figures, 2 tables, accepted by Astronomy &amp; Astrophysics;
  refereed version, 1 figure added, results unchanged, software available at
  http://www.mpa-garching.mpg.de/ift/d3po/</comments><journal-ref>A&amp;A 574, A74 (2015)</journal-ref><doi>10.1051/0004-6361/201323006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of astronomical images is a non-trivial task. The D3PO algorithm
addresses the inference problem of denoising, deconvolving, and decomposing
photon observations. Its primary goal is the simultaneous but individual
reconstruction of the diffuse and point-like photon flux given a single photon
count image, where the fluxes are superimposed. In order to discriminate
between these morphologically different signal components, a probabilistic
algorithm is derived in the language of information field theory based on a
hierarchical Bayesian parameter model. The signal inference exploits prior
information on the spatial correlation structure of the diffuse component and
the brightness distribution of the spatially uncorrelated point-like sources. A
maximum a posteriori solution and a solution minimizing the Gibbs free energy
of the inference problem using variational Bayesian methods are discussed.
Since the derivation of the solution is not dependent on the underlying
position space, the implementation of the D3PO algorithm uses the NIFTY package
to ensure applicability to various spatial grids and at any resolution. The
fidelity of the algorithm is validated by the analysis of simulated data,
including a realistic high energy photon count image showing a 32 x 32 arcmin^2
observation with a spatial resolution of 0.1 arcmin. In all tests the D3PO
algorithm successfully denoised, deconvolved, and decomposed the data into a
diffuse and a point-like signal estimate for the respective photon flux
components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1895</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1895</id><created>2013-11-08</created><updated>2013-11-10</updated><authors><author><keyname>Nam</keyname><forenames>Jaechang</forenames></author><author><keyname>Chen</keyname><forenames>Ning</forenames></author></authors><title>Mining Crash Fix Patterns</title><categories>cs.SE</categories><comments>7 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the life cycle of software development, developers have to fix
different kinds of bugs reported by testers or end users. The efficiency and
effectiveness of fixing bugs have a huge impact on the reliability of the
software as well as the productivity of the development team. Software
companies usually spend a large amount of money and human resources on the
testing and bug fixing departments. As a result, a better and more reliable way
to fix bugs is highly desired by them. In order to achieve such goal, in depth
studies on the characteristics of bug fixes from well maintained, highly
popular software projects are necessary. In this paper, we study the bug fixing
histories extracted from the Eclipse project, a well maintained, highly popular
open source project. After analyzing more than 36,000 bugs that belongs to
three major kinds of exception types, we are able to reveal some common fix
types that are frequently used to fix certain kinds of program exceptions. Our
analysis shows that almost all of the exceptions that belong to a certain
exception can be fixed by less than ten fix types. Our result implies that most
of the bugs in software projects can be and should be fixed by only a few
common fix patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1897</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1897</id><created>2013-11-08</created><authors><author><keyname>Retor&#xe9;</keyname><forenames>Christian</forenames><affiliation>LaBRI, INRIA Bordeaux - Sud-Ouest</affiliation></author></authors><title>Logique math\'ematique et linguistique formelle</title><categories>math.LO cs.CL</categories><comments>Transcription d'une &quot;le\c{c}on de math\'ematique d'aujourd'hui&quot;
  donn\'ee le 7 juillet 2011, in French</comments><proxy>ccsd</proxy><journal-ref>Le\c{c}ons de math\'ematiques d'aujourd'hui, G\'eraud
  S\'enizergues (Ed.) (2013) 24</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the etymology of the word shows, logic is intimately related to language,
as exemplified by the work of philosophers from Antiquity and from the
Middle-Age. At the beginning of the XX century, the crisis of the foundations
of mathematics invented mathematical logic and imposed logic as a
language-based foundation for mathematics. How did the relations between logic
and language evolved in this newly defined mathematical framework? After a
survey of the history of the relation between logic and linguistics,
traditionally focused on semantics, we focus on some present issues: 1) grammar
as a deductive system 2) the transformation of the syntactic structure of a
sentence to a logical formula representing its meaning 3) taking into account
the context when interpreting words. This lecture shows that type theory
provides a convenient framework both for natural language syntax and for the
interpretation of any of tis level (words, sentences, discourse).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1903</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1903</id><created>2013-11-08</created><authors><author><keyname>Telgarsky</keyname><forenames>Matus</forenames></author><author><keyname>Dasgupta</keyname><forenames>Sanjoy</forenames></author></authors><title>Moment-based Uniform Deviation Bounds for $k$-means and Friends</title><categories>cs.LG stat.ML</categories><comments>To appear, NIPS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose $k$ centers are fit to $m$ points by heuristically minimizing the
$k$-means cost; what is the corresponding fit over the source distribution?
This question is resolved here for distributions with $p\geq 4$ bounded
moments; in particular, the difference between the sample cost and distribution
cost decays with $m$ and $p$ as $m^{\min\{-1/4, -1/2+2/p\}}$. The essential
technical contribution is a mechanism to uniformly control deviations in the
face of unbounded parameter sets, cost functions, and source distributions. To
further demonstrate this mechanism, a soft clustering variant of $k$-means cost
is also considered, namely the log likelihood of a Gaussian mixture, subject to
the constraint that all covariance matrices have bounded spectrum. Lastly, a
rate with refined constants is provided for $k$-means instances possessing some
cluster structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1907</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1907</id><created>2013-11-08</created><authors><author><keyname>Rajput</keyname><forenames>Vibha</forenames></author><author><keyname>Katiyar</keyname><forenames>Alok</forenames></author></authors><title>Proactive bottleneck performance analysis in parallel computing using
  openMP</title><categories>cs.DC cs.PF</categories><comments>8 Pages,6 figure</comments><journal-ref>IJASCSE, Volume 2, Issue 5, 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The aim of parallel computing is to increase an application performance by
executing the application on multiple processors. OpenMP is an API that
supports multi platform shared memory programming model and shared-memory
programs are typically executed by multiple threads. The use of multi threading
can enhance the performance of application but its excessive use can degrade
the performance. This paper describes a novel approach to avoid bottlenecks in
application and provide some techniques to improve performance in OpenMP
application. This paper analyzes bottleneck performance as bottleneck inhibits
performance. Performance of multi threaded applications is limited by a variety
of bottlenecks, e.g. critical sections, barriers and so on. This paper provides
some tips how to avoid performance bottleneck problems. This paper focuses on
how to reduce overheads and overall execution time to get better performance of
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1912</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1912</id><created>2013-11-08</created><authors><author><keyname>Alama</keyname><forenames>Jesse</forenames></author></authors><title>Complete independence of an axiom system for central translations</title><categories>math.LO cs.LO</categories><comments>10 pages. Submitted to Note di Matematica</comments><msc-class>03B30, 03B35, 51D15</msc-class><acm-class>F.4.1; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recently proposed axiom system for Andr\'e's central translation structures
is improved upon. First, one of its axioms turns out to be dependent (derivable
from the other axioms). Without this axiom, the axiom system is indeed
independent. Second, whereas most of the original independence models were
infinite, finite independence models are available. Moreover, the independence
proof for one of the axioms employed proof-theoretic techniques rather than
independence models; for this axiom, too, a finite independence model exists.
For every axiom, then, there is a finite independence model. Finally, the axiom
system (without its single dependent axiom) is not only independent, but
completely independent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1915</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1915</id><created>2013-11-08</created><authors><author><keyname>Alama</keyname><forenames>Jesse</forenames></author></authors><title>Sentence complexity of theorems in Mizar</title><categories>math.LO cs.LO</categories><comments>11 pages. Submitted to the Journal of Automated Reasoning special
  issue on the 40th anniversary of the Mizar system</comments><msc-class>03A05, 03F05</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As one of the longest-running computer-assisted formal mathematics projects,
large tracts of mathematical knowledge have been formalized with the help of
the Mizar system. Because Mizar is based on first-order classical logic and set
theory, and because of its emphasis on pure mathematics, the Mizar library
offers a cornucopia for the researcher interested in foundations of
mathematics. With Mizar, one can adopt an experimental approach and take on
problems in foundations, at least those which are amenable to such
experimentation. Addressing a question posed by H. Friedman, we use Mizar to
take on the question of surveying the sentence complexity (measured by
quantifier alternation) of mathematical theorems. We find, as Friedman
suggests, that the sentence complexity of most Mizar theorems is universal
($\Pi_{1}$, or $\forall$), and as one goes higher in the sentence complexity
hierarchy the number of Mizar theorems having these complexities decreases
rapidly. The results support the intuitive idea that mathematical statements,
even when carried out an abstract set-theoretical style, are usually quite low
in the sentence complexity hierarchy (not more complex than
$\forall\exists\forall$ or $\exists\forall$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1916</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1916</id><created>2013-11-08</created><updated>2013-12-10</updated><authors><author><keyname>Salibra</keyname><forenames>Antonino</forenames><affiliation>Universit&#xe0; Ca'Foscari Venezia</affiliation></author><author><keyname>Carraro</keyname><forenames>Alberto</forenames><affiliation>Laboratoire PPS, Universit&#xe9; Paris Diderot</affiliation></author></authors><title>Ordered Models of the Lambda Calculus</title><categories>cs.LO math.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (December
  12, 2013) lmcs:726</journal-ref><doi>10.2168/LMCS-9(4:21)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answering a question by Honsell and Plotkin, we show that there are two
equations between lambda terms, the so-called subtractive equations, consistent
with lambda calculus but not simultaneously satisfied in any partially ordered
model with bottom element. We also relate the subtractive equations to the open
problem of the order-incompleteness of lambda calculus, by studying the
connection between the notion of absolute unorderability in a specific point
and a weaker notion of subtractivity (namely n-subtractivity) for partially
ordered algebras. Finally we study the relation between n-subtractivity and
relativized separation conditions in topological algebras, obtaining an
incompleteness theorem for a general topological semantics of lambda calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1917</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1917</id><created>2013-11-08</created><updated>2013-12-16</updated><authors><author><keyname>Alama</keyname><forenames>Jesse</forenames></author></authors><title>Toward a structure theory for Lorenzen dialogue games</title><categories>math.LO cs.LO</categories><comments>20 pages. Expanded version of an informal presentation given at CiE
  2010 (Computability in Europe)</comments><msc-class>03F05, 03A05</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lorenzen dialogues provide a two-player game formalism that can characterize
a variety of logics: each set $S$ of rules for such a game determines a set
$\mathcal{D}(S)$ of formulas for which one of the players (the so-called
Proponent) has a winning strategy, and the set $\mathcal{D}(S)$ can coincide
with various logics, such as intuitionistic, classical, modal, connexive, and
relevance logics. But the standard sets of rules employed for these games are
often logically opaque and can involve subtle interactions among each other.
Moreover, $\mathcal{D}(S)$ can vary in unexpected ways with $S$; small changes
in $S$, even logically well-motivated ones, can make $\mathcal{D}(S)$ logically
unusual. We pose the problem of providing a structure theory that could explain
how $\mathcal{D}(S)$ varies with $S$, and in particular, when $\mathcal{D}(S)$
is closed under modus ponens (and thus constitutes at least a minimal kind of
logic).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1928</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1928</id><created>2013-11-08</created><updated>2014-10-24</updated><authors><author><keyname>Kumar</keyname><forenames>Mahati</forenames></author><author><keyname>Manasvini</keyname><forenames>S.</forenames></author><author><keyname>Sadagopan</keyname><forenames>N.</forenames></author><author><keyname>Seshadri</keyname><forenames>Adithya</forenames></author></authors><title>On Uni Chord Free Graphs</title><categories>cs.DM</categories><comments>This paper has been withdrawn due to a bug in the algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is unichord free if it does not contain a cycle with exactly one
chord as its subgraph. In [3], it is shown that a graph is unichord free if and
only if every minimal vertex separator is a stable set. In this paper, we first
show that such a graph can be recognized in polynomial time. Further, we show
that the chromatic number of unichord free graphs is one of (2,3, \omega(G)).
We also present a polynomial-time algorithm to produce a coloring with
\omega(G) colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1935</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1935</id><created>2013-11-08</created><authors><author><keyname>Smidtas</keyname><forenames>Serge</forenames></author><author><keyname>Peyrot</keyname><forenames>Magalie</forenames></author></authors><title>Unsupervised learning human's activities by overexpressed recognized
  non-speech sounds</title><categories>cs.AI cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human activity and environment produces sounds such as, at home, the noise
produced by water, cough, or television. These sounds can be used to determine
the activity in the environment. The objective is to monitor a person's
activity or determine his environment using a single low cost microphone by
sound analysis. The purpose is to adapt programs to the activity or environment
or detect abnormal situations. Some patterns of over expressed repeatedly in
the sequences of recognized sounds inter and intra environment allow to
characterize activities such as the entrance of a person in the house, or a tv
program watched. We first manually annotated 1500 sounds of daily life activity
of old persons living at home recognized sounds. Then we inferred an ontology
and enriched the database of annotation with a crowed sourced manual annotation
of 7500 sounds to help with the annotation of the most frequent sounds. Using
learning sound algorithms, we defined 50 types of the most frequent sounds. We
used this set of recognizable sounds as a base to tag sounds and put tags on
them. By using over expressed number of motifs of sequences of the tags, we
were able to categorize using only a single low-cost microphone, complex
activities of daily life of a persona at home as watching TV, entrance in the
apartment of a person, or phone conversation including detecting unknown
activities as repeated tasks performed by users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1939</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1939</id><created>2013-11-08</created><authors><author><keyname>Zhang</keyname><forenames>Kaihua</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Yang</keyname><forenames>Ming-Hsuan</forenames></author><author><keyname>Zhang</keyname><forenames>David</forenames></author></authors><title>Fast Tracking via Spatio-Temporal Context Learning</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we present a simple yet fast and robust algorithm which
exploits the spatio-temporal context for visual tracking. Our approach
formulates the spatio-temporal relationships between the object of interest and
its local context based on a Bayesian framework, which models the statistical
correlation between the low-level features (i.e., image intensity and position)
from the target and its surrounding regions. The tracking problem is posed by
computing a confidence map, and obtaining the best target location by
maximizing an object location likelihood function. The Fast Fourier Transform
is adopted for fast learning and detection in this work. Implemented in MATLAB
without code optimization, the proposed tracker runs at 350 frames per second
on an i7 machine. Extensive experimental results show that the proposed
algorithm performs favorably against state-of-the-art methods in terms of
efficiency, accuracy and robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1940</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1940</id><created>2013-11-08</created><updated>2014-05-21</updated><authors><author><keyname>Nielsen</keyname><forenames>Johan S. R.</forenames></author></authors><title>Power Decoding of Reed-Solomon Codes Revisited</title><categories>cs.IT math.IT</categories><comments>This is a major revision of the previous version: it contains a new
  bound on the failure probability, while some previous parts have been
  considerably shortened. Submitted to ICMCTA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power decoding, or &quot;decoding by virtual interleaving&quot;, of Reed--Solomon codes
is a method for unique decoding beyond half the minimum distance. We give a new
variant of the Power decoding scheme, building upon the key equation of Gao. We
show various interesting properties such as behavioural equivalence to the
classical scheme using syndromes, as well as a new bound on the failure
probability when the powering degree is 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1958</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1958</id><created>2013-11-07</created><updated>2014-05-20</updated><authors><author><keyname>Batyrshin</keyname><forenames>Ildar</forenames></author></authors><title>Constructing Time Series Shape Association Measures: Minkowski Distance
  and Data Standardization</title><categories>cs.LG</categories><comments>Presented at BRICS CCI 2013, Porto de Galinhas, Brasil, 8-11
  September 2013. Reference on Proceedings of BRICS CCI 2013 is added</comments><journal-ref>Published in Proceedings of BRICS CCI 2013, Porto de Galinhas,
  Brasil, 8-11 September 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is surprising that last two decades many works in time series data mining
and clustering were concerned with measures of similarity of time series but
not with measures of association that can be used for measuring possible direct
and inverse relationships between time series. Inverse relationships can exist
between dynamics of prices and sell volumes, between growth patterns of
competitive companies, between well production data in oilfields, between wind
velocity and air pollution concentration etc. The paper develops a theoretical
basis for analysis and construction of time series shape association measures.
Starting from the axioms of time series shape association measures it studies
the methods of construction of measures satisfying these axioms. Several
general methods of construction of such measures suitable for measuring time
series shape similarity and shape association are proposed. Time series shape
association measures based on Minkowski distance and data standardization
methods are considered. The cosine similarity and the Pearsons correlation
coefficient are obtained as particular cases of the proposed general methods
that can be used also for construction of new association measures in data
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1976</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1976</id><created>2013-11-08</created><authors><author><keyname>Cheong</keyname><forenames>Otfried</forenames></author><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Kim</keyname><forenames>Heuna</forenames></author><author><keyname>Kim</keyname><forenames>Hyo-Sil</forenames></author></authors><title>On the Number of Edges of Fan-Crossing Free Graphs</title><categories>cs.CG cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph drawn in the plane with n vertices is k-fan-crossing free for k &gt; 1
if there are no k+1 edges $g,e_1,...e_k$, such that $e_1,e_2,...e_k$ have a
common endpoint and $g$ crosses all $e_i$. We prove a tight bound of 4n-8 on
the maximum number of edges of a 2-fan-crossing free graph, and a tight 4n-9
bound for a straight-edge drawing. For k &gt; 2, we prove an upper bound of
3(k-1)(n-2) edges. We also discuss generalizations to monotone graph
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.1994</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.1994</id><created>2013-11-08</created><authors><author><keyname>Zhang</keyname><forenames>Xin</forenames></author></authors><title>Drawing complete multipartite graphs on the plane with restrictions on
  crossings</title><categories>math.CO cs.DM</categories><msc-class>05C62, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the concept of NIC-planar graphs and present the full
characterization of NIC-planar complete k-partite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2003</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2003</id><created>2013-11-08</created><updated>2015-05-09</updated><authors><author><keyname>Andriyanova</keyname><forenames>Iryna</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author></authors><title>Threshold Saturation for Nonbinary SC-LDPC Codes on the Binary Erasure
  Channel</title><categories>cs.IT math.IT</categories><comments>Revised version of the paper submitted to IT Transactions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the asymptotic performance of nonbinary spatially-coupled
low-density parity-check (SC-LDPC) code ensembles defined over the general
linear group on the binary erasure channel. In particular, we prove threshold
saturation of the belief propagation decoding to the so called potential
threshold, using the proof technique based on potential functions recently
introduced by Yedla \textit{et al.}. We rewrite the density evolution of
nonbinary SC-LDPC codes in an equivalent vector recursion form which is suited
for the use of the potential function. We then discuss the existence of the
potential function for the general case of vector recursions defined by
multivariate polynomials, and give a method to construct the potential
function. We define a potential function in a slightly more general form than
one by Yedla et al., in order to make the technique based on potential
functions applicable to the case of nonbinary LDPC codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2008</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2008</id><created>2013-11-08</created><updated>2014-01-30</updated><authors><author><keyname>Medo</keyname><forenames>Matus</forenames></author></authors><title>Statistical validation of high-dimensional models of growing networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>8 pages, 5 figures, 2 tables</comments><journal-ref>Phys. Rev. E 89, 032801, 2014</journal-ref><doi>10.1103/PhysRevE.89.032801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The abundance of models of complex networks and the current insufficient
validation standards make it difficult to judge which models are strongly
supported by data and which are not. We focus here on likelihood maximization
methods for models of growing networks with many parameters and compare their
performance on artificial and real datasets. While high dimensionality of the
parameter space harms the performance of direct likelihood maximization on
artificial data, this can be improved by introducing a suitable penalization
term. Likelihood maximization on real data shows that the presented approach is
able to discriminate among available network models. To make large-scale
datasets accessible to this kind of analysis, we propose a subset sampling
technique and show that it yields substantial model evidence in a fraction of
time necessary for the analysis of the complete data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2012</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2012</id><created>2013-11-08</created><updated>2014-04-16</updated><authors><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Koch</keyname><forenames>Tobias</forenames></author><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author></authors><title>Quasi-Static Multiple-Antenna Fading Channels at Finite Blocklength</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE Transactions on Information Theory. The
  conditions under which Theorem 9 holds are now milder</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the maximal achievable rate for a given blocklength
and error probability over quasi-static multiple-input multiple-output (MIMO)
fading channels, with and without channel state information (CSI) at the
transmitter and/or the receiver. The principal finding is that outage capacity,
despite being an asymptotic quantity, is a sharp proxy for the
finite-blocklength fundamental limits of slow-fading channels. Specifically,
the channel dispersion is shown to be zero regardless of whether the fading
realizations are available at both transmitter and receiver, at only one of
them, or at neither of them. These results follow from analytically tractable
converse and achievability bounds. Numerical evaluation of these bounds
verifies that zero dispersion may indeed imply fast convergence to the outage
capacity as the blocklength increases. In the example of a particular $1 \times
2$ single-input multiple-output (SIMO) Rician fading channel, the blocklength
required to achieve $90\%$ of capacity is about an order of magnitude smaller
compared to the blocklength required for an AWGN channel with the same
capacity. For this specific scenario, the coding/decoding schemes adopted in
the LTE-Advanced standard are benchmarked against the finite-blocklength
achievability and converse bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2013</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2013</id><created>2013-11-08</created><authors><author><keyname>Kandiah</keyname><forenames>Vivek</forenames></author><author><keyname>Shepelyansky</keyname><forenames>Dima L.</forenames></author></authors><title>Google matrix analysis of C.elegans neural network</title><categories>physics.soc-ph cs.SI q-bio.NC</categories><comments>Additional information on the webpage
  http://www.quantware.ups-tlse.fr/QWLIB/wormgooglematrix/index.html</comments><report-no>PLA22574</report-no><doi>10.1016/j.physleta.2014.04.045</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the structural properties of the neural network of the C.elegans
(worm) from a directed graph point of view. The Google matrix analysis is used
to characterize the neuron connectivity structure and node classifications are
discussed and compared with physiological properties of the cells. Our results
are obtained by a proper definition of neural directed network and subsequent
eigenvector analysis which recovers some results of previous studies. Our
analysis highlights particular sets of important neurons constituting the core
of the neural system. The applications of PageRank, CheiRank and ImpactRank to
characterization of interdependency of neurons are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2014</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2014</id><created>2013-11-08</created><authors><author><keyname>Rodr&#xed;guez</keyname><forenames>Roberto</forenames></author><author><keyname>Torres</keyname><forenames>Esley</forenames></author><author><keyname>Garc&#xe9;s</keyname><forenames>Yasel</forenames></author><author><keyname>Pereira</keyname><forenames>Osvaldo</forenames></author><author><keyname>Sossa</keyname><forenames>Humberto</forenames></author></authors><title>A new stopping criterion for the mean shift iterative algorithm</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mean shift iterative algorithm was proposed in 2006, for using the
entropy as a stopping criterion. From then on, a theoretical base has been
developed and a group of applications has been carried out using this
algorithm. This paper proposes a new stopping criterion for the mean shift
iterative algorithm, where stopping threshold via entropy is used now, but in
another way. Many segmentation experiments were carried out by utilizing
standard images and it was verified that a better segmentation was reached, and
that the algorithm had better stability. An analysis on the convergence,
through a theorem, with the new stopping criterion was carried out. The goal of
this paper is to compare the new stopping criterion with the old criterion. For
this reason, the obtained results were not compared with other segmentation
approaches, since with the old stopping criterion were previously carried out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2019</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2019</id><created>2013-11-08</created><authors><author><keyname>Camarero</keyname><forenames>Crist&#xf3;bal</forenames></author><author><keyname>Mart&#xed;nez</keyname><forenames>Carmen</forenames></author><author><keyname>Beivide</keyname><forenames>Ram&#xf3;n</forenames></author></authors><title>Symmetric Interconnection Networks from Cubic Crystal Lattices</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Torus networks of moderate degree have been widely used in the supercomputer
industry. Tori are superb when used for executing applications that require
near-neighbor communications. Nevertheless, they are not so good when dealing
with global communications. Hence, typical 3D implementations have evolved to
5D networks, among other reasons, to reduce network distances. Most of these
big systems are mixed-radix tori which are not the best option for minimizing
distances and efficiently using network resources. This paper is focused on
improving the topological properties of these networks.
  By using integral matrices to deal with Cayley graphs over Abelian groups, we
have been able to propose and analyze a family of high-dimensional grid-based
interconnection networks. As they are built over $n$-dimensional grids that
induce a regular tiling of the space, these topologies have been denoted
\textsl{lattice graphs}. We will focus on cubic crystal lattices for modeling
symmetric 3D networks. Other higher dimensional networks can be composed over
these graphs, as illustrated in this research. Easy network partitioning can
also take advantage of this network composition operation. Minimal routing
algorithms are also provided for these new topologies. Finally, some practical
issues such as implementability and preliminary performance evaluations have
been addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2022</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2022</id><created>2013-11-08</created><authors><author><keyname>Gadouleau</keyname><forenames>Maximilien</forenames></author><author><keyname>Georgiou</keyname><forenames>Nicholas</forenames></author></authors><title>New constructions and bounds for Winkler's hat game</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hat problems have recently become a popular topic in combinatorics and
discrete mathematics. These have been shown to be strongly related to coding
theory, network coding, and auctions. We consider the following version of the
hat game, introduced by Winkler and studied by Butler et al. A team is composed
of several players; each player is assigned a hat of a given colour; they do
not see their own colour, but can see some other hats, according to a directed
graph. The team wins if they have a strategy such that, for any possible
assignment of colours to their hats, at least one player guesses their own hat
colour correctly. In this paper, we discover some new classes of graphs which
allow a winning strategy, thus answering some of the open questions in Butler
et al. We also derive upper bounds on the maximal number of possible hat
colours that allow for a winning strategy for a given graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2023</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2023</id><created>2013-10-25</created><authors><author><keyname>Avrachenkov</keyname><forenames>Konstantin</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>De Turck</keyname><forenames>Koen</forenames><affiliation>LAAS</affiliation></author><author><keyname>Fiems</keyname><forenames>Dieter</forenames><affiliation>LAAS</affiliation></author><author><keyname>Prabhu</keyname><forenames>Balakrishna</forenames><affiliation>LAAS</affiliation></author></authors><title>Information dissemination processes in directed social networks</title><categories>cs.SI physics.soc-ph</categories><proxy>ccsd</proxy><journal-ref>International Workshop on Modeling, Analysis and Management of
  Social Networks and their Applications (SOCNET). MMB \&amp; DFT 2014, Bamberg :
  Allemagne (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks can have asymmetric relationships. In the online social
network Twitter, a follower receives tweets from a followed person but the
followed person is not obliged to subscribe to the channel of the follower.
Thus, it is natural to consider the dissemination of information in directed
networks. In this work we use the mean-field approach to derive differential
equations that describe the dissemination of information in a social network
with asymmetric relationships. In particular, our model reflects the impact of
the degree distribution on the information propagation process. We further show
that for an important subclass of our model, the differential equations can be
solved analytically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2032</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2032</id><created>2013-11-08</created><updated>2014-01-04</updated><authors><author><keyname>Rahmati</keyname><forenames>Zahed</forenames></author><author><keyname>Abam</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>King</keyname><forenames>Valerie</forenames></author><author><keyname>Whitesides</keyname><forenames>Sue</forenames></author><author><keyname>Zarei</keyname><forenames>Alireza</forenames></author></authors><title>A Simple, Faster Method for Kinetic Proximity Problems</title><categories>cs.CG</categories><comments>Preliminary versions of parts of this paper appeared in Proceedings
  of the 29th ACM Symposium on Computational Geometry (SoCG 2013) and
  Proceedings of the 13th Scandinavian Symposium and Workshops on Algorithm
  Theory (SWAT 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a set of $n$ points in the plane, this paper presents simple kinetic data
structures (KDS's) for solutions to some fundamental proximity problems,
namely, the all nearest neighbors problem, the closest pair problem, and the
Euclidean minimum spanning tree (EMST) problem. Also, the paper introduces
KDS's for maintenance of two well-studied sparse proximity graphs, the Yao
graph and the Semi-Yao graph.
  We use sparse graph representations, the Pie Delaunay graph and the
Equilateral Delaunay graph, to provide new solutions for the proximity
problems. Then we design KDS's that efficiently maintain these sparse graphs on
a set of $n$ moving points, where the trajectory of each point is assumed to be
an algebraic function of constant maximum degree $s$. We use the kinetic Pie
Delaunay graph and the kinetic Equilateral Delaunay graph to create KDS's for
maintenance of the Yao graph, the Semi-Yao graph, all the nearest neighbors,
the closest pair, and the EMST. Our KDS's use $O(n)$ space and $O(n\log n)$
preprocessing time.
  We provide the first KDS's for maintenance of the Semi-Yao graph and the Yao
graph. Our KDS processes $O(n^2\beta_{2s+2}(n))$ (resp.
$O(n^3\beta_{2s+2}^2(n)\log n)$) events to maintain the Semi-Yao graph (resp.
the Yao graph); each event can be processed in time $O(\log n)$ in an amortized
sense. Here, $\beta_s(n)$ is an extremely slow-growing function.
  Our KDS for maintenance of all the nearest neighbors and the closest pair
processes $O(n^2\beta^2_{2s+2}(n)\log n)$ events. For maintenance of the EMST,
our KDS processes $O(n^3\beta_{2s+2}^2(n)\log n)$ events. For all three of
these problems, each event can be handled in time $O(\log n)$ in an amortized
sense.
  We improve the previous randomized kinetic algorithm for maintenance of all
the nearest neighbors by Agarwal, Kaplan, and Sharir, and the previous EMST KDS
by Rahmati and Zarei.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2037</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2037</id><created>2013-11-08</created><updated>2015-04-22</updated><authors><author><keyname>Mitzenmacher</keyname><forenames>Michael</forenames></author><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author></authors><title>Simple Multi-Party Set Reconciliation</title><categories>cs.DS</categories><comments>22 pages, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As users migrate information to cloud storage, many distributed cloud-based
services use multiple loosely consistent replicas of user information to avoid
the high overhead of more tightly coupled synchronization. Periodically, the
information must be synchronized, or reconciled. One can place this problem in
the theoretical framework of {\em set reconciliation}: two parties $A_1$ and
$A_2$ each hold a set of keys, named $S_1$ and $S_2$ respectively, and the goal
is for both parties to obtain $S_1 \cup S_2$. Typically, set reconciliation is
interesting algorithmically when sets are large but the set difference
$|S_1-S_2|+|S_2-S_1|$ is small. In this setting the focus is on accomplishing
reconciliation efficiently in terms of communication; ideally, the
communication should depend on the size of the set difference, and not on the
size of the sets.
  In this paper, we extend recent approaches using Invertible Bloom Lookup
Tables (IBLTs) for set reconciliation to the multi-party setting. In this
setting there are three or more parties $A_1,A_2,\ldots,A_n$ holding sets of
keys $S_1,S_2,\ldots,S_n$ respectively, and the goal is for all parties to
obtain $\cup_i S_i$. This could of course be done by pairwise reconciliations,
but we seek more effective methods.
  Our methodology uses network coding techniques in conjunction with IBLTs,
allowing efficiency in network utilization along with efficiency obtained by
passing messages of size $O(|\cup_i S_i - \cap_i S_i|)$. Further, our approach
can function even if the number of parties is not exactly known in advance, and
in many cases can be used to determine which parties contain keys not in the
joint union. By connecting reconciliation with network coding, we can allow for
substantially more efficient reconciliation methods that apply to a number of
natural distributed computing problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2056</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2056</id><created>2013-11-07</created><authors><author><keyname>Dmitriev</keyname><forenames>V. M.</forenames></author><author><keyname>Gandzha</keyname><forenames>T. V.</forenames></author><author><keyname>Gandzha</keyname><forenames>V. V.</forenames></author><author><keyname>Panov</keyname><forenames>S. A.</forenames></author></authors><title>The structure and functions of an automated project management system
  for the centers of scientific and technical creativity of students</title><categories>cs.OH</categories><comments>in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article discusses the possibility of automating of the student's
projecting through the use of automated project management system. There are
described the purpose, structure and formalism of automated workplace of
student-designer (AWSD), and shown its structural-functional diagram.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2064</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2064</id><created>2013-11-08</created><authors><author><keyname>Wang</keyname><forenames>Timothy</forenames></author><author><keyname>Ashari</keyname><forenames>Alireza Esna</forenames></author><author><keyname>Jobredeaux</keyname><forenames>Romain</forenames></author><author><keyname>Feron</keyname><forenames>Eric M.</forenames></author></authors><title>Credible Autocoding of Fault Detection Observers</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a domain specific process to assist the
verification of observer-based fault detection software. Observer-based fault
detection systems, like control systems, yield invariant properties of
quadratic types. These quadratic invariants express both safety properties of
the software such as the boundedness of the states and correctness properties
such as the absence of false alarms from the fault detector. We seek to
leverage these quadratic invariants, in an automated fashion, for the formal
verification of the fault detection software. The approach, referred to as the
credible autocoding framework [1], can be characterized as autocoding with
proofs. The process starts with the fault detector model, along with its safety
and correctness properties, all expressed formally in a synchronous modeling
environment such as Simulink. The model is then transformed by a prototype
credible autocoder into both code and analyzable annotations for the code. We
demonstrate the credible autocoding process on a running example of an output
observer fault detector for a 3-degrees-of-freedom (3DOF) helicopter control
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2079</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2079</id><created>2013-11-08</created><authors><author><keyname>Kim</keyname><forenames>Myunghwan</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Nonparametric Multi-group Membership Model for Dynamic Networks</title><categories>cs.SI physics.soc-ph stat.ML</categories><comments>In Advances in Neural Information Processing Systems 25 (2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relational data-like graphs, networks, and matrices-is often dynamic, where
the relational structure evolves over time. A fundamental problem in the
analysis of time-varying network data is to extract a summary of the common
structure and the dynamics of the underlying relations between the entities.
Here we build on the intuition that changes in the network structure are driven
by the dynamics at the level of groups of nodes. We propose a nonparametric
multi-group membership model for dynamic networks. Our model contains three
main components: We model the birth and death of individual groups with respect
to the dynamics of the network structure via a distance dependent Indian Buffet
Process. We capture the evolution of individual node group memberships via a
Factorial Hidden Markov model. And, we explain the dynamics of the network
structure by explicitly modeling the connectivity structure of groups. We
demonstrate our model's capability of identifying the dynamics of latent groups
in a number of different types of network data. Experimental results show that
our model provides improved predictive performance over existing dynamic
network models on future network forecasting and missing link prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2092</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2092</id><created>2013-11-08</created><updated>2014-05-07</updated><authors><author><keyname>Bauwens</keyname><forenames>Bruno</forenames></author></authors><title>Relating and contrasting plain and prefix Kolmogorov complexity</title><categories>cs.CC cs.IT math.IT</categories><comments>20 pages, 1 figure</comments><msc-class>03D32</msc-class><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [3] a short proof is given that some strings have maximal plain Kolmogorov
complexity but not maximal prefix-free complexity. The proof uses Levin's
symmetry of information, Levin's formula relating plain and prefix complexity
and Gacs' theorem that complexity of complexity given the string can be high.
We argue that the proof technique and results mentioned above are useful to
simplify existing proofs and to solve open questions.
  We present a short proof of Solovay's result [21] relating plain and prefix
complexity: $K (x) = C (x) + CC (x) + O(CCC (x))$ and $C (x) = K (x) - KK (x) +
O(KKK (x))$, (here $CC(x)$ denotes $C(C(x))$, etc.).
  We show that there exist $\omega$ such that $\liminf C(\omega_1\dots
\omega_n) - C(n)$ is infinite and $\liminf K(\omega_1\dots \omega_n) - K(n)$ is
finite, i.e. the infinitely often C-trivial reals are not the same as the
infinitely often K-trivial reals (i.e. [1,Question 1]).
  Solovay showed that for infinitely many $x$ we have $|x| - C (x) \le O(1)$
and $|x| + K (|x|) - K (x) \ge \log^{(2)} |x| - O(\log^{(3)} |x|)$, (here $|x|$
denotes the length of $x$ and $\log^{(2)} = \log\log$, etc.). We show that this
result holds for prefixes of some 2-random sequences.
  Finally, we generalize our proof technique and show that no monotone relation
exists between expectation and probability bounded randomness deficiency (i.e.
[6, Question 1]).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2095</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2095</id><created>2013-11-08</created><authors><author><keyname>Martinec</keyname><forenames>Dan</forenames></author><author><keyname>Herman</keyname><forenames>Ivo</forenames></author><author><keyname>Hur&#xe1;k</keyname><forenames>Zden&#x11b;k</forenames></author><author><keyname>&#x160;ebek</keyname><forenames>Michael</forenames></author></authors><title>Wave-absorbing vehicular platoon controller</title><categories>cs.SY</categories><journal-ref>Eur. J. Control 20 (2014) 237-248</journal-ref><doi>10.1016/j.ejcon.2014.06.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper tailors the so-called wave-based control popular in the field of
flexible mechanical structures to the field of distributed control of vehicular
platoons. The proposed solution augments the symmetric bidirectional control
algorithm with a wave-absorbing controller implemented on the leader, and/or on
the rear-end vehicle. The wave-absorbing controller actively absorbs an
incoming wave of positional changes in the platoon and thus prevents
oscillations of inter-vehicle distances. The proposed controller significantly
improves the performance of platoon manoeuvrers such as
acceleration/deceleration or changing the distances between vehicles without
making the platoon string unstable. Numerical simulations show that the
wave-absorbing controller performs efficiently even for platoons with a large
number of vehicles, for which other platooning algorithms are inefficient or
require wireless communication between vehicles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2097</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2097</id><created>2013-11-08</created><updated>2014-01-23</updated><authors><author><keyname>Shen</keyname><forenames>Yun</forenames></author><author><keyname>Tobia</keyname><forenames>Michael J.</forenames></author><author><keyname>Sommer</keyname><forenames>Tobias</forenames></author><author><keyname>Obermayer</keyname><forenames>Klaus</forenames></author></authors><title>Risk-sensitive Reinforcement Learning</title><categories>cs.LG</categories><comments>27 pages, 7 figures</comments><journal-ref>Neural Computation, Vol. 26, Nr. 7, pp. 1298--1328, 2014</journal-ref><doi>10.1162/NECO_a_00600</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a family of risk-sensitive reinforcement learning methods for
agents, who face sequential decision-making tasks in uncertain environments. By
applying a utility function to the temporal difference (TD) error, nonlinear
transformations are effectively applied not only to the received rewards but
also to the true transition probabilities of the underlying Markov decision
process. When appropriate utility functions are chosen, the agents' behaviors
express key features of human behavior as predicted by prospect theory
(Kahneman and Tversky, 1979), for example different risk-preferences for gains
and losses as well as the shape of subjective probability curves. We derive a
risk-sensitive Q-learning algorithm, which is necessary for modeling human
behavior when transition probabilities are unknown, and prove its convergence.
As a proof of principle for the applicability of the new framework we apply it
to quantify human behavior in a sequential investment task. We find, that the
risk-sensitive variant provides a significantly better fit to the behavioral
data and that it leads to an interpretation of the subject's responses which is
indeed consistent with prospect theory. The analysis of simultaneously measured
fMRI signals show a significant correlation of the risk-sensitive TD error with
BOLD signal change in the ventral striatum. In addition we find a significant
correlation of the risk-sensitive Q-values with neural activity in the
striatum, cingulate cortex and insula, which is not present if standard
Q-values are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2100</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2100</id><created>2013-11-08</created><authors><author><keyname>Jayaram</keyname><forenames>Nandish</forenames></author><author><keyname>Khan</keyname><forenames>Arijit</forenames></author><author><keyname>Li</keyname><forenames>Chengkai</forenames></author><author><keyname>Yan</keyname><forenames>Xifeng</forenames></author><author><keyname>Elmasri</keyname><forenames>Ramez</forenames></author></authors><title>Querying Knowledge Graphs by Example Entity Tuples</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We witness an unprecedented proliferation of knowledge graphs that record
millions of entities and their relationships. While knowledge graphs are
structure-flexible and content rich, they are difficult to use. The challenge
lies in the gap between their overwhelming complexity and the limited database
knowledge of non-professional users. If writing structured queries over simple
tables is difficult, complex graphs are only harder to query. As an initial
step toward improving the usability of knowledge graphs, we propose to query
such data by example entity tuples, without requiring users to form complex
graph queries. Our system, GQBE (Graph Query By Example), automatically derives
a weighted hidden maximal query graph based on input query tuples, to capture a
user's query intent. It efficiently finds and ranks the top approximate answer
tuples. For fast query processing, GQBE only partially evaluates query graphs.
We conducted experiments and user studies on the large Freebase and DBpedia
datasets and observed appealing accuracy and efficiency. Our system provides a
complementary approach to the existing keyword-based methods, facilitating
user-friendly graph querying. To the best of our knowledge, there was no such
proposal in the past in the context of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2102</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2102</id><created>2013-11-08</created><authors><author><keyname>Gorelick</keyname><forenames>Lena</forenames></author><author><keyname>BenAyed</keyname><forenames>Ismail</forenames></author><author><keyname>Schmidt</keyname><forenames>Frank R.</forenames></author><author><keyname>Boykov</keyname><forenames>Yuri</forenames></author></authors><title>An Experimental Comparison of Trust Region and Level Sets</title><categories>cs.CV</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-order (non-linear) functionals have become very popular in segmentation,
stereo and other computer vision problems. Level sets is a well established
general gradient descent framework, which is directly applicable to
optimization of such functionals and widely used in practice. Recently, another
general optimization approach based on trust region methodology was proposed
for regional non-linear functionals. Our goal is a comprehensive experimental
comparison of these two frameworks in regard to practical efficiency,
robustness to parameters, and optimality. We experiment on a wide range of
problems with non-linear constraints on segment volume, appearance and shape.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2103</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2103</id><created>2013-11-08</created><authors><author><keyname>Pandey</keyname><forenames>Animesh</forenames></author></authors><title>Idea of a new Personality-Type based Recommendation Engine</title><categories>cs.IR</categories><comments>7 Pages, 13 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Myers-Briggs Type Indicator (MBTI) types depict the psychological preferences
by which a person perceives the world and make decisions. There are 4 principal
functions through which the people see the world: sensation, intuition,
feeling, and thinking. These functions along with the Introverted\Extroverted
nature of the person, there are 16 personalities types, the humans are divided
into. Here an idea is presented where a user can get recommendations for books,
web media content, music and movies on the basis of the users' MBTI type. Only
things like books and other media content has been chosen because the
preferences in such things are mostly subjective. Apart from the recommended
content that is generally generated on the basis of the previous purchases,
searches can be enhanced by using the MBTI. A minimalist survey was designed
for collecting the data. This has a more than 100 features that show the
preference of a personality type. Those include preferences in book genres,
music genres, movie genres and even video games genres. After analyzing the
data that is collected from the survey, some inferences were drawn from it
which can be used to design a new recommendation engine for recommending the
content that coincides with the personality of the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2106</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2106</id><created>2013-11-08</created><authors><author><keyname>Iyer</keyname><forenames>Rishabh</forenames></author><author><keyname>Bilmes</keyname><forenames>Jeff</forenames></author></authors><title>Submodular Optimization with Submodular Cover and Submodular Knapsack
  Constraints</title><categories>cs.DS cs.AI cs.DM</categories><comments>23 pages. A short version of this appeared in Advances of NIPS-2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate two new optimization problems -- minimizing a submodular
function subject to a submodular lower bound constraint (submodular cover) and
maximizing a submodular function subject to a submodular upper bound constraint
(submodular knapsack). We are motivated by a number of real-world applications
in machine learning including sensor placement and data subset selection, which
require maximizing a certain submodular function (like coverage or diversity)
while simultaneously minimizing another (like cooperative cost). These problems
are often posed as minimizing the difference between submodular functions [14,
35] which is in the worst case inapproximable. We show, however, that by
phrasing these problems as constrained optimization, which is more natural for
many applications, we achieve a number of bounded approximation guarantees. We
also show that both these problems are closely related and an approximation
algorithm solving one can be used to obtain an approximation guarantee for the
other. We provide hardness results for both problems thus showing that our
approximation factors are tight up to log-factors. Finally, we empirically
demonstrate the performance and good scalability properties of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2109</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2109</id><created>2013-11-08</created><updated>2014-09-16</updated><authors><author><keyname>Bavly</keyname><forenames>Gilad</forenames></author><author><keyname>Peretz</keyname><forenames>Ron</forenames></author></authors><title>How to Gamble Against All Odds</title><categories>cs.GT math.LO</categories><msc-class>91A20 (Primary) 68Q30 (Secondary)</msc-class><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A decision maker observes the evolving state of the world while constantly
trying to predict the next state given the history of past states. The ability
to benefit from such predictions depends not only on the ability to recognize
patters in history, but also on the range of actions available to the decision
maker.
  We assume there are two possible states of the world. The decision maker is a
gambler who has to bet a certain amount of money on the bits of an announced
binary sequence of states. If he makes a correct prediction he wins his wager,
otherwise he loses it.
  We compare the power of betting strategies (aka martingales) whose wagers
take values in different sets of reals. A martingale whose wagers take values
in a set $A$ is called an $A$-martingale. A set of reals $B$ anticipates a set
$A$, if for every $A$-martingale there is a countable set of $B$-martingales,
such that on every binary sequence on which the $A$-martingale gains an
infinite amount at least one of the $B$-martingales gains an infinite amount,
too.
  We show that for two important classes of pairs of sets $A$ and $B$, $B$
anticipates $A$ if and only if the closure of $B$ contains $rA$, for some
positive $r$. One class is when $A$ is bounded and $B$ is bounded away from
zero; the other class is when $B$ is well ordered (has no left-accumulation
points). Our results generalize several recent results in algorithmic
randomness and answer a question posed by Chalcraft et al. (2012).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2110</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2110</id><created>2013-11-08</created><authors><author><keyname>Iyer</keyname><forenames>Rishabh</forenames></author><author><keyname>Jegelka</keyname><forenames>Stefanie</forenames></author><author><keyname>Bilmes</keyname><forenames>Jeff</forenames></author></authors><title>Curvature and Optimal Algorithms for Learning and Minimizing Submodular
  Functions</title><categories>cs.DS cs.DM cs.LG</categories><comments>21 pages. A shorter version appeared in Advances of NIPS-2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate three related and important problems connected to machine
learning: approximating a submodular function everywhere, learning a submodular
function (in a PAC-like setting [53]), and constrained minimization of
submodular functions. We show that the complexity of all three problems depends
on the 'curvature' of the submodular function, and provide lower and upper
bounds that refine and improve previous results [3, 16, 18, 52]. Our proof
techniques are fairly generic. We either use a black-box transformation of the
function (for approximation and learning), or a transformation of algorithms to
use an appropriate surrogate function (for minimization). Curiously, curvature
has been known to influence approximations for submodular maximization [7, 55],
but its effect on minimization, approximation and learning has hitherto been
open. We complete this picture, and also support our theoretical claims by
empirical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2115</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2115</id><created>2013-11-08</created><updated>2014-11-29</updated><authors><author><keyname>Sohl-Dickstein</keyname><forenames>Jascha</forenames></author><author><keyname>Poole</keyname><forenames>Ben</forenames></author><author><keyname>Ganguli</keyname><forenames>Surya</forenames></author></authors><title>Fast large-scale optimization by unifying stochastic gradient and
  quasi-Newton methods</title><categories>cs.LG</categories><msc-class>90C26</msc-class><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for minimizing a sum of functions that combines the
computational efficiency of stochastic gradient descent (SGD) with the second
order curvature information leveraged by quasi-Newton methods. We unify these
disparate approaches by maintaining an independent Hessian approximation for
each contributing function in the sum. We maintain computational tractability
and limit memory requirements even for high dimensional optimization problems
by storing and manipulating these quadratic approximations in a shared, time
evolving, low dimensional subspace. Each update step requires only a single
contributing function or minibatch evaluation (as in SGD), and each step is
scaled using an approximate inverse Hessian and little to no adjustment of
hyperparameters is required (as is typical for quasi-Newton methods). This
algorithm contrasts with earlier stochastic second order techniques that treat
the Hessian of each contributing function as a noisy approximation to the full
Hessian, rather than as a target for direct estimation. We experimentally
demonstrate improved convergence on seven diverse optimization problems. The
algorithm is released as open source Python and MATLAB packages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2121</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2121</id><created>2013-11-08</created><authors><author><keyname>Ahmad</keyname><forenames>Syed Amaar</forenames></author></authors><title>Asynchronous Systems and Binary Diagonal Random Matrices: A Proof and
  Convergence Rate</title><categories>math.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a synchronized network of $n$ nodes, each node will update its parameter
based on the system state in a given iteration. It is well-known that the
updates can converge to a fixed point if the maximum absolute eigenvalue
(spectral radius) of the $n \times n$ iterative matrix ${\bf{F}}$ is less than
one (i.e. $\rho({\bf{F}})&lt;1$). However, if only a subset of the nodes update
their parameter in an iteration (due to delays or stale feedback) then this
effectively renders the spectral radius of the iterative matrix as one. We
consider matrices of unit spectral radii generated from ${\bf{F}}$ due to
random delays in the updates. We show that if each node updates at least once
in every $T$ iterations, then the product of the random matrices (joint
spectral radius) corresponding to these iterations is less than one. We then
use this property to prove convergence of asynchronous iterative systems.
Finally, we show that the convergence rate of such a system is
$\rho({\bf{F}})\frac{(1-(1-\gamma)^T)^n}{T}$, where assuming ergodicity,
$\gamma$ is the lowest bound on the probability that a node will update in any
given iteration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2123</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2123</id><created>2013-11-08</created><authors><author><keyname>Mahdaviani</keyname><forenames>Kaveh</forenames></author><author><keyname>Yazdani</keyname><forenames>Raman</forenames></author><author><keyname>Ardakani</keyname><forenames>Masoud</forenames></author></authors><title>Linear-Complexity Overhead-Optimized Random Linear Network Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse random linear network coding (SRLNC) is an attractive technique
proposed in the literature to reduce the decoding complexity of random linear
network coding. Recognizing the fact that the existing SRLNC schemes are not
efficient in terms of the required reception overhead, we consider the problem
of designing overhead-optimized SRLNC schemes. To this end, we introduce a new
design of SRLNC scheme that enjoys very small reception overhead while
maintaining the main benefit of SRLNC, i.e., its linear encoding/decoding
complexity. We also provide a mathematical framework for the asymptotic
analysis and design of this class of codes based on density evolution (DE)
equations. To the best of our knowledge, this work introduces the first DE
analysis in the context of network coding. Our analysis method then enables us
to design network codes with reception overheads in the order of a few percent.
We also investigate the finite-length performance of the proposed codes and
through numerical examples we show that our proposed codes have significantly
lower reception overheads compared to all existing linear-complexity random
linear network coding schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2128</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2128</id><created>2013-11-08</created><authors><author><keyname>Fujii</keyname><forenames>Keisuke</forenames></author><author><keyname>Morimae</keyname><forenames>Tomoyuki</forenames></author></authors><title>Quantum Commuting Circuits and Complexity of Ising Partition Functions</title><categories>quant-ph cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>33 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Instantaneous quantum polynomial-time (IQP) computation is a class of quantum
computation consisting only of commuting two-qubit gates and is not universal
in the sense of standard quantum computation. Nevertheless, it has been shown
that if there is a classical algorithm that can simulate IQP efficiently, the
polynomial hierarchy (PH) collapses at the third level, which is highly
implausible. However, the origin of the classical intractability is still less
understood. Here we establish a relationship between IQP and computational
complexity of the partition functions of Ising models. We apply the established
relationship in two opposite directions. One direction is to find subclasses of
IQP that are classically efficiently simulatable in the strong sense, by using
exact solvability of certain types of Ising models. Another direction is
applying quantum computational complexity of IQP to investigate (im)possibility
of efficient classical approximations of Ising models with imaginary coupling
constants. Specifically, we show that there is no fully polynomial randomized
approximation scheme (FPRAS) for Ising models with almost all imaginary
coupling constants even on a planar graph of a bounded degree, unless the PH
collapses at the third level. Furthermore, we also show a multiplicative
approximation of such a class of Ising partition functions is at least as hard
as a multiplicative approximation for the output distribution of an arbitrary
quantum circuit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2134</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2134</id><created>2013-11-08</created><authors><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Jayaraman</keyname><forenames>Prem</forenames></author><author><keyname>Zaslavsky</keyname><forenames>Arkady</forenames></author><author><keyname>Christen</keyname><forenames>Peter</forenames></author><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author></authors><title>Context-aware Dynamic Discovery and Configuration of 'Things' in Smart
  Environments</title><categories>cs.NI</categories><comments>Big Data and Internet of Things: A Roadmap for Smart Environments,
  Studies in Computational Intelligence book series, Springer Berlin
  Heidelberg, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of Things (IoT) is a dynamic global information network
consisting of Internet-connected objects, such as RFIDs, sensors, actuators, as
well as other instruments and smart appliances that are becoming an integral
component of the future Internet. Currently, such Internet-connected objects or
`things' outnumber both people and computers connected to the Internet and
their population is expected to grow to 50 billion in the next 5 to 10 years.
To be able to develop IoT applications, such `things' must become dynamically
integrated into emerging information networks supported by architecturally
scalable and economically feasible Internet service delivery models, such as
cloud computing. Achieving such integration through discovery and configuration
of `things' is a challenging task. Towards this end, we propose a Context-Aware
Dynamic Discovery of {Things} (CADDOT) model. We have developed a tool
SmartLink, that is capable of discovering sensors deployed in a particular
location despite their heterogeneity. SmartLink helps to establish the direct
communication between sensor hardware and cloud-based IoT middleware platforms.
We address the challenge of heterogeneity using a plug in architecture. Our
prototype tool is developed on an Android platform. Further, we employ the
Global Sensor Network (GSN) as the IoT middleware for the proof of concept
validation. The significance of the proposed solution is validated using a
test-bed that comprises 52 Arduino-based Libelium sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2137</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2137</id><created>2013-11-09</created><authors><author><keyname>Kidambi</keyname><forenames>Rahul</forenames></author><author><keyname>Nair</keyname><forenames>Vinod</forenames></author><author><keyname>Sellamanickam</keyname><forenames>Sundararajan</forenames></author><author><keyname>Keerthi</keyname><forenames>S. Sathiya</forenames></author></authors><title>A Structured Prediction Approach for Missing Value Imputation</title><categories>cs.LG</categories><comments>9 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Missing value imputation is an important practical problem. There is a large
body of work on it, but there does not exist any work that formulates the
problem in a structured output setting. Also, most applications have
constraints on the imputed data, for example on the distribution associated
with each variable. None of the existing imputation methods use these
constraints. In this paper we propose a structured output approach for missing
value imputation that also incorporates domain constraints. We focus on large
margin models, but it is easy to extend the ideas to probabilistic models. We
deal with the intractable inference step in learning via a piecewise training
technique that is simple, efficient, and effective. Comparison with existing
state-of-the-art and baseline imputation methods shows that our method gives
significantly improved performance on the Hamming loss measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2138</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2138</id><created>2013-11-09</created><authors><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Paparas</keyname><forenames>Dimitris</forenames></author><author><keyname>Sun</keyname><forenames>Xiaorui</forenames></author><author><keyname>Yannakakis</keyname><forenames>Mihalis</forenames></author></authors><title>The Complexity of Optimal Multidimensional Pricing</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We resolve the complexity of revenue-optimal deterministic auctions in the
unit-demand single-buyer Bayesian setting, i.e., the optimal item pricing
problem, when the buyer's values for the items are independent. We show that
the problem of computing a revenue-optimal pricing can be solved in polynomial
time for distributions of support size 2, and its decision version is
NP-complete for distributions of support size 3. We also show that the problem
remains NP-complete for the case of identical distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2139</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2139</id><created>2013-11-09</created><authors><author><keyname>Balamurugan</keyname><forenames>P.</forenames></author><author><keyname>Shevade</keyname><forenames>Shirish</forenames></author><author><keyname>Sellamanickam</keyname><forenames>Sundararajan</forenames></author></authors><title>Large Margin Semi-supervised Structured Output Learning</title><categories>cs.LG</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In structured output learning, obtaining labelled data for real-world
applications is usually costly, while unlabelled examples are available in
abundance. Semi-supervised structured classification has been developed to
handle large amounts of unlabelled structured data. In this work, we consider
semi-supervised structural SVMs with domain constraints. The optimization
problem, which in general is not convex, contains the loss terms associated
with the labelled and unlabelled examples along with the domain constraints. We
propose a simple optimization approach, which alternates between solving a
supervised learning problem and a constraint matching problem. Solving the
constraint matching problem is difficult for structured prediction, and we
propose an efficient and effective hill-climbing method to solve it. The
alternating optimization is carried out within a deterministic annealing
framework, which helps in effective constraint matching, and avoiding local
minima which are not very useful. The algorithm is simple to implement and
achieves comparable generalization performance on benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2146</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2146</id><created>2013-11-09</created><authors><author><keyname>Wang</keyname><forenames>Xiumin</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Xu</keyname><forenames>Yinlong</forenames></author></authors><title>Coding based Data Broadcasting for Time Critical Applications with Rate
  Adaptation</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we dynamically select the transmission rate and design
wireless network coding to improve the quality of services such as delay for
time critical applications. In a network coded system, with low transmission
rate and hence longer transmission range, more packets may be encoded, which
increases the coding opportunity. However, low transmission rate may incur
extra transmission delay, which is intolerable for time critical applications.
We design a novel joint rate selection and wireless network coding (RSNC)
scheme with delay constraint, so as to maximize the total benefit (where we can
define the benefit based on the priority or importance of a packet for example)
of the packets that are successfully received at the destinations without
missing their deadlines. We prove that the proposed problem is NP-hard, and
propose a novel graph model to mathematically formulate the problem. For the
general case, we propose a transmission metric and design an efficient
algorithm to determine the transmission rate and coding strategy for each
transmission. For a special case when all delay constraints are the same, we
study the pairwise coding and present a polynomial time pairwise coding
algorithm that achieves an approximation ratio of 1 - 1/e to the optimal
pairwise coding solution, where e is the base of the natural logarithm.
Finally, simulation results demonstrate the superiority of the proposed RSNC
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2147</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2147</id><created>2013-11-09</created><updated>2013-11-18</updated><authors><author><keyname>Nasre</keyname><forenames>Meghana</forenames></author><author><keyname>Pontecorvi</keyname><forenames>Matteo</forenames></author><author><keyname>Ramachandran</keyname><forenames>Vijaya</forenames></author></authors><title>Betweenness Centrality -- Incremental and Faster</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the incremental computation of the betweenness centrality of all
vertices in a large complex network modeled as a graph G = (V, E), directed or
undirected, with positive real edge-weights. The current widely used algorithm
to compute the betweenness centrality of all vertices in G is the Brandes
algorithm that runs in O(mn + n^2 log n) time, where n = |V| and m = |E|.
  We present an incremental algorithm that updates the betweenness centrality
score of all vertices in G when a new edge is added to G, or the weight of an
existing edge is reduced. Our incremental algorithm runs in O(m' n + n^2) time,
where m' is the size of a certain subset of E*, the set of edges in G that lie
on a shortest path. We achieve the same bound for the more general incremental
update of a vertex v, where the edge update can be performed on any subset of
edges incident to v.
  Our incremental algorithm is the first algorithm that is asymptotically
faster on sparse graphs than recomputing with the Brandes algorithm. Our
algorithm is also likely to be much faster than the Brandes algorithm on dense
graphs since m*, the size of E*, is often close to linear in n.
  Our incremental algorithm is very simple and the only data structures it uses
are arrays, lists, and stack. We give an efficient cache-oblivious
implementation that incurs O(scan(n^2) + n sort(m')) cache misses, where scan
and sort are well-known measures for efficient caching. We also give a static
algorithm for computing betweenness centrality of all vertices that runs in
time O(m* n + n^2 log n), which is faster than the Brandes algorithm on any
graph with n log n = o(m) and m* = o(m).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2150</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2150</id><created>2013-11-09</created><authors><author><keyname>Fang</keyname><forenames>Jun</forenames><affiliation>IEEE</affiliation></author><author><keyname>Shen</keyname><forenames>Yanning</forenames><affiliation>IEEE</affiliation></author><author><keyname>Li</keyname><forenames>Hongbin</forenames><affiliation>IEEE</affiliation></author><author><keyname>Wang</keyname><forenames>Pu</forenames></author></authors><title>Pattern-Coupled Sparse Bayesian Learning for Recovery of Block-Sparse
  Signals</title><categories>cs.IT cs.LG math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering block-sparse signals whose structures
are unknown \emph{a priori}. Block-sparse signals with nonzero coefficients
occurring in clusters arise naturally in many practical scenarios. However, the
knowledge of the block structure is usually unavailable in practice. In this
paper, we develop a new sparse Bayesian learning method for recovery of
block-sparse signals with unknown cluster patterns. Specifically, a
pattern-coupled hierarchical Gaussian prior model is introduced to characterize
the statistical dependencies among coefficients, in which a set of
hyperparameters are employed to control the sparsity of signal coefficients.
Unlike the conventional sparse Bayesian learning framework in which each
individual hyperparameter is associated independently with each coefficient, in
this paper, the prior for each coefficient not only involves its own
hyperparameter, but also the hyperparameters of its immediate neighbors. In
doing this way, the sparsity patterns of neighboring coefficients are related
to each other and the hierarchical model has the potential to encourage
structured-sparse solutions. The hyperparameters, along with the sparse signal,
are learned by maximizing their posterior probability via an
expectation-maximization (EM) algorithm. Numerical results show that the
proposed algorithm presents uniform superiority over other existing methods in
a series of experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2167</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2167</id><created>2013-11-09</created><updated>2014-03-03</updated><authors><author><keyname>Puri</keyname><forenames>Kunal</forenames></author><author><keyname>Ramachandran</keyname><forenames>Prabhu</forenames></author></authors><title>SPH Entropy Errors and the Pressure Blip</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spurious pressure jump at a contact discontinuity, in SPH simulations of
the compressible Euler equations is investigated. From the spatiotemporal
behaviour of the error, the SPH pressure jump is likened to entropy errors
observed for artificial viscosity based finite difference/volume schemes. The
error is observed to be generated at start-up and dissipation is the only
recourse to mitigate it's effect. We show that similar errors are generated for
the Lagrangian plus remap version of the Piecewise Parabolic Method (PPM)
finite volume code (PPMLR). Through a comparison with the direct Eulerian
version of the PPM code (PPMDE), we argue that a lack of diffusion across the
material wave (contact discontinuity) is responsible for the error in PPMLR. We
verify this hypothesis by constructing a more dissipative version of the remap
code using a piecewise constant reconstruction. As an application to SPH, we
propose a hybrid GSPH scheme that adds the requisite dissipation by utilizing a
more dissipative Riemann solver for the energy equation. The proposed
modification to the GSPH scheme, and it's improved treatment of the anomaly is
verified for flows with strong shocks in one and two dimensions. The result
that dissipation must act across the density and energy equations provides a
consistent explanation for many of the hitherto proposed &quot;cures&quot; or &quot;fixes&quot; for
the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2187</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2187</id><created>2013-11-09</created><authors><author><keyname>Aflalo</keyname><forenames>Yonathan</forenames></author><author><keyname>Dubrovina</keyname><forenames>Anastasia</forenames></author><author><keyname>Kimmel</keyname><forenames>Ron</forenames></author></authors><title>Spectral Generalized Multi-Dimensional Scaling</title><categories>cs.CG</categories><doi>10.1073/pnas.1308708110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multidimensional scaling (MDS) is a family of methods that embed a given set
of points into a simple, usually flat, domain. The points are assumed to be
sampled from some metric space, and the mapping attempts to preserve the
distances between each pair of points in the set. Distances in the target space
can be computed analytically in this setting. Generalized MDS is an extension
that allows mapping one metric space into another, that is, multidimensional
scaling into target spaces in which distances are evaluated numerically rather
than analytically. Here, we propose an efficient approach for computing such
mappings between surfaces based on their natural spectral decomposition, where
the surfaces are treated as sampled metric-spaces. The resulting spectral-GMDS
procedure enables efficient embedding by implicitly incorporating smoothness of
the mapping into the problem, thereby substantially reducing the complexity
involved in its solution while practically overcoming its non-convex nature.
The method is compared to existing techniques that compute dense correspondence
between shapes. Numerical experiments of the proposed method demonstrate its
efficiency and accuracy compared to state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2191</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2191</id><created>2013-11-09</created><updated>2014-06-27</updated><authors><author><keyname>Galiano</keyname><forenames>Gonzalo</forenames></author><author><keyname>Velasco</keyname><forenames>Juli&#xe1;n</forenames></author></authors><title>Neighborhood filters and the decreasing rearrangement</title><categories>cs.CV</categories><msc-class>68U10</msc-class><doi>10.1007/s10851-014-0522-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlocal filters are simple and powerful techniques for image denoising. In
this paper, we give new insights into the analysis of one kind of them, the
Neighborhood filter, by using a classical although not very common
transformation: the decreasing rearrangement of a function (the image).
Independently of the dimension of the image, we reformulate the Neighborhood
filter and its iterative variants as an integral operator defined in a
one-dimensional space. The simplicity of this formulation allows to perform a
detailed analysis of its properties. Among others, we prove that the filter
behaves asymptotically as a shock filter combined with a border diffusive term,
responsible for the staircaising effect and the loss of contrast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2208</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2208</id><created>2013-11-09</created><authors><author><keyname>Bloom</keyname><forenames>Kenneth</forenames></author><author><keyname>Gerber</keyname><forenames>Richard</forenames></author></authors><title>Report of the Snowmass 2013 Computing Frontier Working Group on
  Distributed Computing and Facility Infrastructures</title><categories>physics.comp-ph cs.DC hep-ex hep-lat hep-ph</categories><comments>19 pages, 4 figures; ; to be included in proceedings of Community
  Summer Study (&quot;Snowmass&quot;) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the report of the Snowmass 2013 Computing Frontier Working Group on
Distributed Computing and Facility Infrastructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2210</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2210</id><created>2013-11-09</created><updated>2013-11-17</updated><authors><author><keyname>Petrosyan</keyname><forenames>Petros A.</forenames></author></authors><title>On Interval Non-Edge-Colorable Eulerian Multigraphs</title><categories>math.CO cs.DM</categories><comments>4 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  An edge-coloring of a multigraph $G$ with colors $1,\ldots,t$ is called an
interval $t$-coloring if all colors are used, and the colors of edges incident
to any vertex of $G$ are distinct and form an interval of integers. In this
note, we show that all Eulerian multigraphs with an odd number of edges have no
interval coloring. We also give some methods for constructing of interval
non-edge-colorable Eulerian multigraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2222</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2222</id><created>2013-11-09</created><authors><author><keyname>Frey</keyname><forenames>J&#xe9;r&#xe9;my</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>M&#xfc;hl</keyname><forenames>Christian</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Lotte</keyname><forenames>Fabien</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Hachet</keyname><forenames>Martin</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author></authors><title>Review of the Use of Electroencephalography as an Evaluation Method for
  Human-Computer Interaction</title><categories>cs.HC</categories><comments>PhyCS 2014 - International Conference on Physiological Computing
  Systems (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluating human-computer interaction is essential as a broadening population
uses machines, sometimes in sensitive contexts. However, traditional evaluation
methods may fail to combine real-time measures, an &quot;objective&quot; approach and
data contextualization. In this review we look at how adding neuroimaging
techniques can respond to such needs. We focus on electroencephalography (EEG),
as it could be handled effectively during a dedicated evaluation phase. We
identify workload, attention, vigilance, fatigue, error recognition, emotions,
engagement, flow and immersion as being recognizable by EEG. We find that
workload, attention and emotions assessments would benefit the most from EEG.
Moreover, we advocate to study further error recognition through neuroimaging
to enhance usability and increase user experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2229</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2229</id><created>2013-11-09</created><updated>2014-02-03</updated><authors><author><keyname>Chi</keyname><forenames>Yuejie</forenames></author></authors><title>Joint Sparsity Recovery for Spectral Compressed Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed Sensing (CS) is an effective approach to reduce the required
number of samples for reconstructing a sparse signal in an a priori basis, but
may suffer severely from the issue of basis mismatch. In this paper we study
the problem of simultaneously recovering multiple spectrally-sparse signals
that are supported on the same frequencies lying arbitrarily on the unit
circle. We propose an atomic norm minimization problem, which can be regarded
as a continuous counterpart of the discrete CS formulation and be solved
efficiently via semidefinite programming. Through numerical experiments, we
show that the number of samples per signal may be further reduced by harnessing
the joint sparsity pattern of multiple signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2234</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2234</id><created>2013-11-09</created><updated>2014-03-08</updated><authors><author><keyname>Oliva</keyname><forenames>Junier B.</forenames></author><author><keyname>Poczos</keyname><forenames>Barnabas</forenames></author><author><keyname>Verstynen</keyname><forenames>Timothy</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Schneider</keyname><forenames>Jeff</forenames></author><author><keyname>Yeh</keyname><forenames>Fang-Cheng</forenames></author><author><keyname>Tseng</keyname><forenames>Wen-Yih</forenames></author></authors><title>FuSSO: Functional Shrinkage and Selection Operator</title><categories>stat.ML cs.LG math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the FuSSO, a functional analogue to the LASSO, that efficiently
finds a sparse set of functional input covariates to regress a real-valued
response against. The FuSSO does so in a semi-parametric fashion, making no
parametric assumptions about the nature of input functional covariates and
assuming a linear form to the mapping of functional covariates to the response.
We provide a statistical backing for use of the FuSSO via proof of asymptotic
sparsistency under various conditions. Furthermore, we observe good results on
both synthetic and real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2236</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2236</id><created>2013-11-09</created><updated>2014-03-08</updated><authors><author><keyname>Oliva</keyname><forenames>Junier B.</forenames></author><author><keyname>Neiswanger</keyname><forenames>Willie</forenames></author><author><keyname>Poczos</keyname><forenames>Barnabas</forenames></author><author><keyname>Schneider</keyname><forenames>Jeff</forenames></author><author><keyname>Xing</keyname><forenames>Eric</forenames></author></authors><title>Fast Distribution To Real Regression</title><categories>stat.ML cs.LG math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of distribution to real-value regression, where one aims
to regress a mapping $f$ that takes in a distribution input covariate $P\in
\mathcal{I}$ (for a non-parametric family of distributions $\mathcal{I}$) and
outputs a real-valued response $Y=f(P) + \epsilon$. This setting was recently
studied, and a &quot;Kernel-Kernel&quot; estimator was introduced and shown to have a
polynomial rate of convergence. However, evaluating a new prediction with the
Kernel-Kernel estimator scales as $\Omega(N)$. This causes the difficult
situation where a large amount of data may be necessary for a low estimation
risk, but the computation cost of estimation becomes infeasible when the
data-set is too large. To this end, we propose the Double-Basis estimator,
which looks to alleviate this big data problem in two ways: first, the
Double-Basis estimator is shown to have a computation complexity that is
independent of the number of of instances $N$ when evaluating new predictions
after training; secondly, the Double-Basis estimator is shown to have a fast
rate of convergence for a general class of mappings $f\in\mathcal{F}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2241</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2241</id><created>2013-11-09</created><authors><author><keyname>Liu</keyname><forenames>Ying</forenames></author><author><keyname>Willsky</keyname><forenames>Alan S.</forenames></author></authors><title>Learning Gaussian Graphical Models with Observed or Latent FVSs</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian Graphical Models (GGMs) or Gauss Markov random fields are widely
used in many applications, and the trade-off between the modeling capacity and
the efficiency of learning and inference has been an important research
problem. In this paper, we study the family of GGMs with small feedback vertex
sets (FVSs), where an FVS is a set of nodes whose removal breaks all the
cycles. Exact inference such as computing the marginal distributions and the
partition function has complexity $O(k^{2}n)$ using message-passing algorithms,
where k is the size of the FVS, and n is the total number of nodes. We propose
efficient structure learning algorithms for two cases: 1) All nodes are
observed, which is useful in modeling social or flight networks where the FVS
nodes often correspond to a small number of high-degree nodes, or hubs, while
the rest of the networks is modeled by a tree. Regardless of the maximum
degree, without knowing the full graph structure, we can exactly compute the
maximum likelihood estimate in $O(kn^2+n^2\log n)$ if the FVS is known or in
polynomial time if the FVS is unknown but has bounded size. 2) The FVS nodes
are latent variables, where structure learning is equivalent to decomposing a
inverse covariance matrix (exactly or approximately) into the sum of a
tree-structured matrix and a low-rank matrix. By incorporating efficient
inference into the learning steps, we can obtain a learning algorithm using
alternating low-rank correction with complexity $O(kn^{2}+n^{2}\log n)$ per
iteration. We also perform experiments using both synthetic data as well as
real data of flight delays to demonstrate the modeling capacity with FVSs of
various sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2252</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2252</id><created>2013-11-10</created><authors><author><keyname>El-Yaniv</keyname><forenames>Ran</forenames></author><author><keyname>Yanay</keyname><forenames>David</forenames></author></authors><title>Semantic Sort: A Supervised Approach to Personalized Semantic
  Relatedness</title><categories>cs.CL cs.LG</categories><comments>37 pages, 8 figures A short version of this paper was already
  published at ECML/PKDD 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and study a novel supervised approach to learning statistical
semantic relatedness models from subjectively annotated training examples. The
proposed semantic model consists of parameterized co-occurrence statistics
associated with textual units of a large background knowledge corpus. We
present an efficient algorithm for learning such semantic models from a
training sample of relatedness preferences. Our method is corpus independent
and can essentially rely on any sufficiently large (unstructured) collection of
coherent texts. Moreover, the approach facilitates the fitting of semantic
models for specific users or groups of users. We present the results of
extensive range of experiments from small to large scale, indicating that the
proposed method is effective and competitive with the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2271</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2271</id><created>2013-11-10</created><authors><author><keyname>Daniely</keyname><forenames>Amit</forenames></author><author><keyname>Linial</keyname><forenames>Nati</forenames></author><author><keyname>Shwartz</keyname><forenames>Shai Shalev</forenames></author></authors><title>More data speeds up training time in learning halfspaces over sparse
  vectors</title><categories>cs.LG</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increased availability of data in recent years has led several authors to
ask whether it is possible to use data as a {\em computational} resource. That
is, if more data is available, beyond the sample complexity limit, is it
possible to use the extra examples to speed up the computation time required to
perform the learning task?
  We give the first positive answer to this question for a {\em natural
supervised learning problem} --- we consider agnostic PAC learning of
halfspaces over $3$-sparse vectors in $\{-1,1,0\}^n$. This class is
inefficiently learnable using $O\left(n/\epsilon^2\right)$ examples. Our main
contribution is a novel, non-cryptographic, methodology for establishing
computational-statistical gaps, which allows us to show that, under a widely
believed assumption that refuting random $\mathrm{3CNF}$ formulas is hard, it
is impossible to efficiently learn this class using only
$O\left(n/\epsilon^2\right)$ examples. We further show that under stronger
hardness assumptions, even $O\left(n^{1.499}/\epsilon^2\right)$ examples do not
suffice. On the other hand, we show a new algorithm that learns this class
efficiently using $\tilde{\Omega}\left(n^2/\epsilon^2\right)$ examples. This
formally establishes the tradeoff between sample and computational complexity
for a natural supervised learning problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2272</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2272</id><created>2013-11-10</created><updated>2014-03-09</updated><authors><author><keyname>Daniely</keyname><forenames>Amit</forenames></author><author><keyname>Linial</keyname><forenames>Nati</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author></authors><title>From average case complexity to improper learning complexity</title><categories>cs.LG cs.CC</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The basic problem in the PAC model of computational learning theory is to
determine which hypothesis classes are efficiently learnable. There is
presently a dearth of results showing hardness of learning problems. Moreover,
the existing lower bounds fall short of the best known algorithms.
  The biggest challenge in proving complexity results is to establish hardness
of {\em improper learning} (a.k.a. representation independent learning).The
difficulty in proving lower bounds for improper learning is that the standard
reductions from $\mathbf{NP}$-hard problems do not seem to apply in this
context. There is essentially only one known approach to proving lower bounds
on improper learning. It was initiated in (Kearns and Valiant 89) and relies on
cryptographic assumptions.
  We introduce a new technique for proving hardness of improper learning, based
on reductions from problems that are hard on average. We put forward a (fairly
strong) generalization of Feige's assumption (Feige 02) about the complexity of
refuting random constraint satisfaction problems. Combining this assumption
with our new technique yields far reaching implications. In particular,
  1. Learning $\mathrm{DNF}$'s is hard.
  2. Agnostically learning halfspaces with a constant approximation ratio is
hard.
  3. Learning an intersection of $\omega(1)$ halfspaces is hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2276</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2276</id><created>2013-11-10</created><authors><author><keyname>Nair</keyname><forenames>Vinod</forenames></author><author><keyname>Kidambi</keyname><forenames>Rahul</forenames></author><author><keyname>Sellamanickam</keyname><forenames>Sundararajan</forenames></author><author><keyname>Keerthi</keyname><forenames>S. Sathiya</forenames></author><author><keyname>Gehrke</keyname><forenames>Johannes</forenames></author><author><keyname>Narayanan</keyname><forenames>Vijay</forenames></author></authors><title>A Quantitative Evaluation Framework for Missing Value Imputation
  Algorithms</title><categories>cs.LG</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of quantitatively evaluating missing value imputation
algorithms. Given a dataset with missing values and a choice of several
imputation algorithms to fill them in, there is currently no principled way to
rank the algorithms using a quantitative metric. We develop a framework based
on treating imputation evaluation as a problem of comparing two distributions
and show how it can be used to compute quantitative metrics. We present an
efficient procedure for applying this framework to practical datasets,
demonstrate several metrics derived from the existing literature on comparing
distributions, and propose a new metric called Neighborhood-based Dissimilarity
Score which is fast to compute and provides similar results. Results are shown
on several datasets, metrics, and imputations algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2290</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2290</id><created>2013-11-10</created><authors><author><keyname>Pagani</keyname><forenames>Michele</forenames></author><author><keyname>Selinger</keyname><forenames>Peter</forenames></author><author><keyname>Valiron</keyname><forenames>Beno&#xee;t</forenames></author></authors><title>Applying quantitative semantics to higher-order quantum computing</title><categories>cs.LO cs.PL quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding a denotational semantics for higher order quantum computation is a
long-standing problem in the semantics of quantum programming languages. Most
past approaches to this problem fell short in one way or another, either
limiting the language to an unusably small finitary fragment, or giving up
important features of quantum physics such as entanglement. In this paper, we
propose a denotational semantics for a quantum lambda calculus with recursion
and an infinite data type, using constructions from quantitative semantics of
linear logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2296</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2296</id><created>2013-11-10</created><updated>2014-05-19</updated><authors><author><keyname>Ghoshdastidar</keyname><forenames>Debarghya</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>Newton based Stochastic Optimization using q-Gaussian Smoothed
  Functional Algorithms</title><categories>math.OC cs.IT math.IT</categories><comments>This is a longer of version of the paper with the same title accepted
  in Automatica</comments><doi>10.1016/j.automatica.2014.08.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first q-Gaussian smoothed functional (SF) estimator of the
Hessian and the first Newton-based stochastic optimization algorithm that
estimates both the Hessian and the gradient of the objective function using
q-Gaussian perturbations. Our algorithm requires only two system simulations
(regardless of the parameter dimension) and estimates both the gradient and the
Hessian at each update epoch using these. We also present a proof of
convergence of the proposed algorithm. In a related recent work (Ghoshdastidar
et al., 2013), we presented gradient SF algorithms based on the q-Gaussian
perturbations. Our work extends prior work on smoothed functional algorithms by
generalizing the class of perturbation distributions as most distributions
reported in the literature for which SF algorithms are known to work and turn
out to be special cases of the q-Gaussian distribution. Besides studying the
convergence properties of our algorithm analytically, we also show the results
of several numerical simulations on a model of a queuing network, that
illustrate the significance of the proposed method. In particular, we observe
that our algorithm performs better in most cases, over a wide range of
q-values, in comparison to Newton SF algorithms with the Gaussian (Bhatnagar,
2007) and Cauchy perturbations, as well as the gradient q-Gaussian SF
algorithms (Ghoshdastidar et al., 2013).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2309</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2309</id><created>2013-11-10</created><authors><author><keyname>Khuller</keyname><forenames>Samir</forenames></author><author><keyname>Purohit</keyname><forenames>Manish</forenames></author><author><keyname>Sarpatwar</keyname><forenames>Kanthi</forenames></author></authors><title>Analyzing the Optimal Neighborhood: Algorithms for Budgeted and Partial
  Connected Dominating Set Problems</title><categories>cs.DS</categories><comments>15 pages, Conference version to appear in ACM-SIAM SODA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study partial and budgeted versions of the well studied connected
dominating set problem. In the partial connected dominating set problem, we are
given an undirected graph G = (V,E) and an integer n', and the goal is to find
a minimum subset of vertices that induces a connected subgraph of G and
dominates at least n' vertices. We obtain the first polynomial time algorithm
with an O(\ln \Delta) approximation factor for this problem, thereby
significantly extending the results of Guha and Khuller (Algorithmica, Vol.
20(4), Pages 374-387, 1998) for the connected dominating set problem. We note
that none of the methods developed earlier can be applied directly to solve
this problem. In the budgeted connected dominating set problem, there is a
budget on the number of vertices we can select, and the goal is to dominate as
many vertices as possible. We obtain a (1/13)(1 - 1/e) approximation algorithm
for this problem. Finally, we show that our techniques extend to a more general
setting where the profit function associated with a subset of vertices is a
monotone &quot;special&quot; submodular function. This generalization captures the
connected dominating set problem with capacities and/or weighted profits as
special cases. This implies a O(\ln q) approximation (where q denotes the
quota) and an O(1) approximation algorithms for the partial and budgeted
versions of these problems. While the algorithms are simple, the results make a
surprising use of the greedy set cover framework in defining a useful profit
function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2311</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2311</id><created>2013-11-10</created><authors><author><keyname>Krivulin</keyname><forenames>Nikolai</forenames></author><author><keyname>Zimmermann</keyname><forenames>Karel</forenames></author></authors><title>Direct solutions to tropical optimization problems with nonlinear
  objective functions and boundary constraints</title><categories>math.OC cs.SY</categories><comments>Mathematical Methods and Optimization Techniques in Engineering:
  Proc. 1st Intern. Conf. on Optimization Techniques in Engineering (OTENG
  '13), Antalya, Turkey, October 8-10, 2013, WSEAS Press, 2013, pp. 86-91. ISBN
  978-960-474-339-1</comments><msc-class>65K10 (Primary), 15A80, 65K05, 90B85, 41A50 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine two multidimensional optimization problems that are formulated in
terms of tropical mathematics. The problems are to minimize nonlinear objective
functions, which are defined through the multiplicative conjugate vector
transposition on vectors of a finite-dimensional semimodule over an idempotent
semifield, and subject to boundary constraints. The solution approach is
implemented, which involves the derivation of the sharp bounds on the objective
functions, followed by determination of vectors that yield the bound. Based on
the approach, direct solutions to the problems are obtained in a compact vector
form. To illustrate, we apply the results to solving constrained Chebyshev
approximation and location problems, and give numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2318</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2318</id><created>2013-11-10</created><updated>2014-06-11</updated><authors><author><keyname>Richmond</keyname><forenames>L. Bruce</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>Counting the Palstars</title><categories>math.CO cs.DM cs.FL</categories><msc-class>05A15, 05A16, 05A05, 68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A palstar (after Knuth, Morris, and Pratt) is a concatenation of even-length
palindromes. We show that, asymptotically, there are $\Theta(\alpha_k^n)$
palstars of length $2n$ over a $k$-letter alphabet, where $\alpha_k$ is a
constant such that $2k-1 &lt; \alpha_k &lt; 2k-{1 \over 2}$. In particular, $\alpha_2
\doteq 3.33513193$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2321</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2321</id><created>2013-11-10</created><authors><author><keyname>Aboutorab</keyname><forenames>Neda</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author></authors><title>On Improving the Balance between the Completion Time and Decoding Delay
  in Instantly Decodable Network Coded Systems</title><categories>cs.IT math.IT</categories><comments>13 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the complicated interplay of the completion time (as a
measure of throughput) and the decoding delay performance in instantly
decodable network coded (IDNC) systems over wireless broadcast erasure channels
with memory, and proposes two new algorithms that improve the balance between
the completion time and decoding delay of broadcasting a block of packets. We
first formulate the IDNC packet selection problem that provides joint control
of the completion time and decoding delay as a statistical shortest path (SSP)
problem. However, since finding the optimal packet selection policy using the
SSP technique is computationally complex, we employ its geometric structure to
find some guidelines and use them to propose two heuristic packet selection
algorithms that can efficiently improve the balance between the completion time
and decoding delay for broadcast erasure channels with a wide range of memory
conditions. It is shown that each one of the two proposed algorithms is
superior for a specific range of memory conditions. Furthermore, we show that
the proposed algorithms achieve an improved fairness in terms of the decoding
delay across all receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2329</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2329</id><created>2013-11-10</created><authors><author><keyname>Sun</keyname><forenames>Jiajun</forenames></author></authors><title>Uplink Scheduling Strategy Based on A Population Game in Vehicular
  Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in the integration of vehicular sensor network (VSN)
technology, and crowd sensing leveraging pervasive sensors called onboard units
(OBUs), like smartphones and radio frequency IDentifications to provide sensing
services, have attracted increasing attention from both industry and academy.
Nowadays, existing vehicular sensing applications lack good mechanisms to
improve the maximum achievable throughput and minimizing service time of
participating sensing OBUs in vehicular sensor networks. To fill these gaps, in
this paper, first, we introduce real imperfect link states to the calculation
of Markov chains. Second, we incorporate the result of different link states
for multiple types of vehicles with the calculations of uplink throughput and
service time. Third, in order to accurately calculate the service time of an
OBU, we introduce the steady state probability to calculate the exact time of a
duration for back-off decrement, rather than using the traditional relative
probability. Additionally, to our best knowledge, we first explore a
multichannel scheduling strategy of uplink data access in a single roadside
unit (RSU) by using a non-cooperative game in a RSU coverage region to maximize
the uplink throughput and minimize service time under saturated and unsaturated
traffic loads. To this end, we conduct a theoretical analysis and find the
equilibrium point of the scheduling. The numerical results show that the
solution of the equilibrium points are consistent with optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2331</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2331</id><created>2013-11-10</created><authors><author><keyname>Ruan</keyname><forenames>Hang</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Robust Adaptive Beamforming Based on Low-Complexity Shrinkage-Based
  Mismatch Estimation</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures. IEEE Signal Processing Letters, 2013</comments><doi>10.1109/LSP.2013.2290948</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a low-complexity robust adaptive beamforming (RAB)
technique which estimates the steering vector using a Low-Complexity
Shrinkage-Based Mismatch Estimation (LOCSME) algorithm. The proposed LOCSME
algorithm estimates the covariance matrix of the input data and the
interference-plus-noise covariance (INC) matrix by using the Oracle
Approximating Shrinkage (OAS) method. LOCSME only requires prior knowledge of
the angular sector in which the actual steering vector is located and the
antenna array geometry. LOCSME does not require a costly optimization algorithm
and does not need to know extra information from the interferers, which avoids
direction finding for all interferers. Simulations show that LOCSME outperforms
previously reported RAB algorithms and has a performance very close to the
optimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2334</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2334</id><created>2013-11-10</created><updated>2014-01-29</updated><authors><author><keyname>Elgohary</keyname><forenames>Ahmed</forenames></author><author><keyname>Farahat</keyname><forenames>Ahmed K.</forenames></author><author><keyname>Kamel</keyname><forenames>Mohamed S.</forenames></author><author><keyname>Karray</keyname><forenames>Fakhri</forenames></author></authors><title>Embed and Conquer: Scalable Embeddings for Kernel k-Means on MapReduce</title><categories>cs.LG</categories><comments>Appears in Proceedings of the SIAM International Conference on Data
  Mining (SDM), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The kernel $k$-means is an effective method for data clustering which extends
the commonly-used $k$-means algorithm to work on a similarity matrix over
complex data structures. The kernel $k$-means algorithm is however
computationally very complex as it requires the complete data matrix to be
calculated and stored. Further, the kernelized nature of the kernel $k$-means
algorithm hinders the parallelization of its computations on modern
infrastructures for distributed computing. In this paper, we are defining a
family of kernel-based low-dimensional embeddings that allows for scaling
kernel $k$-means on MapReduce via an efficient and unified parallelization
strategy. Afterwards, we propose two methods for low-dimensional embedding that
adhere to our definition of the embedding family. Exploiting the proposed
parallelization strategy, we present two scalable MapReduce algorithms for
kernel $k$-means. We demonstrate the effectiveness and efficiency of the
proposed algorithms through an empirical evaluation on benchmark data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2337</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2337</id><created>2013-11-10</created><updated>2013-11-19</updated><authors><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Tomamichel</keyname><forenames>Marco</forenames></author></authors><title>The Third-Order Term in the Normal Approximation for the AWGN Channel</title><categories>cs.IT math.IT</categories><comments>13 pages, 1 figure</comments><journal-ref>IEEE Trans. Inf. Theory 61(5), 2430 - 2438 (2015)</journal-ref><doi>10.1109/TIT.2015.2411256</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that, under the average error probability formalism, the
third-order term in the normal approximation for the additive white Gaussian
noise channel with a maximal or equal power constraint is at least $\frac{1}{2}
\log n + O(1)$. This matches the upper bound derived by
Polyanskiy-Poor-Verd\'{u} (2010).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2342</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2342</id><created>2013-11-10</created><authors><author><keyname>Rivero</keyname><forenames>Carlos R.</forenames></author><author><keyname>Jamil</keyname><forenames>Hasan M.</forenames></author></authors><title>Anatomy of Graph Matching based on an XQuery and RDF Implementation</title><categories>cs.DB</categories><comments>Technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs are becoming one of the most popular data modeling paradigms since
they are able to model complex relationships that cannot be easily captured
using traditional data models. One of the major tasks of graph management is
graph matching, which aims to find all of the subgraphs in a data graph that
match a query graph. In the literature, proposals in this context are
classified into two different categories: graph-at-a-time, which process the
whole query graph at the same time, and vertex-at-a-time, which process a
single vertex of the query graph at the same time. In this paper, we propose a
new vertex-at-a-time proposal that is based on graphlets, each of which
comprises a vertex of a graph, all of the immediate neighbors of that vertex,
and all of the edges that relate those neighbors. Furthermore, we also use the
concept of minimum hub covers, each of which comprises a subset of vertices in
the query graph that account for all of the edges in that graph. We present the
algorithms of our proposal and describe an implementation based on XQuery and
RDF. Our evaluation results show that our proposal is appealing to perform
graph matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2346</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2346</id><created>2013-11-10</created><authors><author><keyname>Sin</keyname><forenames>Myong-Son</forenames></author><author><keyname>Kim</keyname><forenames>Ryul</forenames></author></authors><title>Some New Results on Equivalency of Collusion-Secure Properties for
  Reed-Solomon Codes</title><categories>cs.IT math.IT math.RA</categories><comments>10 pages</comments><report-no>KISU-MATH-2013-E-R-033</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A. Silverberg (IEEE Trans. Inform. Theory 49, 2003) proposed a question on
the equivalence of identifiable parent property and traceability property for
Reed-Solomon code family. Earlier studies on Silverberg's problem motivate us
to think of the stronger version of the question on equivalence of separation
and traceability properties. Both, however, still remain open. In this article,
we integrate all the previous works on this problem with an algebraic way, and
present some new results. It is notable that the concept of subspace subcode of
Reed-Solomon code, which was introduced in error-correcting code theory,
provides an interesting prospect for our topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2349</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2349</id><created>2013-11-10</created><authors><author><keyname>Amintoosi</keyname><forenames>Haleh</forenames></author><author><keyname>Kanhere</keyname><forenames>Salil S.</forenames></author></authors><title>Providing Trustworthy Contributions via a Reputation Framework in Social
  Participatory Sensing Systems</title><categories>cs.SI cs.IR</categories><report-no>CSE-TR 201304</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social participatory sensing is a newly proposed paradigm that tries to
address the limitations of participatory sensing by leveraging online social
networks as an infrastructure. A critical issue in the success of this paradigm
is to assure the trustworthiness of contributions provided by participants. In
this paper, we propose an application-agnostic reputation framework for social
participatory sensing systems. Our framework considers both the quality of
contribution and the trustworthiness level of participant within the social
network. These two aspects are then combined via a fuzzy inference system to
arrive at a final trust rating for a contribution. A reputation score is also
calculated for each participant as a resultant of the trust ratings assigned to
him. We adopt the utilization of PageRank algorithm as the building block for
our reputation module. Extensive simulations demonstrate the efficacy of our
framework in achieving high overall trust and assigning accurate reputation
scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2355</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2355</id><created>2013-11-11</created><authors><author><keyname>G&#xf6;&#xf6;s</keyname><forenames>Mika</forenames></author><author><keyname>Pitassi</keyname><forenames>Toniann</forenames></author></authors><title>Communication Lower Bounds via Critical Block Sensitivity</title><categories>cs.CC</categories><comments>33 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use critical block sensitivity, a new complexity measure introduced by
Huynh and Nordstr\&quot;om (STOC 2012), to study the communication complexity of
search problems. To begin, we give a simple new proof of the following central
result of Huynh and Nordstr\&quot;om: if $S$ is a search problem with critical block
sensitivity $b$, then every randomised two-party protocol solving a certain
two-party lift of $S$ requires $\Omega(b)$ bits of communication. Besides
simplicity, our proof has the advantage of generalising to the multi-party
setting. We combine these results with new critical block sensitivity lower
bounds for Tseitin and Pebbling search problems to obtain the following
applications:
  (1) Monotone Circuit Depth: We exhibit a monotone function on $n$ variables
whose monotone circuits require depth $\Omega(n/\log n)$; previously, a bound
of $\Omega(\sqrt{n})$ was known (Raz and Wigderson, JACM 1992). Moreover, we
prove a tight $\Theta(\sqrt{n})$ monotone depth bound for a function in
monotone P. This implies an average-case hierarchy theorem within monotone P
similar to a result of Filmus et al. (FOCS 2013).
  (2) Proof Complexity: We prove new rank lower bounds as well as obtain the
first length--space lower bounds for semi-algebraic proof systems, including
Lov\'asz--Schrijver and Lasserre (SOS) systems. In particular, these results
extend and simplify the works of Beame et al. (SICOMP 2007) and Huynh and
Nordstr\&quot;om.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2358</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2358</id><created>2013-11-11</created><authors><author><keyname>Fu</keyname><forenames>Bin</forenames></author></authors><title>Derandomizing Polynomial Identity over Finite Fields Implies
  Super-Polynomial Circuit Lower Bounds for NEXP</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that derandomizing polynomial identity testing over an arbitrary
finite field implies that NEXP does not have polynomial size boolean circuits.
In other words, for any finite field F(q) of size q, $PIT_q\in
NSUBEXP\Rightarrow NEXP\not\subseteq P/poly$, where $PIT_q$ is the polynomial
identity testing problem over F(q), and NSUBEXP is the nondeterministic
subexpoential time class of languages. Our result is in contract to Kabanets
and Impagliazzo's existing theorem that derandomizing the polynomial identity
testing in the integer ring Z implies that NEXP does have polynomial size
boolean circuits or permanent over Z does not have polynomial size arithmetic
circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2362</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2362</id><created>2013-11-11</created><authors><author><keyname>Gunadi</keyname><forenames>Hendra</forenames></author><author><keyname>Tiu</keyname><forenames>Alwen</forenames></author></authors><title>Efficient Runtime Monitoring with Metric Temporal Logic: A Case Study in
  the Android Operating System</title><categories>cs.LO cs.CR cs.OS</categories><acm-class>F.3.1; D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a design and an implementation of a security policy specification
language based on metric linear-time temporal logic (MTL). MTL features
temporal operators that are indexed by time intervals, allowing one to specify
timing-dependent security policies. The design of the language is driven by the
problem of runtime monitoring of applications in mobile devices. A main case
the study is the privilege escalation attack in the Android operating system,
where an app gains access to certain resource or functionalities that are not
explicitly granted to it by the user, through indirect control flow. To capture
these attacks, we extend MTL with recursive definitions, that are used to
express call chains betwen apps. We then show how the metric operators of MTL,
in combination with recursive definitions, can be used to specify policies to
detect privilege escalation, under various fine grained constraints. We present
a new algorithm, extending that of linear time temporal logic, for monitoring
safety policies written in our specification language. The monitor does not
need to store the entire history of events generated by the apps, something
that is crucial for practical implementations. We modified the Android OS
kernel to allow us to insert our generated monitors modularly. We have tested
the modified OS on an actual device, and show that it is effective in detecting
policy violations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2364</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2364</id><created>2013-11-11</created><authors><author><keyname>Samara</keyname><forenames>Ghassan</forenames></author><author><keyname>Salem</keyname><forenames>Amer O Abu</forenames></author><author><keyname>Alhmiedat</keyname><forenames>Tareq</forenames></author></authors><title>Power Control Protocols in VANET</title><categories>cs.NI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular Ad hoc Networks (VANET) is one of the most challenging research
area in the field of the Mobile Ad hoc Network (MANET), Power control is a
critical issue in VANETwhere is should be managed carefully to help the channel
to have high performance. In this paper a comparative study in the published
protocols in the field of safety message dynamic power control will be
presented and evaluated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2369</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2369</id><created>2013-11-11</created><updated>2015-11-22</updated><authors><author><keyname>Rothvoss</keyname><forenames>Thomas</forenames></author></authors><title>The matching polytope has exponential extension complexity</title><categories>cs.CC cs.DM math.CO</categories><msc-class>52B11</msc-class><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A popular method in combinatorial optimization is to express polytopes P,
which may potentially have exponentially many facets, as solutions of linear
programs that use few extra variables to reduce the number of constraints down
to a polynomial. After two decades of standstill, recent years have brought
amazing progress in showing lower bounds for the so called extension
complexity, which for a polytope P denotes the smallest number of inequalities
necessary to describe a higher dimensional polytope Q that can be linearly
projected on P.
  However, the central question in this field remained wide open: can the
perfect matching polytope be written as an LP with polynomially many
constraints?
  We answer this question negatively. In fact, the extension complexity of the
perfect matching polytope in a complete n-node graph is 2^Omega(n). By a known
reduction this also improves the lower bound on the extension complexity for
the TSP polytope from 2^Omega(n^1/2) to 2^Omega(n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2376</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2376</id><created>2013-11-11</created><updated>2014-04-24</updated><authors><author><keyname>Ottaviani</keyname><forenames>Giorgio</forenames></author><author><keyname>Spaenlehauer</keyname><forenames>Pierre-Jean</forenames></author><author><keyname>Sturmfels</keyname><forenames>Bernd</forenames></author></authors><title>Exact Solutions in Structured Low-Rank Approximation</title><categories>math.OC cs.SC math.AG stat.CO</categories><comments>22 pages; changed title, additional material on computations and
  applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured low-rank approximation is the problem of minimizing a weighted
Frobenius distance to a given matrix among all matrices of fixed rank in a
linear space of matrices. We study exact solutions to this problem by way of
computational algebraic geometry. A particular focus lies on Hankel matrices,
Sylvester matrices and generic linear spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2378</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2378</id><created>2013-11-11</created><authors><author><keyname>Balamurugan</keyname><forenames>P.</forenames></author><author><keyname>Shevade</keyname><forenames>Shirish</forenames></author><author><keyname>Sundararajan</keyname><forenames>S.</forenames></author><author><keyname>Keerthi</keyname><forenames>S. S</forenames></author></authors><title>An Empirical Evaluation of Sequence-Tagging Trainers</title><categories>cs.LG</categories><comments>18 pages, 5 figures ams.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of assigning label sequences to a set of observed sequences is
common in computational linguistics. Several models for sequence labeling have
been proposed over the last few years. Here, we focus on discriminative models
for sequence labeling. Many batch and online (updating model parameters after
visiting each example) learning algorithms have been proposed in the
literature. On large datasets, online algorithms are preferred as batch
learning methods are slow. These online algorithms were designed to solve
either a primal or a dual problem. However, there has been no systematic
comparison of these algorithms in terms of their speed, generalization
performance (accuracy/likelihood) and their ability to achieve steady state
generalization performance fast. With this aim, we compare different algorithms
and make recommendations, useful for a practitioner. We conclude that the
selection of an algorithm for sequence labeling depends on the evaluation
criterion used and its implementation simplicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2400</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2400</id><created>2013-11-11</created><updated>2015-12-02</updated><authors><author><keyname>Engelfriet</keyname><forenames>Joost</forenames></author><author><keyname>Maneth</keyname><forenames>Sebastian</forenames></author><author><keyname>Seidl</keyname><forenames>Helmut</forenames></author></authors><title>Look-Ahead Removal for Top-Down Tree Transducers</title><categories>cs.FL</categories><comments>57 pages, to appear in TCS</comments><msc-class>68Q05, 68Q45</msc-class><acm-class>F.1.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Top-down tree transducers are a convenient formalism for describing tree
transformations. They can be equipped with regular look-ahead, which allows
them to inspect a subtree before processing it. In certain cases, such a
look-ahead can be avoided and the transformation can be realized by a
transducer without look-ahead. Removing the look-ahead from a transducer, if
possible, is technically highly challenging. For a restricted class of
transducers with look-ahead, namely those that are total, deterministic,
ultralinear, and bounded erasing, we present an algorithm that, for a given
transducer from that class, (1) decides whether it is equivalent to a total
deterministic transducer without look-ahead, and (2) constructs such a
transducer if the answer is positive. For the whole class of total
deterministic transducers with look-ahead we present a similar algorithm, which
assumes that a so-called difference bound is known for the given transducer.
The designer of a transducer can usually also determine a difference bound for
it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2412</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2412</id><created>2013-11-11</created><authors><author><keyname>Petre</keyname><forenames>Marian</forenames></author><author><keyname>Wilson</keyname><forenames>Greg</forenames></author></authors><title>PLOS/Mozilla Scientific Code Review Pilot: Summary of Findings</title><categories>cs.SE</categories><comments>5 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  PLOS and Mozilla conducted a month-long pilot study in which professional
developers performed code reviews on software associated with papers published
in PLOS Computational Biology. While the developers felt the reviews were
limited by (a) lack of familiarity with the domain and (b) lack of two-way
contact with authors, the scientists appreciated the reviews, and both sides
were enthusiastic about repeating the experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2426</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2426</id><created>2013-11-11</created><authors><author><keyname>Blomer</keyname><forenames>J.</forenames></author><author><keyname>Berzano</keyname><forenames>D.</forenames></author><author><keyname>Buncic</keyname><forenames>P.</forenames></author><author><keyname>Charalampidis</keyname><forenames>I.</forenames></author><author><keyname>Ganis</keyname><forenames>G.</forenames></author><author><keyname>Lestaris</keyname><forenames>G.</forenames></author><author><keyname>Meusel</keyname><forenames>R.</forenames></author><author><keyname>Nicolaou</keyname><forenames>V.</forenames></author></authors><title>Micro-CernVM: Slashing the Cost of Building and Deploying Virtual
  Machines</title><categories>cs.DC</categories><comments>Conference paper at the 2013 Computing in High Energy Physics (CHEP)
  Conference, Amsterdam</comments><doi>10.1088/1742-6596/513/3/032009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The traditional virtual machine building and and deployment process is
centered around the virtual machine hard disk image. The packages comprising
the VM operating system are carefully selected, hard disk images are built for
a variety of different hypervisors, and images have to be distributed and
decompressed in order to instantiate a virtual machine. Within the HEP
community, the CernVM File System has been established in order to decouple the
distribution from the experiment software from the building and distribution of
the VM hard disk images.
  We show how to get rid of such pre-built hard disk images altogether. Due to
the high requirements on POSIX compliance imposed by HEP application software,
CernVM-FS can also be used to host and boot a Linux operating system. This
allows the use of a tiny bootable CD image that comprises only a Linux kernel
while the rest of the operating system is provided on demand by CernVM-FS. This
approach speeds up the initial instantiation time and reduces virtual machine
image sizes by an order of magnitude. Furthermore, security updates can be
distributed instantaneously through CernVM-FS. By leveraging the fact that
CernVM-FS is a versioning file system, a historic analysis environment can be
easily re-spawned by selecting the corresponding CernVM-FS file system
snapshot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2433</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2433</id><created>2013-11-11</created><authors><author><keyname>Valsesia</keyname><forenames>Diego</forenames></author><author><keyname>Coluccia</keyname><forenames>Giulio</forenames></author><author><keyname>Magli</keyname><forenames>Enrico</forenames></author></authors><title>Joint recovery algorithms using difference of innovations for
  distributed compressed sensing</title><categories>cs.IT math.IT</categories><comments>Conference Record of the Forty Seventh Asilomar Conference on
  Signals, Systems and Computers (ASILOMAR), 2013</comments><doi>10.1109/ACSSC.2013.6810309</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed compressed sensing is concerned with representing an ensemble of
jointly sparse signals using as few linear measurements as possible. Two novel
joint reconstruction algorithms for distributed compressed sensing are
presented in this paper. These algorithms are based on the idea of using one of
the signals as side information; this allows to exploit joint sparsity in a
more effective way with respect to existing schemes. They provide gains in
reconstruction quality, especially when the nodes acquire few measurements, so
that the system is able to operate with fewer measurements than is required by
other existing schemes. We show that the algorithms achieve better performance
with respect to the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2442</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2442</id><created>2013-11-11</created><authors><author><keyname>Bianchi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Bonola</keyname><forenames>Marco</forenames></author><author><keyname>Picierro</keyname><forenames>Giulio</forenames></author><author><keyname>Pontarelli</keyname><forenames>Salvatore</forenames></author><author><keyname>Monaci</keyname><forenames>Marco</forenames></author></authors><title>StreaMon: a data-plane programming abstraction for Software-defined
  Stream Monitoring</title><categories>cs.NI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fast evolving nature of modern cyber threats and network monitoring needs
calls for new, &quot;software-defined&quot;, approaches to simplify and quicken
programming and deployment of online (stream-based) traffic analysis functions.
StreaMon is a carefully designed data-plane abstraction devised to scalably
decouple the &quot;programming logic&quot; of a traffic analysis application (tracked
states, features, anomaly conditions, etc.) from elementary primitives
(counting and metering, matching, events generation, etc), efficiently
pre-implemented in the probes, and used as common instruction set for
supporting the desired logic. Multi-stage multi-step real-time tracking and
detection algorithms are supported via the ability to deploy custom states,
relevant state transitions, and associated monitoring actions and triggering
conditions. Such a separation entails platform-independent, portable, online
traffic analysis tasks written in a high level language, without requiring
developers to access the monitoring device internals and program their custom
monitoring logic via low level compiled languages (e.g., C, assembly, VHDL). We
validate our design by developing a prototype and a set of simple (but
functionally demanding) use-case applications and by testing them over real
traffic traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2444</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2444</id><created>2013-11-11</created><authors><author><keyname>Facchinei</keyname><forenames>Francisco</forenames></author><author><keyname>Sagratella</keyname><forenames>Simone</forenames></author><author><keyname>Scutari</keyname><forenames>Gesualdo</forenames></author></authors><title>Flexible Parallel Algorithms for Big Data Optimization</title><categories>cs.DC cs.GT math.OC</categories><comments>submitted to IEEE ICASSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a decomposition framework for the parallel optimization of the sum
of a differentiable function and a (block) separable nonsmooth, convex one. The
latter term is typically used to enforce structure in the solution as, for
example, in Lasso problems. Our framework is very flexible and includes both
fully parallel Jacobi schemes and Gauss-Seidel (Southwell-type) ones, as well
as virtually all possibilities in between (e.g., gradient- or Newton-type
methods) with only a subset of variables updated at each iteration. Our
theoretical convergence results improve on existing ones, and numerical results
show that the new method compares favorably to existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2448</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2448</id><created>2013-11-11</created><authors><author><keyname>Wimalajeewa</keyname><forenames>Thakshila</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Recovery of Sparse Matrices via Matrix Sketching</title><categories>cs.NA cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of recovering an unknown sparse matrix
X from the matrix sketch Y = AX B^T. The dimension of Y is less than that of X,
and A and B are known matrices. This problem can be solved using standard
compressive sensing (CS) theory after converting it to vector form using the
Kronecker operation. In this case, the measurement matrix assumes a Kronecker
product structure. However, as the matrix dimension increases the associated
computational complexity makes its use prohibitive. We extend two algorithms,
fast iterative shrinkage threshold algorithm (FISTA) and orthogonal matching
pursuit (OMP) to solve this problem in matrix form without employing the
Kronecker product. While both FISTA and OMP with matrix inputs are shown to be
equivalent in performance to their vector counterparts with the Kronecker
product, solving them in matrix form is shown to be computationally more
efficient. We show that the computational gain achieved by FISTA with matrix
inputs over its vector form is more significant compared to that achieved by
OMP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2456</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2456</id><created>2013-11-11</created><updated>2014-10-09</updated><authors><author><keyname>Golovnev</keyname><forenames>Alexander</forenames></author><author><keyname>Kulikov</keyname><forenames>Alexander S.</forenames></author><author><keyname>Mihajlin</keyname><forenames>Ivan</forenames></author></authors><title>Families with infants: a general approach to solve hard partition
  problems</title><categories>cs.DS</categories><comments>18 pages, a revised version of this paper is available at
  http://arxiv.org/abs/1410.2209</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a general approach for solving partition problems where the goal
is to represent a given set as a union (either disjoint or not) of subsets
satisfying certain properties. Many NP-hard problems can be naturally stated as
such partition problems. We show that if one can find a large enough system of
so-called families with infants for a given problem, then this problem can be
solved faster than by a straightforward algorithm. We use this approach to
improve known bounds for several NP-hard problems as well as to simplify the
proofs of several known results.
  For the chromatic number problem we present an algorithm with
$O^*((2-\varepsilon(d))^n)$ time and exponential space for graphs of average
degree $d$. This improves the algorithm by Bj\&quot;{o}rklund et al. [Theory Comput.
Syst. 2010] that works for graphs of bounded maximum (as opposed to average)
degree and closes an open problem stated by Cygan and Pilipczuk [ICALP 2013].
  For the traveling salesman problem we give an algorithm working in
$O^*((2-\varepsilon(d))^n)$ time and polynomial space for graphs of average
degree $d$. The previously known results of this kind is a polyspace algorithm
by Bj\&quot;{o}rklund et al. [ICALP 2008] for graphs of bounded maximum degree and
an exponential space algorithm for bounded average degree by Cygan and
Pilipczuk [ICALP 2013].
  For counting perfect matching in graphs of average degree~$d$ we present an
algorithm with running time $O^*((2-\varepsilon(d))^{n/2})$ and polynomial
space. Recent algorithms of this kind due to Cygan, Pilipczuk [ICALP 2013] and
Izumi, Wadayama [FOCS 2012] (for bipartite graphs only) use exponential space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2460</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2460</id><created>2013-11-06</created><updated>2013-12-16</updated><authors><author><keyname>Alameda-Pineda</keyname><forenames>Xavier</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author></authors><title>Vision-Guided Robot Hearing</title><categories>cs.RO cs.CV</categories><comments>26 pages, many figures and tables, journal</comments><journal-ref>International Journal of Robotics Research, 34 (4-5), 437-456,
  2015</journal-ref><doi>10.1177/0278364914548050</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural human-robot interaction in complex and unpredictable environments is
one of the main research lines in robotics. In typical real-world scenarios,
humans are at some distance from the robot and the acquired signals are
strongly impaired by noise, reverberations and other interfering sources. In
this context, the detection and localisation of speakers plays a key role since
it is the pillar on which several tasks (e.g.: speech recognition and speaker
tracking) rely. We address the problem of how to detect and localize people
that are both seen and heard by a humanoid robot. We introduce a hybrid
deterministic/probabilistic model. Indeed, the deterministic component allows
us to map the visual information into the auditory space. By means of the
probabilistic component, the visual features guide the grouping of the auditory
features in order to form AV objects. The proposed model and the associated
algorithm are implemented in real-time (17 FPS) using a stereoscopic camera
pair and two microphones embedded into the head of the humanoid robot NAO. We
performed experiments on (i) synthetic data, (ii) a publicly available data set
and (iii) data acquired using the robot. The results we obtained validate the
approach and encourage us to further investigate how vision can help robot
hearing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2466</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2466</id><created>2013-11-11</created><updated>2014-02-16</updated><authors><author><keyname>Lampis</keyname><forenames>Michael</forenames></author></authors><title>Parameterized Approximation Schemes using Graph Widths</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Combining the techniques of approximation algorithms and parameterized
complexity has long been considered a promising research area, but relatively
few results are currently known. In this paper we study the parameterized
approximability of a number of problems which are known to be hard to solve
exactly when parameterized by treewidth or clique-width. Our main contribution
is to present a natural randomized rounding technique that extends well-known
ideas and can be used for both of these widths. Applying this very generic
technique we obtain approximation schemes for a number of problems, evading
both polynomial-time inapproximability and parameterized intractability bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2483</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2483</id><created>2013-11-11</created><authors><author><keyname>Da Veiga</keyname><forenames>S&#xe9;bastien</forenames><affiliation>IFPEN, - M&#xe9;thodes d'Analyse Stochastique des Codes et Traitements Num&#xe9;riques</affiliation></author></authors><title>Global Sensitivity Analysis with Dependence Measures</title><categories>math.ST cs.LG stat.ML stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Global sensitivity analysis with variance-based measures suffers from several
theoretical and practical limitations, since they focus only on the variance of
the output and handle multivariate variables in a limited way. In this paper,
we introduce a new class of sensitivity indices based on dependence measures
which overcomes these insufficiencies. Our approach originates from the idea to
compare the output distribution with its conditional counterpart when one of
the input variables is fixed. We establish that this comparison yields
previously proposed indices when it is performed with Csiszar f-divergences, as
well as sensitivity indices which are well-known dependence measures between
random variables. This leads us to investigate completely new sensitivity
indices based on recent state-of-the-art dependence measures, such as distance
correlation and the Hilbert-Schmidt independence criterion. We also emphasize
the potential of feature selection techniques relying on such dependence
measures as alternatives to screening in high dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2492</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2492</id><created>2013-11-11</created><authors><author><keyname>Gallier</keyname><forenames>Jean</forenames></author></authors><title>Notes on Elementary Spectral Graph Theory. Applications to Graph
  Clustering Using Normalized Cuts</title><categories>cs.CV</categories><comments>76 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These are notes on the method of normalized graph cuts and its applications
to graph clustering. I provide a fairly thorough treatment of this deeply
original method due to Shi and Malik, including complete proofs. I include the
necessary background on graphs and graph Laplacians. I then explain in detail
how the eigenvectors of the graph Laplacian can be used to draw a graph. This
is an attractive application of graph Laplacians. The main thrust of this paper
is the method of normalized cuts. I give a detailed account for K = 2 clusters,
and also for K &gt; 2 clusters, based on the work of Yu and Shi. Three points that
do not appear to have been clearly articulated before are elaborated:
  1. The solutions of the main optimization problem should be viewed as tuples
in the K-fold cartesian product of projective space RP^{N-1}.
  2. When K &gt; 2, the solutions of the relaxed problem should be viewed as
elements of the Grassmannian G(K,N).
  3. Two possible Riemannian distances are available to compare the closeness
of solutions: (a) The distance on (RP^{N-1})^K. (b) The distance on the
Grassmannian.
  I also clarify what should be the necessary and sufficient conditions for a
matrix to represent a partition of the vertices of a graph to be clustered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2495</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2495</id><created>2013-11-11</created><updated>2015-02-03</updated><authors><author><keyname>Hardt</keyname><forenames>Moritz</forenames></author><author><keyname>Price</keyname><forenames>Eric</forenames></author></authors><title>The Noisy Power Method: A Meta Algorithm with Applications</title><categories>cs.DS cs.LG</categories><comments>NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a new robust convergence analysis of the well-known power method
for computing the dominant singular vectors of a matrix that we call the noisy
power method. Our result characterizes the convergence behavior of the
algorithm when a significant amount noise is introduced after each
matrix-vector multiplication. The noisy power method can be seen as a
meta-algorithm that has recently found a number of important applications in a
broad range of machine learning problems including alternating minimization for
matrix completion, streaming principal component analysis (PCA), and
privacy-preserving spectral analysis. Our general analysis subsumes several
existing ad-hoc convergence bounds and resolves a number of open problems in
multiple applications including streaming PCA and privacy-preserving singular
vector computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2501</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2501</id><created>2013-11-11</created><updated>2014-08-22</updated><authors><author><keyname>Krajicek</keyname><forenames>Jan</forenames></author></authors><title>A reduction of proof complexity to computational complexity for
  $AC^0[p]$ Frege systems</title><categories>math.LO cs.CC</categories><comments>the final version accepted to the Proc.AMS</comments><msc-class>03F20, 68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a general reduction of lengths-of-proofs lower bounds for constant
depth Frege systems in DeMorgan language augmented by a connective counting
modulo a prime $p$ (the so called $AC^0[p]$ Frege systems) to computational
complexity lower bounds for search tasks involving search trees branching upon
values of maps on the vector space of low degree polynomials over the finite
filed with $p$ elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2503</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2503</id><created>2013-11-11</created><authors><author><keyname>Richthofer</keyname><forenames>Stefan</forenames></author><author><keyname>Wiskott</keyname><forenames>Laurenz</forenames></author></authors><title>Predictable Feature Analysis</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Every organism in an environment, whether biological, robotic or virtual,
must be able to predict certain aspects of its environment in order to survive
or perform whatever task is intended. It needs a model that is capable of
estimating the consequences of possible actions, so that planning, control, and
decision-making become feasible. For scientific purposes, such models are
usually created in a problem specific manner using differential equations and
other techniques from control- and system-theory. In contrast to that, we aim
for an unsupervised approach that builds up the desired model in a
self-organized fashion. Inspired by Slow Feature Analysis (SFA), our approach
is to extract sub-signals from the input, that behave as predictable as
possible. These &quot;predictable features&quot; are highly relevant for modeling,
because predictability is a desired property of the needed
consequence-estimating model by definition. In our approach, we measure
predictability with respect to a certain prediction model. We focus here on the
solution of the arising optimization problem and present a tractable algorithm
based on algebraic methods which we call Predictable Feature Analysis (PFA). We
prove that the algorithm finds the globally optimal signal, if this signal can
be predicted with low error. To deal with cases where the optimal signal has a
significant prediction error, we provide a robust, heuristically motivated
variant of the algorithm and verify it empirically. Additionally, we give
formal criteria a prediction-model must meet to be suitable for measuring
predictability in the PFA setting and also provide a suitable default-model
along with a formal proof that it meets these criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2504</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2504</id><created>2013-11-11</created><authors><author><keyname>Alicea</keyname><forenames>Bradly</forenames></author></authors><title>A Semi-automated Peer-review System</title><categories>cs.DL cs.HC cs.SI physics.soc-ph</categories><comments>6 pages, 2 figures, 1 table. Associated code an pseudo-code at:
  https://github.com/balicea/semi-auto-peer-review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A semi-supervised model of peer review is introduced that is intended to
overcome the bias and incompleteness of traditional peer review. Traditional
approaches are reliant on human biases, while consensus decision-making is
constrained by sparse information. Here, the architecture for one potential
improvement (a semi-supervised, human-assisted classifier) to the traditional
approach will be introduced and evaluated. To evaluate the potential advantages
of such a system, hypothetical receiver operating characteristic (ROC) curves
for both approaches will be assessed. This will provide more specific
indications of how automation would be beneficial in the manuscript evaluation
process. In conclusion, the implications for such a system on measurements of
scientific impact and improving the quality of open submission repositories
will be discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2505</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2505</id><created>2013-11-11</created><updated>2016-02-11</updated><authors><author><keyname>La Guardia</keyname><forenames>Giuliano G.</forenames></author></authors><title>On optimal constacyclic codes</title><categories>cs.IT math-ph math.IT math.MP</categories><comments>arXiv admin note: text overlap with arXiv:1310.3265. Accepted for
  publishing in Linear Algebra and its Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the class of constacyclic codes, which is a
natural generalization of the class of cyclic and negacyclic codes. This class
of codes is interesting in the sense that it contains codes with good or even
optimal parameters. In this light, we propose constructions of families of
classical block and convolutional maximum-distance-separable (MDS) constacyclic
codes, as well as families of asymmetric quantum MDS codes derived from
(classical-block) constacyclic codes. These results are mainly derived from the
investigation of suitable properties on cyclotomic cosets of these
corresponding codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2507</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2507</id><created>2013-11-11</created><updated>2014-03-26</updated><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Lo</keyname><forenames>Ernest S.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Robust Beamforming for Secure Communication in Systems with Wireless
  Information and Power Transfer</title><categories>cs.IT math.IT</categories><comments>Accepted for publication, IEEE Trans. Wireless Com., Mar. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a multiuser multiple-input single-output (MISO) downlink
system with simultaneous wireless information and power transfer. In
particular, we focus on secure communication in the presence of passive
eavesdroppers and potential eavesdroppers (idle legitimate receivers). We study
the design of a resource allocation algorithm minimizing the total transmit
power for the case when the legitimate receivers are able to harvest energy
from radio frequency signals. Our design advocates the dual use of both
artificial noise and energy signals in providing secure communication and
facilitating efficient wireless energy transfer. The algorithm design is
formulated as a non-convex optimization problem. The problem formulation takes
into account artificial noise and energy signal generation for protecting the
transmitted information against both considered types of eavesdroppers when
imperfect channel state information (CSI) of the potential eavesdroppers and no
CSI of the passive eavesdroppers are available at the transmitter. In light of
the intractability of the problem, we reformulate the considered problem by
replacing a non-convex probabilistic constraint with a convex deterministic
constraint. Then, a semi-definite programming (SDP) relaxation approach is
adopted to obtain the optimal solution for the reformulated problem.
Furthermore, we propose a suboptimal resource allocation scheme with low
computational complexity for providing communication secrecy and facilitating
efficient energy transfer. Simulation results demonstrate a close-to-optimal
performance achieved by the proposed schemes and significant transmit power
savings by optimization of the artificial noise and energy signal generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2513</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2513</id><created>2013-11-11</created><updated>2014-02-17</updated><authors><author><keyname>Hertli</keyname><forenames>Timon</forenames></author></authors><title>Breaking the PPSZ Barrier for Unique 3-SAT</title><categories>cs.CC cs.DS</categories><comments>13 pages; major revision with simplified algorithm but slightly worse
  constants</comments><msc-class>68R05, 68W20, 68W40</msc-class><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The PPSZ algorithm by Paturi, Pudl\'ak, Saks, and Zane (FOCS 1998) is the
fastest known algorithm for (Promise) Unique k-SAT. We give an improved
algorithm with exponentially faster bounds for Unique 3-SAT.
  For uniquely satisfiable 3-CNF formulas, we do the following case
distinction: We call a clause critical if exactly one literal is satisfied by
the unique satisfying assignment. If a formula has many critical clauses, we
observe that PPSZ by itself is already faster. If there are only few clauses
allover, we use an algorithm by Wahlstr\&quot;om (ESA 2005) that is faster than PPSZ
in this case. Otherwise we have a formula with few critical and many
non-critical clauses. Non-critical clauses have at least two literals
satisfied; we show how to exploit this to improve PPSZ.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2517</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2517</id><created>2013-11-11</created><authors><author><keyname>Ambrosin</keyname><forenames>Moreno</forenames></author><author><keyname>Conti</keyname><forenames>Mauro</forenames></author><author><keyname>Gasti</keyname><forenames>Paolo</forenames></author><author><keyname>Tsudik</keyname><forenames>Gene</forenames></author></authors><title>Covert Ephemeral Communication in Named Data Networking</title><categories>cs.CR cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decade, there has been a growing realization that the current
Internet Protocol is reaching the limits of its senescence. This has prompted
several research efforts that aim to design potential next-generation Internet
architectures. Named Data Networking (NDN), an instantiation of the
content-centric approach to networking, is one such effort. In contrast with
IP, NDN routers maintain a significant amount of user-driven state. In this
paper we investigate how to use this state for covert ephemeral communication
(CEC). CEC allows two or more parties to covertly exchange ephemeral messages,
i.e., messages that become unavailable after a certain amount of time. Our
techniques rely only on network-layer, rather than application-layer, services.
This makes our protocols robust, and communication difficult to uncover. We
show that users can build high-bandwidth CECs exploiting features unique to
NDN: in-network caches, routers' forwarding state and name matching rules. We
assess feasibility and performance of proposed cover channels using a local
setup and the official NDN testbed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2524</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2524</id><created>2013-11-11</created><updated>2014-10-22</updated><authors><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Donahue</keyname><forenames>Jeff</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Rich feature hierarchies for accurate object detection and semantic
  segmentation</title><categories>cs.CV</categories><comments>Extended version of our CVPR 2014 paper; latest update (v5) includes
  results using deeper networks (see Appendix G. Changelog)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object detection performance, as measured on the canonical PASCAL VOC
dataset, has plateaued in the last few years. The best-performing methods are
complex ensemble systems that typically combine multiple low-level image
features with high-level context. In this paper, we propose a simple and
scalable detection algorithm that improves mean average precision (mAP) by more
than 30% relative to the previous best result on VOC 2012---achieving a mAP of
53.3%. Our approach combines two key insights: (1) one can apply high-capacity
convolutional neural networks (CNNs) to bottom-up region proposals in order to
localize and segment objects and (2) when labeled training data is scarce,
supervised pre-training for an auxiliary task, followed by domain-specific
fine-tuning, yields a significant performance boost. Since we combine region
proposals with CNNs, we call our method R-CNN: Regions with CNN features. We
also compare R-CNN to OverFeat, a recently proposed sliding-window detector
based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by
a large margin on the 200-class ILSVRC2013 detection dataset. Source code for
the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2526</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2526</id><created>2013-11-11</created><updated>2014-02-20</updated><authors><author><keyname>Zhao</keyname><forenames>Kang</forenames></author><author><keyname>Wang</keyname><forenames>Xi</forenames></author><author><keyname>Yu</keyname><forenames>Mo</forenames></author><author><keyname>Gao</keyname><forenames>Bo</forenames></author></authors><title>User recommendation in reciprocal and bipartite social networks -- a
  case study of online dating</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>IEEE Intelligent Systems (2014 forthcoming)</comments><doi>10.1109/MIS.2013.104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many social networks in our daily life are bipartite networks built on
reciprocity. How can we recommend users/friends to a user, so that the user is
interested in and attractive to recommended users? In this research, we propose
a new collaborative filtering model to improve user recommendations in
reciprocal and bipartite social networks. The model considers a user's &quot;taste&quot;
in picking others and &quot;attractiveness&quot; in being picked by others. A case study
of an online dating network shows that the new model has good performance in
recommending both initial and reciprocal contacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2531</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2531</id><created>2013-11-11</created><authors><author><keyname>Froese</keyname><forenames>Tom</forenames></author><author><keyname>Virgo</keyname><forenames>Nathaniel</forenames></author><author><keyname>Ikegami</keyname><forenames>Takashi</forenames></author></authors><title>Motility at the origin of life: Its characterization and a model</title><categories>cs.AI cs.NE nlin.AO nlin.PS q-bio.PE</categories><comments>29 pages, 5 figures, Artificial Life</comments><msc-class>35K57</msc-class><acm-class>I.2.0; I.2.9; I.2.11; I.6.0; J.3</acm-class><doi>10.1162/ARTL_a_00096</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to recent advances in synthetic biology and artificial life, the origin
of life is currently a hot topic of research. We review the literature and
argue that the two traditionally competing &quot;replicator-first&quot; and
&quot;metabolism-first&quot; approaches are merging into one integrated theory of
individuation and evolution. We contribute to the maturation of this more
inclusive approach by highlighting some problematic assumptions that still lead
to an impoverished conception of the phenomenon of life. In particular, we
argue that the new consensus has so far failed to consider the relevance of
intermediate timescales. We propose that an adequate theory of life must
account for the fact that all living beings are situated in at least four
distinct timescales, which are typically associated with metabolism, motility,
development, and evolution. On this view, self-movement, adaptive behavior and
morphological changes could have already been present at the origin of life. In
order to illustrate this possibility we analyze a minimal model of life-like
phenomena, namely of precarious, individuated, dissipative structures that can
be found in simple reaction-diffusion systems. Based on our analysis we suggest
that processes in intermediate timescales could have already been operative in
prebiotic systems. They may have facilitated and constrained changes occurring
in the faster- and slower-paced timescales of chemical self-individuation and
evolution by natural selection, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2535</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2535</id><created>2013-11-11</created><authors><author><keyname>Mazzara</keyname><forenames>Manuel</forenames></author><author><keyname>Ciavotta</keyname><forenames>Michele</forenames></author></authors><title>Issues about the Adoption of Formal Methods for Dependable Composition
  of Web Services</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web Services provide interoperable mechanisms for describing, locating and
invoking services over the Internet; composition further enables to build
complex services out of simpler ones for complex B2B applications. While
current studies on these topics are mostly focused - from the technical
viewpoint - on standards and protocols, this paper investigates the adoption of
formal methods, especially for composition. We logically classify and analyze
three different (but interconnected) kinds of important issues towards this
goal, namely foundations, verification and extensions. The aim of this work is
to individuate the proper questions on the adoption of formal methods for
dependable composition of Web Services, not necessarily to find the optimal
answers. Nevertheless, we still try to propose some tentative answers based on
our proposal for a composition calculus, which we hope can animate a proper
discussion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2540</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2540</id><created>2013-11-11</created><updated>2014-01-06</updated><authors><author><keyname>Duda</keyname><forenames>Jarek</forenames></author></authors><title>Asymmetric numeral systems: entropy coding combining speed of Huffman
  coding with compression rate of arithmetic coding</title><categories>cs.IT math.IT</categories><comments>24 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modern data compression is mainly based on two approaches to entropy
coding: Huffman (HC) and arithmetic/range coding (AC). The former is much
faster, but approximates probabilities with powers of 2, usually leading to
relatively low compression rates. The latter uses nearly exact probabilities -
easily approaching theoretical compression rate limit (Shannon entropy), but at
cost of much larger computational cost.
  Asymmetric numeral systems (ANS) is a new approach to accurate entropy
coding, which allows to end this trade-off between speed and rate: the recent
implementation [1] provides about $50\%$ faster decoding than HC for 256 size
alphabet, with compression rate similar to provided by AC. This advantage is
due to being simpler than AC: using single natural number as the state, instead
of two to represent a range. Beside simplifying renormalization, it allows to
put the entire behavior for given probability distribution into a relatively
small table: defining entropy coding automaton. The memory cost of such table
for 256 size alphabet is a few kilobytes. There is a large freedom while
choosing a specific table - using pseudorandom number generator initialized
with cryptographic key for this purpose allows to simultaneously encrypt the
data.
  This article also introduces and discusses many other variants of this new
entropy coding approach, which can provide direct alternatives for standard AC,
for large alphabet range coding, or for approximated quasi arithmetic coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2542</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2542</id><created>2013-11-11</created><updated>2015-08-25</updated><authors><author><keyname>Bourgain</keyname><forenames>Jean</forenames></author><author><keyname>Dirksen</keyname><forenames>Sjoerd</forenames></author><author><keyname>Nelson</keyname><forenames>Jelani</forenames></author></authors><title>Toward a unified theory of sparse dimensionality reduction in Euclidean
  space</title><categories>cs.DS cs.CG cs.IT math.IT math.PR stat.ML</categories><journal-ref>Geometric and Functional Analysis 25 (2015), no. 4, 1009-1088</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\Phi\in\mathbb{R}^{m\times n}$ be a sparse Johnson-Lindenstrauss
transform [KN14] with $s$ non-zeroes per column. For a subset $T$ of the unit
sphere, $\varepsilon\in(0,1/2)$ given, we study settings for $m,s$ required to
ensure $$ \mathop{\mathbb{E}}_\Phi \sup_{x\in T} \left|\|\Phi x\|_2^2 - 1
\right| &lt; \varepsilon , $$ i.e. so that $\Phi$ preserves the norm of every
$x\in T$ simultaneously and multiplicatively up to $1+\varepsilon$. We
introduce a new complexity parameter, which depends on the geometry of $T$, and
show that it suffices to choose $s$ and $m$ such that this parameter is small.
Our result is a sparse analog of Gordon's theorem, which was concerned with a
dense $\Phi$ having i.i.d. Gaussian entries. We qualitatively unify several
results related to the Johnson-Lindenstrauss lemma, subspace embeddings, and
Fourier-based restricted isometries. Our work also implies new results in using
the sparse Johnson-Lindenstrauss transform in numerical linear algebra,
classical and model-based compressed sensing, manifold learning, and
constrained least squares problems such as the Lasso.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2547</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2547</id><created>2013-11-11</created><updated>2014-07-30</updated><authors><author><keyname>Sun</keyname><forenames>Yuekai</forenames></author><author><keyname>Ioannidis</keyname><forenames>Stratis</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>Learning Mixtures of Linear Classifiers</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a discriminative learning (regression) problem, whereby the
regression function is a convex combination of k linear classifiers. Existing
approaches are based on the EM algorithm, or similar techniques, without
provable guarantees. We develop a simple method based on spectral techniques
and a `mirroring' trick, that discovers the subspace spanned by the
classifiers' parameter vectors. Under a probabilistic assumption on the feature
vector distribution, we prove that this approach has nearly optimal statistical
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2549</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2549</id><created>2013-11-11</created><updated>2013-12-08</updated><authors><author><keyname>Tonchev</keyname><forenames>Vladimir D.</forenames></author></authors><title>A doubling construction for self-orthogonal codes</title><categories>cs.IT math.IT</categories><comments>7 pages</comments><msc-class>94B</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple construction of quaternary hermitian self-orthogonal codes with
parameters $[2n+1,k+1]$ and $[2n+2,k+2]$ from a given pair of self-orthogonal
$[n,k]$ codes, and its link to quantum codes is considered. As an application,
an optimal quaternary linear $[28,20,6]$ dual containing code is found that
yields a new optimal $[[28,12,6]]$ quantum code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2551</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2551</id><created>2013-11-11</created><authors><author><keyname>Mazzara</keyname><forenames>Manuel</forenames><affiliation>University of Bologna, Italy</affiliation></author><author><keyname>Biselli</keyname><forenames>Luca</forenames><affiliation>University of Bologna, Italy</affiliation></author><author><keyname>Greco</keyname><forenames>Pier Paolo</forenames><affiliation>University of Bologna, Italy</affiliation></author><author><keyname>Dragoni</keyname><forenames>Nicola</forenames><affiliation>University of Bologna, Italy</affiliation></author><author><keyname>Marraffa</keyname><forenames>Antonio</forenames><affiliation>University of Bologna, Italy</affiliation></author><author><keyname>Qamar</keyname><forenames>Nafees</forenames><affiliation>University of Bologna, Italy</affiliation></author><author><keyname>de Nicola</keyname><forenames>Simona</forenames><affiliation>University of Bologna, Italy</affiliation></author></authors><title>Social Networks and Collective Intelligence: A Return to the Agora</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Preprint without images for copyright reasons</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, acquisition of trustable information is increasingly important in
both professional and private contexts. However, establishing what information
is trustable and what is not, is a very challenging task. For example, how can
information quality be reliably assessed? How can sources? credibility be
fairly assessed? How can gatekeeping processes be found trustworthy when
filtering out news and deciding ranking and priorities of traditional media? An
Internet-based solution to a human-based ancient issue is being studied, and it
is called Polidoxa, from Greek &quot;poly&quot;, meaning &quot;many&quot; or &quot;several&quot; and &quot;doxa&quot;,
meaning &quot;common belief&quot; or &quot;popular opinion&quot;. This old problem will be solved
by means of ancient philosophies and processes with truly modern tools and
technologies. This is why this work required a collaborative and
interdisciplinary joint effort from researchers with very different backgrounds
and institutes with significantly different agendas. Polidoxa aims at offering:
1) a trust-based search engine algorithm, which exploits stigmergic behaviours
of users? network, 2) a trust-based social network, where the notion of trust
derives from network activity and 3) a holonic system for bottom-up
self-protection and social privacy. By presenting the Polidoxa solution, this
work also describes the current state of traditional media as well as newer
ones, providing an accurate analysis of major search engines such as Google and
social network (e.g., Facebook). The advantages that Polidoxa offers, compared
to these, are also clearly detailed and motivated. Finally, a Twitter
application (Polidoxa@twitter) which enables experimentation of basic Polidoxa
principles is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2557</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2557</id><created>2013-11-11</created><updated>2013-11-12</updated><authors><author><keyname>Saha</keyname><forenames>Barna</forenames></author></authors><title>Efficiently Computing Edit Distance to Dyck Language</title><categories>cs.DS cs.FL</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a string $\sigma$ over alphabet $\Sigma$ and a grammar $G$ defined over
the same alphabet, how many minimum number of repairs: insertions, deletions
and substitutions are required to map $\sigma$ into a valid member of $G$ ? We
investigate this basic question in this paper for $Dyck(s)$. $Dyck(s)$ is a
fundamental context free grammar representing the language of well-balanced
parentheses with s different types of parentheses and has played a pivotal role
in the development of theory of context free languages. Computing edit distance
to $Dyck(s)$ significantly generalizes string edit distance problem and has
numerous applications ranging from repairing semi-structured documents such as
XML to memory checking, automated compiler optimization, natural language
processing etc.
  In this paper we give the first near-linear time algorithm for edit distance
computation to $Dyck(s)$ that achieves a nontrivial approximation factor of
$O(\frac{1}{\epsilon}\log{OPT}(\log{n})^{\frac{1}{\epsilon}})$ in
$O(n^{1+\epsilon}\log{n})$ time. In fact, given there exists an algorithm for
computing string edit distance on input of size $n$ in $\alpha(n)$ time with
$\beta(n)$-approximation factor, we can devise an algorithm for edit distance
problem to $Dyck(s)$ running in $\tilde{O}(n^{1+\epsilon}+\alpha(n))$ and
achieving an approximation factor of $O(\frac{1}{\epsilon}\beta(n)\log{OPT})$.
  We show that the framework for efficiently approximating edit distance to
$Dyck(s)$ can be applied to many other languages. We illustrate this by
considering various memory checking languages which comprise of valid
transcripts of stacks, queues, priority queues, double-ended queues etc.
Therefore, any language that can be recognized by these data structures, can
also be repaired efficiently by our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2561</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2561</id><created>2013-11-11</created><updated>2013-11-11</updated><authors><author><keyname>Assirati</keyname><forenames>Lucas</forenames></author><author><keyname>da Silva</keyname><forenames>N&#xfa;bia R.</forenames></author><author><keyname>Berton</keyname><forenames>Lilian</forenames></author><author><keyname>Lopes</keyname><forenames>Alneu de A.</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir M.</forenames></author></authors><title>Performing edge detection by difference of Gaussians using q-Gaussian
  kernels</title><categories>cs.CV physics.comp-ph</categories><comments>5 pages, 5 figures, IC-MSQUARE 2013</comments><doi>10.1088/1742-6596/490/1/012020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In image processing, edge detection is a valuable tool to perform the
extraction of features from an image. This detection reduces the amount of
information to be processed, since the redundant information (considered less
relevant) can be unconsidered. The technique of edge detection consists of
determining the points of a digital image whose intensity changes sharply. This
changes are due to the discontinuities of the orientation on a surface for
example. A well known method of edge detection is the Difference of Gaussians
(DoG). The method consists of subtracting two Gaussians, where a kernel has a
standard deviation smaller than the previous one. The convolution between the
subtraction of kernels and the input image results in the edge detection of
this image. This paper introduces a method of extracting edges using DoG with
kernels based on the q-Gaussian probability distribution, derived from the
q-statistic proposed by Constantino Tsallis. To demonstrate the method's
potential, we compare the introduced method with the traditional DoG using
Gaussians kernels. The results showed that the proposed method can extract
edges with more accurate details.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2563</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2563</id><created>2013-11-11</created><updated>2014-03-18</updated><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Minimum Bisection is fixed parameter tractable</title><categories>cs.DS</categories><comments>A full version of an extended abstract to appear in the proceedings
  of STOC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the classic Minimum Bisection problem we are given as input a graph $G$
and an integer $k$. The task is to determine whether there is a partition of
$V(G)$ into two parts $A$ and $B$ such that $||A|-|B|| \leq 1$ and there are at
most $k$ edges with one endpoint in $A$ and the other in $B$. In this paper we
give an algorithm for Minimum Bisection with running time $O(2^{O(k^{3})}n^3
\log^3 n)$. This is the first fixed parameter tractable algorithm for Minimum
Bisection. At the core of our algorithm lies a new decomposition theorem that
states that every graph $G$ can be decomposed by small separators into parts
where each part is &quot;highly connected&quot; in the following sense: any cut of
bounded size can separate only a limited number of vertices from each part of
the decomposition. Our techniques generalize to the weighted setting, where we
seek for a bisection of minimum weight among solutions that contain at most $k$
edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2571</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2571</id><created>2013-11-11</created><authors><author><keyname>Fawzi</keyname><forenames>Hamza</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo A.</forenames></author></authors><title>Exponential lower bounds on fixed-size psd rank and semidefinite
  extension complexity</title><categories>math.OC cs.CC math.CO</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a lot of interest recently in proving lower bounds on the size
of linear programs needed to represent a given polytope P. In a breakthrough
paper Fiorini et al. [Proceedings of 44th ACM Symposium on Theory of Computing
2012, pages 95-106] showed that any linear programming formulation of
maximum-cut must have exponential size. A natural question to ask is whether
one can prove such strong lower bounds for semidefinite programming
formulations. In this paper we take a step towards this goal and we prove
strong lower bounds for a certain class of SDP formulations, namely SDPs over
the Cartesian product of fixed-size positive semidefinite cones. In practice
this corresponds to semidefinite programs with a block-diagonal structure and
where blocks have constant size d. We show that any such extended formulation
of the cut polytope must have exponential size (when d is fixed). The result of
Fiorini et al. for LP formulations is obtained as a special case when d=1. For
blocks of size d=2 the result rules out any small formulations using
second-order cone programming. Our study of SDP lifts over Cartesian product of
fixed-size positive semidefinite cones is motivated mainly from practical
considerations where it is well known that such SDPs can be solved more
efficiently than general SDPs. The proof of our lower bound relies on new
results about the sparsity pattern of certain matrices with small psd rank,
combined with an induction argument inspired from the recent paper by Kaibel
and Weltge [arXiv:1307.3543] on the LP extension complexity of the correlation
polytope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2578</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2578</id><created>2013-11-11</created><authors><author><keyname>Kesselheim</keyname><forenames>Thomas</forenames></author><author><keyname>Radke</keyname><forenames>Klaus</forenames></author><author><keyname>T&#xf6;nnis</keyname><forenames>Andreas</forenames></author><author><keyname>V&#xf6;cking</keyname><forenames>Berthold</forenames></author></authors><title>Primal Beats Dual on Online Packing LPs in the Random-Order Model</title><categories>cs.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study packing LPs in an online model where the columns are presented to
the algorithm in random order. This natural problem was investigated in various
recent studies motivated, e.g., by online ad allocations and yield management
where rows correspond to resources and columns to requests specifying demands
for resources. Our main contribution is a $1-O(\sqrt{(\log{d})/B})$-competitive
online algorithm, where $d$ denotes the column sparsity, i.e., the maximum
number of resources that occur in a single column, and $B$ denotes the capacity
ratio $B$, i.e., the ratio between the capacity of a resource and the maximum
demand for this resource. In other words, we achieve a $(1 -
\epsilon)$-approximation if the capacity ratio satisfies $B=\Omega((\log
d)/\epsilon^2)$, which is known to be best-possible for any (randomized) online
algorithms.
  Our result improves exponentially on previous work with respect to the
capacity ratio. In contrast to existing results on packing LP problems, our
algorithm does not use dual prices to guide the allocation of resources.
Instead, it simply solves, for each request, a scaled version of the partially
known primal program and randomly rounds the obtained fractional solution to
obtain an integral allocation for this request. We show that this simple
algorithmic technique is not restricted to packing LPs with large capacity
ratio: We prove an upper bound on the competitive ratio of
$\Omega(d^{-1/(B-1)})$, for any $B \ge 2$. In addition, we show that our
approach can be combined with VCG payments and obtain an incentive compatible
$(1-\epsilon)$-competitive mechanism for packing LPs with $B=\Omega((\log
m)/\epsilon^2)$, where $m$ is the number of constraints. Finally, we apply our
technique to the generalized assignment problem for which we obtain the first
online algorithm with competitive ratio $O(1)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2621</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2621</id><created>2013-11-11</created><authors><author><keyname>Nogueira</keyname><forenames>P. A</forenames></author></authors><title>Determining Leishmania Infection Levels by Automatic Analysis of
  Microscopy Images</title><categories>cs.CV</categories><comments>MSc thesis, 105 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of microscopy images is one important tool in many fields of
biomedical research, as it allows the quantification of a multitude of
parameters at the cellular level. However, manual counting of these images is
both tiring and unreliable and ultimately very time-consuming for biomedical
researchers. Not only does this slow down the overall research process, it also
introduces counting errors due to a lack of objectivity and consistency
inherent to the researchers own human nature.
  This thesis addresses this issue by automatically determining infection
indexes of macrophages parasite by Leishmania in microscopy images using
computer vision and pattern recognition methodologies. Initially images are
submitted to a pre-processing stage that consists in a normalization of
illumination conditions. Three algorithms are then applied in parallel to each
image. Algorithm A intends to detect macrophage nuclei and consists of
segmentation via adaptive multi-threshold, and classification of resulting
regions using a set of collected features. Algorithm B intends to detect
parasites and is similar to Algorithm A but the adaptive multi-threshold is
parameterized with a different constraints vector. Algorithm C intends to
detect the macrophages and parasites cytoplasm and consists of a cut-off
version of the previous two algorithms, where the classification step is
skipped. Regions with multiple nuclei or parasites are processed by a voting
system that employs both a Support Vector Machine and a set of region features
for determining the number of objects present in each region. The previous vote
is then taken into account as the number of mixtures to be used in a Gaussian
Mixture Model to decluster the said region. Finally each parasite is assigned
to, at most, a single macrophage using minimum Euclidean distance to a cell
nucleus, thus quantifying Leishmania infection levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2625</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2625</id><created>2013-11-11</created><updated>2015-12-10</updated><authors><author><keyname>Rogers</keyname><forenames>Ryan</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author></authors><title>Asymptotically Truthful Equilibrium Selection in Large Congestion Games</title><categories>cs.GT cs.CR cs.DS</categories><comments>The conference version of this paper appeared in EC 2014. This
  manuscript has been merged and subsumed by the preprint &quot;Robust Mediators in
  Large Games&quot;: http://arxiv.org/abs/1512.02698</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studying games in the complete information model makes them analytically
tractable. However, large $n$ player interactions are more realistically
modeled as games of incomplete information, where players may know little to
nothing about the types of other players. Unfortunately, games in incomplete
information settings lose many of the nice properties of complete information
games: the quality of equilibria can become worse, the equilibria lose their
ex-post properties, and coordinating on an equilibrium becomes even more
difficult. Because of these problems, we would like to study games of
incomplete information, but still implement equilibria of the complete
information game induced by the (unknown) realized player types.
  This problem was recently studied by Kearns et al. and solved in large games
by means of introducing a weak mediator: their mediator took as input reported
types of players, and output suggested actions which formed a correlated
equilibrium of the underlying game. Players had the option to play
independently of the mediator, or ignore its suggestions, but crucially, if
they decided to opt-in to the mediator, they did not have the power to lie
about their type. In this paper, we rectify this deficiency in the setting of
large congestion games. We give, in a sense, the weakest possible mediator: it
cannot enforce participation, verify types, or enforce its suggestions.
Moreover, our mediator implements a Nash equilibrium of the complete
information game. We show that it is an (asymptotic) ex-post equilibrium of the
incomplete information game for all players to use the mediator honestly, and
that when they do so, they end up playing an approximate Nash equilibrium of
the induced complete information game. In particular, truthful use of the
mediator is a Bayes-Nash equilibrium in any Bayesian game for any prior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2626</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2626</id><created>2013-11-11</created><updated>2014-04-13</updated><authors><author><keyname>Balzer</keyname><forenames>J.</forenames></author><author><keyname>Soatto</keyname><forenames>S.</forenames></author></authors><title>Second-order Shape Optimization for Geometric Inverse Problems in Vision</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a method for optimization in shape spaces, i.e., sets of surfaces
modulo re-parametrization. Unlike previously proposed gradient flows, we
achieve superlinear convergence rates through a subtle approximation of the
shape Hessian, which is generally hard to compute and suffers from a series of
degeneracies. Our analysis highlights the role of mean curvature motion in
comparison with first-order schemes: instead of surface area, our approach
penalizes deformation, either by its Dirichlet energy or total variation.
Latter regularizer sparks the development of an alternating direction method of
multipliers on triangular meshes. Therein, a conjugate-gradients solver enables
us to bypass formation of the Gaussian normal equations appearing in the course
of the overall optimization. We combine all of the aforementioned ideas in a
versatile geometric variation-regularized Levenberg-Marquardt-type method
applicable to a variety of shape functionals, depending on intrinsic properties
of the surface such as normal field and curvature as well as its embedding into
space. Promising experimental results are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2630</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2630</id><created>2013-11-11</created><authors><author><keyname>Almajadub</keyname><forenames>Fatma</forenames></author><author><keyname>Abdelfattah</keyname><forenames>Eman</forenames></author><author><keyname>Razaque</keyname><forenames>Abdul</forenames></author></authors><title>Deployment of Stream Control Transmission Protocol (SCTP) to Maintain
  the Applications of Data Centers</title><categories>cs.NI</categories><comments>08 Page, 10 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With developments of real-time applications into data centers, the need for
alternatives of the standard TCP protocol has been prime demand in several
applications of data centers. The several alternatives of TCP protocol has been
proposed but SCTP has edge due to its several well-built characteristics that
make it capable to work efficiently. In this paper, we examine the features of
SCTP into data centers like Multi-streaming and Multi-homing over the features
of TCP protocol. In this paper, our objective is to introduce internal problems
of data centers. Robust transport protocol reduces the problems with some
extend. Focusing the problems of data centers, we also examine weakness of
highly deployed standard TCP, and evaluate the performance of SCTP in context
of faster communication for data centers. We also discover some weaknesses and
shortcomings of SCTP into data centers and try to propose some ways to avoid
them by maintaining SCTP native features. To validate strength and weakness of
TCP and SCTP, we use ns2 for simulation in context of data center. On basis of
findings, we highlight major strength of SCTP. At the end, we Implemen
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2634</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2634</id><created>2013-11-11</created><updated>2014-05-01</updated><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>A New Look at Dual-Hop Relaying: Performance Limits with Hardware
  Impairments</title><categories>cs.IT math.IT</categories><comments>Published in IEEE Transactions on Communications, 14 pages, 8
  figures. The results can be reproduced using the following Matlab code:
  https://github.com/emilbjornson/new-look-at-relaying</comments><journal-ref>IEEE Transactions on Communications, vol. 61, no. 11, pp.
  4512-4525, November 2013</journal-ref><doi>10.1109/TCOMM.2013.100913.130282</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical transceivers have hardware impairments that create distortions which
degrade the performance of communication systems. The vast majority of
technical contributions in the area of relaying neglect hardware impairments
and, thus, assumes ideal hardware. Such approximations make sense in low-rate
systems, but can lead to very misleading results when analyzing future
high-rate systems. This paper quantifies the impact of hardware impairments on
dual-hop relaying, for both amplify-and-forward and decode-and-forward
protocols. The outage probability (OP) in these practical scenarios is a
function of the effective end-to-end signal-to-noise-and-distortion ratio
(SNDR). This paper derives new closed-form expressions for the exact and
asymptotic OPs, accounting for hardware impairments at the source, relay, and
destination. A similar analysis for the ergodic capacity is also pursued,
resulting in new upper bounds. We assume that both hops are subject to
independent but non-identically distributed Nakagami-m fading. This paper
validates that the performance loss is small at low rates, but otherwise can be
very substantial. In particular, it is proved that for high signal-to-noise
ratio (SNR), the end-to-end SNDR converges to a deterministic constant, coined
the SNDR ceiling, which is inversely proportional to the level of impairments.
This stands in contrast to the ideal hardware case in which the end-to-end SNDR
grows without bound in the high-SNR regime. Finally, we provide fundamental
design guidelines for selecting hardware that satisfies the requirements of a
practical relaying system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2637</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2637</id><created>2013-11-11</created><authors><author><keyname>Armario</keyname><forenames>Jos&#xe9; Andr&#xe9;s</forenames></author><author><keyname>Frau</keyname><forenames>Mar&#xed;a Dolores</forenames></author></authors><title>Self-Dual codes from $(-1,1)$-matrices of skew symmetric type</title><categories>cs.IT math.CO math.IT</categories><msc-class>05B20 \and 94B25 \and 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previously, self-dual codes have been constructed from weighing matrices, and
in particular from conference matrices (skew and symmetric). In this paper,
codes constructed from matrices of skew symmetric type whose determinants reach
the Ehlich-Wojtas' bound are presented. A necessary and sufficient condition
for these codes to be self-dual is given, and examples are provided for lengths
up to 52.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2642</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2642</id><created>2013-11-11</created><authors><author><keyname>Balzer</keyname><forenames>J.</forenames></author><author><keyname>Peters</keyname><forenames>M.</forenames></author><author><keyname>Soatto</keyname><forenames>S.</forenames></author></authors><title>Volumetric Reconstruction Applied to Perceptual Studies of Size and
  Weight</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the application of volumetric reconstruction from structured-light
sensors in cognitive neuroscience, specifically in the quantification of the
size-weight illusion, whereby humans tend to systematically perceive smaller
objects as heavier. We investigate the performance of two commercial
structured-light scanning systems in comparison to one we developed
specifically for this application. Our method has two main distinct features:
First, it only samples a sparse series of viewpoints, unlike other systems such
as the Kinect Fusion. Second, instead of building a distance field for the
purpose of points-to-surface conversion directly, we pursue a first-order
approach: the distance function is recovered from its gradient by a screened
Poisson reconstruction, which is very resilient to noise and yet preserves
high-frequency signal components. Our experiments show that the quality of
metric reconstruction from structured light sensors is subject to systematic
biases, and highlights the factors that influence it. Our main performance
index rates estimates of volume (a proxy of size), for which we review a
well-known formula applicable to incomplete meshes. Our code and data will be
made publicly available upon completion of the anonymous review process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2650</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2650</id><created>2013-11-11</created><authors><author><keyname>Yang</keyname><forenames>Chunliang</forenames></author></authors><title>Over-the-air Signaling in Cellular Networks: An Overview</title><categories>cs.IT cs.NI math.IT</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To improve the capacity and coverage of current cellular networks, many
advanced technologies such as massive MIMO, inter-cell coordination, small
cells, device-to-device communications, and so on, are under studying. Many
proposed techniques have been shown to offer significant performance
improvement. Thus, the enabler of those techniques is of great importance. That
is the necessary signaling which guarantee the operation of those techniques.
The design and transmission of those signaling, especially the over-the-air
(OTA) signaling, is challenging. In this article, we provide an overview of the
OTA signaling in cellular networks to provide insights on the design of OTA
signaling. Specifically, we first give a brief introduction of the OTA
signaling in long term evolution (LTE), and then we discuss the challenges and
requirements in designing the OTA signaling in cellular networks in detail. To
better understand the OTA signaling, we give two important classifications of
OTA signaling and address their properties and applications. Finally, we
propose a signature-based signaling named (single-tone signaling, STS) which
can be used for inter-cell OTA signaling and is especially useful and robust in
multi-signal scenario. Simulation results are given to compare the detection
performance of different OTA signaling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2651</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2651</id><created>2013-11-11</created><authors><author><keyname>He</keyname><forenames>Xiang</forenames></author><author><keyname>Khisti</keyname><forenames>Ashish</forenames></author><author><keyname>Yener</keyname><forenames>Aylin</forenames></author></authors><title>MIMO Broadcast Channel with an Unknown Eavesdropper: Secrecy Degrees of
  Freedom</title><categories>cs.IT math.IT</categories><comments>Accepted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a multi-antenna broadcast channel with two legitimate receivers and
an external eavesdropper. We assume that the channel matrix of the eavesdropper
is unknown to the legitimate terminals but satisfies a maximum rank constraint.
As our main result we characterize the associated secrecy degrees of freedom
for the broadcast channel with common and private messages. We show that a
direct extension of the single-user wiretap codebook does not achieve the
secrecy degrees of freedom. Our proposed optimal scheme involves decomposing
the signal space into a common subspace, which can be observed by both
receivers, and private subspaces which can be observed by only one of the
receivers, and carefully transmitting a subset of messages in each subspace. We
also consider the case when each user's private message must additionally
remain confidential from the other legitimate receiver and characterize the
s.d.o.f.\ region in this case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2655</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2655</id><created>2013-11-11</created><updated>2014-09-26</updated><authors><author><keyname>Barman</keyname><forenames>Siddharth</forenames></author><author><keyname>Bhaskar</keyname><forenames>Umang</forenames></author><author><keyname>Echenique</keyname><forenames>Federico</forenames></author><author><keyname>Wierman</keyname><forenames>Adam</forenames></author></authors><title>On the Existence of Low-Rank Explanations for Mixed Strategy Behavior</title><categories>cs.GT</categories><comments>Updated writeup. 19 pages</comments><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nash equilibrium is used as a model to explain the observed behavior of
players in strategic settings. For example, in many empirical applications we
observe player behavior, and the problem is to determine if there exist payoffs
for the players for which the equilibrium corresponds to observed player
behavior. Computational complexity of Nash equilibria is an important
consideration in this framework. If the instance of the model that explains
observed player behavior requires players to have solved a computationally hard
problem, then the explanation provided is questionable. In this paper we
provide conditions under which Nash equilibrium is a reasonable explanation for
strategic behavior, i.e., conditions under which observed behavior of players
can be explained by games in which Nash equilibria are easy to compute. We
identify three structural conditions and show that if the data set of observed
behavior satisfies any of these conditions, then it is consistent with payoff
matrices for which the observed Nash equilibria could have been computed
efficiently. Our conditions admit large and structurally complex data sets of
observed behavior, showing that even with complexity considerations, Nash
equilibrium is often a reasonable model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2661</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2661</id><created>2013-11-11</created><updated>2013-11-16</updated><authors><author><keyname>Sridhar</keyname><forenames>Srikrishna</forenames></author><author><keyname>Bittorf</keyname><forenames>Victor</forenames></author><author><keyname>Liu</keyname><forenames>Ji</forenames></author><author><keyname>Zhang</keyname><forenames>Ce</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author><author><keyname>Wright</keyname><forenames>Stephen J.</forenames></author></authors><title>An Approximate, Efficient Solver for LP Rounding</title><categories>cs.NA</categories><comments>Clarified that this manuscript is a full version of an article that
  is to appear in NIPS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many problems in machine learning can be solved by rounding the solution of
an appropriate linear program (LP). This paper shows that we can recover
solutions of comparable quality by rounding an approximate LP solution instead
of the ex- act one. These approximate LP solutions can be computed efficiently
by applying a parallel stochastic-coordinate-descent method to a
quadratic-penalty formulation of the LP. We derive worst-case runtime and
solution quality guarantees of this scheme using novel perturbation and
convergence analysis. Our experiments demonstrate that on such combinatorial
problems as vertex cover, independent set and multiway-cut, our approximate
rounding scheme is up to an order of magnitude faster than Cplex (a commercial
LP solver) while producing solutions of similar quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2663</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2663</id><created>2013-11-11</created><updated>2014-02-01</updated><authors><author><keyname>Zhe</keyname><forenames>Shandian</forenames></author><author><keyname>Qi</keyname><forenames>Yuan</forenames></author><author><keyname>Park</keyname><forenames>Youngja</forenames></author><author><keyname>Molloy</keyname><forenames>Ian</forenames></author><author><keyname>Chari</keyname><forenames>Suresh</forenames></author></authors><title>DinTucker: Scaling up Gaussian process models on multidimensional arrays
  with billions of elements</title><categories>cs.LG cs.DC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Infinite Tucker Decomposition (InfTucker) and random function prior models,
as nonparametric Bayesian models on infinite exchangeable arrays, are more
powerful models than widely-used multilinear factorization methods including
Tucker and PARAFAC decomposition, (partly) due to their capability of modeling
nonlinear relationships between array elements. Despite their great predictive
performance and sound theoretical foundations, they cannot handle massive data
due to a prohibitively high training time. To overcome this limitation, we
present Distributed Infinite Tucker (DINTUCKER), a large-scale nonlinear tensor
decomposition algorithm on MAPREDUCE. While maintaining the predictive accuracy
of InfTucker, it is scalable on massive data. DINTUCKER is based on a new
hierarchical Bayesian model that enables local training of InfTucker on
subarrays and information integration from all local training results. We use
distributed stochastic gradient descent, coupled with variational inference, to
train this model. We apply DINTUCKER to multidimensional arrays with billions
of elements from applications in the &quot;Read the Web&quot; project (Carlson et al.,
2010) and in information security and compare it with the state-of-the-art
large-scale tensor decomposition method, GigaTensor. On both datasets,
DINTUCKER achieves significantly higher prediction accuracy with less
computational time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2667</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2667</id><created>2013-11-11</created><updated>2013-12-30</updated><authors><author><keyname>Freedman</keyname><forenames>Michael</forenames></author><author><keyname>Krushkal</keyname><forenames>Vyacheslav</forenames></author></authors><title>Geometric complexity of embeddings in ${\mathbb R}^d$</title><categories>math.MG cs.CG math.CO math.GT</categories><comments>v2: an upper bound is established on refinement complexity for
  simplicial n-complexes in R^{2n}. Exponential lower bound is extended to a
  wider range of dimensions. The title is revised to reflect the changes in the
  paper</comments><journal-ref>Geom. Funct. Anal. 24 (2014), no. 5, 1406-1430</journal-ref><doi>10.1007/s00039-014-0272-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a simplicial complex $K$, we consider several notions of geometric
complexity of embeddings of $K$ in a Euclidean space ${\mathbb R}^d$:
thickness, distortion, and refinement complexity (the minimal number of
simplices needed for a PL embedding). We show that any $n$-complex with $N$
simplices which topologically embeds in ${\mathbb R}^{2n}$, $n&gt;2$, can be PL
embedded in ${\mathbb R}^{2n}$ with refinement complexity
$O(e^{N^{4+{\epsilon}}})$. Families of simplicial $n$-complexes $K$ are
constructed such that any embedding of $K$ into ${\mathbb R}^{2n}$ has an
exponential lower bound on thickness and refinement complexity as a function of
the number of simplices of $K$. This contrasts embeddings in the stable range,
$K\subset {\mathbb R}^{2n+k}$, $k&gt;0$, where all known bounds on geometric
complexity functions are polynomial. In addition, we give a geometric argument
for a bound on distortion of expander graphs in Euclidean spaces. Several
related open problems are discussed, including questions about the growth rate
of complexity functions of embeddings, and about the crossing number and the
ropelength of classical links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2669</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2669</id><created>2013-11-11</created><updated>2013-12-30</updated><authors><author><keyname>Duchi</keyname><forenames>John C.</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Distance-based and continuum Fano inequalities with applications to
  statistical estimation</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>16 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this technical note, we give two extensions of the classical Fano
inequality in information theory. The first extends Fano's inequality to the
setting of estimation, providing lower bounds on the probability that an
estimator of a discrete quantity is within some distance $t$ of the quantity.
The second inequality extends our bound to a continuum setting and provides a
volume-based bound. We illustrate how these inequalities lead to direct and
simple proofs of several statistical minimax lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2670</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2670</id><created>2013-11-11</created><updated>2014-01-20</updated><authors><author><keyname>Zhang</keyname><forenames>Yutao</forenames></author><author><keyname>Tang</keyname><forenames>Jie</forenames></author></authors><title>Social Network Integration: Towards Constructing the Social Graph</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we formulate the problem of social network integration. It
takes multiple observed social networks as input and returns an integrated
global social graph where each node corresponds to a real person. The key
challenge for social network integration is to discover the correspondences or
interlinks across different social networks.
  We engaged an in-depth analysis across three online social networks, AMiner,
Linkedin, and Videolectures in order to address what reveals users' social
identity, whether the social factors consistent across different social
networks and how we can leverage these information to perform integration.
  We proposed a unified framework for the social network integration task. It
crawls data from multiple social networks and further discovers accounts
correspond to the same real person from the obtained networks. We use a
probabilistic model to determine such correspondence, it incorporates features
like the consistency of social status and social ties across different, as well
as one-to-one mapping constraint and logical transitivity to jointly make the
prediction. Empirical experiments verify the effectiveness of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2677</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2677</id><created>2013-11-12</created><authors><author><keyname>Singh</keyname><forenames>Raman</forenames></author><author><keyname>Kumar</keyname><forenames>Harish</forenames></author><author><keyname>Singla</keyname><forenames>R. K.</forenames></author></authors><title>Sampling Based Approaches to Handle Imbalances in Network Traffic
  Dataset for Machine Learning Techniques</title><categories>cs.NI cs.CR cs.LG</categories><comments>12 pages</comments><doi>10.5121/csit.2013.3704</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network traffic data is huge, varying and imbalanced because various classes
are not equally distributed. Machine learning (ML) algorithms for traffic
analysis uses the samples from this data to recommend the actions to be taken
by the network administrators as well as training. Due to imbalances in
dataset, it is difficult to train machine learning algorithms for traffic
analysis and these may give biased or false results leading to serious
degradation in performance of these algorithms. Various techniques can be
applied during sampling to minimize the effect of imbalanced instances. In this
paper various sampling techniques have been analysed in order to compare the
decrease in variation in imbalances of network traffic datasets sampled for
these algorithms. Various parameters like missing classes in samples,
probability of sampling of the different instances have been considered for
comparison.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2694</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2694</id><created>2013-11-12</created><updated>2013-11-20</updated><authors><author><keyname>Bickel</keyname><forenames>Peter J.</forenames></author><author><keyname>Sarkar</keyname><forenames>Purnamrita</forenames></author></authors><title>Hypothesis Testing for Automated Community Detection in Networks</title><categories>stat.ML cs.LG cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection in networks is a key exploratory tool with applications
in a diverse set of areas, ranging from finding communities in social and
biological networks to identifying link farms in the World Wide Web. The
problem of finding communities or clusters in a network has received much
attention from statistics, physics and computer science. However, most
clustering algorithms assume knowledge of the number of clusters k. In this
paper we propose to automatically determine k in a graph generated from a
Stochastic Blockmodel. Our main contribution is twofold; first, we
theoretically establish the limiting distribution of the principal eigenvalue
of the suitably centered and scaled adjacency matrix, and use that distribution
for our hypothesis test. Secondly, we use this test to design a recursive
bipartitioning algorithm. Using quantifiable classification tasks on real world
networks with ground truth, we show that our algorithm outperforms existing
probabilistic models for learning overlapping clusters, and on unlabeled
networks, we show that we uncover nested community structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2698</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2698</id><created>2013-11-12</created><updated>2014-04-28</updated><authors><author><keyname>Crismani</keyname><forenames>Alessandro</forenames></author><author><keyname>Schilcher</keyname><forenames>Udo</forenames></author><author><keyname>Toumpis</keyname><forenames>Stavros</forenames></author><author><keyname>Brandner</keyname><forenames>G&#xfc;nther</forenames></author><author><keyname>Bettstetter</keyname><forenames>Christian</forenames></author></authors><title>Packet Travel Times in Wireless Relay Chains under Spatially and
  Temporally Dependent Interference</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the statistics of the number of time slots $T$ that it takes a
packet to travel through a chain of wireless relays. Derivations are performed
assuming an interference model for which interference possesses spatiotemporal
dependency properties. When using this model, results are harder to arrive at
analytically, but they are more realistic than the ones obtained in many
related works that are based on independent interference models.
  First, we present a method for calculating the distribution of $T$. As the
required computations are extensive, we also obtain simple expressions for the
expected value $\mathrm{E} [T]$ and variance $\mathrm{var} [T]$. Finally, we
calculate the asymptotic limit of the average speed of the packet. Our
numerical results show that spatiotemporal dependence has a significant impact
on the statistics of the travel time $T$. In particular, we show that, with
respect to the independent interference case, $\mathrm{E} [T]$ and
$\mathrm{var} [T]$ increase, whereas the packet speed decreases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2702</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2702</id><created>2013-11-12</created><authors><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author><author><keyname>Bergel</keyname><forenames>Alexandre</forenames></author></authors><title>Verifiable Source Code Documentation in Controlled Natural Language</title><categories>cs.SE cs.AI cs.CL cs.HC cs.LO</categories><acm-class>H.5.2; D.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Writing documentation about software internals is rarely considered a
rewarding activity. It is highly time-consuming and the resulting documentation
is fragile when the software is continuously evolving in a multi-developer
setting. Unfortunately, traditional programming environments poorly support the
writing and maintenance of documentation. Consequences are severe as the lack
of documentation on software structure negatively impacts the overall quality
of the software product. We show that using a controlled natural language with
a reasoner and a query engine is a viable technique for verifying the
consistency and accuracy of documentation and source code. Using ACE, a
state-of-the-art controlled natural language, we present positive results on
the comprehensibility and the general feasibility of creating and verifying
documentation. As a case study, we used automatic documentation verification to
identify and fix severe flaws in the architecture of a non-trivial piece of
software. Moreover, a user experiment shows that our language is faster and
easier to learn and understand than other formal languages for software
documentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2705</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2705</id><created>2013-11-12</created><authors><author><keyname>Jin</keyname><forenames>Lingfei</forenames></author></authors><title>Quantum Stabilizer Codes from Maximal Curves</title><categories>cs.IT math.AG math.IT</categories><doi>10.1109/TIT.2013.2287694</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A curve attaining the Hasse-Weil bound is called a maximal curve. Usually
classical error-correcting codes obtained from a maximal curve have good
parameters. However, the quantum stabilizer codes obtained from such classical
error-correcting codes via Euclidean or Hermitian self-orthogonality do not
always possess good parameters. In this paper, the Hermitian self-orthogonality
of algebraic geometry codes obtained from two maximal curves is investigated.
It turns out that the stabilizer quantum codes produced from such Hermitian
self-orthogonal classical codes have good parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2719</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2719</id><created>2013-11-12</created><authors><author><keyname>Majumdar</keyname><forenames>Pushan</forenames></author></authors><title>Lattice Simulations using OpenACC compilers</title><categories>hep-lat cs.MS physics.comp-ph</categories><comments>7 pages, 1 figure, presented at the 31st International Symposium on
  Lattice Field Theory (Lattice 2013), 29 July - 3 August 2013, Mainz, Germany</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  OpenACC compilers allow one to use Graphics Processing Units without having
to write explicit CUDA codes. Programs can be modified incrementally using
OpenMP like directives which causes the compiler to generate CUDA kernels to be
run on the GPUs. In this article we look at the performance gain in lattice
simulations with dynamical fermions using OpenACC compilers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2745</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2745</id><created>2013-11-12</created><updated>2015-07-01</updated><authors><author><keyname>Jaganathan</keyname><forenames>Kishore</forenames></author><author><keyname>Oymak</keyname><forenames>Samet</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Sparse Phase Retrieval: Uniqueness Guarantees and Recovery Algorithms</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of signal recovery from its Fourier transform magnitude is of
paramount importance in various fields of engineering and has been around for
over 100 years. Due to the absence of phase information, some form of
additional information is required in order to be able to uniquely identify the
signal of interest. In this work, we focus our attention on discrete-time
sparse signals (of length $n$). We first show that, if the DFT dimension is
greater than or equal to $2n$, almost all signals with {\em aperiodic} support
can be uniquely identified by their Fourier transform magnitude (up to
time-shift, conjugate-flip and global phase).
  Then, we develop an efficient Two-stage Sparse Phase Retrieval algorithm
(TSPR), which involves: (i) identifying the support, i.e., the locations of the
non-zero components, of the signal using a combinatorial algorithm (ii)
identifying the signal values in the support using a convex algorithm. We show
that TSPR can {\em provably} recover most $O(n^{1/2-\eps})$-sparse signals (up
to a time-shift, conjugate-flip and global phase). We also show that, for most
$O(n^{1/4-\eps})$-sparse signals, the recovery is {\em robust} in the presence
of measurement noise. Numerical experiments complement our theoretical analysis
and verify the effectiveness of TSPR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2746</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2746</id><created>2013-11-12</created><authors><author><keyname>Grais</keyname><forenames>Emad M.</forenames></author><author><keyname>Sen</keyname><forenames>Mehmet Umut</forenames></author><author><keyname>Erdogan</keyname><forenames>Hakan</forenames></author></authors><title>Deep neural networks for single channel source separation</title><categories>cs.NE cs.LG</categories><comments>5 pages, 2 figures, 2 tables, submitted to ICASSP2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel approach for single channel source separation (SCSS)
using a deep neural network (DNN) architecture is introduced. Unlike previous
studies in which DNN and other classifiers were used for classifying
time-frequency bins to obtain hard masks for each source, we use the DNN to
classify estimated source spectra to check for their validity during
separation. In the training stage, the training data for the source signals are
used to train a DNN. In the separation stage, the trained DNN is utilized to
aid in estimation of each source in the mixed signal. Single channel source
separation problem is formulated as an energy minimization problem where each
source spectra estimate is encouraged to fit the trained DNN model and the
mixed signal spectrum is encouraged to be written as a weighted sum of the
estimated source spectra. The proposed approach works regardless of the energy
scale differences between the source signals in the training and separation
stages. Nonnegative matrix factorization (NMF) is used to initialize the DNN
estimate for each source. The experimental results show that using DNN
initialized by NMF for source separation improves the quality of the separated
signal compared with using NMF for source separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2749</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2749</id><created>2013-11-12</created><updated>2014-09-22</updated><authors><author><keyname>Dvorak</keyname><forenames>Zdenek</forenames></author><author><keyname>Mnich</keyname><forenames>Matthias</forenames></author></authors><title>Large Independent Sets in Triangle-Free Planar Graphs</title><categories>cs.DM math.CO</categories><comments>14 pages, 1 figure</comments><msc-class>68R10</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every triangle-free planar graph on n vertices has an independent set of size
at least (n+1)/3, and this lower bound is tight. We give an algorithm that,
given a triangle-free planar graph G on n vertices and an integer k&gt;=0, decides
whether G has an independent set of size at least (n+k)/3, in time
2^{O(sqrt{k})}n. Thus, the problem is fixed-parameter tractable when
parameterized by k. Furthermore, as a corollary of the result used to prove the
correctness of the algorithm, we show that there exists epsilon&gt;0 such that
every planar graph of girth at least five on n vertices has an independent set
of size at least n/(3-epsilon).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2780</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2780</id><created>2013-11-12</created><authors><author><keyname>Vabishchevich</keyname><forenames>Petr N.</forenames></author></authors><title>A priori estimation of a time step for numerically solving parabolic
  problems</title><categories>cs.NA math.NA</categories><comments>13 pages, 7 figures</comments><msc-class>65J08, 65M06, 65M12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work deals with the problem of choosing a time step for the numerical
solution of boundary value problems for parabolic equations. The problem
solution is derived using the fully implicit scheme, whereas a time step is
selected via explicit calculations. The selection strategy consists of the
following steps. First, using the explicit scheme, we calculate the solution at
a new time level. Next, we employ this solution in order to obtain the solution
at the previous time level (the implicit scheme, explicit calculations). This
solution should be close to the solution of our problem at this time level with
a prescribed accuracy. Such an algorithm leads to explicit formulas for the
calculation of the time step and takes into account both the dynamics of the
problem solution and changes in coefficients of the equation and in its
right-hand side. The same formulas for the evaluation of the time step we get
using a comparison of two approximate solutions, which are obtained using the
explicit scheme with the primary time step and the step that is reduced by
half. Numerical results are presented for a model parabolic boundary value
problem, which demonstrate the robustness of the developed algorithm for the
time step selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2789</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2789</id><created>2013-11-12</created><updated>2014-09-19</updated><authors><author><keyname>Hettne</keyname><forenames>Kristina M.</forenames></author><author><keyname>Dharuri</keyname><forenames>Harish</forenames></author><author><keyname>Zhao</keyname><forenames>Jun</forenames></author><author><keyname>Wolstencroft</keyname><forenames>Katherine</forenames></author><author><keyname>Belhajjame</keyname><forenames>Khalid</forenames></author><author><keyname>Soiland-Reyes</keyname><forenames>Stian</forenames></author><author><keyname>Mina</keyname><forenames>Eleni</forenames></author><author><keyname>Thompson</keyname><forenames>Mark</forenames></author><author><keyname>Cruickshank</keyname><forenames>Don</forenames></author><author><keyname>Verdes-Montenegro</keyname><forenames>Lourdes</forenames></author><author><keyname>Garrido</keyname><forenames>Julian</forenames></author><author><keyname>de Roure</keyname><forenames>David</forenames></author><author><keyname>Corcho</keyname><forenames>Oscar</forenames></author><author><keyname>Klyne</keyname><forenames>Graham</forenames></author><author><keyname>van Schouwen</keyname><forenames>Reinout</forenames></author><author><keyname>Hoen</keyname><forenames>Peter A. C. 't</forenames></author><author><keyname>Bechhofer</keyname><forenames>Sean</forenames></author><author><keyname>Goble</keyname><forenames>Carole</forenames></author><author><keyname>Roos</keyname><forenames>Marco</forenames></author></authors><title>Structuring research methods and data with the Research Object model:
  genomics workflows as a case study</title><categories>q-bio.GN cs.DL</categories><comments>35 pages, 10 figures, 1 table. Submitted to Journal of Biomedical
  Semantics on 2013-05-13, resubmitted after reviews 2013-11-09, 2014-06-27.
  Accepted in principle 2014-07-29. Published: 2014-09-18
  http://www.jbiomedsem.com/content/5/1/41. Research Object homepage:
  http://www.researchobject.org/</comments><report-no>uk-ac-man-scw:212837</report-no><acm-class>J.3; I.7.4; H.3.7</acm-class><doi>10.1186/2041-1480-5-41</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  One of the main challenges for biomedical research lies in the
computer-assisted integrative study of large and increasingly complex
combinations of data in order to understand molecular mechanisms. The
preservation of the materials and methods of such computational experiments
with clear annotations is essential for understanding an experiment, and this
is increasingly recognized in the bioinformatics community. Our assumption is
that offering means of digital, structured aggregation and annotation of the
objects of an experiment will provide necessary meta-data for a scientist to
understand and recreate the results of an experiment. To support this we
explored a model for the semantic description of a workflow-centric Research
Object (RO), where an RO is defined as a resource that aggregates other
resources, e.g., datasets, software, spreadsheets, text, etc. We applied this
model to a case study where we analysed human metabolite variation by
workflows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2795</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2795</id><created>2013-11-12</created><updated>2014-04-15</updated><authors><author><keyname>Krivulin</keyname><forenames>Nikolai</forenames></author></authors><title>Complete solution of a constrained tropical optimization problem with
  application to location analysis</title><categories>math.OC cs.SY</categories><comments>20 pages, 3 figures</comments><msc-class>65K10 (Primary), 15A80, 65K05, 90C48, 90B85 (Secondary)</msc-class><journal-ref>Relational and Algebraic Methods in Computer Science, P. Hoefner,
  P. Jipsen, W. Kahl, M. E. Mueller, eds., vol. 8428 of Lecture Notes in
  Computer Science, pp. 362-378, Springer, 2014</journal-ref><doi>10.1007/978-3-319-06251-8_22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a multidimensional optimization problem that is formulated and
solved in the tropical mathematics setting. The problem consists of minimizing
a nonlinear objective function defined on vectors over an idempotent semifield
by means of a conjugate transposition operator, subject to constraints in the
form of linear vector inequalities. A complete direct solution to the problem
under fairly general assumptions is given in a compact vector form suitable for
both further analysis and practical implementation. We apply the result to
solve a multidimensional minimax single facility location problem with
Chebyshev distance and with inequality constraints imposed on the feasible
location area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2796</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2796</id><created>2013-11-12</created><authors><author><keyname>Srivastava</keyname><forenames>Vaibhav</forenames></author><author><keyname>Surana</keyname><forenames>Amit</forenames></author><author><keyname>Eckstein</keyname><forenames>Miguel P.</forenames></author><author><keyname>Bullo</keyname><forenames>Francesco</forenames></author></authors><title>Mixed Human-Robot Team Surveillance</title><categories>math.OC cs.RO cs.SY</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the mixed human-robot team design in a system theoretic setting
using the context of a surveillance mission. The three key coupled components
of a mixed team design are (i) policies for the human operator, (ii) policies
to account for erroneous human decisions, and (iii) policies to control the
automaton. In this paper, we survey elements of human decision-making,
including evidence aggregation, situational awareness, fatigue, and memory
effects. We bring together the models for these elements in human
decision-making to develop a single coherent model for human decision-making in
a two-alternative choice task. We utilize the developed model to design
efficient attention allocation policies for the human operator. We propose an
anomaly detection algorithm that utilizes potentially erroneous decision by the
operator to ascertain an anomalous region among the set of regions surveilled.
Finally, we propose a stochastic vehicle routing policy that surveils an
anomalous region with high probability. Our mixed team design relies on the
certainty-equivalent receding-horizon control framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2799</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2799</id><created>2013-11-12</created><authors><author><keyname>Dai</keyname><forenames>Dong</forenames></author><author><keyname>Rigollet</keyname><forenames>Philippe</forenames></author><author><keyname>Xia</keyname><forenames>Lucy</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Aggregation of Affine Estimators</title><categories>math.ST cs.LG stat.TH</categories><msc-class>62G08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of aggregating a general collection of affine
estimators for fixed design regression. Relevant examples include some commonly
used statistical estimators such as least squares, ridge and robust least
squares estimators. Dalalyan and Salmon (2012) have established that, for this
problem, exponentially weighted (EW) model selection aggregation leads to sharp
oracle inequalities in expectation, but similar bounds in deviation were not
previously known. While results indicate that the same aggregation scheme may
not satisfy sharp oracle inequalities with high probability, we prove that a
weaker notion of oracle inequality for EW that holds with high probability.
Moreover, using a generalization of the newly introduced $Q$-aggregation scheme
we also prove sharp oracle inequalities that hold with high probability.
Finally, we apply our results to universal aggregation and show that our
proposed estimator leads simultaneously to all the best known bounds for
aggregation, including $\ell_q$-aggregation, $q \in (0,1)$, with high
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2820</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2820</id><created>2013-11-12</created><authors><author><keyname>Devanur</keyname><forenames>Nikhil R.</forenames></author><author><keyname>Morgenstern</keyname><forenames>Jamie</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>Draft Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce draft auctions, which is a sequential auction format where at
each iteration players bid for the right to buy items at a fixed price. We show
that draft auctions offer an exponential improvement in social welfare at
equilibrium over sequential item auctions where predetermined items are
auctioned at each time step. Specifically, we show that for any subadditive
valuation the social welfare at equilibrium is an $O(\log^2(m))$-approximation
to the optimal social welfare, where $m$ is the number of items. We also
provide tighter approximation results for several subclasses. Our welfare
guarantees hold for Bayes-Nash equilibria and for no-regret learning outcomes,
via the smooth-mechanism framework. Of independent interest, our techniques
show that in a combinatorial auction setting, efficiency guarantees of a
mechanism via smoothness for a very restricted class of cardinality valuations,
extend with a small degradation, to subadditive valuations, the largest
complement-free class of valuations. Variants of draft auctions have been used
in practice and have been experimentally shown to outperform other auctions.
Our results provide a theoretical justification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2828</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2828</id><created>2013-11-12</created><updated>2014-03-23</updated><authors><author><keyname>Hsu</keyname><forenames>Justin</forenames></author><author><keyname>Huang</keyname><forenames>Zhiyi</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Roughgarden</keyname><forenames>Tim</forenames></author><author><keyname>Wu</keyname><forenames>Zhiwei Steven</forenames></author></authors><title>Private Matchings and Allocations</title><categories>cs.GT cs.CR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a private variant of the classical allocation problem: given k
goods and n agents with individual, private valuation functions over bundles of
goods, how can we partition the goods amongst the agents to maximize social
welfare? An important special case is when each agent desires at most one good,
and specifies her (private) value for each good: in this case, the problem is
exactly the maximum-weight matching problem in a bipartite graph.
  Private matching and allocation problems have not been considered in the
differential privacy literature, and for good reason: they are plainly
impossible to solve under differential privacy. Informally, the allocation must
match agents to their preferred goods in order to maximize social welfare, but
this preference is exactly what agents wish to hide. Therefore, we consider the
problem under the relaxed constraint of joint differential privacy: for any
agent i, no coalition of agents excluding i should be able to learn about the
valuation function of agent i. In this setting, the full allocation is no
longer published---instead, each agent is told what good to get. We first show
that with a small number of identical copies of each good, it is possible to
efficiently and accurately solve the maximum weight matching problem while
guaranteeing joint differential privacy. We then consider the more general
allocation problem, when bidder valuations satisfy the gross substitutes
condition. Finally, we prove that the allocation problem cannot be solved to
non-trivial accuracy under joint differential privacy without requiring
multiple copies of each type of good.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2838</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2838</id><created>2013-11-12</created><updated>2014-05-10</updated><authors><author><keyname>Pentina</keyname><forenames>Anastasia</forenames></author><author><keyname>Lampert</keyname><forenames>Christoph H.</forenames></author></authors><title>A PAC-Bayesian bound for Lifelong Learning</title><categories>stat.ML cs.LG</categories><comments>to appear at ICML 2014</comments><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transfer learning has received a lot of attention in the machine learning
community over the last years, and several effective algorithms have been
developed. However, relatively little is known about their theoretical
properties, especially in the setting of lifelong learning, where the goal is
to transfer information to tasks for which no data have been observed so far.
In this work we study lifelong learning from a theoretical perspective. Our
main result is a PAC-Bayesian generalization bound that offers a unified view
on existing paradigms for transfer learning, such as the transfer of parameters
or the transfer of low-dimensional representations. We also use the bound to
derive two principled lifelong learning algorithms, and we show that these
yield results comparable with existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2839</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2839</id><created>2013-11-12</created><authors><author><keyname>Guo</keyname><forenames>Zeyu</forenames></author><author><keyname>Sun</keyname><forenames>He</forenames></author></authors><title>Gossip vs. Markov Chains, and Randomness-Efficient Rumor Spreading</title><categories>cs.DC cs.CC cs.DS math.PR</categories><comments>41 pages, 1 figure. arXiv admin note: substantial text overlap with
  arXiv:1304.1359</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study gossip algorithms for the rumor spreading problem which asks one
node to deliver a rumor to all nodes in an unknown network. We present the
first protocol for any expander graph $G$ with $n$ nodes such that, the
protocol informs every node in $O(\log n)$ rounds with high probability, and
uses $\tilde{O}(\log n)$ random bits in total. The runtime of our protocol is
tight, and the randomness requirement of $\tilde{O}(\log n)$ random bits almost
matches the lower bound of $\Omega(\log n)$ random bits for dense graphs. We
further show that, for many graph families, polylogarithmic number of random
bits in total suffice to spread the rumor in $O(\mathrm{poly}\log n)$ rounds.
These results together give us an almost complete understanding of the
randomness requirement of this fundamental gossip process.
  Our analysis relies on unexpectedly tight connections among gossip processes,
Markov chains, and branching programs. First, we establish a connection between
rumor spreading processes and Markov chains, which is used to approximate the
rumor spreading time by the mixing time of Markov chains. Second, we show a
reduction from rumor spreading processes to branching programs, and this
reduction provides a general framework to derandomize gossip processes. In
addition to designing rumor spreading protocols, these novel techniques may
have applications in studying parallel and multiple random walks, and
randomness complexity of distributed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2850</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2850</id><created>2013-11-12</created><authors><author><keyname>Myadzelets</keyname><forenames>Dmitry</forenames></author><author><keyname>Paoli</keyname><forenames>Andrea</forenames></author></authors><title>Virtual Modules in Discrete-Event Systems: Achieving Modular
  Diagnosability</title><categories>cs.SY</categories><msc-class>62P30, 68Q45, 68Q60, 93Axx, 94C12</msc-class><acm-class>B.2.3; B.4.5; C.2.4; D.2.5; I.2.2; F.1.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of enforcing modular diagnosability for
discrete-event systems that don't satisfy this property by their natural
modularity. We introduce an approach to achieve this property combining
existing modules into new virtual modules. An underlining mathematical problem
is to find a partition of a set, such that the partition satisfies the required
property. The time complexity of such problem is very high. To overcome it, the
paper introduces a structural analysis of the system's modules. In the analysis
we focus on the case when the modules participate in diagnosis with their
observations, rather then the case when indistinguishable observations are
blocked due to concurrency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2851</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2851</id><created>2013-11-06</created><authors><author><keyname>Shah</keyname><forenames>Nihar B.</forenames></author><author><keyname>Lee</keyname><forenames>Kangwook</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>When Do Redundant Requests Reduce Latency ?</title><categories>cs.NI cs.DC cs.PF</categories><comments>Extended version of paper presented at Allerton Conference 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several systems possess the flexibility to serve requests in more than one
way. For instance, a distributed storage system storing multiple replicas of
the data can serve a request from any of the multiple servers that store the
requested data, or a computational task may be performed in a compute-cluster
by any one of multiple processors. In such systems, the latency of serving the
requests may potentially be reduced by sending &quot;redundant requests&quot;: a request
may be sent to more servers than needed, and it is deemed served when the
requisite number of servers complete service. Such a mechanism trades off the
possibility of faster execution of at least one copy of the request with the
increase in the delay due to an increased load on the system. Due to this
tradeoff, it is unclear when redundant requests may actually help. Several
recent works empirically evaluate the latency performance of redundant requests
in diverse settings.
  This work aims at an analytical study of the latency performance of redundant
requests, with the primary goals of characterizing under what scenarios sending
redundant requests will help (and under what scenarios they will not help), as
well as designing optimal redundant-requesting policies. We first present a
model that captures the key features of such systems. We show that when service
times are i.i.d. memoryless or &quot;heavier&quot;, and when the additional copies of
already-completed jobs can be removed instantly, redundant requests reduce the
average latency. On the other hand, when service times are &quot;lighter&quot; or when
service times are memoryless and removal of jobs is not instantaneous, then not
having any redundancy in the requests is optimal under high loads. Our results
hold for arbitrary arrival processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2852</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2852</id><created>2013-11-12</created><updated>2014-01-15</updated><authors><author><keyname>Bertschinger</keyname><forenames>Nils</forenames></author><author><keyname>Rauh</keyname><forenames>Johannes</forenames></author><author><keyname>Olbrich</keyname><forenames>Eckehard</forenames></author><author><keyname>Jost</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Ay</keyname><forenames>Nihat</forenames></author></authors><title>Quantifying unique information</title><categories>cs.IT math.IT</categories><comments>24 pages, 2 figures. Version 2 contains less typos than version 1</comments><msc-class>94A15, 94A17</msc-class><journal-ref>Entropy, 16 (2014) 4, p. 2161-2183</journal-ref><doi>10.3390/e16042161</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose new measures of shared information, unique information and
synergistic information that can be used to decompose the multi-information of
a pair of random variables $(Y,Z)$ with a third random variable $X$. Our
measures are motivated by an operational idea of unique information which
suggests that shared information and unique information should depend only on
the pair marginal distributions of $(X,Y)$ and $(X,Z)$. Although this
invariance property has not been studied before, it is satisfied by other
proposed measures of shared information. The invariance property does not
uniquely determine our new measures, but it implies that the functions that we
define are bounds to any other measures satisfying the same invariance
property. We study properties of our measures and compare them to other
candidate measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2854</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2854</id><created>2013-11-12</created><updated>2015-05-12</updated><authors><author><keyname>Boutsidis</keyname><forenames>Christos</forenames></author><author><keyname>Gittens</keyname><forenames>Alex</forenames></author><author><keyname>Kambadur</keyname><forenames>Prabhanjan</forenames></author></authors><title>Spectral Clustering via the Power Method -- Provably</title><categories>cs.LG cs.NA</categories><comments>ICML 2015, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral clustering is one of the most important algorithms in data mining
and machine intelligence; however, its computational complexity limits its
application to truly large scale data analysis. The computational bottleneck in
spectral clustering is computing a few of the top eigenvectors of the
(normalized) Laplacian matrix corresponding to the graph representing the data
to be clustered. One way to speed up the computation of these eigenvectors is
to use the &quot;power method&quot; from the numerical linear algebra literature.
Although the power method has been empirically used to speed up spectral
clustering, the theory behind this approach, to the best of our knowledge,
remains unexplored. This paper provides the \emph{first} such rigorous
theoretical justification, arguing that a small number of power iterations
suffices to obtain near-optimal partitionings using the approximate
eigenvectors. Specifically, we prove that solving the $k$-means clustering
problem on the approximate eigenvectors obtained via the power method gives an
additive-error approximation to solving the $k$-means problem on the optimal
eigenvectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2869</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2869</id><created>2013-11-12</created><authors><author><keyname>Kumar</keyname><forenames>Ashish</forenames></author><author><keyname>Narula</keyname><forenames>Lakshay</forenames></author><author><keyname>Singh</keyname><forenames>S. P.</forenames></author></authors><title>Cognitive Radios: A Survey of Methods for Channel State Prediction</title><categories>cs.NI</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the need for Cognitive Radio ability in view of the
physical scarcity of wireless spectrum for communication. A background of the
Cognitive Radio technology is presented and the aspect of 'channel state
prediction' is focused upon. Hidden Markov Models (HMM) have been traditionally
used to model the wireless channel behavior but it suffers from certain
limitations. We discuss few techniques of channel state prediction using
machine-learning methods and will extend the Conditional Random Field (CRF)
procedure to this field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2878</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2878</id><created>2013-11-12</created><authors><author><keyname>Taylor</keyname><forenames>Sean J.</forenames></author><author><keyname>Bakshy</keyname><forenames>Eytan</forenames></author><author><keyname>Aral</keyname><forenames>Sinan</forenames></author></authors><title>Selection Effects in Online Sharing: Consequences for Peer Adoption</title><categories>cs.SI physics.soc-ph</categories><comments>14th ACM Conference on Electronic Commerce, June 16-20, 2013,
  University of Pennsylvania, Philadelphia PA</comments><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most models of social contagion take peer exposure to be a corollary of
adoption, yet in many settings, the visibility of one's adoption behavior
happens through a separate decision process. In online systems, product
designers can define how peer exposure mechanisms work: adoption behaviors can
be shared in a passive, automatic fashion, or occur through explicit, active
sharing. The consequences of these mechanisms are of substantial practical and
theoretical interest: passive sharing may increase total peer exposure but
active sharing may expose higher quality products to peers who are more likely
to adopt.
  We examine selection effects in online sharing through a large-scale field
experiment on Facebook that randomizes whether or not adopters share Offers
(coupons) in a passive manner. We derive and estimate a joint discrete choice
model of adopters' sharing decisions and their peers' adoption decisions. Our
results show that active sharing enables a selection effect that exposes peers
who are more likely to adopt than the population exposed under passive sharing.
  We decompose the selection effect into two distinct mechanisms: active
sharers expose peers to higher quality products, and the peers they share with
are more likely to adopt independently of product quality. Simulation results
show that the user-level mechanism comprises the bulk of the selection effect.
The study's findings are among the first to address downstream peer effects
induced by online sharing mechanisms, and can inform design in settings where a
surplus of sharing could be viewed as costly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2879</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2879</id><created>2013-10-26</created><authors><author><keyname>Awasthi</keyname><forenames>Abhishek</forenames></author><author><keyname>L&#xe4;ssig</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Kramer</keyname><forenames>Oliver</forenames></author></authors><title>Common Due-Date Problem: Exact Polynomial Algorithms for a Given Job
  Sequence</title><categories>cs.DS math.CO</categories><comments>15th International Symposium on Symbolic and Numeric Algorithms for
  Scientific Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of scheduling jobs on single and parallel
machines where all the jobs possess different processing times but a common due
date. There is a penalty involved with each job if it is processed earlier or
later than the due date. The objective of the problem is to find the assignment
of jobs to machines, the processing sequence of jobs and the time at which they
are processed, which minimizes the total penalty incurred due to tardiness or
earliness of the jobs. This work presents exact polynomial algorithms for
optimizing a given job sequence or single and parallel machines with the
run-time complexities of $O(n \log n)$ and $O(mn^2 \log n)$ respectively, where
$n$ is the number of jobs and $m$ the number of machines. The algorithms take a
sequence consisting of all the jobs $(J_i, i=1,2,\dots,n)$ as input and
distribute the jobs to machines (for $m&gt;1$) along with their best completion
times so as to get the least possible total penalty for this sequence. We prove
the optimality for the single machine case and the runtime complexities of
both. Henceforth, we present the results for the benchmark instances and
compare with previous work for single and parallel machine cases, up to $200$
jobs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2880</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2880</id><created>2013-10-26</created><authors><author><keyname>Awasthi</keyname><forenames>Abhishek</forenames></author><author><keyname>Kramer</keyname><forenames>Oliver</forenames></author><author><keyname>L&#xe4;ssig</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Aircraft Landing Problem: Efficient Algorithm for a Given Landing
  Sequence</title><categories>cs.DS math.CO</categories><comments>16th IEEE International Conference on Computational Science and
  Engineering (CSE 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate a special case of the static aircraft landing
problem (ALP) with the objective to optimize landing sequences and landing
times for a set of air planes. The problem is to land the planes on one or
multiple runways within a time window as close as possible to the preferable
target landing time, maintaining a safety distance constraint. The objective of
this well-known NP-hard optimization problem is to minimize the sum of the
total penalty incurred by all the aircraft for arriving earlier or later than
their preferred landing times. For a problem variant that optimizes a given
feasible landing sequence for the single runway case, we present an exact
polynomial algorithm and prove the run-time complexity to lie in $O(N^3)$,
where $N$ is the number of aircraft. The proposed algorithm returns the optimal
solution for the ALP for a given feasible landing sequence on a single runway
for a common practical case of the ALP described in the paper. Furthermore, we
propose a strategy for the ALP with multiple runways and present our results
for all the benchmark instances with single and multiple runways, while
comparing them to previous results in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2886</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2886</id><created>2013-10-09</created><authors><author><keyname>Ayhan</keyname><forenames>Mustafa Batuhan</forenames></author></authors><title>A Fuzzy AHP Approach for Supplier Selection Problem: A Case Study in a
  Gear Motor Company</title><categories>cs.AI</categories><comments>Published in &quot;International Journal of Managing Value and Supply
  Chains (IJMVSC) Vol.4, No. 3, September 2013&quot;</comments><journal-ref>International Journal of Managing Value and Supply Chains (IJMVSC)
  Vol.4, No. 3, September 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suuplier selection is one of the most important functions of a purchasing
department. Since by deciding the best supplier, companies can save material
costs and increase competitive advantage.However this decision becomes
compilcated in case of multiple suppliers, multiple conflicting criteria, and
imprecise parameters. In addition the uncertainty and vagueness of the experts'
opinion is the prominent characteristic of the problem. therefore an
extensively used multi criteria decision making tool Fuzzy AHP can be utilized
as an approach for supplier selection problem. This paper reveals the
application of Fuzzy AHP in a gear motor company determining the best supplier
with respect to selected criteria. the contribution of this study is not only
the application of the Fuzzy AHP methodology for supplier selection problem,
but also releasing a comprehensive literature review of multi criteria decision
making problems. In addition by stating the steps of Fuzzy AHP clearly and
numerically, this study can be a guide of the methodology to be implemented to
other multiple criteria decision making problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2887</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2887</id><created>2013-10-30</created><updated>2014-10-26</updated><authors><author><keyname>Hashmi</keyname><forenames>Aneeq</forenames></author><author><keyname>Zaidi</keyname><forenames>Faraz</forenames></author><author><keyname>Sallaberry</keyname><forenames>Arnaud</forenames></author><author><keyname>Mehmood</keyname><forenames>Tariq</forenames></author></authors><title>Are all Social Networks Structurally Similar? A Comparative Study using
  Network Statistics and Metrics</title><categories>cs.SI</categories><comments>ASONAM 2012, Istanbul : Turkey (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modern age has seen an exponential growth of social network data
available on the web. Analysis of these networks reveal important structural
information about these networks in particular and about our societies in
general. More often than not, analysis of these networks is concerned in
identifying similarities among social networks and how they are different from
other networks such as protein interaction networks, computer networks and food
web. In this paper, our objective is to perform a critical analysis of
different social networks using structural metrics in an effort to highlight
their similarities and differences. We use five different social network
datasets which are contextually and semantically different from each other. We
then analyze these networks using a number of different network statistics and
metrics. Our results show that although these social networks have been
constructed from different contexts, they are structurally similar. We also
review the snowball sampling method and show its vulnerability against
different network metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2889</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2889</id><created>2013-11-01</created><authors><author><keyname>Borkar</keyname><forenames>Vivek S.</forenames></author><author><keyname>Mathkar</keyname><forenames>Adwaitvedant S.</forenames></author></authors><title>Reinforcement Learning for Matrix Computations: PageRank as an Example</title><categories>cs.LG cs.SI stat.ML</categories><comments>12 pages, 6 figures, invited lecture at ICDIT (International
  Conference on Distributed Computing and Internet Technologies), 2014, will be
  published in Lecture notes in Computer Science along with the conference
  proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning has gained wide popularity as a technique for
simulation-driven approximate dynamic programming. A less known aspect is that
the very reasons that make it effective in dynamic programming can also be
leveraged for using it for distributed schemes for certain matrix computations
involving non-negative matrices. In this spirit, we propose a reinforcement
learning algorithm for PageRank computation that is fashioned after analogous
schemes for approximate dynamic programming. The algorithm has the advantage of
ease of distributed implementation and more importantly, of being model-free,
i.e., not dependent on any specific assumptions about the transition
probabilities in the random web-surfer model. We analyze its convergence and
finite time behavior and present some supporting numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2891</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2891</id><created>2013-11-12</created><updated>2014-02-17</updated><authors><author><keyname>Anderson</keyname><forenames>Joseph</forenames></author><author><keyname>Belkin</keyname><forenames>Mikhail</forenames></author><author><keyname>Goyal</keyname><forenames>Navin</forenames></author><author><keyname>Rademacher</keyname><forenames>Luis</forenames></author><author><keyname>Voss</keyname><forenames>James</forenames></author></authors><title>The More, the Merrier: the Blessing of Dimensionality for Learning Large
  Gaussian Mixtures</title><categories>cs.LG cs.DS stat.ML</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that very large mixtures of Gaussians are efficiently
learnable in high dimension. More precisely, we prove that a mixture with known
identical covariance matrices whose number of components is a polynomial of any
fixed degree in the dimension n is polynomially learnable as long as a certain
non-degeneracy condition on the means is satisfied. It turns out that this
condition is generic in the sense of smoothed complexity, as soon as the
dimensionality of the space is high enough. Moreover, we prove that no such
condition can possibly exist in low dimension and the problem of learning the
parameters is generically hard. In contrast, much of the existing work on
Gaussian Mixtures relies on low-dimensional projections and thus hits an
artificial barrier. Our main result on mixture recovery relies on a new
&quot;Poissonization&quot;-based technique, which transforms a mixture of Gaussians to a
linear map of a product distribution. The problem of learning this map can be
efficiently solved using some recent results on tensor decompositions and
Independent Component Analysis (ICA), thus giving an algorithm for recovering
the mixture. In addition, we combine our low-dimensional hardness results for
Gaussian mixtures with Poissonization to show how to embed difficult instances
of low-dimensional Gaussian mixtures into the ICA setting, thus establishing
exponential information-theoretic lower bounds for underdetermined ICA in low
dimension. To the best of our knowledge, this is the first such result in the
literature. In addition to contributing to the problem of Gaussian mixture
learning, we believe that this work is among the first steps toward better
understanding the rare phenomenon of the &quot;blessing of dimensionality&quot; in the
computational aspects of statistical inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2897</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2897</id><created>2013-11-12</created><authors><author><keyname>Feyzmahdavian</keyname><forenames>Hamid Reza</forenames></author><author><keyname>Charalambous</keyname><forenames>Themistoklis</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author></authors><title>Exponential Stability of Homogeneous Positive Systems of Degree One With
  Time-Varying Delays</title><categories>cs.SY</categories><comments>Submitted to IEEE Transactions on Automatic Control</comments><journal-ref>IEEE Transactions on Automatic Control, 59 (6), pp. 1594-1599,
  June 2014</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  While the asymptotic stability of positive linear systems in the presence of
bounded time delays has been thoroughly investigated, the theory for nonlinear
positive systems is considerably less well-developed. This paper presents a set
of conditions for establishing delay-independent stability and bounding the
decay rate of a significant class of nonlinear positive systems which includes
positive linear systems as a special case. Specifically, when the time delays
have a known upper bound, we derive necessary and sufficient conditions for
exponential stability of (a) continuous-time positive systems whose vector
fields are homogeneous and cooperative, and (b) discrete-time positive systems
whose vector fields are homogeneous and order preserving. We then present
explicit expressions that allow us to quantify the impact of delays on the
decay rate and show that the best decay rate of positive linear systems that
our bounds provide can be found via convex optimization. Finally, we extend the
results to general linear systems with time-varying delays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2901</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2901</id><created>2013-11-12</created><updated>2013-11-28</updated><authors><author><keyname>Zeiler</keyname><forenames>Matthew D</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author></authors><title>Visualizing and Understanding Convolutional Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large Convolutional Network models have recently demonstrated impressive
classification performance on the ImageNet benchmark. However there is no clear
understanding of why they perform so well, or how they might be improved. In
this paper we address both issues. We introduce a novel visualization technique
that gives insight into the function of intermediate feature layers and the
operation of the classifier. We also perform an ablation study to discover the
performance contribution from different model layers. This enables us to find
model architectures that outperform Krizhevsky \etal on the ImageNet
classification benchmark. We show our ImageNet model generalizes well to other
datasets: when the softmax classifier is retrained, it convincingly beats the
current state-of-the-art results on Caltech-101 and Caltech-256 datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2903</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2903</id><created>2013-11-12</created><updated>2014-01-18</updated><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Protocol Design and Stability Analysis of Cooperative Cognitive Radio
  Users</title><categories>cs.NI cs.IT math.IT</categories><comments>Accepted in WCNC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A single cognitive radio transmitter--receiver pair shares the spectrum with
two primary users communicating with their respective receivers. Each primary
user has a local traffic queue, whereas the cognitive user has three queues;
one storing its own traffic while the other two are relaying queues used to
store primary relayed packets admitted from the two primary users. A new
cooperative cognitive medium access control protocol for the described network
is proposed, where the cognitive user exploits the idle periods of the primary
spectrum bands. Traffic arrival to each relaying queue is controlled using a
tuneable admittance factor, while relaying queues service scheduling is
controlled via channel access probabilities assigned to each queue based on the
band of operation. The stability region of the proposed protocol is
characterized shedding light on its maximum expected throughput. Numerical
results demonstrate the performance gains of the proposed cooperative cognitive
protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2906</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2906</id><created>2013-11-12</created><authors><author><keyname>De Domenico</keyname><forenames>Manlio</forenames></author><author><keyname>Sol&#xe9;-Ribalta</keyname><forenames>Albert</forenames></author><author><keyname>Omodei</keyname><forenames>Elisa</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Sergio</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author></authors><title>Centrality in Interconnected Multilayer Networks</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>12 pages, 5 figures</comments><journal-ref>Nature Communications 6, 6868 (2015)</journal-ref><doi>10.1038/ncomms7868</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-world complex systems exhibit multiple levels of relationships. In many
cases, they require to be modeled by interconnected multilayer networks,
characterizing interactions on several levels simultaneously. It is of crucial
importance in many fields, from economics to biology, from urban planning to
social sciences, to identify the most (or the less) influent nodes in a
network. However, defining the centrality of actors in an interconnected
structure is not trivial.
  In this paper, we capitalize on the tensorial formalism, recently proposed to
characterize and investigate this kind of complex topologies, to show how
several centrality measures -- well-known in the case of standard (&quot;monoplex&quot;)
networks -- can be extended naturally to the realm of interconnected
multiplexes. We consider diagnostics widely used in different fields, e.g.,
computer science, biology, communication and social sciences, to cite only some
of them. We show, both theoretically and numerically, that using the weighted
monoplex obtained by aggregating the multilayer network leads, in general, to
relevant differences in ranking the nodes by their importance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2911</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2911</id><created>2013-11-12</created><updated>2014-09-24</updated><authors><author><keyname>Kung</keyname><forenames>Kevin S.</forenames></author><author><keyname>Greco</keyname><forenames>Kael</forenames></author><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>Exploring universal patterns in human home-work commuting from mobile
  phone data</title><categories>cs.SI cs.CY physics.soc-ph</categories><journal-ref>Kung KS, Greco K, Sobolevsky S, Ratti C (2014) Exploring Universal
  Patterns in Human Home-Work Commuting from Mobile Phone Data. PLoS ONE 9(6):
  e96180</journal-ref><doi>10.1371/journal.pone.0096180</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Home-work commuting has always attracted significant research attention
because of its impact on human mobility. One of the key assumptions in this
domain of study is the universal uniformity of commute times. However, a true
comparison of commute patterns has often been hindered by the intrinsic
differences in data collection methods, which make observation from different
countries potentially biased and unreliable. In the present work, we approach
this problem through the use of mobile phone call detail records (CDRs), which
offers a consistent method for investigating mobility patterns in wholly
different parts of the world. We apply our analysis to a broad range of
datasets, at both the country and city scale. Additionally, we compare these
results with those obtained from vehicle GPS traces in Milan. While different
regions have some unique commute time characteristics, we show that the
home-work time distributions and average values within a single region are
indeed largely independent of commute distance or country (Portugal, Ivory
Coast, and Boston)--despite substantial spatial and infrastructural
differences. Furthermore, a comparative analysis demonstrates that such
distance-independence holds true only if we consider multimodal commute
behaviors--as consistent with previous studies. In car-only (Milan GPS traces)
and car-heavy (Saudi Arabia) commute datasets, we see that commute time is
indeed influenced by commute distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2912</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2912</id><created>2013-10-26</created><authors><author><keyname>Laufer</keyname><forenames>Michael S.</forenames></author></authors><title>A Misanthropic Reinterpretation of the Chinese Room Problem</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The chinese room problem asks if computers can think; I ask here if most
humans can.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2914</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2914</id><created>2013-10-09</created><updated>2013-12-12</updated><authors><author><keyname>Lemoy</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Alava</keyname><forenames>Mikko</forenames></author><author><keyname>Aurell</keyname><forenames>Erik</forenames></author></authors><title>A novel local search based on variable-focusing for random K-SAT</title><categories>cs.AI cond-mat.dis-nn</categories><comments>7 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new local search algorithm for satisfiability problems. Usual
approaches focus uniformly on unsatisfied clauses. The new method works by
picking uniformly random variables in unsatisfied clauses. A Variable-based
Focused Metropolis Search (V-FMS) is then applied to random 3-SAT. We show that
it is quite comparable in performance to the clause-based FMS. Consequences for
algorithmic design are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2927</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2927</id><created>2013-11-12</created><authors><author><keyname>Kale</keyname><forenames>Archana</forenames></author><author><keyname>Patil</keyname><forenames>Amitkumar</forenames></author><author><keyname>Biswas</keyname><forenames>Supratim</forenames></author></authors><title>Parallelization of Loops with Variable Distance Data Dependences</title><categories>cs.PL cs.DC</categories><comments>10 pages, 3 figures</comments><acm-class>D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extent of parallelization of a loop is largely determined by the
dependences between its statements. While dependence free loops are fully
parallelizable, those with loop carried dependences are not. Dependence
distance is a measure of absolute difference between a pair of dependent
iterations. Loops with constant distance data dependence, because of uniform
distance between the dependent iterations, lead to easy partitioning of the
iteration space and hence they have been successfully dealt with.
  Parallelization of loops with variable distance data dependences is a
considerably difficult problem. It is our belief that partitioning the
iteration space in such loops cannot be done without examining solutions of the
corresponding Linear Diophantine Equations. Focus of this work is to study
variable distance data dependences and examine the relation between dependent
iterations. Our analysis based on parametric solution leads to a mathematical
formulation capturing dependence between iterations. Our approach shows the
existence of reasonable exploitable parallelism in variable distance data
dependences loops with multiple LDEs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2928</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2928</id><created>2013-11-12</created><updated>2015-04-24</updated><authors><author><keyname>Hahn</keyname><forenames>Ernst Moritz</forenames></author><author><keyname>Li</keyname><forenames>Guangyuan</forenames></author><author><keyname>Schewe</keyname><forenames>Sven</forenames></author><author><keyname>Turrini</keyname><forenames>Andrea</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author></authors><title>Lazy Probabilistic Model Checking without Determinisation</title><categories>cs.LO cs.FL</categories><comments>38 pages. Updated version for introducing the following changes: -
  general improvement on paper presentation; - extension of the approach to
  avoid full determinisation; - added proofs for such an extension; - added
  case studies; - updated old case studies to reflect the added extension</comments><acm-class>G.3; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bottleneck in the quantitative analysis of Markov chains and Markov
decision processes against specifications given in LTL or as some form of
nondeterministic B\&quot;uchi automata is the inclusion of a determinisation step of
the automaton under consideration. In this paper, we show that full
determinisation can be avoided: subset and breakpoint constructions suffice. We
have implemented our approach---both explicit and symbolic versions---in a
prototype tool. Our experiments show that our prototype can compete with mature
tools like PRISM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2959</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2959</id><created>2013-11-07</created><updated>2015-09-25</updated><authors><author><keyname>Braibant</keyname><forenames>Thomas</forenames><affiliation>Gallium</affiliation></author><author><keyname>Jourdan</keyname><forenames>Jacques-Henri</forenames><affiliation>Gallium</affiliation></author><author><keyname>Monniaux</keyname><forenames>David</forenames></author></authors><title>Implementing and reasoning about hash-consed data structures in Coq</title><categories>cs.LO cs.DS</categories><proxy>ccsd</proxy><journal-ref>Journal of Automated Reasoning, Springer Verlag (Germany), 2014,
  53 (3), pp.271-304</journal-ref><doi>10.1007/s10817-014-9306-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on four different approaches to implementing hash-consing in Coq
programs. The use cases include execution inside Coq, or execution of the
extracted OCaml code. We explore the different trade-offs between faithful use
of pristine extracted code, and code that is fine-tuned to make use of OCaml
programming constructs not available in Coq. We discuss the possible
consequences in terms of performances and guarantees. We use the running
example of binary decision diagrams and then demonstrate the generality of our
solutions by applying them to other examples of hash-consed data structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2960</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2960</id><created>2013-11-10</created><authors><author><keyname>Wang</keyname><forenames>Yong</forenames></author></authors><title>An Axiomatization for Quantum Processes to Unifying Quantum and
  Classical Computing</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish an axiomatization for quantum processes, which is a quantum
generalization of process algebra ACP (Algebra of Communicating Processes). We
use the framework of a quantum process configuration $\langle p,
\varrho\rangle$, but we treat it as two relative independent part: the
structural part $p$ and the quantum part $\varrho$, because the establishment
of a sound and complete theory is dependent on the structural properties of the
structural part $p$. We let the quantum part $\varrho$ be the outcomes of
execution of $p$ to examine and observe the function of the basic theory of
quantum mechanics. We establish not only a strong bisimularity for quantum
processes, but also a weak bisimularity to model the silent step and abstract
internal computations in quantum processes. The relationship between quantum
bisimularity and classical bisimularity is established, which makes an
axiomatization of quantum processes possible. An axiomatization for quantum
processes called qACP is designed, which involves not only quantum information,
but also classical information and unifies quantum computing and classical
computing. qACP can be used easily and widely for verification of most quantum
communication protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2968</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2968</id><created>2013-11-12</created><authors><author><keyname>Bar</keyname><forenames>Adnan Al</forenames></author><author><keyname>Basili</keyname><forenames>Victor</forenames></author><author><keyname>Jedaibi</keyname><forenames>Wajdi Al</forenames></author><author><keyname>Chaudhry</keyname><forenames>Abdul Jawad</forenames></author></authors><title>An Experience based Evaluation Process for ERP bids</title><categories>cs.SE</categories><comments>14 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enterprise Resource Planning ERP systems integrate information across an
entire organization that automate core activities such as finance accounting,
human resources, manufacturing, production and supply chain management etc. to
facilitate an integrated centralized system and rapid decision making resulting
in cost reduction, greater planning, and increased control. Many organizations
are updating their current management information systems with ERP systems.
This is not a trivial task. They have to identify the organizations objectives
and satisfy a myriad of stakeholders. They have to understand what business
processes they have, how they can be improved, and what particular systems
would best suit their needs. They have to understand how an ERP system is
built, it involves the modification of an existing system with its own set of
business rules. Deciding what to ask for and how to select the best option is a
very complex operation and there is limited experience with this type of
contracting in organizations. In this paper we discuss a particular experience
with contracting out an ERP system, provide some lessons learned, and offer
suggestions in how the RFP and bid selection processes could have been
improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2970</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2970</id><created>2013-11-12</created><updated>2014-05-20</updated><authors><author><keyname>Bithas</keyname><forenames>Petros S.</forenames></author><author><keyname>Lioumpas</keyname><forenames>Athanasios S.</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author><author><keyname>Sharif</keyname><forenames>Bayan S.</forenames></author></authors><title>Hybrid Cellular/WLAN with Wireless Offloading: Enabling Next Generation
  Wireless Networks</title><categories>cs.NI</categories><comments>Intellectual properties issues with a patent pending</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exploitation of already deployed wireless local area networks (WLAN)s
(e.g., WiFi access points (AP)s) has attracted considerable attention, as an
efficient and practical method to improve the performance of beyond 4G wireless
networks. In this paper, we propose a novel communication paradigm to satisfy
the performance demands of future wireless networks: a hybrid Cellular/WLAN
network architecture with wireless offloading. In contrast to the commonly
adopted practice of WiFi offloading, where the WLAN APs have a wired backhaul
(e.g., Digital Subscriber Line), we propose a wireless offloading approach,
where the WLAN APs will share their wireless cellular broadband connection with
other users. These users will select their serving node, i.e., the macro-cell
eNodeB or a WLAN AP, based on a certain selection criterion. Thus a challenging
research field is originated, where interfering effects and wireless resources
limitations play a dominant role. Important performance metrics of the proposed
hybrid scheme, including the bit error probability, the ergodic capacity and
the average signal-to-interference-plus noise ratio, are theoretically studied
and closed form expressions are derived for the single-user case with multiple
interferers, for both identical and non-identical fading conditions. Also,
based on the general multi-cellular hybrid WLAN-Cellular concept, we first
propose a intercell interference minimization approach. Then we present a novel
scheme for achieving frequency reuse equal to one within a single macro-cell,
under specific performance criteria and constraints, that guarantee the overall
cell or the individual user QoS requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2971</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2971</id><created>2013-11-12</created><authors><author><keyname>Affandi</keyname><forenames>Raja Hafiz</forenames></author><author><keyname>Fox</keyname><forenames>Emily B.</forenames></author><author><keyname>Taskar</keyname><forenames>Ben</forenames></author></authors><title>Approximate Inference in Continuous Determinantal Point Processes</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determinantal point processes (DPPs) are random point processes well-suited
for modeling repulsion. In machine learning, the focus of DPP-based models has
been on diverse subset selection from a discrete and finite base set. This
discrete setting admits an efficient sampling algorithm based on the
eigendecomposition of the defining kernel matrix. Recently, there has been
growing interest in using DPPs defined on continuous spaces. While the
discrete-DPP sampler extends formally to the continuous case, computationally,
the steps required are not tractable in general. In this paper, we present two
efficient DPP sampling schemes that apply to a wide range of kernel functions:
one based on low rank approximations via Nystrom and random Fourier feature
techniques and another based on Gibbs sampling. We demonstrate the utility of
continuous DPPs in repulsive mixture modeling and synthesizing human poses
spanning activity spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2972</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2972</id><created>2013-11-12</created><updated>2014-05-17</updated><authors><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author></authors><title>Learning Mixtures of Discrete Product Distributions using Spectral
  Decompositions</title><categories>stat.ML cs.CC cs.IT cs.LG math.IT</categories><comments>30 pages no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of learning a distribution from samples, when the
underlying distribution is a mixture of product distributions over discrete
domains. This problem is motivated by several practical applications such as
crowd-sourcing, recommendation systems, and learning Boolean functions. The
existing solutions either heavily rely on the fact that the number of
components in the mixtures is finite or have sample/time complexity that is
exponential in the number of components. In this paper, we introduce a
polynomial time/sample complexity method for learning a mixture of $r$ discrete
product distributions over $\{1, 2, \dots, \ell\}^n$, for general $\ell$ and
$r$. We show that our approach is statistically consistent and further provide
finite sample guarantees.
  We use techniques from the recent work on tensor decompositions for
higher-order moment matching. A crucial step in these moment matching methods
is to construct a certain matrix and a certain tensor with low-rank spectral
decompositions. These tensors are typically estimated directly from the
samples. The main challenge in learning mixtures of discrete product
distributions is that these low-rank tensors cannot be obtained directly from
the sample moments. Instead, we reduce the tensor estimation problem to: $a$)
estimating a low-rank matrix using only off-diagonal block elements; and $b$)
estimating a tensor using a small number of linear measurements. Leveraging on
recent developments in matrix completion, we give an alternating minimization
based method to estimate the low-rank matrix, and formulate the tensor
completion problem as a least-squares problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2973</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2973</id><created>2013-11-12</created><authors><author><keyname>Sofronie-Stokkermans</keyname><forenames>Viorica</forenames></author></authors><title>Locality and applications to subsumption testing and interpolation in
  $\mathcal{EL}$ and some of its extensions</title><categories>cs.LO</categories><comments>42 pages</comments><acm-class>F.4.1; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that subsumption problems in lightweight description
logics (such as $\mathcal{EL}$ and $\mathcal{EL}^+$) can be expressed as
uniform word problems in classes of semilattices with monotone operators. We
use possibilities of efficient local reasoning in such classes of algebras, to
obtain uniform PTIME decision procedures for CBox subsumption in
$\mathcal{EL}$, $\mathcal{EL}^+$ and extensions thereof. These locality
considerations allow us to present a new family of (possibly many-sorted)
logics which extend $\mathcal{EL}$ and $\mathcal{EL}^+$ with $n$-ary roles
and/or numerical domains. As a by-product, this allows us to show that the
algebraic models of ${\cal EL}$ and ${\cal EL}^+$ have ground interpolation and
thus that ${\cal EL}$, ${\cal EL}^+$, and their extensions studied in this
paper have interpolation. We also show how these ideas can be used for the
description logic $\mathcal{EL}^{++}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2978</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2978</id><created>2013-11-12</created><authors><author><keyname>Lahiri</keyname><forenames>Shibamouli</forenames></author><author><keyname>Mihalcea</keyname><forenames>Rada</forenames></author></authors><title>Authorship Attribution Using Word Network Features</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore a set of novel features for authorship attribution
of documents. These features are derived from a word network representation of
natural language text. As has been noted in previous studies, natural language
tends to show complex network structure at word level, with low degrees of
separation and scale-free (power law) degree distribution. There has also been
work on authorship attribution that incorporates ideas from complex networks.
The goal of our paper is to explore properties of these complex networks that
are suitable as features for machine-learning-based authorship attribution of
documents. We performed experiments on three different datasets, and obtained
promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.2987</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.2987</id><created>2013-11-12</created><authors><author><keyname>Palangi</keyname><forenames>Hamid</forenames></author><author><keyname>Deng</keyname><forenames>Li</forenames></author><author><keyname>Ward</keyname><forenames>Rabab K</forenames></author></authors><title>Learning Input and Recurrent Weight Matrices in Echo State Networks</title><categories>cs.LG</categories><comments>Deep Learning Workshop NIPS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Echo State Networks (ESNs) are a special type of the temporally deep network
model, the Recurrent Neural Network (RNN), where the recurrent matrix is
carefully designed and both the recurrent and input matrices are fixed. An ESN
uses the linearity of the activation function of the output units to simplify
the learning of the output matrix. In this paper, we devise a special technique
that take advantage of this linearity in the output units of an ESN, to learn
the input and recurrent matrices. This has not been done in earlier ESNs due to
their well known difficulty in learning those matrices. Compared to the
technique of BackPropagation Through Time (BPTT) in learning general RNNs, our
proposed method exploits linearity of activation function in the output units
to formulate the relationships amongst the various matrices in an RNN. These
relationships results in the gradient of the cost function having an analytical
form and being more accurate. This would enable us to compute the gradients
instead of obtaining them by recursion as in BPTT. Experimental results on
phone state classification show that learning one or both the input and
recurrent matrices in an ESN yields superior results compared to traditional
ESNs that do not learn these matrices, especially when longer time steps are
used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3001</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3001</id><created>2013-11-12</created><authors><author><keyname>Knuth</keyname><forenames>Kevin H.</forenames></author></authors><title>Informed Source Separation: A Bayesian Tutorial</title><categories>stat.ML cs.LG</categories><comments>8 pages Knuth K.H. 2005. Informed source separation: A Bayesian
  tutorial. (Invited paper) B. Sankur, E. Cetin, M. Tekalp, E. Kuruoglu (eds.),
  Proceedings of the 13th European Signal Processing Conference (EUSIPCO 2005),
  Antalya, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Source separation problems are ubiquitous in the physical sciences; any
situation where signals are superimposed calls for source separation to
estimate the original signals. In this tutorial I will discuss the Bayesian
approach to the source separation problem. This approach has a specific
advantage in that it requires the designer to explicitly describe the signal
model in addition to any other information or assumptions that go into the
problem description. This leads naturally to the idea of informed source
separation, where the algorithm design incorporates relevant information about
the specific problem. This approach promises to enable researchers to design
their own high-quality algorithms that are specifically tailored to the problem
at hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3009</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3009</id><created>2013-11-12</created><authors><author><keyname>Jin</keyname><forenames>Lingfei</forenames></author><author><keyname>Xing</keyname><forenames>Chaoping</forenames></author></authors><title>A Construction of New Quantum MDS Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been a great challenge to construct new quantum MDS codes. In
particular, it is very hard to construct quantum MDS codes with relatively
large minimum distance. So far, except for some sparse lengths, all known
$q$-ary quantum MDS codes have minimum distance less than or equal to $q/2+1$.
In the present paper, we provide a construction of quantum MDS codes with
minimum distance bigger than $q/2+1$. In particular, we show existence of
$q$-ary quantum MDS codes with length $n=q^2+1$ and minimum distance $d$ for
any $d\le q+1$ (this result extends those given in \cite{Gu11,Jin1,KZ12}); and
with length $(q^2+2)/3$ and minimum distance $d$ for any $d\le (2q+2)/3$ if
$3|(q+1)$. Our method is through Hermitian self-orthogonal codes. The main idea
of constructing Hermitian self-orthogonal codes is based on the solvability in
$\F_q$ of a system of homogenous equations over $\F_{q^2}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3011</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3011</id><created>2013-11-12</created><authors><author><keyname>Artzi</keyname><forenames>Yoav</forenames></author><author><keyname>Zettlemoyer</keyname><forenames>Luke</forenames></author></authors><title>UW SPF: The University of Washington Semantic Parsing Framework</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The University of Washington Semantic Parsing Framework (SPF) is a learning
and inference framework for mapping natural language to formal representation
of its meaning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3023</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3023</id><created>2013-11-13</created><authors><author><keyname>Shen</keyname><forenames>Siduo</forenames></author><author><keyname>Lok</keyname><forenames>Tat-Ming</forenames></author></authors><title>Asynchronous Distributed Downlink Beamforming and Power Control in
  Multi-cell Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a multi-cell network where every base station (BS)
serves multiple users with an antenna array. Each user is associated with only
one BS and has a single antenna. Assume that only long-term channel state
information (CSI) is available in the system. The objective is to minimize the
network downlink transmission power needed to meet the users'
signal-to-interference-plus-noise ratio (SINR) requirements. For this
objective, we propose an asynchronous distributed beamforming and power control
algorithm which provides the same optimal solution as given by centralized
algorithms. To design the algorithm, the power minimization problem is
formulated mathematically as a non-convex problem. For distributed
implementation, the non-convex problem is cast into the dual decomposition
framework. Resorting to the theory about matrix pencil, a novel asynchronous
iterative method is proposed for solving the dual of the non-convex problem.
The methods for beamforming and power control are obtained by investigating the
primal problem. At last, simulation results are provided to demonstrate the
convergence and performance of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3026</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3026</id><created>2013-11-13</created><authors><author><keyname>Ocampo</keyname><forenames>Alexis</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Riddle</keyname><forenames>William E.</forenames></author></authors><title>Incrementally Introducing Process Model Rationale Support in an
  Organization</title><categories>cs.SE</categories><comments>11 pages. The The final publication is available at
  link.springer.org.
  http://link.springer.com/chapter/10.1007%2F978-3-642-01680-6_30 Proceedings
  of the International Conference on Software Process: Trustworthy Software
  Development Processes (ICSP '09), Springer Berlin Heidelberg, 2009</comments><doi>10.1007/978-3-642-01680-6_30</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Popular process models such as the Rational Unified Process or the V-Modell
XT are by nature large and complex. Each time that a new release is published
software development organizations are confronted with the big challenge of
understanding the rationale behind the new release and the extent to which it
affects them. Usually, there is no information about what has changed or most
importantly why. This is because of the lack of a flexible approach that
supports organizations responsible for evolving such large process models in
documenting their decisions and that reflects the extent of the capabilities to
which they can provide this information. This paper describes an approach to
incrementally deploying rationale support as needed to match an organisation's
needs, the capabilities and interests of the organisation's process engineering
teams, and the organisation's willingness to support the effort required for
the collection and application of the rationale information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3034</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3034</id><created>2013-11-13</created><authors><author><keyname>Farhadi</keyname><forenames>Maryam</forenames><affiliation>IAU, Mobarakeh</affiliation></author><author><keyname>Salehi</keyname><forenames>Hadi</forenames><affiliation>UKM</affiliation></author><author><keyname>Embi</keyname><forenames>Mohamed Amin</forenames><affiliation>UKM</affiliation></author><author><keyname>Fooladi</keyname><forenames>Masood</forenames><affiliation>IAU, Mobarakeh</affiliation></author><author><keyname>Farhadi</keyname><forenames>Hadi</forenames><affiliation>UKM</affiliation></author><author><keyname>Chadegani</keyname><forenames>Arezoo Aghaei</forenames><affiliation>IAU, Mobarakeh</affiliation></author><author><keyname>Ebrahim</keyname><forenames>Nader Ale</forenames><affiliation>UM</affiliation></author></authors><title>Contribution of Information and Communication Technology (ICT) in
  Country'S H-Index</title><categories>cs.DL</categories><comments>Arezoo Aghaei Chadegani</comments><proxy>ccsd</proxy><journal-ref>Journal of Theoretical and Applied Information Technology 57, 1
  (2013) 122-127</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this study is to examine the effect of Information and
Communication Technology (ICT) development on country's scientific ranking as
measured by H-index. Moreover, this study applies ICT development sub-indices
including ICT Use, ICT Access and ICT skill to find the distinct effect of
these sub-indices on country's H-index. To this purpose, required data for the
panel of 14 Middle East countries over the period 1995 to 2009 is collected.
Findings of the current study show that ICT development increases the H-index
of the sample countries. The results also indicate that ICT Use and ICT Skill
sub-indices positively contribute to higher H-index but the effect of ICT
access on country's H-index is not clear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3037</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3037</id><created>2013-11-13</created><authors><author><keyname>Wang</keyname><forenames>Pinghui</forenames></author><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames></author><author><keyname>Zhao</keyname><forenames>Junzhou</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author><author><keyname>Guan</keyname><forenames>Xiaohong</forenames></author></authors><title>Practical Characterization of Large Networks Using Neighborhood
  Information</title><categories>cs.SI cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterizing large online social networks (OSNs) through node querying is a
challenging task. OSNs often impose severe constraints on the query rate, hence
limiting the sample size to a small fraction of the total network. Various
ad-hoc subgraph sampling methods have been proposed, but many of them give
biased estimates and no theoretical basis on the accuracy. In this work, we
focus on developing sampling methods for OSNs where querying a node also
reveals partial structural information about its neighbors. Our methods are
optimized for NoSQL graph databases (if the database can be accessed directly),
or utilize Web API available on most major OSNs for graph sampling. We show
that our sampling method has provable convergence guarantees on being an
unbiased estimator, and it is more accurate than current state-of-the-art
methods. We characterize metrics such as node label density estimation and edge
label density estimation, two of the most fundamental network characteristics
from which other network characteristics can be derived. We evaluate our
methods on-the-fly over several live networks using their native APIs. Our
simulation studies over a variety of offline datasets show that by including
neighborhood information, our method drastically (4-fold) reduces the number of
samples required to achieve the same estimation accuracy of state-of-the-art
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3045</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3045</id><created>2013-11-13</created><updated>2014-10-01</updated><authors><author><keyname>Liu</keyname><forenames>Ya-Feng</forenames></author><author><keyname>Dai</keyname><forenames>Yu-Hong</forenames></author><author><keyname>Ma</keyname><forenames>Shiqian</forenames></author></authors><title>Joint Power and Admission Control: Non-Convex $L_q$ Approximation and An
  Effective Polynomial Time Deflation Approach</title><categories>cs.IT cs.NI math.IT math.OC</categories><comments>39 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an interference limited network, joint power and admission control (JPAC)
aims at supporting a maximum number of links at their specified signal to
interference plus noise ratio (SINR) targets while using a minimum total
transmission power. Various convex approximation deflation approaches have been
developed for the JPAC problem. In this paper, we propose an effective
polynomial time non-convex approximation deflation approach for solving the
problem. The approach is based on the non-convex $\ell_q$-minimization
approximation of an equivalent sparse $\ell_0$-minimization reformulation of
the JPAC problem where $q\in(0,1).$ We show that, for any instance of the JPAC
problem, there exists a $\bar q\in(0,1)$ such that it can be exactly solved by
solving its $\ell_q$-minimization approximation problem with any $q\in(0, \bar
q]$. We also show that finding the global solution of the $\ell_q$
approximation problem is NP-hard. Then, we propose a potential reduction
interior-point algorithm, which can return an $\epsilon$-KKT solution of the
NP-hard $\ell_q$-minimization approximation problem in polynomial time. The
returned solution can be used to check the simultaneous supportability of all
links in the network and to guide an iterative link removal procedure,
resulting in the polynomial time non-convex approximation deflation approach
for the JPAC problem. Numerical simulations show that the proposed approach
outperforms the existing convex approximation approaches in terms of the number
of supported links and the total transmission power, particularly exhibiting a
quite good performance in selecting which subset of links to support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3048</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3048</id><created>2013-11-13</created><authors><author><keyname>Abraham</keyname><forenames>Ittai</forenames></author><author><keyname>Gavoille</keyname><forenames>Cyril</forenames></author><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Neiman</keyname><forenames>Ofer</forenames></author><author><keyname>Talwar</keyname><forenames>Kunal</forenames></author></authors><title>Cops, Robbers, and Threatening Skeletons: Padded Decomposition for
  Minor-Free Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that any graph excluding $K_r$ as a minor has can be partitioned
into clusters of diameter at most $\Delta$ while removing at most $O(r/\Delta)$
fraction of the edges. This improves over the results of Fakcharoenphol and
Talwar, who building on the work of Klein, Plotkin and Rao gave a partitioning
that required to remove $O(r^2/\Delta)$ fraction of the edges.
  Our result is obtained by a new approach to relate the topological properties
(excluding a minor) of a graph to its geometric properties (the induced
shortest path metric). Specifically, we show that techniques used by Andreae in
his investigation of the cops-and-robbers game on excluded-minor graphs can be
used to construct padded decompositions of the metrics induced by such graphs.
In particular, we get probabilistic partitions with padding parameter $O(r)$
and strong-diameter partitions with padding parameter $O(r^2)$ for $K_r$-free
graphs, padding $O(k)$ for graphs with treewidth $k$, and padding $O(\log g)$
for graphs with genus $g$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3054</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3054</id><created>2013-11-13</created><updated>2015-11-24</updated><authors><author><keyname>Abboud</keyname><forenames>Amir</forenames></author><author><keyname>Lewi</keyname><forenames>Kevin</forenames></author><author><keyname>Williams</keyname><forenames>Ryan</forenames></author></authors><title>Losing Weight by Gaining Edges</title><categories>cs.CC cs.DS</categories><comments>Title of an earlier version of this paper: On the Parameterized
  Complexity of k-SUM</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new way to encode weighted sums into unweighted pairwise
constraints, obtaining the following results.
  - Define the k-SUM problem to be: given n integers in [-n^2k, n^2k] are there
k which sum to zero? (It is well known that the same problem over arbitrary
integers is equivalent to the above definition, by linear-time randomized
reductions.) We prove that this definition of k-SUM remains W[1]-hard, and is
in fact W[1]-complete: k-SUM can be reduced to f(k) * n^o(1) instances of
k-Clique.
  - The maximum node-weighted k-Clique and node-weighted k-dominating set
problems can be reduced to n^o(1) instances of the unweighted k-Clique and
k-dominating set problems, respectively. This implies a strong equivalence
between the time complexities of the node weighted problems and the unweighted
problems: any polynomial improvement on one would imply an improvement for the
other.
  - A triangle of weight 0 in a node weighted graph with m edges can be
deterministically found in m^1.41 time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3062</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3062</id><created>2013-11-13</created><authors><author><keyname>Emek</keyname><forenames>Yuval</forenames></author><author><keyname>Langner</keyname><forenames>Tobias</forenames></author><author><keyname>Uitto</keyname><forenames>Jara</forenames></author><author><keyname>Wattenhofer</keyname><forenames>Roger</forenames></author></authors><title>Ants: Mobile Finite State Machines</title><categories>cs.DC cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the Ants Nearby Treasure Search (ANTS) problem introduced by
Feinerman, Korman, Lotker, and Sereni (PODC 2012), where $n$ mobile agents,
initially placed at the origin of an infinite grid, collaboratively search for
an adversarially hidden treasure. In this paper, the model of Feinerman et al.
is adapted such that the agents are controlled by a (randomized) finite state
machine: they possess a constant-size memory and are able to communicate with
each other through constant-size messages. Despite the restriction to
constant-size memory, we show that their collaborative performance remains the
same by presenting a distributed algorithm that matches a lower bound
established by Feinerman et al. on the run-time of any ANTS algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3064</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3064</id><created>2013-11-13</created><updated>2014-05-09</updated><authors><author><keyname>Liao</keyname><forenames>Hao</forenames></author><author><keyname>Xiao</keyname><forenames>Rui</forenames></author><author><keyname>Cimini</keyname><forenames>Giulio</forenames></author><author><keyname>Medo</keyname><forenames>Matus</forenames></author></authors><title>Ranking users, papers and authors in online scientific communities</title><categories>cs.SI cs.DL cs.IR physics.soc-ph</categories><comments>7 pages, 3 figures, 3 tables</comments><journal-ref>PLoS ONE 9(12): e112022 (2014)</journal-ref><doi>10.1371/journal.pone.0097146</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ever-increasing quantity and complexity of scientific production have
made it difficult for researchers to keep track of advances in their own
fields. This, together with growing popularity of online scientific
communities, calls for the development of effective information filtering
tools. We propose here a method to simultaneously compute reputation of users
and quality of scientific artifacts in an online scientific community.
Evaluation on artificially-generated data and real data from the Econophysics
Forum is used to determine the method's best-performing variants. We show that
when the method is extended by considering author credit, its performance
improves on multiple levels. In particular, top papers have higher citation
count and top authors have higher $h$-index than top papers and top authors
chosen by other algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3070</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3070</id><created>2013-11-13</created><authors><author><keyname>Kahanwal</keyname><forenames>Dr. Brijender</forenames></author><author><keyname>Singh</keyname><forenames>Dr. T. P.</forenames></author></authors><title>The Distributed Computing Paradigms: P2P, Grid, Cluster, Cloud, and
  Jungle</title><categories>cs.DC</categories><comments>5 pages, 7 figures, journal</comments><journal-ref>International Journal of Latest Research in Science and
  Technology, Vol. 1, No. 2, pp. 183-187, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The distributed computing is done on many systems to solve a large scale
problem. The growing of high-speed broadband networks in developed and
developing countries, the continual increase in computing power, and the rapid
growth of the Internet have changed the way. In it the society manages
information and information services. Historically, the state of computing has
gone through a series of platform and environmental changes. Distributed
computing holds great assurance for using computer systems effectively. As a
result, supercomputer sites and data centers have changed from providing high
performance floating point computing capabilities to concurrently servicing
huge number of requests from billions of users. The distributed computing
system uses multiple computers to solve large-scale problems over the Internet.
It becomes data-intensive and network-centric. The applications of distributed
computing have become increasingly wide-spread. In distributed computing, the
main stress is on the large scale resource sharing and always goes for the best
performance. In this article, we have reviewed the work done in the area of
distributed computing paradigms. The main stress is on the evolving area of
cloud computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3071</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3071</id><created>2013-11-13</created><authors><author><keyname>Kahanwal</keyname><forenames>Dr. Brijender</forenames></author><author><keyname>Dua</keyname><forenames>Kanishak</forenames></author><author><keyname>Singh</keyname><forenames>Dr. Girish Pal</forenames></author></authors><title>Java File Security System (JFSS)</title><categories>cs.CR</categories><comments>5 pages, 7 figures, Journal</comments><journal-ref>Global Journal of Computer Science and Technology, Vol. 12(10),
  pp. 45-48, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, storage systems are increasingly subject to attacks. So the
security system is quickly becoming mendatory feature of the data storage
systems. For the security purpose we are always dependent on the cryptography
techniques. These techniques take the performance costs for the complete
system. So we have proposed the Java File Security System(JFSS). It is based on
the on-demand computing system concept, because of the performance issues. It
is a greate comback for the system performance. The concept is used because, we
are not always in need the secure the files, but the selected one only. In this
paper, we have designed a file security system on Windows XP. When we use the
operating system, we have to secure some important data. The date is always
stored in the files, so we secure the important files well. To check the
proposed functionality, we experiment the above said system on the Windows
operating system. With these experiments, we have found that the proposed
system is working properly, according to the needs of the users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3076</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3076</id><created>2013-11-13</created><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author><author><keyname>Vijayalakshmi</keyname><forenames>V. J.</forenames></author></authors><title>An Efficient Method for Recognizing the Low Quality Fingerprint
  Verification by Means of Cross Correlation</title><categories>cs.CV</categories><comments>7 pages and 5 figures</comments><doi>10.5121/ijci.2013.2501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an efficient method to provide personal
identification using fingerprint to get better accuracy even in noisy
condition. The fingerprint matching based on the number of corresponding
minutia pairings, has been in use for a long time, which is not very efficient
for recognizing the low quality fingerprints. To overcome this problem,
correlation technique is used. The correlation-based fingerprint verification
system is capable of dealing with low quality images from which no minutiae can
be extracted reliably and with fingerprints that suffer from non-uniform shape
distortions, also in case of damaged and partial images. Orientation Field
Methodology (OFM) has been used as a preprocessing module, and it converts the
images into a field pattern based on the direction of the ridges, loops and
bifurcations in the image of a fingerprint. The input image is then Cross
Correlated (CC) with all the images in the cluster and the highest correlated
image is taken as the output. The result gives a good recognition rate, as the
proposed scheme uses Cross Correlation of Field Orientation (CCFO = OFM + CC)
for fingerprint identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3078</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3078</id><created>2013-11-13</created><authors><author><keyname>Chamoun</keyname><forenames>Rima Kilany Maroun</forenames></author></authors><title>Smart: Semantically mashup rest web services</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mashup is a combination of information from more than one source, mixed up
in a way to create something new, or at least useful. Anyone can find mashups
on the internet, but these are always specifically designed for a predefined
purpose. To change that fact, we implemented a new platform we called the SMART
platform. SMART enables the user to make his own choices as for the REST web
services he needs to call in order to build an intelligent personalized mashup,
from a Google-like simple search interface, without needing any programming
skills. In order to achieve this goal, we defined an ontology that can hold
REST web services descriptions. These descriptions encapsulate mainly, the
input type needed for a service, its output type, and the kind of relation that
ties the input to the output. Then, by matching the user input query keywords,
with the REST web services definitions in our ontology, we can find registered
services individuals in this ontology, and construct the raw REST query for
each service found. The wrap up from the keywords, into semantic definitions,
in order to find the matching service individual, then the wrap down from the
semantic service description of the found individual, to the raw REST call, and
finally the wrap up of the result again into semantic individuals, is done for
two main purposes: the first to let the user use simple keywords in order to
build complex mashups, and the second to benefit from the ontology inference
engine in a way, where services instances can be tied together into an
intelligent mashup, simply by making each service output individuals, stand as
the next service input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3085</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3085</id><created>2013-11-13</created><authors><author><keyname>Massoulie</keyname><forenames>Laurent</forenames></author></authors><title>Community detection thresholds and the weak Ramanujan property</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decelle et al.\cite{Decelle11} conjectured the existence of a sharp threshold
for community detection in sparse random graphs drawn from the stochastic block
model. Mossel et al.\cite{Mossel12} established the negative part of the
conjecture, proving impossibility of meaningful detection below the threshold.
However the positive part of the conjecture remained elusive so far. Here we
solve the positive part of the conjecture. We introduce a modified adjacency
matrix $B$ that counts self-avoiding paths of a given length $\ell$ between
pairs of nodes and prove that for logarithmic $\ell$, the leading eigenvectors
of this modified matrix provide non-trivial detection, thereby settling the
conjecture. A key step in the proof consists in establishing a {\em weak
Ramanujan property} of matrix $B$. Namely, the spectrum of $B$ consists in two
leading eigenvalues $\rho(B)$, $\lambda_2$ and $n-2$ eigenvalues of a lower
order $O(n^{\epsilon}\sqrt{\rho(B)})$ for all $\epsilon&gt;0$, $\rho(B)$ denoting
$B$'s spectral radius. $d$-regular graphs are Ramanujan when their second
eigenvalue verifies $|\lambda|\le 2 \sqrt{d-1}$. Random $d$-regular graphs have
a second largest eigenvalue $\lambda$ of $2\sqrt{d-1}+o(1)$ (see
Friedman\cite{friedman08}), thus being {\em almost} Ramanujan.
Erd\H{o}s-R\'enyi graphs with average degree $d$ at least logarithmic
($d=\Omega(\log n)$) have a second eigenvalue of $O(\sqrt{d})$ (see Feige and
Ofek\cite{Feige05}), a slightly weaker version of the Ramanujan property.
However this spectrum separation property fails for sparse ($d=O(1)$)
Erd\H{o}s-R\'enyi graphs. Our result thus shows that by constructing matrix $B$
through neighborhood expansion, we regularize the original adjacency matrix to
eventually recover a weak form of the Ramanujan property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3087</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3087</id><created>2013-11-13</created><authors><author><keyname>Kuang</keyname><forenames>Li</forenames></author><author><keyname>Zheng</keyname><forenames>Bojin</forenames></author><author><keyname>Li</keyname><forenames>Deyi</forenames></author><author><keyname>Li</keyname><forenames>Yuanxiang</forenames></author><author><keyname>Sun</keyname><forenames>Yu</forenames></author></authors><title>A Fractal and Scale-free Model of Complex Networks with Hub Attraction
  Behaviors</title><categories>physics.soc-ph cs.SI</categories><comments>9 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is widely believed that fractality of complex networks origins from hub
repulsion behaviors (anticorrelation or disassortativity), which means large
degree nodes tend to connect with small degree nodes. This hypothesis was
demonstrated by a dynamical growth model, which evolves as the inverse
renormalization procedure proposed by Song et al. Now we find that the
dynamical growth model is based on the assumption that all the cross-boxes
links has the same probability e to link to the most connected nodes inside
each box. Therefore, we modify the growth model by adopting the flexible
probability e, which makes hubs have higher probability to connect with hubs
than non-hubs. With this model, we find some fractal and scale-free networks
have hub attraction behaviors (correlation or assortativity). The results are
the counter-examples of former beliefs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3088</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3088</id><created>2013-11-13</created><updated>2013-11-14</updated><authors><author><keyname>Turrini</keyname><forenames>Paolo</forenames></author></authors><title>Endogenous games with goals: side-payments among goal-directed
  artificial agents</title><categories>cs.GT</categories><comments>(under submission)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial agents are typically oriented to the realization of an externally
assigned task and try to optimize over secondary aspects of plan execution such
time lapse or power consumption, technically displaying a quasi-dichotomous
preference relation. Boolean games have been developed as a paradigm for
modelling societies of agents with this type of preference. In boolean games
agents exercise control over propositional variables and strive to achieve a
goal formula whose realization might require the opponents' cooperation.
Recently, a theory of incentive engineering for such games has been devised,
where an external authority steers the outcome of the game towards certain
desirable properties consistent with players' goals, by imposing a taxation
mechanism on the players that makes the outcomes that do not comply with those
properties less appealing to them. The present contribution stems from a
complementary perspective and studies, instead, how games with
quasi-dichotomous preferences can be transformed from inside, rather than from
outside, by endowing players with the possibility of sacrificing a part of
their payoff received at a certain outcome in order to convince other players
to play a certain strategy. Concretely we explore the properties of endogenous
games with goals, obtained coupling strategic games with goals, a
generalization of boolean games, with the machinery of endogenous games coming
from game theory. We analyze equilibria in those structures, showing the
preconditions needed for desirable outcomes to be achieved without external
intervention. What our results show is that endogenous games with goals display
specific irreducible features - with respect to what already known for
endogenous games - which makes them worth studying in their own sake.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3099</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3099</id><created>2013-11-13</created><authors><author><keyname>Vorugunti</keyname><forenames>Chandra Sekhar</forenames></author><author><keyname>Sarvabhatla</keyname><forenames>Mrudula</forenames></author></authors><title>Cryptanalysis of An Advanced Temporal Credential-Based Security Scheme
  with Mutual Authentication and Key Agreement for Wireless Sensor Networks</title><categories>cs.CR</categories><comments>We have cryptanalysed recently proposed Li et al. scheme</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid advancement of wireless network technology, usage of WSN in
real time applications like military, forest monitoring etc. found increasing.
Generally WSN operate in an unattended environment and handles critical data.
Authenticating the user trying to access the sensor memory is one of the
critical requirements. Many researchers have proposed remote user
authentication schemes focusing on various parameters. In 2013, Li et al.
proposed a temporal-credential-based mutual authentication and key agreement
scheme for WSNs. Li et al. claimed that their scheme is secure against all
major cryptographic attacks and requires less computation cost due to usage of
hash function instead encryption operations. Unfortunately, in this paper we
will show that their scheme is vulnerable to offline password guessing attack,
stolen smart card attack, leakage of password etc. and failure to provide data
privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3105</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3105</id><created>2013-11-13</created><authors><author><keyname>Fei</keyname><forenames>Jingjing</forenames></author><author><keyname>Wu</keyname><forenames>Hui</forenames></author><author><keyname>Wang</keyname><forenames>Yongxin</forenames></author></authors><title>k-DAG Based Lifetime Aware Data Collection in Wireless Sensor Networks</title><categories>cs.NI</categories><comments>17 pages, 10 figures</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  5, No. 5, October 2013, pp.17-33</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks need to be organized for efficient data collection
and lifetime maximization. In this paper, we propose a novel routing structure,
namely k-DAG, to balance the load of the base station's neighbours while
providing the worst-case latency guarantee for data collection, and a
distributed algorithm for construction a k-DAG based on a SPD (Shortest Path
DAG). In a k-DAG, the lengths of the longest path and the shortest path of each
sensor node to the base station differ by at most k. By adding sibling edges to
a SPD, our distributed algorithm allows critical nodes to have more routing
choices. The simulation results show that our approach significantly
outperforms the SPD-based data collection approach in both network lifetime and
load balance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3121</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3121</id><created>2013-11-13</created><updated>2013-12-06</updated><authors><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>Simple Tabulation, Fast Expanders, Double Tabulation, and High
  Independence</title><categories>cs.DS</categories><journal-ref>This paper was published in the Proceedings of the 54nd IEEE
  Symposium on Foundations of Computer Science (FOCS'13), pages 90-99, 2013.
  Copyright IEEE</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simple tabulation dates back to Zobrist in 1970. Keys are viewed as c
characters from some alphabet A. We initialize c tables h_0, ..., h_{c-1}
mapping characters to random hash values. A key x=(x_0, ..., x_{c-1}) is hashed
to h_0[x_0] xor...xor h_{c-1}[x_{c-1}]. The scheme is extremely fast when the
character hash tables h_i are in cache. Simple tabulation hashing is not
4-independent, but we show that if we apply it twice, then we get high
independence. First we hash to intermediate keys that are 6 times longer than
the original keys, and then we hash the intermediate keys to the final hash
values.
  The intermediate keys have d=6c characters from A. We can view the hash
function as a degree d bipartite graph with keys on one side, each with edges
to d output characters. We show that this graph has nice expansion properties,
and from that we get that with another level of simple tabulation on the
intermediate keys, the composition is a highly independent hash function. The
independence we get is |A|^{Omega(1/c)}.
  Our space is O(c|A|) and the hash function is evaluated in O(c) time. Siegel
[FOCS'89, SICOMP'04] proved that with this space, if the hash function is
evaluated in o(c) time, then the independence can only be o(c), so our
evaluation time is best possible for Omega(c) independence---our independence
is much higher if c=|A|^{o(1)}.
  Siegel used O(c)^c evaluation time to get the same independence with similar
space. Siegel's main focus was c=O(1), but we are exponentially faster when
c=omega(1).
  Applying our scheme recursively, we can increase our independence to
|A|^{Omega(1)} with o(c^{log c}) evaluation time. Compared with Siegel's scheme
this is both faster and higher independence.
  Our scheme is easy to implement, and it does provide realistic
implementations of 100-independent hashing for, say, 32 and 64-bit keys.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3123</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3123</id><created>2013-11-13</created><authors><author><keyname>Nasser</keyname><forenames>Rajai</forenames></author><author><keyname>Telatar</keyname><forenames>Emre</forenames></author></authors><title>Polar Codes for Arbitrary DMCs and Arbitrary MACs</title><categories>cs.IT math.IT</categories><comments>32 pages, 1 figure. arXiv admin note: text overlap with
  arXiv:1112.1770</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are constructed for arbitrary channels by imposing an arbitrary
quasigroup structure on the input alphabet. Just as with &quot;usual&quot; polar codes,
the block error probability under successive cancellation decoding is
$o(2^{-N^{1/2-\epsilon}})$, where $N$ is the block length. Encoding and
decoding for these codes can be implemented with a complexity of $O(N\log N)$.
It is shown that the same technique can be used to construct polar codes for
arbitrary multiple access channels (MAC) by using an appropriate Abelian group
structure. Although the symmetric sum capacity is achieved by this coding
scheme, some points in the symmetric capacity region may not be achieved. In
the case where the channel is a combination of linear channels, we provide a
necessary and sufficient condition characterizing the channels whose symmetric
capacity region is preserved by the polarization process. We also provide a
sufficient condition for having a maximal loss in the dominant face.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3130</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3130</id><created>2013-11-13</created><authors><author><keyname>Parvatham</keyname><forenames>Niranjan Kumar</forenames></author></authors><title>Impact of Indentation in Programming</title><categories>cs.PL cs.HC</categories><journal-ref>International Journal of Programming Languages and Applications (
  IJPLA ) Vol.3, No.4, October 2013</journal-ref><doi>10.5121/ijpla.2013.3403</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computer programming languages, indentation formats program source code to
improve readability. Programming languages make use of indentation to define
program structure .Programmers use indentation to understand the structure of
their programs to human readers. Especially, indentation is the better way to
represent the relationship between control flow constructs such as selection
statements or loops and code contained within and outside them. This paper
describes about different indentation styles used in Programming and also
describes context of each indentation style. It also describes indentation
styles used for various programming constructs and the best practice for a
particular programming construct. This paper helps the beginners to understand
various indentation styles used in programming and also to choose suitable
indentation style.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3139</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3139</id><created>2013-11-13</created><authors><author><keyname>Ostert&#xe1;g</keyname><forenames>Richard</forenames></author><author><keyname>Stanek</keyname><forenames>Martin</forenames></author></authors><title>Entropy Assessment of Windows OS Performance Counters</title><categories>cs.CR</categories><comments>11 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The security of many cryptographic constructions depends on random number
generators for providing unpredictable keys, nonces, initialization vectors and
other parameters. Modern operating systems implement cryptographic
pseudo-random number generators (PRNGs) to fulfill this need. Performance
counters and other system parameters are often used as a low-entropy source to
initialize (seed) the generators. We perform an experiment to analyze all
performance counters in standard installation of Microsoft Windows 7 operating
system, and assess their suitability as entropy sources. Besides selecting top
19 counters, we analyze their mutual information (independence) as well as
robustness in the virtual environment. Final selection contains 14 counters
with sufficient overall entropy for practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3141</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3141</id><created>2013-11-13</created><updated>2015-02-17</updated><authors><author><keyname>Pappi</keyname><forenames>Koralia N.</forenames></author><author><keyname>Diamantoulakis</keyname><forenames>Panagiotis D.</forenames></author><author><keyname>Otrok</keyname><forenames>Hadi</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Cloud Compute-and-Forward with Relay Cooperation</title><categories>cs.IT cs.GT cs.NI math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a cloud network with M distributed receiving antennas and L users,
which transmit their messages towards a centralized decoder (CD), where M&gt;=L.
We consider that the cloud network applies the Compute-and-Forward (C&amp;F)
protocol, where L antennas/relays are selected to decode integer equations of
the transmitted messages. In this work, we focus on the best relay selection
and the optimization of the Physical-Layer Network Coding (PNC) at the relays,
aiming at the throughput maximization of the network. Existing literature
optimizes PNC with respect to the maximization of the minimum rate among users.
The proposed strategy maximizes the sum rate of the users allowing nonsymmetric
rates, while the optimal solution is explored with the aid of the Pareto
frontier. The problem of relay selection is matched to a coalition formation
game, where the relays and the CD cooperate in order to maximize their profit.
Efficient coalition formation algorithms are proposed, which perform joint
relay selection and PNC optimization. Simulation results show that a
considerable improvement is achieved compared to existing results, both in
terms of the network sum rate and the players' profits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3144</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3144</id><created>2013-11-13</created><updated>2015-02-03</updated><authors><author><keyname>Buluc</keyname><forenames>Aydin</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author><author><keyname>Safro</keyname><forenames>Ilya</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author></authors><title>Recent Advances in Graph Partitioning</title><categories>cs.DS cs.DC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey recent trends in practical algorithms for balanced graph
partitioning together with applications and future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3149</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3149</id><created>2013-08-20</created><authors><author><keyname>Brass</keyname><forenames>Peter</forenames></author><author><keyname>Na</keyname><forenames>Hyeon-Suk</forenames></author><author><keyname>Shin</keyname><forenames>Chan-Su</forenames></author></authors><title>Local Event Boundary Detection with Unreliable Sensors: Analysis of the
  Majority Vote Scheme</title><categories>cs.CG cs.NI</categories><comments>20 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the identification of an event region $X$ within a
larger region $Y$, in which the sensors are distributed by a Poisson process of
density $\lambda$ to detect this event region, i.e., its boundary. The model of
sensor is a 0-1 sensor that decides whether it lies in $X$ or not, and which
might be incorrect with probability $p$. It also collects information on the
0-1 values of the neighbors within some distance $r$ and revises its decision
by the majority vote of these neighbors. In the most general setting, we
analyze this simple majority vote scheme and derive some upper and lower bounds
on the expected number of misclassified sensors. These bounds depend on several
sensing parameters of $p$, $r$, and some geometric parameters of the event
region $X$. By making some assumptions on the shape of $X$, we prove a
significantly improved upper bound on the expected number of misclassified
sensors; especially for convex regions with sufficiently round boundary, and we
find that the majority vote scheme performs well in the simulation rather than
its theoretical upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3151</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3151</id><created>2013-11-13</created><authors><author><keyname>Backes</keyname><forenames>Michael</forenames></author><author><keyname>Clark</keyname><forenames>Jeremy</forenames></author><author><keyname>Druschel</keyname><forenames>Peter</forenames></author><author><keyname>Kate</keyname><forenames>Aniket</forenames></author><author><keyname>Simeonovski</keyname><forenames>Milivoj</forenames></author></authors><title>Introducing Accountability to Anonymity Networks</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many anonymous communication (AC) networks rely on routing traffic through
proxy nodes to obfuscate the originator of the traffic. Without an
accountability mechanism, exit proxy nodes risk sanctions by law enforcement if
users commit illegal actions through the AC network. We present BackRef, a
generic mechanism for AC networks that provides practical repudiation for the
proxy nodes by tracing back the selected outbound traffic to the predecessor
node (but not in the forward direction) through a cryptographically verifiable
chain. It also provides an option for full (or partial) traceability back to
the entry node or even to the corresponding user when all intermediate nodes
are cooperating. Moreover, to maintain a good balance between anonymity and
accountability, the protocol incorporates whitelist directories at exit proxy
nodes. BackRef offers improved deployability over the related work, and
introduces a novel concept of pseudonymous signatures that may be of
independent interest.
  We exemplify the utility of BackRef by integrating it into the onion routing
(OR) protocol, and examine its deployability by considering several
system-level aspects. We also present the security definitions for the BackRef
system (namely, anonymity, backward traceability, no forward traceability, and
no false accusation) and conduct a formal security analysis of the OR protocol
with BackRef using ProVerif, an automated cryptographic protocol verifier,
establishing the aforementioned security properties against a strong
adversarial model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3157</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3157</id><created>2013-11-12</created><authors><author><keyname>Ye</keyname><forenames>Jianbo</forenames></author></authors><title>Multiple Closed-Form Local Metric Learning for K-Nearest Neighbor
  Classifier</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many researches have been devoted to learn a Mahalanobis distance metric,
which can effectively improve the performance of kNN classification. Most
approaches are iterative and computational expensive and linear rigidity still
critically limits metric learning algorithm to perform better. We proposed a
computational economical framework to learn multiple metrics in closed-form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3158</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3158</id><created>2013-11-13</created><updated>2015-10-06</updated><authors><author><keyname>Bun</keyname><forenames>Mark</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author><author><keyname>Vadhan</keyname><forenames>Salil</forenames></author></authors><title>Fingerprinting Codes and the Price of Approximate Differential Privacy</title><categories>cs.CR</categories><comments>Full version of our STOC 2014 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show new lower bounds on the sample complexity of $(\varepsilon,
\delta)$-differentially private algorithms that accurately answer large sets of
counting queries. A counting query on a database $D \in (\{0,1\}^d)^n$ has the
form &quot;What fraction of the individual records in the database satisfy the
property $q$?&quot; We show that in order to answer an arbitrary set $\mathcal{Q}$
of $\gg nd$ counting queries on $D$ to within error $\pm \alpha$ it is
necessary that $$ n \geq \tilde{\Omega}\Bigg(\frac{\sqrt{d} \log
|\mathcal{Q}|}{\alpha^2 \varepsilon} \Bigg). $$ This bound is optimal up to
poly-logarithmic factors, as demonstrated by the Private Multiplicative Weights
algorithm (Hardt and Rothblum, FOCS'10). In particular, our lower bound is the
first to show that the sample complexity required for accuracy and
$(\varepsilon, \delta)$-differential privacy is asymptotically larger than what
is required merely for accuracy, which is $O(\log |\mathcal{Q}| / \alpha^2)$.
In addition, we show that our lower bound holds for the specific case of
$k$-way marginal queries (where $|\mathcal{Q}| = 2^k \binom{d}{k}$) when
$\alpha$ is not too small compared to $d$ (e.g. when $\alpha$ is any fixed
constant).
  Our results rely on the existence of short \emph{fingerprinting codes} (Boneh
and Shaw, CRYPTO'95, Tardos, STOC'03), which we show are closely connected to
the sample complexity of differentially private data release. We also give a
new method for combining certain types of sample complexity lower bounds into
stronger lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3171</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3171</id><created>2013-11-13</created><updated>2014-04-07</updated><authors><author><keyname>Jahanjou</keyname><forenames>Hamid</forenames></author><author><keyname>Miles</keyname><forenames>Eric</forenames></author><author><keyname>Viola</keyname><forenames>Emanuele</forenames></author></authors><title>Local reductions</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reduce non-deterministic time $T \ge 2^n$ to a 3SAT instance $\phi$ of
quasilinear size $|\phi| = T \cdot \log^{O(1)} T$ such that there is an
explicit circuit $C$ that on input an index $i$ of $\log |\phi|$ bits outputs
the $i$th clause, and each output bit of $C$ depends on $O(1)$ input bits. The
previous best result was $C$ in NC$^1$. Even in the simpler setting of
polynomial size $|\phi| = \poly(T)$ the previous best result was $C$ in AC$^0$.
  More generally, for any time $T \ge n$ and parameter $r \leq n$ we obtain
$\log_2 |\phi| = \max(\log T, n/r) + O(\log n) + O(\log\log T)$ and each output
bit of $C$ is a decision tree of depth $O(\log r)$.
  As an application, we tighten Williams' connection between satisfiability
algorithms and circuit lower bounds (STOC 2010; SIAM J. Comput. 2013).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3172</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3172</id><created>2013-11-13</created><authors><author><keyname>Akhtar</keyname><forenames>Md. Amir Khusru</forenames></author><author><keyname>Sahoo</keyname><forenames>G.</forenames></author></authors><title>The principles of humanism for MANETs</title><categories>cs.NI</categories><comments>14 pages, 7 figures, 10 tables</comments><journal-ref>International Journal of Computer Science and Information
  Technology (IJCSIT), Vol. 5, No. 5, pp. 147-160, Oct 2013</journal-ref><doi>10.5121/ijcsit.2013.5511</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proposed humanistic approach mapped the human character and behavior into
a device to evade the bondages of implementation and surely succeed as we live.
Human societies are the complex and most organized networks, in which many
communities having different cultural livelihood. The formation of communities
within a society and the way of associations can be mapped to MANET. In this
work we have presented the principles of humanism for MANETs. The proposed
approach is not only robust and secure but it certainly meets the existing
challenges (such as name resolution, address allocation and authentication).
Its object oriented design defines a service in terms of Arts, Culture, and
Machine. An 'Art' is the smallest unit of work (defined as an interface), the
'Culture' is the integration and implementation of one or more Arts (defined as
a class) and finally the 'Machine' which is an instance of a Culture that
defines a running service. The grouping of all communicable Machines having the
same Culture forms a 'Community'. We have used the term 'Society' for a MANET
having one or more communities and modeled using the humanistic approach. The
proposed approach is compared with GloMoSim and the implementation of file
transfer service is presented using the said approach. Our approach is better
in terms of implementation of the basic services, security, reliability,
throughput, extensibility, scalability etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3175</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3175</id><created>2013-11-13</created><authors><author><keyname>M.</keyname><forenames>Athira P.</forenames></author><author><keyname>M.</keyname><forenames>Sreeja</forenames></author><author><keyname>Raj</keyname><forenames>P. C. Reghu</forenames></author></authors><title>Architecture of an Ontology-Based Domain-Specific Natural Language
  Question Answering System</title><categories>cs.CL cs.IR</categories><journal-ref>International Journal of Web &amp; Semantic Technology (IJWesT) Vol.4,
  No.4, October 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Question answering (QA) system aims at retrieving precise information from a
large collection of documents against a query. This paper describes the
architecture of a Natural Language Question Answering (NLQA) system for a
specific domain based on the ontological information, a step towards semantic
web question answering. The proposed architecture defines four basic modules
suitable for enhancing current QA capabilities with the ability of processing
complex questions. The first module was the question processing, which analyses
and classifies the question and also reformulates the user query. The second
module allows the process of retrieving the relevant documents. The next module
processes the retrieved documents, and the last module performs the extraction
and generation of a response. Natural language processing techniques are used
for processing the question and documents and also for answer extraction.
Ontology and domain knowledge are used for reformulating queries and
identifying the relations. The aim of the system is to generate short and
specific answer to the question that is asked in the natural language in a
specific domain. We have achieved 94 % accuracy of natural language question
answering in our implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3181</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3181</id><created>2013-11-13</created><authors><author><keyname>Ghosh</keyname><forenames>Sutanu</forenames><affiliation>Electronics and Communication Engineering, Dr. Sudhir Chandra Sur Degree Engineering College</affiliation></author></authors><title>Comparative Study of Various VOIP Applications in 802.11a Wireless
  Network Scenario</title><categories>cs.NI</categories><comments>No. of Pages - 11 pages, No. of figures - 10 figures and journal
  paper</comments><journal-ref>International Journal of Mobile Network Communications &amp;
  Telematics ( IJMNCT) Vol. 3, No.5, October 2013</journal-ref><doi>10.5121/ijmnct.2013.3504</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Today, Voice over Wireless Local Area Network (VOWLAN) is the most accepted
Internet application. There are a large number of literatures regarding the
performance of various WLAN networks. Most of them focus on simulations and
modeling, but there are also some experiments with real networks. This paper
explains the comparison of performance of two different VOIP (Voice over
Internet Protocol) applications over the same IEEE 802.11a wireless network.
Radio link standard 802.11a have maximum transmission rate of 54Mbps. First
protocol is session initiation protocol (SIP) and second is H.323 protocol.
First one has an agent called SIP proxy. Second have a gateway reflects the
characteristics of a Switched Circuit Network (SCN). With this comparison we
have required to obtain a better understanding of wireless network suitability
for voice communication in IP network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3184</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3184</id><created>2013-11-13</created><authors><author><keyname>Ghosh</keyname><forenames>Sutanu</forenames></author></authors><title>Comparative Study of QOS Parameters of SIP protocol in 802.11a and
  802.11b Network</title><categories>cs.NI</categories><comments>No. of pages - 10 pages, no. of figures - 10 figures and journal
  paper</comments><journal-ref>International Journal of Mobile Network Communications &amp;
  Telematics (IJMNCT) Vol.2, No.6, December 2012</journal-ref><doi>10.5121/ijmnct.2012.2603</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Present day, the internet telephony growth is much faster than previous. Now
we are familiar with digitized packet of voice stream. So, we have required
VOIP communication. SIP is one type of VOIP protocol. This one has a SIP proxy.
There have one of the important communication environment Wireless LAN (WLAN).
WLAN have different radio link standard. Here I am comparing SIP protocol in
two radio link standard 802.11a and 802.11b environment. The first one have
maximum transmission rate of 54Mbps and second one have maximum transmission
rate of 11Mbps. In this paper I want to show the results in a comparative plot.
These comparisons include server /client throughput, packet drops, end to end
delay etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3192</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3192</id><created>2013-11-13</created><authors><author><keyname>Pas</keyname><forenames>Andreas ten</forenames></author><author><keyname>Platt</keyname><forenames>Robert</forenames></author></authors><title>Localizing Grasp Affordances in 3-D Points Clouds Using Taubin Quadric
  Fitting</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Perception-for-grasping is a challenging problem in robotics. Inexpensive
range sensors such as the Microsoft Kinect provide sensing capabilities that
have given new life to the effort of developing robust and accurate perception
methods for robot grasping. This paper proposes a new approach to localizing
enveloping grasp affordances in 3-D point clouds efficiently. The approach is
based on modeling enveloping grasp affordances as a cylindrical shells that
corresponds to the geometry of the robot hand. A fast and accurate fitting
method for quadratic surfaces is the core of our approach. An evaluation on a
set of cluttered environments shows high precision and recall statistics. Our
results also show that the approach compares favorably with some alternatives,
and that it is efficient enough to be employed for robot grasping in real-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3195</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3195</id><created>2013-11-13</created><authors><author><keyname>Larsen</keyname><forenames>Kim G.</forenames><affiliation>Aalborg University</affiliation></author><author><keyname>Legay</keyname><forenames>Axel</forenames><affiliation>INRIA Rennes</affiliation></author><author><keyname>Nyman</keyname><forenames>Ulrik</forenames><affiliation>Aalborg University</affiliation></author></authors><title>Proceedings 1st Workshop on Advances in Systems of Systems</title><categories>cs.SE cs.DC</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 133, 2013</journal-ref><doi>10.4204/EPTCS.133</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the first workshop on Advances in
Systems of Systems (AISOS'13), held in Roma, Italy, March 16. System-of-Systems
describes the large scale integration of many independent self-contained
systems to satisfy global needs or multi-system requests. Examples are smart
grid, intelligent buildings, smart cities, transport systems, etc. There is a
need for new modeling formalisms, analysis methods and tools to help make
trade-off decisions during design and evolution avoiding leading to sub-optimal
design and rework during integration and in service. The workshop should focus
on the modeling and analysis of System of Systems. AISOS'13 aims to gather
people from different communities in order to encourage exchange of methods and
views.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3198</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3198</id><created>2013-11-13</created><authors><author><keyname>K&#xf6;nig</keyname><forenames>M&#xe9;lanie</forenames></author><author><keyname>Lecl&#xe8;re</keyname><forenames>Michel</forenames></author><author><keyname>Mugnier</keyname><forenames>Marie-Laure</forenames></author><author><keyname>Thomazo</keyname><forenames>Micha&#xeb;l</forenames></author></authors><title>Sound, Complete and Minimal UCQ-Rewriting for Existential Rules</title><categories>cs.AI cs.LO</categories><comments>29 pages</comments><report-no>Report-no: LIRMM-RR-13034</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the issue of Ontology-Based Data Access, with ontologies
represented in the framework of existential rules, also known as Datalog+/-. A
well-known approach involves rewriting the query using ontological knowledge.
We focus here on the basic rewriting technique which consists of rewriting the
initial query into a union of conjunctive queries. First, we study a generic
breadth-first rewriting algorithm, which takes as input any rewriting operator,
and define properties of rewriting operators that ensure the correctness of the
algorithm. Then, we focus on piece-unifiers, which provide a rewriting operator
with the desired properties. Finally, we propose an implementation of this
framework and report some experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3200</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3200</id><created>2013-11-13</created><updated>2013-11-15</updated><authors><author><keyname>Alistarh</keyname><forenames>Dan</forenames></author><author><keyname>Censor-Hillel</keyname><forenames>Keren</forenames></author><author><keyname>Shavit</keyname><forenames>Nir</forenames></author></authors><title>Are Lock-Free Concurrent Algorithms Practically Wait-Free?</title><categories>cs.DC</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lock-free concurrent algorithms guarantee that some concurrent operation will
always make progress in a finite number of steps. Yet programmers prefer to
treat concurrent code as if it were wait-free, guaranteeing that all operations
always make progress. Unfortunately, designing wait-free algorithms is
generally a very complex task, and the resulting algorithms are not always
efficient. While obtaining efficient wait-free algorithms has been a long-time
goal for the theory community, most non-blocking commercial code is only
lock-free.
  This paper suggests a simple solution to this problem. We show that, for a
large class of lock- free algorithms, under scheduling conditions which
approximate those found in commercial hardware architectures, lock-free
algorithms behave as if they are wait-free. In other words, programmers can
keep on designing simple lock-free algorithms instead of complex wait-free
ones, and in practice, they will get wait-free progress.
  Our main contribution is a new way of analyzing a general class of lock-free
algorithms under a stochastic scheduler. Our analysis relates the individual
performance of processes with the global performance of the system using Markov
chain lifting between a complex per-process chain and a simpler system progress
chain. We show that lock-free algorithms are not only wait-free with
probability 1, but that in fact a general subset of lock-free algorithms can be
closely bounded in terms of the average number of steps required until an
operation completes.
  To the best of our knowledge, this is the first attempt to analyze progress
conditions, typically stated in relation to a worst case adversary, in a
stochastic model capturing their expected asymptotic behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3211</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3211</id><created>2013-11-13</created><authors><author><keyname>Petrovici</keyname><forenames>Mihai A.</forenames></author><author><keyname>Bill</keyname><forenames>Johannes</forenames></author><author><keyname>Bytschok</keyname><forenames>Ilja</forenames></author><author><keyname>Schemmel</keyname><forenames>Johannes</forenames></author><author><keyname>Meier</keyname><forenames>Karlheinz</forenames></author></authors><title>Stochastic inference with deterministic spiking neurons</title><categories>q-bio.NC cond-mat.dis-nn cs.NE physics.bio-ph</categories><comments>6 pages, 4 figures</comments><msc-class>92-08</msc-class><acm-class>C.1.3; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The seemingly stochastic transient dynamics of neocortical circuits observed
in vivo have been hypothesized to represent a signature of ongoing stochastic
inference. In vitro neurons, on the other hand, exhibit a highly deterministic
response to various types of stimulation. We show that an ensemble of
deterministic leaky integrate-and-fire neurons embedded in a spiking noisy
environment can attain the correct firing statistics in order to sample from a
well-defined target distribution. We provide an analytical derivation of the
activation function on the single cell level; for recurrent networks, we
examine convergence towards stationarity in computer simulations and
demonstrate sample-based Bayesian inference in a mixed graphical model. This
establishes a rigorous link between deterministic neuron models and functional
stochastic dynamics on the network level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3220</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3220</id><created>2013-11-13</created><authors><author><keyname>Pande</keyname><forenames>Gaurav</forenames></author></authors><title>Chaotic Arithmetic Coding for Secure Video Multicast</title><categories>cs.MM cs.CR cs.IT math.IT</categories><comments>Submitted to SPIN 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arithmetic Coding (AC) is widely used for the entropy coding of text and
video data. It involves recursive partitioning of the range [0,1) in accordance
with the relative probabilities of occurrence of the input symbols. A data
(image or video) encryption scheme based on arithmetic coding called as Chaotic
Arithmetic Coding (CAC) has been presented in previous works. In CAC, a large
number of chaotic maps can be used to perform coding, each achieving Shannon
optimal compression performance. The exact choice of map is governed by a key.
CAC has the effect of scrambling the intervals without making any changes to
the width of interval in which the codeword must lie, thereby allowing
encryption without sacrificing any coding efficiency. In this paper, we use a
redundancy in CAC procedure for secure multicast of videos where multiple users
are distributed with different keys to decode same encrypted file. By
encrypting once, we can generate multiple keys, either of which can be used to
decrypt the encoded file. This is very suitable for video distribution over
Internet where a single video can be distributed to multiple clients in a
privacy preserving manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3226</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3226</id><created>2013-11-13</created><authors><author><keyname>Clark</keyname><forenames>Andrew</forenames></author><author><keyname>Pande</keyname><forenames>Amit</forenames></author><author><keyname>Govindan</keyname><forenames>Kannan</forenames></author><author><keyname>Poovendran</keyname><forenames>Radha</forenames></author><author><keyname>Mohapatra</keyname><forenames>Prasant</forenames></author></authors><title>Using Social Information for Flow Allocation in MANETs</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adhoc networks enable communication between distributed, mobile wireless
nodes without any supporting infrastructure. In the absence of centralized
control, such networks require node interaction, and are inherently based on
cooperation between nodes. In this paper, we use social and behavioral trust of
nodes to form a flow allocation optimization problem. We initialize trust using
information gained from users' social relationships (from social networks) and
update the trusts metric over time based on observed node behaviors. We conduct
analysis of social trust using real data sets and used it as a parameter for
performance evaluation of our frame work in ns-3. Based on our approach we
obtain a significant improvement in both detection rate and packet delivery
ratio using social trust information when compared to behavioral trust alone.
Further, we observe that social trust is critical in the event of mobility and
plays a crucial role in bootstrapping the computation of trust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3231</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3231</id><created>2013-11-12</created><authors><author><keyname>Gahi</keyname><forenames>Youssef</forenames></author><author><keyname>Lamrani</keyname><forenames>Meryem</forenames></author><author><keyname>Guennoun</keyname><forenames>Mouhcine</forenames></author><author><keyname>El-Khatib</keyname><forenames>Khalil</forenames></author></authors><title>Infrastructure Logicielle d un Environnement Hospitalier Intelligent</title><categories>cs.CY</categories><comments>50 pages. Master thesis 2008</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The impact of new technologies in the field of healthcare has been proven to
be satisfactory in improving the quality of care and services for patients. The
hospital environment is undoubtedly one of the environments where human or
hardware errors can cause harmful damage. It is therefore very useful for the
sector to be at the forefront of technology and use the strongest and most
efficient means since it is supposed to be an environment of reliability, trust
and is legally responsible for ensuring the confidentiality of all patient
information. In this project we have proposed and developed a smart hospital
enviornement that allows both, detecting patien fall and send alerts to the
suitable parties, and securing patien stored data through RFID technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3234</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3234</id><created>2013-11-13</created><updated>2013-11-21</updated><authors><author><keyname>Berec</keyname><forenames>Vesna</forenames></author><author><keyname>Braunstein</keyname><forenames>J. S.</forenames></author></authors><title>Spin polarized induction of quantum correlations-entanglement using a 2
  MeV proton beam channeling</title><categories>quant-ph cs.ET hep-lat</categories><comments>8 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In solid_state hybrid electron_nuclear spin systems quantum entanglement
plays vital role in allowing accessible transfer of information between
subatomic particles, regardless of the host lattice coordination spatial
geometry, revealing the powerful resource for nuclear quantum states
engineering. Here we present study of 2 MeV superfocused channeled proton (SCP)
beam induced polarization of atom_photon correlated states, established in
isotopically purified silicon nanocrystal. Two level entangling interaction
which couples an initial quantum state to two possible light_matter states via
silicon nanocrystal interface is presented. The anisotropic hyperfine coupling
is demonstrated by strong mixing of quantum states within the control mechanism
of the coherent proton pulse sequence. Obtained results reveal the mutual
predictable correlation of particles of energy_matter, by using the fully
broadcastable and precise hybrid electron_nuclear spin qubit manipulations
which can be exploited for the speed_superior communication channels keeping at
the same time the maximum degree of data preservation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3238</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3238</id><created>2013-11-13</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Doyen</keyname><forenames>Laurent</forenames></author><author><keyname>Filiot</keyname><forenames>Emmanuel</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Doomsday Equilibria for Omega-Regular Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-player games on graphs provide the theoretical frame- work for many
important problems such as reactive synthesis. While the traditional study of
two-player zero-sum games has been extended to multi-player games with several
notions of equilibria, they are decidable only for perfect-information games,
whereas several applications require imperfect-information games. In this paper
we propose a new notion of equilibria, called doomsday equilibria, which is a
strategy profile such that all players satisfy their own objective, and if any
coalition of players deviates and violates even one of the players objective,
then the objective of every player is violated. We present algorithms and
complexity results for deciding the existence of doomsday equilibria for
various classes of omega-regular objectives, both for imperfect-information
games, and for perfect-information games. We provide optimal complexity bounds
for imperfect-information games, and in most cases for perfect-information
games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3243</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3243</id><created>2013-11-13</created><authors><author><keyname>Younis</keyname><forenames>Ola</forenames></author><author><keyname>Ghoul</keyname><forenames>Said</forenames></author><author><keyname>Alomari</keyname><forenames>Mohammad H.</forenames></author></authors><title>Systems Variability Modeling: A Textual Model Mixing Class and Feature
  Concepts</title><categories>cs.SE</categories><comments>10 pages</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 5, No 5, October 2013</journal-ref><doi>10.5121/ijcsit.2013.5509</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  System reuse and cost are very important in software product line design
area. Developers goal is to increase system reuse and decreasing cost and
efforts for building components from scratch for each software configuration.
This can be reached by developing Software Product Line (SPL). To handle SPL
engineering process, several approaches with several techniques were developed.
One of these approaches is called separated approach. It requires separating
the commonalities and variability for system components to allow configuration
selection based on user defined features. Textual notation-based approaches
have been used for their formal syntax and semantics to represent system
features and implementations. But these approaches are still weak in mixing
features (conceptual level) and classes (physical level) that guarantee smooth
and automatic configuration generation for software releases. The absence of
methodology supporting the mixing process is a real weakness. In this paper, we
enhanced SPL reuse by introducing some meta-features, classified according to
their functions. As a first consequence, mixing class and feature concepts is
supported in a simple way using class interfaces and inherent features for
smooth move from feature model to class model. And as a second consequence, the
mixing process is supported by a textual design and implementation methodology,
mixing class and feature models by combining their concepts in a single
language. The supported configuration generation process is simple, coherent,
and complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3268</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3268</id><created>2013-11-13</created><authors><author><keyname>Agarwal</keyname><forenames>Naman</forenames></author><author><keyname>Kolla</keyname><forenames>Alexandra</forenames></author><author><keyname>Madan</keyname><forenames>Vivek</forenames></author></authors><title>Small Lifts of Expander Graphs are Expanding</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A k-lift of an n-vertex base-graph G is a graph H on $n \times k$ vertices,
where each vertex of G is replaced by k vertices and each edge (u,v) in G is
replaced by a matching representing a bijection $\pi_{uv}$ so that the edges of
H are of the form $\big ((u,i),(v,\pi_{uv}(i))\big )$. H s a (uniformly) random
lift of G if for every edge (u,v) the bijection $\pi_{uv}$ is chosen uniformly
and independently at random. The main motivation for studying lifts has been
understanding Ramanujan expander graphs via two key questions: Is a ``typical''
lift of an expander graph also an expander; and how can we (efficiently)
construct Ramanujan expanders using lifts? Lately, there has been an increased
interest in lifts and their close relation to the notorious Unique Games
Conjecture \cite{Kho02}.
  In this paper, we analyze the spectrum of random k-lifts of d-regular graphs.
We show that, for random shift $k$-lifts, if all the nontrivial eigenvalues of
the base graph G are at most $\lambda$ in absolute value, then with high
probability depending only on the number n of nodes of G (and not on k), the
absolute value of every nontrivial eigenvalue of the lift is at most
$O(\lambda)$. Moreover, if G is moderately expanding, then this bound can be
improved to $\lambda+ O(\sqrt{d})$. While previous results on random lifts were
asymptotically true with high probability in the degree of the lift k, our
result is the first upperbound on spectra of lifts for bounded k. In
particular, it implies that a typical small lift of a Ramanujan graph is almost
Ramanujan, and we believe it will prove crucial in constructing large Ramanujan
expanders of all degrees. We also establish a novel characterization of the
spectrum of shift lifts by the spectrum of certain $k$ symmetric matrices, that
generalize the signed adjacency matrix (see \cite{BL06}). We believe this
characterization is of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3269</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3269</id><created>2013-11-13</created><authors><author><keyname>Galiano</keyname><forenames>Gonzalo</forenames></author><author><keyname>Velasco</keyname><forenames>Juli&#xe1;n</forenames></author></authors><title>On a non-local spectrogram for denoising one-dimensional signals</title><categories>cs.CV</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous works, we investigated the use of local filters based on partial
differential equations (PDE) to denoise one-dimensional signals through the
image processing of time-frequency representations, such as the spectrogram. In
this image denoising algorithms, the particularity of the image was hardly
taken into account. We turn, in this paper, to study the performance of
non-local filters, like Neighborhood or Yaroslavsky filters, in the same
problem. We show that, for certain iterative schemes involving the Neighborhood
filter, the computational time is drastically reduced with respect to
Yaroslavsky or nonlinear PDE based filters, while the outputs of the filtering
processes are similar. This is heuristically justified by the connection
between the (fast) Neighborhood filter applied to a spectrogram and the
corresponding Nonlocal Means filter (accurate) applied to the Wigner-Ville
distribution of the signal. This correspondence holds only for time-frequency
representations of one-dimensional signals, not to usual images, and in this
sense the particularity of the image is exploited. We compare though a series
of experiments on synthetic and biomedical signals the performance of local and
non-local filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3284</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3284</id><created>2013-11-13</created><updated>2014-07-11</updated><authors><author><keyname>Tamo</keyname><forenames>Itzhak</forenames></author><author><keyname>Barg</keyname><forenames>Alexander</forenames></author></authors><title>A family of optimal locally recoverable codes</title><categories>cs.IT math.IT</categories><comments>Minor changes. This is the final published version of the paper</comments><journal-ref>IEEE Transactions on Information Theory, vol. 60, no. 8, 2014</journal-ref><doi>10.1109/TIT.2014.2321280</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A code over a finite alphabet is called locally recoverable (LRC) if every
symbol in the encoding is a function of a small number (at most $r$) other
symbols. We present a family of LRC codes that attain the maximum possible
value of the distance for a given locality parameter and code cardinality. The
codewords are obtained as evaluations of specially constructed polynomials over
a finite field, and reduce to a Reed-Solomon code if the locality parameter $r$
is set to be equal to the code dimension. The size of the code alphabet for
most parameters is only slightly greater than the code length. The recovery
procedure is performed by polynomial interpolation over $r$ points. We also
construct codes with several disjoint recovering sets for every symbol. This
construction enables the system to conduct several independent and simultaneous
recovery processes of a specific symbol by accessing different parts of the
codeword. This property enables high availability of frequently accessed data
(&quot;hot data&quot;).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3286</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3286</id><created>2013-11-13</created><authors><author><keyname>Peng</keyname><forenames>Richard</forenames></author><author><keyname>Spielman</keyname><forenames>Daniel A.</forenames></author></authors><title>An Efficient Parallel Solver for SDD Linear Systems</title><categories>cs.NA cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first parallel algorithm for solving systems of linear
equations in symmetric, diagonally dominant (SDD) matrices that runs in
polylogarithmic time and nearly-linear work. The heart of our algorithm is a
construction of a sparse approximate inverse chain for the input matrix: a
sequence of sparse matrices whose product approximates its inverse. Whereas
other fast algorithms for solving systems of equations in SDD matrices exploit
low-stretch spanning trees, our algorithm only requires spectral graph
sparsifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3287</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3287</id><created>2013-11-13</created><updated>2013-12-07</updated><authors><author><keyname>Song</keyname><forenames>Le</forenames></author><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Dai</keyname><forenames>Bo</forenames></author><author><keyname>Xie</keyname><forenames>Bo</forenames></author></authors><title>Nonparametric Estimation of Multi-View Latent Variable Models</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral methods have greatly advanced the estimation of latent variable
models, generating a sequence of novel and efficient algorithms with strong
theoretical guarantees. However, current spectral algorithms are largely
restricted to mixtures of discrete or Gaussian distributions. In this paper, we
propose a kernel method for learning multi-view latent variable models,
allowing each mixture component to be nonparametric. The key idea of the method
is to embed the joint distribution of a multi-view latent variable into a
reproducing kernel Hilbert space, and then the latent parameters are recovered
using a robust tensor power method. We establish that the sample complexity for
the proposed method is quadratic in the number of latent components and is a
low order polynomial in the other relevant parameters. Thus, our non-parametric
tensor approach to learning latent variable models enjoys good sample and
computational efficiencies. Moreover, the non-parametric tensor power method
compares favorably to EM algorithm and other existing spectral algorithms in
our experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3293</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3293</id><created>2013-11-13</created><authors><author><keyname>Kahanwal</keyname><forenames>Dr. Brijender</forenames></author></authors><title>Abstraction Level Taxonomy of Programming Language Frameworks</title><categories>cs.PL</categories><comments>12 pages, 3 figures</comments><journal-ref>International Journal of Programming Languages and Applications,
  Volume 3, No. 4, pp. 1-12, 2013</journal-ref><doi>10.5121/ijpla.2013.3401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main purpose of this article is to describe the taxonomy of computer
languages according to the levels of abstraction. There exists so many computer
languages because of so many reasons like the evolution of better computer
languages over the time; the socio-economic factors as the proprietary
interests, commercial advantages; expressive power; ease of use of novice;
orientation toward special purposes; orientation toward special hardware; and
diverse ideas about most suitability. Moreover, the important common properties
of most of these languages are discussed here. No programming language is
designed in a vacuity, but it solves some specific kinds of problems. There is
a different framework for each problem and best suitable framework for each
problem. A single framework is not best for all types of problems. So, it is
important to select vigilantly the frameworks supported by the language. The
five generation of the computer programming languages are explored in this
paper to some extent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3297</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3297</id><created>2013-11-13</created><authors><author><keyname>Childs</keyname><forenames>Andrew M.</forenames></author><author><keyname>Gosset</keyname><forenames>David</forenames></author><author><keyname>Webb</keyname><forenames>Zak</forenames></author></authors><title>The Bose-Hubbard model is QMA-complete</title><categories>quant-ph cond-mat.stat-mech cs.CC</categories><journal-ref>Proceedings of the 41st International Colloquium on Automata,
  Languages, and Programming (ICALP 2014), pp. 308-319 (2014)</journal-ref><doi>10.1007/978-3-662-43948-7_26</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bose-Hubbard model is a system of interacting bosons that live on the
vertices of a graph. The particles can move between adjacent vertices and
experience a repulsive on-site interaction. The Hamiltonian is determined by a
choice of graph that specifies the geometry in which the particles move and
interact. We prove that approximating the ground energy of the Bose-Hubbard
model on a graph at fixed particle number is QMA-complete. In our QMA-hardness
proof, we encode the history of an n-qubit computation in the subspace with at
most one particle per site (i.e., hard-core bosons). This feature, along with
the well-known mapping between hard-core bosons and spin systems, lets us prove
a related result for a class of 2-local Hamiltonians defined by graphs that
generalizes the XY model. By avoiding the use of perturbation theory in our
analysis, we circumvent the need to multiply terms in the Hamiltonian by large
coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3312</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3312</id><created>2013-11-13</created><authors><author><keyname>Ayala-Rivera</keyname><forenames>Vanessa</forenames></author><author><keyname>McDonagh</keyname><forenames>Patrick</forenames></author><author><keyname>Cerqueus</keyname><forenames>Thomas</forenames></author><author><keyname>Murphy</keyname><forenames>Liam</forenames></author></authors><title>Synthetic Data Generation using Benerator Tool</title><categories>cs.DB</categories><comments>12 pages, 5 figures, 10 references</comments><report-no>UCD-CSI-2013-03</report-no><msc-class>68-XX (Primary)</msc-class><acm-class>H.2.7; K.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Datasets of different characteristics are needed by the research community
for experimental purposes. However, real data may be difficult to obtain due to
privacy concerns. Moreover, real data may not meet specific characteristics
which are needed to verify new approaches under certain conditions. Given these
limitations, the use of synthetic data is a viable alternative to complement
the real data. In this report, we describe the process followed to generate
synthetic data using Benerator, a publicly available tool. The results show
that the synthetic data preserves a high level of accuracy compared to the
original data. The generated datasets correspond to microdata containing
records with social, economic and demographic data which mimics the
distribution of aggregated statistics from the 2011 Irish Census data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3315</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3315</id><created>2013-11-13</created><updated>2014-05-13</updated><authors><author><keyname>Neyshabur</keyname><forenames>Behnam</forenames></author><author><keyname>Panigrahy</keyname><forenames>Rina</forenames></author></authors><title>Sparse Matrix Factorization</title><categories>cs.LG stat.ML</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of factorizing a matrix into several sparse
matrices and propose an algorithm for this under randomness and sparsity
assumptions. This problem can be viewed as a simplification of the deep
learning problem where finding a factorization corresponds to finding edges in
different layers and values of hidden units. We prove that under certain
assumptions for a sparse linear deep network with $n$ nodes in each layer, our
algorithm is able to recover the structure of the network and values of top
layer hidden units for depths up to $\tilde O(n^{1/6})$. We further discuss the
relation among sparse matrix factorization, deep learning, sparse recovery and
dictionary learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3318</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3318</id><created>2013-11-13</created><authors><author><keyname>Xu</keyname><forenames>Chenliang</forenames></author><author><keyname>Doell</keyname><forenames>Richard F.</forenames></author><author><keyname>Hanson</keyname><forenames>Stephen Jos&#xe9;</forenames></author><author><keyname>Hanson</keyname><forenames>Catherine</forenames></author><author><keyname>Corso</keyname><forenames>Jason J.</forenames></author></authors><title>A Study of Actor and Action Semantic Retention in Video Supervoxel
  Segmentation</title><categories>cs.CV</categories><comments>This article is in review at the International Journal of Semantic
  Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing methods in the semantic computer vision community seem unable to
deal with the explosion and richness of modern, open-source and social video
content. Although sophisticated methods such as object detection or
bag-of-words models have been well studied, they typically operate on low level
features and ultimately suffer from either scalability issues or a lack of
semantic meaning. On the other hand, video supervoxel segmentation has recently
been established and applied to large scale data processing, which potentially
serves as an intermediate representation to high level video semantic
extraction. The supervoxels are rich decompositions of the video content: they
capture object shape and motion well. However, it is not yet known if the
supervoxel segmentation retains the semantics of the underlying video content.
In this paper, we conduct a systematic study of how well the actor and action
semantics are retained in video supervoxel segmentation. Our study has human
observers watching supervoxel segmentation videos and trying to discriminate
both actor (human or animal) and action (one of eight everyday actions). We
gather and analyze a large set of 640 human perceptions over 96 videos in 3
different supervoxel scales. Furthermore, we conduct machine recognition
experiments on a feature defined on supervoxel segmentation, called supervoxel
shape context, which is inspired by the higher order processes in human
perception. Our ultimate findings suggest that a significant amount of
semantics have been well retained in the video supervoxel segmentation and can
be used for further video analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3319</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3319</id><created>2013-11-13</created><authors><author><keyname>Mosbah</keyname><forenames>Mohamed Magdy</forenames></author><author><keyname>Soliman</keyname><forenames>Hany</forenames></author><author><keyname>El-Nasr</keyname><forenames>Mohamad Abou</forenames></author></authors><title>Current Services In Cloud Computing: A Survey</title><categories>cs.OH</categories><comments>8 pages</comments><journal-ref>International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol.3,No.5,October 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the fast development of the Cloud Computing technologies, the rapid
increase of cloud services are became very remarkable. The fact of integration
of these services with many of the modern enterprises cannot be ignored.
Microsoft, Google, Amazon, SalesForce.com and the other leading IT companies
are entered the field of developing these services. This paper presents a
comprehensive survey of current cloud services, which are divided into eleven
categories. Also the most famous providers for these services are listed.
Finally, the Deployment Models of Cloud Computing are mentioned and briefly
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3322</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3322</id><created>2013-11-13</created><authors><author><keyname>Do</keyname><forenames>Thanh</forenames></author><author><keyname>Gunawi</keyname><forenames>Haryadi S.</forenames></author></authors><title>Impact of Limpware on HDFS: A Probabilistic Estimation</title><categories>cs.OS cs.DC</categories><comments>9 pages, 6 figures, detailed probability calculation for SOCC 13
  limplock paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of cloud computing, thousands of machines are connected and
managed collectively. This era is confronted with a new challenge: performance
variability, primarily caused by large-scale management issues such as hardware
failures, software bugs, and configuration mistakes. In our previous work we
highlighted one overlooked cause: limpware - hardware whose performance
degrades significantly compared to its specification. We showed that limpware
can cause severe impact in current scale-out systems. In this report, we
quantify how often these scenarios happen in Hadoop Distributed File System.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3323</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3323</id><created>2013-11-13</created><authors><author><keyname>Dumitrescu</keyname><forenames>Adrian</forenames></author><author><keyname>Jiang</keyname><forenames>Minghui</forenames></author></authors><title>The opaque square</title><categories>math.CO cs.DM</categories><comments>23 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding small sets that block every line passing through a
unit square was first considered by Mazurkiewicz in 1916. We call such a set
{\em opaque} or a {\em barrier} for the square. The shortest known barrier has
length $\sqrt{2}+ \frac{\sqrt{6}}{2}= 2.6389\ldots$. The current best lower
bound for the length of a (not necessarily connected) barrier is $2$, as
established by Jones about 50 years ago. No better lower bound is known even if
the barrier is restricted to lie in the square or in its close vicinity. Under
a suitable locality assumption, we replace this lower bound by $2+10^{-12}$,
which represents the first, albeit small, step in a long time toward finding
the length of the shortest barrier. A sharper bound is obtained for interior
barriers: the length of any interior barrier for the unit square is at least $2
+ 10^{-5}$. Two of the key elements in our proofs are: (i) formulas established
by Sylvester for the measure of all lines that meet two disjoint planar convex
bodies, and (ii) a procedure for detecting lines that are witness to the
invalidity of a short bogus barrier for the square.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3336</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3336</id><created>2013-11-13</created><authors><author><keyname>Casey</keyname><forenames>C. Jasson</forenames></author><author><keyname>Sutton</keyname><forenames>Andrew</forenames></author><author><keyname>Reis</keyname><forenames>Gabriel Dos</forenames></author><author><keyname>Sprintson</keyname><forenames>Alex</forenames></author></authors><title>Eliminating Network Protocol Vulnerabilities Through Abstraction and
  Systems Language Design</title><categories>cs.NI cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incorrect implementations of network protocol message specifications affect
the stability, security, and cost of network system development. Most
implementation defects fall into one of three categories of well defined
message constraints. However, the general process of constructing network
protocol stacks and systems does not capture these categorical con- straints.
We introduce a systems programming language with new abstractions that capture
these constraints. Safe and efficient implementations of standard message
handling operations are synthesized by our compiler, and whole-program analysis
is used to ensure constraints are never violated. We present language examples
using the OpenFlow protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3348</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3348</id><created>2013-11-13</created><authors><author><keyname>Ewelle</keyname><forenames>Richard Ewelle</forenames></author><author><keyname>Goua&#xef;ch</keyname><forenames>Abdelkader</forenames></author><author><keyname>Francillette</keyname><forenames>Yannick</forenames></author><author><keyname>Mahdi</keyname><forenames>Ghulam</forenames></author></authors><title>Network Traffic Adaptation For Cloud Games</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the arrival of cloud technology, game accessibility and ubiquity have a
bright future; Games can be hosted in a centralize server and accessed through
the Internet by a thin client on a wide variety of devices with modest
capabilities: cloud gaming. However, current cloud gaming systems have very
strong requirements in terms of network resources, thus reducing the
accessibility and ubiquity of cloud games, because devices with little
bandwidth and people located in area with limited and unstable network
connectivity, cannot take advantage of these cloud services. In this paper we
present an adaptation technique inspired by the level of detail (LoD) approach
in 3D graphics. It delivers multiple platform accessibility and network
adaptability, while improving user's quality of experience (QoE) by reducing
the impact of poor and unstable network parameters (delay, packet loss, jitter)
on game interactivity. We validate our approach using a prototype game in a
controlled environment and characterize the user QoE in a pilot experiment. The
results show that the proposed framework provides a significant QoE
enhancement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3353</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3353</id><created>2013-11-13</created><updated>2014-05-13</updated><authors><author><keyname>Amadini</keyname><forenames>Roberto</forenames></author><author><keyname>Gabbrielli</keyname><forenames>Maurizio</forenames></author><author><keyname>Mauro</keyname><forenames>Jacopo</forenames></author></authors><title>SUNNY: a Lazy Portfolio Approach for Constraint Solving</title><categories>cs.AI</categories><doi>10.1017/S1471068414000179</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  *** To appear in Theory and Practice of Logic Programming (TPLP) ***
  Within the context of constraint solving, a portfolio approach allows one to
exploit the synergy between different solvers in order to create a globally
better solver. In this paper we present SUNNY: a simple and flexible algorithm
that takes advantage of a portfolio of constraint solvers in order to compute
--- without learning an explicit model --- a schedule of them for solving a
given Constraint Satisfaction Problem (CSP). Motivated by the performance
reached by SUNNY vs. different simulations of other state of the art
approaches, we developed sunny-csp, an effective portfolio solver that exploits
the underlying SUNNY algorithm in order to solve a given CSP. Empirical tests
conducted on exhaustive benchmarks of MiniZinc models show that the actual
performance of SUNNY conforms to the predictions. This is encouraging both for
improving the power of CSP portfolio solvers and for trying to export them to
fields such as Answer Set Programming and Constraint Logic Programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3355</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3355</id><created>2013-11-13</created><authors><author><keyname>He</keyname><forenames>Yongqun</forenames></author><author><keyname>Xiang</keyname><forenames>Zoushuang</forenames></author></authors><title>HINO: a BFO-aligned ontology representing human molecular interactions
  and pathways</title><categories>cs.AI cs.DB q-bio.MN</categories><comments>7 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Many database resources, such as Reactome, collect manually annotated
reactions, interactions, and pathways from peer-reviewed publications. The
interactors (e.g., a protein), interactions, and pathways in these data
resources are often represented as instances in using BioPAX, a standard
pathway data exchange format. However, these interactions are better
represented as classes (or universals) since they always occur given
appropriate conditions. This study aims to represent various human interaction
pathways and networks as classes via a formal ontology aligned with the Basic
Formal Ontology (BFO). Towards this goal, the Human Interaction Network
Ontology (HINO) was generated by extending the BFO-aligned Interaction Network
Ontology (INO). All human pathways and associated processes and interactors
listed in Reactome and represented in BioPAX were first converted to ontology
classes by aligning them under INO. Related terms and associated relations and
hierarchies from external ontologies (e.g., CHEBI and GO) were also retrieved
and imported into HINO. HINO ontology terms were resolved in the linked
ontology data server Ontobee. The RDF triples stored in the RDF triple store
are queryable through a SPARQL program. Such an ontology system supports
advanced pathway data integration and applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3358</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3358</id><created>2013-11-13</created><authors><author><keyname>Haynes</keyname><forenames>Ronald D.</forenames></author><author><keyname>Howse</keyname><forenames>Alexander J. M.</forenames></author></authors><title>Generating Equidistributed Meshes in 2D via Domain Decomposition</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider Schwarz domain decomposition applied to the
generation of 2D spatial meshes by a local equidistribution principle. We
briefly review the derivation of the local equidistribution principle and the
appropriate choice of boundary conditions. We then introduce classical and
optimized Schwarz domain decomposition methods to solve the resulting system of
nonlinear equations. The implementation of these iterations are discussed, and
we conclude with numerical examples to illustrate the performance of the
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3365</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3365</id><created>2013-11-13</created><updated>2015-01-22</updated><authors><author><keyname>Brandenburger</keyname><forenames>Adam</forenames></author><author><keyname>La Mura</keyname><forenames>Pierfrancesco</forenames></author></authors><title>Deriving the Qubit from Entropy Principles</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Heisenberg uncertainty principle is one of the most famous features of
quantum mechanics. However, the non-determinism implied by the Heisenberg
uncertainty principle --- together with other prominent aspects of quantum
mechanics such as superposition, entanglement, and nonlocality --- poses deep
puzzles about the underlying physical reality, even while these same features
are at the heart of exciting developments such as quantum cryptography,
algorithms, and computing. These puzzles might be resolved if the mathematical
structure of quantum mechanics were built up from physically interpretable
axioms, but it is not. We propose three physically-based axioms which together
characterize the simplest quantum system, namely the qubit. Our starting point
is the class of all no-signaling theories. Each such theory can be regarded as
a family of empirical models, and we proceed to associate entropies, i.e.,
measures of information, with these models. To do this, we move to phase space
and impose the condition that entropies are real-valued. This requirement,
which we call the Information Reality Principle, arises because in order to
represent all no-signaling theories (including quantum mechanics itself) in
phase space, it is necessary to allow negative probabilities (Wigner [1932]).
Our second and third principles take two important features of quantum
mechanics and turn them into deliberately chosen physical axioms. One axiom is
an Uncertainty Principle, stated in terms of entropy. The other axiom is an
Unbiasedness Principle, which requires that whenever there is complete
certainty about the outcome of a measurement in one of three mutually
orthogonal directions, there must be maximal uncertainty about the outcomes in
each of the two other directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3368</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3368</id><created>2013-11-13</created><authors><author><keyname>Singh</keyname><forenames>Sameer</forenames></author><author><keyname>Riedel</keyname><forenames>Sebastian</forenames></author><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author></authors><title>Anytime Belief Propagation Using Sparse Domains</title><categories>stat.ML cs.AI cs.LG</categories><comments>NIPS 2013 Workshop on Resource-Efficient Machine Learning</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Belief Propagation has been widely used for marginal inference, however it is
slow on problems with large-domain variables and high-order factors. Previous
work provides useful approximations to facilitate inference on such models, but
lacks important anytime properties such as: 1) providing accurate and
consistent marginals when stopped early, 2) improving the approximation when
run longer, and 3) converging to the fixed point of BP. To this end, we propose
a message passing algorithm that works on sparse (partially instantiated)
domains, and converges to consistent marginals using dynamic message
scheduling. The algorithm grows the sparse domains incrementally, selecting the
next value to add using prioritization schemes based on the gradients of the
marginal inference objective. Our experiments demonstrate local anytime
consistency and fast convergence, providing significant speedups over BP to
obtain low-error marginals: up to 25 times on grid models, and up to 6 times on
a real-world natural language processing task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3371</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3371</id><created>2013-11-13</created><authors><author><keyname>Venugopal</keyname><forenames>Gayatri</forenames></author></authors><title>Android Note Manager Application for People with Visual Impairment</title><categories>cs.HC cs.CY</categories><comments>6 pages, 4 figures</comments><doi>10.5121/ijmnct.2013.3502</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  With the outburst of smart-phones today, the market is exploding with various
mobile applications. This paper proposes an application using which visually
impaired people can type a note in Grade 1 Braille and save it in the external
memory of their smart-phone. The application also shows intelligence by
activating reminders and/or calling certain contacts based on the content in
the notes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3387</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3387</id><created>2013-11-14</created><updated>2014-01-13</updated><authors><author><keyname>He</keyname><forenames>Chen</forenames></author><author><keyname>Chen</keyname><forenames>Xun</forenames></author><author><keyname>Wang</keyname><forenames>Z. Jane</forenames></author></authors><title>Performance of General STCs over Spatially Correlated MIMO
  Single-keyhole Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For MIMO Rayleigh channels, it has been shown that transmitter correlations
always degrade the performance of general space-time codes (STCs) in high SNR
regimes. In this correspondence, however, we show that when MIMO channels
experience single-keyhole conditions, the effect of spatial correlations
between transmission antennas is more sophisticated for general STCs: when
$M&gt;N$ (i.e., the number of transmission antennas is greater than the number of
receiving antennas), depending on how the correlation matrix $\mathbf{P}$
beamforms the code word difference matrix $\mathbf{\Delta}$, the PEP
performance of general STCs can be either degraded or improved in high SNR
regimes. We provide a new measure, which is based on the eigenvalues of
$\mathbf{\Delta}$ and the numbers of transmission and receiving antennas, to
exam if there exists certain correlation matrices that can improve the
performance of general STCs in high SNR regimes. Previous studies on the effect
of spatial correlations over single-keyhole channels only concentrated on
orthogonal STCs, while our study here is for general STCs and can also be used
to explain previous findings for orthogonal STCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3391</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3391</id><created>2013-11-14</created><updated>2013-12-04</updated><authors><author><keyname>Liu</keyname><forenames>Yan</forenames></author><author><keyname>Yan</keyname><forenames>Haode</forenames></author><author><keyname>Liu</keyname><forenames>Chunlei</forenames></author></authors><title>A Class of Six-weight Cyclic Codes and Their Weight Distribution</title><categories>math.NT cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1302.0952, arXiv:1302.0569,
  arXiv:1301.4824 by other authors</comments><msc-class>94B15, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a family of six-weight cyclic codes over GF(p) whose duals
have two zeros is presented, where p is an odd prime. And the weight
distribution of these cyclic codes is determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3394</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3394</id><created>2013-11-14</created><authors><author><keyname>El-korany</keyname><forenames>Abeer</forenames></author></authors><title>Integrated Expert Recommendation Model for Online Communities</title><categories>cs.SI cs.IR</categories><journal-ref>International Journal of Web &amp; Semantic Technology (IJWesT),
  October 2013, Volume 4, Number 4</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online communities have become vital places for Web 2.0 users to share
knowledge and experiences. Recently, finding expertise user in community has
become an important research issue. This paper proposes a novel cascaded model
for expert recommendation using aggregated knowledge extracted from enormous
contents and social network features. Vector space model is used to compute the
relevance of published content with respect to a specific query while PageRank
algorithm is applied to rank candidate experts. The experimental results show
that the proposed model is an effective recommendation which can guarantee that
the most candidate experts are both highly relevant to the specific queries and
highly influential in corresponding areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3396</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3396</id><created>2013-11-14</created><updated>2014-03-07</updated><authors><author><keyname>Feng</keyname><forenames>Yuan</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author></authors><title>When Equivalence and Bisimulation Join Forces in Probabilistic Automata</title><categories>cs.LO cs.FL</categories><comments>21 pages, 1 figure. Comments are welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic automata were introduced by Rabin in 1963 as language
acceptors. Two automata are equivalent if and only if they accept each word
with the same probability. On the other side, in the process algebra community,
probabilistic automata were re-proposed by Segala in 1995 which are more
general than Rabin's automata. Bisimulations have been proposed for Segala's
automata to characterize the equivalence between them. So far the two notions
of equivalences and their characteristics have been studied most independently.
In this paper, we consider Segala's automata, and propose a novel notion of
distribution based bisimulation by joining the existing equivalence and
bisimilarities. Our bisimulation bridges the two closely related concepts in
the community, and provides a uniform way of studying their characteristics. We
demonstrate the utility of our definition by studying distribution based
bisimulation metrics, which gives rise to a robust notion of equivalence for
Rabin's automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3405</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3405</id><created>2013-11-14</created><updated>2013-11-15</updated><authors><author><keyname>Goldstein</keyname><forenames>Tom</forenames></author><author><keyname>Xu</keyname><forenames>Lina</forenames></author><author><keyname>Kelly</keyname><forenames>Kevin F.</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard</forenames></author></authors><title>The STONE Transform: Multi-Resolution Image Enhancement and Real-Time
  Compressive Video</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing enables the reconstruction of high-resolution signals from
under-sampled data. While compressive methods simplify data acquisition, they
require the solution of difficult recovery problems to make use of the
resulting measurements. This article presents a new sensing framework that
combines the advantages of both conventional and compressive sensing. Using the
proposed \stone transform, measurements can be reconstructed instantly at
Nyquist rates at any power-of-two resolution. The same data can then be
&quot;enhanced&quot; to higher resolutions using compressive methods that leverage
sparsity to &quot;beat&quot; the Nyquist limit. The availability of a fast direct
reconstruction enables compressive measurements to be processed on small
embedded devices. We demonstrate this by constructing a real-time compressive
video camera.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3414</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3414</id><created>2013-11-14</created><authors><author><keyname>Martinez</keyname><forenames>Matias</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Mining Software Repair Models for Reasoning on the Search Space of
  Automated Program Fixing</title><categories>cs.SE</categories><comments>Empirical Software Engineering (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is about understanding the nature of bug fixing by analyzing
thousands of bug fix transactions of software repositories. It then places this
learned knowledge in the context of automated program repair. We give extensive
empirical results on the nature of human bug fixes at a large scale and a fine
granularity with abstract syntax tree differencing. We set up mathematical
reasoning on the search space of automated repair and the time to navigate
through it. By applying our method on 14 repositories of Java software and
89,993 versioning transactions, we show that not all probabilistic repair
models are equivalent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3416</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3416</id><created>2013-11-14</created><updated>2014-09-06</updated><authors><author><keyname>Fujiwara</keyname><forenames>Yuichiro</forenames></author><author><keyname>Vandendriessche</keyname><forenames>Peter</forenames></author></authors><title>Quantum synchronizable codes from finite geometries</title><categories>quant-ph cs.IT math.IT</categories><comments>10 pages, 2 tables, final accepted version for publication in the
  IEEE Transactions on Information Theory</comments><journal-ref>IEEE Transactions on Information Theory, 60 (2014) 7345-7354</journal-ref><doi>10.1109/TIT.2014.2357029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum synchronizable error-correcting codes are special quantum
error-correcting codes that are designed to correct both the effect of quantum
noise on qubits and misalignment in block synchronization. It is known that in
principle such a code can be constructed through a combination of a classical
linear code and its subcode if the two are both cyclic and dual-containing.
However, finding such classical codes that lead to promising quantum
synchronizable error-correcting codes is not a trivial task. In fact, although
there are two families of classical codes that are proved to produce quantum
synchronizable codes with good minimum distances and highest possible tolerance
against misalignment, their code lengths have been restricted to primes and
Mersenne numbers. In this paper, examining the incidence vectors of projective
spaces over the finite fields of characteristic $2$, we give quantum
synchronizable codes from cyclic codes whose lengths are not primes or Mersenne
numbers. These projective geometric codes achieve good performance in quantum
error correction and possess the best possible ability to recover
synchronization, thereby enriching the variety of good quantum synchronizable
codes. We also extend the current knowledge of cyclic codes in classical coding
theory by explicitly giving generator polynomials of the finite geometric codes
and completely characterizing the minimum weight nonzero codewords. In addition
to the codes based on projective spaces, we carry out a similar analysis on the
well-known cyclic codes from Euclidean spaces that are known to be majority
logic decodable and determine their exact minimum distances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3425</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3425</id><created>2013-11-14</created><updated>2015-06-26</updated><authors><author><keyname>Feinerman</keyname><forenames>Ofer</forenames></author><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author><author><keyname>Korman</keyname><forenames>Amos</forenames></author></authors><title>Breathe before Speaking: Efficient Information Dissemination Despite
  Noisy, Limited and Anonymous Communication</title><categories>cs.DC</categories><doi>10.1145/2611462.2611469</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed computing models typically assume reliable communication between
processors. While such assumptions often hold for engineered networks, e.g.,
due to underlying error correction protocols, their relevance to biological
systems, wherein messages are often distorted before reaching their
destination, is quite limited. In this study we take a first step towards
reducing this gap by rigorously analyzing a model of communication in large
anonymous populations composed of simple agents which interact through short
and highly unreliable messages.
  We focus on the broadcast problem and the majority-consensus problem. Both
are fundamental information dissemination problems in distributed computing, in
which the goal of agents is to converge to some prescribed desired opinion. We
initiate the study of these problems in the presence of communication noise.
Our model for communication is extremely weak and follows the push gossip
communication paradigm: In each round each agent that wishes to send
information delivers a message to a random anonymous agent. This communication
is further restricted to contain only one bit (essentially representing an
opinion). Lastly, the system is assumed to be so noisy that the bit in each
message sent is flipped independently with probability $1/2-\epsilon$, for some
small $\epsilon &gt;0$.
  Even in this severely restricted, stochastic and noisy setting we give
natural protocols that solve the noisy broadcast and the noisy
majority-consensus problems efficiently. Our protocols run in $O(\log n /
\epsilon^2)$ rounds and use $O(n \log n / \epsilon^2)$ messages/bits in total,
where $n$ is the number of agents. These bounds are asymptotically optimal and,
in fact, are as fast and message efficient as if each agent would have been
simultaneously informed directly by an agent that knows the prescribed desired
opinion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3428</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3428</id><created>2013-11-14</created><authors><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Smith</keyname><forenames>Peter J.</forenames></author></authors><title>Low-complexity End-to-End Performance Optimization in MIMO Full-Duplex
  Relay Systems</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we deal with the deployment of full-duplex relaying in
amplify-and-forward (AF) cooperative networks with multiple-antenna terminals.
In contrast to previous studies, which focus on the spatial mitigation of the
loopback interference (LI) at the relay node, a joint precoding/decoding design
that maximizes the end-to-end (e2e) performance is investigated. The proposed
precoding incorporates rank-1 zero-forcing (ZF) LI suppression at the relay
node and is derived in closed-form by solving appropriate optimization
problems. In order to further reduce system complexity, the antenna selection
(AS) problem for full-duplex AF cooperative systems is discussed. We
investigate different AS schemes to select a single transmit antenna at both
the source and the relay, as well as a single receive antenna at both the relay
and the destination. To facilitate comparison, exact outage probability
expressions and asymptotic approximations of the proposed AS schemes are
provided. In order to overcome zero-diversity effects associated with the AS
operation, a simple power allocation scheme at the relay node is also
investigated and its optimal value is analytically derived. Numerical and
simulation results show that the joint ZF-based precoding significantly
improves e2e performance, while AS schemes are efficient solutions for
scenarios with strict computational constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3429</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3429</id><created>2013-11-14</created><authors><author><keyname>Tabak</keyname><forenames>Ahmet Fatih</forenames></author><author><keyname>Yesilyurt</keyname><forenames>Serhat</forenames></author></authors><title>Hydrodynamic surrogate models for bio-inspired micro-swimming robots</title><categories>physics.flu-dyn cs.RO</categories><comments>37 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on untethered micro-swimming robots is growing fast owing to their
potential impact on minimally invasive medical procedures. Candidate propulsion
mechanisms of robots are based on flagellar mechanisms of micro organisms such
as rotating rigid helices and traveling plane-waves on flexible rods. For
design and control of swimming robots, accurate real-time models are necessary
to compute trajectories, velocities and hydrodynamic forces acting on robots.
Resistive force theory (RFT) provides an excellent framework for the
development of real-time six degrees-of-freedom surrogate models for design
optimization and control. However the accuracy of RFT-based models depends
strongly on hydrodynamic interactions. Here, we introduce interaction
coefficients that only multiply body resistance coefficients with no
modification to local resistance coefficients on the tail. Interaction
coefficients are obtained for a single specimen of Vibrio Algino reported in
literature, and used in the RFT model for comparisons of forward velocities and
body rotation rates against other specimens. Furthermore, CFD simulations are
used to obtain forward and lateral velocities and body rotation rates of
bio-inspired swimmers with helical tails and traveling-plane waves for a range
of amplitudes and wavelengths. Interaction coefficients are obtained from the
CFD simulation for the helical tail with the specified amplitude and
wavelength, and used in the RFT model for comparisons of velocities and body
rotation rates for other designs. Comparisons indicate that hydrodynamic models
that employ interaction coefficients prove to be viable surrogates for
computationally intensive three-dimensional time-dependent CFD models. Lastly,
hydrodynamic models of bio-inspired swimmers are used to obtain optimal
amplitudes and wavelengths of flagellar mechanisms, as a demonstration of the
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3470</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3470</id><created>2013-11-14</created><updated>2015-01-22</updated><authors><author><keyname>Kaibel</keyname><forenames>Volker</forenames></author><author><keyname>Walter</keyname><forenames>Matthias</forenames></author></authors><title>Simple Extensions of Polytopes</title><categories>math.CO cs.DM math.OC</categories><comments>28 pages, 13 figures. Accepted for publication in a special issue on
  IPCO 2014 of the journal Mathematical Programming (Series B), to appear 2015</comments><msc-class>52Bxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the simple extension complexity of a polytope P as the smallest
number of facets of any simple (i.e., non-degenerate in the sense of linear
programming) polytope which can be projected onto P. We devise a combinatorial
method to establish lower bounds on the simple extension complexity and show
for several polytopes that they have large simple extension complexities. These
examples include both the spanning tree and the perfect matching polytopes of
complete graphs, uncapacitated flow polytopes for non-trivially decomposable
directed acyclic graphs, hypersimplices, and random 0/1-polytopes with vertex
numbers within a certain range. On our way to obtain the result on perfect
matching polytopes we generalize a result of Padberg and Rao's on the adjacency
structures of those polytopes. To complement the lower bounding techniques we
characterize in which cases known construction techniques yield simple
extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3475</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3475</id><created>2013-11-14</created><authors><author><keyname>Moussaid</keyname><forenames>Mehdi</forenames></author><author><keyname>Kaemmer</keyname><forenames>Juliane E.</forenames></author><author><keyname>Analytis</keyname><forenames>Pantelis P.</forenames></author><author><keyname>Neth</keyname><forenames>Hansjoerg</forenames></author></authors><title>Social Influence and the Collective Dynamics of Opinion Formation</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>Published Nov 05, 2013. Open access at:
  http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0078433</comments><journal-ref>PLoS ONE 8(11): e78433 (2013)</journal-ref><doi>10.1371/journal.pone.0078433</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social influence is the process by which individuals adapt their opinion,
revise their beliefs, or change their behavior as a result of social
interactions with other people. In our strongly interconnected society, social
influence plays a prominent role in many self-organized phenomena such as
herding in cultural markets, the spread of ideas and innovations, and the
amplification of fears during epidemics. Yet, the mechanisms of opinion
formation remain poorly understood, and existing physics-based models lack
systematic empirical validation. Here, we report two controlled experiments
showing how participants answering factual questions revise their initial
judgments after being exposed to the opinion and confidence level of others.
Based on the observation of 59 experimental subjects exposed to peer-opinion
for 15 different items, we draw an influence map that describes the strength of
peer influence during interactions. A simple process model derived from our
observations demonstrates how opinions in a group of interacting people can
converge or split over repeated interactions. In particular, we identify two
major attractors of opinion: (i) the expert effect, induced by the presence of
a highly confident individual in the group, and (ii) the majority effect,
caused by the presence of a critical mass of laypeople sharing similar
opinions. Additional simulations reveal the existence of a tipping point at
which one attractor will dominate over the other, driving collective opinion in
a given direction. These findings have implications for understanding the
mechanisms of public opinion formation and managing conflicting situations in
which self-confident and better informed minorities challenge the views of a
large uninformed majority.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3483</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3483</id><created>2013-11-14</created><authors><author><keyname>Akhtar</keyname><forenames>Md. Amir Khusru</forenames></author><author><keyname>Sahoo</keyname><forenames>G.</forenames></author></authors><title>Subsiding routing misbehavior in MANET using &quot;Mirror Model&quot;</title><categories>cs.NI</categories><comments>15 pages, 4 figures, 3 tables. arXiv admin note: substantial text
  overlap with arXiv:1309.2208</comments><journal-ref>Fifth International Conference on Wireless &amp; Mobile Networks
  (WimoN 2013) Turkey, CS &amp; IT - CSCP, Vol. 3, No. 7, pp. 1-15, Jun 2013</journal-ref><doi>10.5121/csit.2013.3701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Noncooperation or failure to work together is a big challenge that surely
degrades the performance and reliability of Mobile Adhoc Networks. In MANETs,
nodes have dual responsibilities of forwarding and routing, that's why it needs
unison with nodes. To sort out non-cooperation a real life behavior should be
implemented, so that misbehavior is nullified. In this paper, we present the
&quot;Mirror Model&quot; that strictly enforces cooperation due to its punishment
strategy. Node's behavior is watched by its neighbors in PON mode, to update
the NPF, NPRF values for a threshold time. After the expiry of the threshold
time each node calculates the PFR and broadcasts its neighbors. Similarly all
neighbors broadcasted PFR is received and processed by the node to define the
'G' and 'BP' values. The G value is used to isolate selfish nodes from the
routing paths and BP denotes the amount of packets to be dropped by an honest
node against a selfish node in spite of its misbehavior/packet drops.
Cooperation within the neighbors, certainly result in subsiding misbehavior of
selfish nodes, therein enhancing cooperation of the whole MANET. This model
ensures honesty and reliability in MANET because it does not eliminate a node,
but it behaves in the same way as the node behaved. Therefore, it justifies its
name, after all mirrors reflects the same. We have implemented the model in
&quot;GloMoSim&quot; on top of the DSR protocol, resulting its effectiveness, as compared
to the DSR protocol when the network is misconducting for its selfish needs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3485</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3485</id><created>2013-11-14</created><authors><author><keyname>Ganguly</keyname><forenames>Shouvik</forenames></author><author><keyname>Sahasranand</keyname><forenames>K</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>A New Algorithm for Distributed Nonparametric Sequential Detection</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider nonparametric sequential hypothesis testing problem when the
distribution under the null hypothesis is fully known but the alternate
hypothesis corresponds to some other unknown distribution with some loose
constraints. We propose a simple algorithm to address the problem. These
problems are primarily motivated from wireless sensor networks and spectrum
sensing in Cognitive Radios. A decentralized version utilizing spatial
diversity is also proposed. Its performance is analysed and asymptotic
properties are proved. The simulated and analysed performance of the algorithm
is compared with an earlier algorithm addressing the same problem with similar
assumptions. We also modify the algorithm for optimizing performance when
information about the prior probabilities of occurrence of the two hypotheses
are known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3494</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3494</id><created>2013-11-14</created><updated>2014-10-28</updated><authors><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>Fundamental Limits of Online and Distributed Algorithms for Statistical
  Learning and Estimation</title><categories>cs.LG stat.ML</categories><comments>Full version of NIPS 2014 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many machine learning approaches are characterized by information constraints
on how they interact with the training data. These include memory and
sequential access constraints (e.g. fast first-order methods to solve
stochastic optimization problems); communication constraints (e.g. distributed
learning); partial access to the underlying data (e.g. missing features and
multi-armed bandits) and more. However, currently we have little understanding
how such information constraints fundamentally affect our performance,
independent of the learning problem semantics. For example, are there learning
problems where any algorithm which has small memory footprint (or can use any
bounded number of bits from each example, or has certain communication
constraints) will perform worse than what is possible without such constraints?
In this paper, we describe how a single set of results implies positive answers
to the above, for several different settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3508</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3508</id><created>2013-11-14</created><authors><author><keyname>Pasta</keyname><forenames>Muhammad Qasim</forenames></author><author><keyname>Jan</keyname><forenames>Zohaib</forenames></author><author><keyname>Zaidi</keyname><forenames>Faraz</forenames></author><author><keyname>Rozenblat</keyname><forenames>Celine</forenames></author></authors><title>Demographic and Structural Characteristics to Rationalize Link Formation
  in Online Social Networks</title><categories>cs.SI</categories><comments>Second International Workshop on Complex Networks and their
  Applications (10 pages, 8 figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen tremendous growth of many online social networks such
as Facebook, LinkedIn and MySpace. People connect to each other through these
networks forming large social communities providing researchers rich datasets
to understand, model and predict social interactions and behaviors. New
contacts in these networks can be formed either due to an individual's
demographic profile such as age group, gender, geographic location or due to
network's structural dynamics such as triadic closure and preferential
attachment, or a combination of both demographic and structural
characteristics.
  A number of network generation models have been proposed in the last decade
to explain the structure, evolution and processes taking place in different
types of networks, and notably social networks. Network generation models
studied in the literature primarily consider structural properties, and in some
cases an individual's demographic profile in the formation of new social
contacts. These models do not present a mechanism to combine both structural
and demographic characteristics for the formation of new links. In this paper,
we propose a new network generation algorithm which incorporates both these
characteristics to model growth of a network.We use different publicly
available Facebook datasets as benchmarks to demonstrate the correctness of the
proposed network generation model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3515</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3515</id><created>2013-11-14</created><authors><author><keyname>Farina</keyname><forenames>Marcello</forenames></author><author><keyname>Guagliardi</keyname><forenames>Antonio</forenames></author><author><keyname>Mariani</keyname><forenames>Federico</forenames></author><author><keyname>Sandroni</keyname><forenames>Carlo</forenames></author><author><keyname>Scattolini</keyname><forenames>Riccardo</forenames></author></authors><title>Model predictive control of voltage profiles in MV networks with
  distributed generation</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Model Predictive Control (MPC) approach is used in this paper to control
the voltage profiles in MV networks with distributed generation. The proposed
algorithm lies at the intermediate level of a three-layer hierarchical
structure. At the upper level a static Optimal Power Flow (OPF) manager
computes the required voltage profiles to be transmitted to the MPC level,
while at the lower level local Automatic Voltage Regulators (AVR), one for each
Distributed Generator (DG), track the reactive power reference values computed
by MPC. The control algorithm is based on an impulse response model of the
system, easily obtained by means of a detailed simulator of the network, and
allows to cope with constraints on the voltage profiles and/or on the reactive
power flows along the network. If these constraints cannot be satisfied by
acting on the available DGs, the algorithm acts on the On-Load Tap Changing
(OLTC) transformer. A radial rural network with two feeders, eight DGs, and
thirty-one loads is used as case study. The model of the network is implemented
in DIgSILENT PowerFactory, while the control algorithm runs in Matlab. A number
of simulation results is reported to witness the main characteristics and
limitations of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3523</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3523</id><created>2013-11-14</created><updated>2014-05-02</updated><authors><author><keyname>Katz</keyname><forenames>Daniel S.</forenames></author><author><keyname>Allen</keyname><forenames>Gabrielle</forenames></author><author><keyname>Hong</keyname><forenames>Neil Chue</forenames></author><author><keyname>Parashar</keyname><forenames>Manish</forenames></author><author><keyname>Proctor</keyname><forenames>David</forenames></author></authors><title>First Workshop on Sustainable Software for Science: Practice and
  Experiences (WSSSPE): Submission and Peer-Review Process, and Results</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This technical report discusses the submission and peer-review process used
by the First Workshop on on Sustainable Software for Science: Practice and
Experiences (WSSSPE) and the results of that process. It is intended to record
both this alternative model as well as the papers associated with the workshop
that resulted from that process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3527</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3527</id><created>2013-11-14</created><authors><author><keyname>Wei</keyname><forenames>Daijun</forenames></author><author><keyname>Wei</keyname><forenames>Bo</forenames></author><author><keyname>Hu</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Haixin</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>A new information dimension of complex networks</title><categories>cs.SI physics.soc-ph</categories><comments>14 pages, 2 figures</comments><doi>10.1016/j.physleta.2014.02.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fractal and self-similarity properties are revealed in many real complex
networks. However, the classical information dimension of complex networks is
not practical for real complex networks. In this paper, a new information
dimension to characterize the dimension of complex networks is proposed. The
difference of information for each box in the box-covering algorithm of complex
networks is considered by this measure. The proposed method is applied to
calculate the fractal dimensions of some real networks. Our results show that
the proposed method is efficient for fractal dimension of complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3530</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3530</id><created>2013-11-14</created><authors><author><keyname>Bloem</keyname><forenames>Roderick</forenames></author><author><keyname>Koenighofer</keyname><forenames>Robert</forenames></author><author><keyname>Seidl</keyname><forenames>Martina</forenames></author></authors><title>SAT-Based Synthesis Methods for Safety Specs</title><categories>cs.LO</categories><comments>Extended version of a paper at VMCAI'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic synthesis of hardware components from declarative specifications is
an ambitious endeavor in computer aided design. Existing synthesis algorithms
are often implemented with Binary Decision Diagrams (BDDs), inheriting their
scalability limitations. Instead of BDDs, we propose several new methods to
synthesize finite-state systems from safety specifications using decision
procedures for the satisfiability of quantified and unquantified Boolean
formulas (SAT-, QBF- and EPR-solvers). The presented approaches are based on
computational learning, templates, or reduction to first-order logic. We also
present an efficient parallelization, and optimizations to utilize reachability
information and incremental solving. Finally, we compare all methods in an
extensive case study. Our new methods outperform BDDs and other existing work
on some classes of benchmarks, and our parallelization achieves a super-linear
speedup. This is an extended version of [5], featuring an additional appendix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3533</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3533</id><created>2013-11-14</created><updated>2013-11-15</updated><authors><author><keyname>Gopalkrishnan</keyname><forenames>Manoj</forenames></author></authors><title>The Hot Bit I: The Szilard-Landauer Correspondence</title><categories>cs.IT math-ph math.IT math.MP</categories><comments>v2: 11 pages, no figures. Introduction section added. v1: 9 pages, no
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a precise formulation of a correspondence between information and
thermodynamics that was first observed by Szilard, and later studied by
Landauer. The correspondence identifies available free energy with relative
entropy, and provides a dictionary between information and thermodynamics. We
precisely state and prove this correspondence. The paper should be broadly
accessible since we assume no prior knowledge of information theory, developing
it axiomatically, and we assume almost no thermodynamic background.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3534</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3534</id><created>2013-11-14</created><authors><author><keyname>Holtkamp</keyname><forenames>Hauke</forenames></author></authors><title>Enhancing the Energy Efficiency of Radio Base Stations</title><categories>cs.IT math.IT</categories><comments>PhD Thesis, University of Edinburgh, 2013</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This thesis is concerned with the energy efficiency of cellular networks. It
studies the dominant power consumer in future cellular networks, the Long Term
Evolution radio base stations (BS), and proposes mechanisms that enhance the BS
energy efficiency by reducing its power consumption under target rate
constraints. These mechanisms trade spare capacity for power saving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3548</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3548</id><created>2013-11-14</created><authors><author><keyname>Simpson</keyname><forenames>William R</forenames></author></authors><title>Wireless Computing and IT Ecosystems</title><categories>cs.OH</categories><comments>6 pages, 2 figures, keynote at WIMON13 June 2013, Istanbul</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We have evolved an IT system that is ubiquitous and pervasive and integrated
into most aspects of our lives. Many of us are working on 4th and 5th level
refinements in efficiency and functionality. But, we stand on the shoulders of
those who came before and this restricts our freedom of action. The prior work
has left us with an ecosystem which is the living embodiment of our
state-of-the-art. While we work on integration, refinement, broader application
and efficiency, the results must move seamlessly into the ecosystem.
Fundamental concepts are being researched in the lab and may rebuild the world
we all live in, until that happens, we must work within the ecosystem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3562</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3562</id><created>2013-11-14</created><authors><author><keyname>Said</keyname><forenames>Broumi</forenames></author><author><keyname>Smarandache</keyname><forenames>Florentin</forenames></author></authors><title>Intuitionistic Neutrosophic Soft Set</title><categories>cs.OH</categories><comments>10 pages. arXiv admin note: substantial text overlap with
  arXiv:1305.2724</comments><journal-ref>ISSN 1746-7659, England, UK Journal of Information and Computing
  Science Vol. 8, No. 2, 2013, pp.130-140</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the concept of intuitionistic neutrosophic set of
Bhowmik and Pal. We have introduced this concept in soft sets and defined
intuitionistic neutrosophic soft set. Some definitions and operations have been
introduced on intuitionistic neutrosophic soft set. Some properties of this
concept have been established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3566</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3566</id><created>2013-11-14</created><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author><author><keyname>Vijayalakshmi</keyname><forenames>V. J.</forenames></author></authors><title>Improving The Scalability By Contact Information Compression In Routing</title><categories>cs.IT cs.NI math.IT</categories><comments>5 pages and 3 figures</comments><doi>10.5121/ijit.2014.2404</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existence of reduced scalability and delivery leads to the development of
scalable routing by contact information compression. The previous work dealt
with the result of consistent analysis in the performance of DTN hierarchical
routing (DHR). It increases as the source to destination distance increases
with decreases in the routing performance. This paper focuses on improving the
scalability and delivery through contact information compression algorithm and
also addresses the problem of power awareness routing to increase the lifetime
of the overall network. Thus implementing the contact information compression
(CIC) algorithm the estimated shortest path (ESP) is detected dynamically. The
scalability and release are more improved during multipath multicasting, which
delivers the information to a collection of target concurrently in a single
transmission from the source
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3596</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3596</id><created>2013-11-14</created><authors><author><keyname>Kamola</keyname><forenames>Mariusz</forenames></author><author><keyname>Plamowski</keyname><forenames>Sebastian</forenames></author></authors><title>Simulation-based optimization of transportation costs in high pressure
  gas grid</title><categories>cs.SY</categories><comments>presents solution deployed in a production system</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Design, architecture and deployment details of a decision support system
engineered to minimize operating costs of compressor stations in a gas network
are presented. The system employs standard simulation software for pipelines,
combined with well known optimization routine for finding optimal station
control profiles in a repetitive way. A list of custom improvements is
presented that make the system capable and robust enough to perform the
optimization tasks. Implementation process is described in detail, covering the
case of handling extra optimality criteria postulated by the user. Benefits
from using the system and lessons learned are presented in the conclusions
section.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3598</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3598</id><created>2013-11-14</created><updated>2014-11-27</updated><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>A Statistical Model of Information Evaporation of Perfectly Reflecting
  Black Holes</title><categories>quant-ph cs.IT gr-qc hep-th math.IT</categories><comments>11 pages, Journal-ref: Int. J. Quant. Inf. (2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a statistical communication model for the phenomenon of quantum
information evaporation from black holes. A black hole behaves as a reflecting
quantum channel in a very special regime, which allows for a receiver to
perfectly recover the absorbed quantum information. The quantum channel of a
perfectly reflecting (PR) black hole is the probabilistically weighted sum of
infinitely many qubit cloning channels. In this work, we reveal the statistical
communication background of the information evaporation process of PR black
holes. We show that the density of the cloned quantum particles in function of
the PR black hole's mass approximates a Chi-square distribution, while the
stimulated emission process is characterized by zero-mean, circular symmetric
complex Gaussian random variables. The results lead to the existence of
Rayleigh random distributed coefficients in the probability density evolution,
which confirms the presence of Rayleigh fading (a special type of random
fluctuation) in the statistical communication model of black hole information
evaporation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3607</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3607</id><created>2013-11-14</created><updated>2014-04-28</updated><authors><author><keyname>Angelini</keyname><forenames>Patrizio</forenames></author><author><keyname>Da Lozzo</keyname><forenames>Giordano</forenames></author><author><keyname>Neuwirth</keyname><forenames>Daniel</forenames></author></authors><title>Advancements on SEFE and Partitioned Book Embedding Problems</title><categories>cs.CC cs.DM math.CO</categories><comments>29 pages, 10 figures, extended version of 'On Some NP-complete SEFE
  Problems' (Eighth International Workshop on Algorithms and Computation, 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we investigate the complexity of some problems related to the
{\em Simultaneous Embedding with Fixed Edges} (SEFE) of $k$ planar graphs and
the PARTITIONED $k$-PAGE BOOK EMBEDDING (PBE-$k$) problems, which are known to
be equivalent under certain conditions.
  While the computational complexity of SEFE for $k=2$ is still a central open
question in Graph Drawing, the problem is NP-complete for $k \geq 3$ [Gassner
{\em et al.}, WG '06], even if the intersection graph is the same for each pair
of graphs ({\em sunflower intersection}) [Schaefer, JGAA (2013)].
  We improve on these results by proving that SEFE with $k \geq 3$ and
sunflower intersection is NP-complete even when the intersection graph is a
tree and all the input graphs are biconnected. Also, we prove NP-completeness
for $k \geq 3$ of problem PBE-$k$ and of problem PARTITIONED T-COHERENT
$k$-PAGE BOOK EMBEDDING (PTBE-$k$) - that is the generalization of PBE-$k$ in
which the ordering of the vertices on the spine is constrained by a tree $T$ -
even when two input graphs are biconnected. Further, we provide a linear-time
algorithm for PTBE-$k$ when $k-1$ pages are assigned a connected graph.
Finally, we prove that the problem of maximizing the number of edges that are
drawn the same in a SEFE of two graphs is NP-complete in several restricted
settings ({\em optimization version of SEFE}, Open Problem $9$, Chapter $11$ of
the Handbook of Graph Drawing and Visualization).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3613</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3613</id><created>2013-11-14</created><authors><author><keyname>Carvajal</keyname><forenames>Rodrigo</forenames></author><author><keyname>Godoy</keyname><forenames>Boris I.</forenames></author><author><keyname>Ag&#xfc;ero</keyname><forenames>Juan C.</forenames></author></authors><title>A Bayesian approach to sparse channel estimation in OFDM systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we address the problem of estimating sparse communication
channels in OFDM systems in the presence of carrier frequency offset (CFO) and
unknown noise variance. To this end, we consider a convex optimization problem,
including a probability function, accounting for the sparse nature of the
communication channel. We use the Expectation-Maximization (EM) algorithm to
solve the corresponding Maximum A Posteriori (MAP) estimation problem. We show
that, by concentrating the cost function in one variable, namely the CFO, the
channel estimate can be obtained in closed form within the EM framework in the
maximization step. We present an example where we estimate the communication
channel, the CFO, the symbol, the noise variance, and the parameter defining
the prior distribution of the estimates. We compare the bit error rate
performance of our proposed MAP approach against Maximum Likelihood.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3618</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3618</id><created>2013-11-14</created><updated>2013-11-15</updated><authors><author><keyname>Cimpoi</keyname><forenames>Mircea</forenames></author><author><keyname>Maji</keyname><forenames>Subhransu</forenames></author><author><keyname>Kokkinos</keyname><forenames>Iasonas</forenames></author><author><keyname>Mohamed</keyname><forenames>Sammy</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author></authors><title>Describing Textures in the Wild</title><categories>cs.CV</categories><comments>13 pages; 12 figures Fixed misplaced affiliation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Patterns and textures are defining characteristics of many natural objects: a
shirt can be striped, the wings of a butterfly can be veined, and the skin of
an animal can be scaly. Aiming at supporting this analytical dimension in image
understanding, we address the challenging problem of describing textures with
semantic attributes. We identify a rich vocabulary of forty-seven texture terms
and use them to describe a large dataset of patterns collected in the wild.The
resulting Describable Textures Dataset (DTD) is the basis to seek for the best
texture representation for recognizing describable texture attributes in
images. We port from object recognition to texture recognition the Improved
Fisher Vector (IFV) and show that, surprisingly, it outperforms specialized
texture descriptors not only on our problem, but also in established material
recognition datasets. We also show that the describable attributes are
excellent texture descriptors, transferring between datasets and tasks; in
particular, combined with IFV, they significantly outperform the
state-of-the-art by more than 8 percent on both FMD and KTHTIPS-2b benchmarks.
We also demonstrate that they produce intuitive descriptions of materials and
Internet images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3623</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3623</id><created>2013-11-14</created><updated>2014-05-09</updated><authors><author><keyname>Bansal</keyname><forenames>Nikhil</forenames></author><author><keyname>Nagarajan</keyname><forenames>Viswanath</forenames></author></authors><title>On the Adaptivity Gap of Stochastic Orienteering</title><categories>cs.DS</categories><comments>Full version of IPCO 2014 paper, 27 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The input to the stochastic orienteering problem consists of a budget $B$ and
metric $(V,d)$ where each vertex $v$ has a job with deterministic reward and
random processing time (drawn from a known distribution). The processing times
are independent across vertices. The goal is to obtain a non-anticipatory
policy to run jobs at different vertices, that maximizes expected reward,
subject to the total distance traveled plus processing times being at most $B$.
An adaptive policy is one that can choose the next vertex to visit based on
observed random instantiations. Whereas, a non-adaptive policy is just given by
a fixed ordering of vertices. The adaptivity gap is the worst-case ratio of the
expected rewards of the optimal adaptive and non-adaptive policies.
  We prove an $\Omega(\log\log B)^{1/2}$ lower bound on the adaptivity gap of
stochastic orienteering. This provides a negative answer to the $O(1)$
adaptivity gap conjectured earlier, and comes close to the $O(\log\log B)$
upper bound. This result holds even on a line metric.
  We also show an $O(\log\log B)$ upper bound on the adaptivity gap for the
correlated stochastic orienteering problem, where the reward of each job is
random and possibly correlated to its processing time. Using this, we obtain an
improved quasi-polynomial time approximation algorithm for correlated
stochastic orienteering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3626</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3626</id><created>2013-11-14</created><authors><author><keyname>Honour</keyname><forenames>Eric</forenames><affiliation>Honourcode, Inc</affiliation></author></authors><title>Verification and Validation Issues in Systems of Systems</title><categories>cs.SE</categories><comments>In Proceedings AiSoS 2013, arXiv:1311.3195</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 133, 2013, pp. 2-7</journal-ref><doi>10.4204/EPTCS.133.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cutting edge in systems development today is in the area of &quot;systems of
systems&quot; (SoS) large networks of inter-related systems that are developed and
managed separately, but that also perform collective activities. Such large
systems typically involve constituent systems operating with different life
cycles, often with uncoordinated evolution. The result is an ever-changing SoS
in which adaptation and evolution replace the older engineering paradigm of
&quot;development&quot;. This short paper presents key thoughts about verification and
validation in this environment. Classic verification and validation methods
rely on having (a) a basis of proof, in requirements and in operational
scenarios, and (b) a known system configuration to be proven. However, with
constant SoS evolution, management of both requirements and system
configurations are problematic. Often, it is impossible to maintain a valid set
of requirements for the SoS due to the ongoing changes in the constituent
systems. Frequently, it is even difficult to maintain a vision of the SoS
operational use as users find new ways to adapt the SoS. These features of the
SoS result in significant challenges for system proof. In addition to
discussing the issues, the paper also indicates some of the solutions that are
currently used to prove the SoS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3627</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3627</id><created>2013-11-14</created><authors><author><keyname>Botterweck</keyname><forenames>Goetz</forenames><affiliation>Lero</affiliation></author></authors><title>Variability and Evolution in Systems of Systems</title><categories>cs.SE</categories><comments>In Proceedings AiSoS 2013, arXiv:1311.3195</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 133, 2013, pp. 8-23</journal-ref><doi>10.4204/EPTCS.133.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this position paper (1) we discuss two particular aspects of Systems of
Systems, i.e., variability and evolution. (2) We argue that concepts from
Product Line Engineering and Software Evolution are relevant to Systems of
Systems Engineering. (3) Conversely, concepts from Systems of Systems
Engineering can be helpful in Product Line Engineering and Software Evolution.
Hence, we argue that an exchange of concepts between the disciplines would be
beneficial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3628</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3628</id><created>2013-11-14</created><authors><author><keyname>Pazzi</keyname><forenames>Luca</forenames><affiliation>University of Modena and Reggio Emilia, Italy</affiliation></author></authors><title>Systems of Systems Modeled by a Hierarchical Part-Whole State-Based
  Formalism</title><categories>cs.SE</categories><comments>In Proceedings AiSoS 2013, arXiv:1311.3195</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 133, 2013, pp. 24-34</journal-ref><doi>10.4204/EPTCS.133.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an explicit state-based modeling approach aimed at
modeling Systems of Systems behavior. The approach allows to specify and verify
incrementally safety and liveness rules without using model checking
techniques. The state-based approach allows moreover to use the system behavior
directly as an interface, greatly improving the effectiveness of the recursive
composition needed when assembling Systems of Systems. Such systems are, at the
same time, both parts and wholes, thus giving a formal characterization to the
notion of Holon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3629</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3629</id><created>2013-11-14</created><authors><author><keyname>Kopetz</keyname><forenames>Hermann</forenames><affiliation>Vienna University of Technology</affiliation></author></authors><title>System-of-Systems Complexity</title><categories>cs.SE</categories><comments>In Proceedings AiSoS 2013, arXiv:1311.3195</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 133, 2013, pp. 35-39</journal-ref><doi>10.4204/EPTCS.133.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The global availability of communication services makes it possible to
interconnect independently developed systems, called constituent systems, to
provide new synergistic services and more efficient economic processes. The
characteristics of these new Systems-of-Systems are qualitatively different
from the classic monolithic systems. In the first part of this presentation we
elaborate on these differences, particularly with respect to the autonomy of
the constituent systems, to dependability, continuous evolution, and emergence.
In the second part we look at a SoS from the point of view of cognitive
complexity. Cognitive complexity is seen as a relation between a model of an
SoS and the observer. In order to understand the behavior of a large SoS we
have to generate models of adequate simplicity, i.e, of a cognitive complexity
that can be handled by the limited capabilities of the human mind. We will
discuss the importance of properly specifying and placing the relied-upon
message interfaces between the constituent systems that form an open SoS and
discuss simplification strategies that help to reduce the cognitive complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3630</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3630</id><created>2013-11-14</created><authors><author><keyname>Haverkort</keyname><forenames>Boudewijn R.</forenames><affiliation>University of Twente</affiliation></author></authors><title>Challenges for modelling and analysis in embedded systems and
  systems-of-systems design</title><categories>cs.SE</categories><comments>In Proceedings AiSoS 2013, arXiv:1311.3195</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 133, 2013, pp. 40-46</journal-ref><doi>10.4204/EPTCS.133.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last decade we have witnessed an increasing use of data processing
in embedded systems. Where in the past the data processing was limited (if
present at all) to the handling of a small number of &quot;on-off control signals&quot;,
more recently much more complex sensory data is being captured, processed and
used to improve system performance and dependability. The advent of
systems-of-systems aggravates the use of more and more data, for instance, by
bringing together data from several independent sources, allowing, in
principle, for even better performing systems. However, this ever stronger
data-orientation brings along several challenges in system design, both
technically and organisationally, and also forces manufacturers to think beyond
their traditional field of expertise. In this short paper, I will address these
new design challenges, through a number of examples. The paper finishes with
concrete challenges for supporting tools and techniques for system design in
this new context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3631</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3631</id><created>2013-11-14</created><authors><author><keyname>Arnold</keyname><forenames>Alexandre</forenames><affiliation>EADS Innovation Works</affiliation></author><author><keyname>Boyer</keyname><forenames>Beno&#xee;t</forenames><affiliation>INRIA Rennes</affiliation></author><author><keyname>Legay</keyname><forenames>Axel</forenames><affiliation>INRIA Rennes</affiliation></author></authors><title>Contracts and Behavioral Patterns for SoS: The EU IP DANSE approach</title><categories>cs.SE</categories><comments>In Proceedings AiSoS 2013, arXiv:1311.3195</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 133, 2013, pp. 47-66</journal-ref><doi>10.4204/EPTCS.133.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents some of the results of the first year of DANSE, one of
the first EU IP projects dedicated to SoS. Concretely, we offer a tool chain
that allows to specify SoS and SoS requirements at high level, and analyse them
using powerful toolsets coming from the formal verification area. At the high
level, we use UPDM, the system model provided by the british army as well as a
new type of contract based on behavioral patterns. At low level, we rely on a
powerful simulation toolset combined with recent advances from the area of
statistical model checking. The approach has been applied to a case study
developed at EADS Innovation Works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3632</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3632</id><created>2013-11-14</created><authors><author><keyname>Mignogna</keyname><forenames>Alessandro</forenames><affiliation>ALES S.r.l.</affiliation></author><author><keyname>Mangeruca</keyname><forenames>Leonardo</forenames><affiliation>ALES S.r.l.</affiliation></author><author><keyname>Boyer</keyname><forenames>Beno&#xee;t</forenames><affiliation>INRIA</affiliation></author><author><keyname>Legay</keyname><forenames>Axel</forenames><affiliation>INRIA</affiliation></author><author><keyname>Arnold</keyname><forenames>Alexandre</forenames><affiliation>EADS</affiliation></author></authors><title>SoS contract verification using statistical model checking</title><categories>cs.SE</categories><comments>In Proceedings AiSoS 2013, arXiv:1311.3195</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 133, 2013, pp. 67-83</journal-ref><doi>10.4204/EPTCS.133.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exhaustive formal verification for systems of systems (SoS) is impractical
and cannot be applied on a large scale. In this paper we propose to use
statistical model checking for efficient verification of SoS. We address three
relevant aspects for systems of systems: 1) the model of the SoS, which
includes stochastic aspects; 2) the formalization of the SoS requirements in
the form of contracts; 3) the tool-chain to support statistical model checking
for SoS. We adapt the SMC technique for application to heterogeneous SoS. We
extend the UPDM/SysML specification language to express the SoS requirements
that the implemented strategies over the SoS must satisfy. The requirements are
specified with a new contract language specifically designed for SoS, targeting
a high-level English- pattern language, but relying on an accurate semantics
given by the standard temporal logics. The contracts are verified against the
UPDM/SysML specification using the Statistical Model Checker (SMC) PLASMA
combined with the simulation engine DESYRE, which integrates heterogeneous
behavioral models through the functional mock-up interface (FMI) standard. The
tool-chain allows computing an estimation of the satisfiability of the
contracts by the SoS. The results help the system architect to trade-off
different solutions to guide the evolution of the SoS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3633</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3633</id><created>2013-11-14</created><authors><author><keyname>Bujorianu</keyname><forenames>Manuela L.</forenames><affiliation>University of Warwick</affiliation></author><author><keyname>Bujorianu</keyname><forenames>Marius C.</forenames><affiliation>University of Birmingham</affiliation></author></authors><title>A coordination model for ultra-large scale systems of systems</title><categories>cs.SY</categories><comments>In Proceedings AiSoS 2013, arXiv:1311.3195</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 133, 2013, pp. 84-98</journal-ref><doi>10.4204/EPTCS.133.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ultra large multi-agent systems are becoming increasingly popular due to
quick decay of the individual production costs and the potential of speeding up
the solving of complex problems. Examples include nano-robots, or systems of
nano-satellites for dangerous meteorite detection, or cultures of stem cells
for organ regeneration or nerve repair. The topics associated with these
systems are usually dealt within the theories of intelligent swarms or
biologically inspired computation systems. Stochastic models play an important
role and they are based on various formulations of the mechanical statistics.
In these cases, the main assumption is that the swarm elements have a simple
behaviour and that some average properties can be deduced for the entire swarm.
In contrast, complex systems in areas like aeronautics are formed by elements
with sophisticated behaviour, which are even autonomous. In situations like
this, a new approach to swarm coordination is necessary. We present a
stochastic model where the swarm elements are communicating autonomous systems,
the coordination is separated from the component autonomous activity and the
entire swarm can be abstracted away as a piecewise deterministic Markov
process, which constitutes one of the most popular model in stochastic control.
Keywords: ultra large multi-agent systems, system of systems, autonomous
systems, stochastic hybrid systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3638</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3638</id><created>2013-11-14</created><authors><author><keyname>Sahraoui</keyname><forenames>Leila</forenames></author><author><keyname>Messadeg</keyname><forenames>Djmail</forenames></author><author><keyname>Doghmane</keyname><forenames>Nouredinne</forenames></author></authors><title>Analyses and performance of techniques PAPR reduction for STBC MIMO-OFDM
  system in (4G) wireless communication</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1205.2269 by other authors</comments><msc-class>C.2</msc-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  An OFDM system is combined with multiple-input multiple-output (MIMO) in
order to increase the diversity gain and system capacity over the time variant
frequency-selective channels. However, a major drawback of MIMO-OFDM system is
that the transmitted signals on different antennas might exhibit high
peak-to-average power ratio (PAPR).In this paper, we present a PAPR analysis
reduction of space-timeblock-coded (STBC) MIMO-OFDM system for 4G
wirelessnetworks. Several techniques have been used to reduce the PAPR of the
(STBC) MIMOOFDM system: clipping and filtering, partial transmit sequence (PTS)
and selected mapping (SLM). Simulation results show that clipping and filtering
provides a better PAPR reduction than the others methods and only SLM technique
conserve the PAPR reduction in reception part of signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3640</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3640</id><created>2013-11-14</created><updated>2014-10-31</updated><authors><author><keyname>Karp</keyname><forenames>Jeremy</forenames></author><author><keyname>Ravi</keyname><forenames>R.</forenames></author></authors><title>A 9/7-Approximation Algorithm for Graphic TSP in Cubic Bipartite Graphs</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove new results for approximating Graphic TSP. Specifically, we provide
a polynomial-time \frac{9}{7}-approximation algorithm for cubic bipartite
graphs and a (\frac{9}{7}+\frac{1}{21(k-2)})-approximation algorithm for
k-regular bipartite graphs, both of which are improved approximation factors
compared to previous results. Our approach involves finding a cycle cover with
relatively few cycles, which we are able to do by leveraging the fact that all
cycles in bipartite graphs are of even length along with our knowledge of the
structure of cubic graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3646</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3646</id><created>2013-11-14</created><authors><author><keyname>Pedarsani</keyname><forenames>Ramtin</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Niesen</keyname><forenames>Urs</forenames></author></authors><title>Online Coded Caching</title><categories>cs.IT cs.NI math.IT</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a basic content distribution scenario consisting of a single
origin server connected through a shared bottleneck link to a number of users
each equipped with a cache of finite memory. The users issue a sequence of
content requests from a set of popular files, and the goal is to operate the
caches as well as the server such that these requests are satisfied with the
minimum number of bits sent over the shared link. Assuming a basic Markov model
for renewing the set of popular files, we characterize approximately the
optimal long-term average rate of the shared link. We further prove that the
optimal online scheme has approximately the same performance as the optimal
offline scheme, in which the cache contents can be updated based on the entire
set of popular files before each new request. To support these theoretical
results, we propose an online coded caching scheme termed coded least-recently
sent (LRS) and simulate it for a demand time series derived from the dataset
made available by Netflix for the Netflix Prize. For this time series, we show
that the proposed coded LRS algorithm significantly outperforms the popular
least-recently used (LRU) caching algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3651</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3651</id><created>2013-11-14</created><updated>2014-01-20</updated><authors><author><keyname>Bhaskara</keyname><forenames>Aditya</forenames></author><author><keyname>Charikar</keyname><forenames>Moses</forenames></author><author><keyname>Moitra</keyname><forenames>Ankur</forenames></author><author><keyname>Vijayaraghavan</keyname><forenames>Aravindan</forenames></author></authors><title>Smoothed Analysis of Tensor Decompositions</title><categories>cs.DS cs.LG stat.ML</categories><comments>32 pages (including appendix)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low rank tensor decompositions are a powerful tool for learning generative
models, and uniqueness results give them a significant advantage over matrix
decomposition methods. However, tensors pose significant algorithmic challenges
and tensors analogs of much of the matrix algebra toolkit are unlikely to exist
because of hardness results. Efficient decomposition in the overcomplete case
(where rank exceeds dimension) is particularly challenging. We introduce a
smoothed analysis model for studying these questions and develop an efficient
algorithm for tensor decomposition in the highly overcomplete case (rank
polynomial in the dimension). In this setting, we show that our algorithm is
robust to inverse polynomial error -- a crucial property for applications in
learning since we are only allowed a polynomial number of samples. While
algorithms are known for exact tensor decomposition in some overcomplete
settings, our main contribution is in analyzing their stability in the
framework of smoothed analysis.
  Our main technical contribution is to show that tensor products of perturbed
vectors are linearly independent in a robust sense (i.e. the associated matrix
has singular values that are at least an inverse polynomial). This key result
paves the way for applying tensor methods to learning problems in the smoothed
setting. In particular, we use it to obtain results for learning multi-view
models and mixtures of axis-aligned Gaussians where there are many more
&quot;components&quot; than dimensions. The assumption here is that the model is not
adversarially chosen, formalized by a perturbation of model parameters. We
believe this an appealing way to analyze realistic instances of learning
problems, since this framework allows us to overcome many of the usual
limitations of using tensor methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3669</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3669</id><created>2013-11-14</created><authors><author><keyname>Du</keyname><forenames>Nan</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author><author><keyname>Rodriguez</keyname><forenames>Manuel Gomez</forenames></author><author><keyname>Zha</keyname><forenames>Hongyuan</forenames></author></authors><title>Scalable Influence Estimation in Continuous-Time Diffusion Networks</title><categories>cs.SI cs.LG</categories><comments>To appear in Advances in Neural Information Processing Systems
  (NIPS), 2013</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If a piece of information is released from a media site, can it spread, in 1
month, to a million web pages? This influence estimation problem is very
challenging since both the time-sensitive nature of the problem and the issue
of scalability need to be addressed simultaneously. In this paper, we propose a
randomized algorithm for influence estimation in continuous-time diffusion
networks. Our algorithm can estimate the influence of every node in a network
with |V| nodes and |E| edges to an accuracy of $\varepsilon$ using
$n=O(1/\varepsilon^2)$ randomizations and up to logarithmic factors
O(n|E|+n|V|) computations. When used as a subroutine in a greedy influence
maximization algorithm, our proposed method is guaranteed to find a set of
nodes with an influence of at least (1-1/e)OPT-2$\varepsilon$, where OPT is the
optimal value. Experiments on both synthetic and real-world data show that the
proposed method can easily scale up to networks of millions of nodes while
significantly improves over previous state-of-the-arts in terms of the accuracy
of the estimated influence and the quality of the selected nodes in maximizing
the influence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3672</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3672</id><created>2013-11-14</created><authors><author><keyname>Mettler</keyname><forenames>Berenice</forenames></author><author><keyname>Kong</keyname><forenames>Zhaodan</forenames></author></authors><title>Hierarchical Model of Human Guidance Performance Based on Interaction
  Patterns in Behavior</title><categories>cs.HC</categories><comments>2nd International Conference on Application and Theory of Automation
  in Command and Control Systems (ICARUS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a framework for the investigation and modeling of human
spatial guidance behavior in complex environments. The model is derived from
the concept of interaction patterns, which represent the invariances or
symmetries inherent in the interactions between an agent and its environment.
These patterns provide the basic elements needed for the formalization of
spatial behavior and determine a natural hierarchy that can be unified under a
hierarchical hidden Markov model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3674</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3674</id><created>2013-11-14</created><authors><author><keyname>Sayama</keyname><forenames>Hiroki</forenames></author><author><keyname>Dionne</keyname><forenames>Shelley D.</forenames></author><author><keyname>Yammarino</keyname><forenames>Francis J.</forenames></author></authors><title>Evolutionary perspectives on collective decision making: Studying the
  implications of diversity and social network structure with agent-based
  simulations</title><categories>cs.MA cs.NE cs.SI physics.soc-ph</categories><comments>40 pages, 5 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collective, especially group-based, managerial decision making is crucial in
organizations. Using an evolutionary theory approach to collective decision
making, agent-based simulations were conducted to investigate how collective
decision making would be affected by the agents' diversity in problem
understanding and/or behavior in discussion, as well as by their social network
structure. Simulation results indicated that groups with consistent problem
understanding tended to produce higher utility values of ideas and displayed
better decision convergence, but only if there was no group-level bias in
collective problem understanding. Simulation results also indicated the
importance of balance between selection-oriented (i.e., exploitative) and
variation-oriented (i.e., explorative) behaviors in discussion to achieve
quality final decisions. Expanding the group size and introducing non-trivial
social network structure generally improved the quality of ideas at the cost of
decision convergence. Simulations with different social network topologies
revealed that collective decision making on small-world networks with high
local clustering tended to achieve highest decision quality more often than on
random or scale-free networks. Implications of this evolutionary theory and
simulation approach for future managerial research on collective, group, and
multi-level decision making are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3681</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3681</id><created>2013-11-14</created><updated>2015-01-27</updated><authors><author><keyname>Bauer</keyname><forenames>Ulrich</forenames></author><author><keyname>Lesnick</keyname><forenames>Michael</forenames></author></authors><title>Induced Matchings of Barcodes and the Algebraic Stability of Persistence</title><categories>math.AT cs.CG math.AC</categories><comments>Expanded journal version, to appear in Journal of Computational
  Geometry. Includes a proof that no definition of induced matching can be
  fully functorial (Proposition 5.10), and an extension of our single-morphism
  characterization of the interleaving relation to multidimensional persistence
  modules (Remark 6.7). Exposition is improved throughout. 11 Figures added</comments><msc-class>13P20, 55U99</msc-class><journal-ref>Journal of Computational Geometry 6:2 (2015), 162-191</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a simple, explicit map sending a morphism $f:M \rightarrow N$ of
pointwise finite dimensional persistence modules to a matching between the
barcodes of $M$ and $N$. Our main result is that, in a precise sense, the
quality of this matching is tightly controlled by the lengths of the longest
intervals in the barcodes of $\ker f$ and $\mathop{\mathrm{coker}} f$. As an
immediate corollary, we obtain a new proof of the algebraic stability of
persistence, a fundamental result in the theory of persistent homology. In
contrast to previous proofs, ours shows explicitly how a $\delta$-interleaving
morphism between two persistence modules induces a $\delta$-matching between
the barcodes of the two modules. Our main result also specializes to a
structure theorem for submodules and quotients of persistence modules, and
yields a novel &quot;single-morphism&quot; characterization of the interleaving relation
on persistence modules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3686</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3686</id><created>2013-11-13</created><authors><author><keyname>Kahanwal</keyname><forenames>Brijender</forenames></author><author><keyname>Singh</keyname><forenames>Dr. Tejinder Pal</forenames></author><author><keyname>Tuteja</keyname><forenames>Dr. R. K.</forenames></author></authors><title>Performance Evaluation of Java File Security System (JFSS)</title><categories>cs.OS cs.CR cs.PF</categories><comments>7 pages, 5 figures, journal</comments><journal-ref>Pelagia Research Library Advances in Applied Science Research,
  Vol. 2(6), pp. 254-260, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security is a critical issue of the modern file and storage systems, it is
imperative to protect the stored data from unauthorized access. We have
developed a file security system named as Java File Security System (JFSS) [1]
that guarantee the security to files on the demand of all users. It has been
developed on Java platform. Java has been used as programming language in order
to provide portability, but it enforces some performance limitations. It is
developed in FUSE (File System in User space) [3]. Many efforts have been done
over the years for developing file systems in user space (FUSE). All have their
own merits and demerits. In this paper we have evaluated the performance of
Java File Security System (JFSS). Over and over again, the increased security
comes at the expense of user convenience, performance or compatibility with
other systems. JFSS system performance evaluations show that encryption
overheads are modest as compared to security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3687</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3687</id><created>2013-11-14</created><authors><author><keyname>Murta</keyname><forenames>Daniel</forenames></author><author><keyname>Oliveira</keyname><forenames>Jose Nuno</forenames></author></authors><title>Calculating risk in functional programming</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the trend towards tolerating hardware unreliability, accuracy is exchanged
for cost savings. Running on less reliable machines, &quot;functionally correct&quot;
code becomes risky and one needs to know how risk propagates so as to mitigate
it. Risk estimation, however, seems to live outside the average programmer's
technical competence and core practice. In this paper we propose that risk be
constructively handled in functional programming by (a) writing programs which
may choose between expected and faulty behaviour, and by (b) reasoning about
them in a linear algebra extension to standard, a la Bird-Moor algebra of
programming. In particular, the propagation of faults across standard program
transformation techniques known as tupling and fusion is calculated, enabling
the fault of the whole to be expressed in terms of the faults of its parts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3715</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3715</id><created>2013-11-14</created><updated>2014-07-23</updated><authors><author><keyname>Karayev</keyname><forenames>Sergey</forenames></author><author><keyname>Trentacoste</keyname><forenames>Matthew</forenames></author><author><keyname>Han</keyname><forenames>Helen</forenames></author><author><keyname>Agarwala</keyname><forenames>Aseem</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Hertzmann</keyname><forenames>Aaron</forenames></author><author><keyname>Winnemoeller</keyname><forenames>Holger</forenames></author></authors><title>Recognizing Image Style</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The style of an image plays a significant role in how it is viewed, but style
has received little attention in computer vision research. We describe an
approach to predicting style of images, and perform a thorough evaluation of
different image features for these tasks. We find that features learned in a
multi-layer network generally perform best -- even when trained with object
class (not style) labels. Our large-scale learning methods results in the best
published performance on an existing dataset of aesthetic ratings and
photographic style annotations. We present two novel datasets: 80K Flickr
photographs annotated with 20 curated style labels, and 85K paintings annotated
with 25 style/genre labels. Our approach shows excellent classification
performance on both datasets. We use the learned classifiers to extend
traditional tag-based image search to consider stylistic constraints, and
demonstrate cross-dataset understanding of style.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3716</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3716</id><created>2013-11-14</created><authors><author><keyname>Obert</keyname><forenames>James</forenames></author><author><keyname>Cao</keyname><forenames>Huiping</forenames></author></authors><title>Determination of Multipath Security Using Efficient Pattern Matching</title><categories>cs.CR</categories><comments>10 pages, 12 figures</comments><journal-ref>IJCSIS, Vol. 11, No. 11, Nov. 2013</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Multipath routing is the use of multiple potential paths through a network in
order to enhance fault tolerance, optimize bandwidth use, and improve security.
Selecting data flow paths based on cost addresses performance issues but
ignores security threats. Attackers can disrupt the data flows by attacking the
links along the paths. Denial-of-service, remote exploitation, and other such
attacks launched on any single link can severely limit throughput. Networks can
be secured using a secure quality of service approach in which a sender
disperses data along multiple secure paths. In this secure multi-path approach,
a portion of the data from the sender is transmitted over each path and the
receiver assembles the data fragments that arrive. One of the largest
challenges in secure multipath routing is determining the security threat level
along each path and providing a commensurate level of encryption along that
path. The research presented explores the effects of real-world attack
scenarios in systems, and gauges the threat levels along each path. Optimal
sampling and compression of network data is provided via compressed sensing.
The probability of the presence of specific attack signatures along a network
path is determined using machine learning techniques. Using these
probabilities, information assurance levels are derived such that security
measures along vulnerable paths are increased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3720</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3720</id><created>2013-11-14</created><updated>2014-02-23</updated><authors><author><keyname>Kauers</keyname><forenames>Manuel</forenames></author><author><keyname>Yen</keyname><forenames>Lily</forenames></author></authors><title>On the length of integers in telescopers for proper hypergeometric terms</title><categories>cs.SC</categories><comments>21 pages, 2 figures, to appear in the Journal of Symbolic Computation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the number of digits in the integers of a creative telescoping
relation of expected minimal order for a bivariate proper hypergeometric term
has essentially cubic growth with the problem size. For telescopers of higher
order but lower degree we obtain a quintic bound. Experiments suggest that
these bounds are tight. As applications of our results, we give an improved
bound on the maximal possible integer root of the leading coefficient of a
telescoper, and the first discussion of the bit complexity of creative
telescoping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3728</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3728</id><created>2013-11-15</created><updated>2014-04-02</updated><authors><author><keyname>Liu</keyname><forenames>Jingcheng</forenames></author><author><keyname>Lu</keyname><forenames>Pinyan</forenames></author></authors><title>FPTAS for Counting Monotone CNF</title><categories>cs.DS</categories><comments>24 pages, 2 figures. version 1=&gt;2: minor edits, highlighted the
  picture of set cover/packing, and an implication of our previous result in 3D
  matching</comments><msc-class>05C70</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A monotone CNF formula is a Boolean formula in conjunctive normal form where
each variable appears positively. We design a deterministic fully
polynomial-time approximation scheme (FPTAS) for counting the number of
satisfying assignments for a given monotone CNF formula when each variable
appears in at most $5$ clauses. Equivalently, this is also an FPTAS for
counting set covers where each set contains at most $5$ elements. If we allow
variables to appear in a maximum of $6$ clauses (or sets to contain $6$
elements), it is NP-hard to approximate it. Thus, this gives a complete
understanding of the approximability of counting for monotone CNF formulas. It
is also an important step towards a complete characterization of the
approximability for all bounded degree Boolean #CSP problems. In addition, we
study the hypergraph matching problem, which arises naturally towards a
complete classification of bounded degree Boolean #CSP problems, and show an
FPTAS for counting 3D matchings of hypergraphs with maximum degree $4$.
  Our main technique is correlation decay, a powerful tool to design
deterministic FPTAS for counting problems defined by local constraints among a
number of variables. All previous uses of this design technique fall into two
categories: each constraint involves at most two variables, such as independent
set, coloring, and spin systems in general; or each variable appears in at most
two constraints, such as matching, edge cover, and holant problem in general.
The CNF problems studied here have more complicated structures than these
problems and require new design and proof techniques. As it turns out, the
technique we developed for the CNF problem also works for the hypergraph
matching problem. We believe that it may also find applications in other CSP or
more general counting problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3731</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3731</id><created>2013-11-15</created><authors><author><keyname>Emiris</keyname><forenames>Ioannis Z.</forenames></author><author><keyname>Pan</keyname><forenames>Victor Y.</forenames></author><author><keyname>Tsigaridas</keyname><forenames>Elias P.</forenames></author></authors><title>Chapter 10: Algebraic Algorithms</title><categories>cs.DS cs.NA cs.SC</categories><comments>41.1 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our Chapter in the upcoming Volume I: Computer Science and Software
Engineering of Computing Handbook (Third edition), Allen Tucker, Teo Gonzales
and Jorge L. Diaz-Herrera, editors, covers Algebraic Algorithms, both symbolic
and numerical, for matrix computations and root-finding for polynomials and
systems of polynomials equations. We cover part of these large subjects and
include basic bibliography for further study. To meet space limitation we cite
books, surveys, and comprehensive articles with pointers to further references,
rather than including all the original technical papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3732</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3732</id><created>2013-11-15</created><authors><author><keyname>Nguyen</keyname><forenames>Kien Duy</forenames></author><author><keyname>Minh</keyname><forenames>Tuan Pham</forenames></author><author><keyname>Nguyen</keyname><forenames>Quang Nhat</forenames></author><author><keyname>Nguyen</keyname><forenames>Thanh Trung</forenames></author></authors><title>Exploiting Direct and Indirect Information for Friend Suggestion in
  ZingMe</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>NIPS workshop, 9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Friend suggestion is a fundamental problem in social networks with the goal
of assisting users in creating more relationships, and thereby enhances
interest of users to the social networks. This problem is often considered to
be the link prediction problem in the network. ZingMe is one of the largest
social networks in Vietnam. In this paper, we analyze the current approach for
the friend suggestion problem in ZingMe, showing its limitations and
disadvantages. We propose a new efficient approach for friend suggestion that
uses information from the network structure, attributes and interactions of
users to create resources for the evaluation of friend connection amongst
users. Friend connection is evaluated exploiting both direct communication
between the users and information from other ones in the network. The proposed
approach has been implemented in a new system version of ZingMe. We conducted
experiments, exploiting a dataset derived from the users' real use of ZingMe,
to compare the newly proposed approach to the current approach and some
well-known ones for the accuracy of friend suggestion. The experimental results
show that the newly proposed approach outperforms the current one, i.e., by an
increase of 7% to 98% on average in the friend suggestion accuracy. The
proposed approach also outperforms other ones for users who have a small number
of friends with improvements from 20% to 85% on average. In this paper, we also
discuss a number of open issues and possible improvements for the proposed
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3735</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3735</id><created>2013-11-15</created><authors><author><keyname>Di Mauro</keyname><forenames>Nicola</forenames></author><author><keyname>Esposito</keyname><forenames>Floriana</forenames></author></authors><title>Ensemble Relational Learning based on Selective Propositionalization</title><categories>cs.LG cs.AI</categories><comments>10 pages. arXiv admin note: text overlap with arXiv:1006.5188</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dealing with structured data needs the use of expressive representation
formalisms that, however, puts the problem to deal with the computational
complexity of the machine learning process. Furthermore, real world domains
require tools able to manage their typical uncertainty. Many statistical
relational learning approaches try to deal with these problems by combining the
construction of relevant relational features with a probabilistic tool. When
the combination is static (static propositionalization), the constructed
features are considered as boolean features and used offline as input to a
statistical learner; while, when the combination is dynamic (dynamic
propositionalization), the feature construction and probabilistic tool are
combined into a single process. In this paper we propose a selective
propositionalization method that search the optimal set of relational features
to be used by a probabilistic learner in order to minimize a loss function. The
new propositionalization approach has been combined with the random subspace
ensemble method. Experiments on real-world datasets shows the validity of the
proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3746</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3746</id><created>2013-11-15</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>BiBi</keyname><forenames>A.</forenames></author><author><keyname>Javaid</keyname><forenames>A.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Latif</keyname><forenames>K.</forenames></author><author><keyname>Ishfaq</keyname><forenames>M.</forenames></author></authors><title>Investigating Quality Routing Link Metrics in Wireless Multi-hop
  Networks</title><categories>cs.NI</categories><comments>Journal of Annales of Telecommunications, 2013. arXiv admin note:
  substantial text overlap with arXiv:1108.3706</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new Quality Link Metric (QLM), ``Inverse Expected
Transmission Count (InvETX)'' in Optimized Link State Routing (OLSR) protocol.
Then we compare performance of three existing QLMs which are based on loss
probability measurements; Expected Transmission Count (ETX), Minimum Delay
(MD), Minimum Loss (ML) in Static Wireless Multi-hop Networks (SWMhNs). A novel
contribution of this paper is enhancement in conventional OLSR to achieve high
efficiency in terms of optimized routing load and routing latency. For this
purpose, first we present a mathematical framework, and then to validate this
frame work, we select three performance parameters to simulate default and
enhanced versions of OLSR. Three chosen performance parameters are; throughput,
Normalized Routing Load and End-to-End Delay. From simulation results, we
conclude that adjusting the frequencies of topological information exchange
results in high efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3749</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3749</id><created>2013-11-15</created><updated>2015-08-11</updated><authors><author><keyname>Shiraga</keyname><forenames>Takeharu</forenames></author><author><keyname>Yamauchi</keyname><forenames>Yukiko</forenames></author><author><keyname>Kijima</keyname><forenames>Shuji</forenames></author><author><keyname>Yamashita</keyname><forenames>Masafumi</forenames></author></authors><title>Deterministic Random Walks for Rapidly Mixing Chains</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rotor-router model is a deterministic process analogous to a simple
random walk on a graph. This paper is concerned with a generalized model,
functional-router model, which imitates a Markov chain possibly containing
irrational transition probabilities. We investigate the discrepancy of the
number of tokens at a single vertex between the functional-router model and its
corresponding Markov chain, and give an upper bound in terms of the mixing time
of the Markov chain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3764</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3764</id><created>2013-11-15</created><authors><author><keyname>Rej</keyname><forenames>Abhijnan</forenames></author></authors><title>Modeling systemic risks in financial markets</title><categories>q-fin.RM cs.SI physics.soc-ph</categories><comments>9 pages, discussion paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey systemic risks to financial markets and present a high-level
description of an algorithm that measures systemic risk in terms of coupled
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3766</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3766</id><created>2013-11-15</created><authors><author><keyname>Kolesov</keyname><forenames>A. E.</forenames></author><author><keyname>Vabishchevich</keyname><forenames>P. N.</forenames></author><author><keyname>Vasilyeva</keyname><forenames>M. V.</forenames></author></authors><title>Splitting schemes for poroelasticity and thermoelasticity problems</title><categories>cs.NA math.NA</categories><comments>19 pages, 8 figures</comments><msc-class>35Q74, 65M12, 65M60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider the coupled systems of linear unsteady partial
differential equations, which arise in the modeling of poroelasticity
processes. Stability estimates of weighted difference schemes for the coupled
system of equations are presented. Approximation in space is based on the
finite element method. We construct splitting schemes and give some numerical
comparisons for typical poroelasticity problems. The results of numerical
simulation of a 3D problem are presented. Special attention is given to using
hight performance computing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3772</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3772</id><created>2013-11-15</created><authors><author><keyname>Nagananda</keyname><forenames>K. G.</forenames></author></authors><title>Impact of system state dynamics on PMU placement in the electric power
  grid</title><categories>cs.SY</categories><comments>Submitted to IEEE Transactions on Power Systems. arXiv admin note:
  substantial text overlap with arXiv:1309.1300</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to study the impact of the dynamic nature of bus
voltage magnitudes and phase angles, which constitute the state of the power
system, on the phasor measurement unit (PMU) placement problem. To facilitate
this study, the placement problem is addressed from the perspective of the
electrical structure which, unlike existing work on PMU placement, accounts for
the sensitivity between power injections and nodal phase angle differences
between various buses in the power network. A linear dynamic model captures the
time evolution of system states, and a simple procedure is devised to estimate
the state transition function at each time instant. The placement problem is
formulated as a series (time steps) of binary integer programs, with the goal
to obtain the minimum number of PMUs at each time step for complete network
observability in the absence of zero injection measurements. Experiments are
conducted on several standard IEEE test bus systems. The main thesis of this
study is that, owing to the dynamic nature of the system states, for optimal
power system operation the best one could do is to install a PMU on each bus of
the given network, though it is undesirable from an economic standpoint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3773</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3773</id><created>2013-11-15</created><authors><author><keyname>Ghadermarzy</keyname><forenames>Navid</forenames></author><author><keyname>Mansour</keyname><forenames>Hassan</forenames></author><author><keyname>Yilmaz</keyname><forenames>Ozgur</forenames></author></authors><title>Non-Convex Compressed Sensing Using Partial Support Information</title><categories>cs.IT math.IT math.OC</categories><comments>22 pages, 10 figures</comments><msc-class>94A12, 94A20, 94A08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the recovery conditions of weighted $\ell_p$
minimization for signal reconstruction from compressed sensing measurements
when partial support information is available. We show that weighted $\ell_p$
minimization with $0&lt;p&lt;1$ is stable and robust under weaker sufficient
conditions compared to weighted $\ell_1$ minimization. Moreover, the sufficient
recovery conditions of weighted $\ell_p$ are weaker than those of regular
$\ell_p$ minimization if at least $50%$ of the support estimate is accurate. We
also review some algorithms which exist to solve the non-convex $\ell_p$
problem and illustrate our results with numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3779</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3779</id><created>2013-11-15</created><authors><author><keyname>Bajcinca</keyname><forenames>Naim</forenames></author></authors><title>On Pole Placement and Invariant Subspaces</title><categories>cs.SY</categories><comments>Presented at ICAT2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical eigenvalue assignment problem is revisited in this note. We
derive an analytic expression for pole placement which represents a slight
generalization of the celebrated Bass-Gura and Ackermann formulae, and also is
closely related to the modal procedure of Simon and Mitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3785</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3785</id><created>2013-11-15</created><authors><author><keyname>Menon</keyname><forenames>Vijay</forenames></author></authors><title>Deterministic Primality Testing - understanding the AKS algorithm</title><categories>cs.CC cs.CR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prime numbers play a very vital role in modern cryptography and especially
the difficulties involved in factoring numbers composed of product of two large
prime numbers have been put to use in many modern cryptographic designs. Thus,
the problem of distinguishing prime numbers from the rest is vital and
therefore there is a need to have efficient primality testing algorithms.
Although there had been many probabilistic algorithms for primality testing,
there wasn't a deterministic polynomial time algorithm until 2002 when Agrawal,
Kayal and Saxena came with an algorithm, popularly known as the AKS algorithm,
which could test whether a given number is prime or composite in polynomial
time. This project is an attempt at understanding the ingenious idea behind
this algorithm and the underlying principles of mathematics that is required to
study it. In fact, through out this project, one of the major objectives has
been to make it as much self contained as possible. Finally, the project
provides an implementation of the algorithm using Software for Algebra and
Geometry Experimentation (SAGE) and arrives at conclusions on how practical or
otherwise it is.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3798</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3798</id><created>2013-11-15</created><authors><author><keyname>Elberzhager</keyname><forenames>Frank</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Rombach</keyname><forenames>Dieter</forenames></author><author><keyname>Freimut</keyname><forenames>Bernd</forenames></author></authors><title>Integrating Inspection and Test Processes Based on Context-Specific
  Assumptions</title><categories>cs.SE</categories><comments>15 pages. The final version is available at
  http://onlinelibrary.wiley.com/doi/10.1002/smr.1569/abstract. Elberzhager,
  F., M\&quot;unch, J., Rombach, D. and Freimut, B. (2012), Integrating inspection
  and test processes based on context-specific assumptions. J. Softw. Evol. and
  Proc</comments><doi>10.1002/smr.1569</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspections and testing are two of the most commonly performed software
quality assurance processes today. Typically, these processes are applied in
isolation, which, however, fails to exploit the benefits of systematically
combining and integrating them. In consequence, tests are not focused based on
early defect detection data. Expected benefits of such process integration
include higher defect detection rates or reduced quality assurance effort.
Moreover, when conducting testing without any prior information regarding the
system's quality, it is often unclear how to focus testing. A systematic
integration of inspection and testing processes requires context-specific
knowledge about the relationships between inspections and testing. This
knowledge is typically not available and needs to be empirically identified and
validated. Often, context-specific assumptions can be seen as a starting point
for generating such knowledge. Based on the In2Test approach, which uses
inspection data to focus testing, we present in this article how knowledge
about the relationship between inspections and testing can be gained,
documented, and evolved in an analytical or empirical manner. In addition, this
article gives an overview of related work and highlights future research
directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3800</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3800</id><created>2013-11-15</created><authors><author><keyname>Keikha</keyname><forenames>Mohammad Mehdi</forenames></author><author><keyname>Nematbakhsh</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Ladani</keyname><forenames>Behrouz Tork</forenames></author></authors><title>Structural Weights in Ontology Matching</title><categories>cs.AI cs.IR</categories><journal-ref>2013-Vol 4- International Journal of Web &amp; Semantic Technology
  (IJWesT)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ontology matching finds correspondences between similar entities of different
ontologies. Two ontologies may be similar in some aspects such as structure,
semantic etc. Most ontology matching systems integrate multiple matchers to
extract all the similarities that two ontologies may have. Thus, we face a
major problem to aggregate different similarities. Some matching systems use
experimental weights for aggregation of similarities among different matchers
while others use machine learning approaches and optimization algorithms to
find optimal weights to assign to different matchers. However, both approaches
have their own deficiencies. In this paper, we will point out the problems and
shortcomings of current similarity aggregation strategies. Then, we propose a
new strategy, which enables us to utilize the structural information of
ontologies to get weights of matchers, for the similarity aggregation task. For
achieving this goal, we create a new Ontology Matching system which it uses
three available matchers, namely GMO, ISub and VDoc. We have tested our
similarity aggregation strategy on the OAEI 2012 data set. Experimental results
show significant improvements in accuracies of several cases, especially in
matching the classes of ontologies. We will compare the performance of our
similarity aggregation strategy with other well-known strategies
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3808</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3808</id><created>2013-11-15</created><authors><author><keyname>Asha</keyname><forenames>V.</forenames></author><author><keyname>Bhajantri</keyname><forenames>N. U.</forenames></author><author><keyname>Nagabhushan</keyname><forenames>P.</forenames></author></authors><title>Periodicity Extraction using Superposition of Distance Matching Function
  and One-dimensional Haar Wavelet Transform</title><categories>cs.CV</categories><comments>12 pages, ICSCI-2010</comments><msc-class>65D19, 65T60, 11K70, 62H30</msc-class><acm-class>I.0; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Periodicity of a texture is one of the important visual characteristics and
is often used as a measure for textural discrimination at the structural level.
Knowledge about periodicity of a texture is very essential in the field of
texture synthesis and texture compression and also in the design of frieze and
wall papers. In this paper, we propose a method of periodicity extraction from
noisy images based on superposition of distance matching function (DMF) and
wavelet decomposition without de-noising the test images. Overall DMFs are
subjected to single-level Haar wavelet decomposition to obtain approximate and
detailed coefficients. Extracted coefficients help in determination of
periodicities in row and column directions. We illustrate the usefulness and
the effectiveness of the proposed method in a texture synthesis application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3813</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3813</id><created>2013-11-15</created><authors><author><keyname>Badani</keyname><forenames>Dhruvil</forenames></author></authors><title>On Generating Permutations Under User-Defined Constraints</title><categories>cs.DM math.CO</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a method to generate permutations of a string under a set of
constraints decided by the user is presented. The required permutations are
generated without generating all the permutations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3816</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3816</id><created>2013-11-15</created><authors><author><keyname>Kalani</keyname><forenames>Geet</forenames></author><author><keyname>Nagaraju</keyname><forenames>A</forenames></author></authors><title>Applying Network Coding To Neighbour Topology Based Broadcasting
  Techniques in MANETs</title><categories>cs.NI</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In Mobile Ad-Hoc networks, broadcasting is a fundamental operation in the
network layer. A node transmits a rebroadcast message to any or all other nodes
whenever it receives for the first time. It will generate several redundant
transmissions and it ends up in a significant downside Broadcast Storm problem.
Within the literature, researchers have proposed 2-Hop Neighbour based protocol
like DP, TDP, PDP and APDP to reduce broadcast storm in MANETs by choosing the
minimum number of forwarding nodes from 1-Hop nodes to cover all 2-Hop nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3821</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3821</id><created>2013-11-15</created><authors><author><keyname>Al-Husainy</keyname><forenames>Dr. Mohammed Abbas Fadhil</forenames></author></authors><title>MAC Address as a Key for Data Encryption</title><categories>cs.CR</categories><comments>5 pages, 2 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computer networking, the Media Access Control (MAC) address is a unique
value associated with a network adapter. MAC addresses are also known as
hardware addresses or physical addresses. TCP/IP and other mainstream
networking architectures generally adopt the OSI model. MAC addresses function
at the data link layer (layer 2 in the OSI model). They allow computers to
uniquely identify themselves on a network at this relatively low level. In this
paper, suggested data encryption technique is presented by using the MAC
address as a key that is used to authenticate the receiver device like PC,
mobile phone, laptop or any other devices that is connected to the network.
This technique was tested on some data, visual and numerical measurements were
used to check the strength and performance of the technique. The experiments
showed that the suggested technique can be used easily to encrypt data that is
transmitted through networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3826</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3826</id><created>2013-11-15</created><updated>2014-06-19</updated><authors><author><keyname>Krishna</keyname><forenames>Shankara Narayanan</forenames></author><author><keyname>Mathur</keyname><forenames>Umang</forenames></author><author><keyname>Trivedi</keyname><forenames>Ashutosh</forenames></author></authors><title>Weak Singular Hybrid Automata</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The framework of Hybrid automata, introduced by Alur, Courcourbetis,
Henzinger, and Ho, provides a formal modeling and analysis environment to
analyze the interaction between the discrete and the continuous parts of
cyber-physical systems. Hybrid automata can be considered as generalizations of
finite state automata augmented with a finite set of real-valued variables
whose dynamics in each state is governed by a system of ordinary differential
equations. Moreover, the discrete transitions of hybrid automata are guarded by
constraints over the values of these real-valued variables, and enable
discontinuous jumps in the evolution of these variables. Singular hybrid
automata are a subclass of hybrid automata where dynamics is specified by
state-dependent constant vectors. Henzinger, Kopke, Puri, and Varaiya showed
that for even very restricted subclasses of singular hybrid automata, the
fundamental verification questions, like reachability and schedulability, are
undecidable. In this paper we present \emph{weak singular hybrid automata}
(WSHA), a previously unexplored subclass of singular hybrid automata, and show
the decidability (and the exact complexity) of various verification questions
for this class including reachability (NP-Complete) and LTL model-checking
(PSPACE-Complete). We further show that extending WSHA with a single
unrestricted clock or extending WSHA with unrestricted variable updates lead to
undecidability of reachability problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3829</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3829</id><created>2013-11-15</created><authors><author><keyname>Benbelkacem</keyname><forenames>Sofia</forenames></author><author><keyname>Atmani</keyname><forenames>Baghdad</forenames></author><author><keyname>Benamina</keyname><forenames>Mohamed</forenames></author></authors><title>Planning based on classification by induction graph</title><categories>cs.AI</categories><comments>International Conference on Data Mining &amp; Knowledge Management
  Process CDKP-2013</comments><doi>10.5121/csit.2013.3823</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Artificial Intelligence, planning refers to an area of research that
proposes to develop systems that can automatically generate a result set, in
the form of an integrated decision-making system through a formal procedure,
known as plan. Instead of resorting to the scheduling algorithms to generate
plans, it is proposed to operate the automatic learning by decision tree to
optimize time. In this paper, we propose to build a classification model by
induction graph from a learning sample containing plans that have an associated
set of descriptors whose values change depending on each plan. This model will
then operate for classifying new cases by assigning the appropriate plan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3833</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3833</id><created>2013-11-15</created><authors><author><keyname>Prieto-Castrillo</keyname><forenames>Francisco</forenames></author><author><keyname>Astillero</keyname><forenames>Antonio</forenames></author><author><keyname>Bot&#xf3;n-Fern&#xe1;ndez</keyname><forenames>Mar&#xed;a</forenames></author></authors><title>Distributed Computing on Complex Networks</title><categories>nlin.AO cs.DC</categories><comments>15 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers the problem of finding analytical expressions for the
expected values of dis- tributed computing performance metrics when the
underlying communication network has a complex structure. Through active
probing tests a real distributed computing environment is analysed. From the
resulting network, ensembles of synthetic graphs with additional structure are
used in Monte Carlo simulations to both validate analytical expressions and
explore the performance metrics under different conditions. Computing paradigms
with different hierarchical structures in computing ser- vices are gauged,
fully decentralised (i.e., peer-to-peer) environments providing the best
performance. Moreover, it is found that by implementing more intelligent
computing services configurations (e.g., betweenness centrality based mappings)
and task allocations strategies, significant improvements in the parallel
efficiency can be achieved. We qualitatively reproduce results from previous
works and provide closed-form solutions for the expected performance metrics
linking topological, application structure and allocation parameters when job
dependencies and a complex network structure are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3837</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3837</id><created>2013-11-15</created><authors><author><keyname>Hamami</keyname><forenames>Dalila</forenames></author><author><keyname>Atmani</keyname><forenames>Baghdad</forenames></author></authors><title>SBML for optimizing decision support's tools</title><categories>cs.CE</categories><journal-ref>Aircc.Proc. 3.8 (2013) 109-119</journal-ref><doi>10.5121/csit.2013.3810</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Many theoretical works and tools on epidemiological field reflect the
emphasis on decision-making Tools by both public health and the scientific
community, which continues to increase. Indeed, in the epidemiological field,
modeling tools are proving a very important way in helping to make decision.
However, the variety, the large volume of data and the nature of epidemics lead
us to seek solutions to alleviate the heavy burden imposed on both experts and
developers. In this paper, we present a new approach: the passage of an
epidemic model realized in Bio-PEPA to a narrative language using the basics of
SBML language. Our goal is to allow on one hand, epidemiologists to verify and
validate the model, and the other hand, developers to optimize the model in
order to achieve a better model of decision making. We also present some
preliminary results and some suggestions to improve the simulated model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3840</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3840</id><created>2013-11-15</created><authors><author><keyname>Rashid</keyname><forenames>Mahmood A.</forenames></author><author><keyname>Newton</keyname><forenames>M. A. Hakim</forenames></author><author><keyname>Hoque</keyname><forenames>Md. Tamjidul</forenames></author><author><keyname>Sattar</keyname><forenames>Abdul</forenames></author></authors><title>Mixing Energy Models in Genetic Algorithms for On-Lattice Protein
  Structure Prediction</title><categories>cs.CE cs.NE</categories><comments>Volume 2013, 15 pages, BioMed Research International, 2013</comments><report-no>Article ID 924137</report-no><doi>10.1155/2013/924137</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protein structure prediction (PSP) is computationally a very challenging
problem. The challenge largely comes from the fact that the energy function
that needs to be minimised in order to obtain the native structure of a given
protein is not clearly known. A high resolution 20x20 energy model could better
capture the behaviour of the actual energy function than a low resolution
energy model such as hydrophobic polar. However, the fine grained details of
the high resolution interaction energy matrix are often not very informative
for guiding the search. In contrast, a low resolution energy model could
effectively bias the search towards certain promising directions. In this
paper, we develop a genetic algorithm that mainly uses a high resolution energy
model for protein structure evaluation but uses a low resolution HP energy
model in focussing the search towards exploring structures that have
hydrophobic cores. We experimentally show that this mixing of energy models
leads to significant lower energy structures compared to the state-of-the-art
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3852</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3852</id><created>2013-11-15</created><authors><author><keyname>Rakic</keyname><forenames>Gordana</forenames></author><author><keyname>Budimac</keyname><forenames>Zoran</forenames></author></authors><title>Problems in Systematic Application of Software Metrics and Possible
  Solution</title><categories>cs.SE cs.PL</categories><comments>Rakic G, Budimac Z., Problems In Systematic Application Of Software
  Metrics And Possible Solution, In Proc. of The 5th International Conference
  on Information Technology (ICIT) May 11-13, 2011, Amman, Jordan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systematic application of software metric techniques can lead to significant
improvements of the quality of a final software product. However, there is
still the evident lack of wider utilization of software metrics techniques and
tools due to many reasons. In this paper we investigate some limitations of
contemporary software metrics tools and then propose construction of a new tool
that would solve some of the problems. We describe the promising prototype, its
internal structure, and then focus on its independency of the input language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3859</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3859</id><created>2013-11-15</created><updated>2013-11-20</updated><authors><author><keyname>Schwartz</keyname><forenames>Yannick</forenames><affiliation>INRIA Saclay - Ile de France, NEUROSPIN</affiliation></author><author><keyname>Thirion</keyname><forenames>Bertrand</forenames><affiliation>INRIA Saclay - Ile de France, NEUROSPIN</affiliation></author><author><keyname>Varoquaux</keyname><forenames>Ga&#xeb;l</forenames><affiliation>INRIA Saclay - Ile de France, LNAO</affiliation></author></authors><title>Mapping cognitive ontologies to and from the brain</title><categories>stat.ML cs.LG q-bio.NC</categories><comments>NIPS (Neural Information Processing Systems), United States (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Imaging neuroscience links brain activation maps to behavior and cognition
via correlational studies. Due to the nature of the individual experiments,
based on eliciting neural response from a small number of stimuli, this link is
incomplete, and unidirectional from the causal point of view. To come to
conclusions on the function implied by the activation of brain regions, it is
necessary to combine a wide exploration of the various brain functions and some
inversion of the statistical inference. Here we introduce a methodology for
accumulating knowledge towards a bidirectional link between observed brain
activity and the corresponding function. We rely on a large corpus of imaging
studies and a predictive engine. Technically, the challenges are to find
commonality between the studies without denaturing the richness of the corpus.
The key elements that we contribute are labeling the tasks performed with a
cognitive ontology, and modeling the long tail of rare paradigms in the corpus.
To our knowledge, our approach is the first demonstration of predicting the
cognitive content of completely new brain images. To that end, we propose a
method that predicts the experimental paradigms across different studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3867</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3867</id><created>2013-11-15</created><updated>2014-01-16</updated><authors><author><keyname>Haslegrave</keyname><forenames>John</forenames></author><author><keyname>Johnson</keyname><forenames>Richard A. B.</forenames></author><author><keyname>Koch</keyname><forenames>Sebastian</forenames></author></authors><title>The Robber Locating game</title><categories>math.CO cs.DM</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a game in which a cop searches for a moving robber on a graph
using distance probes, studied by Carragher, Choi, Delcourt, Erickson and West,
which is a slight variation on one introduced by Seager. Carragher, Choi,
Delcourt, Erickson and West show that for any fixed graph $G$ there is a
winning strategy for the cop on the graph $G^{1/m}$, obtained by replacing each
edge of $G$ by a path of length $m$, if $m$ is sufficiently large. They
conjecture that the cop does not have a winning strategy on $K_n^{1/m}$ if
$m&lt;n$; we show that in fact the cop wins if and only if $m\geqslant n/2$, for
all but a few small values of $n$. They also show that the robber can avoid
capture on any graph of girth 3, 4 or 5, and ask whether there is any graph of
girth 6 on which the cop wins. We show that there is, but that no such graph
can be bipartite; in the process we give a counterexample for their conjecture
that the set of graphs on which the cop wins is closed under the operation of
subdividing edges. We also give a complete answer to the question of when the
cop has a winning strategy on $K_{a,b}^{1/m}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3868</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3868</id><created>2013-11-15</created><authors><author><keyname>Borello</keyname><forenames>Martino</forenames></author></authors><title>On the automorphism groups of binary linear codes</title><categories>cs.IT math.CO math.IT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let C be a binary linear code and suppose that its automorphism group
contains a non trivial subgroup G. What can we say about C knowing G? In this
paper we collect some answers to this question in the cases G=C_p, G=C_2p and
G=D_2p (p an odd prime), with a particular regard to the case in which C is
self-dual. Furthermore we generalize some methods used in other papers on this
subject. Finally we give a short survey on the problem of determining the
automorphism group of a putative self-dual [72,36,16] code, in order to show
where these methods can be applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3874</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3874</id><created>2013-11-15</created><authors><author><keyname>Nyblom</keyname><forenames>M. A.</forenames></author><author><keyname>Evans</keyname><forenames>C. D.</forenames></author></authors><title>An Algorithm to Solve the Equal-Sum-Product Problem</title><categories>cs.DM math.CO</categories><msc-class>11D99</msc-class><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recursive algorithm is constructed which finds all solutions to a class of
Diophantine equations connected to the problem of determining ordered n-tuples
of positive integers satisfying the property that their sum is equal to their
product. An examination of the use of Binary Search Trees in implementing the
algorithm into a working program is given. In addition an application of the
algorithm for searching possible extra exceptional values of the
equal-sum-product problem is explored after demonstrating a link between these
numbers and the Sophie Germain primes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3877</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3877</id><created>2013-10-27</created><authors><author><keyname>Tomic</keyname><forenames>Ratko V.</forenames></author></authors><title>Optimal Networks from Error Correcting Codes</title><categories>cs.IT cs.NI math.CO math.IT</categories><comments>14 pages, accepted at ANCS 2013 conference</comments><acm-class>C.2.1; E.4; G.2.2</acm-class><journal-ref>Proc. ACM/IEEE ANCS 2013, pp. 169-180</journal-ref><doi>10.1109/ANCS.2013.6665199</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To address growth challenges facing large Data Centers and supercomputing
clusters a new construction is presented for scalable, high throughput, low
latency networks. The resulting networks require 1.5-5 times fewer switches,
2-6 times fewer cables, have 1.2-2 times lower latency and correspondingly
lower congestion and packet losses than the best present or proposed networks
providing the same number of ports at the same total bisection. These advantage
ratios increase with network size. The key new ingredient is the exact
equivalence discovered between the problem of maximizing network bisection for
large classes of practically interesting Cayley graphs and the problem of
maximizing codeword distance for linear error correcting codes. Resulting
translation recipe converts existent optimal error correcting codes into
optimal throughput networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3879</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3879</id><created>2013-11-15</created><authors><author><keyname>Alkhateeb</keyname><forenames>Faisal</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Euzenat</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author></authors><title>Answering SPARQL queries modulo RDF Schema with paths</title><categories>cs.DB</categories><comments>RR-8394; alkhateeb2003a</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SPARQL is the standard query language for RDF graphs. In its strict
instantiation, it only offers querying according to the RDF semantics and would
thus ignore the semantics of data expressed with respect to (RDF) schemas or
(OWL) ontologies. Several extensions to SPARQL have been proposed to query RDF
data modulo RDFS, i.e., interpreting the query with RDFS semantics and/or
considering external ontologies. We introduce a general framework which allows
for expressing query answering modulo a particular semantics in an homogeneous
way. In this paper, we discuss extensions of SPARQL that use regular
expressions to navigate RDF graphs and may be used to answer queries
considering RDFS semantics. We also consider their embedding as extensions of
SPARQL. These SPARQL extensions are interpreted within the proposed framework
and their drawbacks are presented. In particular, we show that the PSPARQL
query language, a strict extension of SPARQL offering transitive closure,
allows for answering SPARQL queries modulo RDFS graphs with the same complexity
as SPARQL through a simple transformation of the queries. We also consider
languages which, in addition to paths, provide constraints. In particular, we
present and compare nSPARQL and our proposal CPSPARQL. We show that CPSPARQL is
expressive enough to answer full SPARQL queries modulo RDFS. Finally, we
compare the expressiveness and complexity of both nSPARQL and the corresponding
fragment of CPSPARQL, that we call cpSPARQL. We show that both languages have
the same complexity through cpSPARQL, being a proper extension of SPARQL graph
patterns, is more expressive than nSPARQL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3882</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3882</id><created>2013-11-13</created><authors><author><keyname>Wang</keyname><forenames>Pinghui</forenames></author><author><keyname>Zhao</keyname><forenames>Junzhou</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author><author><keyname>Guan</keyname><forenames>Xiaohong</forenames></author></authors><title>Sampling Content Distributed Over Graphs</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite recent effort to estimate topology characteristics of large graphs
(i.e., online social networks and peer-to-peer networks), little attention has
been given to develop a formal methodology to characterize the vast amount of
content distributed over these networks. Due to the large scale nature of these
networks, exhaustive enumeration of this content is computationally
prohibitive. In this paper, we show how one can obtain content properties by
sampling only a small fraction of vertices. We first show that when sampling is
naively applied, this can produce a huge bias in content statistics (i.e.,
average number of content duplications). To remove this bias, one may use
maximum likelihood estimation to estimate content characteristics. However our
experimental results show that one needs to sample most vertices in the graph
to obtain accurate statistics using such a method. To address this challenge,
we propose two efficient estimators: special copy estimator (SCE) and weighted
copy estimator (WCE) to measure content characteristics using available
information in sampled contents. SCE uses the special content copy indicator to
compute the estimate, while WCE derives the estimate based on meta-information
in sampled vertices. We perform experiments to show WCE and SCE are cost
effective and also ``{\em asymptotically unbiased}''. Our methodology provides
a new tool for researchers to efficiently query content distributed in large
scale networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3887</identifier>
 <datestamp>2014-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3887</id><created>2013-11-15</created><updated>2014-08-14</updated><authors><author><keyname>Tomamichel</keyname><forenames>Marco</forenames></author><author><keyname>Berta</keyname><forenames>Mario</forenames></author><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author></authors><title>Relating different quantum generalizations of the conditional Renyi
  entropy</title><categories>quant-ph cs.IT math.IT</categories><comments>10 pages, 1 figure</comments><journal-ref>J. Math. Phys. 55 (8), 082206 (2014)</journal-ref><doi>10.1063/1.4892761</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently a new quantum generalization of the Renyi divergence and the
corresponding conditional Renyi entropies was proposed. Here we report on a
surprising relation between conditional Renyi entropies based on this new
generalization and conditional Renyi entropies based on the quantum relative
Renyi entropy that was used in previous literature. Our result generalizes the
well-known duality relation H(A|B) + H(A|C) = 0 of the conditional von Neumann
entropy for tripartite pure states to Renyi entropies of two different kinds.
  As a direct application, we prove a collection of inequalities that relate
different conditional Renyi entropies and derive a new entropic uncertainty
relation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3899</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3899</id><created>2013-11-15</created><updated>2014-01-27</updated><authors><author><keyname>Grohe</keyname><forenames>Martin</forenames></author><author><keyname>Kreutzer</keyname><forenames>Stephan</forenames></author><author><keyname>Siebertz</keyname><forenames>Sebastian</forenames></author></authors><title>Deciding first-order properties of nowhere dense graphs</title><categories>cs.LO math.CO</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowhere dense graph classes, introduced by Nesetril and Ossona de Mendez,
form a large variety of classes of &quot;sparse graphs&quot; including the class of
planar graphs, actually all classes with excluded minors, and also bounded
degree graphs and graph classes of bounded expansion.
  We show that deciding properties of graphs definable in first-order logic is
fixed-parameter tractable on nowhere dense graph classes. At least for graph
classes closed under taking subgraphs, this result is optimal: it was known
before that for all classes C of graphs closed under taking subgraphs, if
deciding first-order properties of graphs in C is fixed-parameter tractable,
then C must be nowhere dense (under a reasonable complexity theoretic
assumption).
  As a by-product, we give an algorithmic construction of sparse neighbourhood
covers for nowhere dense graphs. This extends and improves previous
constructions of neighbourhood covers for graph classes with excluded minors.
At the same time, our construction is considerably simpler than those. Our
proofs are based on a new game-theoretic characterisation of nowhere dense
graphs that allows for a recursive version of locality-based algorithms on
these classes. On the logical side, we prove a &quot;rank-preserving&quot; version of
Gaifman's locality theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3900</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3900</id><created>2013-11-15</created><authors><author><keyname>Vinogradova</keyname><forenames>Galina</forenames></author><author><keyname>Galam</keyname><forenames>Serge</forenames></author></authors><title>The Stabilizing Role of Global Alliances in the Dynamics of Coalition
  Forming</title><categories>physics.soc-ph cs.SI</categories><comments>19 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coalition forming is investigated among countries, which are coupled with
short range interactions, under the influence of external fields produced by
the existence of global alliances. The model rests on the natural model of
coalition forming inspired from Statistical Physics, where instabilities are a
consequence of decentralized maximization of the individual benefits of actors
within their long horizon of rationality as the ability to envision a way
through intermediate loosing states, to a better configuration. The effects of
those external incentives on the interactions between countries and the
eventual stabilization of coalitions are studied. The results shed a new light
on the understanding of the complex phenomena of stabilization and
fragmentation in the coalition dynamics and on the possibility to design stable
coalitions. In addition to the formal implementation of the model, the
phenomena is illustrated through some historical cases of conflicts in Western
Europe.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3903</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3903</id><created>2013-11-13</created><authors><author><keyname>Mimram</keyname><forenames>Samuel</forenames><affiliation>LIST</affiliation></author><author><keyname>Di Giusto</keyname><forenames>Cinzia</forenames><affiliation>LIST</affiliation></author></authors><title>A Categorical Theory of Patches</title><categories>cs.LO math.CT</categories><proxy>ccsd</proxy><journal-ref>MFPS - Mathematical Foundations of Programming Semantics 298
  (2013) 283-307</journal-ref><doi>10.1016/j.entcs.2013.09.018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When working with distant collaborators on the same documents, one often uses
a version control system, which is a program tracking the history of files and
helping importing modifications brought by others as patches. The
implementation of such a system requires to handle lots of situations depending
on the operations performed by users on files, and it is thus difficult to
ensure that all the corner cases have been correctly addressed. Here, instead
of verifying the implementation of such a system, we adopt a complementary
approach: we introduce a theoretical model, which is defined abstractly by the
universal property that it should satisfy, and work out a concrete description
of it. We begin by defining a category of files and patches, where the
operation of merging the effect of two coinitial patches is defined by pushout.
Since two patches can be incompatible, such a pushout does not necessarily
exist in the category, which raises the question of which is the correct
category to represent and manipulate files in conflicting state. We provide an
answer by investigating the free completion of the category of files under
finite colimits, and give an explicit description of this category: its objects
are finite sets labeled by lines equipped with a transitive relation and
morphisms are partial functions respecting labeling and relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3918</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3918</id><created>2013-11-15</created><updated>2014-01-16</updated><authors><author><keyname>Vishwakarma</keyname><forenames>Sanjay</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>Sum Secrecy Rate in Full-Duplex Wiretap Channel with Imperfect CSI</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the achievable sum secrecy rate in full-duplex
wiretap channel in the presence of an eavesdropper and imperfect channel state
information (CSI). We assume that the users participating in full-duplex
communication and the eavesdropper have single antenna each. The users have
individual transmit power constraints. They also transmit jamming signals to
improve the secrecy rates. We obtain the achievable perfect secrecy rate region
by maximizing the sum secrecy rate. We also obtain the corresponding optimum
powers of the message signals and the jamming signals. Numerical results that
show the impact of imperfect CSI on the achievable secrecy rate region are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3928</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3928</id><created>2013-11-15</created><authors><author><keyname>Kluth</keyname><forenames>Stefan</forenames></author></authors><title>HS06 Benchmark for an ARM Server</title><categories>physics.comp-ph cs.PF hep-ex</categories><comments>three pages, two figures</comments><report-no>MPP-2013-295</report-no><doi>10.1088/1742-6596/513/6/062025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We benchmarked an ARM cortex-A9 based server system with a four-core CPU
running at 1.1 GHz. The system used Ubuntu 12.04 as operating system and the
HEPSPEC 2006 (HS06) benchmarking suite was compiled natively with gcc-4.4 on
the system. The benchmark was run for various settings of the relevant gcc
compiler options. We did not find significant influence from the compiler
options on the benchmark result. The final HS06 benchmark result is 10.4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3939</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3939</id><created>2013-11-15</created><updated>2014-06-09</updated><authors><author><keyname>Hassidim</keyname><forenames>Avinatan</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Vardi</keyname><forenames>Shai</forenames></author></authors><title>Local computation mechanism design</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of Local Computation Mechanism Design - designing
game theoretic mechanisms which run in polylogarithmic time and space. Local
computation mechanisms reply to each query in polylogarithmic time and space,
and the replies to different queries are consistent with the same global
feasible solution. In addition, the computation of the payments is also done in
polylogarithmic time and space. Furthermore, the mechanisms need to maintain
incentive compatibility with respect to the allocation and payments.
  We present local computation mechanisms for a variety of classical
game-theoretical problems: 1. stable matching, 2. job scheduling, 3.
combinatorial auctions for unit-demand and k-minded bidders, and 4. the housing
allocation problem.
  For stable matching, some of our techniques may have general implications.
Specifically, we show that when the men's preference lists are bounded, we can
achieve an arbitrarily good approximation to the stable matching within a fixed
number of iterations of the Gale-Shapley algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3955</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3955</id><created>2013-11-15</created><authors><author><keyname>Brough</keyname><forenames>Tara</forenames></author></authors><title>Inverse semigroups with rational word problem are finite</title><categories>math.GR cs.FL</categories><comments>6 pages, no figures</comments><msc-class>20M35, 68Q45, 03D40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note proves a generalisation to inverse semigroups of Anisimov's theorem
that a group has regular word problem if and only if it is finite, answering a
question of Stuart Margolis. The notion of word problem used is the two-tape
word problem -- the set of all pairs of words over a generating set for the
semigroup which both represent the same element.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3959</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3959</id><created>2013-11-15</created><updated>2016-01-17</updated><authors><author><keyname>Mahmud</keyname><forenames>M. M. Hassan</forenames></author><author><keyname>Hawasly</keyname><forenames>Majd</forenames></author><author><keyname>Rosman</keyname><forenames>Benjamin</forenames></author><author><keyname>Ramamoorthy</keyname><forenames>Subramanian</forenames></author></authors><title>Clustering Markov Decision Processes For Continual Transfer</title><categories>cs.AI cs.LG</categories><comments>56 pages, submitted to Journal of Machine Learning Research</comments><msc-class>68T05</msc-class><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present algorithms to effectively represent a set of Markov decision
processes (MDPs), whose optimal policies have already been learned, by a
smaller source subset for lifelong, policy-reuse-based transfer learning in
reinforcement learning. This is necessary when the number of previous tasks is
large and the cost of measuring similarity counteracts the benefit of transfer.
The source subset forms an `$\epsilon$-net' over the original set of MDPs, in
the sense that for each previous MDP $M_p$, there is a source $M^s$ whose
optimal policy has $&lt;\epsilon$ regret in $M_p$. Our contributions are as
follows. We present EXP-3-Transfer, a principled policy-reuse algorithm that
optimally reuses a given source policy set when learning for a new MDP. We
present a framework to cluster the previous MDPs to extract a source subset.
The framework consists of (i) a distance $d_V$ over MDPs to measure
policy-based similarity between MDPs; (ii) a cost function $g(\cdot)$ that uses
$d_V$ to measure how good a particular clustering is for generating useful
source tasks for EXP-3-Transfer and (iii) a provably convergent algorithm,
MHAV, for finding the optimal clustering. We validate our algorithms through
experiments in a surveillance domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3961</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3961</id><created>2013-11-15</created><authors><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author><author><keyname>Mathur</keyname><forenames>Iti</forenames></author><author><keyname>Darbari</keyname><forenames>Hemant</forenames></author><author><keyname>Kumar</keyname><forenames>Ajai</forenames></author></authors><title>HEVAL: Yet Another Human Evaluation Metric</title><categories>cs.CL</categories><journal-ref>International Journal on Natural Language Computing Vol. 2, No.5,
  November 2013</journal-ref><doi>10.5121/ijnlc.2013.2502</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine translation evaluation is a very important activity in machine
translation development. Automatic evaluation metrics proposed in literature
are inadequate as they require one or more human reference translations to
compare them with output produced by machine translation. This does not always
give accurate results as a text can have several different translations. Human
evaluation metrics, on the other hand, lacks inter-annotator agreement and
repeatability. In this paper we have proposed a new human evaluation metric
which addresses these issues. Moreover this metric also provides solid grounds
for making sound assumptions on the quality of the text produced by a machine
translation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3979</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3979</id><created>2013-11-15</created><updated>2014-03-18</updated><authors><author><keyname>Shin</keyname><forenames>Dongmyoung</forenames></author><author><keyname>Park</keyname><forenames>Sung Gil</forenames></author><author><keyname>Song</keyname><forenames>Byung Soo</forenames></author><author><keyname>Kim</keyname><forenames>Eung Su</forenames></author><author><keyname>Kupervasser</keyname><forenames>Oleg</forenames></author><author><keyname>Pivovartchuk</keyname><forenames>Denis</forenames></author><author><keyname>Gartseev</keyname><forenames>Ilya</forenames></author><author><keyname>Antipov</keyname><forenames>Oleg</forenames></author><author><keyname>Kruchenkov</keyname><forenames>Evgeniy</forenames></author><author><keyname>Milovanov</keyname><forenames>Alexey</forenames></author><author><keyname>Kochetov</keyname><forenames>Andrey</forenames></author><author><keyname>Sazonov</keyname><forenames>Igor</forenames></author><author><keyname>Nogtev</keyname><forenames>Igor</forenames></author><author><keyname>Hyun</keyname><forenames>Sun Woo</forenames></author></authors><title>Precision improvement of MEMS gyros for indoor mobile robots with
  horizontal motion inspired by methods of TRIZ</title><categories>cs.RO cs.SY</categories><comments>6 pages, the paper is accepted to 9th IEEE International Conference
  on Nano/Micro Engineered and Molecular Systems, Hawaii, USA (IEEE-NEMS 2014)
  as an oral presentation</comments><journal-ref>Proceedings of 9th IEEE International Conference on Nano/Micro
  Engineered and Molecular Systems (IEEE-NEMS 2014) April 13-16,
  2014,Hawaii,USA, pp 102-107</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper, the problem of precision improvement for the MEMS gyrosensors
on indoor robots with horizontal motion is solved by methods of TRIZ (&quot;the
theory of inventive problem solving&quot;).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3982</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3982</id><created>2013-11-15</created><authors><author><keyname>Schein</keyname><forenames>Aaron</forenames></author><author><keyname>Moore</keyname><forenames>Juston</forenames></author><author><keyname>Wallach</keyname><forenames>Hanna</forenames></author></authors><title>Inferring Multilateral Relations from Dynamic Pairwise Interactions</title><categories>cs.AI cs.SI</categories><comments>NIPS 2013 Workshop on Frontiers of Network Analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlations between anomalous activity patterns can yield pertinent
information about complex social processes: a significant deviation from normal
behavior, exhibited simultaneously by multiple pairs of actors, provides
evidence for some underlying relationship involving those pairs---i.e., a
multilateral relation. We introduce a new nonparametric Bayesian latent
variable model that explicitly captures correlations between anomalous
interaction counts and uses these shared deviations from normal activity
patterns to identify and characterize multilateral relations. We showcase our
model's capabilities using the newly curated Global Database of Events,
Location, and Tone, a dataset that has seen considerable interest in the social
sciences and the popular press, but which has is largely unexplored by the
machine learning community. We provide a detailed analysis of the latent
structure inferred by our model and show that the multilateral relations
correspond to major international events and long-term international
relationships. These findings lead us to recommend our model for any
data-driven analysis of interaction networks where dynamic interactions over
the edges provide evidence for latent social structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3984</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3984</id><created>2013-11-15</created><updated>2014-12-01</updated><authors><author><keyname>Darst</keyname><forenames>Richard K.</forenames></author><author><keyname>Nussinov</keyname><forenames>Zohar</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author></authors><title>Improving the performance of algorithms to find communities in networks</title><categories>physics.soc-ph cs.SI</categories><comments>9 pages, 6 figures. Published version</comments><journal-ref>Phys. Rev. E 89, 032809 (2014)</journal-ref><doi>10.1103/PhysRevE.89.032809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many algorithms to detect communities in networks typically work without any
information on the cluster structure to be found, as one has no a priori
knowledge of it, in general. Not surprisingly, knowing some features of the
unknown partition could help its identification, yielding an improvement of the
performance of the method. Here we show that, if the number of clusters were
known beforehand, standard methods, like modularity optimization, would
considerably gain in accuracy, mitigating the severe resolution bias that
undermines the reliability of the results of the original unconstrained
version. The number of clusters can be inferred from the spectra of the
recently introduced non-backtracking and flow matrices, even in benchmark
graphs with realistic community structure. The limit of such two-step procedure
is the overhead of the computation of the spectra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3987</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3987</id><created>2013-11-14</created><authors><author><keyname>Beheshti</keyname><forenames>Seyed-Mehdi-Reza</forenames></author><author><keyname>Venugopal</keyname><forenames>Srikumar</forenames></author><author><keyname>Ryu</keyname><forenames>Seung Hwan</forenames></author><author><keyname>Benatallah</keyname><forenames>Boualem</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author></authors><title>Big Data and Cross-Document Coreference Resolution: Current State and
  Future Opportunities</title><categories>cs.CL cs.DC cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information Extraction (IE) is the task of automatically extracting
structured information from unstructured/semi-structured machine-readable
documents. Among various IE tasks, extracting actionable intelligence from
ever-increasing amount of data depends critically upon Cross-Document
Coreference Resolution (CDCR) - the task of identifying entity mentions across
multiple documents that refer to the same underlying entity. Recently, document
datasets of the order of peta-/tera-bytes has raised many challenges for
performing effective CDCR such as scaling to large numbers of mentions and
limited representational power. The problem of analysing such datasets is
called &quot;big data&quot;. The aim of this paper is to provide readers with an
understanding of the central concepts, subtasks, and the current
state-of-the-art in CDCR process. We provide assessment of existing
tools/techniques for CDCR subtasks and highlight big data challenges in each of
them to help readers identify important and outstanding issues for further
investigation. Finally, we provide concluding remarks and discuss possible
directions for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.3995</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.3995</id><created>2013-11-15</created><updated>2014-04-21</updated><authors><author><keyname>Zhang</keyname><forenames>Zhilin</forenames></author><author><keyname>Rao</keyname><forenames>Bhaskar D.</forenames></author><author><keyname>Jung</keyname><forenames>Tzyy-Ping</forenames></author></authors><title>Compressed Sensing for Energy-Efficient Wireless Telemonitoring:
  Challenges and Opportunities</title><categories>cs.IT math.IT stat.ML</categories><comments>Invited paper for 2013 Asilomar Conference on Signals, Systems &amp;
  Computers (Asilomar 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a lossy compression framework, compressed sensing has drawn much attention
in wireless telemonitoring of biosignals due to its ability to reduce energy
consumption and make possible the design of low-power devices. However, the
non-sparseness of biosignals presents a major challenge to compressed sensing.
This study proposes and evaluates a spatio-temporal sparse Bayesian learning
algorithm, which has the desired ability to recover such non-sparse biosignals.
It exploits both temporal correlation in each individual biosignal and
inter-channel correlation among biosignals from different channels. The
proposed algorithm was used for compressed sensing of multichannel
electroencephalographic (EEG) signals for estimating vehicle drivers'
drowsiness. Results showed that the drowsiness estimation was almost unaffected
even if raw EEG signals (containing various artifacts) were compressed by 90%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4001</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4001</id><created>2013-11-15</created><updated>2016-03-02</updated><authors><author><keyname>Braun</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Fiorini</keyname><forenames>Samuel</forenames></author><author><keyname>Pokutta</keyname><forenames>Sebastian</forenames></author></authors><title>Average case polyhedral complexity of the maximum stable set problem</title><categories>cs.CC</categories><msc-class>68Q17, 05C69, 05C80, 90C05</msc-class><doi>10.1007/s10107-016-0989-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the minimum number of constraints needed to formulate random
instances of the maximum stable set problem via linear programs (LPs), in two
distinct models. In the uniform model, the constraints of the LP are not
allowed to depend on the input graph, which should be encoded solely in the
objective function. There we prove a $2^{\Omega(n/ \log n)}$ lower bound with
probability at least $1 - 2^{-2^n}$ for every LP that is exact for a randomly
selected set of instances; each graph on at most n vertices being selected
independently with probability $p \geq 2^{-\binom{n/4}{2}+n}$. In the
non-uniform model, the constraints of the LP may depend on the input graph, but
we allow weights on the vertices. The input graph is sampled according to the
G(n, p) model. There we obtain upper and lower bounds holding with high
probability for various ranges of p. We obtain a super-polynomial lower bound
all the way from $p = \Omega(\log^{6+\varepsilon} / n)$ to $p = o (1 / \log
n)$. Our upper bound is close to this as there is only an essentially quadratic
gap in the exponent, which currently also exists in the worst-case model.
Finally, we state a conjecture that would close this gap, both in the
average-case and worst-case models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4002</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4002</id><created>2013-11-15</created><updated>2015-03-27</updated><authors><author><keyname>Kraus</keyname><forenames>Nicolai</forenames></author><author><keyname>Sattler</keyname><forenames>Christian</forenames></author></authors><title>Higher Homotopies in a Hierarchy of Univalent Universes</title><categories>math.LO cs.LO</categories><comments>v1: 30 pages, main results and a connectedness construction; v2: 14
  pages, only main results, improved presentation, final journal version,
  ancillary files with electronic appendix; v3: content unchanged, different
  documentclass reduced the number of pages to 12</comments><msc-class>03B15</msc-class><acm-class>F.4.1</acm-class><journal-ref>ACM Transactions on Computational Logic (TOCL), Volume 16 Issue 2,
  Article No. 18, March 2015</journal-ref><doi>10.1145/2729979</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For Martin-Lof type theory with a hierarchy U(0): U(1): U(2): ... of
univalent universes, we show that U(n) is not an n-type. Our construction also
solves the problem of finding a type that strictly has some high truncation
level without using higher inductive types. In particular, U(n) is such a type
if we restrict it to n-types. We have fully formalized and verified our results
within the dependently typed language and proof assistant Agda.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4007</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4007</id><created>2013-11-15</created><updated>2014-01-09</updated><authors><author><keyname>Ammendola</keyname><forenames>R.</forenames></author><author><keyname>Biagioni</keyname><forenames>A.</forenames></author><author><keyname>Frezza</keyname><forenames>O.</forenames></author><author><keyname>Lamanna</keyname><forenames>G.</forenames></author><author><keyname>Lonardo</keyname><forenames>A.</forenames></author><author><keyname>Cicero</keyname><forenames>F. Lo</forenames></author><author><keyname>Paolucci</keyname><forenames>P. S.</forenames></author><author><keyname>Pantaleo</keyname><forenames>F.</forenames></author><author><keyname>Rossetti</keyname><forenames>D.</forenames></author><author><keyname>Simula</keyname><forenames>F.</forenames></author><author><keyname>Sozzi</keyname><forenames>M.</forenames></author><author><keyname>Tosoratto</keyname><forenames>L.</forenames></author><author><keyname>Vicini</keyname><forenames>P.</forenames></author></authors><title>NaNet: a flexible and configurable low-latency NIC for real-time trigger
  systems based on GPUs</title><categories>physics.ins-det cs.DC</categories><comments>Proceedings for the TWEPP 2013 - Topical Workshop on Electronics for
  Particle Physics workshop</comments><doi>10.1088/1748-0221/9/02/C02023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NaNet is an FPGA-based PCIe X8 Gen2 NIC supporting 1/10 GbE links and the
custom 34 Gbps APElink channel. The design has GPUDirect RDMA capabilities and
features a network stack protocol offloading module, making it suitable for
building low-latency, real-time GPU-based computing systems. We provide a
detailed description of the NaNet hardware modular architecture. Benchmarks for
latency and bandwidth for GbE and APElink channels are presented, followed by a
performance analysis on the case study of the GPU-based low level trigger for
the RICH detector in the NA62 CERN experiment, using either the NaNet GbE and
APElink channels. Finally, we give an outline of project future activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4008</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4008</id><created>2013-11-15</created><updated>2014-06-16</updated><authors><author><keyname>Chatzikokolakis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Palamidessi</keyname><forenames>Catuscia</forenames></author><author><keyname>Stronati</keyname><forenames>Marco</forenames></author></authors><title>A Predictive Differentially-Private Mechanism for Mobility Traces</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing popularity of GPS-enabled hand-held devices,
location-based applications and services have access to accurate and real-time
location information, raising serious privacy concerns for their millions of
users. Trying to address these issues, the notion of geo-indistinguishability
was recently introduced, adapting the well-known concept of Differential
Privacy to the area of location-based systems. A Laplace-based obfuscation
mechanism satisfying this privacy notion works well in the case of a sporadic
use; Under repeated use, however, independently applying noise leads to a quick
loss of privacy due to the correlation between the location in the trace.
  In this paper we show that correlations in the trace can be in fact exploited
in terms of a prediction function that tries to guess the new location based on
the previously reported locations. The proposed mechanism tests the quality of
the predicted location using a private test; in case of success the prediction
is reported otherwise the location is sanitized with new noise. If there is
considerable correlation in the input trace, the extra cost of the test is
small compared to the savings in budget, leading to a more efficient mechanism.
  We evaluate the mechanism in the case of a user accessing a location-based
service while moving around in a city. Using a simple prediction function and
two budget spending stategies, optimizing either the utility or the budget
consumption rate, we show that the predictive mechanim can offer substantial
improvements over the independently applied noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4013</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4013</id><created>2013-11-15</created><authors><author><keyname>Li</keyname><forenames>Chenguang</forenames></author><author><keyname>Zhang</keyname><forenames>Yongan</forenames></author></authors><title>Percolation on the institute-enterprise R&amp;D collaboration networks</title><categories>physics.soc-ph cs.SI</categories><comments>14 pages, 4 figures, 6 tables, and 21 conferences</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Realistic network-like systems are usually composed of multiple networks with
interacting relations such as school-enterprise research and development
collaboration networks. Here we study the percolation properties of a special
kind of that R&amp;D collaboration networks, namely institute-enterprise R&amp;D
collaboration networks. We introduce two actual IERDCNs to show their
structural properties, and present a mathematical framework based on generating
functions for analyzing an interacting network with any connection probability.
Then we illustrate the percolation threshold and structural parameter
arithmetic in the sub-critical and supercritical regimes. We compare the
predictions of our mathematical framework and arithmetic to data for two real
R&amp;D collaboration networks and a number of simulations, and we find that they
are in remarkable agreement with the data. We show applications of the
framework to electronics R&amp;D collaboration networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4015</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4015</id><created>2013-11-15</created><authors><author><keyname>Liu</keyname><forenames>Jianming</forenames></author><author><keyname>Liang</keyname><forenames>Weiqian</forenames></author><author><keyname>Liu</keyname><forenames>Runsheng</forenames></author></authors><title>A Three-class ROC for Evaluating Doubletalk Detectors in Acoustic Echo
  Cancellation</title><categories>cs.IT math.IT</categories><comments>4 pages, ICASSP-2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Doubletalk detector (DTD) is essential to keep adaptive filter from diverging
in the presence of near-end speech in acoustic echo cancellation (AEC), and
there was a receiver operating characteristic (ROC) to characterize DTD
performance. However, the traditional ROC for evaluating DTD used a static
time-invariant room acoustic impulse response and could not evaluate DTDs which
distinguish echo path change from doubletalk. We solve these problems by
extending the traditional binary detection ROC to three class, and simulations
show the efficiency of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4021</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4021</id><created>2013-11-16</created><authors><author><keyname>Mnich</keyname><forenames>Matthias</forenames></author><author><keyname>Wiese</keyname><forenames>Andreas</forenames></author></authors><title>Scheduling Meets Fixed-Parameter Tractability</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fixed-parameter tractability analysis and scheduling are two core domains of
combinatorial optimization which led to deep understanding of many important
algorithmic questions. However, even though fixed-parameter algorithms are
appealing for many reasons, no such algorithms are known for many fundamental
scheduling problems.
  In this paper we present the first fixed-parameter algorithms for classical
scheduling problems such as makespan minimization, scheduling with
job-dependent cost functions-one important example being weighted flow time-and
scheduling with rejection. To this end, we identify crucial parameters that
determine the problems' complexity. In particular, we manage to cope with the
problem complexity stemming from numeric input values, such as job processing
times, which is usually a core bottleneck in the design of fixed-parameter
algorithms. We complement our algorithms with W[1]-hardness results showing
that for smaller sets of parameters the respective problems do not allow
FPT-algorithms. In particular, our positive and negative results for scheduling
with rejection explore a research direction proposed by D\'aniel Marx.
  We hope that our contribution yields a new and fresh perspective on
scheduling and fixed-parameter algorithms and will lead to further fruitful
interdisciplinary research connecting these two areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4029</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4029</id><created>2013-11-16</created><updated>2014-06-16</updated><authors><author><keyname>Krishnan</keyname><forenames>Dilip</forenames></author><author><keyname>Bruna</keyname><forenames>Joan</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author></authors><title>Blind Deconvolution with Non-local Sparsity Reweighting</title><categories>cs.CV</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind deconvolution has made significant progress in the past decade. Most
successful algorithms are classified either as Variational or Maximum
a-Posteriori ($MAP$). In spite of the superior theoretical justification of
variational techniques, carefully constructed $MAP$ algorithms have proven
equally effective in practice. In this paper, we show that all successful $MAP$
and variational algorithms share a common framework, relying on the following
key principles: sparsity promotion in the gradient domain, $l_2$ regularization
for kernel estimation, and the use of convex (often quadratic) cost functions.
Our observations lead to a unified understanding of the principles required for
successful blind deconvolution. We incorporate these principles into a novel
algorithm that improves significantly upon the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4033</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4033</id><created>2013-11-16</created><authors><author><keyname>Patel</keyname><forenames>Omprakash</forenames></author><author><keyname>Maravi</keyname><forenames>Yogendra P. S.</forenames></author><author><keyname>Sharma</keyname><forenames>Sanjeev</forenames></author></authors><title>A Comparative Study of Histogram Equalization Based Image Enhancement
  Techniques for Brightness Preservation and Contrast Enhancement</title><categories>cs.CV</categories><comments>15 pages, 5 figures, 4 tables, Signal &amp; Image Processing : An
  International Journal (SIPIJ)</comments><journal-ref>Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.4, No.5, October 2013</journal-ref><doi>10.5121/sipij.2013.4502</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Histogram Equalization is a contrast enhancement technique in the image
processing which uses the histogram of image. However histogram equalization is
not the best method for contrast enhancement because the mean brightness of the
output image is significantly different from the input image. There are several
extensions of histogram equalization has been proposed to overcome the
brightness preservation challenge. Contrast enhancement using brightness
preserving bi-histogram equalization (BBHE) and Dualistic sub image histogram
equalization (DSIHE) which divides the image histogram into two parts based on
the input mean and median respectively then equalizes each sub histogram
independently. This paper provides review of different popular histogram
equalization techniques and experimental study based on the absolute mean
brightness error (AMBE), peak signal to noise ratio (PSNR), Structure
similarity index (SSI) and Entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4036</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4036</id><created>2013-11-16</created><authors><author><keyname>Singh</keyname><forenames>Sunil Kumar</forenames></author><author><keyname>Duvvuru</keyname><forenames>Rajesh</forenames></author><author><keyname>Thakur</keyname><forenames>Saurabh Singh</forenames></author></authors><title>Congestion Control Technique Using Intelligent Traffic and Vanet</title><categories>cs.NI</categories><comments>10 Pages. IJCEA, 2013. arXiv admin note: substantial text overlap
  with arXiv:1203.2195 by other authors without attribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Road traffic jams is a most important problem in nearly all cities around the
world, especially in developing regions resulting in enormous delays, increased
fuel wastage and monetary losses. In this paper, we have obtained an in-sight
idea of simulating real world scenario of a critical region where traffic
congestion is very high. As it is not easy to set up and implement such a
complicated system in real world before knowing the impact of all parameters
used in Vehicle Ad hoc Network (VANET), a small real world area i.e. Jamshedpur
Regal area was taken into consideration, for studying the impact of mobility in
the VANET. Traffic movement has been deployed across the area under
consideration using one of the realistic vehicular mobility models. The
behavior of this network was simulated using SUMO and NS2 to study the impact
of traffic and traffic lights at the intersection on packet transmission over
Vehicle to Vehicle (V2V) communication using Ad hoc On Demand Vector (AODV)
routing protocol and IEEE 802.11 standard. From the traffic simulation results,
it is proved that congestion problem for the given sample map with traffic
lights and predefined flow can be solved. There was negligible congestion at
lanes when traffic logic was changed in accordance with allowing flow with high
traffic overload to have high priority than those with low load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4037</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4037</id><created>2013-11-16</created><authors><author><keyname>Mathew</keyname><forenames>Gloriya</forenames></author><author><keyname>Thomas</keyname><forenames>Shiney</forenames></author></authors><title>A Novel Multifactor Authentication System Ensuring Usability and
  Security</title><categories>cs.CR</categories><comments>10 pages, 4 figures, 1 Table</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  User authentication is one of the most important part of information
security. Computer security most commonly depends on passwords to authenticate
human users. Password authentication systems will be either been usable but not
secure, or secure but not usable. While there are different types of
authentication systems available alphanumeric password is the most commonly
used authentication mechanism. But this method has significant drawbacks. An
alternative solution to the text based authentication is Graphical User
Authentication based on the fact that humans tends to remember images better
than text. Graphical password authentication systems provide passwords which
are easy to be created and remembered by the user. However, the main issues of
simple graphical password techniques are shoulder surfing attack and image
gallery attack. Studies reveals that most of the graphical passwords are either
secure but not usable or usable but not secure. In this paper, a new technique
that uses cued click point graphical password method along with the one-time
session key is proposed. The goal is to propose a new authentication mechanism
using graphical password to achieve higher security and better usability
levels. The result of the system testing is evaluated and it reveals that the
proposed system ensures security and usability to a great extent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4040</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4040</id><created>2013-11-16</created><authors><author><keyname>Kalman</keyname><forenames>Miklos</forenames></author><author><keyname>Havasi</keyname><forenames>Ferenc</forenames></author></authors><title>Enhanced XML Validation using SRML</title><categories>cs.DB</categories><comments>18 pages</comments><journal-ref>International Journal of Web &amp; Semantic Technology (IJWesT) Vol.4,
  No.4, October 2013</journal-ref><doi>10.5121/ijwest.2013.4401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data validation is becoming more and more important with the ever-growing
amount of data being consumed and transmitted by systems over the Internet. It
is important to ensure that the data being sent is valid as it may contain
entry errors, which may be consumed by different systems causing further
errors. XML has become the defacto standard for data transfer. The XML Schema
Definition language (XSD) was created to help XML structural validation and
provide a schema for data type restrictions, however it does not allow for more
complex situations. In this article we introduce a way to provide rule based
XML validation and correction through the extension and improvement of our SRML
metalanguage. We also explore the option of applying it in a database as a
trigger for CRUD operations allowing more granular dataset validation on an
atomic level allowing for more complex dataset record validation rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4045</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4045</id><created>2013-11-16</created><updated>2014-08-12</updated><authors><author><keyname>van Iersel</keyname><forenames>Leo</forenames></author><author><keyname>Kelk</keyname><forenames>Steven</forenames></author><author><keyname>Scornavacca</keyname><forenames>Celine</forenames></author></authors><title>Kernelizations for the hybridization number problem on multiple
  nonbinary trees</title><categories>cs.DM q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a finite set $X$, a collection $\mathcal{T}$ of rooted phylogenetic
trees on $X$ and an integer $k$, the Hybridization Number problem asks if there
exists a phylogenetic network on $X$ that displays all trees from $\mathcal{T}$
and has reticulation number at most $k$. We show two kernelization algorithms
for Hybridization Number, with kernel sizes $4k(5k)^t$ and $20k^2(\Delta^+-1)$
respectively, with $t$ the number of input trees and $\Delta^+$ their maximum
outdegree. In addition, we present an $O(n^{f(k)})$ time algorithm, with
$n=|X|$ and $f$ some computable function of $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4046</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4046</id><created>2013-11-16</created><authors><author><keyname>Leike</keyname><forenames>Jan</forenames></author><author><keyname>Tiwari</keyname><forenames>Ashish</forenames></author></authors><title>Synthesis for Polynomial Lasso Programs</title><categories>cs.LO</categories><comments>Paper at VMCAI'14, including appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for the synthesis of polynomial lasso programs. These
programs consist of a program stem, a set of transitions, and an exit
condition, all in the form of algebraic assertions (conjunctions of polynomial
equalities). Central to this approach is the discovery of non-linear
(algebraic) loop invariants. We extend Sankaranarayanan, Sipma, and Manna's
template-based approach and prove a completeness criterion. We perform program
synthesis by generating a constraint whose solution is a synthesized program
together with a loop invariant that proves the program's correctness. This
constraint is non-linear and is passed to an SMT solver. Moreover, we can
enforce the termination of the synthesized program with the support of test
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4055</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4055</id><created>2013-11-16</created><authors><author><keyname>Bliznets</keyname><forenames>Ivan</forenames></author><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Villanger</keyname><forenames>Yngve</forenames></author></authors><title>Largest chordal and interval subgraphs faster than 2^n</title><categories>cs.DS</categories><comments>The preliminary version of this work appeared in the proceedings of
  ESA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that in an n-vertex graph, induced chordal and interval subgraphs
with the maximum number of vertices can be found in time $O(2^{\lambda n})$ for
some $\lambda&lt;1$. These are the first algorithms breaking the trivial $2^n
n^{O(1)}$ bound of the brute-force search for these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4056</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4056</id><created>2013-11-16</created><authors><author><keyname>Mo</keyname><forenames>Hongming</forenames></author><author><keyname>Su</keyname><forenames>Xiaoyan</forenames></author><author><keyname>Hu</keyname><forenames>Yong</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>A generalized evidence distance</title><categories>cs.AI cs.IT math.IT</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dempster-Shafer theory of evidence (D-S theory) is widely used in uncertain
information process. The basic probability assignment(BPA) is a key element in
D-S theory. How to measure the distance between two BPAs is an open issue. In
this paper, a new method to measure the distance of two BPAs is proposed. The
proposed method is a generalized of existing evidence distance. Numerical
examples are illustrated that the proposed method can overcome the shortcomings
of existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4064</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4064</id><created>2013-11-16</created><authors><author><keyname>Derbinsky</keyname><forenames>Nate</forenames></author><author><keyname>Bento</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Yedidia</keyname><forenames>Jonathan S.</forenames></author></authors><title>Methods for Integrating Knowledge with the Three-Weight Optimization
  Algorithm for Hybrid Cognitive Processing</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider optimization as an approach for quickly and
flexibly developing hybrid cognitive capabilities that are efficient, scalable,
and can exploit knowledge to improve solution speed and quality. In this
context, we focus on the Three-Weight Algorithm, which aims to solve general
optimization problems. We propose novel methods by which to integrate knowledge
with this algorithm to improve expressiveness, efficiency, and scaling, and
demonstrate these techniques on two example problems (Sudoku and circle
packing).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4066</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4066</id><created>2013-11-16</created><authors><author><keyname>Margulies</keyname><forenames>S.</forenames></author><author><keyname>Morton</keyname><forenames>J.</forenames></author></authors><title>Polynomial-time Solvable #CSP Problems via Algebraic Models and Pfaffian
  Circuits</title><categories>math.AC cs.CC cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Pfaffian circuit is a tensor contraction network where the edges are
labeled with changes of bases in such a way that a very specific set of
combinatorial properties are satisfied. By modeling the permissible changes of
bases as systems of polynomial equations, and then solving via computation, we
are able to identify classes of 0/1 planar #CSP problems solvable in
polynomial-time via the Pfaffian circuit evaluation theorem (a variant of L.
Valiant's Holant Theorem). We present two different models of 0/1 variables,
one that is possible under a homogeneous change of basis, and one that is
possible under a heterogeneous change of basis only. We enumerate a series of
1,2,3, and 4-arity gates/cogates that represent constraints, and define a class
of constraints that is possible under the assumption of a ``bridge&quot; between two
particular changes of bases. We discuss the issue of planarity of Pfaffian
circuits, and demonstrate possible directions in algebraic computation for
designing a Pfaffian tensor contraction network fragment that can simulate a
swap gate/cogate. We conclude by developing the notion of a decomposable
gate/cogate, and discuss the computational benefits of this definition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4082</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4082</id><created>2013-11-16</created><updated>2014-03-26</updated><authors><author><keyname>Liao</keyname><forenames>Qianli</forenames></author><author><keyname>Leibo</keyname><forenames>Joel Z</forenames></author><author><keyname>Mroueh</keyname><forenames>Youssef</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author></authors><title>Can a biologically-plausible hierarchy effectively replace face
  detection, alignment, and recognition pipelines?</title><categories>cs.CV</categories><comments>11 Pages, 4 Figures. Mar 26, (2014): Improved exposition. Added CBMM
  memo cover page. No substantive changes</comments><report-no>CBMM-003</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The standard approach to unconstrained face recognition in natural
photographs is via a detection, alignment, recognition pipeline. While that
approach has achieved impressive results, there are several reasons to be
dissatisfied with it, among them is its lack of biological plausibility. A
recent theory of invariant recognition by feedforward hierarchical networks,
like HMAX, other convolutional networks, or possibly the ventral stream,
implies an alternative approach to unconstrained face recognition. This
approach accomplishes detection and alignment implicitly by storing
transformations of training images (called templates) rather than explicitly
detecting and aligning faces at test time. Here we propose a particular
locality-sensitive hashing based voting scheme which we call &quot;consensus of
collisions&quot; and show that it can be used to approximate the full 3-layer
hierarchy implied by the theory. The resulting end-to-end system for
unconstrained face recognition operates on photographs of faces taken under
natural conditions, e.g., Labeled Faces in the Wild (LFW), without aligning or
cropping them, as is normally done. It achieves a drastic improvement in the
state of the art on this end-to-end task, reaching the same level of
performance as the best systems operating on aligned, closely cropped images
(no outside training data). It also performs well on two newer datasets,
similar to LFW, but more difficult: LFW-jittered (new here) and SUFR-W.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4086</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4086</id><created>2013-11-16</created><authors><author><keyname>Mansoul</keyname><forenames>Abdelhak</forenames></author><author><keyname>Atmani</keyname><forenames>Baghdad</forenames></author><author><keyname>Benbelkacem</keyname><forenames>Sofia</forenames></author></authors><title>A hybrid decision support system : application on healthcare</title><categories>cs.AI cs.LG</categories><comments>13 pages, 4 figures, SEAS 2013 Dubai conference, paper id : 16)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many systems based on knowledge, especially expert systems for medical
decision support have been developed. Only systems are based on production
rules, and cannot learn and evolve only by updating them. In addition, taking
into account several criteria induces an exorbitant number of rules to be
injected into the system. It becomes difficult to translate medical knowledge
or a support decision as a simple rule. Moreover, reasoning based on generic
cases became classic and can even reduce the range of possible solutions. To
remedy that, we propose an approach based on using a multi-criteria decision
guided by a case-based reasoning (CBR) approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4088</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4088</id><created>2013-11-16</created><authors><author><keyname>Kolaei</keyname><forenames>Adel Alinezhad</forenames></author><author><keyname>Ahmadzadeh</keyname><forenames>Marzieh</forenames></author></authors><title>The Optimization of Running Queries in Relational Databases Using
  ANT-Colony Algorithm</title><categories>cs.DB cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The issue of optimizing queries is a cost-sensitive process and with respect
to the number of associated tables in a query, its number of permutations grows
exponentially. On one hand, in comparison with other operators in relational
database, join operator is the most difficult and complicated one in terms of
optimization for reducing its runtime. Accordingly, various algorithms have so
far been proposed to solve this problem. On the other hand, the success of any
database management system (DBMS) means exploiting the query model. In the
current paper, the heuristic ant algorithm has been proposed to solve this
problem and improve the runtime of join operation. Experiments and observed
results reveal the efficiency of this algorithm compared to its similar
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4096</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4096</id><created>2013-11-16</created><updated>2014-11-06</updated><authors><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Tian</keyname><forenames>Chao</forenames></author><author><keyname>Vaishampayan</keyname><forenames>Vinay A.</forenames></author><author><keyname>Chen</keyname><forenames>Yih-Farn R.</forenames></author></authors><title>Distributed Data Storage Systems with Opportunistic Repair</title><categories>cs.IT math.IT</categories><comments>18 pages, revision from Infocom paper. arXiv admin note: text overlap
  with arXiv:0803.0632 by other authors</comments><journal-ref>Proc.Infocomm, Apr. 2014, pp.1-9</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reliability of erasure-coded distributed storage systems, as measured by
the mean time to data loss (MTTDL), depends on the repair bandwidth of the
code. Repair-efficient codes provide reliability values several orders of
magnitude better than conventional erasure codes. Current state of the art
codes fix the number of helper nodes (nodes participating in repair) a priori.
In practice, however, it is desirable to allow the number of helper nodes to be
adaptively determined by the network traffic conditions. In this work, we
propose an opportunistic repair framework to address this issue. It is shown
that there exists a threshold on the storage overhead, below which such an
opportunistic approach does not lose any efficiency from the optimal
storage-repair-bandwidth tradeoff; i.e. it is possible to construct a code
simultaneously optimal for different numbers of helper nodes. We further
examine the benefits of such opportunistic codes, and derive the MTTDL
improvement for two repair models: one with limited total repair bandwidth and
the other with limited individual-node repair bandwidth. In both settings, we
show orders of magnitude improvement in MTTDL. Finally, the proposed framework
is examined in a network setting where a significant improvement in MTTDL is
observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4111</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4111</id><created>2013-11-16</created><authors><author><keyname>Yang</keyname><forenames>Gang</forenames></author><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author></authors><title>Dynamic Resource Allocation for Multiple-Antenna Wireless Power Transfer</title><categories>cs.IT math.IT</categories><comments>30 pages, 6 figures, Submitted to the IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a point-to-point multiple-input-single-output (MISO) system where
a receiver harvests energy from a wireless power transmitter to power itself
for various applications. The transmitter performs energy beamforming by using
an instantaneous channel state information (CSI). The CSI is estimated at the
receiver by training via a preamble, and fed back to the transmitter. The
channel estimate is more accurate when longer preamble is used, but less time
is left for wireless power transfer before the channel changes. To maximize the
harvested energy, in this paper, we address the key challenge of balancing the
time resource used for channel estimation and wireless power transfer (WPT),
and also investigate the allocation of energy resource used for wireless power
transfer. First, we consider the general scenario where the preamble length is
allowed to vary dynamically. Taking into account the effects of imperfect CSI,
the optimal preamble length is obtained online by solving a dynamic programming
(DP) problem. The solution is shown to be a threshold-type policy that depends
only on the channel estimate power. Next, we consider the scenario in which the
preamble length is fixed. The optimal preamble length is optimized offline.
Furthermore, we derive the optimal power allocation schemes for both scenarios.
For the scenario of dynamic-length preamble, the power is allocated according
to both the optimal preamble length and the channel estimate power; while for
the scenario of fixed-length preamble, the power is allocated according to only
the channel estimate power. The analysis results are validated by numerical
simulations. Encouragingly, with optimal power allocation, the harvested energy
by using optimized fixed-length preamble is almost the same as the harvested
energy by employing dynamic-length preamble, hence allowing a low-complexity
WPT system to be implemented in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4112</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4112</id><created>2013-11-16</created><authors><author><keyname>Ding</keyname><forenames>Guoru</forenames></author><author><keyname>Wang</keyname><forenames>Long</forenames></author><author><keyname>Wu</keyname><forenames>Qihui</forenames></author></authors><title>Big Data Analytics in Future Internet of Things</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current research on Internet of Things (IoT) mainly focuses on how to enable
general objects to see, hear, and smell the physical world for themselves, and
make them connected to share the observations. In this paper, we argue that
only connected is not enough, beyond that, general objects should have the
capability to learn, think, and understand both the physical world by
themselves. On the other hand, the future IoT will be highly populated by large
numbers of heterogeneous networked embedded devices, which are generating
massive or big data in an explosive fashion. Although there is a consensus
among almost everyone on the great importance of big data analytics in IoT, to
date, limited results, especially the mathematical foundations, are obtained.
These practical needs impels us to propose a systematic tutorial on the
development of effective algorithms for big data analytics in future IoT, which
are grouped into four classes: 1) heterogeneous data processing, 2) nonlinear
data processing, 3) high-dimensional data processing, and 4) distributed and
parallel data processing. We envision that the presented research is offered as
a mere baby step in a potentially fruitful research direction. We hope that
this article, with interdisciplinary perspectives, will stimulate more
interests in research and development of practical and effective algorithms for
specific IoT applications, to enable smart resource allocation, automatic
network operation, and intelligent service provisioning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4115</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4115</id><created>2013-11-17</created><updated>2015-08-25</updated><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Neeman</keyname><forenames>Joe</forenames></author><author><keyname>Sly</keyname><forenames>Allan</forenames></author></authors><title>A Proof Of The Block Model Threshold Conjecture</title><categories>math.PR cs.SI</categories><comments>32 pages, v3 extends the results to slowly growing degrees</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a random graph model named the &quot;block model&quot; in statistics and the
&quot;planted partition model&quot; in theoretical computer science. In its simplest
form, this is a random graph with two equal-sized clusters, with a
between-class edge probability of $q$ and a within-class edge probability of
$p$.
  A striking conjecture of Decelle, Krzkala, Moore and Zdeborov\'a based on
deep, non-rigorous ideas from statistical physics, gave a precise prediction
for the algorithmic threshold of clustering in the sparse planted partition
model. In particular, if $p = a/n$ and $q = b/n$, $s=(a-b)/2$ and $p=(a+b)/2$
then Decelle et al.\ conjectured that it is possible to efficiently cluster in
a way correlated with the true partition if $s^2 &gt; p$ and impossible if $s^2 &lt;
p$. By comparison, the best-known rigorous result is that of Coja-Oghlan, who
showed that clustering is possible if $s^2 &gt; C p \ln p$ for some sufficiently
large $C$.
  In a previous work, we proved that indeed it is information theoretically
impossible to to cluster if $s^2 &lt; p$ and furthermore it is information
theoretically impossible to even estimate the model parameters from the graph
when $s^2 &lt; p$. Here we complete the proof of the conjecture by providing an
efficient algorithm for clustering in a way that is correlated with the true
partition when $s^2 &gt; p$. A different independent proof of the same result was
recently obtained by Laurent Massoulie.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4121</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4121</id><created>2013-11-17</created><authors><author><keyname>Slimani</keyname><forenames>Thabet</forenames></author></authors><title>Application of Rough Set Theory in Data Mining</title><categories>cs.DB</categories><comments>10 pages</comments><journal-ref>International journal of Computer Science &amp; Network Solutions,
  Volume 1. No3, pages 1-10, November 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rough set theory is a new method that deals with vagueness and uncertainty
emphasized in decision making. Data mining is a discipline that has an
important contribution to data analysis, discovery of new meaningful knowledge,
and autonomous decision making. The rough set theory offers a viable approach
for decision rule extraction from data.This paper, introduces the fundamental
concepts of rough set theory and other aspects of data mining, a discussion of
data representation with rough set theory including pairs of attribute-value
blocks, information tables reducts, indiscernibility relation and decision
tables. Additionally, the rough set approach to lower and upper approximations
and certain possible rule sets concepts are introduced. Finally, some
description about applications of the data mining system with rough set theory
is included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4126</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4126</id><created>2013-11-17</created><authors><author><keyname>Peng</keyname><forenames>Xiao-Long</forenames></author><author><keyname>Small</keyname><forenames>Michael</forenames></author><author><keyname>Xu</keyname><forenames>Xin-Jian</forenames></author><author><keyname>Fu</keyname><forenames>Xinchu</forenames></author></authors><title>Temporal prediction of epidemic patterns in community networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>21 pages, 10 figures, IoP-tex</comments><journal-ref>New Journal of Physics 15 (2013) 113033</journal-ref><doi>10.1088/1367-2630/15/11/113033</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most previous studies of epidemic dynamics on complex networks suppose that
the disease will eventually stabilize at either a disease-free state or an
endemic one. In reality, however, some epidemics always exhibit sporadic and
recurrent behaviour in one region because of the invasion from an endemic
population elsewhere. In this paper we address this issue and study a
susceptible-infected-susceptible epidemiological model on a network consisting
of two communities, where the disease is endemic in one community but
alternates between outbreaks and extinctions in the other. We provide a
detailed characterization of the temporal dynamics of epidemic patterns in the
latter community. In particular, we investigate the time duration of both
outbreak and extinction, and the time interval between two consecutive
inter-community infections, as well as their frequency distributions. Based on
the mean-field theory, we theoretically analyze these three timescales and
their dependence on the average node degree of each community, the transmission
parameters, and the number of intercommunity links, which are in good agreement
with simulations, except when the probability of overlaps between successive
outbreaks is too large. These findings aid us in better understanding the
bursty nature of disease spreading in a local community, and thereby suggesting
effective time-dependent control strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4150</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4150</id><created>2013-11-17</created><authors><author><keyname>Yan</keyname><forenames>Jian-Feng</forenames></author><author><keyname>Zeng</keyname><forenames>Jia</forenames></author><author><keyname>Liu</keyname><forenames>Zhi-Qiang</forenames></author><author><keyname>Gao</keyname><forenames>Yang</forenames></author></authors><title>Towards Big Topic Modeling</title><categories>cs.LG cs.DC cs.IR stat.ML</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To solve the big topic modeling problem, we need to reduce both time and
space complexities of batch latent Dirichlet allocation (LDA) algorithms.
Although parallel LDA algorithms on the multi-processor architecture have low
time and space complexities, their communication costs among processors often
scale linearly with the vocabulary size and the number of topics, leading to a
serious scalability problem. To reduce the communication complexity among
processors for a better scalability, we propose a novel communication-efficient
parallel topic modeling architecture based on power law, which consumes orders
of magnitude less communication time when the number of topics is large. We
combine the proposed communication-efficient parallel architecture with the
online belief propagation (OBP) algorithm referred to as POBP for big topic
modeling tasks. Extensive empirical results confirm that POBP has the following
advantages to solve the big topic modeling problem: 1) high accuracy, 2)
communication-efficient, 3) fast speed, and 4) constant memory usage when
compared with recent state-of-the-art parallel LDA algorithms on the
multi-processor architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4151</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4151</id><created>2013-11-17</created><authors><author><keyname>Benfriha</keyname><forenames>Hichem</forenames></author><author><keyname>Barigou</keyname><forenames>Fatiha</forenames></author><author><keyname>Atmani</keyname><forenames>Baghdad</forenames></author></authors><title>Lattice-cell : Hybrid approach for text categorization</title><categories>cs.IR</categories><comments>Computer Science &amp; Information Technology (CS &amp; IT) 2013</comments><doi>10.5121/csit.2013.3817</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new text categorization framework based on
Concepts Lattice and cellular automata. In this framework, concept structure
are modeled by a Cellular Automaton for Symbolic Induction (CASI). Our
objective is to reduce time categorization caused by the Concept Lattice. We
examine, by experiments the performance of the proposed approach and compare it
with other algorithms such as Naive Bayes and k nearest neighbors. The results
show performance improvement while reducing time categorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4158</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4158</id><created>2013-11-17</created><updated>2014-03-11</updated><authors><author><keyname>Anselmi</keyname><forenames>Fabio</forenames></author><author><keyname>Leibo</keyname><forenames>Joel Z.</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author><author><keyname>Mutch</keyname><forenames>Jim</forenames></author><author><keyname>Tacchetti</keyname><forenames>Andrea</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author></authors><title>Unsupervised Learning of Invariant Representations in Hierarchical
  Architectures</title><categories>cs.CV cs.LG</categories><comments>23 pages, 10 figures. November 21 2013: Added acknowledgment of NSF
  funding. No other changes. December 18 (2013): Fixed a figure. January 10
  (2014): Fixed a figure and some math in SI. March 10 2014: modified abstract
  and implementation section (main and SI); added a paragraph about sample
  complexity in SI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present phase of Machine Learning is characterized by supervised learning
algorithms relying on large sets of labeled examples ($n \to \infty$). The next
phase is likely to focus on algorithms capable of learning from very few
labeled examples ($n \to 1$), like humans seem able to do. We propose an
approach to this problem and describe the underlying theory, based on the
unsupervised, automatic learning of a ``good'' representation for supervised
learning, characterized by small sample complexity ($n$). We consider the case
of visual object recognition though the theory applies to other domains. The
starting point is the conjecture, proved in specific cases, that image
representations which are invariant to translations, scaling and other
transformations can considerably reduce the sample complexity of learning. We
prove that an invariant and unique (discriminative) signature can be computed
for each image patch, $I$, in terms of empirical distributions of the
dot-products between $I$ and a set of templates stored during unsupervised
learning. A module performing filtering and pooling, like the simple and
complex cells described by Hubel and Wiesel, can compute such estimates.
Hierarchical architectures consisting of this basic Hubel-Wiesel moduli inherit
its properties of invariance, stability, and discriminability while capturing
the compositional organization of the visual world in terms of wholes and
parts. The theory extends existing deep learning convolutional architectures
for image and speech recognition. It also suggests that the main computational
goal of the ventral stream of visual cortex is to provide a hierarchical
representation of new objects/images which is invariant to transformations,
stable, and discriminative for recognition---and that this representation may
be continuously learned in an unsupervised way during development and visual
experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4163</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4163</id><created>2013-11-17</created><updated>2014-09-29</updated><authors><author><keyname>Akofor</keyname><forenames>Earnest</forenames></author><author><keyname>Chen</keyname><forenames>Biao</forenames></author></authors><title>Interactive Distributed Detection: Architecture and Performance Analysis</title><categories>cs.IT math.IT math.OC math.ST stat.AP stat.TH</categories><comments>32 pages, 7 figures, Final version, Published in IEEE Transactions on
  Information Theory</comments><msc-class>94A13 (Primary), 94A05 (Secondary)</msc-class><journal-ref>Interactive Distributed Detection: Architecture and Performance
  Analysis, IEEE Trans. Info. Theory, Vol. 60, Issue 10, pgs. 6456 - 6473, 2014</journal-ref><doi>10.1109/TIT.2014.2346497</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the impact of interactive fusion on detection performance
in tandem fusion networks with conditionally independent observations. Within
the Neyman-Pearson framework, two distinct regimes are considered: the fixed
sample size test and the large sample test. For the former, it is established
that interactive distributed detection may strictly outperform the one-way
tandem fusion structure. However, for the large sample regime, it is shown that
interactive fusion has no improvement on the asymptotic performance
characterized by the Kullback-Leibler (KL) distance compared with the simple
one-way tandem fusion. The results are then extended to interactive fusion
systems where the fusion center and the sensor may undergo multiple steps of
memoryless interactions or that involve multiple peripheral sensors, as well as
to interactive fusion with soft sensor outputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4166</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4166</id><created>2013-11-17</created><authors><author><keyname>Chen</keyname><forenames>Shiyu</forenames></author><author><keyname>Hu</keyname><forenames>Yong</forenames></author><author><keyname>Mahadevan</keyname><forenames>Sankaran</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>A Visibility Graph Averaging Aggregation Operator</title><categories>cs.AI</categories><comments>33 pages, 9 figures</comments><doi>10.1016/j.physa.2014.02.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of aggregation is considerable importance in many disciplines. In
this paper, a new type of operator called visibility graph averaging (VGA)
aggregation operator is proposed. This proposed operator is based on the
visibility graph which can convert a time series into a graph. The weights are
obtained according to the importance of the data in the visibility graph.
Finally, the VGA operator is used in the analysis of the TAIEX database to
illustrate that it is practical and compared with the classic aggregation
operators, it shows its advantage that it not only implements the aggregation
of the data purely, but also conserves the time information, and meanwhile, the
determination of the weights is more reasonable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4167</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4167</id><created>2013-11-17</created><authors><author><keyname>Mayne</keyname><forenames>Richard</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Towards Hybrid Artificial-Slime Mould Devices</title><categories>cs.ET physics.bio-ph</categories><comments>Submitted to Biophysical Reviews and Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The plasmodium of the slime mould Physarum polycephalum has recently received
significant attention for its value as a highly malleable amorphous computing
substrate. In laboratory-based experiments, micro- and nanoscale artificial
circuit components were introduced into the P. polycephalum plasmdodium to
investigate the electrical properties and computational abilities of hybridised
slime mould. It was found through a combination of imaging techniques and
electrophysiological measurements that P. polycephalum is able to internalise a
range of electrically active nanoparticles, assemble them in vivo and
distribute them around the plasmodium. Hybridised plasmodium is able to form
biomorphic mineralised networks, both inside the living plasmodium and the
empty trails left in its wake by taxis, both of which facilitate the
transmission of electricity. Hybridisation also alters the bioelectrical
activity of the plasmodium and likely influences its information processing
capabilities. It was concluded that hybridised slime mould is a suitable
substrate for producing functional unconventional computing devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4168</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4168</id><created>2013-11-17</created><authors><author><keyname>Ucar</keyname><forenames>Inaki</forenames></author><author><keyname>Morato</keyname><forenames>Daniel</forenames></author><author><keyname>Magana</keyname><forenames>Eduardo</forenames></author><author><keyname>Izal</keyname><forenames>Mikel</forenames></author></authors><title>Duplicate detection methodology for IP network traffic analysis</title><categories>cs.NI</categories><comments>7 pages, 8 figures. For the GitHub project, see
  https://github.com/Enchufa2/nantools</comments><acm-class>C.2.3</acm-class><journal-ref>IEEE International Workshop on Measurements and Networking
  Proceedings, pp.161-166, 2013</journal-ref><doi>10.1109/IWMN.2013.6663796</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network traffic monitoring systems have to deal with a challenging problem:
the traffic capturing process almost invariably produces duplicate packets. In
spite of this, and in contrast with other fields, there is no scientific
literature addressing it. This paper establishes the theoretical background
concerning data duplication in network traffic analysis: generating mechanisms,
types of duplicates and their characteristics are described. On this basis, a
duplicate detection and removal methodology is proposed. Moreover, an
analytical and experimental study is presented, whose results provide a
dimensioning rule for this methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4176</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4176</id><created>2013-11-17</created><updated>2014-11-26</updated><authors><author><keyname>Kayes</keyname><forenames>Imrul</forenames></author><author><keyname>Chakareski</keyname><forenames>Jacob</forenames></author></authors><title>ComReg: A Complex Network Approach to Prioritize Test Cases for
  Regression Testing</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Regression testing is performed to provide confidence that changes in a part
of software do not affect other parts of the software. An execution of all
existing test cases is the best way to re-establish this confidence. However,
regression testing is an expensive process---there might be insufficient
resources (e.g., time, workforce) to allow for the re-execution of all test
cases. Regression test prioritization techniques attempt to re-order a
regression test suite based on some criteria so that highest priority test
cases are executed earlier.
  In this study, we want to prioritize test cases for regression testing based
on the dependency network of faults. In software testing, it is common that
some faults are consequences of other faults (leading faults). Moreover,
dependent faults can be removed if and only if the leading faults have been
removed. Our goal is to prioritize test cases so that test cases that exposed
leading faults (the most central faults in the fault dependency network) in the
system testing phase, are executed first in regression testing.
  We present ComReg, a test case prioritization technique based on the
dependency network of faults. We model a fault dependency network as a directed
graph and identify leading faults to prioritize test cases for regression
testing. We use a centrality aggregation technique which considers six network
representative centrality metrics to identify leading faults in the fault
dependency network. We also discuss the use of fault communities to select an
arbitrary percentage of the test cases from a prioritized regression test
suite. We conduct a case study that evaluates the effectiveness and
applicability of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4180</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4180</id><created>2013-11-17</created><updated>2013-12-30</updated><authors><author><keyname>Tresp</keyname><forenames>Volker</forenames></author><author><keyname>Zillner</keyname><forenames>Sonja</forenames></author><author><keyname>Costa</keyname><forenames>Maria J.</forenames></author><author><keyname>Huang</keyname><forenames>Yi</forenames></author><author><keyname>Cavallaro</keyname><forenames>Alexander</forenames></author><author><keyname>Fasching</keyname><forenames>Peter A.</forenames></author><author><keyname>Reis</keyname><forenames>Andre</forenames></author><author><keyname>Sedlmayr</keyname><forenames>Martin</forenames></author><author><keyname>Ganslandt</keyname><forenames>Thomas</forenames></author><author><keyname>Budde</keyname><forenames>Klemens</forenames></author><author><keyname>Hinrichs</keyname><forenames>Carl</forenames></author><author><keyname>Schmidt</keyname><forenames>Danilo</forenames></author><author><keyname>Daumke</keyname><forenames>Philipp</forenames></author><author><keyname>Sonntag</keyname><forenames>Daniel</forenames></author><author><keyname>Wittenberg</keyname><forenames>Thomas</forenames></author><author><keyname>Oppelt</keyname><forenames>Patricia G.</forenames></author><author><keyname>Krompass</keyname><forenames>Denis</forenames></author></authors><title>Towards a New Science of a Clinical Data Intelligence</title><categories>cs.CY cs.AI</categories><comments>NIPS 2013 Workshop: Machine Learning for Clinical Data Analysis and
  Healthcare, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we define Clinical Data Intelligence as the analysis of data
generated in the clinical routine with the goal of improving patient care. We
define a science of a Clinical Data Intelligence as a data analysis that
permits the derivation of scientific, i.e., generalizable and reliable results.
We argue that a science of a Clinical Data Intelligence is sensible in the
context of a Big Data analysis, i.e., with data from many patients and with
complete patient information. We discuss that Clinical Data Intelligence
requires the joint efforts of knowledge engineering, information extraction
(from textual and other unstructured data), and statistics and statistical
machine learning. We describe some of our main results as conjectures and
relate them to a recently funded research project involving two major German
university hospitals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4198</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4198</id><created>2013-11-17</created><authors><author><keyname>Liang</keyname><forenames>Shuying</forenames></author><author><keyname>Might</keyname><forenames>Matthew</forenames></author><author><keyname>Van Horn</keyname><forenames>David</forenames></author></authors><title>AnaDroid: Malware Analysis of Android with User-supplied Predicates</title><categories>cs.PL</categories><comments>Appears in the Workshop on Tools for Automatic Program Analysis,
  Seattle, Washington, June 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's mobile platforms provide only coarse-grained permissions to users
with regard to how third- party applications use sensitive private data.
Unfortunately, it is easy to disguise malware within the boundaries of
legitimately-granted permissions. For instance, granting access to &quot;contacts&quot;
and &quot;internet&quot; may be necessary for a text-messaging application to function,
even though the user does not want contacts transmitted over the internet. To
understand fine-grained application use of permissions, we need to statically
analyze their behavior. Even then, malware detection faces three hurdles: (1)
analyses may be prohibitively expensive, (2) automated analyses can only find
behaviors that they are designed to find, and (3) the maliciousness of any
given behavior is application-dependent and subject to human judgment. To
remedy these issues, we propose semantic-based program analysis, with a human
in the loop as an alternative approach to malware detection. In particular, our
analysis allows analyst-crafted semantic predicates to search and filter
analysis results. Human-oriented semantic-based program analysis can
systematically, quickly and concisely characterize the behaviors of mobile
applications. We describe a tool that provides analysts with a library of the
semantic predicates and the ability to dynamically trade speed and precision.
It also provides analysts the ability to statically inspect details of every
suspicious state of (abstract) execution in order to make a ruling as to
whether or not the behavior is truly malicious with respect to the intent of
the application. In addition, permission and profiling reports are generated to
aid analysts in identifying common malicious behaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4201</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4201</id><created>2013-11-17</created><authors><author><keyname>Liang</keyname><forenames>Shuying</forenames></author><author><keyname>Keep</keyname><forenames>Andrew W.</forenames></author><author><keyname>Might</keyname><forenames>Matthew</forenames></author><author><keyname>Lyde</keyname><forenames>Steven</forenames></author><author><keyname>Gilray</keyname><forenames>Thomas</forenames></author><author><keyname>Aldous</keyname><forenames>Petey</forenames></author><author><keyname>Van Horn</keyname><forenames>David</forenames></author></authors><title>Sound and Precise Malware Analysis for Android via Pushdown Reachability
  and Entry-Point Saturation</title><categories>cs.PL cs.CR</categories><comments>Appears in 3rd Annual ACM CCS workshop on Security and Privacy in
  SmartPhones and Mobile Devices (SPSM'13), Berlin, Germany, 2013</comments><acm-class>D.2.0; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Anadroid, a static malware analysis framework for Android apps.
Anadroid exploits two techniques to soundly raise precision: (1) it uses a
pushdown system to precisely model dynamically dispatched interprocedural and
exception-driven control-flow; (2) it uses Entry-Point Saturation (EPS) to
soundly approximate all possible interleavings of asynchronous entry points in
Android applications. (It also integrates static taint-flow analysis and least
permissions analysis to expand the class of malicious behaviors which it can
catch.) Anadroid provides rich user interface support for human analysts which
must ultimately rule on the &quot;maliciousness&quot; of a behavior.
  To demonstrate the effectiveness of Anadroid's malware analysis, we had teams
of analysts analyze a challenge suite of 52 Android applications released as
part of the Auto- mated Program Analysis for Cybersecurity (APAC) DARPA
program. The first team analyzed the apps using a ver- sion of Anadroid that
uses traditional (finite-state-machine-based) control-flow-analysis found in
existing malware analysis tools; the second team analyzed the apps using a
version of Anadroid that uses our enhanced pushdown-based
control-flow-analysis. We measured machine analysis time, human analyst time,
and their accuracy in flagging malicious applications. With pushdown analysis,
we found statistically significant (p &lt; 0.05) decreases in time: from 85
minutes per app to 35 minutes per app in human plus machine analysis time; and
statistically significant (p &lt; 0.05) increases in accuracy with the
pushdown-driven analyzer: from 71% correct identification to 95% correct
identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4211</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4211</id><created>2013-11-17</created><updated>2014-04-03</updated><authors><author><keyname>Cerina</keyname><forenames>Federica</forenames></author><author><keyname>Chessa</keyname><forenames>Alessandro</forenames></author><author><keyname>Pammolli</keyname><forenames>Fabio</forenames></author><author><keyname>Riccaboni</keyname><forenames>Massimo</forenames></author></authors><title>Network communities within and across borders</title><categories>physics.soc-ph cs.SI</categories><comments>Scientific Reports 4, 2014</comments><doi>10.1038/srep04546</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the impact of borders on the topology of spatially embedded
networks. Indeed territorial subdivisions and geographical borders
significantly hamper the geographical span of networks thus playing a key role
in the formation of network communities. This is especially important in
scientific and technological policy-making, highlighting the interplay between
pressure for the internationalization to lead towards a global innovation
system and the administrative borders imposed by the national and regional
institutions. In this study we introduce an outreach index to quantify the
impact of borders on the community structure and apply it to the case of the
European and US patent co-inventors networks. We find that (a) the US
connectivity decays as a power of distance, whereas we observe a faster
exponential decay for Europe; (b) European network communities essentially
correspond to nations and contiguous regions while US communities span multiple
states across the whole country without any characteristic geographic scale. We
confirm our findings by means of a set of simulations aimed at exploring the
relationship between different patterns of cross-border community structures
and the outreach index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4219</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4219</id><created>2013-11-17</created><updated>2014-11-25</updated><authors><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author><author><keyname>Thapper</keyname><forenames>Johan</forenames></author><author><keyname>Zivny</keyname><forenames>Stanislav</forenames></author></authors><title>The power of linear programming for general-valued CSPs</title><categories>cs.CC</categories><comments>A full version of a FOCS'12 paper by the last two authors
  (arXiv:1204.1079) and an ICALP'13 paper by the first author (arXiv:1207.7213)
  to appear in SIAM Journal on Computing (SICOMP)</comments><acm-class>F.2.m</acm-class><journal-ref>SIAM Journal on Computing 44(1) (2015) 1-36</journal-ref><doi>10.1137/130945648</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $D$, called the domain, be a fixed finite set and let $\Gamma$, called
the valued constraint language, be a fixed set of functions of the form
$f:D^m\to\mathbb{Q}\cup\{\infty\}$, where different functions might have
different arity $m$. We study the valued constraint satisfaction problem
parametrised by $\Gamma$, denoted by VCSP$(\Gamma)$. These are minimisation
problems given by $n$ variables and the objective function given by a sum of
functions from $\Gamma$, each depending on a subset of the $n$ variables.
Finite-valued constraint languages contain functions that take on only rational
values and not infinite values.
  Our main result is a precise algebraic characterisation of valued constraint
languages whose instances can be solved exactly by the basic linear programming
relaxation (BLP). For a valued constraint language $\Gamma$, BLP is a decision
procedure for $\Gamma$ if and only if $\Gamma$ admits a symmetric fractional
polymorphism of every arity. For a finite-valued constraint language $\Gamma$,
BLP is a decision procedure if and only if $\Gamma$ admits a symmetric
fractional polymorphism of some arity, or equivalently, if $\Gamma$ admits a
symmetric fractional polymorphism of arity 2.
  Using these results, we obtain tractability of several novel classes of
problems, including problems over valued constraint languages that are: (1)
submodular on arbitrary lattices; (2) $k$-submodular on arbitrary finite
domains; (3) weakly (and hence strongly) tree-submodular on arbitrary trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4223</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4223</id><created>2013-11-17</created><authors><author><keyname>B&#xe9;al</keyname><forenames>Marie-Pierre</forenames></author><author><keyname>Blockelet</keyname><forenames>Michel</forenames></author><author><keyname>Dima</keyname><forenames>C&#x1ce;t&#x1ce;lin</forenames></author></authors><title>Finite-type-Dyck shift spaces</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study some basic properties of sofic-Dyck shifts and finite-type-Dyck
shifts. We prove that the class of sofic-Dyck shifts is stable under proper
conjugacies. We prove a Decomposition Theorem of a proper conjugacy between
edge-Dyck shifts into a sequence of Dyck splittings and amalgamations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4224</identifier>
 <datestamp>2014-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4224</id><created>2013-11-17</created><updated>2014-05-12</updated><authors><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Pan</keyname><forenames>Indranil</forenames></author></authors><title>On the Mixed H2/H-infinity Loop Shaping Trade-offs in Fractional Order
  Control of the AVR System</title><categories>cs.SY math.OC</categories><comments>10 pages, 24 figures, 1 table, Accepted in IEEE Transactions on
  Industrial Informatics</comments><doi>10.1109/TII.2014.2322812</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper looks at frequency domain design of a fractional order (FO) PID
controller for an Automatic Voltage Regulator (AVR) system. Various performance
criteria of the AVR system are formulated as system norms and is then coupled
with an evolutionary multi-objective optimization (MOO) algorithm to yield
Pareto optimal design trade-offs. The conflicting performance measures consist
of the mixed H2/H-infinity designs for objectives like set-point tracking, load
disturbance and noise rejection, controller effort and as such are an
exhaustive study of various conflicting design objectives. A fuzzy logic based
mechanism is used to identify the best compromise solution on the Pareto
fronts. The advantages and disadvantages of using a FOPID controller over the
conventional PID controller, which are popular for industrial use, are
enunciated from the presented simulations. The relevance and impact of FO
controller design from the perspective of the dynamics of AVR control loop is
also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4227</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4227</id><created>2013-11-17</created><authors><author><keyname>Xiao</keyname><forenames>Yuanzhang</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Optimal Foresighted Multi-User Wireless Video</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen an explosion in wireless video communication systems.
Optimization in such systems is crucial - but most existing methods intended to
optimize the performance of multi-user wireless video transmission are
inefficient. Some works (e.g. Network Utility Maximization (NUM)) are myopic:
they choose actions to maximize instantaneous video quality while ignoring the
future impact of these actions. Such myopic solutions are known to be inferior
to foresighted solutions that optimize the long-term video quality.
Alternatively, foresighted solutions such as rate-distortion optimized packet
scheduling focus on single-user wireless video transmission, while ignoring the
resource allocation among the users.
  In this paper, we propose an optimal solution for performing joint
foresighted resource allocation and packet scheduling among multiple users
transmitting video over a shared wireless network. A key challenge in
developing foresighted solutions for multiple video users is that the users'
decisions are coupled. To decouple the users' decisions, we adopt a novel dual
decomposition approach, which differs from the conventional optimization
solutions such as NUM, and determines foresighted policies. Specifically, we
propose an informationally-decentralized algorithm in which the network manager
updates resource &quot;prices&quot; (i.e. the dual variables associated with the resource
constraints), and the users make individual video packet scheduling decisions
based on these prices. Because a priori knowledge of the system dynamics is
almost never available at run-time, the proposed solution can learn online,
concurrently with performing the foresighted optimization. Simulation results
show 7 dB and 3 dB improvements in Peak Signal-to-Noise Ratio (PSNR) over
myopic solutions and existing foresighted solutions, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4231</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4231</id><created>2013-11-17</created><authors><author><keyname>Might</keyname><forenames>Matthew</forenames></author><author><keyname>Smaragdakis</keyname><forenames>Yannis</forenames></author><author><keyname>Van Horn</keyname><forenames>David</forenames></author></authors><title>Resolving and Exploiting the $k$-CFA Paradox</title><categories>cs.PL</categories><comments>Appears in the ACM SIGPLAN 2010 Conference on Programming Language
  Design and Implementation (PLDI'10)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-level program analysis is a fundamental problem, taking the shape of
&quot;flow analysis&quot; in functional languages and &quot;points-to&quot; analysis in imperative
and object-oriented languages. Despite the similarities, the vocabulary and
results in the two communities remain largely distinct, with limited
cross-understanding. One of the few links is Shivers's $k$-CFA work, which has
advanced the concept of &quot;context-sensitive analysis&quot; and is widely known in
both communities.
  Recent results indicate that the relationship between the functional and
object-oriented incarnations of $k$-CFA is not as well understood as thought.
Van Horn and Mairson proved $k$-CFA for $k \geq 1$ to be EXPTIME-complete;
hence, no polynomial-time algorithm can exist. Yet, there are several
polynomial-time formulations of context-sensitive points-to analyses in
object-oriented languages. Thus, it seems that functional $k$-CFA may actually
be a profoundly different analysis from object-oriented $k$-CFA. We resolve
this paradox by showing that the exact same specification of $k$-CFA is
polynomial-time for object-oriented languages yet exponential- time for
functional ones: objects and closures are subtly different, in a way that
interacts crucially with context-sensitivity and complexity. This illumination
leads to an immediate payoff: by projecting the object-oriented treatment of
objects onto closures, we derive a polynomial-time hierarchy of
context-sensitive CFAs for functional programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4235</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4235</id><created>2013-11-17</created><authors><author><keyname>Mart&#xed;nez-Plumed</keyname><forenames>Fernando</forenames></author><author><keyname>Ferri</keyname><forenames>C&#xe8;sar</forenames></author><author><keyname>Hern&#xe1;ndez-Orallo</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Ram&#xed;rez-Quintana</keyname><forenames>Mar&#xed;a-Jos&#xe9;</forenames></author></authors><title>On the definition of a general learning system with user-defined
  operators</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we push forward the idea of machine learning systems whose
operators can be modified and fine-tuned for each problem. This allows us to
propose a learning paradigm where users can write (or adapt) their operators,
according to the problem, data representation and the way the information
should be navigated. To achieve this goal, data instances, background
knowledge, rules, programs and operators are all written in the same functional
language, Erlang. Since changing operators affect how the search space needs to
be explored, heuristics are learnt as a result of a decision process based on
reinforcement learning where each action is defined as a choice of operator and
rule. As a result, the architecture can be seen as a 'system for writing
machine learning systems' or to explore new operators where the policy reuse
(as a kind of transfer learning) is allowed. States and actions are represented
in a Q matrix which is actually a table, from which a supervised model is
learnt. This makes it possible to have a more flexible mapping between old and
new problems, since we work with an abstraction of rules and actions. We
include some examples sharing reuse and the application of the system gErl to
IQ problems. In order to evaluate gErl, we will test it against some structured
problems: a selection of IQ test tasks and some experiments on some structured
prediction problems (list patterns).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4243</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4243</id><created>2013-11-17</created><updated>2016-01-17</updated><authors><author><keyname>Bhattacharya</keyname><forenames>Bhaswar B.</forenames></author></authors><title>Collision Times in Multicolor Urn Models and Sequential Graph Coloring
  With Applications to Discrete Logarithms</title><categories>math.PR cs.CR math.CO</categories><comments>Revised. 25 pages, 2 figures. To appear in Annals of Applied
  Probability</comments><msc-class>05C15, 60F05, 94A62, 60G55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider an urn model where at each step one of $q$ colors is sampled
according to some probability distribution and a ball of that color is placed
in an urn. The distribution of assigning balls to urns may depend on the color
of the ball. Collisions occur when a ball is placed in an urn which already
contains a ball of different color. Equivalently, this can be viewed as
sequentially coloring a complete $q$-partite graph wherein a collision
corresponds to the appearance of a monochromatic edge. Using a Poisson
embedding technique, the limiting distribution of the first collision time is
determined and the possible limits are explicitly described. Joint distribution
of successive collision times and multi-fold collision times are also derived.
The results can be used to obtain the limiting distributions of running times
in various birthday problem based algorithms for solving the discrete logarithm
problem, and to generalize previous results which only consider expected
running times. Asymptotic distributions of the time of appearance of a
monochromatic edge are also obtained for preferential attachment graphs and the
infinite path.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4252</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4252</id><created>2013-11-17</created><authors><author><keyname>Backes</keyname><forenames>Andr&#xe9; Ricardo</forenames></author><author><keyname>Casanova</keyname><forenames>Dalcimar</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir Martinez</forenames></author></authors><title>Contour polygonal approximation using shortest path in networks</title><categories>physics.comp-ph cs.CV</categories><comments>12 pages, 12 figures</comments><doi>10.1142/S0129183113500903</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contour polygonal approximation is a simplified representation of a contour
by line segments, so that the main characteristics of the contour remain in a
small number of line segments. This paper presents a novel method for polygonal
approximation based on the Complex Networks theory. We convert each point of
the contour into a vertex, so that we model a regular network. Then we
transform this network into a Small-World Complex Network by applying some
transformations over its edges. By analyzing of network properties, especially
the geodesic path, we compute the polygonal approximation. The paper presents
the main characteristics of the method, as well as its functionality. We
evaluate the proposed method using benchmark contours, and compare its results
with other polygonal approximation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4257</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4257</id><created>2013-11-17</created><authors><author><keyname>Benson</keyname><forenames>Austin R.</forenames></author><author><keyname>Poulson</keyname><forenames>Jack</forenames></author><author><keyname>Tran</keyname><forenames>Kenneth</forenames></author><author><keyname>Engquist</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Ying</keyname><forenames>Lexing</forenames></author></authors><title>A parallel directional Fast Multipole Method</title><categories>math.NA cs.NA</categories><msc-class>65Y05, 65Y20, 78A45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a parallel directional fast multipole method (FMM) for
solving N-body problems with highly oscillatory kernels, with a focus on the
Helmholtz kernel in three dimensions. This class of oscillatory kernels
requires a more restrictive low-rank criterion than that of the low-frequency
regime, and thus effective parallelizations must adapt to the modified data
dependencies. We propose a simple partition at a fixed level of the octree and
show that, if the partitions are properly balanced between p processes, the
overall runtime is essentially O(N log N/p+ p). By the structure of the
low-rank criterion, we are able to avoid communication at the top of the
octree. We demonstrate the effectiveness of our parallelization on several
challenging models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4268</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4268</id><created>2013-11-18</created><updated>2015-05-30</updated><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author><author><keyname>Vijayalakshmi</keyname><forenames>V. J.</forenames></author><author><keyname>Jeyakumar</keyname><forenames>P.</forenames></author></authors><title>Quadrant Based DIR in CWin Adaptation Mechanism for Multihop Wireless
  Network</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><doi>10.5121/ijci.2013.2404</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Multihop Wireless Networks, traffic forwarding capability of each node
varies according to its level of contention. Each node can yield its channel
access opportunity to its neighbouring nodes, so that all the nodes can evenly
share the channel and have similar forwarding capability. In this manner the
wireless channel is utilize d effectively, which is achieved using Contention
Window Adaptation Mechanism (CWAM). This mechanism achieves a higher end to -
end throughout but consumes the network power to a higher level. So, a newly
proposed algorithm Quadrant Based Directional Routing Protocol (Q-DIR) is
implemented as a cross - layer with CWAM, to reduce the total network power
consumption through limited flooding and also reduce the routing overheads,
which eventually increases overall network throughput. This algorithm limits
the broadcast region to a quadrant where the source node and the destination
nodes are located. Implementation of the algorithm is done in Linux based NS-2
simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4276</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4276</id><created>2013-11-18</created><updated>2014-01-05</updated><authors><author><keyname>Fire</keyname><forenames>Michael</forenames></author><author><keyname>Elovici</keyname><forenames>Yuval</forenames></author></authors><title>Data Mining of Online Genealogy Datasets for Revealing Lifespan Patterns
  in Human Population</title><categories>cs.SI q-bio.PE stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online genealogy datasets contain extensive information about millions of
people and their past and present family connections. This vast amount of data
can assist in identifying various patterns in human population. In this study,
we present methods and algorithms which can assist in identifying variations in
lifespan distributions of human population in the past centuries, in detecting
social and genetic features which correlate with human lifespan, and in
constructing predictive models of human lifespan based on various features
which can easily be extracted from genealogy datasets.
  We have evaluated the presented methods and algorithms on a large online
genealogy dataset with over a million profiles and over 9 million connections,
all of which were collected from the WikiTree website. Our findings indicate
that significant but small positive correlations exist between the parents'
lifespan and their children's lifespan. Additionally, we found slightly higher
and significant correlations between the lifespans of spouses. We also
discovered a very small positive and significant correlation between longevity
and reproductive success in males, and a small and significant negative
correlation between longevity and reproductive success in females. Moreover,
our machine learning algorithms presented better than random classification
results in predicting which people who outlive the age of 50 will also outlive
the age of 80.
  We believe that this study will be the first of many studies which utilize
the wealth of data on human populations, existing in online genealogy datasets,
to better understand factors which influence human lifespan. Understanding
these factors can assist scientists in providing solutions for successful
aging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4289</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4289</id><created>2013-11-18</created><authors><author><keyname>Laitinen</keyname><forenames>Tero</forenames></author><author><keyname>Junttila</keyname><forenames>Tommi</forenames></author><author><keyname>Niemel&#xe4;</keyname><forenames>Ilkka</forenames></author></authors><title>Simulating Parity Reasoning (extended version)</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Propositional satisfiability (SAT) solvers, which typically operate using
conjunctive normal form (CNF), have been successfully applied in many domains.
However, in some application areas such as circuit verification, bounded model
checking, and logical cryptanalysis, instances can have many parity (xor)
constraints which may not be handled efficiently if translated to CNF. Thus,
extensions to the CNF-driven search with various parity reasoning engines
ranging from equivalence reasoning to incremental Gaussian elimination have
been proposed. This paper studies how stronger parity reasoning techniques in
the DPLL(XOR) framework can be simulated by simpler systems: resolution, unit
propagation, and parity explanations. Such simulations are interesting, for
example, for developing the next generation SAT solvers capable of handling
parity constraints efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4293</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4293</id><created>2013-11-18</created><authors><author><keyname>Lopez</keyname><forenames>Pablo</forenames></author><author><keyname>Fernandez</keyname><forenames>David</forenames></author><author><keyname>Marin-Perez</keyname><forenames>Rafael</forenames></author><author><keyname>Jara</keyname><forenames>Antonio J.</forenames></author><author><keyname>Gomez-Skarmeta</keyname><forenames>Antonio F.</forenames></author></authors><title>Scalable Oriented-Service Architecture for Heterogeneous and Ubiquitous
  IoT Domains</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of Things (IoT) grows quickly, and 50 billion of IoT devices will be
interconnected by 2020. For the huge number of IoT devices, a high scalable
discovery architecture is required to provide autonomous registration and
look-up of IoT resources and services. The architecture should enable dynamic
updates when new IoT devices are incorporated into Internet, and changes are
made to the existing ones. Nowadays in Internet, the most used discovery
architecture is the Domain Name System (DNS). DNS offers a scalable solution
through two distributed mechanisms: multicast DNS (mDNS) and DNS Service
Directory (DNS-SD). Both mechanisms have been applied to discover resources and
services in local IoT domains. However, a full architecture has not still been
designed to support global discovery, local directories and a search engine for
ubiquitous IoT domains. Moreover, the architecture should provide other
transversal functionalities such as a common semantic for describing services
and resources, and a service layer for interconnecting with M2M platforms and
mobile clients. This paper presents an oriented-service architecture based on
DNS to support a global discovery, local directories and a distributed search
engine to enable a scalable looking-up of IoT resources and services. The
architecture provides two lightweight discovery mechanisms based on mDNS and
DNS-SD that have been optimized for the constraints of IoT devices to allow
autonomous registration. Moreover, we analyse and provide other relevant
elements such semantic description and communications interfaces to support the
heterogeneity of IoT devices and clients. All these elements contribute to
build a scalable architecture for the discovery and access of heterogeneous and
ubiquitous IoT domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4294</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4294</id><created>2013-11-18</created><updated>2014-07-03</updated><authors><author><keyname>Zhang</keyname><forenames>Haizhang</forenames></author></authors><title>Exponential Approximation of Bandlimited Functions from Average
  Oversampling</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weighted average sampling is more practical and numerically more stable than
sampling at single points as in the classical Shannon sampling framework. Using
the frame theory, one can completely reconstruct a bandlimited function from
its suitably-chosen average sample data. When only finitely many sample data
are available, truncating the complete reconstruction series with the standard
dual frame results in very slow convergence. We present in this note a method
of reconstructing a bandlimited function from finite average oversampling with
an exponentially-decaying approximation error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4296</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4296</id><created>2013-11-18</created><authors><author><keyname>Jegelka</keyname><forenames>Stefanie</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author><author><keyname>Sra</keyname><forenames>Suvrit</forenames><affiliation>MPI</affiliation></author></authors><title>Reflection methods for user-friendly submodular optimization</title><categories>cs.LG cs.NA cs.RO math.OC</categories><comments>Neural Information Processing Systems (NIPS), \'Etats-Unis (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it has become evident that submodularity naturally captures widely
occurring concepts in machine learning, signal processing and computer vision.
Consequently, there is need for efficient optimization procedures for
submodular functions, especially for minimization problems. While general
submodular minimization is challenging, we propose a new method that exploits
existing decomposability of submodular functions. In contrast to previous
approaches, our method is neither approximate, nor impractical, nor does it
need any cumbersome parameter tuning. Moreover, it is easy to implement and
parallelize. A key component of our method is a formulation of the discrete
submodular minimization problem as a continuous best approximation problem that
is solved through a sequence of reflections, and its solution can be easily
thresholded to obtain an optimal discrete solution. This method solves both the
continuous and discrete formulations of the problem, and therefore has
applications in learning, inference, and reconstruction. In our experiments, we
illustrate the benefits of our method on two image segmentation tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4297</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4297</id><created>2013-11-18</created><updated>2014-03-23</updated><authors><author><keyname>Vlasov</keyname><forenames>Alexander Yu.</forenames></author></authors><title>On generalization of reversible second-order cellular automata</title><categories>nlin.CG cs.FL</categories><comments>LaTeX, 12pt article class, 10 pages, 7 figures, v2. 11pt, reference
  added, software implementation is available
  http://cc.embarcadero.com/Item/29779</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A cellular automaton with $n$ states may be used for construction of
reversible second-order cellular automaton with $n^2$ states. Reversible
cellular automata with hidden parameters discussed in this paper are
generalization of such construction and may have number of states $N=n m$ with
arbitrary $m$. Further modification produces reversible cellular automata with
reduced number of states $N' &lt; N = n m$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4303</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4303</id><created>2013-11-18</created><authors><author><keyname>Qadir</keyname><forenames>Junaid</forenames></author><author><keyname>Hasan</keyname><forenames>Osman</forenames></author></authors><title>Applying Formal Methods to Networking: Theory, Techniques and
  Applications</title><categories>cs.NI cs.SE</categories><comments>30 pages, submitted to IEEE Communications Surveys and Tutorials</comments><doi>10.1109/COMST.2014.2345792</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite its great importance, modern network infrastructure is remarkable for
the lack of rigor in its engineering. The Internet which began as a research
experiment was never designed to handle the users and applications it hosts
today. The lack of formalization of the Internet architecture meant limited
abstractions and modularity, especially for the control and management planes,
thus requiring for every new need a new protocol built from scratch. This led
to an unwieldy ossified Internet architecture resistant to any attempts at
formal verification, and an Internet culture where expediency and pragmatism
are favored over formal correctness. Fortunately, recent work in the space of
clean slate Internet design---especially, the software defined networking (SDN)
paradigm---offers the Internet community another chance to develop the right
kind of architecture and abstractions. This has also led to a great resurgence
in interest of applying formal methods to specification, verification, and
synthesis of networking protocols and applications. In this paper, we present a
self-contained tutorial of the formidable amount of work that has been done in
formal methods, and present a survey of its applications to networking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4305</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4305</id><created>2013-11-18</created><authors><author><keyname>Yuki</keyname><forenames>Tomofumi</forenames></author><author><keyname>Feautrier</keyname><forenames>Paul</forenames></author><author><keyname>Rajopadhye</keyname><forenames>Sanjay</forenames></author><author><keyname>Saraswat</keyname><forenames>Vijay</forenames></author></authors><title>Checking Race Freedom of Clocked X10 Programs</title><categories>cs.DC cs.PL</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of many approaches to better take advantage of parallelism, which has now
become mainstream, is the introduction of parallel programming languages.
However, parallelism is by nature non-deterministic, and not all parallel bugs
can be avoided by language design. This paper proposes a method for
guaranteeing absence of data races in the polyhedral subset of clocked X10
programs.
  Clocks in X10 are similar to barriers, but are more dynamic; the subset of
processes that participate in the synchronization can dynamically change at
runtime. We construct the happens-before relation for clocked X10 programs, and
show that the problem of race detection is undecidable. However, in many
practical cases, modern tools are able to find solutions or disprove their
existence. We present a set of benchmarks for which the analysis is possible
and has an acceptable running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4306</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4306</id><created>2013-11-18</created><authors><author><keyname>Riverso</keyname><forenames>S.</forenames></author><author><keyname>Rubini</keyname><forenames>D.</forenames></author><author><keyname>Ferrari-Trecate</keyname><forenames>G.</forenames></author></authors><title>Distributed bounded-error state estimation for partitioned systems based
  on practical robust positive invariance</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a partition-based state estimator for linear discrete-time systems
composed by coupled subsystems affected by bounded disturbances. The
architecture is distributed in the sense that each subsystem is equipped with a
local state estimator that exploits suitable pieces of information from parent
subsystems. Moreover, differently from methods based on moving horizon
estimation, our approach does not require the on-line solution to optimization
problems. Our state-estimation scheme, that is based on the notion of practical
robust positive invariance developed in Rakovic 2011, also guarantees
satisfaction of constraints on local estimation errors and it can be updated
with a limited computational effort when subsystems are added or removed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4310</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4310</id><created>2013-11-18</created><authors><author><keyname>Jamali</keyname><forenames>Vahid</forenames></author><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Ikhlef</keyname><forenames>Aissa</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Achievable Rate Region of the Bidirectional Buffer-Aided Relay Channel
  with Block Fading</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transaction on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bidirectional relay channel, in which two users communicate with each
other through a relay node, is a simple but fundamental and practical network
architecture. In this paper, we consider the block fading bidirectional relay
channel and propose efficient transmission strategies that exploit the block
fading property of the channel. Thereby, we consider a decode-and-forward relay
and assume that a direct link between the two users is not present. Our aim is
to characterize the long-term achievable rate region and to develop protocols
which achieve all points of the obtained rate region. Specifically, in the
bidirectional relay channel, there exist six possible transmission modes: four
point-to-point modes (user 1-to-relay, user 2-to-relay, relay-to-user 1,
relay-to-user 2), a multiple-access mode (both users to the relay), and a
broadcast mode (the relay to both users). Most existing protocols assume a
fixed schedule for using a subset of the aforementioned transmission modes.
Motivated by this limitation, we develop protocols which are not restricted to
adhere to a predefined schedule for using the transmission modes. In fact,
based on the instantaneous channel state information (CSI) of the involved
links, the proposed protocol selects the optimal transmission mode in each time
slot to maximize the long-term achievable rate region. Thereby, we consider two
different types of transmit power constraints: 1) a joint long-term power
constraint for all nodes, and 2) a fixed transmit power for each node.
Furthermore, to enable the use of a non-predefined schedule for transmission
mode selection, the relay has to be equipped with two buffers for storage of
the information received from both users. As data buffering increases the
end-to-end delay, we consider both delay-unconstrained and delay-constrained
transmission in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4317</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4317</id><created>2013-11-18</created><updated>2014-05-24</updated><authors><author><keyname>Hoque</keyname><forenames>Mohammad Ashraful</forenames></author><author><keyname>Siekkinen</keyname><forenames>Matti</forenames></author><author><keyname>Nurminen</keyname><forenames>Jukka K.</forenames></author><author><keyname>Aalto</keyname><forenames>Mika</forenames></author><author><keyname>Tarkoma</keyname><forenames>Sasu</forenames></author></authors><title>Mobile Multimedia Streaming Techniques : QoE and Energy Consumption
  Perspective</title><categories>cs.MM cs.NI</categories><comments>Accepted in Pervasive and Mobile Computing, Elsevier, May 2014</comments><doi>10.1016/j.pmcj.2014.05.004</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Multimedia streaming to mobile devices is challenging for two reasons. First,
the way content is delivered to a client must ensure that the user does not
experience a long initial playback delay or a distorted playback in the middle
of a streaming session. Second, multimedia streaming applications are among the
most energy hungry applications in smartphones. The energy consumption mostly
depends on the delivery techniques and on the power management techniques of
wireless access technologies (Wi-Fi, 3G, and 4G). In order to provide insights
on what kind of streaming techniques exist, how they work on different mobile
platforms, their efforts in providing smooth quality of experience, and their
impact on energy consumption of mobile phones, we did a large set of active
measurements with several smartphones having both Wi-Fi and cellular network
access. Our analysis reveals five different techniques to deliver the content
to the video players. The selection of a technique depends on the mobile
platform, device, player, quality, and service. The results from our traffic
and power measurements allow us to conclude that none of the identified
techniques is optimal because they take none of the following facts into
account: access technology used, user behavior, and user preferences concerning
data waste. We point out the technique with optimal playback buffer
configuration, which provides the most attractive trade-offs in particular
situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4319</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4319</id><created>2013-11-18</created><authors><author><keyname>Kotthoff</keyname><forenames>Lars</forenames></author></authors><title>Ranking Algorithms by Performance</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common way of doing algorithm selection is to train a machine learning
model and predict the best algorithm from a portfolio to solve a particular
problem. While this method has been highly successful, choosing only a single
algorithm has inherent limitations -- if the choice was bad, no remedial action
can be taken and parallelism cannot be exploited, to name but a few problems.
In this paper, we investigate how to predict the ranking of the portfolio
algorithms on a particular problem. This information can be used to choose the
single best algorithm, but also to allocate resources to the algorithms
according to their rank. We evaluate a range of approaches to predict the
ranking of a set of algorithms on a problem. We furthermore introduce a
framework for categorizing ranking predictions that allows to judge the
expressiveness of the predictive output. Our experimental evaluation
demonstrates on a range of data sets from the literature that it is beneficial
to consider the relationship between algorithms when predicting rankings. We
furthermore show that relatively naive approaches deliver rankings of good
quality already.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4336</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4336</id><created>2013-11-18</created><updated>2014-06-18</updated><authors><author><keyname>Huang</keyname><forenames>Junming</forenames></author><author><keyname>Li</keyname><forenames>Chao</forenames></author><author><keyname>Wang</keyname><forenames>Wen-Qiang</forenames></author><author><keyname>Shen</keyname><forenames>Hua-Wei</forenames></author><author><keyname>Li</keyname><forenames>Guojie</forenames></author><author><keyname>Cheng</keyname><forenames>Xue-Qi</forenames></author></authors><title>Temporal scaling in information propagation</title><categories>cs.SI physics.soc-ph</categories><comments>13 pages, 2 figures. published on Scientific Reports</comments><journal-ref>Scientific Reports 4, 5334, (2014)</journal-ref><doi>10.1038/srep05334</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the study of information propagation, one fundamental problem is
uncovering universal laws governing the dynamics of information propagation.
This problem, from the microscopic perspective, is formulated as estimating the
propagation probability that a piece of information propagates from one
individual to another. Such a propagation probability generally depends on two
major classes of factors: the intrinsic attractiveness of information and the
interactions between individuals. Despite the fact that the temporal effect of
attractiveness is widely studied, temporal laws underlying individual
interactions remain unclear, causing inaccurate prediction of information
propagation on evolving social networks. In this report, we empirically study
the dynamics of information propagation, using the dataset from a
population-scale social media website. We discover a temporal scaling in
information propagation: the probability a message propagates between two
individuals decays with the length of time latency since their latest
interaction, obeying a power-law rule. Leveraging the scaling law, we further
propose a temporal model to estimate future propagation probabilities between
individuals, reducing the error rate of information propagation prediction from
6.7% to 2.6% and improving viral marketing with 9.7% incremental customers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4349</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4349</id><created>2013-11-18</created><authors><author><keyname>Alty</keyname><forenames>James L.</forenames></author><author><keyname>Vickers</keyname><forenames>Paul</forenames></author></authors><title>The CAITLIN Auralization System: Hierarchical Leitmotif Design as a Clue
  to Program Comprehension</title><categories>cs.HC</categories><comments>In ICAD '97 Fourth International Conference on Auditory Display (J.
  A. Ballas and E. D. Mynatt, eds.), (Palo Alto), pp. 89-96, Xerox PARC, Palo
  Alto, CA 94304, 1997, 7 pages, 10 figures</comments><acm-class>H.5.1; H.5.2; H.5.5; D.2.5; D.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Early experiments have suggested that program auralization can convey
information about program structure [8]. Languages like Pascal contain classes
of construct that are similar in nature allowing hierarchical classification of
their features. This taxonomy can be reflected in the design of musical
signatures which are used within the CAITLIN program auralization system.
Experiments using these hierarchical leitmotifs indicate whether or not their
similarities can be put to good use in communicating information about program
structure and state. (Note, at time of going to press experimental results
could not be included. These will be presented at the conference and included
later.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4362</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4362</id><created>2013-11-18</created><updated>2014-03-14</updated><authors><author><keyname>Calafiore</keyname><forenames>Giuseppe C.</forenames></author><author><keyname>Ghaoui</keyname><forenames>Laurent El</forenames></author><author><keyname>Novara</keyname><forenames>Carlo</forenames></author></authors><title>Sparse Identification of Posynomial Models</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Posynomials are nonnegative combinations of monomials with possibly
fractional and both positive and negative exponents. Posynomial models are
widely used in various engineering design endeavors, such as circuits,
aerospace and structural design, mainly due to the fact that design problems
cast in terms of posynomial objectives and constraints can be solved
efficiently by means of a convex optimization technique known as geometric
programming (GP). However, while quite a vast literature exists on GP-based
design, very few contributions can yet be found on the problem of identifying
posynomial models from experimental data. Posynomial identification amounts to
determining not only the coefficients of the combination, but also the
exponents in the monomials, which renders the identification problem
numerically hard. In this draft, we propose an approach to the identification
of multivariate posynomial models, based on the expansion on a given
large-scale basis of monomials. The model is then identified by seeking
coefficients of the combination that minimize a mixed objective, composed by a
term representing the fitting error and a term inducing sparsity in the
representation, which results in a problem formulation of the ``square-root
LASSO'' type, with nonnegativity constraints on the variables. We propose to
solve the problem via a sequential coordinate-descent scheme, which is suitable
for large-scale implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4369</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4369</id><created>2013-11-18</created><authors><author><keyname>Dini</keyname><forenames>Dahir H.</forenames></author><author><keyname>Kanna</keyname><forenames>Sithan</forenames></author><author><keyname>Mandic</keyname><forenames>Danilo P.</forenames></author></authors><title>Distributed Widely Linear Complex Kalman Filtering</title><categories>cs.SY cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We introduce cooperative sequential state space estimation in the domain of
augmented complex statistics, whereby nodes in a network collaborate locally to
estimate noncircular complex signals. For rigour, a distributed augmented
(widely linear) complex Kalman filter (D-ACKF) suited to the generality of
complex signals is introduced, allowing for unified treatment of both proper
(rotation invariant) and improper (rotation dependent) signal distributions.
Its duality with the bivariate real-valued distributed Kalman filter, along
with several issues of implementation are also illuminated. The analysis and
simulations show that unlike existing distributed Kalman filter solutions, the
D-ACKF caters for both the improper data and the correlations between nodal
observation noises, thus providing enhanced performance in real-world
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4376</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4376</id><created>2013-11-18</created><authors><author><keyname>Vickers</keyname><forenames>Paul</forenames></author><author><keyname>Faith</keyname><forenames>Joe</forenames></author><author><keyname>Rossiter</keyname><forenames>Nick</forenames></author></authors><title>Understanding Visualization: A Formal Approach using Category Theory and
  Semiotics</title><categories>cs.LO cs.GR cs.HC</categories><comments>15 pages, 14 figures</comments><journal-ref>IEEE Transactions on Visualization and Computer Graphics, vol. 19,
  pp. 1048-1061, 2013</journal-ref><doi>10.1109/TVCG.2012.294</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article combines the vocabulary of semiotics and category theory to
provide a formal analysis of visualization. It shows how familiar processes of
visualization fit the semiotic frameworks of both Saussure and Peirce, and
extends these structures using the tools of category theory to provide a
general framework for understanding visualization in practice, including:
relationships between systems, data collected from those systems, renderings of
those data in the form of representations, the reading of those representations
to create visualizations, and the use of those visualizations to create
knowledge and understanding of the system under inspection. The resulting
framework is validated by demonstrating how familiar information visualization
concepts (such as literalness, sensitivity, redundancy, ambiguity,
generalizability, and chart junk) arise naturally from it and can be defined
formally and precisely. This article generalizes previous work on the formal
characterization of visualization by, inter alia, Ziemkiewicz and Kosara and
allows us to formally distinguish properties of the visualization process that
previous work does not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4388</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4388</id><created>2013-11-18</created><authors><author><keyname>Lemoy</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Mozeika</keyname><forenames>Alexander</forenames></author><author><keyname>Seki</keyname><forenames>Shinnosuke</forenames></author></authors><title>Transfer matrix analysis of one-dimensional majority cellular automata
  with thermal noise</title><categories>cond-mat.stat-mech cs.DM math.PR</categories><comments>12 pages, 4 figures</comments><doi>10.1088/1751-8113/47/10/105001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thermal noise in a cellular automaton refers to a random perturbation to its
function which eventually leads this automaton to an equilibrium state
controlled by a temperature parameter. We study the 1-dimensional majority-3
cellular automaton under this model of noise. Without noise, each cell in this
automaton decides its next state by majority voting among itself and its left
and right neighbour cells. Transfer matrix analysis shows that the automaton
always reaches a state in which every cell is in one of its two states with
probability 1/2 and thus cannot remember even one bit of information. Numerical
experiments, however, support the possibility of reliable computation for a
long but finite time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4389</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4389</id><created>2013-11-14</created><authors><author><keyname>Paris</keyname><forenames>C&#xe9;cile</forenames></author><author><keyname>Colineau</keyname><forenames>Nathalie</forenames></author><author><keyname>Nepal</keyname><forenames>Surya</forenames></author><author><keyname>Bista</keyname><forenames>Sanat</forenames></author><author><keyname>Beschorner</keyname><forenames>Gina</forenames></author></authors><title>Ethical considerations in an online community: the balancing act</title><categories>cs.CY</categories><journal-ref>Paris, C., Colineau, N., Nepal, S., Bista, S. K., &amp; Beschorner, G.
  (2013). Ethical considerations in an online community: the balancing act.
  Ethics and Information Technology, 1-16</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the emergence and rapid growth of Social Media, a number of government
departments in several countries have embraced Social Media as a privilege
channel to interact with their constituency. We are exploring, in collaboration
with the Australian Department of Human Services, the possibility to exploit
the potential of social networks to support specific groups of citizens. To
this end, we have developed Next Step, an online community to help people
currently receiving welfare payments find a job and become financially
self-sufficient. In this paper, we explore some ethical issues that arise when
governments engage directly with citizens, in particular with communities in
difficult situations, and when researchers are involved. We describe some of
the challenges we faced and how we addressed them. Our work highlights the
complexity of the problem, when an online community involves a government
department and a welfare recipient group with a dependency relationship with
that department. It becomes a balancing act, with the need to ensure privacy of
the community members whilst still fulfilling the government's legal
responsibilities. While difficult, these issues must be addressed if
governments are to engage with their citizens using Social Media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4394</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4394</id><created>2013-11-18</created><authors><author><keyname>Davoodi</keyname><forenames>Pooya</forenames></author><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author><author><keyname>Raman</keyname><forenames>Rajeev</forenames></author><author><keyname>Rao</keyname><forenames>S. Srinivasa</forenames></author></authors><title>Encoding Range Minimum Queries</title><categories>cs.DS</categories><comments>20 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of encoding range minimum queries (RMQs): given an
array A[1..n] of distinct totally ordered values, to pre-process A and create a
data structure that can answer the query RMQ(i,j), which returns the index
containing the smallest element in A[i..j], without access to the array A at
query time. We give a data structure whose space usage is 2n + o(n) bits, which
is asymptotically optimal for worst-case data, and answers RMQs in O(1)
worst-case time. This matches the previous result of Fischer and Heun [SICOMP,
2011], but is obtained in a more natural way. Furthermore, our result can
encode the RMQs of a random array A in 1.919n + o(n) bits in expectation, which
is not known to hold for Fischer and Heun's result. We then generalize our
result to the encoding range top-2 query (RT2Q) problem, which is like the
encoding RMQ problem except that the query RT2Q(i,j) returns the indices of
both the smallest and second-smallest elements of A[i..j]. We introduce a data
structure using 3.272n+o(n) bits that answers RT2Qs in constant time, and also
give lower bounds on the effective entropy} of RT2Q.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4419</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4419</id><created>2013-11-15</created><authors><author><keyname>Kong</keyname><forenames>Zhaodan</forenames></author><author><keyname>Ozcimder</keyname><forenames>Kayhan</forenames></author><author><keyname>Fuller</keyname><forenames>Nathan W.</forenames></author><author><keyname>Baillieul</keyname><forenames>John</forenames></author></authors><title>Perception and Steering Control in Paired Bat Flight</title><categories>cs.SY cs.RO physics.bio-ph</categories><comments>Submitted to the 19th World Congress of the International Federation
  of Automatic Control (IFAC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Animals within groups need to coordinate their reactions to perceived
environmental features and to each other in order to safely move from one point
to another. This paper extends our previously published work on the flight
patterns of Myotis velifer that have been observed in a habitat near Johnson
City, Texas. Each evening, these bats emerge from a cave in sequences of small
groups that typically contain no more than three or four individuals, and they
thus provide ideal subjects for studying leader-follower behaviors. By
analyzing the flight paths of a group of M. velifer, the data show that the
flight behavior of a follower bat is influenced by the flight behavior of a
leader bat in a way that is not well explained by existing pursuit laws, such
as classical pursuit, constant bearing and motion camouflage. Thus we propose
an alternative steering law based on virtual loom, a concept we introduce to
capture the geometrical configuration of the leader-follower pair. It is shown
that this law may be integrated with our previously proposed vision-enabled
steering laws to synthesize trajectories, the statistics of which fit with
those of the bats in our data set. The results suggest that bats use perceived
information of both the environment and their neighbors for navigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4420</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4420</id><created>2013-11-18</created><authors><author><keyname>Sree</keyname><forenames>P. Kiran</forenames></author><author><keyname>Babu</keyname><forenames>Inampudi Ramesh</forenames></author><author><keyname>N</keyname><forenames>SSSN Usha Devi</forenames></author></authors><title>CAVDM: Cellular Automata Based Video Cloud Mining Framework for
  Information Retrieval</title><categories>cs.IR</categories><journal-ref>Parallel Computing and Cloud Computing Research (PCCR) Volume 1
  Issue 1, April 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Mining technique can be applied to various documents. Acquisition and
storage of video data is an easy task but retrieval of information from video
data is a challenging task. So video Cloud Mining plays an important role in
efficient video data management for information retrieval. This paper proposes
a Cellular Automata based framework for video Cloud Mining to extract the
information from video data. This includes developing the technique for shot
detection then key frame analysis is considered to compare the frames of each
shot to each others to define the relationship between shots. Cellular automata
based hierarchical clustering technique is adopted to make a group of similar
shots to detect the particular event on some requirement as per user demand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4425</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4425</id><created>2013-11-18</created><updated>2013-11-25</updated><authors><author><keyname>Aminof</keyname><forenames>Benjamin</forenames></author><author><keyname>Jacobs</keyname><forenames>Swen</forenames></author><author><keyname>Khalimov</keyname><forenames>Ayrat</forenames></author><author><keyname>Rubin</keyname><forenames>Sasha</forenames></author></authors><title>Parameterized Model Checking of Token-Passing Systems</title><categories>cs.LO</categories><comments>We had to remove an appendix until the proofs and notations there is
  cleared</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the parameterized model checking problem for token-passing systems
and specifications in indexed $\textsf{CTL}^\ast \backslash \textsf{X}$.
Emerson and Namjoshi (1995, 2003) have shown that parameterized model checking
of indexed $\textsf{CTL}^\ast \backslash \textsf{X}$ in uni-directional token
rings can be reduced to checking rings up to some \emph{cutoff} size. Clarke et
al. (2004) have shown a similar result for general topologies and indexed
$\textsf{LTL} \backslash \textsf{X}$, provided processes cannot choose the
directions for sending or receiving the token.
  We unify and substantially extend these results by systematically exploring
fragments of indexed $\textsf{CTL}^\ast \backslash \textsf{X}$ with respect to
general topologies. For each fragment we establish whether a cutoff exists, and
for some concrete topologies, such as rings, cliques and stars, we infer small
cutoffs. Finally, we show that the problem becomes undecidable, and thus no
cutoffs exist, if processes are allowed to choose the directions in which they
send or from which they receive the token.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4431</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4431</id><created>2013-11-18</created><authors><author><keyname>Hsieh</keyname><forenames>Ya-Ping</forenames></author><author><keyname>Yeh</keyname><forenames>Ping-Cheng</forenames></author></authors><title>Mathematical Foundations for Information Theory in Diffusion-Based
  Molecular Communications</title><categories>cs.IT math.IT q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Molecular communication emerges as a promising communication paradigm for
nanotechnology. However, solid mathematical foundations for
information-theoretic analysis of molecular communication have not yet been
built. In particular, no one has ever proven that the channel coding theorem
applies for molecular communication, and no relationship between information
rate capacity (maximum mutual information) and code rate capacity (supremum
achievable code rate) has been established. In this paper, we focus on a major
subclass of molecular communication - the diffusion-based molecular
communication. We provide solid mathematical foundations for information theory
in diffusion-based molecular communication by creating a general
diffusion-based molecular channel model in measure-theoretic form and prove its
channel coding theorems. Various equivalence relationships between statistical
and operational definitions of channel capacity are also established, including
the most classic information rate capacity and code rate capacity. As
byproducts, we have shown that the diffusion-based molecular channel is with
&quot;asymptotically decreasing input memory and anticipation&quot; and &quot;d-continuous&quot;.
Other properties of diffusion-based molecular channel such as stationarity or
ergodicity are also proven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4439</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4439</id><created>2013-11-18</created><authors><author><keyname>Khademi</keyname><forenames>Seyran</forenames></author><author><keyname>Chepuri</keyname><forenames>Sundeep Prabhakar</forenames></author><author><keyname>Irahhauten</keyname><forenames>Zoubir</forenames></author><author><keyname>Janssen</keyname><forenames>Gerard J. M.</forenames></author><author><keyname>van der Veen</keyname><forenames>Alle-Jan</forenames></author></authors><title>60 GHz Wireless Link Within Metal Enclosures: Channel Measurements and
  System Analysis</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless channel measurement results for 60 GHz within a closed metal cabinet
are provided. A metal cabinet is chosen to emulate the environment within a
mechatronic system, which have metal enclosures in general. A frequency domain
sounding technique is used to measure the wireless channel for different
volumes of the metal enclosure, considering both line-of-sight (LOS) and
non-line-of-sight (NLOS) scenarios. Large-scale and small-scale characteristics
of the wireless channel are extracted in order to build a comprehensive channel
model. In contrast to conventional indoor channels at 60 GHz, the channel in
the metal enclosure is highly reflective resulting in a rich scattering
environment with a significantly large root-mean-square (RMS) delay spread.
Based on the obtained measurement results, the bit error rate (BER) performance
is evaluated for a wideband orthogonal frequency division multiplexing (OFDM)
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4448</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4448</id><created>2013-11-18</created><authors><author><keyname>Brzozowski</keyname><forenames>Janusz</forenames></author><author><keyname>Davies</keyname><forenames>Gareth</forenames></author></authors><title>Most Complex Regular Right-Ideal Languages</title><categories>cs.FL</categories><comments>19 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A right ideal is a language L over an alphabet A that satisfies L = LA*. We
show that there exists a stream (sequence) (R_n : n \ge 3) of regular right
ideal languages, where R_n has n left quotients and is most complex under the
following measures of complexity: the state complexities of the left quotients,
the number of atoms (intersections of complemented and uncomplemented left
quotients), the state complexities of the atoms, the size of the syntactic
semigroup, the state complexities of the operations of reversal, star, and
product, and the state complexities of all binary boolean operations. In that
sense, this stream of right ideals is a universal witness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4451</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4451</id><created>2013-11-18</created><updated>2015-09-21</updated><authors><author><keyname>Cai</keyname><forenames>Jin-Yi</forenames></author><author><keyname>Galanis</keyname><forenames>Andreas</forenames></author><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Guo</keyname><forenames>Heng</forenames></author><author><keyname>Jerrum</keyname><forenames>Mark</forenames></author><author><keyname>Stefankovic</keyname><forenames>Daniel</forenames></author><author><keyname>Vigoda</keyname><forenames>Eric</forenames></author></authors><title>#BIS-Hardness for 2-Spin Systems on Bipartite Bounded Degree Graphs in
  the Tree Nonuniqueness Region</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counting independent sets on bipartite graphs (#BIS) is considered a
canonical counting problem of intermediate approximation complexity. It is
conjectured that #BIS neither has an FPRAS nor is as hard as #SAT to
approximate. We study #BIS in the general framework of two-state spin systems
on bipartite graphs. We define two notions, nearly-independent phase-correlated
spins and unary symmetry breaking. We prove that it is #BIS-hard to approximate
the partition function of any 2-spin system on bipartite graphs supporting
these two notions. As a consequence, we classify the complexity of
approximating the partition function of antiferromagnetic 2-spin systems on
bounded-degree bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4460</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4460</id><created>2013-11-18</created><updated>2014-06-12</updated><authors><author><keyname>Czaplicka</keyname><forenames>Agnieszka</forenames></author><author><keyname>Suchecki</keyname><forenames>Krzysztof</forenames></author><author><keyname>Minano</keyname><forenames>Borja</forenames></author><author><keyname>Trias</keyname><forenames>Miquel</forenames></author><author><keyname>Holyst</keyname><forenames>Janusz A.</forenames></author></authors><title>Information slows down hierarchy growth</title><categories>physics.soc-ph cs.SI</categories><comments>LaTeX, 12 pages, 17 figures; revision after referee reports with
  significant changes</comments><doi>10.1103/PhysRevE.89.062810</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider models of growing multi-level systems wherein the growth process
is driven by rules of tournament selection. A system can be conceived as an
evolving tree with a new node being attached to a contestant node at the best
hierarchy level (a level nearest to the tree root). The proposed evolution
reflects limited information on system properties available to new nodes. It
can also be expressed in terms of population dynamics. Two models are
considered: a constant tournament (CT) model wherein the number of tournament
participants is constant throughout system evolution, and a proportional
tournament (PT) model where this number increases proportionally to the growing
size of the system itself. The results of analytical calculations based on a
rate equation fit well to numerical simulations for both models. In the CT
model all hierarchy levels emerge but the birth time of a consecutive hierarchy
level increases exponentially or faster for each new level. The number of nodes
at the first hierarchy level grows logarithmically in time, while the size of
the last, &quot;worst&quot; hierarchy level oscillates quasi log-periodically. In the PT
model the occupations of the first two hierarchy levels increase linearly but
worse hierarchy levels either do not emerge at all or appear only by chance in
early stage of system evolution to further stop growing at all. The results
allow to conclude that information available to each new node in tournament
dynamics restrains the emergence of new hierarchy levels and that it is the
absolute amount of information, not relative, which governs such behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4468</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4468</id><created>2013-11-18</created><updated>2014-04-01</updated><authors><author><keyname>Calliess</keyname><forenames>Jan-Peter</forenames></author><author><keyname>Papachristodoulou</keyname><forenames>Antonis</forenames></author><author><keyname>Roberts</keyname><forenames>Stephen J.</forenames></author></authors><title>Stochastic processes and feedback-linearisation for online
  identification and Bayesian adaptive control of fully-actuated mechanical
  systems</title><categories>cs.LG cs.SY physics.data-an stat.ML</categories><msc-class>68T05, 68T40, 62G08, 37H10</msc-class><acm-class>I.2.9; I.2.8; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a new method for simultaneous probabilistic identification
and control of an observable, fully-actuated mechanical system. Identification
is achieved by conditioning stochastic process priors on observations of
configurations and noisy estimates of configuration derivatives. In contrast to
previous work that has used stochastic processes for identification, we
leverage the structural knowledge afforded by Lagrangian mechanics and learn
the drift and control input matrix functions of the control-affine system
separately. We utilise feedback-linearisation to reduce, in expectation, the
uncertain nonlinear control problem to one that is easy to regulate in a
desired manner. Thereby, our method combines the flexibility of nonparametric
Bayesian learning with epistemological guarantees on the expected closed-loop
trajectory. We illustrate our method in the context of torque-actuated pendula
where the dynamics are learned with a combination of normal and log-normal
processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4472</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4472</id><created>2013-11-18</created><updated>2013-12-06</updated><authors><author><keyname>Hussami</keyname><forenames>Nadine</forenames></author><author><keyname>Tibshirani</keyname><forenames>Robert</forenames></author></authors><title>A Component Lasso</title><categories>stat.ML cs.LG</categories><comments>18 pages, 4 figures</comments><msc-class>62J07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new sparse regression method called the component lasso, based
on a simple idea. The method uses the connected-components structure of the
sample covariance matrix to split the problem into smaller ones. It then solves
the subproblems separately, obtaining a coefficient vector for each one. Then,
it uses non-negative least squares to recombine the different vectors into a
single solution. This step is useful in selecting and reweighting components
that are correlated with the response. Simulated and real data examples show
that the component lasso can outperform standard regression methods such as the
lasso and elastic net, achieving a lower mean squared error as well as better
support recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4486</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4486</id><created>2013-11-18</created><updated>2013-11-25</updated><authors><author><keyname>Miao</keyname><forenames>Yun-Qian</forenames></author><author><keyname>Farahat</keyname><forenames>Ahmed K.</forenames></author><author><keyname>Kamel</keyname><forenames>Mohamed S.</forenames></author></authors><title>Discriminative Density-ratio Estimation</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The covariate shift is a challenging problem in supervised learning that
results from the discrepancy between the training and test distributions. An
effective approach which recently drew a considerable attention in the research
community is to reweight the training samples to minimize that discrepancy. In
specific, many methods are based on developing Density-ratio (DR) estimation
techniques that apply to both regression and classification problems. Although
these methods work well for regression problems, their performance on
classification problems is not satisfactory. This is due to a key observation
that these methods focus on matching the sample marginal distributions without
paying attention to preserving the separation between classes in the reweighted
space. In this paper, we propose a novel method for Discriminative
Density-ratio (DDR) estimation that addresses the aforementioned problem and
aims at estimating the density-ratio of joint distributions in a class-wise
manner. The proposed algorithm is an iterative procedure that alternates
between estimating the class information for the test data and estimating new
density ratio for each class. To incorporate the estimated class information of
the test data, a soft matching technique is proposed. In addition, we employ an
effective criterion which adopts mutual information as an indicator to stop the
iterative procedure while resulting in a decision boundary that lies in a
sparse region. Experiments on synthetic and benchmark datasets demonstrate the
superiority of the proposed method in terms of both accuracy and robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4502</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4502</id><created>2013-11-18</created><authors><author><keyname>Lavault</keyname><forenames>Christian</forenames><affiliation>LIPN</affiliation></author></authors><title>Multiplicate inverse forms of terminating hypergeometric series</title><categories>math.CO cs.DC cs.DM math.NT</categories><comments>15 pages</comments><proxy>ccsd</proxy><report-no>Equipe CALIN. LIPN CNRS UMR 7030</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multiplicate form of Gould--Hsu's inverse series relations enables to
investigate the dual relations of the Chu-Vandermonde-Gau{\ss}'s, the
Pfaff-Saalsch\&quot;utz's summation theorems and the binomial convolution formula
due to Hagen and Rothe. Several identitity and reciprocal relations are thus
established for terminating hypergeometric series. By virtue of the duplicate
inversions, we establish several dual formulae of Chu-Vandermonde-Gau{\ss}'s
and Pfaff-Saalsch\&quot;utz's summation theorems in Section (3)\cite{ChuVanGauss}
and (4)\cite{PfaffSaalsch}, respectively. Finally, the last section is devoted
to deriving several identities and reciprocal relations for terminating
balanced hypergeometric series from Hagen-Rothe's convolution identity in
accordance with the duplicate, triplicate and multiplicate inversions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4521</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4521</id><created>2013-11-18</created><authors><author><keyname>Deuzeman</keyname><forenames>A.</forenames></author><author><keyname>Jansen</keyname><forenames>K.</forenames></author><author><keyname>Kostrzewa</keyname><forenames>B.</forenames></author><author><keyname>Urbach</keyname><forenames>C.</forenames></author></authors><title>Experiences with OpenMP in tmLQCD</title><categories>hep-lat cs.DC physics.comp-ph</categories><comments>presented at the 31st International Symposium on Lattice Field Theory
  (Lattice 2013), 29 July - 3 August 2013, Mainz, Germany</comments><report-no>HU-EP-13/60, DESY 13-217, SFB/CPP-13-93</report-no><journal-ref>PoS(LATTICE 2013)416</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  An overview is given of the lessons learned from the introduction of
multi-threading using OpenMP in tmLQCD. In particular, programming style,
performance measurements, cache misses, scaling, thread distribution for hybrid
codes, race conditions, the overlapping of communication and computation and
the measurement and reduction of certain overheads are discussed. Performance
measurements and sampling profiles are given for different implementations of
the hopping matrix computational kernel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4527</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4527</id><created>2013-11-18</created><authors><author><keyname>Bento</keyname><forenames>Jose</forenames></author><author><keyname>Derbinsky</keyname><forenames>Nate</forenames></author><author><keyname>Alonso-Mora</keyname><forenames>Javier</forenames></author><author><keyname>Yedidia</keyname><forenames>Jonathan</forenames></author></authors><title>A message-passing algorithm for multi-agent trajectory planning</title><categories>cs.AI cs.DC cs.MA cs.RO cs.SY</categories><comments>In Advances in Neural Information Processing Systems (NIPS), 2013.
  Demo video available at http://www.youtube.com/watch?v=yuGCkVT8Bew</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a novel approach for computing collision-free \emph{global}
trajectories for $p$ agents with specified initial and final configurations,
based on an improved version of the alternating direction method of multipliers
(ADMM). Compared with existing methods, our approach is naturally
parallelizable and allows for incorporating different cost functionals with
only minor adjustments. We apply our method to classical challenging instances
and observe that its computational requirements scale well with $p$ for several
cost functionals. We also show that a specialization of our algorithm can be
used for {\em local} motion planning by solving the problem of joint
optimization in velocity space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4529</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4529</id><created>2013-11-18</created><updated>2014-03-26</updated><authors><author><keyname>Sultana</keyname><forenames>Afroza</forenames></author><author><keyname>Hassan</keyname><forenames>Naeemul</forenames></author><author><keyname>Li</keyname><forenames>Chengkai</forenames></author><author><keyname>Yang</keyname><forenames>Jun</forenames></author><author><keyname>Yu</keyname><forenames>Cong</forenames></author></authors><title>Incremental Discovery of Prominent Situational Facts</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the novel problem of finding new, prominent situational facts, which
are emerging statements about objects that stand out within certain contexts.
Many such facts are newsworthy---e.g., an athlete's outstanding performance in
a game, or a viral video's impressive popularity. Effective and efficient
identification of these facts assists journalists in reporting, one of the main
goals of computational journalism. Technically, we consider an ever-growing
table of objects with dimension and measure attributes. A situational fact is a
&quot;contextual&quot; skyline tuple that stands out against historical tuples in a
context, specified by a conjunctive constraint involving dimension attributes,
when a set of measure attributes are compared. New tuples are constantly added
to the table, reflecting events happening in the real world. Our goal is to
discover constraint-measure pairs that qualify a new tuple as a contextual
skyline tuple, and discover them quickly before the event becomes yesterday's
news. A brute-force approach requires exhaustive comparison with every tuple,
under every constraint, and in every measure subspace. We design algorithms in
response to these challenges using three corresponding ideas---tuple reduction,
constraint pruning, and sharing computation across measure subspaces. We also
adopt a simple prominence measure to rank the discovered facts when they are
numerous. Experiments over two real datasets validate the effectiveness and
efficiency of our techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4533</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4533</id><created>2013-11-18</created><updated>2014-01-14</updated><authors><author><keyname>P</keyname><forenames>Kirana Kumara</forenames></author></authors><title>A Study of Speed of the Boundary Element Method as applied to the
  Realtime Computational Simulation of Biological Organs</title><categories>cs.CE cs.DC cs.MS physics.comp-ph physics.med-ph</categories><comments>preprint, draft, 2 tables, 47 references, 7 files, Codes that can
  solve three dimensional linear elastostatic problems using constant boundary
  elements (of triangular shape) while ignoring body forces are provided as
  supplementary files; codes are distributed under the MIT License in three
  versions: i) MATLAB version ii) Fortran 90 version (sequential code) iii)
  Fortran 90 version (parallel code)</comments><journal-ref>Electronic Journal of Boundary Elements, Vol. 12, No. 2, pp. 1-25
  (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, possibility of simulating biological organs in realtime using
the Boundary Element Method (BEM) is investigated. Biological organs are
assumed to follow linear elastostatic material behavior, and constant boundary
element is the element type used. First, a Graphics Processing Unit (GPU) is
used to speed up the BEM computations to achieve the realtime performance.
Next, instead of the GPU, a computer cluster is used. Results indicate that BEM
is fast enough to provide for realtime graphics if biological organs are
assumed to follow linear elastostatic material behavior. Although the present
work does not conduct any simulation using nonlinear material models, results
from using the linear elastostatic material model imply that it would be
difficult to obtain realtime performance if highly nonlinear material models
that properly characterize biological organs are used. Although the use of BEM
for the simulation of biological organs is not new, the results presented in
the present study are not found elsewhere in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4547</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4547</id><created>2013-11-18</created><authors><author><keyname>Frauchiger</keyname><forenames>Daniela</forenames></author><author><keyname>Renner</keyname><forenames>Renato</forenames></author><author><keyname>Troyer</keyname><forenames>Matthias</forenames></author></authors><title>True randomness from realistic quantum devices</title><categories>quant-ph cs.CR</categories><comments>12 pages + appendix; see ancillary files for extractor software</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even if the output of a Random Number Generator (RNG) is perfectly uniformly
distributed, it may be correlated to pre-existing information and therefore be
predictable. Statistical tests are thus not sufficient to guarantee that an RNG
is usable for applications, e.g., in cryptography or gambling, where
unpredictability is important. To enable such applications a stronger notion of
randomness, termed &quot;true randomness&quot;, is required, which includes independence
from prior information.
  Quantum systems are particularly suitable for true randomness generation, as
their unpredictability can be proved based on physical principles. Practical
implementations of Quantum RNGs (QRNGs) are however always subject to noise,
i.e., influences which are not fully controlled. This reduces the quality of
the raw randomness generated by the device, making it necessary to post-process
it. Here we provide a framework to analyse realistic QRNGs and to determine the
post-processing that is necessary to turn their raw output into true
randomness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4552</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4552</id><created>2013-11-18</created><authors><author><keyname>Deorowicz</keyname><forenames>Sebastian</forenames></author><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author></authors><title>Efficient algorithms for the longest common subsequence in $k$-length
  substrings</title><categories>cs.DS</categories><comments>Submitted to a journal</comments><msc-class>68W32</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the longest common subsequence in $k$-length substrings (LCS$k$) is a
recently proposed problem motivated by computational biology. This is a
generalization of the well-known LCS problem in which matching symbols from two
sequences $A$ and $B$ are replaced with matching non-overlapping substrings of
length $k$ from $A$ and $B$. We propose several algorithms for LCS$k$, being
non-trivial incarnations of the major concepts known from LCS research (dynamic
programming, sparse dynamic programming, tabulation). Our algorithms make use
of a linear-time and linear-space preprocessing finding the occurrences of all
the substrings of length $k$ from one sequence in the other sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4563</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4563</id><created>2013-11-18</created><authors><author><keyname>Bienstock</keyname><forenames>Daniel</forenames></author><author><keyname>Sethuraman</keyname><forenames>Jay</forenames></author><author><keyname>Ye</keyname><forenames>Chun</forenames></author></authors><title>Approximation Algorithms for the Incremental Knapsack Problem via
  Disjunctive Programming</title><categories>cs.DS</categories><comments>Key words: Approximation Algorithms, Integer Programming, Disjunctive
  Programming</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the incremental knapsack problem ($\IK$), we are given a knapsack whose
capacity grows weakly as a function of time. There is a time horizon of $T$
periods and the capacity of the knapsack is $B_t$ in period $t$ for $t = 1,
\ldots, T$. We are also given a set $S$ of $N$ items to be placed in the
knapsack. Item $i$ has a value of $v_i$ and a weight of $w_i$ that is
independent of the time period. At any time period $t$, the sum of the weights
of the items in the knapsack cannot exceed the knapsack capacity $B_t$.
Moreover, once an item is placed in the knapsack, it cannot be removed from the
knapsack at a later time period. We seek to maximize the sum of (discounted)
knapsack values over time subject to the capacity constraints. We first give a
constant factor approximation algorithm for $\IK$, under mild restrictions on
the growth rate of $B_t$ (the constant factor depends on the growth rate). We
then give a PTAS for $\IIK$, the special case of $\IK$ with no discounting,
when $T = O(\sqrt{\log N})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4564</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4564</id><created>2013-11-18</created><authors><author><keyname>Atmani</keyname><forenames>Baghdad</forenames></author><author><keyname>Benbelkacem</keyname><forenames>Sofia</forenames></author><author><keyname>Benamina</keyname><forenames>Mohamed</forenames></author></authors><title>Planning by case-based reasoning based on fuzzy logic</title><categories>cs.AI</categories><comments>International Conference of Artificial Intelligence and Fuzzy Logic
  AIFL-2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The treatment of complex systems often requires the manipulation of vague,
imprecise and uncertain information. Indeed, the human being is competent in
handling of such systems in a natural way. Instead of thinking in mathematical
terms, humans describes the behavior of the system by language proposals. In
order to represent this type of information, Zadeh proposed to model the
mechanism of human thought by approximate reasoning based on linguistic
variables. He introduced the theory of fuzzy sets in 1965, which provides an
interface between language and digital worlds. In this paper, we propose a
Boolean modeling of the fuzzy reasoning that we baptized Fuzzy-BML and uses the
characteristics of induction graph classification. Fuzzy-BML is the process by
which the retrieval phase of a CBR is modelled not in the conventional form of
mathematical equations, but in the form of a database with membership functions
of fuzzy rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4566</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4566</id><created>2013-11-18</created><authors><author><keyname>Wood</keyname><forenames>Michael</forenames></author></authors><title>Journals, repositories, peer review, non-peer review, and the future of
  scholarly communication</title><categories>cs.DL physics.soc-ph</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer reviewed journals are a key part of the system by which academic
knowledge is developed and communicated. Problems have often been noted, and
alternatives proposed, but the journal system still survives. In this article I
focus on problems relating to reliance on subject-specific journals and peer
review. Contrary to what is often assumed, there are alternatives to the
current system, some of which have only becoming viable since the rise of the
world wide web. The market for academic ideas should be opened up by separating
the publication service from the review service: the former would ideally be
served by an open access, web-based repository system encompassing all
disciplines, whereas the latter should be opened up to encourage non-peer
reviews from different perspectives, user reviews, statistics reviews, reviews
from the perspective of different disciplines, and so on. The possibility of
multiple reviews of the same artefact should encourage competition between
reviewing organizations and should make the system more responsive to the
requirements of the differing audience groups. These possibilities offer the
potential to make the academic system far more productive.
  Keywords: Academic journals, Open access, Peer review, Scholarly
communication, Science communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4570</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4570</id><created>2013-11-18</created><authors><author><keyname>Neto</keyname><forenames>Diogo Mariano</forenames></author><author><keyname>Neto</keyname><forenames>Pedro</forenames></author></authors><title>Numerical modeling of friction stir welding process: a literature review</title><categories>cs.CE</categories><comments>The International Journal of Advanced Manufacturing Technology</comments><journal-ref>Volume 65, 2013 , pp 115-126</journal-ref><doi>10.1007/s00170-012-4154-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This survey presents a literature review on friction stir welding (FSW)
modeling with a special focus on the heat generation due to the contact
conditions between the FSW tool and the workpiece. The physical process is
described and the main process parameters that are relevant to its modeling are
highlighted. The contact conditions (sliding/sticking) are presented as well as
an analytical model that allows estimating the associated heat generation. The
modeling of the FSW process requires the knowledge of the heat loss mechanisms,
which are discussed mainly considering the more commonly adopted formulations.
Different approaches that have been used to investigate the material flow are
presented and their advantages/drawbacks are discussed. A reliable FSW process
modeling depends on the fine tuning of some process and material parameters.
Usually, these parameters are achieved with base on experimental data. The
numerical modeling of the FSW process can help to achieve such parameters with
less effort and with economic advantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4572</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4572</id><created>2013-11-18</created><authors><author><keyname>Neto</keyname><forenames>P.</forenames></author><author><keyname>Pires</keyname><forenames>J. N.</forenames></author><author><keyname>Moreira</keyname><forenames>A. P</forenames></author></authors><title>3-D position estimation from inertial sensing: minimizing the error from
  the process of double integration of accelerations</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new approach to 3-D position estimation from
acceleration data, i.e., a 3-D motion tracking system having a small size and
low-cost magnetic and inertial measurement unit (MIMU) composed by both a
digital compass and a gyroscope as interaction technology. A major challenge is
to minimize the error caused by the process of double integration of
accelerations due to motion (these ones have to be separated from the
accelerations due to gravity). Owing to drift error, position estimation cannot
be performed with adequate accuracy for periods longer than few seconds. For
this reason, we propose a method to detect motion stops and only integrate
accelerations in moments of effective hand motion during the demonstration
process. The proposed system is validated and evaluated with experiments
reporting a common daily life pick-and-place task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4573</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4573</id><created>2013-11-18</created><authors><author><keyname>Neto</keyname><forenames>Pedro</forenames></author></authors><title>Off-line Programming and Simulation from CAD Drawings: Robot-Assisted
  Sheet Metal Bending</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasingly, industrial robots are being used in production systems. This is
because they are highly flexible machines and economically competitive with
human labor. The problem is that they are difficult to program. Thus,
manufacturing system designers are looking for more intuitive ways to program
robots, especially using the CAD drawings of the production system they
developed. This paper presents an industrial application of a novel CAD-based
off-line robot programming (OLP) and simulation system in which the CAD package
used for cell design is also used for OLP and robot simulation. Thus, OLP
becomes more accessible to anyone with basic knowledge of CAD and robotics. The
system was tested in a robot-assisted sheet metal bending cell. Experiments
allowed identifying the pros and cons of the proposed solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4588</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4588</id><created>2013-11-18</created><updated>2014-10-15</updated><authors><author><keyname>Steiner</keyname><forenames>Johannes</forenames></author><author><keyname>Ruprecht</keyname><forenames>Daniel</forenames></author><author><keyname>Speck</keyname><forenames>Robert</forenames></author><author><keyname>Krause</keyname><forenames>Rolf</forenames></author></authors><title>Convergence of Parareal for the Navier-Stokes equations depending on the
  Reynolds number</title><categories>math.NA cs.DC</categories><journal-ref>Lecture Notes in Computational Science and Engineering 103,
  Springer International Publishing, pages 195 - 202, 2015</journal-ref><doi>10.1007/978-3-319-10705-9__19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents first a linear stability analysis for the time-parallel
Parareal method, using an IMEX Euler as coarse and a Runge-Kutta-3 method as
fine propagator, confirming that dominant imaginary eigenvalues negatively
affect Parareal's convergence. This suggests that when Parareal is applied to
the nonlinear Navier-Stokes equations, problems for small viscosities could
arise. Numerical results for a driven cavity benchmark are presented,
confirming that Parareal's convergence can indeed deteriorate as viscosity
decreases and the flow becomes increasingly dominated by convection. The effect
is found to strongly depend on the spatial resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4591</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4591</id><created>2013-11-18</created><updated>2015-12-16</updated><authors><author><keyname>Edman</keyname><forenames>Matt</forenames></author><author><keyname>Kiayias</keyname><forenames>Aggelos</forenames></author><author><keyname>Tang</keyname><forenames>Qiang</forenames></author><author><keyname>Yener</keyname><forenames>Bulent</forenames></author></authors><title>On the Security of Key Extraction from Measuring Physical Quantities</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Key extraction via measuring a physical quantity is a class of information
theoretic key exchange protocols that rely on the physical characteristics of
the communication channel to enable the computation of a shared key by two (or
more) parties that share no prior secret information. The key is supposed to be
information theoretically hidden to an eavesdropper. Despite the recent surge
of research activity in the area, concrete claims about the security of the
protocols typically rely on channel abstractions that are not fully
experimentally substantiated. In this work, we propose a novel methodology for
the {\em experimental} security analysis of these protocols. The crux of our
methodology is a falsifiable channel abstraction that is accompanied by an
efficient experimental approximation algorithm of the {\em conditional
min-entropy} available to the two parties given the view of the eavesdropper.
  We focus on the signal strength between two wirelessly communicating
transceivers as the measured quantity and we use an experimental setup to
compute the conditional min-entropy of the channel given the view of the
attacker which we find to be linearly increasing. Armed with this understanding
of the channel, we showcase the methodology by providing a general protocol for
key extraction in this setting that is shown to be secure for a concrete
parameter selection. In this way we provide a first comprehensively analyzed
wireless key extraction protocol that is demonstrably secure against passive
adversaries. Our methodology uses hidden Markov models as the channel model and
a dynamic programming approach to approximate conditional min-entropy but other
possible instantiations of the methodology can be motivated by our work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4601</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4601</id><created>2013-11-18</created><authors><author><keyname>Dougherty</keyname><forenames>Randall</forenames></author><author><keyname>Freiling</keyname><forenames>Chris</forenames></author><author><keyname>Zeger</keyname><forenames>Kenneth</forenames></author></authors><title>Achievable Rate Regions for Network Coding</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determining the achievable rate region for networks using routing, linear
coding, or non-linear coding is thought to be a difficult task in general, and
few are known. We describe the achievable rate regions for four interesting
networks (completely for three and partially for the fourth). In addition to
the known matrix-computation method for proving outer bounds for linear coding,
we present a new method which yields actual characteristic-dependent linear
rank inequalities from which the desired bounds follow immediately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4606</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4606</id><created>2013-11-18</created><authors><author><keyname>Nepal</keyname><forenames>Surya</forenames></author><author><keyname>Paris</keyname><forenames>Cecile</forenames></author><author><keyname>Bista</keyname><forenames>Sanat Kumar</forenames></author><author><keyname>Sherchan</keyname><forenames>Wanita</forenames></author></authors><title>A Trust Model Based Analysis of Social Networks</title><categories>cs.SI physics.soc-ph</categories><journal-ref>International Journal of Trust Management in Computing and
  Communications 1, no. 1 (2013): 3-22</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyse the sustainability of social networks using STrust,
our social trust model. The novelty of the model is that it introduces the
concept of engagement trust and combines it with the popularity trust to derive
the social trust of the community as well as of individual members in the
community. This enables the recommender system to use these different types of
trust to recommend different things to the community, and identify (and
recommend) different roles. For example, it recommends mentors using the
engagement trust and leaders using the popularity trust. We then show the
utility of the model by analysing data from two types of social networks. We
also study the sustainability of a community through our social trust model. We
observe that a 5% drop in highly trusted members causes more than a 50% drop in
social capital that, in turn, raises the question of sustainability of the
community. We report our analysis and its results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4609</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4609</id><created>2013-11-18</created><authors><author><keyname>Treleaven</keyname><forenames>Kyle</forenames></author><author><keyname>Bialkowski</keyname><forenames>Josh</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author></authors><title>An O(M log M) Algorithm for Bipartite Matching with Roadmap Distances</title><categories>cs.DS</categories><comments>14 pages, 1 figure, 1 algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm is presented which produces the minimum cost bipartite matching
between two sets of M points each, where the cost of matching two points is
proportional to the minimum distance by which a particle could reach one point
from the other while constrained to travel on a connected set of curves, or
roads. Given any such roadmap, the algorithm obtains O(M log M) total runtime
in terms of M, which is the best possible bound in the sense that any algorithm
for minimal matching has runtime Omega(M log M). The algorithm is strongly
polynomial and is based on a capacity-scaling approach to the [minimum] convex
cost flow problem. The result generalizes the known Theta(M log M) complexity
of computing optimal matchings between two sets of points on (i) a line
segment, and (ii) a circle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4610</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4610</id><created>2013-11-18</created><updated>2013-11-23</updated><authors><author><keyname>Cuevas-Vicentt&#xed;n</keyname><forenames>V&#xed;ctor</forenames></author><author><keyname>Dey</keyname><forenames>Saumen</forenames></author><author><keyname>K&#xf6;hler</keyname><forenames>Sven</forenames></author><author><keyname>Riddle</keyname><forenames>Sean</forenames></author><author><keyname>Lud&#xe4;scher</keyname><forenames>Bertram</forenames></author></authors><title>Scientific Workflows and Provenance: Introduction and Research
  Opportunities</title><categories>cs.DB</categories><comments>12 pages, 2 figures</comments><journal-ref>Datenbank-Spektrum, November 2012, Volume 12, Issue 3, pp 193-203</journal-ref><doi>10.1007/s13222-012-0100-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific workflows are becoming increasingly popular for compute-intensive
and data-intensive scientific applications. The vision and promise of
scientific workflows includes rapid, easy workflow design, reuse, scalable
execution, and other advantages, e.g., to facilitate &quot;reproducible science&quot;
through provenance (e.g., data lineage) support. However, as described in the
paper, important research challenges remain. While the database community has
studied (business) workflow technologies extensively in the past, most current
work in scientific workflows seems to be done outside of the database
community, e.g., by practitioners and researchers in the computational sciences
and eScience. We provide a brief introduction to scientific workflows and
provenance, and identify areas and problems that suggest new opportunities for
database research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4615</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4615</id><created>2013-11-18</created><authors><author><keyname>Esmaeilsabzali</keyname><forenames>Shahram</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author><author><keyname>Wies</keyname><forenames>Thomas</forenames></author><author><keyname>Zufferey</keyname><forenames>Damien</forenames></author></authors><title>A Notion of Dynamic Interface for Depth-Bounded Object-Oriented Packages</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programmers using software components have to follow protocols that specify
when it is legal to call particular methods with particular arguments. For
example, one cannot use an iterator over a set once the set has been changed
directly or through another iterator. We formalize the notion of dynamic
package interfaces (DPI), which generalize state-machine interfaces for single
objects, and give an algorithm to statically compute a sound abstraction of a
DPI. States of a DPI represent (unbounded) sets of heap configurations and
edges represent the effects of method calls on the heap. We introduce a novel
heap abstract domain based on depth-bounded systems to deal with potentially
unboundedly many objects and the references among them. We have implemented our
algorithm and show that it is effective in computing representations of common
patterns of package usage, such as relationships between viewer and label,
container and iterator, and JDBC statements and cursors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4617</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4617</id><created>2013-11-18</created><authors><author><keyname>Xu</keyname><forenames>Zhaowei</forenames></author></authors><title>A New Perspective for Hoare's Logic and Peano's Arithmetic</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Hoare's logic is an axiomatic system of proving programs correct, which has
been extended to be a separation logic to reason about mutable heap structure.
We develop the most fundamental logical structure of strongest postcondition of
Hoare's logic in Peano's arithmetic $PA$. Let $p\in L$ and $S$ be any
while-program. The arithmetical definability of $\textbf{N}$-computable
function $f_S^{\textbf{N}}$ leads to separate $S$ from $SP(p,S)$, which defines
the strongest postcondition of $p$ and $S$ over $\textbf{N}$, achieving an
equivalent but more meaningful form in $PA$. From the reduction of Hoare's
logic to PA, together with well-defined underlying semantics, it follows that
Hoare's logic is sound and complete relative to the theory of $PA$, which is
different from the relative completeness in the sense of Cook. Finally, we
discuss two ways to extend computability from the standard structure to
nonstandard models of $PA$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4625</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4625</id><created>2013-11-19</created><updated>2013-11-19</updated><authors><author><keyname>Manchester</keyname><forenames>Ian R.</forenames></author><author><keyname>Slotine</keyname><forenames>Jean-Jacques E.</forenames></author></authors><title>Control Contraction Metrics and Universal Stabilizability</title><categories>math.OC cs.RO cs.SY</categories><comments>Conference submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce the concept of universal stabilizability: the
condition that every solution of a nonlinear system can be globally stabilized.
We give sufficient conditions in terms of the existence of a control
contraction metric, which can be found by solving a pointwise linear matrix
inequality. Extensions to approximate optimal control are straightforward. The
conditions we give are necessary and sufficient for linear systems and certain
classes of nonlinear systems, and have interesting connections to the theory of
control Lyapunov functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4627</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4627</id><created>2013-11-19</created><authors><author><keyname>Porter</keyname><forenames>Troy A.</forenames></author><author><keyname>Vladimirov</keyname><forenames>Andrey E.</forenames></author></authors><title>Calculation of Stochastic Heating and Emissivity of Cosmic Dust Grains
  with Optimization for the Intel Many Integrated Core Architecture</title><categories>astro-ph.IM astro-ph.GA cs.DC physics.comp-ph</categories><comments>23 pages. Submitted to Computer Physics Communications. Comments to a
  subsequent revision will indicate resources from which the source code of the
  HEATCODE library can be freely obtained</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cosmic dust particles effectively attenuate starlight. Their absorption of
starlight produces emission spectra from the near- to far-infrared, which
depends on the sizes and properties of the dust grains, and spectrum of the
heating radiation field. The near- to mid-infrared is dominated by the
emissions by very small grains. Modeling the absorption of starlight by these
particles is, however, computationally expensive and a significant bottleneck
for self-consistent radiation transport codes treating the heating of dust by
stars. In this paper, we summarize the formalism for computing the stochastic
emissivity of cosmic dust, which was developed in earlier works, and present a
new library HEATCODE implementing this formalism for the calculation for
arbitrary grain properties and heating radiation fields. Our library is highly
optimized for general-purpose processors with multiple cores and vector
instructions, with hierarchical memory cache structure. The HEATCODE library
also efficiently runs on co-processor cards implementing the Intel Many
Integrated Core (Intel MIC) architecture. We discuss in detail the optimization
steps that we took in order to optimize for the Intel MIC architecture, which
also significantly benefited the performance of the code on general-purpose
processors, and provide code samples and performance benchmarks for each step.
The HEATCODE library performance on a single Intel Xeon Phi coprocessor (Intel
MIC architecture) is approximately 2 times a general-purpose two-socket
multicore processor system with approximately the same nominal power
consumption. The library supports heterogeneous calculations employing host
processors simultaneously with multiple coprocessors, and can be easily
incorporated into existing radiation transport codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4634</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4634</id><created>2013-11-19</created><authors><author><keyname>Mashiach</keyname><forenames>Adam</forenames></author><author><keyname>Ostergaard</keyname><forenames>Jan</forenames></author><author><keyname>Zamir</keyname><forenames>Ram</forenames></author></authors><title>Sampling versus Random Binning for Multiple Descriptions of a
  Bandlimited Source</title><categories>cs.IT math.IT</categories><comments>Presented at the ITW'13. 5 pages, two-column mode, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random binning is an efficient, yet complex, coding technique for the
symmetric L-description source coding problem. We propose an alternative
approach, that uses the quantized samples of a bandlimited source as
&quot;descriptions&quot;. By the Nyquist condition, the source can be reconstructed if
enough samples are received. We examine a coding scheme that combines sampling
and noise-shaped quantization for a scenario in which only K &lt; L descriptions
or all L descriptions are received. Some of the received K-sets of descriptions
correspond to uniform sampling while others to non-uniform sampling. This
scheme achieves the optimum rate-distortion performance for uniform-sampling
K-sets, but suffers noise amplification for nonuniform-sampling K-sets. We then
show that by increasing the sampling rate and adding a random-binning stage,
the optimal operation point is achieved for any K-set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4639</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4639</id><created>2013-11-19</created><authors><author><keyname>Inoue</keyname><forenames>Katsumi</forenames><affiliation>Editors</affiliation></author><author><keyname>Sakama</keyname><forenames>Chiaki</forenames><affiliation>Editors</affiliation></author></authors><title>Post-Proceedings of the First International Workshop on Learning and
  Nonmonotonic Reasoning</title><categories>cs.AI cs.LG cs.LO</categories><comments>67 pages, 5 papers, 1 abstract, 1 cover</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge Representation and Reasoning and Machine Learning are two important
fields in AI. Nonmonotonic logic programming (NMLP) and Answer Set Programming
(ASP) provide formal languages for representing and reasoning with commonsense
knowledge and realize declarative problem solving in AI. On the other side,
Inductive Logic Programming (ILP) realizes Machine Learning in logic
programming, which provides a formal background to inductive learning and the
techniques have been applied to the fields of relational learning and data
mining. Generally speaking, NMLP and ASP realize nonmonotonic reasoning while
lack the ability of learning. By contrast, ILP realizes inductive learning
while most techniques have been developed under the classical monotonic logic.
With this background, some researchers attempt to combine techniques in the
context of nonmonotonic ILP. Such combination will introduce a learning
mechanism to programs and would exploit new applications on the NMLP side,
while on the ILP side it will extend the representation language and enable us
to use existing solvers. Cross-fertilization between learning and nonmonotonic
reasoning can also occur in such as the use of answer set solvers for ILP,
speed-up learning while running answer set solvers, learning action theories,
learning transition rules in dynamical systems, abductive learning, learning
biological networks with inhibition, and applications involving default and
negation. This workshop is the first attempt to provide an open forum for the
identification of problems and discussion of possible collaborations among
researchers with complementary expertise. The workshop was held on September
15th of 2013 in Corunna, Spain. This post-proceedings contains five technical
papers (out of six accepted papers) and the abstract of the invited talk by Luc
De Raedt.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4643</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4643</id><created>2013-11-19</created><authors><author><keyname>Achlioptas</keyname><forenames>Dimitris</forenames></author><author><keyname>Karnin</keyname><forenames>Zohar</forenames></author><author><keyname>Liberty</keyname><forenames>Edo</forenames></author></authors><title>Near-Optimal Entrywise Sampling for Data Matrices</title><categories>cs.LG cs.IT cs.NA math.IT stat.ML</categories><comments>14 pages, to appear in NIPS' 13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of selecting non-zero entries of a matrix $A$ in
order to produce a sparse sketch of it, $B$, that minimizes $\|A-B\|_2$. For
large $m \times n$ matrices, such that $n \gg m$ (for example, representing $n$
observations over $m$ attributes) we give sampling distributions that exhibit
four important properties. First, they have closed forms computable from
minimal information regarding $A$. Second, they allow sketching of matrices
whose non-zeros are presented to the algorithm in arbitrary order as a stream,
with $O(1)$ computation per non-zero. Third, the resulting sketch matrices are
not only sparse, but their non-zero entries are highly compressible. Lastly,
and most importantly, under mild assumptions, our distributions are provably
competitive with the optimal offline distribution. Note that the probabilities
in the optimal offline distribution may be complex functions of all the entries
in the matrix. Therefore, regardless of computational complexity, the optimal
distribution might be impossible to compute in the streaming model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4644</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4644</id><created>2013-11-19</created><authors><author><keyname>Gao</keyname><forenames>Yong</forenames></author><author><keyname>Liu</keyname><forenames>Lei</forenames></author><author><keyname>Lin</keyname><forenames>Xing</forenames></author><author><keyname>Liu</keyname><forenames>Yu</forenames></author></authors><title>A Qualitative Representation and Similarity Measurement Method in
  Geographic Information Retrieval</title><categories>cs.IR</categories><comments>17 Pages, 2 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modern geographic information retrieval technology is based on
quantitative models and methods. The semantic information in web documents and
queries cannot be effectively represented, leading to information lost or
misunderstanding so that the results are either unreliable or inconsistent. A
new qualitative approach is thus proposed for supporting geographic information
retrieval based on qualitative representation, semantic matching, and
qualitative reasoning. A qualitative representation model and the corresponding
similarity measurement method are defined. Information in documents and user
queries are represented using propositional logic, which considers the thematic
and geographic semantics synthetically. Thematic information is represented as
thematic propositions on the base of domain ontology. Similarly, spatial
information is represented as geo-spatial propositions with the support of
geographic knowledge base. Then the similarity is divided into thematic
similarity and spatial similarity. The former is calculated by the weighted
distance of proposition keywords in the domain ontology, and the latter
similarity is further divided into conceptual similarity and spatial
similarity. Represented by propositions and information units, the similarity
measurement can take evidence theory and fuzzy logic to combine all sub
similarities to get the final similarity between documents and queries. This
novel retrieval method is mainly used to retrieve the qualitative geographic
information to support the semantic matching and results ranking. It does not
deal with geometric computation and is consistent with human commonsense
cognition, and thus can improve the efficiency of geographic information
retrieval technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4658</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4658</id><created>2013-11-19</created><authors><author><keyname>Graells-Garrido</keyname><forenames>Eduardo</forenames></author><author><keyname>Lalmas</keyname><forenames>Mounia</forenames></author><author><keyname>Quercia</keyname><forenames>Daniele</forenames></author></authors><title>Data Portraits: Connecting People of Opposing Views</title><categories>cs.HC cs.SI</categories><comments>12 pages, 8 figures</comments><acm-class>H.5.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Social networks allow people to connect with each other and have
conversations on a wide variety of topics. However, users tend to connect with
like-minded people and read agreeable information, a behavior that leads to
group polarization. Motivated by this scenario, we study how to take advantage
of partial homophily to suggest agreeable content to users authored by people
with opposite views on sensitive issues. We introduce a paradigm to present a
data portrait of users, in which their characterizing topics are visualized and
their corresponding tweets are displayed using an organic design. Among their
tweets we inject recommended tweets from other people considering their views
on sensitive issues in addition to topical relevance, indirectly motivating
connections between dissimilar people. To evaluate our approach, we present a
case study on Twitter about a sensitive topic in Chile, where we estimate user
stances for regular people and find intermediary topics. We then evaluated our
design in a user study. We found that recommending topically relevant content
from authors with opposite views in a baseline interface had a negative
emotional effect. We saw that our organic visualization design reverts that
effect. We also observed significant individual differences linked to
evaluation of recommendations. Our results suggest that organic visualization
may revert the negative effects of providing potentially sensitive content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4665</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4665</id><created>2013-11-19</created><authors><author><keyname>Kamousi</keyname><forenames>Pegah</forenames></author><author><keyname>Lazard</keyname><forenames>Sylvain</forenames></author><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Wuhrer</keyname><forenames>Stefanie</forenames></author></authors><title>Analysis of Farthest Point Sampling for Approximating Geodesics in a
  Graph</title><categories>cs.CG cs.CV cs.GR</categories><comments>13 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A standard way to approximate the distance between any two vertices $p$ and
$q$ on a mesh is to compute, in the associated graph, a shortest path from $p$
to $q$ that goes through one of $k$ sources, which are well-chosen vertices.
Precomputing the distance between each of the $k$ sources to all vertices of
the graph yields an efficient computation of approximate distances between any
two vertices. One standard method for choosing $k$ sources, which has been used
extensively and successfully for isometry-invariant surface processing, is the
so-called Farthest Point Sampling (FPS), which starts with a random vertex as
the first source, and iteratively selects the farthest vertex from the already
selected sources.
  In this paper, we analyze the stretch factor $\mathcal{F}_{FPS}$ of
approximate geodesics computed using FPS, which is the maximum, over all pairs
of distinct vertices, of their approximated distance over their geodesic
distance in the graph. We show that $\mathcal{F}_{FPS}$ can be bounded in terms
of the minimal value $\mathcal{F}^*$ of the stretch factor obtained using an
optimal placement of $k$ sources as $\mathcal{F}_{FPS}\leq 2 r_e^2
\mathcal{F}^*+ 2 r_e^2 + 8 r_e + 1$, where $r_e$ is the ratio of the lengths of
the longest and the shortest edges of the graph. This provides some evidence
explaining why farthest point sampling has been used successfully for
isometry-invariant shape processing. Furthermore, we show that it is
NP-complete to find $k$ sources that minimize the stretch factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4703</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4703</id><created>2013-11-19</created><updated>2014-09-14</updated><authors><author><keyname>Horovitz</keyname><forenames>Michal</forenames></author><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author></authors><title>Constructions of Snake-in-the-Box Codes for Rank Modulation</title><categories>cs.IT math.CO math.IT</categories><comments>IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Snake-in-the-box code is a Gray code which is capable of detecting a single
error. Gray codes are important in the context of the rank modulation scheme
which was suggested recently for representing information in flash memories.
For a Gray code in this scheme the codewords are permutations, two consecutive
codewords are obtained by using the &quot;push-to-the-top&quot; operation, and the
distance measure is defined on permutations. In this paper the Kendall's
$\tau$-metric is used as the distance measure. We present a general method for
constructing such Gray codes. We apply the method recursively to obtain a snake
of length $M_{2n+1}=((2n+1)(2n)-1)M_{2n-1}$ for permutations of $S_{2n+1}$,
from a snake of length $M_{2n-1}$ for permutations of~$S_{2n-1}$. Thus, we have
$\lim\limits_{n\to \infty} \frac{M_{2n+1}}{S_{2n+1}}\approx 0.4338$, improving
on the previous known ratio of $\lim\limits_{n\to \infty} \frac{1}{\sqrt{\pi
n}}$. By using the general method we also present a direct construction. This
direct construction is based on necklaces and it might yield snakes of length
$\frac{(2n+1)!}{2} -2n+1$ for permutations of $S_{2n+1}$. The direct
construction was applied successfully for $S_7$ and $S_9$, and hence
$\lim\limits_{n\to \infty} \frac{M_{2n+1}}{S_{2n+1}}\approx 0.4743$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4715</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4715</id><created>2013-11-19</created><authors><author><keyname>Zhang</keyname><forenames>Chuang</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Xiong</keyname><forenames>Ke</forenames></author><author><keyname>Dong</keyname><forenames>Yunquan</forenames></author></authors><title>Every-user delay guarantee for wireless multiple access systems</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quality of service (QoS) requirements are usually different from user to
user in a multiaccess system, and it is necessary to take the different
requirements into account when allocating the shared resources of the system.
In this paper, we consider one QoS criterion--delay in a multiaccess system,
and we combine information theory and queueing theory in an attempt to analyze
whether a multiaccess system can meet the different delay requirements of
users. For users with the same transmission power, we prove that only $N$
inequalities are necessary for the checking, and for users with different
transmission powers, we provide a polynomial-time algorithm for such a
decision. In cases where the system cannot satisfy the delay requirements of
all users, we prove that as long as the sum power is larger than a threshold,
there is always an approach to adjust the transmission power of each user to
make the system delay feasible if power reallocation is available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4721</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4721</id><created>2013-11-19</created><updated>2014-04-17</updated><authors><author><keyname>Dobzinski</keyname><forenames>Shahar</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author><author><keyname>Oren</keyname><forenames>Sigal</forenames></author></authors><title>Economic Efficiency Requires Interaction</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the necessity of interaction between individuals for obtaining
approximately efficient allocations. The role of interaction in markets has
received significant attention in economic thinking, e.g. in Hayek's 1945
classic paper.
  We consider this problem in the framework of simultaneous communication
complexity. We analyze the amount of simultaneous communication required for
achieving an approximately efficient allocation. In particular, we consider two
settings: combinatorial auctions with unit demand bidders (bipartite matching)
and combinatorial auctions with subadditive bidders. For both settings we first
show that non-interactive systems have enormous communication costs relative to
interactive ones. On the other hand, we show that limited interaction enables
us to find approximately efficient allocations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4723</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4723</id><created>2013-11-19</created><authors><author><keyname>Kaspi</keyname><forenames>Yonatan</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Zero-Delay and Causal Secure Source Coding</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the combination between causal/zero-delay source coding and
information-theoretic secrecy. Two source coding models with secrecy
constraints are considered. We start by considering zero-delay perfectly secret
lossless transmission of a memoryless source. We derive bounds on the key rate
and coding rate needed for perfect zero-delay secrecy. In this setting, we
consider two models which differ by the ability of the eavesdropper to parse
the bit-stream passing from the encoder to the legitimate decoder into separate
messages. We also consider causal source coding with a fidelity criterion and
side information at the decoder and the eavesdropper. Unlike the zero-delay
setting where variable-length coding is traditionally used but might leak
information on the source through the length of the codewords, in this setting,
since delay is allowed, block coding is possible. We show that in this setting,
separation of encryption and causal source coding is optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4728</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4728</id><created>2013-11-19</created><updated>2014-12-12</updated><authors><author><keyname>Sviridenko</keyname><forenames>Maxim</forenames></author><author><keyname>Vondr&#xe1;k</keyname><forenames>Jan</forenames></author><author><keyname>Ward</keyname><forenames>Justin</forenames></author></authors><title>Optimal approximation for submodular and supermodular optimization with
  bounded curvature</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design new approximation algorithms for the problems of optimizing
submodular and supermodular functions subject to a single matroid constraint.
Specifically, we consider the case in which we wish to maximize a nondecreasing
submodular function or minimize a nonincreasing supermodular function in the
setting of bounded total curvature $c$. In the case of submodular maximization
with curvature $c$, we obtain a $(1-c/e)$-approximation --- the first
improvement over the greedy $(1-e^{-c})/c$-approximation of Conforti and
Cornuejols from 1984, which holds for a cardinality constraint, as well as
recent approaches that hold for an arbitrary matroid constraint.
  Our approach is based on modifications of the continuous greedy algorithm and
non-oblivious local search, and allows us to approximately maximize the sum of
a nonnegative, nondecreasing submodular function and a (possibly negative)
linear function. We show how to reduce both submodular maximization and
supermodular minimization to this general problem when the objective function
has bounded total curvature. We prove that the approximation results we obtain
are the best possible in the value oracle model, even in the case of a
cardinality constraint.
  We define an extension of the notion of curvature to general monotone set
functions and show $(1-c)$-approximation for maximization and
$1/(1-c)$-approximation for minimization cases. Finally, we give two concrete
applications of our results in the settings of maximum entropy sampling, and
the column-subset selection problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4731</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4731</id><created>2013-11-19</created><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Wang</keyname><forenames>Jian</forenames></author></authors><title>How to improve the prediction based on citation impact percentiles for
  years shortly after the publication date?</title><categories>cs.DL stat.AP</categories><comments>Accepted for publication in the Journal of Informetrics. arXiv admin
  note: text overlap with arXiv:1306.4454</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The findings of Bornmann, Leydesdorff, and Wang (in press) revealed that the
consideration of journal impact improves the prediction of long-term citation
impact. This paper further explores the possibility of improving citation
impact measurements on the base of a short citation window by the consideration
of journal impact and other variables, such as the number of authors, the
number of cited references, and the number of pages. The dataset contains
475,391 journal papers published in 1980 and indexed in Web of Science (WoS,
Thomson Reuters), and all annual citation counts (from 1980 to 2010) for these
papers. As an indicator of citation impact, we used percentiles of citations
calculated using the approach of Hazen (1914). Our results show that citation
impact measurement can really be improved: If factors generally influencing
citation impact are considered in the statistical analysis, the explained
variance in the long-term citation impact can be much increased. However, this
increase is only visible when using the years shortly after publication but not
when using later years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4733</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4733</id><created>2013-11-19</created><authors><author><keyname>Van Horn</keyname><forenames>David</forenames></author></authors><title>The Complexity of Flow Analysis in Higher-Order Languages</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This dissertation proves lower bounds on the inherent difficulty of deciding
flow analysis problems in higher-order programming languages. We give exact
characterizations of the computational complexity of 0CFA, the $k$CFA
hierarchy, and related analyses. In each case, we precisely capture both the
expressiveness and feasibility of the analysis, identifying the elements
responsible for the trade-off.
  0CFA is complete for polynomial time. This result relies on the insight that
when a program is linear (each bound variable occurs exactly once), the
analysis makes no approximation; abstract and concrete interpretation coincide,
and therefore pro- gram analysis becomes evaluation under another guise.
Moreover, this is true not only for 0CFA, but for a number of further
approximations to 0CFA. In each case, we derive polynomial time completeness
results.
  For any $k &gt; 0$, $k$CFA is complete for exponential time. Even when $k = 1$,
the distinction in binding contexts results in a limited form of closures,
which do not occur in 0CFA. This theorem validates empirical observations that
$k$CFA is intractably slow for any $k &gt; 0$. There is, in the worst case---and
plausibly, in practice---no way to tame the cost of the analysis. Exponential
time is required. The empirically observed intractability of this analysis can
be understood as being inherent in the approximation problem being solved,
rather than reflecting unfortunate gaps in our programming abilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4759</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4759</id><created>2013-11-19</created><updated>2014-09-12</updated><authors><author><keyname>Aardal</keyname><forenames>Karen</forenames></author><author><keyname>Berg</keyname><forenames>Pieter van den</forenames></author><author><keyname>Gijswijt</keyname><forenames>Dion</forenames></author><author><keyname>Li</keyname><forenames>Shanfei</forenames></author></authors><title>Approximation Algorithms for Hard Capacitated $k$-facility Location
  Problems</title><categories>cs.DS math.OC</categories><comments>We add new results obtained with Karen Aardal and Pieter van den Berg
  to the previous version</comments><msc-class>90B80 (primary), 68W25 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the capacitated $k$-facility location problem, in which we are given
a set of clients with demands, a set of facilities with capacities and a
constant number $k$. It costs $f_i$ to open facility $i$, and $c_{ij}$ for
facility $i$ to serve one unit of demand from client $j$. The objective is to
open at most $k$ facilities serving all the demands and satisfying the capacity
constraints while minimizing the sum of service and opening costs.
  In this paper, we give the first fully polynomial time approximation scheme
(FPTAS) for the single-sink (single-client) capacitated $k$-facility location
problem. Then, we show that the capacitated $k$-facility location problem with
uniform capacities is solvable in polynomial time if the number of clients is
fixed by reducing it to a collection of transportation problems. Third, we
analyze the structure of extreme point solutions, and examine the efficiency of
this structure in designing approximation algorithms for capacitated
$k$-facility location problems. Finally, we extend our results to obtain an
improved approximation algorithm for the capacitated facility location problem
with uniform opening cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4762</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4762</id><created>2013-11-19</created><updated>2014-02-10</updated><authors><author><keyname>de Rigo</keyname><forenames>Daniele</forenames></author></authors><title>Software Uncertainty in Integrated Environmental Modelling: the role of
  Semantics and Open Science</title><categories>cs.SY cs.CE</categories><comments>This is the author's version of the work. The definitive version is
  published in the Vol. 15 of Geophysical Research Abstracts (ISSN 1607-7962)
  and has been presented at the European Geosciences Union (EGU) General
  Assembly 2013, Vienna, Austria, 07-12 April 2013 http://www.egu2013.eu/
  [Updated style and fixed few typos]</comments><journal-ref>Geophysical Research Abstracts 15 (2013) 13292+</journal-ref><doi>10.6084/m9.figshare.155701</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Computational aspects increasingly shape environmental sciences. Actually,
transdisciplinary modelling of complex and uncertain environmental systems is
challenging computational science (CS) and also the science-policy interface.
Large spatial-scale problems falling within this category - i.e. wide-scale
transdisciplinary modelling for environment (WSTMe) - often deal with factors
(a) for which deep-uncertainty may prevent usual statistical analysis of
modelled quantities and need different ways for providing policy-making with
science-based support. Here, practical recommendations are proposed for
tempering a peculiar - not infrequently underestimated - source of uncertainty.
Software errors in complex WSTMe may subtly affect the outcomes with possible
consequences even on collective environmental decision-making. Semantic
transparency in CS and free software are discussed as possible mitigations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4764</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4764</id><created>2013-11-19</created><authors><author><keyname>Stowell</keyname><forenames>Dan</forenames></author><author><keyname>Plumbley</keyname><forenames>Mark D.</forenames></author></authors><title>Large-scale analysis of frequency modulation in birdsong databases</title><categories>cs.SD</categories><journal-ref>Methods in Ecology and Evolution, Volume 5, Issue 9, pages
  901-912, September 2014</journal-ref><doi>10.1111/2041-210X.12223</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Birdsong often contains large amounts of rapid frequency modulation (FM). It
is believed that the use or otherwise of FM is adaptive to the acoustic
environment, and also that there are specific social uses of FM such as trills
in aggressive territorial encounters. Yet temporal fine detail of FM is often
absent or obscured in standard audio signal analysis methods such as Fourier
analysis or linear prediction. Hence it is important to consider high
resolution signal processing techniques for analysis of FM in bird
vocalisations. If such methods can be applied at big data scales, this offers a
further advantage as large datasets become available.
  We introduce methods from the signal processing literature which can go
beyond spectrogram representations to analyse the fine modulations present in a
signal at very short timescales. Focusing primarily on the genus Phylloscopus,
we investigate which of a set of four analysis methods most strongly captures
the species signal encoded in birdsong. In order to find tools useful in
practical analysis of large databases, we also study the computational time
taken by the methods, and their robustness to additive noise and MP3
compression.
  We find three methods which can robustly represent species-correlated FM
attributes, and that the simplest method tested also appears to perform the
best. We find that features representing the extremes of FM encode species
identity supplementary to that captured in frequency features, whereas
bandwidth features do not encode additional information.
  Large-scale FM analysis can efficiently extract information useful for
bioacoustic studies, in addition to measures more commonly used to characterise
vocalisations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4766</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4766</id><created>2013-11-18</created><authors><author><keyname>Ham</keyname><forenames>Nicholas</forenames></author></authors><title>Classifications of Symmetric Normal Form Games</title><categories>math.CO cs.GT</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we survey various classifications of symmetric games and their
characterisations under the theme of fairness; show that game bijections and
game isomorphisms form groupoids; introduce matchings as a convenient
characterisation of strategy triviality; and outline how to construct and
partially order parameterised symmetric games with numerous examples that range
over various classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4768</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4768</id><created>2013-11-19</created><updated>2014-12-23</updated><authors><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author></authors><title>Editing to a Graph of Given Degrees</title><categories>cs.DS</categories><acm-class>F.2.2; G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Editing to a Graph of Given Degrees problem that asks for a
graph G, non-negative integers d,k and a function \delta:V(G)-&gt;{1,...,d},
whether it is possible to obtain a graph G' from G such that the degree of v is
\delta(v) for any vertex v by at most k vertex or edge deletions or edge
additions. We construct an FPT-algorithm for Editing to a Graph of Given
Degrees parameterized by d+k. We complement this result by showing that the
problem has no polynomial kernel unless NP\subseteq coNP/poly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4769</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4769</id><created>2013-11-17</created><updated>2013-11-24</updated><authors><author><keyname>Wu</keyname><forenames>Yuanxin</forenames></author></authors><title>On 'A Kalman Filter-Based Algorithm for IMU-Camera Calibration:
  Observability Analysis and Performance Evaluation'</title><categories>cs.RO cs.SY</categories><comments>3 pages. This work was done in 2009. Abstract revised and More refs
  added in this new version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The above-mentioned work [1] in IEEE-TR'08 presented an extended Kalman
filter for calibrating the misalignment between a camera and an IMU. As one of
the main contributions, the locally weakly observable analysis was carried out
using Lie derivatives. The seminal paper [1] is undoubtedly the cornerstone of
current observability work in SLAM and a number of real SLAM systems have been
developed on the observability result of this paper, such as [2, 3]. However,
the main observability result of this paper [1] is founded on an incorrect
proof and actually cannot be acquired using the local observability technique
therein, a fact that is apparently not noticed by the SLAM community over a
number of years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4778</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4778</id><created>2013-11-19</created><authors><author><keyname>Barth</keyname><forenames>Lukas</forenames></author><author><keyname>Fabrikant</keyname><forenames>Sara Irina</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen</forenames></author><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author><author><keyname>N&#xf6;llenburg</keyname><forenames>Martin</forenames></author><author><keyname>Okamoto</keyname><forenames>Yoshio</forenames></author><author><keyname>Pupyrev</keyname><forenames>Sergey</forenames></author><author><keyname>Squarcella</keyname><forenames>Claudio</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author><author><keyname>Wolff</keyname><forenames>Alexander</forenames></author></authors><title>Semantic Word Cloud Representations: Hardness and Approximation
  Algorithms</title><categories>cs.CG</categories><comments>14 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:1304.8016</comments><msc-class>68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a geometric representation problem, where we are given a set $\cal
R$ of axis-aligned rectangles with fixed dimensions and a graph with vertex set
$\cal R$. The task is to place the rectangles without overlap such that two
rectangles touch if and only if the graph contains an edge between them. We
call this problem Contact Representation of Word Networks (CROWN). It
formalizes the geometric problem behind drawing word clouds in which
semantically related words are close to each other. Here, we represent words by
rectangles and semantic relationships by edges. We show that CROWN is strongly
NP-hard even restricted trees and weakly NP-hard if restricted stars. We
consider the optimization problem Max-CROWN where each adjacency induces a
certain profit and the task is to maximize the sum of the profits. For this
problem, we present constant-factor approximations for several graph classes,
namely stars, trees, planar graphs, and graphs of bounded degree. Finally, we
evaluate the algorithms experimentally and show that our best method improves
upon the best existing heuristic by 45%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4780</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4780</id><created>2013-11-19</created><updated>2014-03-21</updated><authors><author><keyname>Neiswanger</keyname><forenames>Willie</forenames></author><author><keyname>Wang</keyname><forenames>Chong</forenames></author><author><keyname>Xing</keyname><forenames>Eric</forenames></author></authors><title>Asymptotically Exact, Embarrassingly Parallel MCMC</title><categories>stat.ML cs.DC cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication costs, resulting from synchronization requirements during
learning, can greatly slow down many parallel machine learning algorithms. In
this paper, we present a parallel Markov chain Monte Carlo (MCMC) algorithm in
which subsets of data are processed independently, with very little
communication. First, we arbitrarily partition data onto multiple machines.
Then, on each machine, any classical MCMC method (e.g., Gibbs sampling) may be
used to draw samples from a posterior distribution given the data subset.
Finally, the samples from each machine are combined to form samples from the
full posterior. This embarrassingly parallel algorithm allows each machine to
act independently on a subset of the data (without communication) until the
final combination stage. We prove that our algorithm generates asymptotically
exact samples and empirically demonstrate its ability to parallelize burn-in
and sampling in several models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4782</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4782</id><created>2013-11-19</created><authors><author><keyname>Budi&#x161;in</keyname><forenames>Srdjan</forenames></author><author><keyname>Spasojevi&#x107;</keyname><forenames>Predrag</forenames></author></authors><title>Universal Generator for Complementary Pairs of Sequences Based on
  Boolean Functions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general algorithm for generating arbitrary standard
complementary pairs of sequences (including binary, polyphase, M-PSK and QAM)
of length 2^N using Boolean functions. The algorithm follows our earlier
paraunitary algorithm, but does not require matrix multiplications. The
algorithm can be easily and efficiently implemented in hardware. As a special
case, it reduces to the non-recursive (direct) algorithm for generating binary
sequences given by Golay, to the algorithm for generating M-PSK sequences given
by Davis and Jedwab (and later improved by Paterson) and to all published
algorithms for generating QAM sequences. However the algorithm does not solve
the problem of sequence uniqueness (except for the trivial M-PSK case), which
must be treated separately for each QAM constellation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4799</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4799</id><created>2013-11-19</created><authors><author><keyname>Xu</keyname><forenames>Xi</forenames></author><author><keyname>Ansari</keyname><forenames>Rashid</forenames></author><author><keyname>Khokhar</keyname><forenames>Ashfaq</forenames></author></authors><title>Adaptive Hierarchical Data Aggregation using Compressive Sensing
  (A-HDACS) for Non-smooth Data Field</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Compressive Sensing (CS) has been applied successfully in a wide variety of
applications in recent years, including photography, shortwave infrared
cameras, optical system research, facial recognition, MRI, etc. In wireless
sensor networks (WSNs), significant research work has been pursued to
investigate the use of CS to reduce the amount of data communicated,
particularly in data aggregation applications and thereby improving energy
efficiency. However, most of the previous work in WSN has used CS under the
assumption that data field is smooth with negligible white Gaussian noise. In
these schemes signal sparsity is estimated globally based on the entire data
field, which is then used to determine the CS parameters. In more realistic
scenarios, where data field may have regional fluctuations or it is piecewise
smooth, existing CS based data aggregation schemes yield poor compression
efficiency. In order to take full advantage of CS in WSNs, we propose an
Adaptive Hierarchical Data Aggregation using Compressive Sensing (A-HDACS)
scheme. The proposed schemes dynamically chooses sparsity values based on
signal variations in local regions. We prove that A-HDACS enables more sensor
nodes to employ CS compared to the schemes that do not adapt to the changing
field. The simulation results also demonstrate the improvement in energy
efficiency as well as accurate signal recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4803</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4803</id><created>2013-11-19</created><updated>2014-02-06</updated><authors><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Mahdavi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author></authors><title>Beating the Minimax Rate of Active Learning with Prior Knowledge</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active learning refers to the learning protocol where the learner is allowed
to choose a subset of instances for labeling. Previous studies have shown that,
compared with passive learning, active learning is able to reduce the label
complexity exponentially if the data are linearly separable or satisfy the
Tsybakov noise condition with parameter $\kappa=1$. In this paper, we propose a
novel active learning algorithm using a convex surrogate loss, with the goal to
broaden the cases for which active learning achieves an exponential
improvement. We make use of a convex loss not only because it reduces the
computational cost, but more importantly because it leads to a tight bound for
the empirical process (i.e., the difference between the empirical estimation
and the expectation) when the current solution is close to the optimal one.
Under the assumption that the norm of the optimal classifier that minimizes the
convex risk is available, our analysis shows that the introduction of the
convex surrogate loss yields an exponential reduction in the label complexity
even when the parameter $\kappa$ of the Tsybakov noise is larger than $1$. To
the best of our knowledge, this is the first work that improves the minimax
rate of active learning by utilizing certain priori knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4805</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4805</id><created>2013-11-19</created><authors><author><keyname>Cruise</keyname><forenames>James</forenames></author><author><keyname>Ganesh</keyname><forenames>Ayalvadi</forenames></author></authors><title>Probabilistic consensus via polling and majority rules</title><categories>math.PR cs.DC</categories><msc-class>60J10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider lightweight decentralised algorithms for achieving
consensus in distributed systems. Each member of a distributed group has a
private value from a fixed set consisting of, say, two elements, and the goal
is for all members to reach consensus on the majority value. We explore
variants of the voter model applied to this problem. In the voter model, each
node polls a randomly chosen group member and adopts its value. The process is
repeated until consensus is reached. We generalize this so that each member
polls a (deterministic or random) number of other group members and changes
opinion only if a suitably defined super-majority has a different opinion. We
show that this modification greatly speeds up the convergence of the algorithm,
as well as substantially reducing the probability of it reaching consensus on
the incorrect value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4809</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4809</id><created>2013-11-19</created><authors><author><keyname>Govindasamy</keyname><forenames>Siddhartan</forenames></author></authors><title>Uplink Performance of Large Optimum-Combining Antenna Arrays in
  Poisson-Cell Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The uplink of a wireless network with base stations distributed according to
a Poisson Point Process (PPP) is analyzed. The base stations are assumed to
have a large number of antennas and use linear minimum-mean-square-error (MMSE)
spatial processing for multiple access. The number of active mobiles per cell
is limited to permit channel estimation using pilot sequences that are
orthogonal in each cell. The cumulative distribution function (CDF) of a
randomly located link in a typical cell of such a system is derived when
accurate channel estimation is available. A simple bound is provided for the
spectral efficiency when channel estimates suffer from pilot contamination. The
results provide insight into the performance of so-called massive
Multiple-Input-Multiple-Output (MIMO) systems in spatially distributed cellular
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4818</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4818</id><created>2013-11-19</created><updated>2014-01-10</updated><authors><author><keyname>Bi</keyname><forenames>Huibo</forenames></author><author><keyname>Gelenbe</keyname><forenames>Erol</forenames></author></authors><title>Routing Diverse Evacuees with Cognitive Packets</title><categories>cs.OH</categories><comments>7 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the idea of smart building evacuation when evacuees can
belong to different categories with respect to their ability to move and their
health conditions. This leads to new algorithms that use the Cognitive Packet
Network concept to tailor different quality of service needs to different
evacuees. These ideas are implemented in a simulated environment and evaluated
with regard to their effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4821</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4821</id><created>2013-11-19</created><updated>2016-01-15</updated><authors><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author><author><keyname>Perkins</keyname><forenames>Will</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh</forenames></author></authors><title>On the Complexity of Random Satisfiability Problems with Planted
  Solutions</title><categories>cs.CC cs.DM cs.DS math.CO math.PR</categories><comments>Extended abstract appears in STOC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of identifying a planted assignment given a random
$k$-SAT formula consistent with the assignment. This problem exhibits a large
algorithmic gap: while the planted solution can always be identified given a
formula with $O(n\log n)$ clauses, there are distributions over clauses for
which the best known efficient algorithms require $n^{k/2}$ clauses. We propose
and study a unified model for planted $k$-SAT, which captures well-known
special cases. An instance is described by a planted assignment and a
distribution on clauses with $k$ literals. We define its distribution
complexity as the largest $r$ for which the distribution is not $r$-wise
independent ($1 \leq r \leq k$ for any distribution with a planted assignment).
  Our main result is an unconditional lower bound, tight up to logarithmic
factors, of $\tilde\Omega(n^{r/2})$ clauses for statistical (query) algorithms,
matching the known upper bound (which, as we show, can be implemented using a
statistical algorithm). Since known approaches for problems over distributions
have statistical analogues (spectral, MCMC, gradient-based, convex optimization
etc.), this lower bound provides a rigorous explanation of the observed
algorithmic gap. The proof introduces a new general technique for the analysis
of statistical algorithms. It also points to a geometric paring phenomenon in
the space of all planted assignments.
  We describe consequences of our lower bounds to Feige's (2002) refutation
hypothesis and to lower bounds on general convex programs that solve planted
$k$-SAT. Our bounds also extend to other planted $k$-CSP models, and, in
particular, provide concrete evidence for the security of Goldreich's (2000)
one-way function and the associated pseudorandom generator when used with a
sufficiently hard predicate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4825</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4825</id><created>2013-11-19</created><updated>2015-06-08</updated><authors><author><keyname>Contal</keyname><forenames>Emile</forenames></author><author><keyname>Perchet</keyname><forenames>Vianney</forenames></author><author><keyname>Vayatis</keyname><forenames>Nicolas</forenames></author></authors><title>Gaussian Process Optimization with Mutual Information</title><categories>stat.ML cs.LG</categories><comments>Proceedings of The 31st International Conference on Machine Learning
  (ICML 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze a generic algorithm scheme for sequential global
optimization using Gaussian processes. The upper bounds we derive on the
cumulative regret for this generic algorithm improve by an exponential factor
the previously known bounds for algorithms like GP-UCB. We also introduce the
novel Gaussian Process Mutual Information algorithm (GP-MI), which
significantly improves further these upper bounds for the cumulative regret. We
confirm the efficiency of this algorithm on synthetic and real tasks against
the natural competitor, GP-UCB, and also the Expected Improvement heuristic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4830</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4830</id><created>2013-11-19</created><updated>2015-10-02</updated><authors><author><keyname>Ferrante</keyname><forenames>Guido Carlo</forenames></author><author><keyname>Di Benedetto</keyname><forenames>Maria-Gabriella</forenames></author></authors><title>Spectral Efficiency of Random Time-Hopping CDMA</title><categories>cs.IT math.IT</categories><comments>26 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally paired with impulsive communications, Time-Hopping CDMA
(TH-CDMA) is a multiple access technique that separates users in time by coding
their transmissions into pulses occupying a subset of $N_\mathsf{s}$ chips out
of the total $N$ included in a symbol period, in contrast with traditional
Direct-Sequence CDMA (DS-CDMA) where $N_\mathsf{s}=N$. This work analyzes
TH-CDMA with random spreading, by determining whether peculiar theoretical
limits are identifiable, with both optimal and sub-optimal receiver structures,
in particular in the archetypal case of sparse spreading, that is,
$N_\mathsf{s}=1$. Results indicate that TH-CDMA has a fundamentally different
behavior than DS-CDMA, where the crucial role played by energy concentration,
typical of time-hopping, directly relates with its intrinsic &quot;uneven&quot; use of
degrees of freedom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4833</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4833</id><created>2013-11-19</created><authors><author><keyname>Morvant</keyname><forenames>Emilie</forenames><affiliation>IST Austria</affiliation></author></authors><title>Domain Adaptation of Majority Votes via Perturbed Variation-based Label
  Transfer</title><categories>stat.ML cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tackle the PAC-Bayesian Domain Adaptation (DA) problem. This arrives when
one desires to learn, from a source distribution, a good weighted majority vote
(over a set of classifiers) on a different target distribution. In this
context, the disagreement between classifiers is known crucial to control. In
non-DA supervised setting, a theoretical bound - the C-bound - involves this
disagreement and leads to a majority vote learning algorithm: MinCq. In this
work, we extend MinCq to DA by taking advantage of an elegant divergence
between distribution called the Perturbed Varation (PV). Firstly, justified by
a new formulation of the C-bound, we provide to MinCq a target sample labeled
thanks to a PV-based self-labeling focused on regions where the source and
target marginal distributions are closer. Secondly, we propose an original
process for tuning the hyperparameters. Our framework shows very promising
results on a toy problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4834</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4834</id><created>2013-11-19</created><updated>2015-07-24</updated><authors><author><keyname>Haimi-Cohen</keyname><forenames>Raziel</forenames></author><author><keyname>Lai</keyname><forenames>Yenming Mark</forenames></author></authors><title>Compressive Measurements Generated by Structurally Random Matrices:
  Asymptotic Normality and Quantization</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in Signal Processing Journal, Elsevier in
  July 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structurally random matrices (SRMs) are a practical alternative to fully
random matrices (FRMs) when generating compressive sensing measurements because
of their computational efficiency and their universality with respect to the
sparsifing basis. In this work we derive the statistical distribution of
compressive measurements generated by various types of SRMs, as a function of
the signal properties. We show that under a wide range of conditions, that
distribution is a mixture of asymptotically multi-variate normal components. We
point out the implications for quantization and coding of the measurements and
discuss design consideration for measurements transmission systems. Simulations
on real-world video signals confirm the theoretical findings and show that the
signal randomization of SRMs yields a dramatic improvement in quantization
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4839</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4839</id><created>2013-11-19</created><updated>2014-04-17</updated><authors><author><keyname>Galanis</keyname><forenames>Andreas</forenames></author><author><keyname>Stefankovic</keyname><forenames>Daniel</forenames></author><author><keyname>Vigoda</keyname><forenames>Eric</forenames></author><author><keyname>Yang</keyname><forenames>Linji</forenames></author></authors><title>Ferromagnetic Potts Model: Refined #BIS-hardness and Related Results</title><categories>cs.CC math-ph math.MP math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent results establish for 2-spin antiferromagnetic systems that the
computational complexity of approximating the partition function on graphs of
maximum degree D undergoes a phase transition that coincides with the
uniqueness/non-uniqueness phase transition on the infinite D-regular tree. For
the ferromagnetic Potts model we investigate whether analogous hardness results
hold. Goldberg and Jerrum showed that approximating the partition function of
the ferromagnetic Potts model is at least as hard as approximating the number
of independent sets in bipartite graphs (#BIS-hardness). We improve this
hardness result by establishing it for bipartite graphs of maximum degree D. We
first present a detailed picture for the phase diagram for the infinite
D-regular tree, giving a refined picture of its first-order phase transition
and establishing the critical temperature for the coexistence of the disordered
and ordered phases. We then prove for all temperatures below this critical
temperature (the region where the ordered phase &quot;dominates&quot;) that it is
#BIS-hard to approximate the partition function on bipartite graphs of maximum
degree D.
  The #BIS-hardness result uses random bipartite regular graphs as a gadget in
the reduction. The analysis of these random graphs relies on recent results
establishing connections between the maxima of the expectation of their
partition function, attractive fixpoints of the associated tree recursions, and
induced matrix norms. In this paper we extend these connections to random
regular graphs for all ferromagnetic models. Using these connections, we
establish the Bethe prediction for every ferromagnetic spin system on random
regular graphs. As a further consequence, we prove for the ferromagnetic Potts
model that the Swendsen-Wang algorithm is torpidly mixing on random D-regular
graphs at the critical temperature for sufficiently large q.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4854</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4854</id><created>2013-11-19</created><updated>2013-11-20</updated><authors><author><keyname>Beingessner</keyname><forenames>Alexis</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>Computing the Coverage of an Opaque Forest</title><categories>cs.CG</categories><comments>4 pages, 7 figures. Accepted and presented at the 24th annual
  Canadian Conference on Computational Geometry (2012)</comments><journal-ref>CCCG 2012: 95-100</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of taking an opaque forest and determining the
regions that are covered by it. We provide a tight upper bound on the
complexity of this problem, and an algorithm for computing this area, which is
worst-case optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4859</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4859</id><created>2013-11-19</created><authors><author><keyname>Cheng</keyname><forenames>Jialong</forenames></author><author><keyname>Sitharam</keyname><forenames>Meera</forenames></author><author><keyname>Streinu</keyname><forenames>Ileana</forenames></author></authors><title>Nucleation-free $3D$ rigidity</title><categories>cs.CG math.CO math.MG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When all non-edge distances of a graph realized in $\mathbb{R}^{d}$ as a {\em
bar-and-joint framework} are generically {\em implied} by the bar (edge)
lengths, the graph is said to be {\em rigid} in $\mathbb{R}^{d}$. For $d=3$,
characterizing rigid graphs, determining implied non-edges and {\em dependent}
edge sets remains an elusive, long-standing open problem.
  One obstacle is to determine when implied non-edges can exist without
non-trivial rigid induced subgraphs, i.e., {\em nucleations}, and how to deal
with them.
  In this paper, we give general inductive construction schemes and proof
techniques to generate {\em nucleation-free graphs} (i.e., graphs without any
nucleation) with implied non-edges. As a consequence, we obtain (a) dependent
graphs in $3D$ that have no nucleation; and (b) $3D$ nucleation-free {\em
rigidity circuits}, i.e., minimally dependent edge sets in $d=3$. It
additionally follows that true rigidity is strictly stronger than a tractable
approximation to rigidity given by Sitharam and Zhou
\cite{sitharam:zhou:tractableADG:2004}, based on an inductive combinatorial
characterization.
  As an independently interesting byproduct, we obtain a new inductive
construction for independent graphs in $3D$. Currently, very few such inductive
constructions are known, in contrast to $2D$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4860</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4860</id><created>2013-11-19</created><authors><author><keyname>Barba</keyname><forenames>Luis</forenames></author><author><keyname>Beingessner</keyname><forenames>Alexis</forenames></author><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Smid</keyname><forenames>Michiel H. M.</forenames></author></authors><title>Computing Covers of Plane Forests</title><categories>cs.CG</categories><comments>6 pages, 3 figures. Accepted and presented at the 25th annual
  Canadian Conference on Computational Geometry (CCCG 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\phi$ be a function that maps any non-empty subset $A$ of $\mathbb{R}^2$
to a non-empty subset $\phi(A)$ of $\mathbb{R}^2$. A $\phi$-cover of a set
$T=\{T_1, T_2, \dots, T_m\}$ of pairwise non-crossing trees in the plane is a
set of pairwise disjoint connected regions such that each tree $T_i$ is
contained in some region of the cover, and each region of the cover is either
(1) $\phi(T_i)$ for some $i$, or (2) $\phi(A \cup B)$, where $A$ and $B$ are
constructed by either (1) or (2), and $A \cap B \neq \emptyset$.
  We present two properties for the function $\phi$ that make the $\phi$-cover
well-defined. Examples for such functions $\phi$ are the convex hull and the
axis-aligned bounding box. For both of these functions $\phi$, we show that the
$\phi$-cover can be computed in $O(n\log^2n)$ time, where $n$ is the total
number of vertices of the trees in $T$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4861</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4861</id><created>2013-11-19</created><authors><author><keyname>N&#xf3;brega</keyname><forenames>Roberto W.</forenames></author><author><keyname>Feng</keyname><forenames>Chen</forenames></author><author><keyname>Silva</keyname><forenames>Danilo</forenames></author><author><keyname>Uch&#xf4;a-Filho</keyname><forenames>Bartolomeu F.</forenames></author></authors><title>On Multiplicative Matrix Channels over Finite Chain Rings</title><categories>cs.IT math.IT</categories><comments>23 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by physical-layer network coding, this paper considers
communication in multiplicative matrix channels over finite chain rings. Such
channels are defined by the law $Y =A X$, where $X$ and $Y$ are the input and
output matrices, respectively, and $A$ is called the transfer matrix. It is
assumed a coherent scenario in which the instances of the transfer matrix are
unknown to the transmitter, but available to the receiver. It is also assumed
that $A$ and $X$ are independent. Besides that, no restrictions on the
statistics of $A$ are imposed. As contributions, a closed-form expression for
the channel capacity is obtained, and a coding scheme for the channel is
proposed. It is then shown that the scheme can achieve the capacity with
polynomial time complexity and can provide correcting guarantees under a
worst-case channel model. The results in the paper extend the corresponding
ones for finite fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4864</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4864</id><created>2013-11-19</created><authors><author><keyname>Horovitz</keyname><forenames>Michal</forenames></author></authors><title>Local Rank Modulation for Flash Memories</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1310.5515 by other authors
  without attribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local rank modulation scheme was suggested recently for representing
information in flash memories in order to overcome drawbacks of rank
modulation. For $s\leq t\leq n$ with $s|n$, $(s,t,n)$-LRM scheme is a local
rank modulation scheme where the $n$ cells are locally viewed through a sliding
window of size $t$ resulting in a sequence of small permutations which requires
less comparisons and less distinct values. The distance between two windows
equals to $s$. To get the simplest hardware implementation the case of sliding
window of size two was presented. Gray codes and constant weight Gray codes
were presented in order to exploit the full representational power of the
scheme. In this work, a tight upper-bound for cyclic constant weight Gray code
in $(1,2,n)$-LRM scheme where the weight equals to $2$ is given. Encoding,
decoding and enumeration of $(1,3,n)$-LRM scheme is studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4894</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4894</id><created>2013-11-02</created><authors><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Richard</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Sayed</keyname><forenames>Ali. H.</forenames></author></authors><title>Multitask Diffusion Adaptation over Networks</title><categories>cs.MA cs.SY</categories><comments>29 pages, 11 figures, submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive networks are suitable for decentralized inference tasks, e.g., to
monitor complex natural phenomena. Recent research works have intensively
studied distributed optimization problems in the case where the nodes have to
estimate a single optimum parameter vector collaboratively. However, there are
many important applications that are multitask-oriented in the sense that there
are multiple optimum parameter vectors to be inferred simultaneously, in a
collaborative manner, over the area covered by the network. In this paper, we
employ diffusion strategies to develop distributed algorithms that address
multitask problems by minimizing an appropriate mean-square error criterion
with $\ell_2$-regularization. The stability and convergence of the algorithm in
the mean and in the mean-square sense is analyzed. Simulations are conducted to
verify the theoretical findings, and to illustrate how the distributed strategy
can be used in several useful applications related to spectral sensing, target
localization, and hyperspectral data unmixing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4900</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4900</id><created>2013-11-16</created><authors><author><keyname>Ranjan</keyname><forenames>Sudhakar</forenames></author><author><keyname>Bhatia</keyname><forenames>Komal K.</forenames></author></authors><title>Query Interface Integrator For Domain Specific Hidden Web</title><categories>cs.IR cs.DB</categories><comments>8 Pages. International Journal of Computer Engineering and
  Applications, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web is title admittance today mainly relies on search engines. A large amount
of data is hidden in the databases behind the search interfaces referred to as
Hidden web, which needs to be indexed so in order to serve user query. In this
paper database and data mining techniques are used for query interface
integration. The query interface must resemble the look and feel of local
interface as much as possible despite being automatically generated without
human support.This technique keeps the related documents in the same domain so
that searching of documents becomes more efficient in terms of time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4904</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4904</id><created>2013-11-19</created><authors><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author></authors><title>On the Structure of Bispecial Sturmian Words</title><categories>cs.FL cs.DM</categories><comments>arXiv admin note: substantial text overlap with arXiv:1204.1672</comments><msc-class>68R15</msc-class><journal-ref>Journal of Computer and System Sciences, 80(4): 711-719 (2014)</journal-ref><doi>10.1016/j.jcss.2013.11.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A balanced word is one in which any two factors of the same length contain
the same number of each letter of the alphabet up to one. Finite binary
balanced words are called Sturmian words. A Sturmian word is bispecial if it
can be extended to the left and to the right with both letters remaining a
Sturmian word. There is a deep relation between bispecial Sturmian words and
Christoffel words, that are the digital approximations of Euclidean segments in
the plane. In 1997, J. Berstel and A. de Luca proved that \emph{palindromic}
bispecial Sturmian words are precisely the maximal internal factors of
\emph{primitive} Christoffel words. We extend this result by showing that
bispecial Sturmian words are precisely the maximal internal factors of
\emph{all} Christoffel words. Our characterization allows us to give an
enumerative formula for bispecial Sturmian words. We also investigate the
minimal forbidden words for the language of Sturmian words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4915</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4915</id><created>2013-11-19</created><authors><author><keyname>Hague</keyname><forenames>Matthew</forenames></author></authors><title>Senescent Ground Tree Rewrite Systems</title><categories>cs.FL cs.LO</categories><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ground Tree Rewrite Systems with State are known to have an undecidable
control state reachability problem. Taking inspiration from the recent
introduction of scope-bounded multi-stack pushdown systems, we define Senescent
Ground Tree Rewrite Systems. These are a restriction of ground tree rewrite
systems with state such that nodes of the tree may no longer be rewritten after
having witnessed an a priori fixed number of control state changes. As well as
generalising scope-bounded multi-stack pushdown systems, we show --- via
reductions to and from reset Petri-nets --- that these systems have an
Ackermann-complete control state reachability problem. However, reachability of
a regular set of trees remains undecidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4922</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4922</id><created>2013-11-19</created><authors><author><keyname>Avonds</keyname><forenames>Yurrit</forenames></author><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Van Huffel</keyname><forenames>Sabine</forenames></author></authors><title>Simultaneous Greedy Analysis Pursuit for Compressive Sensing of
  Multi-Channel ECG Signals</title><categories>cs.IT cs.DS math.IT stat.AP</categories><comments>8 pages, 2 figures, Internal Report 13-82, ESAT-STADIUS, University
  of Leuven, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses compressive sensing for multi-channel ECG. Compared to
the traditional sparse signal recovery approach which decomposes the signal
into the product of a dictionary and a sparse vector, the recently developed
cosparse approach exploits sparsity of the product of an analysis matrix and
the original signal. We apply the cosparse Greedy Analysis Pursuit (GAP)
algorithm for compressive sensing of ECG signals. Moreover, to reduce
processing time, classical signal-channel GAP is generalized to the
multi-channel GAP algorithm, which simultaneously reconstructs multiple signals
with similar support. Numerical experiments show that the proposed method
outperforms the classical sparse multi-channel greedy algorithms in terms of
accuracy and the single-channel cosparse approach in terms of processing speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4924</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4924</id><created>2013-11-19</created><updated>2015-07-02</updated><authors><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author></authors><title>Robust Compressed Sensing Under Matrix Uncertainties</title><categories>cs.IT cs.CV math.IT math.RT stat.AP stat.ML</categories><comments>17 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing (CS) shows that a signal having a sparse or compressible
representation can be recovered from a small set of linear measurements. In
classical CS theory, the sampling matrix and representation matrix are assumed
to be known exactly in advance. However, uncertainties exist due to sampling
distortion, finite grids of the parameter space of dictionary, etc. In this
paper, we take a generalized sparse signal model, which simultaneously
considers the sampling and representation matrix uncertainties. Based on the
new signal model, a new optimization model for robust sparse signal
reconstruction is proposed. This optimization model can be deduced with
stochastic robust approximation analysis. Both convex relaxation and greedy
algorithms are used to solve the optimization problem. For the convex
relaxation method, a sufficient condition for recovery by convex relaxation is
given; For the greedy algorithm, it is realized by the introduction of a
pre-processing of the sensing matrix and the measurements. In numerical
experiments, both simulated data and real-life ECG data based results show that
the proposed method has a better performance than the current methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4925</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4925</id><created>2013-11-19</created><authors><author><keyname>Tait</keyname><forenames>Michael</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author><author><keyname>Verstraete</keyname><forenames>Jacques</forenames></author></authors><title>Asymptotic Improvement of the Gilbert-Varshamov Bound on the Size of
  Permutation Codes</title><categories>math.CO cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given positive integers $n$ and $d$, let $M(n,d)$ denote the maximum size of
a permutation code of length $n$ and minimum Hamming distance $d$. The
Gilbert-Varshamov bound asserts that $M(n,d) \geq n!/V(n,d-1)$ where $V(n,d)$
is the volume of a Hamming sphere of radius $d$ in $\S_n$.
  Recently, Gao, Yang, and Ge showed that this bound can be improved by a
factor $\Omega(\log n)$, when $d$ is fixed and $n \to \infty$. Herein, we
consider the situation where the ratio $d/n$ is fixed and improve the
Gilbert-Varshamov bound by a factor that is \emph{linear in $n$}. That is, we
show that if $d/n &lt; 0.5$, then $$ M(n,d)\geq cn\,\frac{n!}{V(n,d-1)} $$ where
$c$ is a positive constant that depends only on $d/n$. To establish this
result, we follow the method of Jiang and Vardy. Namely, we recast the problem
of bounding $M(n,d)$ into a graph-theoretic framework and prove that the
resulting graph is locally sparse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4934</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4934</id><created>2013-11-19</created><updated>2014-01-18</updated><authors><author><keyname>Esmaeilsabzali</keyname><forenames>Shahram</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author><author><keyname>Wies</keyname><forenames>Thomas</forenames></author><author><keyname>Zufferey</keyname><forenames>Damien</forenames></author></authors><title>Dynamic Package Interfaces - Extended Version</title><categories>cs.SE</categories><comments>The only changes compared to v1 are improvements to the Abstract and
  Introduction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hallmark of object-oriented programming is the ability to perform
computation through a set of interacting objects. A common manifestation of
this style is the notion of a package, which groups a set of commonly used
classes together. A challenge in using a package is to ensure that a client
follows the implicit protocol of the package when calling its methods.
Violations of the protocol can cause a runtime error or latent invariant
violations. These protocols can extend across different, potentially
unboundedly many, objects, and are specified informally in the documentation.
As a result, ensuring that a client does not violate the protocol is hard.
  We introduce dynamic package interfaces (DPI), a formalism to explicitly
capture the protocol of a package. The DPI of a package is a finite set of
rules that together specify how any set of interacting objects of the package
can evolve through method calls and under what conditions an error can happen.
We have developed a dynamic tool that automatically computes an approximation
of the DPI of a package, given a set of abstraction predicates. A key property
of DPI is that the unbounded number of configurations of objects of a package
are summarized finitely in an abstract domain. This uses the observation that
many packages behave monotonically: the semantics of a method call over a
configuration does not essentially change if more objects are added to the
configuration. We have exploited monotonicity and have devised heuristics to
obtain succinct yet general DPIs. We have used our tool to compute DPIs for
several commonly used Java packages with complex protocols, such as JDBC,
HashSet, and ArrayList.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4941</identifier>
 <datestamp>2014-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4941</id><created>2013-11-19</created><updated>2014-08-15</updated><authors><author><keyname>Si</keyname><forenames>Hongbo</forenames></author><author><keyname>Koyluoglu</keyname><forenames>O. Ozan</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Polar Coding for Fading Channels: Binary and Exponential Channel Cases</title><categories>cs.IT math.IT</categories><comments>31 pages, 8 figures, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a polar coding scheme for fading channels, focusing
primarily on fading binary symmetric and additive exponential noise channels.
For fading binary symmetric channels, a hierarchical coding scheme is
presented, utilizing polar coding both over channel uses and over fading
blocks. The receiver uses its channel state information (CSI) to distinguish
states, thus constructing an overlay erasure channel over the underlying fading
channels. By using this scheme, the capacity of a fading binary symmetric
channel is achieved without CSI at the transmitter. Noting that a fading AWGN
channel with BPSK modulation and demodulation corresponds to a fading binary
symmetric channel, this result covers a fairly large set of practically
relevant channel settings.
  For fading additive exponential noise channels, expansion coding is used in
conjunction to polar codes. Expansion coding transforms the continuous-valued
channel to multiple (independent) discrete-valued ones. For each level after
expansion, the approach described previously for fading binary symmetric
channels is used. Both theoretical analysis and numerical results are
presented, showing that the proposed coding scheme approaches the capacity in
the high SNR regime. Overall, utilizing polar codes in this (hierarchical)
fashion enables coding without CSI at the transmitter, while approaching the
capacity with low complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4947</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4947</id><created>2013-11-19</created><updated>2015-03-05</updated><authors><author><keyname>Li</keyname><forenames>Jie</forenames></author><author><keyname>Tang</keyname><forenames>Xiaohu</forenames></author><author><keyname>Parampalli</keyname><forenames>Udaya</forenames></author></authors><title>A Framework of Constructions of Minimal Storage Regenerating Codes with
  the Optimal Access/Update Property</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Information Theory</comments><doi>10.1109/TIT.2015.2408600</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a generic framework for constructing systematic
minimum storage regenerating codes with two parity nodes based on the invariant
subspace technique. Codes constructed in our framework not only contain some
best known codes as special cases, but also include some new codes with key
properties such as the optimal access property and the optimal update property.
In particular, for a given storage capacity of an individual node, one of the
new codes has the largest number of systematic nodes and two of the new codes
have the largest number of systematic nodes with the optimal update property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4952</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4952</id><created>2013-11-19</created><authors><author><keyname>Das</keyname><forenames>Deepanwita</forenames></author><author><keyname>Mukhopadhyaya</keyname><forenames>Srabani</forenames></author></authors><title>Distributed Painting by a Swarm of Robots with Unlimited Sensing
  Capabilities and Its Simulation</title><categories>cs.DC cs.RO</categories><comments>15 pages, 10 figures</comments><journal-ref>International Journal of Information Processing, Volume 7, Issue
  3, page:1-15, 2013. ISSN: 0973-8215</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a distributed painting algorithm for painting a priori
known rectangular region by swarm of autonomous mobile robots. We assume that
the region is obstacle free and of rectangular in shape. The basic approach is
to divide the region into some cells, and to let each robot to paint one of
these cells. Assignment of different cells to the robots is done by ranking the
robots according to their relative positions. In this algorithm, the robots
follow the basic Wait-Observe-Compute-Move model together with the synchronous
timing model. This paper also presents a simulation of the proposed algorithm.
The simulation is performed using the Player/Stage Robotic Simulator on Ubuntu
10.04 (Lucid Lynx) platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4963</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4963</id><created>2013-11-20</created><updated>2013-11-21</updated><authors><author><keyname>Saini</keyname><forenames>Shubham</forenames></author><author><keyname>Kasliwal</keyname><forenames>Bhavesh</forenames></author><author><keyname>Bhatia</keyname><forenames>Shraey</forenames></author></authors><title>Comparative Study Of Image Edge Detection Algorithms</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since edge detection is in the forefront of image processing for object
detection, it is crucial to have a good understanding of edge detection
algorithms. The reason for this is that edges form the outline of an object. An
edge is the boundary between an object and the background, and indicates the
boundary between overlapping objects. This means that if the edges in an image
can be identified accurately, all of the objects can be located and basic
properties such as area, perimeter, and shape can be measured. Since computer
vision involves the identification and classification of objects in an image,
edge detection is an essential tool. We tested two edge detectors that use
different methods for detecting edges and compared their results under a
variety of situations to determine which detector was preferable under
different sets of conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4964</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4964</id><created>2013-11-20</created><authors><author><keyname>H</keyname><forenames>Su</forenames></author><author><keyname>Bi</keyname><forenames>Guoan</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author><author><keyname>Li</keyname><forenames>Shaoqian</forenames></author></authors><title>TDCS-based Cognitive Radio Networks with Multiuser Interference
  Avoidance</title><categories>cs.IT math.IT</categories><comments>to be appeared in IEEE Transaction on Communications, 2014</comments><journal-ref>IEEE Transactions on Communications 61(12): 4828-4835, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For overlay cognitive radio networks (CRNs), transform domain communication
system (TDCS) has been proposed to support multiuser communications through
spectrum bin nulling and frequency domain spreading. In TDCS-based CRNs, each
user is assigned a specific pseudorandom spreading sequence. However, the
existence of multiuser interference (MUI) is one of main concerns, due to the
non-zero cross-correlations between any pair of TDCS signals. In this paper, a
novel framework of TDCS-based CRNs with the joint design of sequences and
modulation schemes is presented to realize MUI avoidance. With the uncertainty
of spectrum sensing results in CRNs, we first introduce a unique sequence
design through two-dimensional time-frequency synthesis and obtain a class of
almost perfect sequences. That is, periodic auto-correlation and
cross-correlations are identically zero for most circular shifts. These
correlation properties are further exploited in conjunction with a
specially-designed cyclic code shift keying in order to achieve the advantage
of MUI avoidance. Numerical results demonstrate that the proposed TDCS-based
CRNs are considered as preferable candidates for decentralized networks against
the near-far problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.4987</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.4987</id><created>2013-11-20</created><authors><author><keyname>Qian</keyname><forenames>Chao</forenames></author><author><keyname>Yu</keyname><forenames>Yang</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Analyzing Evolutionary Optimization in Noisy Environments</title><categories>cs.AI cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many optimization tasks have to be handled in noisy environments, where we
cannot obtain the exact evaluation of a solution but only a noisy one. For
noisy optimization tasks, evolutionary algorithms (EAs), a kind of stochastic
metaheuristic search algorithm, have been widely and successfully applied.
Previous work mainly focuses on empirical studying and designing EAs for noisy
optimization, while, the theoretical counterpart has been little investigated.
In this paper, we investigate a largely ignored question, i.e., whether an
optimization problem will always become harder for EAs in a noisy environment.
We prove that the answer is negative, with respect to the measurement of the
expected running time. The result implies that, for optimization tasks that
have already been quite hard to solve, the noise may not have a negative
effect, and the easier a task the more negatively affected by the noise. On a
representative problem where the noise has a strong negative effect, we examine
two commonly employed mechanisms in EAs dealing with noise, the re-evaluation
and the threshold selection strategies. The analysis discloses that the two
strategies, however, both are not effective, i.e., they do not make the EA more
noise tolerant. We then find that a small modification of the threshold
selection allows it to be proven as an effective strategy for dealing with the
noise in the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5006</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5006</id><created>2013-11-20</created><authors><author><keyname>Simonetto</keyname><forenames>Andrea</forenames></author></authors><title>Indagini in Deep Inference</title><categories>cs.LO</categories><comments>in Italian</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Italian master's thesis in Computer Science. It is an overview of the
standard tecniques developed in the field of Proof Theory, ending with some
results in the new field of Deep Inference, plus an original contribution
trying to relate Deep Inference and Process Algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5013</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5013</id><created>2013-11-20</created><authors><author><keyname>Sridharan</keyname><forenames>Srivatsan</forenames></author><author><keyname>Malladi</keyname><forenames>Kausal</forenames></author><author><keyname>Muralitharan</keyname><forenames>Yamini</forenames></author></authors><title>Data Mining Model for the Data Retrieval from Central Server
  Configuration</title><categories>cs.IR cs.DB</categories><comments>9 Pages, 10 References, 6 Figures presented in ACITY 2013 Conference</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 5, No 5, October 2013</journal-ref><doi>10.5121/ijcsit.2013.5514</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A server, which is to keep track of heavy document traffic, is unable to
filter the documents that are most relevant and updated for continuous text
search queries. This paper focuses on handling continuous text extraction
sustaining high document traffic. The main objective is to retrieve recent
updated documents that are most relevant to the query by applying sliding
window technique. Our solution indexes the streamed documents in the main
memory with structure based on the principles of inverted file, and processes
document arrival and expiration events with incremental threshold-based method.
It also ensures elimination of duplicate document retrieval using unsupervised
duplicate detection. The documents are ranked based on user feedback and given
higher priority for retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5014</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5014</id><created>2013-11-20</created><updated>2014-10-13</updated><authors><author><keyname>Patras</keyname><forenames>P.</forenames></author><author><keyname>Feghhi</keyname><forenames>H.</forenames></author><author><keyname>Malone</keyname><forenames>D.</forenames></author><author><keyname>Leith</keyname><forenames>D. J.</forenames></author></authors><title>Policing 802.11 MAC Misbehaviours</title><categories>cs.NI</categories><comments>14 pages, 18 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  With the increasing availability of flexible wireless 802.11 devices, the
potential exists for users to selfishly manipulate their channel access
parameters and gain a performance advantage. Such practices can have a severe
negative impact on compliant stations. To enable access points to counteract
these selfish behaviours and preserve fairness in wireless networks, in this
paper we propose a policing mechanism that drives misbehaving users into
compliant operation without requiring any cooperation from clients. This
approach is demonstrably effective against a broad class of misbehaviours,
soundly-based, i.e. provably hard to circumvent and amenable to practical
implementation on existing commodity hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5018</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5018</id><created>2013-11-20</created><authors><author><keyname>Cioaca</keyname><forenames>Teodor</forenames></author><author><keyname>Caramizaru</keyname><forenames>Horea</forenames></author></authors><title>On the impact of explicit or semi-implicit integration methods over the
  stability of real-time numerical simulations</title><categories>cs.GR</categories><comments>Submitted to the ROMAI Journal of Applied Mathematics. Presented at
  the CAIM 2013 Conference on Applied and Industrial Mathematics</comments><msc-class>65D18</msc-class><acm-class>I.3.7; I.6.8; G.1.0</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Physics-based animation of soft or rigid bodies for real-time applications
often suffers from numerical instabilities. We analyse one of the most common
sources of unwanted behaviour: the numerical integration strategy. To assess
the impact of popular integration methods, we consider a scenario where soft
and hard constraints are added to a custom designed deformable linear object.
Since the goal for this class of simulation methods is to attain interactive
frame-rates, we present the drawbacks of using explicit integration methods
over inherently stable, implicit integrators. To help numerical solver
designers better understand the impact of an integrator on a certain simulated
world, we have conceived a method of benchmarking the efficiency of an
integrator with respect to its speed, stability and symplecticity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5022</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5022</id><created>2013-11-20</created><updated>2015-09-30</updated><authors><author><keyname>Ghosh</keyname><forenames>Shaona</forenames></author><author><keyname>Prugel-Bennett</keyname><forenames>Adam</forenames></author></authors><title>Extended Formulations for Online Linear Bandit Optimization</title><categories>cs.LG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On-line linear optimization on combinatorial action sets (d-dimensional
actions) with bandit feedback, is known to have complexity in the order of the
dimension of the problem. The exponential weighted strategy achieves the best
known regret bound that is of the order of $d^{2}\sqrt{n}$ (where $d$ is the
dimension of the problem, $n$ is the time horizon). However, such strategies
are provably suboptimal or computationally inefficient. The complexity is
attributed to the combinatorial structure of the action set and the dearth of
efficient exploration strategies of the set. Mirror descent with entropic
regularization function comes close to solving this problem by enforcing a
meticulous projection of weights with an inherent boundary condition. Entropic
regularization in mirror descent is the only known way of achieving a
logarithmic dependence on the dimension. Here, we argue otherwise and recover
the original intuition of exponential weighting by borrowing a technique from
discrete optimization and approximation algorithms called `extended
formulation'. Such formulations appeal to the underlying geometry of the set
with a guaranteed logarithmic dependence on the dimension underpinned by an
information theoretic entropic analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5027</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5027</id><created>2013-11-20</created><authors><author><keyname>Csirmaz</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>Ligeti</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>Tardos</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>Erd\H{o}s-Pyber theorem for hypergraphs and secret sharing</title><categories>math.CO cs.CR</categories><msc-class>05C99, 05C65, 05D40, 94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new, constructive proof with a small explicit constant is given to the
Erd\H{o}s-Pyber theorem which says that the edges of a graph on $n$ vertices
can be partitioned into complete bipartite subgraphs so that every vertex is
covered at most $O(n/\log n)$ times. The theorem is generalized to uniform
hypergraphs. Similar bounds with smaller constant value is provided for
fractional partitioning both for graphs and for uniform hypergraphs. We show
that these latter constants cannot be improved by more than a factor of 1.89
even for fractional covering by arbitrary complete multipartite subgraphs or
subhypergraphs. In the case every vertex of the graph is connected to at least
$n-m$ other vertices, we prove the existence of a fractional covering of the
edges by complete bipartite graphs such that every vertex is covered at most
$O(m/\log m)$ times, with only a slightly worse explicit constant. This result
also generalizes to uniform hypergraphs. Our results give new improved bounds
on the complexity of graph and uniform hypergraph based secret sharing schemes,
and show the limits of the method at the same time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5048</identifier>
 <datestamp>2014-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5048</id><created>2013-11-20</created><updated>2014-04-25</updated><authors><author><keyname>Matou&#x161;ek</keyname><forenames>Ji&#x159;&#xed;</forenames></author></authors><title>String graphs and separators</title><categories>math.CO cs.CG</categories><comments>Expository paper based on course notes</comments><msc-class>05C62</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  String graphs, that is, intersection graphs of curves in the plane, have been
studied since the 1960s. We provide an expository presentation of several
results, including very recent ones: some string graphs require an exponential
number of crossings in every string representation; exponential number is
always sufficient; string graphs have small separators; and the current best
bound on the crossing number of a graph in terms of the pair-crossing number.
For the existence of small separators, unwrapping the complete proof include
generally useful results on approximate flow-cut dualities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5058</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5058</id><created>2013-11-20</created><authors><author><keyname>Maneth</keyname><forenames>Sebastian</forenames><affiliation>University of Edinburgh</affiliation></author></authors><title>Proceedings Second International Workshop on Trends in Tree Automata and
  Tree Transducers</title><categories>cs.FL cs.LO cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 134, 2013</journal-ref><doi>10.4204/EPTCS.134</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers that were presented at the second
international workshop on Trends in Tree Automata and Transducers (TTATT 2013)
which took place on October 19th, 2013 in Hanoi/Vietnam. The workshop was
colocated with the verification conference ATVA. The first edition of the
workshop was colocated with RTA and took place in Nagoya/Japan. The interest of
the workshop lies at the intersection of programming languages, verification,
and database theory, which are areas to which tree automata and transducers are
applied recently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5064</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5064</id><created>2013-11-07</created><authors><author><keyname>Ellens</keyname><forenames>W.</forenames></author><author><keyname>Kooij</keyname><forenames>R. E.</forenames></author></authors><title>Graph measures and network robustness</title><categories>cs.DM cs.SI math.CO physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network robustness research aims at finding a measure to quantify network
robustness. Once such a measure has been established, we will be able to
compare networks, to improve existing networks and to design new networks that
are able to continue to perform well when it is subject to failures or attacks.
In this paper we survey a large amount of robustness measures on simple,
undirected and unweighted graphs, in order to offer a tool for network
administrators to evaluate and improve the robustness of their network. The
measures discussed in this paper are based on the concepts of connectivity
(including reliability polynomials), distance, betweenness and clustering. Some
other measures are notions from spectral graph theory, more precisely, they are
functions of the Laplacian eigenvalues. In addition to surveying these graph
measures, the paper also contains a discussion of their functionality as a
measure for topological network robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5068</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5068</id><created>2013-11-20</created><authors><author><keyname>Mart&#xed;nez-P&#xe9;rez</keyname><forenames>A.</forenames></author></authors><title>Gromov-Hausdorff stability of linkage-based hierarchical clustering
  methods</title><categories>cs.LG</categories><comments>25 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hierarchical clustering method is stable if small perturbations on the data
set produce small perturbations in the result. These perturbations are measured
using the Gromov-Hausdorff metric. We study the problem of stability on
linkage-based hierarchical clustering methods. We obtain that, under some basic
conditions, standard linkage-based methods are semi-stable. This means that
they are stable if the input data is close enough to an ultrametric space. We
prove that, apart from exotic examples, introducing any unchaining condition in
the algorithm always produces unstable methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5072</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5072</id><created>2013-11-20</created><authors><author><keyname>Zeng</keyname><forenames>An</forenames></author></authors><title>Inferring network topology via the propagation process</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>12 pages, 4 figures</comments><journal-ref>J. Stat. Mech. P11010 (2013)</journal-ref><doi>10.1088/1742-5468/2013/11/P11010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inferring the network topology from the dynamics is a fundamental problem
with wide applications in geology, biology and even counter-terrorism. Based on
the propagation process, we present a simple method to uncover the network
topology. The numerical simulation on artificial networks shows that our method
enjoys a high accuracy in inferring the network topology. We find the infection
rate in the propagation process significantly influences the accuracy, and each
network is corresponding to an optimal infection rate. Moreover, the method
generally works better in large networks. These finding are confirmed in both
real social and nonsocial networks. Finally, the method is extended to directed
networks and a similarity measure specific for directed networks is designed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5081</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5081</id><created>2013-11-20</created><authors><author><keyname>Shinn</keyname><forenames>Tong-Wook</forenames></author><author><keyname>Takaoka</keyname><forenames>Tadao</forenames></author></authors><title>Combining the Shortest Paths and the Bottleneck Paths Problems</title><categories>cs.DS</categories><comments>Will be presented at ACSC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We combine the well known Shortest Paths (SP) problem and the Bottleneck
Paths (BP) problem to introduce a new problem called the Shortest Paths for All
Flows (SP-AF) problem that has relevance in real life applications. We first
solve the Single Source Shortest Paths for All Flows (SSSP-AF) problem on
directed graphs with unit edge costs in $O(mn)$ worst case time bound. We then
present two algorithms to solve SSSP-AF on directed graphs with integer edge
costs bounded by $c$ in $O(m^2 + nc)$ and $O(m^2 + mn\log{(\frac{c}{m})})$ time
bounds. Finally we extend our algorithms for the SSSP-AF problem to solve the
All Pairs Shortest Paths for All Flows (APSP-AF) problem in $O(m^{2}n + nc)$
and $O(m^{2}n + mn^{2}\log{(\frac{c}{mn})})$ time bounds. All algorithms
presented in this paper are practical for implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5084</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5084</id><created>2013-11-19</created><authors><author><keyname>Sparavigna</keyname><forenames>Amelia Carolina</forenames></author></authors><title>Pearltrees as a tool for referencing and teaching</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pearltrees is a social service, using which people can organise Web contents,
photos and notes. These contents are the &quot;pearls&quot; on some trees. Besides the
social opportunities, the trees can be used as a tool to organise references
and for content curation. However, they can be helpful for teaching purposes
too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5090</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5090</id><created>2013-11-13</created><authors><author><keyname>Bhattacharyya</keyname><forenames>Arnab</forenames></author><author><keyname>Hatami</keyname><forenames>Pooya</forenames></author><author><keyname>Tulsiani</keyname><forenames>Madhur</forenames></author></authors><title>Algorithmic regularity for polynomials and applications</title><categories>cs.CC cs.DM cs.DS math.CO</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In analogy with the regularity lemma of Szemer\'edi, regularity lemmas for
polynomials shown by Green and Tao (Contrib. Discrete Math. 2009) and by
Kaufman and Lovett (FOCS 2008) modify a given collection of polynomials \calF =
{P_1,...,P_m} to a new collection \calF' so that the polynomials in \calF' are
&quot;pseudorandom&quot;. These lemmas have various applications, such as (special cases)
of Reed-Muller testing and worst-case to average-case reductions for
polynomials. However, the transformation from \calF to \calF' is not
algorithmic for either regularity lemma. We define new notions of regularity
for polynomials, which are analogous to the above, but which allow for an
efficient algorithm to compute the pseudorandom collection \calF'. In
particular, when the field is of high characteristic, in polynomial time, we
can refine \calF into \calF' where every nonzero linear combination of
polynomials in \calF' has desirably small Gowers norm.
  Using the algorithmic regularity lemmas, we show that if a polynomial P of
degree d is within (normalized) Hamming distance 1-1/|F| -\eps of some unknown
polynomial of degree k over a prime field F (for k &lt; d &lt; |F|), then there is an
efficient algorithm for finding a degree-k polynomial Q, which is within
distance 1-1/|F| -\eta of P, for some \eta depending on \eps. This can be
thought of as decoding the Reed-Muller code of order k beyond the list decoding
radius (finding one close codeword), when the received word P itself is a
polynomial of degree d (with k &lt; d &lt; |F|).
  We also obtain an algorithmic version of the worst-case to average-case
reductions by Kaufman and Lovett. They show that if a polynomial of degree d
can be weakly approximated by a polynomial of lower degree, then it can be
computed exactly using a collection of polynomials of degree at most d-1. We
give an efficient (randomized) algorithm to find this collection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5102</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5102</id><created>2013-11-20</created><authors><author><keyname>Dvir</keyname><forenames>Zeev</forenames></author><author><keyname>Saraf</keyname><forenames>Shubhangi</forenames></author><author><keyname>Wigderson</keyname><forenames>Avi</forenames></author></authors><title>Breaking the quadratic barrier for 3-LCCs over the Reals</title><categories>cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that 3-query linear locally correctable codes over the Reals of
dimension $d$ require block length $n&gt;d^{2+\lambda}$ for some fixed, positive
$\lambda &gt;0$. Geometrically, this means that if $n$ vectors in $R^d$ are such
that each vector is spanned by a linear number of disjoint triples of others,
then it must be that $n &gt; d^{2+\lambda}$. This improves the known quadratic
lower bounds (e.g. {KdW04, Wood07}). While a modest improvement, we expect that
the new techniques introduced in this work will be useful for further progress
on lower bounds of locally correctable and decodable codes with more than 2
queries, possibly over other fields as well.
  Our proof introduces several new ideas to existing lower bound techniques,
several of which work over every field. At a high level, our proof has two
parts, {\it clustering} and {\it random restriction}.
  The clustering step uses a powerful theorem of Barthe from convex geometry.
It can be used (after preprocessing our LCC to be {\it balanced}), to apply a
basis change (and rescaling) of the vectors, so that the resulting unit vectors
become {\it nearly isotropic}. This together with the fact that any LCC must
have many `correlated' pairs of points, lets us deduce that the vectors must
have a surprisingly strong geometric clustering, and hence also combinatorial
clustering with respect to the spanning triples.
  In the restriction step, we devise a new variant of the dimension reduction
technique used in previous lower bounds, which is able to take advantage of the
combinatorial clustering structure above. The analysis of our random projection
method reduces to a simple (weakly) random graph process, and works over any
field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5108</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5108</id><created>2013-11-20</created><authors><author><keyname>Soyez</keyname><forenames>Jean-Baptiste</forenames></author><author><keyname>Morvan</keyname><forenames>Gildas</forenames></author><author><keyname>Dupont</keyname><forenames>Daniel</forenames></author><author><keyname>Merzouki</keyname><forenames>Rochdi</forenames></author></authors><title>A Methodology to Engineer and Validate Dynamic Multi-level Multi-agent
  Based Simulations</title><categories>cs.MA</categories><comments>Presented at 3th International Workshop on Multi-Agent Based
  Simulation, Valencia, Spain, 5th June 2012</comments><journal-ref>Multi-Agent-Based Simulation XIII LNCS 7838 p 130-142, 2013</journal-ref><doi>10.1007/978-3-642-38859-0_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes a methodology to model and simulate complex systems,
based on IRM4MLS, a generic agent-based meta-model able to deal with
multi-level systems. This methodology permits the engineering of dynamic
multi-level agent-based models, to represent complex systems over several
scales and domains of interest. Its goal is to simulate a phenomenon using
dynamically the lightest representation to save computer resources without loss
of information. This methodology is based on two mechanisms: (1) the activation
or deactivation of agents representing different domain parts of the same
phenomenon and (2) the aggregation or disaggregation of agents representing the
same phenomenon at different scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5114</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5114</id><created>2013-11-20</created><authors><author><keyname>Baracca</keyname><forenames>Paolo</forenames></author><author><keyname>Boccardi</keyname><forenames>Federico</forenames></author><author><keyname>Benvenuto</keyname><forenames>Nevio</forenames></author></authors><title>A Dynamic Clustering and Resource Allocation Algorithm for Downlink CoMP
  Systems with Multiple Antenna UEs</title><categories>cs.IT math.IT</categories><comments>27 pages, 8 figures</comments><journal-ref>EURASIP Journal on Wireless Communications and Networking 2014,
  2014:125</journal-ref><doi>10.1186/1687-1499-2014-125</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coordinated multi-point (CoMP) schemes have been widely studied in the recent
years to tackle the inter-cell interference. In practice, latency and
throughput constraints on the backhaul allow the organization of only small
clusters of base stations (BSs) where joint processing (JP) can be implemented.
In this work we focus on downlink CoMP-JP with multiple antenna user equipments
(UEs) and propose a novel dynamic clustering algorithm. The additional degrees
of freedom at the UE can be used to suppress the residual interference by using
an interference rejection combiner (IRC) and allow a multistream transmission.
In our proposal we first define a set of candidate clusters depending on
long-term channel conditions. Then, in each time block, we develop a resource
allocation scheme by jointly optimizing transmitter and receiver where: a)
within each candidate cluster a weighted sum rate is estimated and then b) a
set of clusters is scheduled in order to maximize the system weighted sum rate.
Numerical results show that much higher rates are achieved when UEs are
equipped with multiple antennas. Moreover, as this performance improvement is
mainly due to the IRC, the gain achieved by the proposed approach with respect
to the non-cooperative scheme decreases by increasing the number of UE
antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5123</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5123</id><created>2013-11-20</created><authors><author><keyname>Ponieman</keyname><forenames>Nicolas</forenames><affiliation>Grandata Labs</affiliation></author><author><keyname>Salles</keyname><forenames>Alejo</forenames><affiliation>Physics Dept., Universidad de Buenos Aires</affiliation></author><author><keyname>Sarraute</keyname><forenames>Carlos</forenames><affiliation>Grandata Labs</affiliation></author></authors><title>Human Mobility and Predictability enriched by Social Phenomena
  Information (extended abstract)</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Third Conference on the Analysis of Mobile Phone Datasets (NetMob
  2013). May 1-3, 2013, MIT</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The information collected by mobile phone operators can be considered as the
most detailed information on human mobility across a large part of the
population. The study of the dynamics of human mobility using the collected
geolocations of users, and applying it to predict future users' locations, has
been an active field of research in recent years. In this work, we study the
extent to which social phenomena are reflected in mobile phone data, focusing
in particular in the cases of urban commute and major sports events. We
illustrate how these events are reflected in the data, and show how information
about the events can be used to improve predictability in a simple model for a
mobile phone user's location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5125</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5125</id><created>2013-11-20</created><updated>2015-06-08</updated><authors><author><keyname>Nock</keyname><forenames>Richard</forenames></author><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author><author><keyname>Amari</keyname><forenames>Shun-ichi</forenames></author></authors><title>On conformal divergences and their population minimizers</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Total Bregman divergences are a recent tweak of ordinary Bregman divergences
originally motivated by applications that required invariance by rotations.
They have displayed superior results compared to ordinary Bregman divergences
on several clustering, computer vision, medical imaging and machine learning
tasks. These preliminary results raise two important problems : First, report a
complete characterization of the left and right population minimizers for this
class of total Bregman divergences. Second, characterize a principled superset
of total and ordinary Bregman divergences with good clustering properties, from
which one could tailor the choice of a divergence to a particular application.
In this paper, we provide and study one such superset with interesting
geometric features, that we call conformal divergences, and focus on their left
and right population minimizers. Our results are obtained in a recently coined
$(u, v)$-geometric structure that is a generalization of the dually flat affine
connections in information geometry. We characterize both analytically and
geometrically the population minimizers. We prove that conformal divergences
(resp. total Bregman divergences) are essentially exhaustive for their left
(resp. right) population minimizers. We further report new results and extend
previous results on the robustness to outliers of the left and right population
minimizers, and discuss the role of the $(u, v)$-geometric structure in
clustering. Additional results are also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5126</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5126</id><created>2013-11-20</created><authors><author><keyname>Wolter</keyname><forenames>Jan</forenames></author></authors><title>Visual Representation of 3D Language Constructs Specified by Generic
  Depictions</title><categories>cs.PL cs.CG</categories><comments>16 pages, 11 figures, extended version of conference paper
  &quot;Specifying Generic Depictions of Language Constructs for 3D Visual
  Languages&quot;, which I presented at the VL/HCC 2013:
  http://dx.doi.org/10.1109/VLHCC.2013.6645258</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several modeling domains make use of three-dimensional representations, e.g.,
the &quot;ball-and-stick&quot; models of molecules. Our generator framework DEViL3D
supports the design and implementation of visual 3D languages for such modeling
purposes. The front-end of a language implementation generated by DEViL3D is a
dedicated 3D graphical structure editor, which is used to construct programs in
that domain. DEViL3D supports the language designer to describe the visual
appearance of the constructs of the particular language in terms of generic 3D
depictions. Their parameters specify where substructures are embedded, and how
the graphic adapts to space requirements of nested constructs. The 3D editor
used for such specifications is generated by DEViL3D, too. In this paper, we
briefly introduce the research field of 3D visual languages and report about
our generator framework and the role that generic depictions play in the
specification process for 3D languages. Our results show that our approach is
suitable for a wide range of 3D languages. We emphasize this suitability by
presenting requirements on the visual appearance for different languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5133</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5133</id><created>2013-11-20</created><authors><author><keyname>Sharma</keyname><forenames>Rupam Kumar</forenames></author><author><keyname>gogoi</keyname><forenames>Dhrubajyoti</forenames></author></authors><title>Android Based Emergency Alert Button</title><categories>cs.CY</categories><comments>3 pages, ISSN: 2278-3075, Volume-2, Issue-4, March 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Android is a java based operating system which runs on the Linux kernel. It
is lightweight and full featured. Android applications are developed using Java
and can be ported to new platform easily thereby fostering huge number of
useful mobile applications. This paper describes about a SOS application being
developed and its successful implementation with tested results. The
application has target users those sections of the people who surprisingly
falls into a situation where instant communication of their whereabouts becomes
indispensable to be informed to certain authorized persons at remote end
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5143</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5143</id><created>2013-11-19</created><authors><author><keyname>De Persis</keyname><forenames>Claudio</forenames></author><author><keyname>Tesi</keyname><forenames>Pietro</forenames></author></authors><title>Resilient Control under Denial-of-Service</title><categories>cs.SY</categories><comments>10 pages, abridged version submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate resilient control strategies for linear systems under
Denial-of-Service (DoS) attacks. By DoS attacks we mean interruptions of
communication on measurement (sensor-to-controller) and/or control
(controller-to-actuator) channels carried out by an intelligent adversary. We
characterize the duration of these interruptions under which stability of the
closed-loop system is preserved. The resilient nature of the control descends
from its ability to adapt the sampling rate to the occurrence of the DoS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5158</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5158</id><created>2013-11-20</created><updated>2014-01-20</updated><authors><author><keyname>Pia</keyname><forenames>Maria Grazia</forenames></author><author><keyname>Basaglia</keyname><forenames>Tullio</forenames></author><author><keyname>Bell</keyname><forenames>Zane W.</forenames></author><author><keyname>Dressendorfer</keyname><forenames>Paul. V.</forenames></author></authors><title>Scholarly literature and the press: scientific impact and social
  perception of physics computing</title><categories>physics.soc-ph cs.DL hep-ex physics.comp-ph</categories><comments>To be published in the Proceedings of CHEP 2013 (Computing in High
  Energy Physics)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The broad coverage of the search for the Higgs boson in the mainstream media
is a relative novelty for high energy physics (HEP) research, whose
achievements have traditionally been limited to scholarly literature. This
paper illustrates the results of a scientometric analysis of HEP computing in
scientific literature, institutional media and the press, and a comparative
overview of similar metrics concerning representative particle physics
measurements. The picture emerging from these scientometric data documents the
scientific impact and social perception of HEP computing. The results of this
analysis suggest that improved communication of the scientific and social role
of HEP computing would be beneficial to the high energy physics community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5184</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5184</id><created>2013-11-20</created><authors><author><keyname>Xia</keyname><forenames>Minghua</forenames></author><author><keyname>A&#xef;ssa</keyname><forenames>Sonia</forenames></author></authors><title>Spectrum-Sharing Multi-Hop Cooperative Relaying: Performance Analysis
  Using Extreme Value Theory</title><categories>cs.IT math.IT</categories><comments>12 pages, 6 figures</comments><doi>10.1109/TWC.2013.112013.130357</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In spectrum-sharing cognitive radio systems, the transmit power of secondary
users has to be very low due to the restrictions on the tolerable interference
power dictated by primary users. In order to extend the coverage area of
secondary transmission and reduce the corresponding interference region,
multi-hop amplify-and-forward (AF) relaying can be implemented for the
communication between secondary transmitters and receivers. This paper
addresses the fundamental limits of this promising technique. Specifically, the
effect of major system parameters on the performance of spectrum-sharing
multi-hop AF relaying is investigated. To this end, the optimal transmit power
allocation at each node along the multi-hop link is firstly addressed. Then,
the extreme value theory is exploited to study the limiting distribution
functions of the lower and upper bounds on the end-to-end signal-to-noise ratio
of the relaying path. Our results disclose that the diversity gain of the
multi-hop link is always unity, regardless of the number of relaying hops. On
the other hand, the coding gain is proportional to the water level of the
optimal water-filling power allocation at secondary transmitter and to the
large-scale path-loss ratio of the desired link to the interference link at
each hop, yet is inversely proportional to the accumulated noise, i.e. the
product of the number of relays and the noise variance, at the destination.
These important findings do not only shed light on the performance of the
secondary transmissions but also benefit system designers improving the
efficiency of future spectrum-sharing cooperative systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5186</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5186</id><created>2013-11-20</created><updated>2014-11-16</updated><authors><author><keyname>Bavarian</keyname><forenames>Mohammad</forenames></author><author><keyname>Shor</keyname><forenames>Peter W.</forenames></author></authors><title>Information Causality, Szemer\'{e}di-Trotter and Algebraic Variants of
  CHSH</title><categories>quant-ph cs.CC</categories><comments>Fixed some typos and added minor errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider the following family of two prover one-round games.
In the CHSH_q game, two parties are given x,y in F_q uniformly at random, and
each must produce an output a,b in F_q without communicating with the other.
The players' objective is to maximize the probability that their outputs
satisfy a+b=xy in F_q. This game was introduced by Buhrman and Massar (PRA
2005) as a large alphabet generalization of the celebrated CHSH game---which is
one of the most well-studied two-prover games in quantum information theory,
and which has a large number of applications to quantum cryptography and
quantum complexity.
  Our main contributions in this paper are the first asymptotic and explicit
bounds on the entangled and classical values of CHSH_q, and the realization of
a rather surprising connection between CHSH_q and geometric incidence theory.
On the way to these results, we also resolve a problem of Pawlowski and Winter
about pairwise independent Information Causality, which, beside being
interesting on its own, gives as an application a short proof of our upper
bound for the entangled value of CHSH_q.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5193</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5193</id><created>2013-11-20</created><authors><author><keyname>Gargano</keyname><forenames>Luisa</forenames></author><author><keyname>Hell</keyname><forenames>Pavol</forenames></author><author><keyname>Peters</keyname><forenames>Joseph G.</forenames></author><author><keyname>Vaccaro</keyname><forenames>Ugo</forenames></author></authors><title>Influence Diffusion in Social Networks under Time Window Constraints</title><categories>cs.DS cs.SI math.CO physics.soc-ph</categories><comments>An extended abstract of a preliminary version of this paper appeared
  in: Proceedings of 20th International Colloquium on Structural Information
  and Communication Complexity (Sirocco 2013), Lectures Notes in Computer
  Science vol. 8179, T. Moscibroda and A.A. Rescigno (Eds.), pp. 141-152, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a combinatorial model of the spread of influence in networks that
generalizes existing schemata recently proposed in the literature. In our
model, agents change behaviors/opinions on the basis of information collected
from their neighbors in a time interval of bounded size whereas agents are
assumed to have unbounded memory in previously studied scenarios. In our
mathematical framework, one is given a network $G=(V,E)$, an integer value
$t(v)$ for each node $v\in V$, and a time window size $\lambda$. The goal is to
determine a small set of nodes (target set) that influences the whole graph.
The spread of influence proceeds in rounds as follows: initially all nodes in
the target set are influenced; subsequently, in each round, any uninfluenced
node $v$ becomes influenced if the number of its neighbors that have been
influenced in the previous $\lambda$ rounds is greater than or equal to $t(v)$.
We prove that the problem of finding a minimum cardinality target set that
influences the whole network $G$ is hard to approximate within a
polylogarithmic factor. On the positive side, we design exact polynomial time
algorithms for paths, rings, trees, and complete graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5197</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5197</id><created>2013-11-20</created><updated>2015-08-22</updated><authors><author><keyname>Abu-Affash</keyname><forenames>A. Karim</forenames></author><author><keyname>Biniaz</keyname><forenames>Ahmad</forenames></author><author><keyname>Carmi</keyname><forenames>Paz</forenames></author><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>Approximating the Bottleneck Plane Perfect Matching of a Point Set</title><categories>cs.CG</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A bottleneck plane perfect matching of a set of $n$ points in $\mathbb{R}^2$
is defined to be a perfect non-crossing matching that minimizes the length of
the longest edge; the length of this longest edge is known as {\em bottleneck}.
The problem of computing a bottleneck plane perfect matching has been proved to
be NP-hard. We present an algorithm that computes a bottleneck plane matching
of size at least $\frac{n}{5}$ in $O(n \log^2 n)$-time. Then we extend our idea
toward an $O(n\log n)$-time approximation algorithm which computes a plane
matching of size at least $\frac{2n}{5}$ whose edges have length at most
$\sqrt{2}+\sqrt{3}$ times the bottleneck.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5202</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5202</id><created>2013-11-17</created><updated>2014-02-21</updated><authors><author><keyname>Cao</keyname><forenames>Yanchuang</forenames></author><author><keyname>Wen</keyname><forenames>Lihua</forenames></author><author><keyname>Xiao</keyname><forenames>Jinyou</forenames></author><author><keyname>Liu</keyname><forenames>Yijun</forenames></author></authors><title>A fast directional BEM for large-scale acoustic problems based on the
  Burton-Miller formulation</title><categories>cs.NA</categories><comments>22 pages</comments><journal-ref>Engineering Analysis with Boundary Elements. 50(1):47-58. 2015</journal-ref><doi>10.1016/j.enganabound.2014.07.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a highly efficient fast boundary element method (BEM) for
solving large-scale engineering acoustic problems in a broad frequency range is
developed and implemented. The acoustic problems are modeled by the
Burton-Miller boundary integral equation (BIE), thus the fictitious frequency
issue is completely avoided. The BIE is discretized by using the Nystr\&quot;om
method based on the curved quadratic elements, leading to simple numerical
implementation (no edge or corner problems) and high accuracy in the BEM
analysis. The linear systems are solved iteratively and accelerated by using a
newly developed kernel-independent wideband fast directional algorithm (FDA)
for fast summation of oscillatory kernels. In addition, the computational
efficiency of the FDA is further promoted by exploiting the low-rank features
of the translation matrices, resulting in two- to three-fold reduction in the
computational time of the multipole-to-local translations. The high accuracy
and nearly linear computational complexity of the present method are clearly
demonstrated by typical examples. An acoustic scattering problem with
dimensionless wave number $kD$ (where $k$ is the wave number and $D$ is the
typical length of the obstacle) up to 1000 and the degrees of freedom up to 4
million is successfully solved within 10 hours on a computer with one core and
the memory usage is 24 GB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5204</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5204</id><created>2013-11-18</created><authors><author><keyname>Skoumas</keyname><forenames>Georgios</forenames></author><author><keyname>Pfoser</keyname><forenames>Dieter</forenames></author><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author></authors><title>On Quantifying Qualitative Geospatial Data: A Probabilistic Approach</title><categories>cs.DB</categories><msc-class>62-07</msc-class><acm-class>H.2.8</acm-class><journal-ref>Proceeding GEOCROWD '13 Proceedings of the Second ACM SIGSPATIAL
  International Workshop on Crowdsourced and Volunteered Geographic Information
  Pages 71-78 ACM New York, NY, USA \c{opyright}2013 table of contents ISBN:
  978-1-4503-2528-8</journal-ref><doi>10.1145/2534732.2534742</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Living in the era of data deluge, we have witnessed a web content explosion,
largely due to the massive availability of User-Generated Content (UGC). In
this work, we specifically consider the problem of geospatial information
extraction and representation, where one can exploit diverse sources of
information (such as image and audio data, text data, etc), going beyond
traditional volunteered geographic information. Our ambition is to include
available narrative information in an effort to better explain geospatial
relationships: with spatial reasoning being a basic form of human cognition,
narratives expressing such experiences typically contain qualitative spatial
data, i.e., spatial objects and spatial relationships.
  To this end, we formulate a quantitative approach for the representation of
qualitative spatial relations extracted from UGC in the form of texts. The
proposed method quantifies such relations based on multiple text observations.
Such observations provide distance and orientation features which are utilized
by a greedy Expectation Maximization-based (EM) algorithm to infer a
probability distribution over predefined spatial relationships; the latter
represent the quantified relationships under user-defined probabilistic
assumptions. We evaluate the applicability and quality of the proposed approach
using real UGC data originating from an actual travel blog text corpus. To
verify the quality of the result, we generate grid-based maps visualizing the
spatial extent of the various relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5220</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5220</id><created>2013-11-20</created><updated>2014-04-01</updated><authors><author><keyname>Thunberg</keyname><forenames>Johan</forenames></author><author><keyname>Hu</keyname><forenames>Xiaoming</forenames></author></authors><title>Convergence Tools for Consensus in Multi-Agent Systems with Switching
  Topologies</title><categories>cs.SY math.OC</categories><comments>28 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two main theorems along the lines of Lyapunov's second method that
guarantee asymptotic state consensus in multi-agent systems of agents in R^m
with switching interconnection topologies. The two theorems complement each
other in the sense that the first one is formulated in terms of the states of
the agents in the multi-agent system, whereas the second one is formulated in
terms of the pairwise states for each pair of agents in the multi-agent system.
In the first theorem, under the assumption that the interconnection topology is
uniformly strongly connected and the agents are contained in a compact set, a
strong form of attractiveness of the consensus set is assured. In the second
theorem, under the weaker assumption that the interconnection topology is
uniformly quasi strongly connected, the consensus set is guaranteed to be
uniformly asymptotically stable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5242</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5242</id><created>2013-11-15</created><authors><author><keyname>Liu</keyname><forenames>Jianming</forenames></author><author><keyname>Grant</keyname><forenames>Steven L.</forenames></author></authors><title>An Improved Variable Step-size Affine Projection Sign Algorithm for Echo
  Cancellation</title><categories>cs.OH</categories><comments>5 pages, EUSIPCO-2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an improved variable step-size (VSS) algorithm for the
recently introduced affine projection sign algorithm (APSA) based on the
recovery of the near-end signal energy in the error signal. Simulation results
demonstrate that, compared to the previous VSS for APSA, the proposed approach
provides both more robustness to impulse interference and better tracking
ability of echo path change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5244</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5244</id><created>2013-11-20</created><authors><author><keyname>Holena</keyname><forenames>Martin</forenames></author><author><keyname>Chotard</keyname><forenames>Alexandre</forenames></author></authors><title>A Generalized Markov-Chain Modelling Approach to $(1,\lambda )$-ES
  Linear Optimization</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The manuscript generalizes several recent results of the 2nd author
concerning Markov-Chain Modelling of $(1,\lambda )$-ES Linear Optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5270</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5270</id><created>2013-11-20</created><updated>2013-12-05</updated><authors><author><keyname>Marciuska</keyname><forenames>Sarunas</forenames></author><author><keyname>Gencel</keyname><forenames>Cigdem</forenames></author><author><keyname>Abrahamsson</keyname><forenames>Pekka</forenames></author></authors><title>Automated Feature Identification in Web Applications</title><categories>cs.SE</categories><comments>This paper has been withdrawn</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Market-driven software intensive product development companies have been more
and more experiencing the problem of feature expansion over time. Product
managers face the challenge of identifying and locating the high value features
in an application and weeding out the ones of low value from the next releases.
Currently, there are few methods and tools that deal with feature
identification and they address the problem only partially. Therefore, there is
an urgent need of methods and tools that would enable systematic feature
reduction to resolve issues resulting from feature creep. This paper presents
an approach and an associated tool to automate feature identification for web
applications. For empirical validation, a multiple case study was conducted
using three well known web applications: Youtube, Google and BBC. The results
indicate that there is a good potential for automating feature identification
in web applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5290</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5290</id><created>2013-11-20</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Wesley Nunes</forenames></author><author><keyname>Machado</keyname><forenames>Bruno Brandoli</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir Martinez</forenames></author></authors><title>Texture descriptor combining fractal dimension and artificial crawlers</title><categories>physics.data-an cs.CV</categories><comments>12 pages 9 figures. Paper in press: Physica A: Statistical Mechanics
  and its Applications</comments><doi>10.1016/j.physa.2013.10.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Texture is an important visual attribute used to describe images. There are
many methods available for texture analysis. However, they do not capture the
details richness of the image surface. In this paper, we propose a new method
to describe textures using the artificial crawler model. This model assumes
that each agent can interact with the environment and each other. Since this
swarm system alone does not achieve a good discrimination, we developed a new
method to increase the discriminatory power of artificial crawlers, together
with the fractal dimension theory. Here, we estimated the fractal dimension by
the Bouligand-Minkowski method due to its precision in quantifying structural
properties of images. We validate our method on two texture datasets and the
experimental results reveal that our method leads to highly discriminative
textural features. The results indicate that our method can be used in
different texture applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5304</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5304</id><created>2013-11-20</created><updated>2014-05-12</updated><authors><author><keyname>Sodsong</keyname><forenames>Wasuwee</forenames></author><author><keyname>Hong</keyname><forenames>Jingun</forenames></author><author><keyname>Chung</keyname><forenames>Seongwook</forenames></author><author><keyname>Lim</keyname><forenames>Yeongkyu</forenames></author><author><keyname>Kim</keyname><forenames>Shin-Dug</forenames></author><author><keyname>Burgstaller</keyname><forenames>Bernd</forenames></author></authors><title>Dynamic Partitioning-based JPEG Decompression on Heterogeneous Multicore
  Architectures</title><categories>cs.DC</categories><comments>Abstract shortened to respect the arXiv limit of 1920 characters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the emergence of social networks and improvements in computational
photography, billions of JPEG images are shared and viewed on a daily basis.
Desktops, tablets and smartphones constitute the vast majority of hardware
platforms used for displaying JPEG images. Despite the fact that these
platforms are heterogeneous multicores, no approach exists yet that is capable
of joining forces of a system's CPU and GPU for JPEG decoding. In this paper we
introduce a novel JPEG decoding scheme for heterogeneous architectures
consisting of a CPU and an OpenCL-programmable GPU. We employ an offline
profiling step to determine the performance of a system's CPU and GPU with
respect to JPEG decoding. For a given JPEG image, our performance model uses
(1) the CPU and GPU performance characteristics, (2) the image entropy and (3)
the width and height of the image to balance the JPEG decoding workload on the
underlying hardware. Our run-time partitioning and scheduling scheme exploits
task, data and pipeline parallelism by scheduling the non-parallelizable
entropy decoding task on the CPU, whereas inverse cosine transformations
(IDCTs), color conversions and upsampling are conducted on both the CPU and the
GPU. Our kernels have been optimized for GPU memory hierarchies. We have
implemented the proposed method in the context of the libjpeg-turbo library,
which is an industrial-strength JPEG encoding and decoding engine.
Libjpeg-turbo's hand-optimized SIMD routines for ARM and x86 constitute a
competitive yardstick for the comparison to the proposed approach.
Retro-fitting our method with libjpeg-turbo provided insights on the
software-engineering aspects of re-engineering legacy code for heterogeneous
multicores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5317</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5317</id><created>2013-11-21</created><authors><author><keyname>Censor-Hillel</keyname><forenames>Keren</forenames></author><author><keyname>Ghaffari</keyname><forenames>Mohsen</forenames></author><author><keyname>Kuhn</keyname><forenames>Fabian</forenames></author></authors><title>Distributed Connectivity Decomposition</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present time-efficient distributed algorithms for decomposing graphs with
large edge or vertex connectivity into multiple spanning or dominating trees,
respectively. As their primary applications, these decompositions allow us to
achieve information flow with size close to the connectivity by parallelizing
it along the trees. More specifically, our distributed decomposition algorithms
are as follows:
  (I) A decomposition of each undirected graph with vertex-connectivity $k$
into (fractionally) vertex-disjoint weighted dominating trees with total weight
$\Omega(\frac{k}{\log n})$, in $\widetilde{O}(D+\sqrt{n})$ rounds.
  (II) A decomposition of each undirected graph with edge-connectivity
$\lambda$ into (fractionally) edge-disjoint weighted spanning trees with total
weight $\lceil\frac{\lambda-1}{2}\rceil(1-\varepsilon)$, in
$\widetilde{O}(D+\sqrt{n\lambda})$ rounds.
  We also show round complexity lower bounds of
$\tilde{\Omega}(D+\sqrt{\frac{n}{k}})$ and
$\tilde{\Omega}(D+\sqrt{\frac{n}{\lambda}})$ for the above two decompositions,
using techniques of [Das Sarma et al., STOC'11]. Moreover, our
vertex-connectivity decomposition extends to centralized algorithms and
improves the time complexity of [Censor-Hillel et al., SODA'14] from $O(n^3)$
to near-optimal $\tilde{O}(m)$.
  As corollaries, we also get distributed oblivious routing broadcast with
$O(1)$-competitive edge-congestion and $O(\log n)$-competitive
vertex-congestion. Furthermore, the vertex connectivity decomposition leads to
near-time-optimal $O(\log n)$-approximation of vertex connectivity: centralized
$\widetilde{O}(m)$ and distributed $\tilde{O}(D+\sqrt{n})$. The former moves
toward the 1974 conjecture of Aho, Hopcroft, and Ullman postulating an $O(m)$
centralized exact algorithm while the latter is the first distributed vertex
connectivity approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5322</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5322</id><created>2013-11-21</created><updated>2015-08-18</updated><authors><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author><author><keyname>Tsurumaru</keyname><forenames>Toyohiro</forenames></author></authors><title>More Efficient Privacy Amplification with Less Random Seeds via Dual
  Universal Hash Function</title><categories>quant-ph cs.CR cs.IT math.IT</categories><comments>33 pages, no figure, 1 table; v3: revised arguments with new hash
  functions proposed additionally, v4: minor corrections and clarifications,
  some new references added, v5: minor corrections, and enhanced arguments
  related with applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explicitly construct random hash functions for privacy amplification
(extractors) that require smaller random seed lengths than the previous
literature, and still allow efficient implementations with complexity $O(n\log
n)$ for input length $n$. The key idea is the concept of dual universal$_2$
hash function introduced recently. We also use a new method for constructing
extractors by concatenating $\delta$-almost dual universal$_2$ hash functions
with other extractors. Besides minimizing seed lengths, we also introduce
methods that allow one to use non-uniform random seeds for extractors. These
methods can be applied to a wide class of extractors, including dual
universal$_2$ hash function, as well as to conventional universal$_2$ hash
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5355</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5355</id><created>2013-11-21</created><authors><author><keyname>Voskoglou</keyname><forenames>Michael Gr.</forenames></author><author><keyname>Subbotin</keyname><forenames>Igor Ya.</forenames></author></authors><title>Dealing with the Fuzziness of Human Reasoning</title><categories>cs.AI</categories><comments>16 pages, 3 figures, 1 table. arXiv admin note: substantial text
  overlap with arXiv:1212.2614</comments><msc-class>03E72</msc-class><journal-ref>International Journal of Applications of Fuzzy Sets and Artifcial
  Intelligence (ISSN 2241-1240), Vol.3, 91-106, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reasoning, the most important human brain operation, is charactrized by a
degree fuzziness. In the present paper we construct a fuzzy model for the
reasoning process giving through the calculation of the possibilities of all
possible individuals' profiles a quantitative/qualitative view of their
behaviour during the above process and we use the centroid defuzzification
technique for measuring the reasoning skills. We also present a number of
classroom experiments illustrating our results in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5360</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5360</id><created>2013-11-21</created><authors><author><keyname>Smirani</keyname><forenames>Sinda</forenames></author><author><keyname>Kamoun</keyname><forenames>Mohamed</forenames></author><author><keyname>Sarkiss</keyname><forenames>Mireille</forenames></author><author><keyname>Zaidi</keyname><forenames>Abdellatif</forenames></author><author><keyname>Duhamel</keyname><forenames>Pierre</forenames></author></authors><title>Achievable Rate Regions for Two-Way Relay Channel using Nested Lattice
  Coding</title><categories>cs.IT math.IT</categories><comments>27 pages, 13 figures, Submitted to IEEE Transactions on Wireless
  Communications (October 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies Gaussian Two-Way Relay Channel where two communication
nodes exchange messages with each other via a relay. It is assumed that all
nodes operate in half duplex mode without any direct link between the
communication nodes. A compress-and-forward relaying strategy using nested
lattice codes is first proposed. Then, the proposed scheme is improved by
performing a layered coding : a common layer is decoded by both receivers and a
refinement layer is recovered only by the receiver which has the best channel
conditions. The achievable rates of the new scheme are characterized and are
shown to be higher than those provided by the decode-and-forward strategy in
some regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5362</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5362</id><created>2013-11-21</created><authors><author><keyname>Baccelli</keyname><forenames>Francois</forenames></author><author><keyname>Giovanidis</keyname><forenames>Anastasios</forenames></author></authors><title>Coverage by Pairwise Base Station Cooperation under Adaptive Geometric
  Policies</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, conference. In proceedings of 47th Asilomar
  Conference on Signals, Systems and Computers 2013 (invited paper)</comments><journal-ref>47th Asilomar Conference on Signals, Systems and Computers (2013)</journal-ref><doi>10.1109/ACSSC.2013.6810384</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a cooperation model where the positions of base stations follow a
Poisson point process distribution and where Voronoi cells define the planar
areas associated with them. For the service of each user, either one or two
base stations are involved. If two, these cooperate by exchange of user data
and reduced channel information (channel phase, second neighbour interference)
with conferencing over some backhaul link. The total user transmission power is
split between them and a common message is encoded, which is coherently
transmitted by the stations. The decision for a user to choose service with or
without cooperation is directed by a family of geometric policies. The
suggested policies further control the shape of coverage contours in favor of
cell-edge areas. Analytic expressions based on stochastic geometry are derived
for the coverage probability in the network. Their numerical evaluation shows
benefits from cooperation, which are enhanced when Dirty Paper Coding is
applied to eliminate the second neighbour interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5376</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5376</id><created>2013-11-21</created><updated>2014-07-14</updated><authors><author><keyname>Tervo</keyname><forenames>Valtteri</forenames></author><author><keyname>T&#xf6;lli</keyname><forenames>Antti</forenames></author><author><keyname>Karjalainen</keyname><forenames>Juha</forenames></author><author><keyname>Matsumoto</keyname><forenames>Tad</forenames></author></authors><title>PAPR Constrained Power Allocation for Iterative Frequency Domain
  Multiuser SIMO Detector</title><categories>cs.IT math.IT</categories><comments>Presented in IEEE International Conference on Communications (ICC)
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peak to average power ratio (PAPR) constrained power allocation in single
carrier multiuser (MU) single-input multiple-output (SIMO) systems with
iterative frequency domain (FD) soft cancelation (SC) minimum mean squared
error (MMSE) equalization is considered in this paper. To obtain full benefit
of the iterative receiver, its convergence properties need to be taken into
account also at the transmitter side. In this paper, we extend the existing
results on the area of convergence constrained power allocation (CCPA) to
consider the instantaneous PAPR at the transmit antenna of each user. In other
words, we will introduce a constraint that PAPR cannot exceed a predetermined
threshold. By adding the aforementioned constraint into the CCPA optimization
framework, the power efficiency of a power amplifier (PA) can be significantly
enhanced by enabling it to operate on its linear operation range. Hence, PAPR
constraint is especially beneficial for power limited cell-edge users. In this
paper, we will derive the instantaneous PAPR constraint as a function of
transmit power allocation. Furthermore, successive convex approximation is
derived for the PAPR constrained problem. Numerical results show that the
proposed method can achieve the objectives described above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5382</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5382</id><created>2013-11-21</created><authors><author><keyname>Mahmood</keyname><forenames>A.</forenames></author><author><keyname>Fakhar</keyname><forenames>H.</forenames></author><author><keyname>Ahmed</keyname><forenames>S. H.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author></authors><title>Analysis of Wireless Power Transmission</title><categories>cs.NI</categories><comments>The International Industrial and Information Systems Conference
  (IIISC), Chiang Mai, 2014, Thailand</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of various wireless technologies have revolutionized the
communication infrastructure and consequently changed the entire world into a
global village. Use of wireless technology has also been made for transmission
of electric power wirelessly. It increases the portability of power systems and
integrates the communication technologies and electric power to the same
platform. This paper presents a comprehensive review and detailed analysis of
various techniques used for wireless power transmission. Feasibility,
implementations, operations, results and comparison among different methods
have also been covered in order to identify the favorable and economical method
for low power and small distance applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5385</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5385</id><created>2013-11-21</created><authors><author><keyname>Mahmood</keyname><forenames>A.</forenames></author><author><keyname>Fakhar</keyname><forenames>H.</forenames></author><author><keyname>Ahmed</keyname><forenames>S. H.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author></authors><title>Home Energy Management in Smart Grid</title><categories>cs.NI</categories><comments>The International Industrial and Information Systems Conference
  (IIISC), Chiang Mai, 2014, Thailand</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A significant amount of research has been conducted in order to make home
appliances more efficient in terms of energy usage. Various techniques have
been designed and implemented in order to control the power demand and supply.
This paper encompasses reviews of different research works on a wide range of
energy management techniques for smart homes aimed at reducing energy
consumption and minimizing energy wastage. The idea of smart home is elaborated
followed by a review of existing energy management methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5401</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5401</id><created>2013-11-21</created><updated>2014-04-10</updated><authors><author><keyname>Turenne</keyname><forenames>Nicolas</forenames></author></authors><title>Clustering and Relational Ambiguity: from Text Data to Natural Data</title><categories>cs.CL cs.IR</categories><journal-ref>Journal of Data Mining &amp; Digital Humanities, 2014 (June 24, 2014)
  jdmdh:13</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Text data is often seen as &quot;take-away&quot; materials with little noise and easy
to process information. Main questions are how to get data and transform them
into a good document format. But data can be sensitive to noise oftenly called
ambiguities. Ambiguities are aware from a long time, mainly because polysemy is
obvious in language and context is required to remove uncertainty. I claim in
this paper that syntactic context is not suffisant to improve interpretation.
In this paper I try to explain that firstly noise can come from natural data
themselves, even involving high technology, secondly texts, seen as verified
but meaningless, can spoil content of a corpus; it may lead to contradictions
and background noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5414</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5414</id><created>2013-11-21</created><updated>2014-02-08</updated><authors><author><keyname>Kawamura</keyname><forenames>Akitoshi</forenames><affiliation>University of Tokyo</affiliation></author><author><keyname>Ota</keyname><forenames>Hiroyuki</forenames><affiliation>University of Tokyo</affiliation></author><author><keyname>R&#xf6;snick</keyname><forenames>Carsten</forenames><affiliation>Technische Universit&#xe4;t Darmstadt</affiliation></author><author><keyname>Ziegler</keyname><forenames>Martin</forenames><affiliation>Technische Universit&#xe4;t Darmstadt</affiliation></author></authors><title>Computational Complexity of Smooth Differential Equations</title><categories>cs.CC cs.NA math.NA</categories><comments>15 pages, 3 figures</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (February
  11, 2014) lmcs:960</journal-ref><doi>10.2168/LMCS-10(1:6)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computational complexity of the solutions $h$ to the ordinary
differential equation $h(0)=0$, $h'(t) = g(t, h(t))$ under various assumptions
on the function $g$ has been investigated. Kawamura showed in 2010 that the
solution $h$ can be PSPACE-hard even if $g$ is assumed to be Lipschitz
continuous and polynomial-time computable. We place further requirements on the
smoothness of $g$ and obtain the following results: the solution $h$ can still
be PSPACE-hard if $g$ is assumed to be of class $C^1$; for each $k\ge2$, the
solution $h$ can be hard for the counting hierarchy even if $g$ is of class
$C^k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5417</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5417</id><created>2013-11-21</created><updated>2014-07-22</updated><authors><author><keyname>Sun</keyname><forenames>Jiajun</forenames></author></authors><title>General Privacy-Preserving Verifiable Incentive Mechanism for
  Crowdsourcing Markets</title><categories>cs.GT cs.CR</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1 and Figure 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In crowdsourcing markets, there are two different type jobs, i.e. homogeneous
jobs and heterogeneous jobs, which need to be allocated to workers. Incentive
mechanisms are essential to attract extensive user participating for achieving
good service quality, especially under a given budget constraint condition. To
this end, recently, Singer et al. propose a novel class of auction mechanisms
for determining near-optimal prices of tasks for crowdsourcing markets
constrained by the given budget. Their mechanisms are very useful to motivate
extensive user to truthfully participate in crowdsourcing markets. Although
they are so important, there still exist many security and privacy challenges
in real-life environments. In this paper, we present a general
privacy-preserving verifiable incentive mechanism for crowdsourcing markets
with the budget constraint, not only to exploit how to protect the bids and
assignments' privacy, and the chosen winners' privacy in crowdsourcing markets
with homogeneous jobs and heterogeneous jobs and identity privacy from users,
but also to make the verifiable payment between the platform and users for
crowdsourcing applications. Results show that our general privacy-preserving
verifiable incentive mechanisms achieve the same results as the generic one
without privacy preservation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5422</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5422</id><created>2013-11-20</created><updated>2013-11-21</updated><authors><author><keyname>Rao</keyname><forenames>Nikhil</forenames></author><author><keyname>Cox</keyname><forenames>Christopher</forenames></author><author><keyname>Nowak</keyname><forenames>Robert</forenames></author><author><keyname>Rogers</keyname><forenames>Timothy</forenames></author></authors><title>Sparse Overlapping Sets Lasso for Multitask Learning and its Application
  to fMRI Analysis</title><categories>cs.LG stat.ML</categories><comments>To appear in Advances in Neural Information Processing Systems, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multitask learning can be effective when features useful in one task are also
useful for other tasks, and the group lasso is a standard method for selecting
a common subset of features. In this paper, we are interested in a less
restrictive form of multitask learning, wherein (1) the available features can
be organized into subsets according to a notion of similarity and (2) features
useful in one task are similar, but not necessarily identical, to the features
best suited for other tasks. The main contribution of this paper is a new
procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization
that automatically selects similar features for related learning tasks. Error
bounds are derived for SOSlasso and its consistency is established for squared
error loss. In particular, SOSlasso is motivated by multi- subject fMRI studies
in which functional activity is classified using brain voxels as features.
Experiments with real and synthetic data demonstrate the advantages of SOSlasso
compared to the lasso and group lasso.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5426</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5426</id><created>2013-11-21</created><authors><author><keyname>Vickers</keyname><forenames>Paul</forenames></author><author><keyname>Hogg</keyname><forenames>Bennett</forenames></author></authors><title>Sonification Abstraite/Sonification Concr\`ete: An 'Aesthetic
  Perspective Space' for Classifying Auditory Displays in the Ars Musica Domain</title><categories>cs.HC</categories><comments>in ICAD 2006, The 12th Meeting of the International Conference on
  Auditory Display (T. Stockman, L. V. Nickerson, C. Frauenberger, A. D. N.
  Edwards, and D. Brock, eds.), (London, UK), pp. 210-216, 20-23 June 2006</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses {\ae}sthetic issues of sonifications and the
relationships between sonification (ars informatica) and music &amp; sound art (ars
musica). It is posited that many sonifications have suffered from poor internal
ecological validity which makes listening more difficult, thereby resulting in
poorer data extraction and inference on the part of the listener. Lessons are
drawn from the electroacoustic music and musique concr\`ete communities as it
is argued that it is not instructive to distinguish between sonifications and
music/sound art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5427</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5427</id><created>2013-11-19</created><updated>2013-11-22</updated><authors><author><keyname>Febres</keyname><forenames>Gerardo</forenames></author><author><keyname>Jaffe</keyname><forenames>Klaus</forenames></author><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Complexity measurement of natural and artificial languages</title><categories>cs.CL cs.IT math.IT nlin.AO physics.soc-ph</categories><comments>29 pages, 11 figures, 3 tables, 2 appendixes</comments><journal-ref>Complexity 20 6 429- (2015)</journal-ref><doi>10.1002/cplx.21529</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compared entropy for texts written in natural languages (English, Spanish)
and artificial languages (computer software) based on a simple expression for
the entropy as a function of message length and specific word diversity. Code
text written in artificial languages showed higher entropy than text of similar
length expressed in natural languages. Spanish texts exhibit more symbolic
diversity than English ones. Results showed that algorithms based on complexity
measures differentiate artificial from natural languages, and that text
analysis based on complexity measures allows the unveiling of important aspects
of their nature. We propose specific expressions to examine entropy related
aspects of tests and estimate the values of entropy, emergence,
self-organization and complexity based on specific diversity and message
length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5434</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5434</id><created>2013-11-21</created><authors><author><keyname>Vickers</keyname><forenames>Paul</forenames></author><author><keyname>Alty</keyname><forenames>James L</forenames></author></authors><title>The Well-tempered Compiler? The Aesthetics of Program Auralization</title><categories>cs.HC</categories><comments>in Aesthetic Computing (P. A. Fishwick, ed.), ch. 17, pp. 335-354,
  Boston, MA: MIT Press, 2006</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this chapter we are concerned with external auditory representations of
programs, also known as program auralization. As program auralization systems
tend to use musical representations they are necessarily affected by artistic
and aesthetic considerations. Therefore, it is instructive to explore program
auralization in the light of aesthetic computing principles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5447</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5447</id><created>2013-11-21</created><authors><author><keyname>Grushko</keyname><forenames>Carmi</forenames></author></authors><title>Reducing Linear Programs into Min-max Problems</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to reduce a general, strictly-feasible LP problem, into a min-max
problem, which can be solved by the algorithm from the third section of my
thesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5467</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5467</id><created>2013-11-21</created><authors><author><keyname>Viola</keyname><forenames>Emanuele</forenames></author></authors><title>Challenges in computational lower bounds</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We draw two incomplete, biased maps of challenges in computational complexity
lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5481</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5481</id><created>2013-11-21</created><authors><author><keyname>Mastrolilli</keyname><forenames>Monaldo</forenames></author><author><keyname>Stamoulis</keyname><forenames>Georgios</forenames></author></authors><title>Bi-Criteria and Approximation Algorithms for Restricted Matchings</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study approximation algorithms for the \textit{Bounded Color
Matching} problem (a.k.a. Restricted Matching problem) which is defined as
follows: given a graph in which each edge $e$ has a color $c_e$ and a profit
$p_e \in \mathbb{Q}^+$, we want to compute a maximum (cardinality or profit)
matching in which no more than $w_j \in \mathbb{Z}^+$ edges of color $c_j$ are
present. This kind of problems, beside the theoretical interest on its own
right, emerges in multi-fiber optical networking systems, where we interpret
each unique wavelength that can travel through the fiber as a color class and
we would like to establish communication between pairs of systems. We study
approximation and bi-criteria algorithms for this problem which are based on
linear programming techniques and, in particular, on polyhedral
characterizations of the natural linear formulation of the problem. In our
setting, we allow violations of the bounds $w_j$ and we model our problem as a
bi-criteria problem: we have two objectives to optimize namely (a) to maximize
the profit (maximum matching) while (b) minimizing the violation of the color
bounds. We prove how we can &quot;beat&quot; the integrality gap of the natural linear
programming formulation of the problem by allowing only a slight violation of
the color bounds. In particular, our main result is \textit{constant}
approximation bounds for both criteria of the corresponding bi-criteria
optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5502</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5502</id><created>2013-11-21</created><authors><author><keyname>Sarraute</keyname><forenames>Carlos</forenames><affiliation>Grandata Labs</affiliation></author><author><keyname>Calderon</keyname><forenames>Gervasio</forenames><affiliation>Grandata Labs</affiliation></author></authors><title>Evolution of Communities with Focus on Stability (extended abstract)</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Third Conference on the Analysis of Mobile Phone Datasets (NetMob
  2013). May 1-3, 2013, MIT</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The detection of communities is an important tool used to analyze the social
graph of mobile phone users. Within each community, customers are susceptible
of attracting new ones, retaining old ones and/or accepting new products or
services through the leverage of mutual influences. The communities of users
are smaller units, easier to grasp, and allow for example the computation of
role analysis -- based on the centrality of an actor within his community.
  The problem of finding communities in static graphs has been widely studied.
However, from the point of view of a telecom analyst, to be really useful, the
detected communities must evolve as the social graph of communications changes
over time -- for example, in order to perform marketing actions on communities
and track the results of those actions over time. Additionally the behaviors of
communities of users over time can be used to predict future activity that
interests the telecom operators, such as subscriber churn or handset adoption.
Similary group evolution can provide insights for designing strategies, such as
the early warning of group churn.
  Stability is a crucial issue: the analysis performed on a given community
will be lost, if the analyst cannot keep track of this community in the
following time steps. This is the particular use case that we tackle in this
paper: tracking the evolution of communities in dynamic scenarios with focus on
stability.
  We propose two modifications to a widely used static community detection
algorithm. We then describe experiments to study the stability and quality of
the resulting partitions on real-world social networks, represented by monthly
call graphs for millions of subscribers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5527</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5527</id><created>2013-11-21</created><updated>2014-06-10</updated><authors><author><keyname>Naderializadeh</keyname><forenames>Navid</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>ITLinQ: A New Approach for Spectrum Sharing in Device-to-Device
  Communication Systems</title><categories>cs.IT math.IT</categories><comments>Final version to appear in IEEE JSAC Special Issue on 5G Wireless
  Communication Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of spectrum sharing in device-to-device communication
systems. Inspired by the recent optimality condition for treating interference
as noise, we define a new concept of &quot;information-theoretic independent sets&quot;
(ITIS), which indicates the sets of links for which simultaneous communication
and treating the interference from each other as noise is
information-theoretically optimal (to within a constant gap). Based on this
concept, we develop a new spectrum sharing mechanism, called
&quot;information-theoretic link scheduling&quot; (ITLinQ), which at each time schedules
those links that form an ITIS. We first provide a performance guarantee for
ITLinQ by characterizing the fraction of the capacity region that it can
achieve in a network with sources and destinations located randomly within a
fixed area. Furthermore, we demonstrate how ITLinQ can be implemented in a
distributed manner, using an initial 2-phase signaling mechanism which provides
the required channel state information at all the links. Through numerical
analysis, we show that distributed ITLinQ can outperform similar
state-of-the-art spectrum sharing mechanisms, such as FlashLinQ, by more than a
100% of sum-rate gain, while keeping the complexity at the same level. Finally,
we discuss a variation of the distributed ITLinQ scheme which can also
guarantee fairness among the links in the network and numerically evaluate its
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5547</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5547</id><created>2013-11-17</created><updated>2013-11-22</updated><authors><author><keyname>Jiang</keyname><forenames>J.</forenames></author><author><keyname>Wang</keyname><forenames>R.</forenames></author><author><keyname>Pezeril</keyname><forenames>M.</forenames></author><author><keyname>Wang</keyname><forenames>Q. A.</forenames></author></authors><title>Long division unites - long union divides, a model for social network
  evolution</title><categories>physics.soc-ph cs.SI</categories><comments>Short report, 6 pages, 4 figures</comments><journal-ref>Chin. Phys. Lett., 30(2013)038901</journal-ref><doi>10.1088/0256-307X/30/3/038901</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A remarkable phenomenon in the time evolution of many networks such as
cultural, political, national and economic systems, is the recurrent transition
between the states of union and division of nodes. In this work, we propose a
phenomenological modeling, inspired by the maxim &quot;long union divides and long
division unites&quot;, in order to investigate the evolutionary characters of these
networks composed of the entities whose behaviors are dominated by these two
events. The nodes are endowed with quantities such as identity, ingredient,
richness (power), openness (connections), age, distance, interaction etc. which
determine collectively the evolution in a probabilistic way. Depending on a
tunable parameter, the time evolution of this model is mainly an alternative
domination of union or division state, with a possible state of final union
dominated by one single node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5550</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5550</id><created>2013-11-21</created><updated>2013-11-22</updated><authors><author><keyname>Simi</keyname><forenames>Manuele</forenames></author><author><keyname>Campagne</keyname><forenames>Fabien</forenames></author></authors><title>Composable Languages for Bioinformatics: The NYoSh experiment</title><categories>cs.SE cs.CE q-bio.QM</categories><comments>10 pages, 9 figures. Supplementary material: 4 generated source
  listings. Comment on this manuscript on Twitter or Google+ with this handle:
  #NYoSh</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Language workbenches are software engineering tools that help domain experts
develop solutions to various classes of problems. Some of these tools focus on
non-technical users and provide languages to help organize knowledge while
other workbenches provide means to create new programming languages. A key
advantage of language workbenches is that they support the composition of
independently developed languages. This capability is useful when developing
programs that can benefit from different levels of abstraction. We reasoned
that language workbenches could be useful to develop bioinformatics software
solutions. In order to evaluate the potential of language workbenches in
bioinformatics, we tested a prominent workbench by developing an alternative to
shell scripting. While shell scripts are widely used in bioinformatics to
automate computational analysis, existing scripting languages do not provide
many of the features present in modern programming languages. We report on our
design of NYoSh (Not Your ordinary Shell). NYoSh was implemented as a
collection of languages that can be composed to write programs as expressive
and concise as shell scripts. NYoSh offers a concrete illustration of the
advantages that language workbench technologies can bring to bioinformatics.
For instance, NYoSh scripts can be edited with an environment-aware editor that
provides semantic error detection and can be compiled interactively with an
automatic build and deployment system. In contrast to shell scripts, NYoSh
scripts can be written in a modern development environment, supporting context
dependent intentions and can be extended seamlessly with new abstractions and
language constructs. We demonstrate language extension and composition by
presenting a tight integration of NYoSh scripts with the GobyWeb system. The
NYoSh Workbench prototype is distributed at http://nyosh.campagnelab.org
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5552</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5552</id><created>2013-11-21</created><updated>2014-09-08</updated><authors><author><keyname>Smith</keyname><forenames>Steven T.</forenames></author><author><keyname>Kao</keyname><forenames>Edward K.</forenames></author><author><keyname>Senne</keyname><forenames>Kenneth D.</forenames></author><author><keyname>Bernstein</keyname><forenames>Garrett</forenames></author><author><keyname>Philips</keyname><forenames>Scott</forenames></author></authors><title>Bayesian Discovery of Threat Networks</title><categories>cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH</categories><comments>IEEE Trans. Signal Process., major revision of
  arxiv.org/abs/1303.5613. arXiv admin note: substantial text overlap with
  arXiv:1303.5613</comments><journal-ref>IEEE Trans. Signal Process., vol. 62, no. 20, pp. 5324-5338,
  October 2014</journal-ref><doi>10.1109/TSP.2014.2336613</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel unified Bayesian framework for network detection is developed, under
which a detection algorithm is derived based on random walks on graphs. The
algorithm detects threat networks using partial observations of their activity,
and is proved to be optimum in the Neyman-Pearson sense. The algorithm is
defined by a graph, at least one observation, and a diffusion model for threat.
A link to well-known spectral detection methods is provided, and the
equivalence of the random walk and harmonic solutions to the Bayesian
formulation is proven. A general diffusion model is introduced that utilizes
spatio-temporal relationships between vertices, and is used for a specific
space-time formulation that leads to significant performance improvements on
coordinated covert networks. This performance is demonstrated using a new
hybrid mixed-membership blockmodel introduced to simulate random covert
networks with realistic properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5567</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5567</id><created>2013-11-21</created><authors><author><keyname>Nishida</keyname><forenames>Naoki</forenames><affiliation>Nagoya University</affiliation></author><author><keyname>Sakai</keyname><forenames>Masahiko</forenames><affiliation>Nagoya University</affiliation></author><author><keyname>Nakano</keyname><forenames>Yasuhiro</forenames><affiliation>Nagoya University</affiliation></author></authors><title>On Constructing Constrained Tree Automata Recognizing Ground Instances
  of Constrained Terms</title><categories>cs.FL cs.LO</categories><comments>In Proceedings TTATT 2013, arXiv:1311.5058</comments><proxy>EPTCS</proxy><acm-class>F.1.1 Automata</acm-class><journal-ref>EPTCS 134, 2013, pp. 1-10</journal-ref><doi>10.4204/EPTCS.134.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An inductive theorem proving method for constrained term rewriting systems,
which is based on rewriting induction, needs a decision procedure for
reduction-completeness of constrained terms. In addition, the sufficient
complete property of constrained term rewriting systems enables us to relax the
side conditions of some inference rules in the proving method. These two
properties can be reduced to intersection emptiness problems related to sets of
ground instances for constrained terms. This paper proposes a method to
construct deterministic, complete, and constraint-complete constrained tree
automata recognizing ground instances of constrained terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5568</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5568</id><created>2013-11-21</created><authors><author><keyname>Hanneforth</keyname><forenames>Thomas</forenames><affiliation>Universit&#xe4;t Potsdam</affiliation></author><author><keyname>Maletti</keyname><forenames>Andreas</forenames><affiliation>Universit&#xe4;t Stuttgart</affiliation></author><author><keyname>Quernheim</keyname><forenames>Daniel</forenames><affiliation>Universit&#xe4;t Stuttgart</affiliation></author></authors><title>Random Generation of Nondeterministic Finite-State Tree Automata</title><categories>cs.FL</categories><comments>In Proceedings TTATT 2013, arXiv:1311.5058. Andreas Maletti and
  Daniel Quernheim were financially supported by the German Research Foundation
  (DFG) grant MA/4959/1-1</comments><proxy>EPTCS</proxy><acm-class>F.4.3; G.3</acm-class><journal-ref>EPTCS 134, 2013, pp. 11-16</journal-ref><doi>10.4204/EPTCS.134.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms for (nondeterministic) finite-state tree automata (FTAs) are often
tested on random FTAs, in which all internal transitions are equiprobable. The
run-time results obtained in this manner are usually overly optimistic as most
such generated random FTAs are trivial in the sense that the number of states
of an equivalent minimal deterministic FTA is extremely small. It is
demonstrated that nontrivial random FTAs are obtained only for a narrow band of
transition probabilities. Moreover, an analytic analysis yields a formula to
approximate the transition probability that yields the most complex random
FTAs, which should be used in experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5571</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5571</id><created>2013-11-21</created><authors><author><keyname>Caralp</keyname><forenames>Mathieu</forenames><affiliation>Aix-Marseille Universit&#xe9; and CNRS</affiliation></author><author><keyname>Filiot</keyname><forenames>Emmanuel</forenames><affiliation>Universit&#xe9; Libre de Bruxelles</affiliation></author><author><keyname>Reynier</keyname><forenames>Pierre-Alain</forenames><affiliation>Aix-Marseille Universit&#xe9; and CNRS</affiliation></author><author><keyname>Servais</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>Hasselt University and transnational University of Limburg</affiliation></author><author><keyname>Talbot</keyname><forenames>Jean-Marc</forenames><affiliation>Aix-Marseille Universit&#xe9; and CNRS</affiliation></author></authors><title>Expressiveness of Visibly Pushdown Transducers</title><categories>cs.FL</categories><comments>In Proceedings TTATT 2013, arXiv:1311.5058</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.4.3</acm-class><journal-ref>EPTCS 134, 2013, pp. 17-26</journal-ref><doi>10.4204/EPTCS.134.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visibly pushdown transducers (VPTs) are visibly pushdown automata extended
with outputs. They have been introduced to model transformations of nested
words, i.e. words with a call/return structure. As trees and more generally
hedges can be linearized into (well) nested words, VPTs are a natural formalism
to express tree transformations evaluated in streaming. This paper aims at
characterizing precisely the expressive power of VPTs with respect to other
tree transducer models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5572</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5572</id><created>2013-11-21</created><authors><author><keyname>Miyahara</keyname><forenames>Kazuki</forenames><affiliation>NAIST</affiliation></author><author><keyname>Hashimoto</keyname><forenames>Kenji</forenames><affiliation>Nagoya University</affiliation></author><author><keyname>Seki</keyname><forenames>Hiroyuki</forenames><affiliation>Nagoya University</affiliation></author></authors><title>Node Query Preservation for Deterministic Linear Top-Down Tree
  Transducers</title><categories>cs.FL cs.DB</categories><comments>In Proceedings TTATT 2013, arXiv:1311.5058</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 134, 2013, pp. 27-37</journal-ref><doi>10.4204/EPTCS.134.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the decidability of node query preservation problems for
XML document transformations. We assume a transformation given by a
deterministic linear top-down data tree transducer (abbreviated as DLT^V) and
an n-ary query based on runs of a tree automaton. We say that a DLT^V Tr
strongly preserves a query Q if there is a query Q' such that for every
document t, the answer set of Q' for Tr(t) is equal to the answer set of Q for
t. Also we say that Tr weakly preserves Q if there is a query Q' such that for
every t_d in the range of Tr, the answer set of Q' for t_d is equal to the
union of the answer set of Q for t such that t_d = Tr(t). We show that the weak
preservation problem is coNP-complete and the strong preservation problem is in
2-EXPTIME.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5573</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5573</id><created>2013-11-21</created><authors><author><keyname>Maneth</keyname><forenames>Sebastian</forenames><affiliation>University of Edinburgh</affiliation></author><author><keyname>Sebastian</keyname><forenames>Tom</forenames><affiliation>Innovimax and INRIA</affiliation></author></authors><title>XPath Node Selection over Grammar-Compressed Trees</title><categories>cs.DB cs.FL</categories><comments>In Proceedings TTATT 2013, arXiv:1311.5058</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 134, 2013, pp. 38-48</journal-ref><doi>10.4204/EPTCS.134.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  XML document markup is highly repetitive and therefore well compressible
using grammar-based compression. Downward, navigational XPath can be executed
over grammar-compressed trees in PTIME: the query is translated into an
automaton which is executed in one pass over the grammar. This result is
well-known and has been mentioned before. Here we present precise bounds on the
time complexity of this problem, in terms of big-O notation. For a given
grammar and XPath query, we consider three different tasks: (1) to count the
number of nodes selected by the query, (2) to materialize the pre-order numbers
of the selected nodes, and (3) to serialize the subtrees at the selected nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5587</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5587</id><created>2013-11-21</created><authors><author><keyname>Wielsch</keyname><forenames>Max</forenames></author><author><keyname>Bieniek</keyname><forenames>Raik</forenames></author><author><keyname>Grams</keyname><forenames>Bernd</forenames></author><author><keyname>L&#xe4;ssig</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Dynamic Integration of ALM Tools for Agile Software Development</title><categories>cs.SE</categories><acm-class>D.2.6; D.2.11; D.2.12</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes the need for and goals of tool-integration within
software development processes. In particular we focus on agile software
development but are not limited to. The integration of tools and data between
the different domains of the process is essential for an efficient, effective
and customized software development. We describe what the next steps in the
pursuit of integration are and how major goals can be achieved. Beyond
theoretical and architectural considerations we describe the prototypical
implementation of an open platform approach. The paper introduces platform apps
and a functionality store as general concepts to make apps and their
functionalities available to the community. We describe the implementation of
the approach and how it can be practically utilized. The description is based
on one major use case and further steps are motivated by various other
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5590</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5590</id><created>2013-11-21</created><authors><author><keyname>Zhou</keyname><forenames>Yuzhu</forenames></author><author><keyname>Li</keyname><forenames>Le</forenames></author><author><keyname>Zhang</keyname><forenames>Honggang</forenames></author></authors><title>Adaptive Learning of Region-based pLSA Model for Total Scene Annotation</title><categories>cs.CV</categories><comments>Volume 2, Page 131-136. 2010 International Conference on Information
  and Multimedia Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a region-based pLSA model to accomplish the task of
total scene annotation. To be more specific, we not only properly generate a
list of tags for each image, but also localizing each region with its
corresponding tag. We integrate advantages of different existing region-based
works: employ efficient and powerful JSEG algorithm for segmentation so that
each region can easily express meaningful object information; the introduction
of pLSA model can help better capturing semantic information behind the
low-level features. Moreover, we also propose an adaptive padding mechanism to
automatically choose the optimal padding strategy for each region, which
directly increases the overall system performance. Finally we conduct 3
experiments to verify our ideas on Corel database and demonstrate the
effectiveness and accuracy of our system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5591</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5591</id><created>2013-11-21</created><updated>2014-05-05</updated><authors><author><keyname>Zhang</keyname><forenames>Ning</forenames></author><author><keyname>Paluri</keyname><forenames>Manohar</forenames></author><author><keyname>Ranzato</keyname><forenames>Marc'Aurelio</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Bourdev</keyname><forenames>Lubomir</forenames></author></authors><title>PANDA: Pose Aligned Networks for Deep Attribute Modeling</title><categories>cs.CV</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for inferring human attributes (such as gender, hair
style, clothes style, expression, action) from images of people under large
variation of viewpoint, pose, appearance, articulation and occlusion.
Convolutional Neural Nets (CNN) have been shown to perform very well on large
scale object recognition problems. In the context of attribute classification,
however, the signal is often subtle and it may cover only a small part of the
image, while the image is dominated by the effects of pose and viewpoint.
Discounting for pose variation would require training on very large labeled
datasets which are not presently available. Part-based models, such as poselets
and DPM have been shown to perform well for this problem but they are limited
by shallow low-level features. We propose a new method which combines
part-based models and deep learning by training pose-normalized CNNs. We show
substantial improvement vs. state-of-the-art methods on challenging attribute
classification tasks in unconstrained settings. Experiments confirm that our
method outperforms both the best part-based methods on this problem and
conventional CNNs trained on the full bounding box of the person.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5595</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5595</id><created>2013-11-18</created><authors><author><keyname>Shtern</keyname><forenames>Alon</forenames></author><author><keyname>Kimmel</keyname><forenames>Ron</forenames></author></authors><title>On Nonrigid Shape Similarity and Correspondence</title><categories>cs.CV cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important operation in geometry processing is finding the correspondences
between pairs of shapes. The Gromov-Hausdorff distance, a measure of
dissimilarity between metric spaces, has been found to be highly useful for
nonrigid shape comparison. Here, we explore the applicability of related shape
similarity measures to the problem of shape correspondence, adopting spectral
type distances. We propose to evaluate the spectral kernel distance, the
spectral embedding distance and the novel spectral quasi-conformal distance,
comparing the manifolds from different viewpoints. By matching the shapes in
the spectral domain, important attributes of surface structure are being
aligned. For the purpose of testing our ideas, we introduce a fully automatic
framework for finding intrinsic correspondence between two shapes. The proposed
method achieves state-of-the-art results on the Princeton isometric shape
matching protocol applied, as usual, to the TOSCA and SCAPE benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5599</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5599</id><created>2013-11-21</created><authors><author><keyname>Jain</keyname><forenames>Swayambhoo</forenames></author><author><keyname>Soni</keyname><forenames>Akshay</forenames></author><author><keyname>Haupt</keyname><forenames>Jarvis</forenames></author></authors><title>Compressive Measurement Designs for Estimating Structured Signals in
  Structured Clutter: A Bayesian Experimental Design Approach</title><categories>stat.ML cs.LG</categories><comments>5 pages, 4 figures. Accepted for publication at The Asilomar
  Conference on Signals, Systems, and Computers 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers an estimation task in compressive sensing, where the goal
is to estimate an unknown signal from compressive measurements that are
corrupted by additive pre-measurement noise (interference, or clutter) as well
as post-measurement noise, in the specific setting where some (perhaps limited)
prior knowledge on the signal, interference, and noise is available. The
specific aim here is to devise a strategy for incorporating this prior
information into the design of an appropriate compressive measurement strategy.
Here, the prior information is interpreted as statistics of a prior
distribution on the relevant quantities, and an approach based on Bayesian
Experimental Design is proposed. Experimental results on synthetic data
demonstrate that the proposed approach outperforms traditional random
compressive measurement designs, which are agnostic to the prior information,
as well as several other knowledge-enhanced sensing matrix designs based on
more heuristic notions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5603</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5603</id><created>2013-11-21</created><authors><author><keyname>Samadi</keyname><forenames>Samira</forenames></author></authors><title>Submodular Welfare Maximization</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An overview of different variants of the submodular welfare maximization
problem in combinatorial auctions. In particular, I studied the existing
algorithmic and game theoretic results for submodular welfare maximization
problem and its applications in other areas such as social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5612</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5612</id><created>2013-11-21</created><authors><author><keyname>Schiavoni</keyname><forenames>Stefano</forenames><affiliation>Politecnico di Milano</affiliation></author><author><keyname>Maggi</keyname><forenames>Federico</forenames><affiliation>Politecnico di Milano</affiliation></author><author><keyname>Cavallaro</keyname><forenames>Lorenzo</forenames><affiliation>Royal Holloway University of London</affiliation></author><author><keyname>Zanero</keyname><forenames>Stefano</forenames><affiliation>Politecnico di Milano</affiliation></author></authors><title>Tracking and Characterizing Botnets Using Automatically Generated
  Domains</title><categories>cs.CR</categories><comments>14 pages, 10 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern botnets rely on domain-generation algorithms (DGAs) to build resilient
command-and-control infrastructures. Recent works focus on recognizing
automatically generated domains (AGDs) from DNS traffic, which potentially
allows to identify previously unknown AGDs to hinder or disrupt botnets'
communication capabilities.
  The state-of-the-art approaches require to deploy low-level DNS sensors to
access data whose collection poses practical and privacy issues, making their
adoption problematic. We propose a mechanism that overcomes the above
limitations by analyzing DNS traffic data through a combination of linguistic
and IP-based features of suspicious domains. In this way, we are able to
identify AGD names, characterize their DGAs and isolate logical groups of
domains that represent the respective botnets. Moreover, our system enriches
these groups with new, previously unknown AGD names, and produce novel
knowledge about the evolving behavior of each tracked botnet.
  We used our system in real-world settings, to help researchers that requested
intelligence on suspicious domains and were able to label them as belonging to
the correct botnet automatically.
  Additionally, we ran an evaluation on 1,153,516 domains, including AGDs from
both modern (e.g., Bamital) and traditional (e.g., Conficker, Torpig) botnets.
Our approach correctly isolated families of AGDs that belonged to distinct
DGAs, and set automatically generated from non-automatically generated domains
apart in 94.8 percent of the cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5622</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5622</id><created>2013-11-21</created><updated>2013-12-03</updated><authors><author><keyname>Gabizon</keyname><forenames>Ariel</forenames></author></authors><title>Improved Extractors for Affine Lines</title><categories>cs.CC</categories><comments>The paper has been withdrawn as it is being merged into a joint paper
  with additional authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $F$ be the field of $q$ elements.
  We investigate the following Ramsey coloring problem for vector spaces: Given
a vector space $\F^n$, give a coloring of the points of $F^n$ with two colors
such that no affine line (i.e., affine subspace of dimension $1$) is
monochromatic. Our main result is as follows:
  For any $q\geq 25\cdot n$ and $n&gt;4$, we give an explicit coloring $D:F^n\ar
\set{0,1}$ such that for every affine line $l\subseteq F^n$, $D(l)=\set{0,1}$.
Previously this was known only for $q\geq c\cdot n^2$ for some constant $c$
\cite{GR05}. We note that this beats the random coloring for which the expected
number of monochromatic lines will be 0 only when $q\geq c\cdot n\log n$ for
some constant $c$. Furthermore, our coloring will be `almost balanced' on every
affine line. Let us state this formally in the lanuage of \emph{extractors}. We
say that a function $D:F^n\mapsto \set{0,1}$ is a \afsext{1}{\eps} if for every
affine line $l\subseteq \F^n$, $D(X)$ is $\eps$-close to uniform when $X$ is
uniformly distributed over $l$. We construct a \afsext{1}{\eps} with $\eps =
\Omega(\sqrt{n/q})$ whenever $q\geq c\cdot n$ for some constant $c$.
  The previous result of \cite{GR05} gave a \afsext{1}{\eps} only for
$q=\Omega(n^2)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5629</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5629</id><created>2013-11-21</created><authors><author><keyname>Jabi</keyname><forenames>Mohammed</forenames></author><author><keyname>Szczecinski</keyname><forenames>Leszek</forenames></author><author><keyname>Benjillali</keyname><forenames>Mustapha</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>Outage Minimization via Power Adaptation and Allocation for Truncated
  Hybrid ARQ</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we analyze hybrid ARQ (HARQ) protocols over the independent
block fading channel. We assume that the transmitter is unaware of the channel
state information (CSI) but has a knowledge about the channel statistics. We
consider two scenarios with respect to the feedback received by the
transmitter: i) ''conventional'', one-bit feedback about the decoding
success/failure (ACK/NACK), and ii) the multi-bit feedback scheme when, on top
of ACK/NACK, the receiver provides additional information about the state of
the decoder to the transmitter. In both cases, the feedback is used to allocate
(in the case of one-bit feedback) or adapt (in the case of multi-bit feedback)
the power across the HARQ transmission attempts. The objective in both cases is
the minimization of the outage probability under long-term average and peak
power constraints. We cast the problems into the dynamic programming (DP)
framework and solve them for Nakagami-m fading channels. A simplified solution
for the high signal-to-noise ratio (SNR) regime is presented using a geometric
programming (GP) approach. The obtained results quantify the advantage of the
multi-bit feedback over the conventional approach, and show that the power
optimization can provide significant gains over conventional power-constant
HARQ transmissions even in the presence of peak-power constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5634</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5634</id><created>2013-11-21</created><authors><author><keyname>Cioab&#x103;</keyname><forenames>Sebastian M.</forenames></author><author><keyname>Koolen</keyname><forenames>Jack H.</forenames></author><author><keyname>Li</keyname><forenames>Weiqiang</forenames></author></authors><title>Disconnecting strongly regular graphs</title><categories>math.CO cs.DM</categories><msc-class>05E30, 05B05, 05B15, 05C40, 05C50, 15A18, 68R10</msc-class><journal-ref>European Journal of Combinatorics 38 (2014) pp. 1-11</journal-ref><doi>10.1016/j.ejc.2013.10.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that the minimum number of vertices whose removal
disconnects a connected strongly regular graph into non-singleton components,
equals the size of the neighborhood of an edge for many graphs. These include
blocks graphs of Steiner $2$-designs, many Latin square graphs and strongly
regular graphs whose intersection parameters are at most a quarter of their
valency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5636</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5636</id><created>2013-11-21</created><authors><author><keyname>Athanasakis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Shawe-Taylor</keyname><forenames>John</forenames></author><author><keyname>Fernandez-Reyes</keyname><forenames>Delmiro</forenames></author></authors><title>Learning Non-Linear Feature Maps</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection plays a pivotal role in learning, particularly in areas
were parsimonious features can provide insight into the underlying process,
such as biology. Recent approaches for non-linear feature selection employing
greedy optimisation of Centred Kernel Target Alignment(KTA), while exhibiting
strong results in terms of generalisation accuracy and sparsity, can become
computationally prohibitive for high-dimensional datasets. We propose randSel,
a randomised feature selection algorithm, with attractive scaling properties.
Our theoretical analysis of randSel provides strong probabilistic guarantees
for the correct identification of relevant features. Experimental results on
real and artificial data, show that the method successfully identifies
effective features, performing better than a number of competitive approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5639</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5639</id><created>2013-11-21</created><authors><author><keyname>Banerjee</keyname><forenames>Swati</forenames></author><author><keyname>Mitra</keyname><forenames>Madhuchhanda</forenames></author></authors><title>Classification of ST and Q Type MI variant using thresholding and
  neighbourhood estimation method after cross wavelet based analysis</title><categories>cs.OH</categories><comments>arXiv admin note: text overlap with arXiv:astro-ph/0301002 by other
  authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a cross wavelet transform based method for
Electrocardiogram signal analysis where parameters are identified from wavelet
cross spectrum and wavelet cross coherence of ECG patterns. Most of the ECG
analysing systems use explicit time plane features for cardiac pattern
classification. Application of this proposed technique for classification
eliminates the need for extraction of various explicit time plane features and
hence reduces the complexity of the system. The cross-correlation is the
measure of similarity between two waveforms or two time series and the cross
examination reveals localized similarities in time and scale. Parameters
extracted from Wavelet Cross Spectrum (WCS) and Wavelet Coherence (WCOH) is
used for classification. A pathologically varying pattern in QT zone of
inferior lead III shows the presence of Inferior Myocardial Infarction (IMI).
The Cross Wavelet Transform and Wavelet Coherence is used for the cross
examination of single normal and abnormal (IMI) beats. A normal template beat
is selected as the absolute normal pattern. Computation of the WCS and WCOH of
the selected normal template and various other normal and abnormal beats
reveals the existence of variation among patterns under study. The Wavelet
cross spectrum and Wavelet coherence of various ECG patterns shows
distinguishing characteristics over two specific regions R1 and R2, where R1 is
the QRS complex location and R2 is the T wave region. Parameters are identified
for classification of Type 1 IMI (non Q type, with ST elevation and attenuated
QRS complex) and Type 2 IMI (Q type MI with deep Q and inverted T) and normal
subjects. Accuracy of the proposed classification method is obtained as 99.43%
for normal and abnormal class and 88.5% and 87.02% for Type I and Type II
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5663</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5663</id><created>2013-11-22</created><authors><author><keyname>Wang</keyname><forenames>Zhengkui</forenames></author><author><keyname>Chu</keyname><forenames>Yan</forenames></author><author><keyname>Tan</keyname><forenames>Kian-Lee</forenames></author><author><keyname>Agrawal</keyname><forenames>Divyakant</forenames></author><author><keyname>Abbadi</keyname><forenames>Amr EI</forenames></author><author><keyname>Xu</keyname><forenames>Xiaolong</forenames></author></authors><title>Scalable Data Cube Analysis over Big Data</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data cubes are widely used as a powerful tool to provide multidimensional
views in data warehousing and On-Line Analytical Processing (OLAP). However,
with increasing data sizes, it is becoming computationally expensive to perform
data cube analysis. The problem is exacerbated by the demand of supporting more
complicated aggregate functions (e.g. CORRELATION, Statistical Analysis) as
well as supporting frequent view updates in data cubes. This calls for new
scalable and efficient data cube analysis systems. In this paper, we introduce
HaCube, an extension of MapReduce, designed for efficient parallel data cube
analysis on large-scale data by taking advantages from both MapReduce (in terms
of scalability) and parallel DBMS (in terms of efficiency). We also provide a
general data cube materialization algorithm which is able to facilitate the
features in MapReduce-like systems towards an efficient data cube computation.
Furthermore, we demonstrate how HaCube supports view maintenance through either
incremental computation (e.g. used for SUM or COUNT) or recomputation (e.g.
used for MEDIAN or CORRELATION). We implement HaCube by extending Hadoop and
evaluate it based on the TPC-D benchmark over billions of tuples on a cluster
with over 320 cores. The experimental results demonstrate the efficiency,
scalability and practicality of HaCube for cube analysis over a large amount of
data in a distributed environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5665</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5665</id><created>2013-11-22</created><authors><author><keyname>Marx</keyname><forenames>Werner</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>Tracing the origin of a scientific legend by Reference Publication Year
  Spectroscopy (RPYS): the legend of the Darwin finches</title><categories>stat.AP cs.DL physics.soc-ph</categories><comments>Accepted for publication in Scientometrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a previews paper we introduced the quantitative method named Reference
Publication Year Spectroscopy (RPYS). With this method one can determine the
historical roots of research fields and quantify their impact on current
research. RPYS is based on the analysis of the frequency with which references
are cited in the publications of a specific research field in terms of the
publication years of these cited references. In this study, we illustrate that
RPYS can also be used to reveal the origin of scientific legends. We selected
Darwin finches as an example for illustration. Charles Darwin, the originator
of evolutionary theory, was given credit for finches he did not see and for
observations and insights about the finches he never made. We have shown that a
book published in 1947 is the most-highly cited early reference cited within
the relevant literature. This book had already been revealed as the origin of
the term Darwin finches by Sulloway through careful historical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5677</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5677</id><created>2013-11-22</created><authors><author><keyname>Podaras</keyname><forenames>Athanasios</forenames></author><author><keyname>Zizka</keyname><forenames>Tomas</forenames></author></authors><title>Criticality estimation of IT business functions with the Business
  Continuity Testing Points method for implementing effective recovery
  exercises of crisis scenarios</title><categories>cs.OH</categories><comments>9 pages. International Journal of Computer Science Issues, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The primary goal of the present paper is the introduction of a new approach
of defining IT unit business functions exact criticality levels and
respectively categorize them to the appropriate recovery tests, prior to their
thorough documentation which includes actual desired recovery time frames. The
method is entitled as Business Continuity Testing Points and it is based on the
concept of Use Case Points, a fundamental project estimation tool utilized for
sizing of object-oriented system development. The aim of the contribution is to
ameliorate the existing manual way of determining recovery time of IT business
functions that is based exclusively on experience of IT personnel, by
introducing a calculation method of multiple factors that can negatively affect
the recovery process. The elimination of damage as a result of tested immediate
response action in a crisis situation that disrupts core IT operations
constitutes the aimed advantage of the proposed contribution
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5681</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5681</id><created>2013-11-22</created><authors><author><keyname>Li</keyname><forenames>Jiachen</forenames></author><author><keyname>Gao</keyname><forenames>Feifei</forenames></author><author><keyname>Jiang</keyname><forenames>Tao</forenames></author><author><keyname>Chen</keyname><forenames>Wen</forenames></author></authors><title>Sensing and Recognition When Primary User Has Multiple Power Levels</title><categories>cs.IT math.IT</categories><comments>30 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new cognitive radio (CR) scenario when the
primary user (PU) operates under more than one transmit power levels. Different
from the existing studies where PU is assumed to have only one constant
transmit power, the new consideration well matches the practical standards,
i.e., IEEE 802.11 Series, GSM, LTE, LTE-A, etc., as well as the adaptive power
concept that has been studied over the past decades. The primary target in this
new CR scenario is, of course, still to detect the presence of PU. However,
there appears a secondary target as to identify the PU's transmit power level.
Compared to the existing works where the secondary user (SU) only senses the
``on-off'' status of PU, recognizing the power level of PU achieves more
``cognition&quot;, and could be utilized to protect different powered PU with
different interference levels. We derived quite many closed-form results for
either the threshold expressions or the performance analysis, from which many
interesting points and discussions are raised. We then further study the
cooperative sensing strategy in this new cognitive scenario and show its
significant difference from traditional algorithms. Numerical examples are
provided to corroborate the proposed studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5685</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5685</id><created>2013-11-22</created><authors><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Rau-Chaplin</keyname><forenames>Andrew</forenames></author></authors><title>Data Challenges in High-Performance Risk Analytics</title><categories>cs.DC cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Risk Analytics is important to quantify, manage and analyse risks from the
manufacturing to the financial setting. In this paper, the data challenges in
the three stages of the high-performance risk analytics pipeline, namely risk
modelling, portfolio risk management and dynamic financial analysis is
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5686</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5686</id><created>2013-11-22</created><authors><author><keyname>Yao</keyname><forenames>Zhimin</forenames></author><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Rau-Chaplin</keyname><forenames>Andrew</forenames></author></authors><title>High Performance Risk Aggregation: Addressing the Data Processing
  Challenge the Hadoop MapReduce Way</title><categories>cs.DC cs.CE</categories><comments>ScienceCloud 2013 at HPDC 2013, New York, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monte Carlo simulations employed for the analysis of portfolios of
catastrophic risk process large volumes of data. Often times these simulations
are not performed in real-time scenarios as they are slow and consume large
data. Such simulations can benefit from a framework that exploits parallelism
for addressing the computational challenge and facilitates a distributed file
system for addressing the data challenge. To this end, the Apache Hadoop
framework is chosen for the simulation reported in this paper so that the
computational challenge can be tackled using the MapReduce model and the data
challenge can be addressed using the Hadoop Distributed File System. A parallel
algorithm for the analysis of aggregate risk is proposed and implemented using
the MapReduce model in this paper. An evaluation of the performance of the
algorithm indicates that the Hadoop MapReduce model offers a framework for
processing large data in aggregate risk analysis. A simulation of aggregate
risk employing 100,000 trials with 1000 catastrophic events per trial on a
typical exposure set and contract structure is performed on multiple worker
nodes in less than 6 minutes. The result indicates the scope and feasibility of
MapReduce for tackling the computational and data challenge in the analysis of
aggregate risk for real-time use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5690</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5690</id><created>2013-11-22</created><authors><author><keyname>Zheng</keyname><forenames>Bojin</forenames></author><author><keyname>Su</keyname><forenames>Yangqian</forenames></author><author><keyname>Wu</keyname><forenames>Hongrun</forenames></author><author><keyname>Kuang</keyname><forenames>Li</forenames></author></authors><title>The Ergodicity of the Collatz Process in Positive Integer Field</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $3x+1$ problem, also called the Collatz conjecture, is a very interesting
unsolved mathematical problem related to computer science. This paper
generalized this problem by relaxing the constraints, i.e., generalizing this
deterministic process to non-deterministic process, and set up three models.
This paper analyzed the ergodicity of these models and proved that the
ergodicity of the Collatz process in positive integer field holds, i.e., all
the positive integers can be transformed to 1 by the iterations of the Collatz
function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5694</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5694</id><created>2013-11-22</created><authors><author><keyname>Chattopadhyay</keyname><forenames>Arkadev</forenames></author><author><keyname>Grenet</keyname><forenames>Bruno</forenames></author><author><keyname>Koiran</keyname><forenames>Pascal</forenames></author><author><keyname>Portier</keyname><forenames>Natacha</forenames></author><author><keyname>Strozecki</keyname><forenames>Yann</forenames></author></authors><title>Computing the multilinear factors of lacunary polynomials without
  heights</title><categories>cs.SC cs.CC</categories><comments>37 pages. arXiv admin note: substantial text overlap with
  arXiv:1206.4224</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deterministic polynomial-time algorithm which computes the
multilinear factors of multivariate lacunary polynomials over number fields. It
is based on a new Gap theorem which allows to test whether $P(X)=\sum_{j=1}^k
a_j X^{\alpha_j}(vX+t)^{\beta_j}(uX+w)^{\gamma_j}$ is identically zero in
polynomial time. Previous algorithms for this task were based on Gap Theorems
expressed in terms of the height of the coefficients. Our Gap Theorem is based
on the valuation of the polynomial and is valid for any field of characteristic
zero. As a consequence we obtain a faster and more elementary algorithm.
Furthermore, we can partially extend the algorithm to other situations, such as
absolute and approximate factorizations.
  We also give a version of our Gap Theorem valid for fields of large
characteristic, and deduce a randomized polynomial-time algorithm to compute
multilinear factors with at least three monomials of multivariate lacunary
polynomials of finite fields of large characteristic. We provide
$\mathsf{NP}$-hardness results to explain our inability to compute binomial
factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5728</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5728</id><created>2013-11-22</created><authors><author><keyname>Koster</keyname><forenames>Maurice</forenames></author><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author><author><keyname>Lindner</keyname><forenames>Ines</forenames></author><author><keyname>Napel</keyname><forenames>Stefan</forenames></author></authors><title>The Prediction value</title><categories>cs.GT</categories><comments>26 pages, 2 tables</comments><msc-class>91B12, 91A12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the prediction value (PV) as a measure of players' informational
importance in probabilistic TU games. The latter combine a standard TU game and
a probability distribution over the set of coalitions. Player $i$'s prediction
value equals the difference between the conditional expectations of $v(S)$ when
$i$ cooperates or not. We characterize the prediction value as a special member
of the class of (extended) values which satisfy anonymity, linearity and a
consistency property. Every $n$-player binomial semivalue coincides with the PV
for a particular family of probability distributions over coalitions. The PV
can thus be regarded as a power index in specific cases. Conversely, some
semivalues -- including the Banzhaf but not the Shapley value -- can be
interpreted in terms of informational importance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5735</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5735</id><created>2013-11-22</created><authors><author><keyname>Egea</keyname><forenames>Jose A</forenames></author><author><keyname>Henriques</keyname><forenames>David</forenames></author><author><keyname>Cokelaer</keyname><forenames>Thomas</forenames></author><author><keyname>Villaverde</keyname><forenames>Alejandro F</forenames></author><author><keyname>Banga</keyname><forenames>Julio R</forenames></author><author><keyname>Saez-Rodriguez</keyname><forenames>Julio</forenames></author></authors><title>MEIGO: an open-source software suite based on metaheuristics for global
  optimization in systems biology and bioinformatics</title><categories>math.OC cs.CE cs.MS q-bio.QM</categories><comments>12 pages, 7 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimization is key to solve many problems in computational biology. Global
optimization methods provide a robust methodology, and metaheuristics in
particular have proven to be the most efficient methods for many applications.
Despite their utility, there is limited availability of metaheuristic tools. We
present MEIGO, an R and Matlab optimization toolbox (also available in Python
via a wrapper of the R version), that implements metaheuristics capable of
solving diverse problems arising in systems biology and bioinformatics:
enhanced scatter search method (eSS) for continuous nonlinear programming
(cNLP) and mixed-integer programming (MINLP) problems, and variable
neighborhood search (VNS) for Integer Programming (IP) problems. Both methods
can be run on a single-thread or in parallel using a cooperative strategy. The
code is supplied under GPLv3 and is available at
\url{http://www.iim.csic.es/~gingproc/meigo.html}. Documentation and examples
are included. The R package has been submitted to Bioconductor. We evaluate
MEIGO against optimization benchmarks, and illustrate its applicability to a
series of case studies in bioinformatics and systems biology, outperforming
other state-of-the-art methods. MEIGO provides a free, open-source platform for
optimization, that can be applied to multiple domains of systems biology and
bioinformatics. It includes efficient state of the art metaheuristics, and its
open and modular structure allows the addition of further methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5740</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5740</id><created>2013-11-22</created><authors><author><keyname>Borgdorff</keyname><forenames>Joris</forenames></author><author><keyname>Mamonski</keyname><forenames>Mariusz</forenames></author><author><keyname>Bosak</keyname><forenames>Bartosz</forenames></author><author><keyname>Kurowski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Belgacem</keyname><forenames>Mohamed Ben</forenames></author><author><keyname>Chopard</keyname><forenames>Bastien</forenames></author><author><keyname>Groen</keyname><forenames>Derek</forenames></author><author><keyname>Coveney</keyname><forenames>Peter V.</forenames></author><author><keyname>Hoekstra</keyname><forenames>Alfons G.</forenames></author></authors><title>Distributed Multiscale Computing with MUSCLE 2, the Multiscale Coupling
  Library and Environment</title><categories>cs.DC cs.CE cs.PF</categories><comments>18 pages, 22 figures, submitted to journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Multiscale Coupling Library and Environment: MUSCLE 2. This
multiscale component-based execution environment has a simple to use Java, C++,
C, Python and Fortran API, compatible with MPI, OpenMP and threading codes. We
demonstrate its local and distributed computing capabilities and compare its
performance to MUSCLE 1, file copy, MPI, MPWide, and GridFTP. The local
throughput of MPI is about two times higher, so very tightly coupled code
should use MPI as a single submodel of MUSCLE 2; the distributed performance of
GridFTP is lower, especially for small messages. We test the performance of a
canal system model with MUSCLE 2, where it introduces an overhead as small as
5% compared to MPI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5750</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5750</id><created>2013-11-22</created><updated>2013-11-24</updated><authors><author><keyname>Yuan</keyname><forenames>Xiao-Tong</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Gradient Hard Thresholding Pursuit for Sparsity-Constrained Optimization</title><categories>cs.LG cs.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hard Thresholding Pursuit (HTP) is an iterative greedy selection procedure
for finding sparse solutions of underdetermined linear systems. This method has
been shown to have strong theoretical guarantee and impressive numerical
performance. In this paper, we generalize HTP from compressive sensing to a
generic problem setup of sparsity-constrained convex optimization. The proposed
algorithm iterates between a standard gradient descent step and a hard
thresholding step with or without debiasing. We prove that our method enjoys
the strong guarantees analogous to HTP in terms of rate of convergence and
parameter estimation accuracy. Numerical evidences show that our method is
superior to the state-of-the-art greedy selection methods in sparse logistic
regression and sparse precision matrix estimation tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5757</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5757</id><created>2013-11-21</created><authors><author><keyname>Vickers</keyname><forenames>Paul</forenames></author></authors><title>Lemma 4: Haptic Input + Auditory Display = Musical Instrument?</title><categories>cs.HC</categories><comments>in Haptic and Audio Interaction Design: First International Workshop,
  HAID 2006, Glasgow, UK, August 31 - September 1, 2006. Proceedings (D.
  McGookin and S. Brewster, eds.), vol. 4129/2006 of Lecture Notes in Computer
  Science, pp. 56-67, Springer-Verlag, 2006</comments><doi>10.1007/11821731</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we look at some of the design issues that affect the success of
multimodal displays that combine acoustic and haptic modalities. First, issues
affecting successful sonification design are explored and suggestions are made
about how the language of electroacoustic music can assist. Next, haptic
interaction is introduced in the light of this discussion, particularly
focusing on the roles of gesture and mimesis. Finally, some observations are
made regarding some of the issues that arise when the haptic and acoustic
modalities are combined in the interface. This paper looks at examples of where
auditory and haptic interaction have been successfully combined beyond the
strict confines of the human-computer application interface (musical
instruments in particular) and discusses lessons that may be drawn from these
domains and applied to the world of multimodal human-computer interaction. The
argument is made that combined haptic-auditory interaction schemes can be
thought of as musical instruments and some of the possible ramifications of
this are raised.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5763</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5763</id><created>2013-11-22</created><authors><author><keyname>Sarlin</keyname><forenames>Peter</forenames></author></authors><title>Automated and Weighted Self-Organizing Time Maps</title><categories>cs.NE cs.HC</categories><comments>Preprint submitted to a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes schemes for automated and weighted Self-Organizing Time
Maps (SOTMs). The SOTM provides means for a visual approach to evolutionary
clustering, which aims at producing a sequence of clustering solutions. This
task we denote as visual dynamic clustering. The implication of an automated
SOTM is not only a data-driven parametrization of the SOTM, but also the
feature of adjusting the training to the characteristics of the data at each
time step. The aim of the weighted SOTM is to improve learning from more
trustworthy or important data with an instance-varying weight. The schemes for
automated and weighted SOTMs are illustrated on two real-world datasets: (i)
country-level risk indicators to measure the evolution of global imbalances,
and (ii) credit applicant data to measure the evolution of firm-level credit
risks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5765</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5765</id><created>2013-11-22</created><authors><author><keyname>Bethu</keyname><forenames>Srikanth</forenames></author><author><keyname>Babu</keyname><forenames>G Charless</forenames></author><author><keyname>Vinoda</keyname><forenames>J</forenames></author><author><keyname>Priyadarshini</keyname><forenames>E</forenames></author><author><keyname>rao</keyname><forenames>M Raghavendra</forenames></author></authors><title>Text Classification and Distributional features techniques in Datamining
  and Warehousing</title><categories>cs.IR</categories><comments>arXiv admin note: text overlap with arXiv:0912.1014, arXiv:1002.3985
  by other authors without attribution</comments><journal-ref>IJIP 2013, Volume 7 issue 3</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text Categorization is traditionally done by using the term frequency and
inverse document frequency.This type of method is not very good because, some
words which are not so important may appear in the document .The term frequency
of unimportant words may increase and document may be classi?ed in the wrong
category.For reducing the error of classifying of documents in wrong category.
The Distributional features are introduced. In the Distribuional Features, the
Distribution of the words in the whole document is analyzed. Whole Document is
very closely analyzed for di?erent measures like FirstAppearence, Last
Appearance, Centriod, Count, etc.The measures are calculated and they are used
in tf*idf equation and result is used in k- nearest neighbor and K-means
algorithm for classifying the documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5787</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5787</id><created>2013-11-22</created><authors><author><keyname>Novaes</keyname><forenames>Carlos Eduardo de Brito</forenames></author><author><keyname>da Silva</keyname><forenames>Paulo Sergio Pereira</forenames></author><author><keyname>Rouchon</keyname><forenames>Pierre</forenames></author></authors><title>Trajectory control of a bipedal walking robot with inertial disc</title><categories>math.OC cs.RO</categories><comments>6 pages, 3 figures, submit</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we exploit some interesting properties of a class of bipedal
robots which have an inertial disc. One of this properties is the ability to
control every position and speed except for the disc position. The proposed
control is designed in two hierarchic levels. The first will drive the robot
geometry, while the second will control the speed and also the angular
momentum. The exponential stability of this approach is proved around some
neighborhood of the nominal trajectory defining the geometry of the step. This
control will not spend energy to adjust the disc position and neither to
synchronize the trajectory with the time. The proposed control only takes
action to correct the essential aspects of the walking gait. Computational
simulations are presented for different conditions, serving as a empirical test
for the neighborhood of attraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5796</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5796</id><created>2013-11-22</created><authors><author><keyname>Gilitschenski</keyname><forenames>Igor</forenames></author><author><keyname>Kurz</keyname><forenames>Gerhard</forenames></author><author><keyname>Julier</keyname><forenames>Simon J.</forenames></author><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author></authors><title>Unscented Orientation Estimation Based on the Bingham Distribution</title><categories>cs.SY cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orientation estimation for 3D objects is a common problem that is usually
tackled with traditional nonlinear filtering techniques such as the extended
Kalman filter (EKF) or the unscented Kalman filter (UKF). Most of these
techniques assume Gaussian distributions to account for system noise and
uncertain measurements. This distributional assumption does not consider the
periodic nature of pose and orientation uncertainty. We propose a filter that
considers the periodicity of the orientation estimation problem in its
distributional assumption. This is achieved by making use of the Bingham
distribution, which is defined on the hypersphere and thus inherently more
suitable to periodic problems. Furthermore, handling of non-trivial system
functions is done using deterministic sampling in an efficient way. A
deterministic sampling scheme reminiscent of the UKF is proposed for the
nonlinear manifold of orientations. It is the first deterministic sampling
scheme that truly reflects the nonlinear manifold of the orientation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5799</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5799</id><created>2013-11-22</created><updated>2014-02-08</updated><authors><author><keyname>Jan&#x10d;i&#x107;</keyname><forenames>Zorana</forenames></author><author><keyname>&#x106;iri&#x107;</keyname><forenames>Miroslav</forenames></author></authors><title>Brzozowski type determinization for fuzzy automata</title><categories>cs.FL</categories><comments>To appear in Fuzzy Sets and Systems</comments><msc-class>68Q45, 68Q70, 68T37, 03E72</msc-class><acm-class>F.1.1; I.2.3</acm-class><doi>10.1016/j.fss.2014.02.021.</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we adapt the well-known Brzozowski determinization method to
fuzzy automata. This method gives better results than all previously known
methods for determinization of fuzzy automata developed by B\v{e}lohl\'avek
[Inform Sciences 143 (2002) 205--209], Li and Pedrycz [Fuzzy Set Syst 156
(2005) 68--92], Ignjatovi\'c et al. [Inform Sciences 178 (2008) 164--180], and
Jan\v{c}i\'c et al. [Inform Sciences 181 (2011) 1358--1368]. Namely, as in the
case of ordinary nondeterministic automata, Brzozowski type determinization of
a fuzzy automaton results in a minimal crisp-deterministic fuzzy automaton
equivalent to the starting fuzzy automaton, and we show that there are cases
when all previous methods result in infinite automata, while Brzozowski type
determinization results in a finite one. The paper deals with fuzzy automata
over complete residuated lattices, but identical results can also be obtained
in a more general context, for fuzzy automata over lattice-ordered monoids, and
even for weighted automata over commutative semirings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5802</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5802</id><created>2013-11-22</created><updated>2014-10-27</updated><authors><author><keyname>Barbanera</keyname><forenames>Franco</forenames><affiliation>Dipartimento di Matematica e Informatica, Univ. Catania</affiliation></author><author><keyname>Liguoro</keyname><forenames>Ugo de'</forenames><affiliation>Dipartimento di Informatica, Univ. Torino</affiliation></author></authors><title>Loosening the notions of compliance and sub-behaviour in client/server
  systems</title><categories>cs.LO</categories><comments>In Proceedings ICE 2014, arXiv:1410.7013</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 166, 2014, pp. 94-110</journal-ref><doi>10.4204/EPTCS.166.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of &quot;session behaviors&quot; for client/server systems, we propose a
weakening of the compliance and sub-behaviour relations where the bias toward
the client (whose &quot;requests&quot; must be satisfied) is pushed further with respect
to the usual definitions, by admitting that &quot;not needed&quot; output actions from
the server side can be &quot;skipped&quot; by the client. Both compliance and
sub-behaviour relations resulting from this weakening remain decidable, though
the proof of the duals-as-minima property for servers, on which the
decidability of the sub-behaviour relation relies, requires a tighter analysis
of client/server interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5806</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5806</id><created>2013-11-22</created><updated>2015-02-09</updated><authors><author><keyname>Mukhopadhyay</keyname><forenames>Arpan</forenames></author><author><keyname>Mazumdar</keyname><forenames>Ravi R.</forenames></author></authors><title>Analysis of Load Balancing in Large Heterogeneous Processor Sharing
  Systems</title><categories>cs.DC cs.PF math.OC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze randomized dynamic load balancing schemes for multi-server
processor sharing systems when the number of servers in the system is large and
the servers have heterogeneous service rates. In particular, we focus on the
classical power-of-two load balancing scheme and a variant of it in which a
newly arrived job is assigned to the server having the least instantaneous
Lagrange shadow cost among two randomly chosen servers. The instantaneous
Lagrange shadow cost at a server is given by the ratio of the number of
unfinished jobs at the server to the capacity of the server. Two different
approaches of analysis are presented for each scheme. For exponential job
length distribution, the analysis is done using the mean field approach and for
more general job length distributions the analysis is carried out assuming an
asymptotic independence property. Analytical expressions to compute mean
sojourn time of jobs are found for both schemes. Asymptotic insensitivity of
the schemes to the type of job length distribution is established. Numerical
results are presented to validate the theoretical results and to show that,
unlike the homogeneous scenario, the power-of-two type schemes considered in
this paper may not always result in better behaviour in terms of the mean
sojourn time of jobs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5810</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5810</id><created>2013-11-22</created><authors><author><keyname>Van Horn</keyname><forenames>David</forenames></author><author><keyname>Mairson</keyname><forenames>Harry G.</forenames></author></authors><title>Deciding $k$CFA is complete for EXPTIME</title><categories>cs.PL</categories><comments>Appeared in The 13th ACM SIGPLAN International Conference on
  Functional Programming (ICFP'08), Victoria, British Columbia, Canada,
  September 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an exact characterization of the computational complexity of the
$k$CFA hierarchy. For any $k &gt; 0$, we prove that the control flow decision
problem is complete for deterministic exponential time. This theorem validates
empirical observations that such control flow analysis is intractable. It also
provides more general insight into the complexity of abstract interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5816</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5816</id><created>2013-11-22</created><authors><author><keyname>Knowles</keyname><forenames>Bryan</forenames></author><author><keyname>Yang</keyname><forenames>Rong</forenames></author></authors><title>Sinkless: A Preliminary Study of Stress Propagation in Group Project
  Social Networks using a Variant of the Abelian Sandpile Model</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We perform social network analysis on 53 students split over three semesters
and 13 groups, using conventional measures like eigenvector centrality,
betweeness centrality, and degree centrality, as well as defining a variant of
the Abelian Sandpile Model (ASM) with the intention of modeling stress
propagation in the college classroom. We correlate the results of these
analyses with group project grades received; due to a small or poorly collected
dataset, we are unable to conclude that any of these network measures relates
to those grades. However, we are successful in using this dataset to define a
discrete, recursive, and more generalized variant of the ASM. Abelian Sandpile
Model, College Grades, Self-organized Criticality, Sinkless Sandpile Model,
Social Network Analysis, Stress Propagation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5825</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5825</id><created>2013-11-22</created><authors><author><keyname>Van Horn</keyname><forenames>David</forenames></author><author><keyname>Mairson</keyname><forenames>Harry G.</forenames></author></authors><title>Flow analysis, linearity, and PTIME</title><categories>cs.PL</categories><comments>Appears in The 15th International Static Analysis Symposium (SAS
  2008), Valencia, Spain, July 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flow analysis is a ubiquitous and much-studied component of compiler
technology---and its variations abound. Amongst the most well known is Shivers'
0CFA; however, the best known algorithm for 0CFA requires time cubic in the
size of the analyzed program and is unlikely to be improved. Consequently,
several analyses have been designed to approximate 0CFA by trading precision
for faster computation. Henglein's simple closure analysis, for example,
forfeits the notion of directionality in flows and enjoys an &quot;almost linear&quot;
time algorithm. But in making trade-offs between precision and complexity, what
has been given up and what has been gained? Where do these analyses differ and
where do they coincide?
  We identify a core language---the linear $\lambda$-calculus---where 0CFA,
simple closure analysis, and many other known approximations or restrictions to
0CFA are rendered identical. Moreover, for this core language, analysis
corresponds with (instrumented) evaluation. Because analysis faithfully
captures evaluation, and because the linear $\lambda$-calculus is complete for
PTIME, we derive PTIME-completeness results for all of these analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5829</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5829</id><created>2013-11-20</created><authors><author><keyname>Kadir</keyname><forenames>Abdul</forenames></author><author><keyname>Nugroho</keyname><forenames>Lukito Edi</forenames></author><author><keyname>Susanto</keyname><forenames>Adhi</forenames></author><author><keyname>Santosa</keyname><forenames>Paulus Insap</forenames></author></authors><title>Neural Network Application on Foliage Plant Identification</title><categories>cs.CV cs.NE</categories><comments>8 pages</comments><journal-ref>International Journal of Computer Applications Volume 29 No.9,
  September 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several researches in leaf identification did not include color information
as features. The main reason is caused by a fact that they used green colored
leaves as samples. However, for foliage plants, plants with colorful leaves,
fancy patterns in their leaves, and interesting plants with unique shape, color
and also texture could not be neglected. For example, Epipremnum pinnatum
'Aureum' and Epipremnum pinnatum 'Marble Queen' have similar patterns, same
shape, but different colors. Combination of shape, color, texture features, and
other attribute contained on the leaf is very useful in leaf identification. In
this research, Polar Fourier Transform and three kinds of geometric features
were used to represent shape features, color moments that consist of mean,
standard deviation, skewness were used to represent color features, texture
features are extracted from GLCMs, and vein features were added to improve
performance of the identification system. The identification system uses
Probabilistic Neural Network (PNN) as a classifier. The result shows that the
system gives average accuracy of 93.0833% for 60 kinds of foliage plants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5830</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5830</id><created>2013-11-22</created><authors><author><keyname>Liu</keyname><forenames>Baodong</forenames></author><author><keyname>Yu</keyname><forenames>Hengyong</forenames></author><author><keyname>Verbridge</keyname><forenames>Scott S.</forenames></author><author><keyname>Sun</keyname><forenames>Lizhi</forenames></author><author><keyname>Wang</keyname><forenames>Ge</forenames></author></authors><title>Dictionary-Learning-Based Reconstruction Method for Electron Tomography</title><categories>cs.CV physics.med-ph</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Electron tomography usually suffers from so called missing wedge artifacts
caused by limited tilt angle range. An equally sloped tomography (EST)
acquisition scheme (which should be called the linogram sampling scheme) was
recently applied to achieve 2.4-angstrom resolution. On the other hand, a
compressive sensing-inspired reconstruction algorithm, known as adaptive
dictionary based statistical iterative reconstruction (ADSIR), has been
reported for x-ray computed tomography. In this paper, we evaluate the EST,
ADSIR and an ordered-subset simultaneous algebraic reconstruction technique
(OS-SART), and compare the ES and equally angled (EA) data acquisition modes.
Our results show that OS-SART is comparable to EST, and the ADSIR outperforms
EST and OS-SART. Furthermore, the equally sloped projection data acquisition
mode has no advantage over the conventional equally angled mode in the context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5831</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5831</id><created>2013-11-22</created><updated>2013-12-02</updated><authors><author><keyname>Liu</keyname><forenames>Xiteng</forenames></author></authors><title>Unveil Compressed Sensing</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures and 21 images</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the applicability of compressed sensing theory. We take a genuine
look at both experimental results and theoretical works. We answer the
following questions: 1) What can compressed sensing really do? 2) More
importantly, why?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5834</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5834</id><created>2013-11-22</created><authors><author><keyname>Pulipaka</keyname><forenames>Akshay</forenames></author><author><keyname>Seeling</keyname><forenames>Patrick</forenames></author><author><keyname>Reisslein</keyname><forenames>Martin</forenames></author><author><keyname>Karam</keyname><forenames>Lina J.</forenames></author></authors><title>Traffic and Statistical Multiplexing Characterization of 3D Video
  Representation Formats (Extended Version)</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The network transport of 3D video, which contains two views of a video scene,
poses significant challenges due to the increased video data compared to
conventional single-view video. Addressing these challenges requires a thorough
understanding of the traffic and multiplexing characteristics of the different
representation formats of 3D video. We examine the average bitrate-distortion
(RD) and bitrate variability-distortion (VD) characteristics of three main
representation formats. Specifically, we compare multiview video (MV)
representation and encoding, frame sequential (FS) representation, and
side-by-side (SBS) representation, whereby conventional single-view encoding is
employed for the FS and SBS representations. Our results for long 3D videos in
full HD format indicate that the MV representation and encoding achieves the
highest RD efficiency, while exhibiting the highest bitrate variabilities. We
examine the impact of these bitrate variabilities on network transport through
extensive statistical multiplexing simulations. We find that when multiplexing
a small number of streams, the MV and FS representations require the same
bandwidth. However, when multiplexing a large number of streams or smoothing
traffic, the MV representation and encoding reduces the bandwidth requirement
relative to the FS representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5836</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5836</id><created>2013-11-22</created><authors><author><keyname>Gupta</keyname><forenames>Pooja</forenames></author><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author><author><keyname>Mathur</keyname><forenames>Iti</forenames></author></authors><title>Automatic Ranking of MT Outputs using Approximations</title><categories>cs.CL</categories><journal-ref>International Journal of Computer Applications 81(17):27-31,
  November 2013</journal-ref><doi>10.5120/14217-2463</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since long, research on machine translation has been ongoing. Still, we do
not get good translations from MT engines so developed. Manual ranking of these
outputs tends to be very time consuming and expensive. Identifying which one is
better or worse than the others is a very taxing task. In this paper, we show
an approach which can provide automatic ranks to MT outputs (translations)
taken from different MT Engines and which is based on N-gram approximations. We
provide a solution where no human intervention is required for ranking systems.
Further we also show the evaluations of our results which show equivalent
results as that of human ranking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5843</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5843</id><created>2013-11-22</created><authors><author><keyname>P&#x142;aczek</keyname><forenames>Bart&#x142;omiej</forenames></author></authors><title>A traffic model based on fuzzy cellular automata</title><categories>cs.ET cs.SY</categories><comments>23 pages, 11 figures. arXiv admin note: substantial text overlap with
  arXiv:1112.4631</comments><journal-ref>P{\l}aczek B.: A traffic model based on fuzzy cellular automata.
  Journal of Cellular Automata, vol. 8, no. 3-4, pp. 261-282 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular automata (CA) play an important role in the development of
computationally efficient microscopic traffic models and recently have gained
considerable importance as a mean of optimising traffic control strategies.
However, real-time application of the available CA models in traffic control
systems is a difficult task due to their discrete and stochastic nature. This
paper introduces a novel method for simulation of signalised traffic streams,
which combines CA and fuzzy numbers. The introduced traffic simulation
algorithm eliminates main drawbacks of the CA approach, i.e. necessity of
multiple Monte Carlo simulations and calibration issues. Computational cost of
traffic simulation for the proposed algorithm is considerably lower than the
cost of simulation based on stochastic CA. Thus, the simulation results can be
obtained in a much shorter time. Experiments confirmed that the simulation
results for the introduced algorithm are consistent with that observed for
stochastic CA. The proposed simulation algorithm is suitable for real-time
applications in traffic control systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5863</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5863</id><created>2013-11-22</created><updated>2014-12-03</updated><authors><author><keyname>Lafond</keyname><forenames>Manuel</forenames></author><author><keyname>Seamone</keyname><forenames>Ben</forenames></author></authors><title>Hamiltonian chordal graphs are not cycle extendible</title><categories>math.CO cs.DM</categories><comments>Some results from Section 3 were incorrect and have been removed. To
  appear in SIAM Journal on Discrete Mathematics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1990, Hendry conjectured that every Hamiltonian chordal graph is cycle
extendible; that is, the vertices of any non-Hamiltonian cycle are contained in
a cycle of length one greater. We disprove this conjecture by constructing
counterexamples on $n$ vertices for any $n \geq 15$. Furthermore, we show that
there exist counterexamples where the ratio of the length of a non-extendible
cycle to the total number of vertices can be made arbitrarily small. We then
consider cycle extendibility in Hamiltonian chordal graphs where certain
induced subgraphs are forbidden, notably $P_n$ and the bull.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5871</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5871</id><created>2013-11-22</created><updated>2014-07-16</updated><authors><author><keyname>Lauer</keyname><forenames>Fabien</forenames><affiliation>LORIA</affiliation></author><author><keyname>Ohlsson</keyname><forenames>Henrik</forenames></author></authors><title>Finding sparse solutions of systems of polynomial equations via
  group-sparsity optimization</title><categories>cs.IT cs.LG math.IT math.OC stat.ML</categories><comments>Journal of Global Optimization (2014) to appear</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper deals with the problem of finding sparse solutions to systems of
polynomial equations possibly perturbed by noise. In particular, we show how
these solutions can be recovered from group-sparse solutions of a derived
system of linear equations. Then, two approaches are considered to find these
group-sparse solutions. The first one is based on a convex relaxation resulting
in a second-order cone programming formulation which can benefit from efficient
reweighting techniques for sparsity enhancement. For this approach, sufficient
conditions for the exact recovery of the sparsest solution to the polynomial
system are derived in the noiseless setting, while stable recovery results are
obtained for the noisy case. Though lacking a similar analysis, the second
approach provides a more computationally efficient algorithm based on a greedy
strategy adding the groups one-by-one. With respect to previous work, the
proposed methods recover the sparsest solution in a very short computing time
while remaining at least as accurate in terms of the probability of success.
This probability is empirically analyzed to emphasize the relationship between
the ability of the methods to solve the polynomial system and the sparsity of
the solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5880</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5880</id><created>2013-11-21</created><authors><author><keyname>Vickers</keyname><forenames>Paul</forenames></author></authors><title>Ways of Listening and Modes of Being: Electroacoustic Auditory Display</title><categories>cs.HC</categories><comments>available at http://journal.sonicstudies.org/vol02/nr01/a04</comments><journal-ref>Journal of Sonic Studies, Vol 2, 2012, ISSN 2212-6252</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Auditory display is concerned with the use of non-speech sound to communicate
information. If the term seems at first oxymoronic, then consider auditory
display as an activity of perceptualization, that is, the process of making
perceptible to humans aspects or features of a given data set or system. Most
commonly this is done using visual representations (which process we call
visualization) but it is not limited to the visual channel and recent years
have witnessed the increased use of auditory representations in the production
of tools for exploring data. By way of semiotics and an aesthetic perspective
shift this article posits that auditory display may be considered a form of
organized sound and explores the listening experience in this context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5904</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5904</id><created>2013-11-22</created><updated>2014-08-22</updated><authors><author><keyname>Aartsen</keyname><forenames>M. G.</forenames></author><author><keyname>Abbasi</keyname><forenames>R.</forenames></author><author><keyname>Ackermann</keyname><forenames>M.</forenames></author><author><keyname>Adams</keyname><forenames>J.</forenames></author><author><keyname>Aguilar</keyname><forenames>J. A.</forenames></author><author><keyname>Ahlers</keyname><forenames>M.</forenames></author><author><keyname>Altmann</keyname><forenames>D.</forenames></author><author><keyname>Arguelles</keyname><forenames>C.</forenames></author><author><keyname>Auffenberg</keyname><forenames>J.</forenames></author><author><keyname>Bai</keyname><forenames>X.</forenames></author><author><keyname>Baker</keyname><forenames>M.</forenames></author><author><keyname>Barwick</keyname><forenames>S. W.</forenames></author><author><keyname>Baum</keyname><forenames>V.</forenames></author><author><keyname>Bay</keyname><forenames>R.</forenames></author><author><keyname>Beatty</keyname><forenames>J. J.</forenames></author><author><keyname>Tjus</keyname><forenames>J. Becker</forenames></author><author><keyname>Becker</keyname><forenames>K. -H.</forenames></author><author><keyname>BenZvi</keyname><forenames>S.</forenames></author><author><keyname>Berghaus</keyname><forenames>P.</forenames></author><author><keyname>Berley</keyname><forenames>D.</forenames></author><author><keyname>Bernardini</keyname><forenames>E.</forenames></author><author><keyname>Bernhard</keyname><forenames>A.</forenames></author><author><keyname>Besson</keyname><forenames>D. Z.</forenames></author><author><keyname>Binder</keyname><forenames>G.</forenames></author><author><keyname>Bindig</keyname><forenames>D.</forenames></author><author><keyname>Bissok</keyname><forenames>M.</forenames></author><author><keyname>Blaufuss</keyname><forenames>E.</forenames></author><author><keyname>Blumenthal</keyname><forenames>J.</forenames></author><author><keyname>Boersma</keyname><forenames>D. J.</forenames></author><author><keyname>Bohm</keyname><forenames>C.</forenames></author><author><keyname>Bose</keyname><forenames>D.</forenames></author><author><keyname>B&#xf6;ser</keyname><forenames>S.</forenames></author><author><keyname>Botner</keyname><forenames>O.</forenames></author><author><keyname>Brayeur</keyname><forenames>L.</forenames></author><author><keyname>Bretz</keyname><forenames>H. -P.</forenames></author><author><keyname>Brown</keyname><forenames>A. M.</forenames></author><author><keyname>Bruijn</keyname><forenames>R.</forenames></author><author><keyname>Casey</keyname><forenames>J.</forenames></author><author><keyname>Casier</keyname><forenames>M.</forenames></author><author><keyname>Chirkin</keyname><forenames>D.</forenames></author><author><keyname>Christov</keyname><forenames>A.</forenames></author><author><keyname>Christy</keyname><forenames>B.</forenames></author><author><keyname>Clark</keyname><forenames>K.</forenames></author><author><keyname>Classen</keyname><forenames>L.</forenames></author><author><keyname>Clevermann</keyname><forenames>F.</forenames></author><author><keyname>Coenders</keyname><forenames>S.</forenames></author><author><keyname>Cohen</keyname><forenames>S.</forenames></author><author><keyname>Cowen</keyname><forenames>D. F.</forenames></author><author><keyname>Silva</keyname><forenames>A. H. Cruz</forenames></author><author><keyname>Danninger</keyname><forenames>M.</forenames></author><author><keyname>Daughhetee</keyname><forenames>J.</forenames></author><author><keyname>Davis</keyname><forenames>J. C.</forenames></author><author><keyname>Day</keyname><forenames>M.</forenames></author><author><keyname>De Clercq</keyname><forenames>C.</forenames></author><author><keyname>De Ridder</keyname><forenames>S.</forenames></author><author><keyname>Desiati</keyname><forenames>P.</forenames></author><author><keyname>de Vries</keyname><forenames>K. D.</forenames></author><author><keyname>de With</keyname><forenames>M.</forenames></author><author><keyname>DeYoung</keyname><forenames>T.</forenames></author><author><keyname>D&#xed;az-V&#xe9;lez</keyname><forenames>J. C.</forenames></author><author><keyname>Dunkman</keyname><forenames>M.</forenames></author><author><keyname>Eagan</keyname><forenames>R.</forenames></author><author><keyname>Eberhardt</keyname><forenames>B.</forenames></author><author><keyname>Eichmann</keyname><forenames>B.</forenames></author><author><keyname>Eisch</keyname><forenames>J.</forenames></author><author><keyname>Euler</keyname><forenames>S.</forenames></author><author><keyname>Evenson</keyname><forenames>P. A.</forenames></author><author><keyname>Fadiran</keyname><forenames>O.</forenames></author><author><keyname>Fazely</keyname><forenames>A. R.</forenames></author><author><keyname>Fedynitch</keyname><forenames>A.</forenames></author><author><keyname>Feintzeig</keyname><forenames>J.</forenames></author><author><keyname>Feusels</keyname><forenames>T.</forenames></author><author><keyname>Filimonov</keyname><forenames>K.</forenames></author><author><keyname>Finley</keyname><forenames>C.</forenames></author><author><keyname>Fischer-Wasels</keyname><forenames>T.</forenames></author><author><keyname>Flis</keyname><forenames>S.</forenames></author><author><keyname>Franckowiak</keyname><forenames>A.</forenames></author><author><keyname>Frantzen</keyname><forenames>K.</forenames></author><author><keyname>Fuchs</keyname><forenames>T.</forenames></author><author><keyname>Gaisser</keyname><forenames>T. K.</forenames></author><author><keyname>Gallagher</keyname><forenames>J.</forenames></author><author><keyname>Gerhardt</keyname><forenames>L.</forenames></author><author><keyname>Gladstone</keyname><forenames>L.</forenames></author><author><keyname>Gl&#xfc;senkamp</keyname><forenames>T.</forenames></author><author><keyname>Goldschmidt</keyname><forenames>A.</forenames></author><author><keyname>Golup</keyname><forenames>G.</forenames></author><author><keyname>Gonzalez</keyname><forenames>J. G.</forenames></author><author><keyname>Goodman</keyname><forenames>J. A.</forenames></author><author><keyname>G&#xf3;ra</keyname><forenames>D.</forenames></author><author><keyname>Grandmont</keyname><forenames>D. T.</forenames></author><author><keyname>Grant</keyname><forenames>D.</forenames></author><author><keyname>Gretskov</keyname><forenames>P.</forenames></author><author><keyname>Groh</keyname><forenames>J. C.</forenames></author><author><keyname>Gro&#xdf;</keyname><forenames>A.</forenames></author><author><keyname>Ha</keyname><forenames>C.</forenames></author><author><keyname>Ismail</keyname><forenames>A. Haj</forenames></author><author><keyname>Hallen</keyname><forenames>P.</forenames></author><author><keyname>Hallgren</keyname><forenames>A.</forenames></author><author><keyname>Halzen</keyname><forenames>F.</forenames></author><author><keyname>Hanson</keyname><forenames>K.</forenames></author><author><keyname>Hebecker</keyname><forenames>D.</forenames></author><author><keyname>Heereman</keyname><forenames>D.</forenames></author><author><keyname>Heinen</keyname><forenames>D.</forenames></author><author><keyname>Helbing</keyname><forenames>K.</forenames></author><author><keyname>Hellauer</keyname><forenames>R.</forenames></author><author><keyname>Hickford</keyname><forenames>S.</forenames></author><author><keyname>Hill</keyname><forenames>G. C.</forenames></author><author><keyname>Hoffman</keyname><forenames>K. D.</forenames></author><author><keyname>Hoffmann</keyname><forenames>R.</forenames></author><author><keyname>Homeier</keyname><forenames>A.</forenames></author><author><keyname>Hoshina</keyname><forenames>K.</forenames></author><author><keyname>Huang</keyname><forenames>F.</forenames></author><author><keyname>Huelsnitz</keyname><forenames>W.</forenames></author><author><keyname>Hulth</keyname><forenames>P. O.</forenames></author><author><keyname>Hultqvist</keyname><forenames>K.</forenames></author><author><keyname>Hussain</keyname><forenames>S.</forenames></author><author><keyname>Ishihara</keyname><forenames>A.</forenames></author><author><keyname>Jacobi</keyname><forenames>E.</forenames></author><author><keyname>Jacobsen</keyname><forenames>J.</forenames></author><author><keyname>Jagielski</keyname><forenames>K.</forenames></author><author><keyname>Japaridze</keyname><forenames>G. S.</forenames></author><author><keyname>Jero</keyname><forenames>K.</forenames></author><author><keyname>Jlelati</keyname><forenames>O.</forenames></author><author><keyname>Kaminsky</keyname><forenames>B.</forenames></author><author><keyname>Kappes</keyname><forenames>A.</forenames></author><author><keyname>Karg</keyname><forenames>T.</forenames></author><author><keyname>Karle</keyname><forenames>A.</forenames></author><author><keyname>Kauer</keyname><forenames>M.</forenames></author><author><keyname>Kelley</keyname><forenames>J. L.</forenames></author><author><keyname>Kiryluk</keyname><forenames>J.</forenames></author><author><keyname>Kl&#xe4;s</keyname><forenames>J.</forenames></author><author><keyname>Klein</keyname><forenames>S. R.</forenames></author><author><keyname>K&#xf6;hne</keyname><forenames>J. -H.</forenames></author><author><keyname>Kohnen</keyname><forenames>G.</forenames></author><author><keyname>Kolanoski</keyname><forenames>H.</forenames></author><author><keyname>K&#xf6;pke</keyname><forenames>L.</forenames></author><author><keyname>Kopper</keyname><forenames>C.</forenames></author><author><keyname>Kopper</keyname><forenames>S.</forenames></author><author><keyname>Koskinen</keyname><forenames>D. J.</forenames></author><author><keyname>Kowalski</keyname><forenames>M.</forenames></author><author><keyname>Krasberg</keyname><forenames>M.</forenames></author><author><keyname>Kriesten</keyname><forenames>A.</forenames></author><author><keyname>Krings</keyname><forenames>K.</forenames></author><author><keyname>Kroll</keyname><forenames>G.</forenames></author><author><keyname>Kunnen</keyname><forenames>J.</forenames></author><author><keyname>Kurahashi</keyname><forenames>N.</forenames></author><author><keyname>Kuwabara</keyname><forenames>T.</forenames></author><author><keyname>Labare</keyname><forenames>M.</forenames></author><author><keyname>Landsman</keyname><forenames>H.</forenames></author><author><keyname>Larson</keyname><forenames>M. J.</forenames></author><author><keyname>Lesiak-Bzdak</keyname><forenames>M.</forenames></author><author><keyname>Leuermann</keyname><forenames>M.</forenames></author><author><keyname>Leute</keyname><forenames>J.</forenames></author><author><keyname>L&#xfc;nemann</keyname><forenames>J.</forenames></author><author><keyname>Mac&#xed;as</keyname><forenames>O.</forenames></author><author><keyname>Madsen</keyname><forenames>J.</forenames></author><author><keyname>Maggi</keyname><forenames>G.</forenames></author><author><keyname>Maruyama</keyname><forenames>R.</forenames></author><author><keyname>Mase</keyname><forenames>K.</forenames></author><author><keyname>Matis</keyname><forenames>H. S.</forenames></author><author><keyname>McNally</keyname><forenames>F.</forenames></author><author><keyname>Meagher</keyname><forenames>K.</forenames></author><author><keyname>Merck</keyname><forenames>M.</forenames></author><author><keyname>Merino</keyname><forenames>G.</forenames></author><author><keyname>Meures</keyname><forenames>T.</forenames></author><author><keyname>Miarecki</keyname><forenames>S.</forenames></author><author><keyname>Middell</keyname><forenames>E.</forenames></author><author><keyname>Milke</keyname><forenames>N.</forenames></author><author><keyname>Miller</keyname><forenames>J.</forenames></author><author><keyname>Mohrmann</keyname><forenames>L.</forenames></author><author><keyname>Montaruli</keyname><forenames>T.</forenames></author><author><keyname>Morse</keyname><forenames>R.</forenames></author><author><keyname>Nahnhauer</keyname><forenames>R.</forenames></author><author><keyname>Naumann</keyname><forenames>U.</forenames></author><author><keyname>Niederhausen</keyname><forenames>H.</forenames></author><author><keyname>Nowicki</keyname><forenames>S. C.</forenames></author><author><keyname>Nygren</keyname><forenames>D. R.</forenames></author><author><keyname>Obertacke</keyname><forenames>A.</forenames></author><author><keyname>Odrowski</keyname><forenames>S.</forenames></author><author><keyname>Olivas</keyname><forenames>A.</forenames></author><author><keyname>Omairat</keyname><forenames>A.</forenames></author><author><keyname>O'Murchadha</keyname><forenames>A.</forenames></author><author><keyname>Paul</keyname><forenames>L.</forenames></author><author><keyname>Pepper</keyname><forenames>J. A.</forenames></author><author><keyname>Heros</keyname><forenames>C. P&#xe9;rez de los</forenames></author><author><keyname>Pfendner</keyname><forenames>C.</forenames></author><author><keyname>Pieloth</keyname><forenames>D.</forenames></author><author><keyname>Pinat</keyname><forenames>E.</forenames></author><author><keyname>Posselt</keyname><forenames>J.</forenames></author><author><keyname>Price</keyname><forenames>P. B.</forenames></author><author><keyname>Przybylski</keyname><forenames>G. T.</forenames></author><author><keyname>Quinnan</keyname><forenames>M.</forenames></author><author><keyname>&#xe4;del</keyname><forenames>L. R</forenames></author><author><keyname>Rae</keyname><forenames>I.</forenames></author><author><keyname>Rameez</keyname><forenames>M.</forenames></author><author><keyname>Rawlins</keyname><forenames>K.</forenames></author><author><keyname>Redl</keyname><forenames>P.</forenames></author><author><keyname>Reimann</keyname><forenames>R.</forenames></author><author><keyname>Resconi</keyname><forenames>E.</forenames></author><author><keyname>Rhode</keyname><forenames>W.</forenames></author><author><keyname>Ribordy</keyname><forenames>M.</forenames></author><author><keyname>Richman</keyname><forenames>M.</forenames></author><author><keyname>Riedel</keyname><forenames>B.</forenames></author><author><keyname>Rodrigues</keyname><forenames>J. P.</forenames></author><author><keyname>Rott</keyname><forenames>C.</forenames></author><author><keyname>Ruhe</keyname><forenames>T.</forenames></author><author><keyname>Ruzybayev</keyname><forenames>B.</forenames></author><author><keyname>Ryckbosch</keyname><forenames>D.</forenames></author><author><keyname>Saba</keyname><forenames>S. M.</forenames></author><author><keyname>Sander</keyname><forenames>H. -G.</forenames></author><author><keyname>Santander</keyname><forenames>M.</forenames></author><author><keyname>Sarkar</keyname><forenames>S.</forenames></author><author><keyname>Schatto</keyname><forenames>K.</forenames></author><author><keyname>Scheriau</keyname><forenames>F.</forenames></author><author><keyname>Schmidt</keyname><forenames>T.</forenames></author><author><keyname>Schmitz</keyname><forenames>M.</forenames></author><author><keyname>Schoenen</keyname><forenames>S.</forenames></author><author><keyname>Sch&#xf6;neberg</keyname><forenames>S.</forenames></author><author><keyname>Sch&#xf6;nwald</keyname><forenames>A.</forenames></author><author><keyname>Schukraft</keyname><forenames>A.</forenames></author><author><keyname>Schulte</keyname><forenames>L.</forenames></author><author><keyname>Schultz</keyname><forenames>D.</forenames></author><author><keyname>Schulz</keyname><forenames>O.</forenames></author><author><keyname>Seckel</keyname><forenames>D.</forenames></author><author><keyname>Sestayo</keyname><forenames>Y.</forenames></author><author><keyname>Seunarine</keyname><forenames>S.</forenames></author><author><keyname>Shanidze</keyname><forenames>R.</forenames></author><author><keyname>Sheremata</keyname><forenames>C.</forenames></author><author><keyname>Smith</keyname><forenames>M. W. E.</forenames></author><author><keyname>Soldin</keyname><forenames>D.</forenames></author><author><keyname>Spiczak</keyname><forenames>G. M.</forenames></author><author><keyname>Spiering</keyname><forenames>C.</forenames></author><author><keyname>Stamatikos</keyname><forenames>M.</forenames></author><author><keyname>Stanev</keyname><forenames>T.</forenames></author><author><keyname>Stanisha</keyname><forenames>N. A.</forenames></author><author><keyname>Stasik</keyname><forenames>A.</forenames></author><author><keyname>Stezelberger</keyname><forenames>T.</forenames></author><author><keyname>Stokstad</keyname><forenames>R. G.</forenames></author><author><keyname>St&#xf6;&#xdf;l</keyname><forenames>A.</forenames></author><author><keyname>Strahler</keyname><forenames>E. A.</forenames></author><author><keyname>Str&#xf6;m</keyname><forenames>R.</forenames></author><author><keyname>Strotjohann</keyname><forenames>N. L.</forenames></author><author><keyname>Sullivan</keyname><forenames>G. W.</forenames></author><author><keyname>Taavola</keyname><forenames>H.</forenames></author><author><keyname>Taboada</keyname><forenames>I.</forenames></author><author><keyname>Tamburro</keyname><forenames>A.</forenames></author><author><keyname>Tepe</keyname><forenames>A.</forenames></author><author><keyname>Ter-Antonyan</keyname><forenames>S.</forenames></author><author><keyname>Te&#x161;i&#x107;</keyname><forenames>G.</forenames></author><author><keyname>Tilav</keyname><forenames>S.</forenames></author><author><keyname>Toale</keyname><forenames>P. A.</forenames></author><author><keyname>Tobin</keyname><forenames>M. N.</forenames></author><author><keyname>Toscano</keyname><forenames>S.</forenames></author><author><keyname>Tselengidou</keyname><forenames>M.</forenames></author><author><keyname>Unger</keyname><forenames>E.</forenames></author><author><keyname>Usner</keyname><forenames>M.</forenames></author><author><keyname>Vallecorsa</keyname><forenames>S.</forenames></author><author><keyname>van Eijndhoven</keyname><forenames>N.</forenames></author><author><keyname>Van Overloop</keyname><forenames>A.</forenames></author><author><keyname>van Santen</keyname><forenames>J.</forenames></author><author><keyname>Vehring</keyname><forenames>M.</forenames></author><author><keyname>Voge</keyname><forenames>M.</forenames></author><author><keyname>Vraeghe</keyname><forenames>M.</forenames></author><author><keyname>Walck</keyname><forenames>C.</forenames></author><author><keyname>Waldenmaier</keyname><forenames>T.</forenames></author><author><keyname>Wallraff</keyname><forenames>M.</forenames></author><author><keyname>Weaver</keyname><forenames>Ch.</forenames></author><author><keyname>Wellons</keyname><forenames>M.</forenames></author><author><keyname>Wendt</keyname><forenames>C.</forenames></author><author><keyname>Westerhoff</keyname><forenames>S.</forenames></author><author><keyname>Whitehorn</keyname><forenames>N.</forenames></author><author><keyname>Wiebe</keyname><forenames>K.</forenames></author><author><keyname>Wiebusch</keyname><forenames>C. H.</forenames></author><author><keyname>Williams</keyname><forenames>D. R.</forenames></author><author><keyname>Wissing</keyname><forenames>H.</forenames></author><author><keyname>Wolf</keyname><forenames>M.</forenames></author><author><keyname>Wood</keyname><forenames>T. R.</forenames></author><author><keyname>Woschnagg</keyname><forenames>K.</forenames></author><author><keyname>Xu</keyname><forenames>D. L.</forenames></author><author><keyname>Xu</keyname><forenames>X. W.</forenames></author><author><keyname>Yanez</keyname><forenames>J. P.</forenames></author><author><keyname>Yodh</keyname><forenames>G.</forenames></author><author><keyname>Yoshida</keyname><forenames>S.</forenames></author><author><keyname>Zarzhitsky</keyname><forenames>P.</forenames></author><author><keyname>Ziemann</keyname><forenames>J.</forenames></author><author><keyname>Zierke</keyname><forenames>S.</forenames></author><author><keyname>Zoll</keyname><forenames>M.</forenames></author></authors><title>The IceProd Framework: Distributed Data Processing for the IceCube
  Neutrino Observatory</title><categories>cs.DC</categories><journal-ref>Journal of Parallel &amp; Distributed Computing 75:198,2015</journal-ref><doi>10.1016/j.jpdc.2014.08.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IceCube is a one-gigaton instrument located at the geographic South Pole,
designed to detect cosmic neutrinos, iden- tify the particle nature of dark
matter, and study high-energy neutrinos themselves. Simulation of the IceCube
detector and processing of data require a significant amount of computational
resources. IceProd is a distributed management system based on Python, XML-RPC
and GridFTP. It is driven by a central database in order to coordinate and
admin- ister production of simulations and processing of data produced by the
IceCube detector. IceProd runs as a separate layer on top of other middleware
and can take advantage of a variety of computing resources, including grids and
batch systems such as CREAM, Condor, and PBS. This is accomplished by a set of
dedicated daemons that process job submission in a coordinated fashion through
the use of middleware plugins that serve to abstract the details of job
submission and job management from the framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5917</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5917</id><created>2013-11-22</created><authors><author><keyname>Ponieman</keyname><forenames>Nicolas</forenames></author><author><keyname>Salles</keyname><forenames>Alejo</forenames></author><author><keyname>Sarraute</keyname><forenames>Carlos</forenames></author></authors><title>Human Mobility and Predictability enriched by Social Phenomena
  Information</title><categories>physics.soc-ph cs.CY cs.SI</categories><journal-ref>Proc. 2013 IEEE/ACM International Conference on Advances in Social
  Networks Analysis and Mining ASONAM, Niagara Falls, Canada, August 25-28,
  2013, pp. 1331-1336</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The massive amounts of geolocation data collected from mobile phone records
has sparked an ongoing effort to understand and predict the mobility patterns
of human beings. In this work, we study the extent to which social phenomena
are reflected in mobile phone data, focusing in particular in the cases of
urban commute and major sports events. We illustrate how these events are
reflected in the data, and show how information about the events can be used to
improve predictability in a simple model for a mobile phone user's location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5921</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5921</id><created>2013-11-22</created><authors><author><keyname>Khalek</keyname><forenames>Amin Abdel</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Delay-Constrained Video Transmission: Quality-driven Resource Allocation
  and Scheduling</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Journal of Selected Topics in Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-time video demands quality-of-service (QoS) guarantees such as delay
bounds for end-user satisfaction. Furthermore, the tolerable delay varies
depending on the use case such as live streaming or two-way video conferencing.
Due to the inherently stochastic nature of wireless fading channels,
deterministic delay bounds are difficult to guarantee. Instead, we propose
providing statistical delay guarantees using the concept of effective capacity.
We consider a multiuser setup whereby different users have (possibly different)
delay QoS constraints. We derive the resource allocation policy that maximizes
the sum video quality and applies to any quality metric with concave
rate-quality mapping. We show that the optimal operating point per user is such
that the rate-distortion slope is the inverse of the supported video source
rate per unit bandwidth, a key metric we refer to as the source spectral
efficiency. We also solve the alternative problem of fairness-based resource
allocation whereby the objective is to maximize the minimum video quality
across users. Finally, we derive user admission and scheduling policies that
enable selecting a maximal user subset such that all selected users can meet
their statistical delay requirement. Results show that video users with
differentiated QoS requirements can achieve similar video quality with vastly
different resource requirements. Thus, QoS-aware scheduling and resource
allocation enable supporting significantly more users under the same resource
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5924</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5924</id><created>2013-11-22</created><authors><author><keyname>Brodeur</keyname><forenames>Simon</forenames></author><author><keyname>Rouat</keyname><forenames>Jean</forenames></author></authors><title>Objets Sonores: Une Repr\'esentation Bio-Inspir\'ee Hi\'erarchique
  Parcimonieuse \`A Tr\`es Grandes Dimensions Utilisable En Reconnaissance;
  Auditory Objects: Bio-Inspired Hierarchical Sparse High Dimensional
  Representation for Recognition</title><categories>cs.SD</categories><comments>16 pages, Invited journal paper</comments><journal-ref>Canadian Acoustics / Acoustique Canadienne, Vol 41, nb 2, June
  2013, pp. 33 - 48</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  L'accent est plac\'e dans cet article sur la structure hi\'erarchique,
l'aspect parcimonieux de la repr\'esentation de l'information sonore, la tr\`es
grande dimension des caract\'eristiques ainsi que sur l'ind\'ependance des
caract\'eristiques permettant de d\'efinir les composantes des objets sonores.
Les notions d'objet sonore et de repr\'esentation neuronale sont d'abord
introduites, puis illustr\'ees avec une application en analyse de signaux
sonores vari\'es: parole, musique et environnements naturels ext\'erieurs.
Finalement, un nouveau syst\`eme de reconnaissance automatique de parole est
propos\'e. Celui-ci est compar\'e \`a un syst\`eme statistique conventionnel.
Il montre tr\`es clairement que l'analyse par objets sonores introduit une
grande polyvalence et robustesse en reconnaissance de parole. Cette
int\'egration des connaissances en neurosciences et traitement des signaux
acoustiques ouvre de nouvelles perspectives dans le domaine de la
reconnaissance de signaux acoustiques.
  The emphasis is put on the hierarchical structure, independence and
sparseness aspects of auditory signal representations in high-dimensional
spaces, so as to define the components of auditory objects. The concept of an
auditory object and its neural representation is introduced. An illustrative
application then follows, consisting in the analysis of various auditory
signals: speech, music and natural outdoor environments. A new automatic speech
recognition (ASR) system is then proposed and compared to a conventional
statistical system. The proposed system clearly shows that an object-based
analysis introduces a great flexibility and robustness for the task of speech
recognition. The integration of knowledge from neuroscience and acoustic signal
processing brings new ways of thinking to the field of classification of
acoustic signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5925</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5925</id><created>2013-11-22</created><authors><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Mahini</keyname><forenames>Hamid</forenames></author><author><keyname>Sawant</keyname><forenames>Anshul</forenames></author></authors><title>Scheduling a Cascade with Opposing Influences</title><categories>cs.GT cs.SI</categories><journal-ref>In proceedings of the 6th International Symposium on Algorithmic
  Game Theory (SAGT), Aachen, Germany, pages 195-206, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adoption or rejection of ideas, products, and technologies in a society is
often governed by simultaneous propagation of positive and negative influences.
Consider a planner trying to introduce an idea in different parts of a society
at different times. How should the planner design a schedule considering this
fact that positive reaction to the idea in early areas has a positive impact on
probability of success in later areas, whereas a flopped reaction has exactly
the opposite impact? We generalize a well-known economic model which has been
recently used by Chierichetti, Kleinberg, and Panconesi (ACM EC'12). In this
model the reaction of each area is determined by its initial preference and the
reaction of early areas. We generalize previous works by studying the problem
when people in different areas have various behaviors.
  We first prove, independent of the planner's schedule, influences help
(resp., hurt) the planner to propagate her idea if it is an appealing (resp.,
unappealing) idea. We also study the problem of designing the optimal
non-adaptive spreading strategy. In the non-adaptive spreading strategy, the
schedule is fixed at the beginning and is never changed. Whereas, in adaptive
spreading strategy the planner decides about the next move based on the current
state of the cascade. We demonstrate that it is hard to propose a non-adaptive
spreading strategy in general. Nevertheless, we propose an algorithm to find
the best non-adaptive spreading strategy when probabilities of different
behaviors of people in various areas drawn i.i.d from an unknown distribution.
Then, we consider the influence propagation phenomenon when the underlying
influence network can be any arbitrary graph. We show it is $\#P$-complete to
compute the expected number of adopters for a given spreading strategy.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="52000" completeListSize="102538">1122234|53001</resumptionToken>
</ListRecords>
</OAI-PMH>
