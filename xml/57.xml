<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T02:05:24Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|56001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6846</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6846</id><created>2014-01-27</created><authors><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Delayed Channel State Information: Incremental Redundancy with Backtrack
  Retransmission</title><categories>cs.IT math.IT</categories><comments>Accepted at IEEE ICC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many practical wireless systems, the Signal-to-Interference-and-Noise
Ratio (SINR) that is applicable to a certain transmission, referred to as
Channel State Information (CSI), can only be learned after the transmission has
taken place and is thereby outdated (delayed). For example, this occurs under
intermittent interference. We devise the backward retransmission (BRQ) scheme,
which uses the delayed CSIT to send the optimal amount of incremental
redundancy (IR). BRQ uses fixed-length packets, fixed-rate R transmission
codebook, and operates as Markov block coding, where the correlation between
the adjacent packets depends on the amount of IR parity bits. When the delayed
CSIT is full and R grows asymptotically, the average throughput of BRQ becomes
equal to the value achieved with prior CSIT and a fixed-power transmitter;
however, at the expense of increased delay. The second contribution is a method
for employing BRQ when a limited number of feedback bits is available to report
the delayed CSIT. The main novelty is the idea to assemble multiple feedback
opportunities and report multiple SINRs through vector quantization. This
challenges the conventional wisdom in ARQ protocols where feedback bits are
used to only quantize the CSIT of the immediate previous transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6848</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6848</id><created>2014-01-27</created><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author><author><keyname>Impagliazzo</keyname><forenames>Russell</forenames></author><author><keyname>Moshkovitz</keyname><forenames>Dana</forenames></author></authors><title>AM with Multiple Merlins</title><categories>cs.CC quant-ph</categories><comments>48 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and study a new model of interactive proofs: AM(k), or
Arthur-Merlin with k non-communicating Merlins. Unlike with the better-known
MIP, here the assumption is that each Merlin receives an independent random
challenge from Arthur. One motivation for this model (which we explore in
detail) comes from the close analogies between it and the quantum complexity
class QMA(k), but the AM(k) model is also natural in its own right.
  We illustrate the power of multiple Merlins by giving an AM(2) protocol for
3SAT, in which the Merlins' challenges and responses consist of only
n^{1/2+o(1)} bits each. Our protocol has the consequence that, assuming the
Exponential Time Hypothesis (ETH), any algorithm for approximating a dense CSP
with a polynomial-size alphabet must take n^{(log n)^{1-o(1)}} time. Algorithms
nearly matching this lower bound are known, but their running times had never
been previously explained. Brandao and Harrow have also recently used our 3SAT
protocol to show quasipolynomial hardness for approximating the values of
certain entangled games.
  In the other direction, we give a simple quasipolynomial-time approximation
algorithm for free games, and use it to prove that, assuming the ETH, our 3SAT
protocol is essentially optimal. More generally, we show that multiple Merlins
never provide more than a polynomial advantage over one: that is, AM(k)=AM for
all k=poly(n). The key to this result is a subsampling theorem for free games,
which follows from powerful results by Alon et al. and Barak et al. on
subsampling dense CSPs, and which says that the value of any free game can be
closely approximated by the value of a logarithmic-sized random subgame.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6853</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6853</id><created>2014-01-27</created><authors><author><keyname>Bauckhage</keyname><forenames>Christian</forenames></author></authors><title>Computing the Kullback-Leibler Divergence between two Generalized Gamma
  Distributions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a closed form solution for the Kullback-Leibler divergence between
two generalized gamma distributions. These notes are meant as a reference and
provide a guided tour towards a result of practical interest that is rarely
explicated in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6875</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6875</id><created>2014-01-15</created><authors><author><keyname>Qu</keyname><forenames>Shaolin</forenames></author><author><keyname>Chai</keyname><forenames>Joyce Y.</forenames></author></authors><title>Context-based Word Acquisition for Situated Dialogue in a Virtual World</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 37, pages
  247-277, 2010</journal-ref><doi>10.1613/jair.2912</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To tackle the vocabulary problem in conversational systems, previous work has
applied unsupervised learning approaches on co-occurring speech and eye gaze
during interaction to automatically acquire new words. Although these
approaches have shown promise, several issues related to human language
behavior and human-machine conversation have not been addressed. First,
psycholinguistic studies have shown certain temporal regularities between human
eye movement and language production. While these regularities can potentially
guide the acquisition process, they have not been incorporated in the previous
unsupervised approaches. Second, conversational systems generally have an
existing knowledge base about the domain and vocabulary. While the existing
knowledge can potentially help bootstrap and constrain the acquired new words,
it has not been incorporated in the previous models. Third, eye gaze could
serve different functions in human-machine conversation. Some gaze streams may
not be closely coupled with speech stream, and thus are potentially detrimental
to word acquisition. Automated recognition of closely-coupled speech-gaze
streams based on conversation context is important. To address these issues, we
developed new approaches that incorporate user language behavior, domain
knowledge, and conversation context in word acquisition. We evaluated these
approaches in the context of situated dialogue in a virtual world. Our
experimental results have shown that incorporating the above three types of
contextual information significantly improves word acquisition performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6876</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6876</id><created>2014-01-22</created><authors><author><keyname>Nakov</keyname><forenames>Preslav Ivanov</forenames></author><author><keyname>Ng</keyname><forenames>Hwee Tou</forenames></author></authors><title>Improving Statistical Machine Translation for a Resource-Poor Language
  Using Related Resource-Rich Languages</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 44, pages
  179-222, 2012</journal-ref><doi>10.1613/jair.3540</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel language-independent approach for improving machine
translation for resource-poor languages by exploiting their similarity to
resource-rich ones. More precisely, we improve the translation from a
resource-poor source language X_1 into a resource-rich language Y given a
bi-text containing a limited number of parallel sentences for X_1-Y and a
larger bi-text for X_2-Y for some resource-rich language X_2 that is closely
related to X_1. This is achieved by taking advantage of the opportunities that
vocabulary overlap and similarities between the languages X_1 and X_2 in
spelling, word order, and syntax offer: (1) we improve the word alignments for
the resource-poor language, (2) we further augment it with additional
translation options, and (3) we take care of potential spelling differences
through appropriate transliteration. The evaluation for Indonesian- &gt;English
using Malay and for Spanish -&gt; English using Portuguese and pretending Spanish
is resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points,
respectively, which is an improvement over the best rivaling approaches, while
using much less additional data. Overall, our method cuts the amount of
necessary &quot;real training data by a factor of 2--5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6877</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6877</id><created>2014-01-02</created><authors><author><keyname>Abdmeziem</keyname><forenames>Riad</forenames></author><author><keyname>Tandjaoui</keyname><forenames>Djamel</forenames></author></authors><title>Internet of Things: Concept, Building blocks, Applications and
  Challenges</title><categories>cs.CY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1207.0203,
  arXiv:1105.1693 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of things (IoT) constitutes one of the most important technology
that has the potential to affect deeply our way of life, after mobile phones
and Internet. The basic idea is that every objet that is around us will be part
of the network (Internet), interacting to reach a common goal. In another word,
the Internet of Things concept aims to link the physical world to the digital
one. Technology advances along with popular demand will foster the wide spread
deployement of IoT's services, it would radically transform our corporations,
communities, and personal spheres. In this survey, we aim to provide the reader
with a broad overview of the Internet of things concept, its building blocks,
its applications along with its challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6887</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6887</id><created>2014-01-27</created><authors><author><keyname>Khan</keyname><forenames>Kifayat Ullah</forenames></author><author><keyname>Najeebullah</keyname><forenames>Kamran</forenames></author><author><keyname>Nawaz</keyname><forenames>Waqas</forenames></author><author><keyname>Lee</keyname><forenames>Young-Koo</forenames></author></authors><title>OLAP on Structurally Significant Data in Graphs</title><categories>cs.DB</categories><comments>4 Pages, Accepted in 5th International Conference on Data Mining and
  Intelligent Information Technology Applications (ICMIA) 2013, Korea</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Summarized data analysis of graphs using OLAP (Online Analytical Processing)
is very popular these days. However due to high dimensionality and large size,
it is not easy to decide which data should be aggregated for OLAP analysis.
Though iceberg cubing is useful, but it is unaware of the significance of
dimensional values with respect to the structure of the graph. In this paper,
we propose a Structural Significance, SS, measure to identify the structurally
significant dimensional values in each dimension. This leads to structure aware
pruning. We then propose an algorithm, iGraphCubing, to compute the graph cube
to analyze the structurally significant data using the proposed measure. We
evaluated the proposed ideas on real and synthetic data sets and observed very
encouraging results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6891</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6891</id><created>2014-01-27</created><authors><author><keyname>Csurka</keyname><forenames>Gabriela</forenames></author><author><keyname>Ah-Pine</keyname><forenames>Julien</forenames></author><author><keyname>Clinchant</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Unsupervised Visual and Textual Information Fusion in Multimedia
  Retrieval - A Graph-based Point of View</title><categories>cs.IR</categories><comments>An extended version of the paper: Visual and Textual Information
  Fusion in Multimedia Retrieval using Semantic Filtering and Graph based
  Methods, by J. Ah-Pine, G. Csurka and S. Clinchant, submitted to ACM
  Transactions on Information Systems</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multimedia collections are more than ever growing in size and diversity.
Effective multimedia retrieval systems are thus critical to access these
datasets from the end-user perspective and in a scalable way. We are interested
in repositories of image/text multimedia objects and we study multimodal
information fusion techniques in the context of content based multimedia
information retrieval. We focus on graph based methods which have proven to
provide state-of-the-art performances. We particularly examine two of such
methods : cross-media similarities and random walk based scores. From a
theoretical viewpoint, we propose a unifying graph based framework which
encompasses the two aforementioned approaches. Our proposal allows us to
highlight the core features one should consider when using a graph based
technique for the combination of visual and textual information. We compare
cross-media and random walk based results using three different real-world
datasets. From a practical standpoint, our extended empirical analysis allow us
to provide insights and guidelines about the use of graph based methods for
multimodal information fusion in content based multimedia information
retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6904</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6904</id><created>2014-01-27</created><updated>2015-04-27</updated><authors><author><keyname>Wang</keyname><forenames>Hanlei</forenames></author></authors><title>Adaptive Visual Tracking for Robotic Systems Without Image-Space
  Velocity Measurement</title><categories>cs.RO cs.SY math.OC</categories><comments>21 pages, 3 figures, revised for making improvements based on the
  reviewers' and AE's comments from Automatica and for adding the journal
  reference</comments><journal-ref>Automatica, 55: 294-301, May 2015</journal-ref><doi>10.1016/j.automatica.2015.02.029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the visual tracking problem for robotic systems
without image-space velocity measurement, simultaneously taking into account
the uncertainties of the camera model and the manipulator kinematics and
dynamics. We propose a new image-space observer that exploits the image-space
velocity information contained in the unknown kinematics, upon which, we design
an adaptive controller without using the image-space velocity signal where the
adaptations of the depth-rate-independent kinematic parameter and depth
parameter are driven by both the image-space tracking errors and observation
errors. The major superiority of the proposed observer-based adaptive
controller lies in its simplicity and the separation of the handling of
multiple uncertainties in visually servoed robotic systems, thus avoiding the
overparametrization problem of the existing work. Using Lyapunov analysis, we
demonstrate that the image-space tracking errors converge to zero
asymptotically. The performance of the proposed adaptive control scheme is
illustrated by a numerical simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6929</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6929</id><created>2014-01-27</created><authors><author><keyname>Wi&#x15b;licki</keyname><forenames>W.</forenames></author><author><keyname>Bednarski</keyname><forenames>T.</forenames></author><author><keyname>Bia&#x142;as</keyname><forenames>P.</forenames></author><author><keyname>Czerwi&#x144;ski</keyname><forenames>E.</forenames></author><author><keyname>Kap&#x142;on</keyname><forenames>&#x141;.</forenames></author><author><keyname>Kochanowski</keyname><forenames>A.</forenames></author><author><keyname>Korcyl</keyname><forenames>G.</forenames></author><author><keyname>Kowal</keyname><forenames>J.</forenames></author><author><keyname>Kowalski</keyname><forenames>P.</forenames></author><author><keyname>Kozik</keyname><forenames>T.</forenames></author><author><keyname>Krzemie&#x144;</keyname><forenames>W.</forenames></author><author><keyname>Molenda</keyname><forenames>M.</forenames></author><author><keyname>Moskal</keyname><forenames>P.</forenames></author><author><keyname>Nied&#x17a;wiecki</keyname><forenames>S.</forenames></author><author><keyname>Pa&#x142;ka</keyname><forenames>M.</forenames></author><author><keyname>Pawlik</keyname><forenames>M.</forenames></author><author><keyname>Raczy&#x144;ski</keyname><forenames>L.</forenames></author><author><keyname>Rudy</keyname><forenames>Z.</forenames></author><author><keyname>Salabura</keyname><forenames>P.</forenames></author><author><keyname>Sharma</keyname><forenames>N. G.</forenames></author><author><keyname>Silarski</keyname><forenames>M.</forenames></author><author><keyname>S&#x142;omski</keyname><forenames>A.</forenames></author><author><keyname>Smyrski</keyname><forenames>J.</forenames></author><author><keyname>Strzelecki</keyname><forenames>A.</forenames></author><author><keyname>Wieczorek</keyname><forenames>A.</forenames></author><author><keyname>Zieli&#x144;ski</keyname><forenames>M.</forenames></author><author><keyname>Zo&#x144;</keyname><forenames>N.</forenames></author></authors><title>Computing support for advanced medical data analysis and imaging</title><categories>physics.comp-ph cs.CV cs.DC physics.ins-det physics.med-ph</categories><comments>9 p, 3 figs, based on talk given at Symposium on Positron Emission
  Tomography, Sept. 19-22, 2013, Jagiellonian University, Krak\'ow, Pl</comments><journal-ref>Bio-Algorithms and Med-Systems 10(2014)52</journal-ref><doi>10.1515/bams-2014-0001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss computing issues for data analysis and image reconstruction of
PET-TOF medical scanner or other medical scanning devices producing large
volumes of data. Service architecture based on the grid and cloud concepts for
distributed processing is proposed and critically discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6931</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6931</id><created>2014-01-27</created><authors><author><keyname>Ge</keyname><forenames>Xi</forenames></author><author><keyname>Shepherd</keyname><forenames>David</forenames></author><author><keyname>Damevski</keyname><forenames>Kostadin</forenames></author><author><keyname>Murphy-Hill</keyname><forenames>Emerson</forenames></author></authors><title>How the Sando Search Tool Recommends Queries</title><categories>cs.SE cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developers spend a significant amount of time searching their local codebase.
To help them search efficiently, researchers have proposed novel tools that
apply state-of-the-art information retrieval algorithms to retrieve relevant
code snippets from the local codebase. However, these tools still rely on the
developer to craft an effective query, which requires that the developer is
familiar with the terms contained in the related code snippets. Our empirical
data from a state-of-the-art local code search tool, called Sando, suggests
that developers are sometimes unacquainted with their local codebase. In order
to bridge the gap between developers and their ever-increasing local codebase,
in this paper we demonstrate the recommendation techniques integrated in Sando.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6951</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6951</id><created>2014-01-27</created><updated>2014-05-26</updated><authors><author><keyname>Ghosh</keyname><forenames>Asim</forenames></author><author><keyname>Chattopadhyay</keyname><forenames>Nachiketa</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Bikas K.</forenames></author></authors><title>Inequality in Societies, Academic Institutions and Science Journals:
  Gini and k-indices</title><categories>physics.soc-ph cs.DL</categories><comments>5 pages, 3 figures</comments><journal-ref>Physica A 410 (2014) 30-34</journal-ref><doi>10.1016/j.physa.2014.05.026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social inequality is traditionally measured by the Gini-index ($g$). The
$g$-index takes values from $0$ to $1$ where $g=0$ represents complete equality
and $g=1$ represents complete inequality. Most of the estimates of the income
or wealth data indicate the $g$ value to be widely dispersed across the
countries of the world: \textit{g} values typically range from $0.30$ to $0.65$
at a particular time (year). We estimated similarly the Gini-index for the
citations earned by the yearly publications of various academic institutions
and the science journals. The ISI web of science data suggests remarkably
strong inequality and universality ($g=0.70\pm0.07$) across all the
universities and institutions of the world, while for the journals we find
$g=0.65\pm0.15$ for any typical year. We define a new inequality measure,
namely the $k$-index, saying that the cumulative income or citations of ($1-k$)
fraction of people or papers exceed those earned by the fraction ($k$) of the
people or publications respectively. We find, while the $k$-index value for
income ranges from $0.60$ to $0.75$ for income distributions across the world,
it has a value around $0.75\pm0.05$ for different universities and institutions
across the world and around $0.77\pm0.10$ for the science journals. Apart from
above indices, we also analyze the same institution and journal citation data
by measuring Pietra index and median index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6956</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6956</id><created>2014-01-27</created><updated>2014-02-27</updated><authors><author><keyname>Kwon</keyname><forenames>Joon</forenames></author><author><keyname>Mertikopoulos</keyname><forenames>Panayotis</forenames></author></authors><title>A continuous-time approach to online optimization</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a family of learning strategies for online optimization problems
that evolve in continuous time and we show that they lead to no regret. From a
more traditional, discrete-time viewpoint, this continuous-time approach allows
us to derive the no-regret properties of a large class of discrete-time
algorithms including as special cases the exponential weight algorithm, online
mirror descent, smooth fictitious play and vanishingly smooth fictitious play.
In so doing, we obtain a unified view of many classical regret bounds, and we
show that they can be decomposed into a term stemming from continuous-time
considerations and a term which measures the disparity between discrete and
continuous time. As a result, we obtain a general class of infinite horizon
learning strategies that guarantee an $\mathcal{O}(n^{-1/2})$ regret bound
without having to resort to a doubling trick.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6961</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6961</id><created>2014-01-27</created><authors><author><keyname>Challacombe</keyname><forenames>Matt</forenames></author><author><keyname>Bock</keyname><forenames>Nicolas</forenames></author></authors><title>An N-Body Solution to the Problem of Fock Exchange</title><categories>cs.DS</categories><comments>5 pages, 2 figures</comments><report-no>LA-UR-14-20354</report-no><doi>10.1063/1.4868636</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We report an N-Body approach to computing the Fock exchange matrix with and
without permutational symmetry. The method achieves an O(N lg N) computational
complexity through an embedded metric-query, allowing hierarchical application
of direct SCF criteria. The advantages of permutational symmetry are found to
be 4-fold for small systems, but decreasing with increasing system size and/or
more permissive neglect criteria. This work sets the stage for: (1) the
introduction of range queries in multi-level multipole schemes for rank
reduction, and (2) recursive task parallelism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6962</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6962</id><created>2014-01-27</created><authors><author><keyname>Reboredo</keyname><forenames>Hugo</forenames></author><author><keyname>Renna</keyname><forenames>Francesco</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author><author><keyname>Rodrigues</keyname><forenames>Miguel R. D.</forenames></author></authors><title>Compressive Classification of a Mixture of Gaussians: Analysis, Designs
  and Geometrical Interpretation</title><categories>cs.IT math.IT</categories><comments>38 pages, 7 figures. Submitted for publication in IEEE Transactions
  on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper derives fundamental limits on the performance of compressive
classification when the source is a mixture of Gaussians. It provides an
asymptotic analysis of a Bhattacharya based upper bound on the
misclassification probability for the optimal Maximum-A-Posteriori (MAP)
classifier that depends on quantities that are dual to the concepts of
diversity-order and coding gain in multi-antenna communications. The
diversity-order of the measurement system determines the rate at which the
probability of misclassification decays with signal-to-noise ratio (SNR) in the
low-noise regime. The counterpart of coding gain is the measurement gain which
determines the power offset of the probability of misclassification in the
low-noise regime. These two quantities make it possible to quantify differences
in misclassification probability between random measurement and
(diversity-order) optimized measurement. Results are presented for two-class
classification problems first with zero-mean Gaussians then with nonzero-mean
Gaussians, and finally for multiple-class Gaussian classification problems. The
behavior of misclassification probability is revealed to be intimately related
to certain fundamental geometric quantities determined by the measurement
system, the source and their interplay. Numerical results, representative of
compressive classification of a mixture of Gaussians, demonstrate alignment of
the actual misclassification probability with the Bhattacharya based upper
bound. The connection between the misclassification performance and the
alignment between source and measurement geometry may be used to guide the
design of dictionaries for compressive classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6963</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6963</id><created>2014-01-27</created><updated>2016-02-22</updated><authors><author><keyname>Hunt</keyname><forenames>Fern Y.</forenames></author></authors><title>Optimal Spread in Network Consensus Models</title><categories>cs.DM cs.DS</categories><comments>6 pages, 4 figures. This paper replaces an earlier version. The
  entire paper has been rewritten. In addition to the results of the previous
  version, a normalized submodular function is introduced and is used to obtain
  a performance ratio for our algorithm. We also provide a comparison with the
  approximation obtained using the greedy algorithm</comments><msc-class>05C69, 05C81, 05C90, 68M10, 90C27</msc-class><acm-class>G.2; G.3</acm-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In a model of network communication based on a random walk in an undirected
graph, what subset of nodes (subject to constraints on the set size), enable
the fastest spread of information? The dynamics of spread is described by a
process dual to the movement from informed to uninformed nodes. In this
setting, an optimal set $A$ minimizes the sum of the expected first hitting
times $F(A)$, of random walks that start at nodes outside the set.
  In this paper,the problem is reformulated so that the search for solutions is
restricted to a class of optimal and &quot;near&quot; optimal subsets of the graph. We
introduce a submodular, non-decreasing rank function $\rho$, that permits some
comparison between the solution obtained by the classical greedy algorithm and
one obtained by our methods. The supermodularity and non-increasing properties
of $F$ are used to show that the rank of our solution is at least
$(1-\frac{1}{e})$ times the rank of the optimal set. When the solution has a
higher rank than the greedy solution this constant can be improved to
$(1-\frac{1}{e})(1+\chi)$ where $\chi &gt;0$ is determined a posteriori. The
method requires the evaluation of $F$ for sets of some fixed cardinality $m$,
where $m$ is much smaller than the cardinality of the optimal set. When $F$ has
forward elemental curvature $\kappa$, we can provide a rough description of the
trade-off between solution quality and computational effort $m$ in terms of
$\kappa$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6964</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6964</id><created>2014-01-27</created><authors><author><keyname>Zinoviev</keyname><forenames>Dmitry</forenames></author><author><keyname>Llewelyn</keyname><forenames>Sarah</forenames></author></authors><title>Co-Evolution of Friendship and Publishing in Online Blogging Social
  Networks</title><categories>cs.SI physics.soc-ph</categories><comments>6 pages, 7 figures. Has been presented as a poster at WebSci-2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past decade, blogging web sites have become more sophisticated and
influential than ever. Much of this sophistication and influence follows from
their network organization. Blogging social networks (BSNs) allow individual
bloggers to form contact lists, subscribe to other blogs, comment on blog
posts, declare interests, and participate in collective blogs. Thus, a BSN is a
bimodal venue, where users can engage in publishing (post) as well as in social
(make friends) activities. In this paper, we study the co-evolution of both
activities. We observed a significant positive correlation between blogging and
socializing. In addition, we identified a number of user archetypes that
correspond to &quot;mainly bloggers,&quot; &quot;mainly socializers,&quot; etc. We analyzed a BSN
at the level of individual posts and changes in contact lists and at the level
of trajectories in the friendship-publishing space. Both approaches produced
consistent results: the majority of BSN users are passive readers; publishing
is the dominant active behavior in a BSN; and social activities complement
blogging, rather than compete with it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6968</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6968</id><created>2014-01-27</created><authors><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Karystinos</keyname><forenames>George N.</forenames></author></authors><title>Fixed-rank Rayleigh Quotient Maximization by an $M$PSK Sequence</title><categories>cs.IT math.CO math.IT math.OC</categories><comments>15 pages, 12 figures, To appear in IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Certain optimization problems in communication systems, such as
limited-feedback constant-envelope beamforming or noncoherent $M$-ary
phase-shift keying ($M$PSK) sequence detection, result in the maximization of a
fixed-rank positive semidefinite quadratic form over the $M$PSK alphabet. This
form is a special case of the Rayleigh quotient of a matrix and, in general,
its maximization by an $M$PSK sequence is $\mathcal{NP}$-hard. However, if the
rank of the matrix is not a function of its size, then the optimal solution can
be computed with polynomial complexity in the matrix size. In this work, we
develop a new technique to efficiently solve this problem by utilizing
auxiliary continuous-valued angles and partitioning the resulting continuous
space of solutions into a polynomial-size set of regions, each of which
corresponds to a distinct $M$PSK sequence. The sequence that maximizes the
Rayleigh quotient is shown to belong to this polynomial-size set of sequences,
thus efficiently reducing the size of the feasible set from exponential to
polynomial. Based on this analysis, we also develop an algorithm that
constructs this set in polynomial time and show that it is fully
parallelizable, memory efficient, and rank scalable. The proposed algorithm
compares favorably with other solvers for this problem that have appeared
recently in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6971</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6971</id><created>2014-01-24</created><authors><author><keyname>Kumar</keyname><forenames>Tangudu Bharat</forenames></author><author><keyname>Awadhiya</keyname><forenames>Bhaskar</forenames></author><author><keyname>MeherAbhinav</keyname><forenames>E.</forenames></author><author><keyname>Ghosh</keyname><forenames>Bahniman</forenames></author><author><keyname>Bishnoi</keyname><forenames>Bhupesh</forenames></author></authors><title>Performance Analysis of Spin Transfer Torque Random Access Memory with
  cross shaped free layer using Heusler Alloys by using micromagnetic studies</title><categories>cs.ET</categories><comments>7 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigated the performance of spin transfer torque random access memory
(STT-RAM) cell with cross shaped Heusler compound based free layer using
micromagnetic simulations. We designed the free layer using Cobalt based
Heusler compounds. Here in this paper, simulation results predict that
switching time from one state to other state is reduced. Also it is examined
that critical switching current density to switch the magnetization of free
layer of STT RAM cell is reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6975</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6975</id><created>2014-01-27</created><authors><author><keyname>Delfosse</keyname><forenames>Nicolas</forenames></author><author><keyname>Tillich</keyname><forenames>Jean-Pierre</forenames></author></authors><title>A decoding algorithm for CSS codes using the X/Z correlations</title><categories>cs.IT math.IT quant-ph</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple decoding algorithm for CSS codes taking into account the
correlations between the X part and the Z part of the error. Applying this idea
to surface codes, we derive an improved version of the perfect matching
decoding algorithm which uses these X/Z correlations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6981</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6981</id><created>2014-01-27</created><updated>2015-04-28</updated><authors><author><keyname>Kourtellis</keyname><forenames>Nicolas</forenames></author><author><keyname>Morales</keyname><forenames>Gianmarco De Francisci</forenames></author><author><keyname>Bonchi</keyname><forenames>Francesco</forenames></author></authors><title>Scalable Online Betweenness Centrality in Evolving Graphs</title><categories>cs.DS</categories><comments>15 pages, 9 Figures, accepted for publication in IEEE Transactions on
  Knowledge and Data Engineering</comments><msc-class>94C15, 05C85</msc-class><acm-class>G.2.2; H.2.8; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Betweenness centrality is a classic measure that quantifies the importance of
a graph element (vertex or edge) according to the fraction of shortest paths
passing through it. This measure is notoriously expensive to compute, and the
best known algorithm runs in O(nm) time. The problems of efficiency and
scalability are exacerbated in a dynamic setting, where the input is an
evolving graph seen edge by edge, and the goal is to keep the betweenness
centrality up to date. In this paper we propose the first truly scalable
algorithm for online computation of betweenness centrality of both vertices and
edges in an evolving graph where new edges are added and existing edges are
removed. Our algorithm is carefully engineered with out-of-core techniques and
tailored for modern parallel stream processing engines that run on clusters of
shared-nothing commodity hardware. Hence, it is amenable to real-world
deployment. We experiment on graphs that are two orders of magnitude larger
than previous studies. Our method is able to keep the betweenness centrality
measures up to date online, i.e., the time to update the measures is smaller
than the inter-arrival time between two consecutive updates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6984</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6984</id><created>2014-01-27</created><authors><author><keyname>Miao</keyname><forenames>Yajie</forenames></author></authors><title>Kaldi+PDNN: Building DNN-based ASR Systems with Kaldi and PDNN</title><categories>cs.LG cs.CL</categories><comments>unpublished manuscript</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The Kaldi toolkit is becoming popular for constructing automated speech
recognition (ASR) systems. Meanwhile, in recent years, deep neural networks
(DNNs) have shown state-of-the-art performance on various ASR tasks. This
document describes our open-source recipes to implement fully-fledged DNN
acoustic modeling using Kaldi and PDNN. PDNN is a lightweight deep learning
toolkit developed under the Theano environment. Using these recipes, we can
build up multiple systems including DNN hybrid systems, convolutional neural
network (CNN) systems and bottleneck feature systems. These recipes are
directly based on the Kaldi Switchboard 110-hour setup. However, adapting them
to new datasets is easy to achieve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6988</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6988</id><created>2014-01-27</created><authors><author><keyname>Portnoi</keyname><forenames>Marcos</forenames></author></authors><title>Criptografia com Curvas El\'ipticas</title><categories>cs.CR</categories><comments>Portuguese, 14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an overview of the use of elliptic curves in
cryptography. The security of this cryptosystem is based on the discrete
logarithm problem, which appears to be much harder compared to the discrete
logarithm problem in other cryptosystems. An overview of common cryptosystems
is given, such as Diffie-Hellman and RSA, and an elliptic curve cryptography
scheme is discussed.
  --------
  Este trabalho apresenta o uso das curvas el\'ipticas em criptografia. Sua
seguran\c{c}a est\'a baseada no problema do logaritmo discreto. Este problema
aparentemente \'e significativamente mais dif\'icil de resolver, comparado com
o problema do logaritmo discreto usado por outros sistemas de criptografia. \'E
dada uma vis\~ao geral de sistemas de criptografia comuns, como Diffie-Hellman
e RSA, e discute-se um esquema de criptografia usando curvas el\'ipticas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7002</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7002</id><created>2014-01-27</created><authors><author><keyname>Rahman</keyname><forenames>Nabila</forenames></author><author><keyname>Wright</keyname><forenames>Matthew</forenames></author><author><keyname>Liu</keyname><forenames>Donggang</forenames></author></authors><title>Fast and energy-efficient technique for jammed region mapping in
  wireless sensor networks</title><categories>cs.NI</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks (WSNs) have great practical importance for
surveillance systems to perform monitoring by acquiring and sending information
on any intrusion in a secured area. Requirement of very little human
intervention is one of the most desirable features of WSNs, thus making it a
cheaper and safer alternative for securing large areas such as international
borders. Jamming attacks in WSNs can be applied to disrupt communications among
the sensor nodes in the network. Since it is difficult to prevent jamming
attacks, detection and mapping out the jammed regions is critical to overcome
this problem. In a security monitoring scenario, the network operators will be
able to take proper measures against jamming once the jammed regions in the
network are known to them. It is also desirable to keep the interactions of the
sensor nodes in the network minimal, as they are low powered devices and need
to conserve their resources. In this paper we propose a light-weight technique
for faster mapping of the jammed regions. We minimize the load on the sensors
by removing the actual responsibility of mapping from the network to the
central base station (BS). After a few nodes report to the BS, it carries out
the task of mapping of the jammed regions in the network. We use our simulation
results to compare our proposed system with the existing techniques and also to
measure the performance of our system. Our results show that the jammed regions
in a network can be mapped from fewer nodes reporting to the base station.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7006</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7006</id><created>2014-01-25</created><updated>2014-04-27</updated><authors><author><keyname>Sahebi</keyname><forenames>Aria G.</forenames></author><author><keyname>Pradhan</keyname><forenames>S. Sandeep</forenames></author></authors><title>Polar Codes for Some Multi-terminal Communications Problems</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1401.6482</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that polar coding schemes achieve the known achievable rate
regions for several multi-terminal communications problems including lossy
distributed source coding, multiple access channels and multiple descriptions
coding. The results are valid for arbitrary alphabet sizes (binary or
nonbinary) and arbitrary distributions (symmetric or asymmetric).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7020</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7020</id><created>2014-01-27</created><updated>2015-02-18</updated><authors><author><keyname>Byrd</keyname><forenames>R. H.</forenames></author><author><keyname>Hansen</keyname><forenames>S. L.</forenames></author><author><keyname>Nocedal</keyname><forenames>J.</forenames></author><author><keyname>Singer</keyname><forenames>Y.</forenames></author></authors><title>A Stochastic Quasi-Newton Method for Large-Scale Optimization</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question of how to incorporate curvature information in stochastic
approximation methods is challenging. The direct application of classical
quasi- Newton updating techniques for deterministic optimization leads to noisy
curvature estimates that have harmful effects on the robustness of the
iteration. In this paper, we propose a stochastic quasi-Newton method that is
efficient, robust and scalable. It employs the classical BFGS update formula in
its limited memory form, and is based on the observation that it is beneficial
to collect curvature information pointwise, and at regular intervals, through
(sub-sampled) Hessian-vector products. This technique differs from the
classical approach that would compute differences of gradients, and where
controlling the quality of the curvature estimates can be difficult. We present
numerical results on problems arising in machine learning that suggest that the
proposed method shows much promise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7034</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7034</id><created>2014-01-27</created><authors><author><keyname>Portnoi</keyname><forenames>Marcos</forenames></author><author><keyname>Martins</keyname><forenames>Joberto S. B.</forenames></author></authors><title>TARVOS - an Event-Based Simulator for Performance Analysis, Supporting
  MPLS, RSVP-TE, and Fast Recovery</title><categories>cs.NI</categories><comments>XIII Brazilian Symposium on Multimedia and the Web - Webmedia 2007,
  Oct. 20-24, 2007, Gramado, RS, Brazil, vol. 1, p. 222-229</comments><journal-ref>XIII Brazilian Symposium on Multimedia and the Web - Webmedia
  2007, Oct. 20-24, 2007, Gramado, RS, Brazil, vol. 1, p. 222-229</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new discrete event-based network simulator named TARVOS
- Computer Networks Simulator, being designed as part of the first Author's
Masters research and will provide support to simulating MPLS architecture,
several RSVP-TE protocol functionalities and fast recovery in case of link
failure. The tool is used in a case study, where the impact of a link failure
on a VoIP application, within an MPLS domain network, is analyzed. The paper
displays a preliminary research of six already available simulators and reasons
why they were not adopted as tools for the Masters research. Then, it follows
to describe the basics of TARVOS implementation and exhibits the case study
simulated by this new tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7042</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7042</id><created>2014-01-27</created><updated>2015-09-01</updated><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Nayyeri</keyname><forenames>Amir</forenames></author><author><keyname>Salavatipour</keyname><forenames>Mohammad</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Anastasios</forenames></author></authors><title>How to Walk Your Dog in the Mountains with No Magic Leash</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a $O(\log n )$-approximation algorithm for computing the
homotopic \Frechet distance between two polygonal curves that lie on the
boundary of a triangulated topological disk. Prior to this work, algorithms
were known only for curves on the Euclidean plane with polygonal obstacles.
  A key technical ingredient in our analysis is a $O(\log n)$-approximation
algorithm for computing the minimum height of a homotopy between two curves. No
algorithms were previously known for approximating this parameter.
Surprisingly, it is not even known if computing either the homotopic \Frechet
distance, or the minimum height of a homotopy, is in NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7043</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7043</id><created>2014-01-27</created><updated>2014-09-19</updated><authors><author><keyname>Mastin</keyname><forenames>Andrew</forenames></author><author><keyname>Jaillet</keyname><forenames>Patrick</forenames></author><author><keyname>Chin</keyname><forenames>Sang</forenames></author></authors><title>Randomized Minmax Regret for Combinatorial Optimization Under
  Uncertainty</title><categories>cs.DM cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minmax regret problem for combinatorial optimization under uncertainty
can be viewed as a zero-sum game played between an optimizing player and an
adversary, where the optimizing player selects a solution and the adversary
selects costs with the intention of maximizing the regret of the player. The
existing minmax regret model considers only deterministic solutions/strategies,
and minmax regret versions of most polynomial solvable problems are NP-hard. In
this paper, we consider a randomized model where the optimizing player selects
a probability distribution (corresponding to a mixed strategy) over solutions
and the adversary selects costs with knowledge of the player's distribution,
but not its realization. We show that under this randomized model, the minmax
regret version of any polynomial solvable combinatorial problem becomes
polynomial solvable. This holds true for both the interval and discrete
scenario representations of uncertainty. Using the randomized model, we show
new proofs of existing approximation algorithms for the deterministic model
based on primal-dual approaches. Finally, we prove that minmax regret problems
are NP-hard under general convex uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7074</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7074</id><created>2014-01-27</created><authors><author><keyname>Sakzad</keyname><forenames>Amin</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author><author><keyname>Boutros</keyname><forenames>Joseph Jean</forenames></author><author><keyname>Hong</keyname><forenames>Yi</forenames></author></authors><title>Phase Precoded Compute-and-Forward with Partial Feedback</title><categories>cs.IT math.IT</categories><comments>5 Pages, 4 figures, submitted to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose phase precoding for the compute-and-forward (CoF)
protocol. We derive the phase precoded computation rate and show that it is
greater than the original computation rate of CoF protocol without precoder. To
maximize the phase precoded computation rate, we need to 'jointly' find the
optimum phase precoding matrix and the corresponding network equation
coefficients. This is a mixed integer programming problem where the optimum
precoders should be obtained at the transmitters and the network equation
coefficients have to be computed at the relays. To solve this problem, we
introduce phase precoded CoF with partial feedback. It is a quantized precoding
system where the relay jointly computes both a quasi-optimal precoder from a
finite codebook and the corresponding network equations. The index of the
obtained phase precoder within the codebook will then be fedback to the
transmitters. A &quot;deep hole phase precoder&quot; is presented as an example of such a
scheme. We further simulate our scheme with a lattice code carved out of the
Gosset lattice and show that significant coding gains can be obtained in terms
of equation error performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7076</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7076</id><created>2014-01-27</created><authors><author><keyname>Berdinsky</keyname><forenames>Dmitry</forenames></author><author><keyname>Kim</keyname><forenames>Tae-wan</forenames></author><author><keyname>Cho</keyname><forenames>Durkbin</forenames></author><author><keyname>Bracco</keyname><forenames>Cesare</forenames></author><author><keyname>Kiatpanichgij</keyname><forenames>Sutipong</forenames></author></authors><title>Bases of T-meshes and the refinement of hierarchical B-splines</title><categories>cs.CG math.NA</categories><doi>10.1016/j.cma.2014.09.023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider spaces of bivariate splines of bi-degree (m, n)
with maximal order of smoothness over domains associated to a two-dimensional
grid. We define admissible classes of domains for which suitable combinatorial
technique allows us to obtain the dimension of such spline spaces and the
number of tensor-product B-splines acting effectively on these domains.
Following the strategy introduced recently by Giannelli and Juettler, these
results enable us to prove that under certain assumptions about the
configuration of a hierarchical T-mesh the hierarchical B-splines form a basis
of bivariate splines of bi-degree (m, n) with maximal order of smoothness over
this hierarchical T-mesh. In addition, we derive a sufficient condition about
the configuration of a hierarchical T-mesh that ensures a weighted partition of
unity property for hierarchical B-splines with only positive weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7077</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7077</id><created>2014-01-27</created><updated>2015-01-23</updated><authors><author><keyname>Febres</keyname><forenames>Gerardo</forenames></author><author><keyname>Jaffe</keyname><forenames>Klaus</forenames></author></authors><title>Quantifying literature quality using complexity criteria</title><categories>cs.CL</categories><comments>Submitted for publication. 29 pages. 8 figures, 4 tables, 4
  appendixes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We measured entropy and symbolic diversity for English and Spanish texts
including literature Nobel laureates and other famous authors. Entropy, symbol
diversity and symbol frequency profiles were compared for these four groups. We
also built a scale sensitive to the quality of writing and evaluated its
relationship with the Flesch's readability index for English and the
Szigriszt's perspicuity index for Spanish. Results suggest a correlation
between entropy and word diversity with quality of writing. Text genre also
influences the resulting entropy and diversity of the text. Results suggest the
plausibility of automated quality assessment of texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7085</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7085</id><created>2014-01-27</created><authors><author><keyname>Huang</keyname><forenames>Wentao</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author></authors><title>Reverse Edge Cut-Set Bounds for Secure Network Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of secure communication over a network in the
presence of wiretappers. We give a new cut-set bound on secrecy capacity which
takes into account the contribution of both forward and backward edges crossing
the cut, and the connectivity between their endpoints in the rest of the
network. We show the bound is tight on a class of networks, which demonstrates
that it is not possible to find a tighter bound by considering only cut set
edges and their connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7088</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7088</id><created>2014-01-28</created><authors><author><keyname>Tabassum</keyname><forenames>Hina</forenames></author><author><keyname>Siddique</keyname><forenames>Uzma</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author><author><keyname>Hossain</keyname><forenames>Md. Jahangir</forenames></author></authors><title>Cellular Downlink Performance with Base Station Sleeping, User
  Association, and Scheduling</title><categories>cs.NI cs.IT math.IT stat.AP</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Base station (BS) sleeping has emerged as a viable solution to enhance the
overall network energy efficiency by inactivating the underutilized BSs.
However, it affects the performance of users in sleeping cells depending on
their BS association criteria, their channel conditions towards the active BSs,
and scheduling criteria and traffic loads at the active BSs. This paper
characterizes the performance of cellular systems with BS sleeping by
developing a systematic framework to derive the spectral efficiency and outage
probability of downlink transmission to the sleeping cell users taking into
account the aforementioned factors. In this context, we develop a user
association scheme in which a typical user in a sleeping cell selects a BS with
\textbf{M}aximum best-case \textbf{M}ean channel \textbf{A}ccess
\textbf{P}robability (MMAP) which is calculated by all active BSs based on
their existing traffic loads. We consider both greedy and round-robin schemes
at active BSs for scheduling users in a channel. Once the association is
performed, the exact access probability for a typical sleeping cell user and
the statistics of its received signal and interference powers are derived to
evaluate the spectral and energy efficiencies of transmission. For the sleeping
cell users, we also consider the conventional \textbf{M}aximum
\textbf{R}eceived \textbf{S}ignal \textbf{P}ower (MRSP)-based user association
scheme along with greedy and round-robin schemes at the BSs. The impact of
cell-zooming is incorporated in the derivations to analyze its feasibility in
reducing the coverage holes created by BS sleeping. Numerical results show the
trade-offs between spectral efficiency and energy efficiency in various network
scenarios. The accuracy of the analysis is verified through Monte-Carlo
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7100</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7100</id><created>2014-01-28</created><authors><author><keyname>Zolfaghari</keyname><forenames>Reza</forenames></author><author><keyname>Epain</keyname><forenames>Nicolas</forenames></author><author><keyname>Jin</keyname><forenames>Craig T.</forenames></author><author><keyname>Glaun&#xe8;s</keyname><forenames>Joan</forenames></author><author><keyname>Tew</keyname><forenames>Anthony</forenames></author></authors><title>Large Deformation Diffeomorphic Metric Mapping And Fast-Multipole
  Boundary Element Method Provide New Insights For Binaural Acoustics</title><categories>cs.CG</categories><comments>Submitted as a conference paper to IEEE ICASSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes how Large Deformation Diffeomorphic Metric Mapping
(LDDMM) can be coupled with a Fast Multipole (FM) Boundary Element Method (BEM)
to investigate the relationship between morphological changes in the head,
torso, and outer ears and their acoustic filtering (described by Head Related
Transfer Functions, HRTFs). The LDDMM technique provides the ability to study
and implement morphological changes in ear, head and torso shapes. The FM-BEM
technique provides numerical simulations of the acoustic properties of an
individual's head, torso, and outer ears. This paper describes the first
application of LDDMM to the study of the relationship between a listener's
morphology and a listener's HRTFs. To demonstrate some of the new capabilities
provided by the coupling of these powerful tools, we examine the classical
question of what it means to ``listen through another individual's outer
ears.'' This work utilizes the data provided by the Sydney York Morphological
and Acoustic Recordings of Ears (SYMARE) database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7104</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7104</id><created>2014-01-28</created><authors><author><keyname>Jaufman</keyname><forenames>Olga</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Acquisition of a Project-Specific Process</title><categories>cs.SE</categories><comments>15 pages. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F11497455_27</comments><journal-ref>Product Focused Software Process Improvement, volume 3547 of
  Lecture Notes in Computer Science, pages 328-342. Springer Berlin Heidelberg,
  2005</journal-ref><doi>10.1007/11497455_27</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, proposed development processes are often considered too generic
for operational use. This often leads to a misunderstanding of the
project-specific processes and its refuse. One reason for non-appropriate
project-specific processes is insufficient support for the tailoring of generic
processes to project characteristics and context constraints. To tackle this
problem, we propose a method for the acquisition of a project-specific process.
This method uses a domain-specific process line for top-down process tailoring
and supports bottom-up refinement of the defined generic process based on
tracking process activities. The expected advantage of the method is tailoring
efficiency gained by usage of a process line and higher process adherence
gained by bottom-up adaptation of the process. The work described was conducted
in the automotive domain. This article presents an overview of the so-called
Emergent Process Acquisition method (EPAc) and sketches an initial validation
study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7110</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7110</id><created>2014-01-28</created><authors><author><keyname>Cao</keyname><forenames>Zhengjun</forenames></author><author><keyname>Liu</keyname><forenames>Lihua</forenames></author></authors><title>A Fast String Matching Algorithm Based on Lowlight Characters in the
  Pattern</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We put forth a new string matching algorithm which matches the pattern from
neither the left nor the right end, instead a special position. Comparing with
the Knuth-Morris-Pratt algorithm and the Boyer-Moore algorithm, the new
algorithm is more flexible to pick the position for starting comparisons. The
option really brings it a saving in cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7114</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7114</id><created>2014-01-28</created><updated>2014-10-10</updated><authors><author><keyname>Nam</keyname><forenames>Junyoung</forenames></author></authors><title>Fundamental Limits in Correlated Fading MIMO Broadcast Channels:
  Benefits of Transmit Correlation Diversity</title><categories>cs.IT math.IT</categories><comments>29 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate asymptotic capacity limits of the Gaussian MIMO broadcast
channel (BC) with spatially correlated fading to understand when and how much
transmit correlation helps the capacity. By imposing a structure on channel
covariances (equivalently, transmit correlations at the transmitter side) of
users, also referred to as \emph{transmit correlation diversity}, the impact of
transmit correlation on the power gain of MIMO BCs is characterized in several
regimes of system parameters, with a particular interest in the large-scale
array (or massive MIMO) regime. Taking the cost for downlink training into
account, we provide asymptotic capacity bounds of multiuser MIMO downlink
systems to see how transmit correlation diversity affects the system
multiplexing gain. We make use of the notion of joint spatial division and
multiplexing (JSDM) to derive the capacity bounds. It is advocated in this
paper that transmit correlation diversity may be of use to significantly
increase multiplexing gain as well as power gain in multiuser MIMO systems. In
particular, the new type of diversity in wireless communications is shown to
improve the system multiplexing gain up to by a factor of the number of degrees
of such diversity. Finally, performance limits of conventional large-scale MIMO
systems not exploiting transmit correlation are also characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7116</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7116</id><created>2014-01-28</created><authors><author><keyname>Barron</keyname><forenames>Andrew</forenames></author><author><keyname>Roos</keyname><forenames>Teemu</forenames></author><author><keyname>Watanabe</keyname><forenames>Kazuho</forenames></author></authors><title>Bayesian Properties of Normalized Maximum Likelihood and its Fast
  Computation</title><categories>cs.IT cs.LG math.IT stat.ML</categories><comments>Submitted to ISIT-2004 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The normalized maximized likelihood (NML) provides the minimax regret
solution in universal data compression, gambling, and prediction, and it plays
an essential role in the minimum description length (MDL) method of statistical
modeling and estimation. Here we show that the normalized maximum likelihood
has a Bayes-like representation as a mixture of the component models, even in
finite samples, though the weights of linear combination may be both positive
and negative. This representation addresses in part the relationship between
MDL and Bayes modeling. This representation has the advantage of speeding the
calculation of marginals and conditionals required for coding and prediction
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7129</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7129</id><created>2014-01-28</created><authors><author><keyname>Murthy</keyname><forenames>Garimella Rama</forenames></author></authors><title>Towards a Resolution of P = NP Conjecture</title><categories>cs.NA</categories><comments>15 pages. arXiv admin note: substantial text overlap with
  arXiv:1207.0634</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research paper, the problem of optimization of a quadratic form over
the convex hull generated by the corners of hypercube is attempted and solved.
It is reasoned that under some conditions, the optimum occurs at the corners of
hypercube. Results related to the computation of global optimum stable state
(an NP hard problem) are discussed. An algorithm is proposed. It is hoped that
the results shed light on resolving the P not equal to NP problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7134</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7134</id><created>2014-01-28</created><authors><author><keyname>Trillingsgaard</keyname><forenames>Kasper Fl&#xf8;e</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Block-Fading Channels with Delayed CSIT at Finite Blocklength</title><categories>cs.IT math.IT</categories><comments>Extended version of a paper submitted to ISIT'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many wireless systems, the channel state information at the transmitter
(CSIT) can not be learned until after a transmission has taken place and is
thereby outdated. In this paper, we study the benefits of delayed CSIT on a
block-fading channel at finite blocklength. First, the achievable rates of a
family of codes that allows the number of codewords to expand during
transmission, based on delayed CSIT, are characterized. A fixed-length and a
variable-length characterization of the rates are provided using the dependency
testing bound and the variable-length setting introduced by Polyanskiy et al.
Next, a communication protocol based on codes with expandable message space is
put forth, and numerically, it is shown that higher rates are achievable
compared to coding strategies that do not benefit from delayed CSIT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7141</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7141</id><created>2014-01-28</created><authors><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Lu</keyname><forenames>Xiao</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author></authors><title>Adaptive Power Management for Wireless Base Station in Smart Grid
  Environment</title><categories>cs.NI</categories><comments>IEEE Wireless Communication (17 pages, 6 figures.)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing concerns of a global environmental change raises a revolution on
the way of utilizing energy. In wireless industry, green wireless
communications has recently gained increasing attention and is expected to play
a major role in reduction of electrical power consumption. In particular,
actions to promote energy saving of wireless communications with regard to
environmental protection are becoming imperative. To this purpose, we study a
green communication system model where wireless base station is provisioned
with a combination of renewable power source and electrical grid to minimize
power consumption as well as meeting the users' demand. More specifically, we
focus on an adaptive power management for wireless base station to minimize
power consumption under various uncertainties including renewable power
generation, power price, and wireless traffic load. We believe that demand side
power management solution based on the studied communication architecture is a
major step towards green wireless communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7146</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7146</id><created>2014-01-28</created><authors><author><keyname>Lu</keyname><forenames>Xiao</forenames></author><author><keyname>Zhang</keyname><forenames>Ke</forenames></author><author><keyname>Foh</keyname><forenames>Chuan Heng</forenames></author><author><keyname>Fu</keyname><forenames>Cheng Peng</forenames></author></authors><title>SSthreshless Start: A Sender-Side TCP Intelligence for Long Fat Network</title><categories>cs.NI</categories><comments>25 pages, 10 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measurement shows that 85% of TCP flows in the internet are short-lived flows
that stay most of their operation in the TCP startup phase. However, many
previous studies indicate that the traditional TCP Slow Start algorithm does
not perform well, especially in long fat networks. Two obvious problems are
known to impact the Slow Start performance, which are the blind initial setting
of the Slow Start threshold and the aggressive increase of the probing rate
during the startup phase regardless of the buffer sizes along the path. Current
efforts focusing on tuning the Slow Start threshold and/or probing rate during
the startup phase have not been considered very effective, which has prompted
an investigation with a different approach. In this paper, we present a novel
TCP startup method, called threshold-less slow start or SSthreshless Start,
which does not need the Slow Start threshold to operate. Instead, SSthreshless
Start uses the backlog status at bottleneck buffer to adaptively adjust probing
rate which allows better seizing of the available bandwidth. Comparing to the
traditional and other major modified startup methods, our simulation results
show that SSthreshless Start achieves significant performance improvement
during the startup phase. Moreover, SSthreshless Start scales well with a wide
range of buffer size, propagation delay and network bandwidth. Besides, it
shows excellent friendliness when operating simultaneously with the currently
popular TCP NewReno connections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7148</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7148</id><created>2014-01-28</created><authors><author><keyname>Drugarin</keyname><forenames>Anghel</forenames></author><author><keyname>Victoria</keyname><forenames>Cornelia</forenames></author></authors><title>A Software Design through Electrical System for a Building</title><categories>cs.OH</categories><comments>Politehnica Timisoara Press, nov.2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer aided design of lighting systems made new installations of lighting
dimensioning and verification of existing lighting systems for both indoor and
outdoor lighting systems.The design of the building light system was in a
dedicated software, named DiaLux, version 4.11.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7161</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7161</id><created>2014-01-28</created><updated>2015-04-20</updated><authors><author><keyname>Jang</keyname><forenames>Hwanchol</forenames></author><author><keyname>Nooshabadi</keyname><forenames>Saeid</forenames></author><author><keyname>Kim</keyname><forenames>Kiseon</forenames></author><author><keyname>Lee</keyname><forenames>Heung-No</forenames></author></authors><title>Complex Valued Sphere Decoding with Element-wise Selective Prescreening
  for General Two-dimensional Signal Constellations</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sphere decoding (SD) is a promising detection strategy for multiple-input
multiple-output (MIMO) systems because it can achieve maximum-likelihood (ML)
detection performance with a reasonable complexity. The standard and most SD
algorithms operate on real valued systems. These real valued SDs (RV-SDs) are
known to be applicable to MIMO communication systems with only rectangular QAM
signal constellations. In addition to this restriction on applicable
constellations, RV-SDs are not suitable for VLSI implementations. Complex
valued SD (CV-SD) is a good SD candidate for its flexibility on the choice of
constellations and its efficiency in VLSI implementations. But, the low
complexity CV-SD algorithm for general two dimensional (2D) constellations is
not available, especially with the one that attains the ML performance. In this
paper, we present a low complexity CV-SD algorithm, referred to as Circular
Sphere Decoding (CSD) which is applicable to arbitrary 2D constellations. CSD
provides a new constraint test. This constraint test is carefully designed so
that the element-wise dependency is removed in the metric computation for the
test. As a result, the constraint test becomes simple to perform without
restriction on its constellation structure. By additionally employing this
simple test as a prescreening test, CSD reduces the complexity of the CV-SD
search. We show that the complexity reduction is significant while its ML
performance is not compromised. We also provide a powerful tool to estimate the
pruning capacity of any particular search tree. Using this tool, we propose the
Predict-And-Change (PAC) strategy which leads to a further considerable
complexity reduction in CSD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7169</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7169</id><created>2014-01-28</created><updated>2015-05-18</updated><authors><author><keyname>Erseghe</keyname><forenames>Tomaso</forenames></author></authors><title>On the Evaluation of the Polyanskiy-Poor-Verdu Converse Bound for Finite
  Blocklength Coding in AWGN</title><categories>cs.IT math.IT</categories><comments>12 pages, 10 figures, submitted to IEEE Transactions on Information
  Theory</comments><journal-ref>IEEE Transactions on Information Theory, Vol. 61, No. 12, pp.
  6578-6590, December 2015</journal-ref><doi>10.1109/TIT.2015.2494061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A tight converse bound to channel coding rate in the finite block-length
regime and under AWGN conditions was recently proposed by Polyanskiy, Poor, and
Verdu (PPV). The bound is a generalization of a number of other classical
results, and it was also claimed to be equivalent to Shannon's 1959 cone
packing bound. Unfortunately, its numerical evaluation is troublesome even for
not too large values of the block-length n. In this paper we tackle the
numerical evaluation by compactly expressing the PPV converse bound in terms of
non-central chi-squared distributions, and by evaluating those through a an
integral expression and a corresponding series expansion which exploit a method
proposed by Temme. As a result, a robust evaluation method and new insights on
the bound's asymptotics, as well as new approximate expressions, are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7171</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7171</id><created>2014-01-28</created><updated>2014-05-16</updated><authors><author><keyname>Katoen</keyname><forenames>Joost-Pieter</forenames></author><author><keyname>Song</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author></authors><title>Probably Safe or Live</title><categories>cs.LO</categories><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a formal characterisation of safety and liveness
properties \`a la Alpern and Schneider for fully probabilistic systems. As for
the classical setting, it is established that any (probabilistic tree) property
is equivalent to a conjunction of a safety and liveness property. A simple
algorithm is provided to obtain such property decomposition for flat
probabilistic CTL (PCTL). A safe fragment of PCTL is identified that provides a
sound and complete characterisation of safety properties. For liveness
properties, we provide two PCTL fragments, a sound and a complete one. We show
that safety properties only have finite counterexamples, whereas liveness
properties have none. We compare our characterisation for qualitative
properties with the one for branching time properties by Manolios and Trefler,
and present sound and complete PCTL fragments for characterising the notions of
strong safety and absolute liveness coined by Sistla.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7188</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7188</id><created>2014-01-28</created><authors><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author><author><keyname>Coon</keyname><forenames>Justin P.</forenames></author></authors><title>Network Connectivity: Stochastic vs. Deterministic Wireless Channels</title><categories>cs.NI cs.IT math.IT</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the effect of stochastic wireless channel models on the connectivity
of ad hoc networks. Unlike in the deterministic geometric disk model where
nodes connect if they are within a certain distance from each other, stochastic
models attempt to capture small-scale fading effects due to shadowing and
multipath received signals. Through analysis of local and global network
observables, we present conclusive evidence suggesting that network behaviour
is highly dependent upon whether a stochastic or deterministic connection model
is employed. Specifically we show that the network mean degree is lower
(higher) for stochastic wireless channels than for deterministic ones, if the
path loss exponent is greater (lesser) than the spatial dimension. Similarly,
the probability of forming isolated pairs of nodes in an otherwise dense random
network is much less for stochastic wireless channels than for deterministic
ones. The latter realisation explains why the upper bound of $k$-connectivity
is tighter for stochastic wireless channels. We obtain closed form analytic
results and compare to extensive numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7191</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7191</id><created>2014-01-28</created><authors><author><keyname>Dodig-Crnkovic</keyname><forenames>Gordana</forenames></author></authors><title>Modeling Life as Cognitive Info-Computation</title><categories>cs.OH</categories><comments>Manuscript submitted to Computability in Europe CiE 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a naturalist approach to cognition understood as a
network of info-computational, autopoietic processes in living systems. It
provides a conceptual framework for the unified view of cognition as evolved
from the simplest to the most complex organisms, based on new empirical and
theoretical results. It addresses three fundamental questions: what cognition
is, how cognition works and what cognition does at different levels of
complexity of living organisms. By explicating the info-computational character
of cognition, its evolution, agent-dependency and generative mechanisms we can
better understand its life-sustaining and life-propagating role. The
info-computational approach contributes to rethinking cognition as a process of
natural computation in living beings that can be applied for cognitive
computation in artificial systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7193</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7193</id><created>2014-01-28</created><authors><author><keyname>Iorio</keyname><forenames>Antony W.</forenames></author><author><keyname>Abbass</keyname><forenames>Hussein A.</forenames></author><author><keyname>Gaidow</keyname><forenames>Svetoslav</forenames></author><author><keyname>Bender</keyname><forenames>Axel</forenames></author></authors><title>Visualizing Cognitive Moves for Assessing Information Perception Biases
  in Decision Making</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In decision making a key source of uncertainty is people's perception of
information which is influenced by their attitudes toward risk. Both,
perception of information and risk attitude, affect the interpretation of
information and hence the choice of suitable courses of action in a variety of
contexts ranging from project planning to military operations. Visualization
associated with the dynamics of cognitive states of people processing
information and making decision is therefore not only important for analysis
but has also significant practical applications, in particular in the military
command and control domain. In this paper, we focus on a major concept that
affect human cognition in this context: reliability of information. We
introduce Cognitive Move Diagrams (CMD)---a simple visualization tool---to
represent and evaluate the impact of this concept on decision making. We
demonstrate through both a hypothetical example and a subject matter expert
based experiment that CMD are effective in visualizing, detecting and
qualifying human biases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7216</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7216</id><created>2014-01-28</created><authors><author><keyname>Riera-Palou</keyname><forenames>Felip</forenames></author></authors><title>Reconfigurable Structures for Direct Equalisation in Mobile Receivers</title><categories>cs.IT math.IT</categories><comments>PhD Thesis, University of Bradford (UK), 2002</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any communication channel will usually distort the transmitted signal. This
is especially true in the case of mobile systems, where multipath propagation
causes the received signal to be seriously degraded. Over the years, many
techniques have been proposed to combat channel effects. Two of the most
popular are linear equalisation (LE) and decision feedback equalisation (DFE).
These methods offer a good compromise between performance and computational
complexity. LE and DFE are implemented using finite impulse response (FIR)
filters whose frequency spectrum approximates the inverse of the channel
spectrum plus noise. In mobile systems, the equaliser is made adaptable in
order to be able to respond to the channel variations. Adaptability is achieved
using adaptive FIR filters whose coefficients are iteratively updated. In
principle, an infinite number of filter coefficients would be needed to achieve
perfect channel inversion. In practice, the number of taps must be finite.
Simulations show that, in realistic scenarios, making the equaliser longer than
a certain (undetermined) number of taps will not yield any benefit. Moreover,
computation and power will be wasted. In battery powered devices, like mobile
terminals, it would be desirable to have the equaliser properly dimensioned.
The equaliser's optimum length strongly depends on the particular scenario, and
as channel conditions vary, this optimum is likely to vary. This thesis
presents novel techniques to perform equaliser length adjustment. Methods for
the LE and the DFE have been developed. Simulations in many different scenarios
show that the proposed schemes optimise the number of taps to be used.
Moreover, these techniques are able to detect changes in the channel and
re-adjust the equaliser length appropriately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7220</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7220</id><created>2014-01-28</created><updated>2014-11-09</updated><authors><author><keyname>Marsden</keyname><forenames>Daniel</forenames></author></authors><title>Category Theory Using String Diagrams</title><categories>math.CT cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In work of Fokkinga and Meertens a calculational approach to category theory
is developed. The scheme has many merits, but sacrifices useful type
information in the move to an equational style of reasoning. By contrast,
traditional proofs by diagram pasting retain the vital type information, but
poorly express the reasoning and development of categorical proofs. In order to
combine the strengths of these two perspectives, we propose the use of string
diagrams, common folklore in the category theory community, allowing us to
retain the type information whilst pursuing a calculational form of proof.
These graphical representations provide a topological perspective on
categorical proofs, and silently handle functoriality and naturality conditions
that require awkward bookkeeping in more traditional notation.
  Our approach is to proceed primarily by example, systematically applying
graphical techniques to many aspects of category theory. We develop string
diagrammatic formulations of many common notions, including adjunctions,
monads, Kan extensions, limits and colimits. We describe representable functors
graphically, and exploit these as a uniform source of graphical calculation
rules for many category theoretic concepts. These graphical tools are then used
to explicitly prove many standard results in our proposed diagrammatic style.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7227</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7227</id><created>2014-01-28</created><updated>2014-02-06</updated><authors><author><keyname>Jackson</keyname><forenames>Haran</forenames></author><author><keyname>Taroni</keyname><forenames>Michele</forenames></author><author><keyname>Ponting</keyname><forenames>David</forenames></author></authors><title>A Two-Level Variant of Additive Schwarz Preconditioning for Use in
  Reservoir Simulation</title><categories>math.NA cs.NA</categories><comments>Submitted to Numerical Linear Algebra with Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computation time for reservoir simulation is dominated by the linear
solver. The sets of linear equations which arise in reservoir simulation have
two distinctive features: the problems are usually highly anisotropic, with a
dominant vertical flow direction, and the commonly used fully implicit method
requires a simultaneous solution for pressure and saturation or molar
concentration variables. These variables behave quite differently, with the
pressure feeling long-range effects while the saturations vary locally. In this
paper we review preconditioned iterative methods used for solving the linear
system equations in reservoir simulation and their parallelisation. We then
propose a variant of the classical additive Schwarz preconditioner designed to
achieve better results on a large number of processors and discuss some
directions for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7229</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7229</id><created>2014-01-28</created><updated>2014-08-27</updated><authors><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Yuan</keyname><forenames>Xiaojun</forenames></author></authors><title>MIMO Multiway Relaying with Pairwise Data Exchange: A Degrees of Freedom
  Perspective</title><categories>cs.IT math.IT</categories><comments>13 pages, 7 figures</comments><doi>10.1109/TSP.2014.2347924</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we study achievable degrees of freedom (DoF) of a
multiple-input multiple-output (MIMO) multiway relay channel (mRC) where $K$
users, each equipped with $M$ antennas, exchange messages in a pairwise manner
via a common $N$-antenna relay node. % A novel and systematic way of joint
beamforming design at the users and at the relay is proposed to align signals
for efficient implementation of physical-layer network coding (PNC). It is
shown that, when the user number $K=3$, the proposed beamforming design can
achieve the DoF capacity of the considered mRC for any $(M,N)$ setups. % For
the scenarios with $K&gt;3$, we show that the proposed signaling scheme can be
improved by disabling a portion of relay antennas so as to align signals more
efficiently. Our analysis reveals that the obtained achievable DoF is always
piecewise linear, and is bounded either by the number of user antennas $M$ or
by the number of relay antennas $N$. Further, we show that the DoF capacity can
be achieved for $\frac{M}{N} \in \left(0,\frac{K-1}{K(K-2)} \right]$ and
$\frac{M}{N} \in \left[\frac{1}{K(K-1)}+\frac{1}{2},\infty \right)$, which
provides a broader range of the DoF capacity than the existing results.
Asymptotic DoF as $K\rightarrow \infty$ is also derived based on the proposed
signaling scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7233</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7233</id><created>2014-01-28</created><updated>2014-02-13</updated><authors><author><keyname>Stopczynski</keyname><forenames>Arkadiusz</forenames></author><author><keyname>Sekara</keyname><forenames>Vedran</forenames></author><author><keyname>Sapiezynski</keyname><forenames>Piotr</forenames></author><author><keyname>Cuttone</keyname><forenames>Andrea</forenames></author><author><keyname>Madsen</keyname><forenames>Mette My</forenames></author><author><keyname>Larsen</keyname><forenames>Jakob Eg</forenames></author><author><keyname>Lehmann</keyname><forenames>Sune</forenames></author></authors><title>Measuring large-scale social networks with high resolution</title><categories>cs.SI physics.soc-ph</categories><doi>10.1371/journal.pone.0095978</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the deployment of a large-scale study designed to
measure human interactions across a variety of communication channels, with
high temporal resolution and spanning multiple years - the Copenhagen Networks
Study. Specifically, we collect data on face-to-face interactions,
telecommunication, social networks, location, and background information
(personality, demographic, health, politics) for a densely connected population
of 1,000 individuals, using state-of-art smartphones as social sensors. Here we
provide an overview of the related work and describe the motivation and
research agenda driving the study. Additionally the paper details the
data-types measured, and the technical infrastructure in terms of both backend
and phone software, as well as an outline of the deployment procedures. We
document the participant privacy procedures and their underlying principles.
The paper is concluded with early results from data analysis, illustrating the
importance of multi-channel high-resolution approach to data collection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7234</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7234</id><created>2014-01-28</created><authors><author><keyname>Teheux</keyname><forenames>Bruno</forenames></author></authors><title>Propositional dynamic logic for searching games with errors</title><categories>cs.LO</categories><msc-class>03B45, 03B70, 03B50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate some finitely-valued generalizations of propositional dynamic
logic with tests. We start by introducing the (n+1)-valued Kripke models and a
corresponding language based on a modal extension of {\L}ukasiewicz many-valued
logic. We illustrate the definitions by providing a framework for an analysis
of the R\'enyi - Ulam searching game with errors.
  Our main result is the axiomatization of the theory of the (n+1)-valued
Kripke models. This result is obtained through filtration of the canonical
model of the smallest (n+1)-valued propositional dynamic logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7239</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7239</id><created>2014-01-28</created><authors><author><keyname>Sheble</keyname><forenames>Laura</forenames></author><author><keyname>Chen</keyname><forenames>Annie T.</forenames></author></authors><title>Contexts of diffusion: Adoption of research synthesis in Social Work and
  Women's Studies</title><categories>cs.SI cs.DL physics.soc-ph</categories><comments>To appear in proceedings of the 2014 International Conference on
  Social Computing, Behavioral-Cultural Modeling, and Prediction (SBP2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Texts reveal the subjects of interest in research fields, and the values,
beliefs, and practices of researchers. In this study, texts are examined
through bibliometric mapping and topic modeling to provide a birds eye view of
the social dynamics associated with the diffusion of research synthesis methods
in the contexts of Social Work and Women's Studies. Research synthesis texts
are especially revealing because the methods, which include meta-analysis and
systematic review, are reliant on the availability of past research and data,
sometimes idealized as objective, egalitarian approaches to research
evaluation, fundamentally tied to past research practices, and performed with
the goal informing future research and practice. This study highlights the
co-influence of past and subsequent research within research fields;
illustrates dynamics of the diffusion process; and provides insight into the
cultural contexts of research in Social Work and Women's Studies. This study
suggests the potential to further develop bibliometric mapping and topic
modeling techniques to inform research problem selection and resource
allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7249</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7249</id><created>2014-01-24</created><authors><author><keyname>Khan</keyname><forenames>Atif Ali</forenames></author><author><keyname>Naseer</keyname><forenames>Oumair</forenames></author><author><keyname>Iliescu</keyname><forenames>Daciana</forenames></author><author><keyname>Hines</keyname><forenames>Evor</forenames></author></authors><title>Fuzzy Controller Design for Assisted Omni-Directional Treadmill Therapy</title><categories>cs.AI</categories><comments>Presented at: &quot;The International Conference on Soft Computing and
  Software Engineering (SCSE 2013)&quot; at San Francisco State University at
  Downtown Campus, in San Francisco, California, USA, March 1-2, 2013</comments><journal-ref>The International Journal of Soft Computing and Software
  Engineering [JSCSE], Vol. 3, No. 3, pp. 30-37, 2013</journal-ref><doi>10.7321/jscse.v3.n3.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the defining characteristic of human being is their ability to walk
upright. Loss or restriction of such ability whether due to the accident, spine
problem, stroke or other neurological injuries can cause tremendous stress on
the patients and hence will contribute negatively to their quality of life.
Modern research shows that physical exercise is very important for maintaining
physical fitness and adopting a healthier life style. In modern days treadmill
is widely used for physical exercises and training which enables the user to
set up an exercise regime that can be adhered to irrespective of the weather
conditions. Among the users of treadmills today are medical facilities such as
hospitals, rehabilitation centres, medical and physiotherapy clinics etc. The
process of assisted training or doing rehabilitation exercise through treadmill
is referred to as treadmill therapy. A modern treadmill is an automated machine
having built in functions and predefined features. Most of the treadmills used
today are one dimensional and user can only walk in one direction. This paper
presents the idea of using omnidirectional treadmills which will be more
appealing to the patients as they can walk in any direction, hence encouraging
them to do exercises more frequently. This paper proposes a fuzzy control
design and possible implementation strategy to assist patients in treadmill
therapy. By intelligently controlling the safety belt attached to the treadmill
user, one can help them steering left, right or in any direction. The use of
intelligent treadmill therapy can help patients to improve their walking
ability without being continuously supervised by the specialists. The patients
can walk freely within a limited space and the support system will provide
continuous evaluation of their position and can adjust the control parameters
of treadmill accordingly to provide best possible assistance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7261</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7261</id><created>2014-01-28</created><authors><author><keyname>Kazemi</keyname><forenames>Mohammad</forenames></author><author><keyname>Hashemgeloogerdi</keyname><forenames>Sahar</forenames></author></authors><title>On the Cooperative Communication over Cognitive Interference Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of communication over cognitive
interference channel (CIC) with partially cooperating (PC) destinations
(CIC-PC). This channel consists of two source nodes communicating two
independent messages to their corresponding destination nodes. One of the
sources, referred to as the cognitive source, has a noncausal knowledge of the
message of the other source, referred to as the primary source. Each
destination is assumed to decode only its intended message. In addition, the
destination corresponding to the cognitive source assists the other destination
by transmitting cooperative information through a relay link. We derive a new
upper bound on the capacity region of discrete memoryless CI-CPC. Moreover, we
characterize the capacity region for two new classes of this channel: (1)
degraded CIC-PC, and (2) a class of semideterministic CIC-PC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7262</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7262</id><created>2014-01-28</created><authors><author><keyname>Hefnawy</keyname><forenames>Marwa El</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>Impact of Spectrum Sharing on the Efficiency of Faster-Than-Nyquist
  Signaling</title><categories>cs.IT math.IT</categories><comments>IEEE copyrights notice applies. This paper is accepted at WCNC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Capacity computations are presented for Faster-Than-Nyquist (FTN) signaling
in the presence of interference from neighboring frequency bands. It is shown
that Shannon's sinc pulses maximize the spectral efficiency for a multi-access
channel, where spectral efficiency is defined as the sum rate in bits per
second per Hertz. Comparisons using root raised cosine pulses show that the
spectral efficiency decreases monotonically with the roll-off factor. At high
signal-to-noise ratio, these pulses have an additive gap to capacity that
increases monotonically with the roll-off factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7263</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7263</id><created>2014-01-28</created><authors><author><keyname>Bradley</keyname><forenames>William F.</forenames></author></authors><title>Superconcentration on a Pair of Butterflies</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose we concatenate two directed graphs, each isomorphic to a $d$
dimensional butterfly (but not necessarily identical to each other). Select any
set of $2^k$ input and $2^k$ output nodes on the resulting graph. Then there
exist node disjoint paths from the input nodes to the output nodes. If we take
two standard butterflies and permute the order of the layers, then the result
holds on sets of any size, not just powers of two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7267</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7267</id><created>2014-01-28</created><authors><author><keyname>Yang</keyname><forenames>Jaewon</forenames></author><author><keyname>McAuley</keyname><forenames>Julian</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Community Detection in Networks with Node Attributes</title><categories>cs.SI physics.soc-ph</categories><comments>Published in the proceedings of IEEE ICDM '13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection algorithms are fundamental tools that allow us to uncover
organizational principles in networks. When detecting communities, there are
two possible sources of information one can use: the network structure, and the
features and attributes of nodes. Even though communities form around nodes
that have common edges and common attributes, typically, algorithms have only
focused on one of these two data modalities: community detection algorithms
traditionally focus only on the network structure, while clustering algorithms
mostly consider only node attributes. In this paper, we develop Communities
from Edge Structure and Node Attributes (CESNA), an accurate and scalable
algorithm for detecting overlapping communities in networks with node
attributes. CESNA statistically models the interaction between the network
structure and the node attributes, which leads to more accurate community
detection as well as improved robustness in the presence of noise in the
network structure. CESNA has a linear runtime in the network size and is able
to process networks an order of magnitude larger than comparable approaches.
Last, CESNA also helps with the interpretation of detected communities by
finding relevant node attributes for each community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7273</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7273</id><created>2014-01-28</created><authors><author><keyname>Al-Bashabsheh</keyname><forenames>Ali</forenames></author><author><keyname>Mao</keyname><forenames>Yongyi</forenames></author></authors><title>On Stochastic Estimation of Partition Function</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show analytically that the duality of normal factor graphs
(NFG) can facilitate stochastic estimation of partition functions. In
particular, our analysis suggests that for the $q-$ary two-dimensional
nearest-neighbor Potts model, sampling from the primal NFG of the model and
sampling from its dual exhibit opposite behaviours with respect to the
temperature of the model. For high-temperature models, sampling from the primal
NFG gives rise to better estimators whereas for low-temperature models,
sampling from the dual gives rise to better estimators. This analysis is
validated by experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7284</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7284</id><created>2014-01-28</created><updated>2015-06-09</updated><authors><author><keyname>Bansal</keyname><forenames>Nikhil</forenames></author><author><keyname>Kulkarni</keyname><forenames>Janardhan</forenames></author></authors><title>Minimizing Flow-Time on Unrelated Machines</title><categories>cs.DS</categories><comments>The new version fixes some typos in the previous version. The paper
  is accepted for publication in STOC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider some flow-time minimization problems in the unrelated machines
setting. In this setting, there is a set of $m$ machines and a set of $n$ jobs,
and each job $j$ has a machine dependent processing time of $p_{ij}$ on machine
$i$. The flow-time of a job is the total time the job spends in the system
(completion time minus its arrival time), and is one of the most natural
quality of service measure. We show the following two results: an
$O(\min(\log^2 n,\log n \log P))$ approximation algorithm for minimizing the
total-flow time, and an $O(\log n)$ approximation for minimizing the maximum
flow-time. Here $P$ is the ratio of maximum to minimum job size. These are the
first known poly-logarithmic guarantees for both the problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7288</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7288</id><created>2014-01-28</created><authors><author><keyname>Sakata</keyname><forenames>Kosuke</forenames></author><author><keyname>Kasai</keyname><forenames>Kenta</forenames></author><author><keyname>Sakaniwa</keyname><forenames>Kohichi</forenames></author></authors><title>Spatially-Coupled Precoded Rateless Codes with Bounded Degree Achieve
  the Capacity of BEC under BP decoding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Raptor codes are known as precoded rateless codes that achieve the capacity
of BEC. However the maximum degree of Raptor codes needs to be unbounded to
achieve the capacity. In this paper, we prove that spatially-coupled precoded
rateless codes achieve the capacity with bounded degree under BP decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7289</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7289</id><created>2014-01-28</created><authors><author><keyname>Okazaki</keyname><forenames>Takuya</forenames></author><author><keyname>Kasai</keyname><forenames>Kenta</forenames></author></authors><title>Spatially-Coupled MacKay-Neal Codes with No Bit Nodes of Degree Two
  Achieve the Capacity of BEC</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Obata et al. proved that spatially-coupled (SC) MacKay-Neal (MN) codes
achieve the capacity of BEC. However, the SC-MN codes codes have many variable
nodes of degree two and have higher error floors. In this paper, we prove that
SC-MN codes with no variable nodes of degree two achieve the capacity of BEC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7290</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7290</id><created>2014-01-28</created><authors><author><keyname>Tazoe</keyname><forenames>Koji</forenames></author><author><keyname>Kasai</keyname><forenames>Kenta</forenames></author><author><keyname>Sakaniwa</keyname><forenames>Kohichi</forenames></author></authors><title>Non-Binary LDPC Codes with Large Alphabet Size</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study LDPC codes for the channel with input ${x}\in \mathbb{F}_q^m$ and
output ${y}={x}+{z}\in \mathbb{F}_q^m$. The aim of this paper is to evaluate
decoding performance of $q^m$-ary non-binary LDPC codes for large $m$. We give
density evolution and decoding performance evaluation for regular non-binary
LDPC codes and spatially-coupled (SC) codes. We show the regular codes do not
achieve the capacity of the channel while SC codes do.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7293</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7293</id><created>2014-01-28</created><authors><author><keyname>Wang</keyname><forenames>Lele</forenames></author><author><keyname>Sasoglu</keyname><forenames>Eren</forenames></author></authors><title>Polar coding for interference networks</title><categories>cs.IT math.IT</categories><comments>Shorter version submitted to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A polar coding scheme for interference networks is introduced. The scheme
combines Arikan's monotone chain rules for multiple-access channels and a
method by Hassani and Urbanke to 'align' two incompatible polarization
processes. It achieves the Han--Kobayashi inner bound for two-user interference
channels and generalizes to interference networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7294</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7294</id><created>2014-01-28</created><updated>2015-03-19</updated><authors><author><keyname>Kolomenskiy</keyname><forenames>Dmitry</forenames></author><author><keyname>Nave</keyname><forenames>Jean-Christophe</forenames></author><author><keyname>Schneider</keyname><forenames>Kai</forenames></author></authors><title>Adaptive gradient-augmented level set method with multiresolution error
  estimation</title><categories>physics.comp-ph cs.NA math.NA</categories><msc-class>35L65, 35Q35, 65M25, 65M50</msc-class><doi>10.1007/s10915-015-0014-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A space-time adaptive scheme is presented for solving advection equations in
two space dimensions. The gradient-augmented level set method using a
semi-Lagrangian formulation with backward time integration is coupled with a
point value multiresolution analysis using Hermite interpolation. Thus locally
refined dyadic spatial grids are introduced which are efficiently implemented
with dynamic quadtree data structures. For adaptive time integration, an
embedded Runge-Kutta method is employed. The precision of the new fully
adaptive method is analysed and speed up of CPU time and memory compression
with respect to the uniform grid discretization are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7304</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7304</id><created>2014-01-28</created><authors><author><keyname>Nithyanand</keyname><forenames>Rishab</forenames></author><author><keyname>Toohill</keyname><forenames>Jonathan</forenames></author><author><keyname>Johnson</keyname><forenames>Rob</forenames></author></authors><title>How Best to Handle a Dicey Situation</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the {Destructive Object Handling} (DOH) problem, which models
aspects of many real-world allocation problems, such as shipping explosive
munitions, scheduling processes in a cluster with fragile nodes, re-using
passwords across multiple websites, and quarantining patients during a disease
outbreak. In these problems, objects must be assigned to handlers, but each
object has a probability of destroying itself and all the other objects
allocated to the same handler. The goal is to maximize the expected value of
the objects handled successfully.
  We show that finding the optimal allocation is
$\mathsf{NP}$-$\mathsf{complete}$, even if all the handlers are identical. We
present an FPTAS when the number of handlers is constant. We note in passing
that the same technique also yields a first FPTAS for the weapons-target
allocation problem \cite{manne_wta} with a constant number of targets. We study
the structure of DOH problems and find that they have a sort of phase
transition -- in some instances it is better to spread risk evenly among the
handlers, in others, one handler should be used as a ``sacrificial lamb''. We
show that the problem is solvable in polynomial time if the destruction
probabilities depend only on the handler to which an object is assigned; if all
the handlers are identical and the objects all have the same value; or if each
handler can be assigned at most one object.
  Finally, we empirically evaluate several heuristics based on a combination of
greedy and genetic algorithms. The proposed heuristics return fairly high
quality solutions to very large problem instances (upto 250 objects and 100
handlers) in tens of seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7313</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7313</id><created>2014-01-28</created><authors><author><keyname>Chen</keyname><forenames>Sixia</forenames></author><author><keyname>Russell</keyname><forenames>Alexander</forenames></author><author><keyname>Samanta</keyname><forenames>Abhishek</forenames></author><author><keyname>Sundaram</keyname><forenames>Ravi</forenames></author></authors><title>Deterministic Blind Rendezvous in Cognitive Radio Networks</title><categories>cs.NI</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind rendezvous is a fundamental problem in cognitive radio networks. The
problem involves a collection of agents (radios) that wish to discover each
other in the blind setting where there is no shared infrastructure and they
initially have no knowledge of each other. Time is divided into discrete slots;
spectrum is divided into discrete channels, $\{1,2,..., n\}$. Each agent may
access a single channel in a single time slot and we say that two agents
rendezvous when they access the same channel in the same time slot. The model
is asymmetric: each agent $A_i$ may only use a particular subset $S_i$ of the
channels and different agents may have access to different subsets of channels.
The goal is to design deterministic channel hopping schedules for each agent so
as to guarantee rendezvous between any pair of agents with overlapping channel
sets.
  Two independent sets of authors, Shin et al. and Lin et al., gave the first
constructions guaranteeing asynchronous blind rendezvous in $O(n^2)$ and
$O(n^3)$ time, respectively. We present a substantially improved construction
guaranteeing that any two agents, $A_i$, $A_j$, will rendezvous in $O(|S_i|
|S_j| \log\log n)$ time. Our results are the first that achieve nontrivial
dependence on $|S_i|$, the size of the set of available channels. This allows
us, for example, to save roughly a quadratic factor over the best previous
results in the important case when channel subsets have constant size. We also
achieve the best possible bound of $O(1)$ time for the symmetric situation;
previous works could do no better than $O(n)$. Using the probabilistic method
and Ramsey theory we provide evidence in support of our suspicion that our
construction is asymptotically optimal for small size channel subsets: we show
both a $c |S_i||S_j|$ lower bound and a $c \log\log n$ lower bound when $|S_i|,
|S_j| \leq n/2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7344</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7344</id><created>2014-01-25</created><authors><author><keyname>Hanley</keyname><forenames>Brian P.</forenames></author></authors><title>Release of the Kraken: A Novel Money Multiplier Equation's Debut in 21st
  Century Banking</title><categories>q-fin.GN cs.CE</categories><comments>22 pages, 3 figures, 5 significant equations (of 7). Published in
  Economics E-Journal</comments><journal-ref>Economics: The Open-Access, Open-Assessment E-Journal, Vol. 6,
  2012-3</journal-ref><doi>10.5018/economics-ejournal.ja.2012-3</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Historically, the banking multiplier has been in a range of 4 to 100, with
25% to 1% reserve ratios at most layers of the banking system encompassing the
majority of its range in recent centuries. Here it is shown that multipliers
over 1 000 can occur from a new mechanism in banking. This new multiplier uses
a default insurance note to insure an outstanding loan in order to return the
value of the insured amount into capital. The economic impact of this invention
is calculably greater than the original invention of reserve banking. The
consequence of this lending invention is to render the existing money
multiplier equations of reserve banking obsolete where it occurs. The equations
describing this new multiplier do not converge. Each set of parameters for
reserve percentage, nesting depth, etc. creates a unique logarithmic curve
rather than approaching a limit. Thus it is necessary to show the behavior of
this new equation by numerical methods. Understanding this new multiplier and
associated issues is necessary for economic analyses of the Global Financial
Crisis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7360</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7360</id><created>2014-01-28</created><updated>2014-03-26</updated><authors><author><keyname>Lee</keyname><forenames>Eun Jee</forenames></author><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author></authors><title>A Shannon Approach to Secure Multi-party Computations</title><categories>cs.IT cs.CR math.IT</categories><journal-ref>52nd Annual Allerton Conference on. IEEE (2014) 1287-1293</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In secure multi-party computations (SMC), parties wish to compute a function
on their private data without revealing more information about their data than
what the function reveals. In this paper, we investigate two Shannon-type
questions on this problem. We first consider the traditional one-shot model for
SMC which does not assume a probabilistic prior on the data. In this model,
private communication and randomness are the key enablers to secure computing,
and we investigate a notion of randomness cost and capacity. We then move to a
probabilistic model for the data, and propose a Shannon model for discrete
memoryless SMC. In this model, correlations among data are the key enablers for
secure computing, and we investigate a notion of dependency which permits the
secure computation of a function. While the models and questions are general,
this paper focuses on summation functions, and relies on polar code
constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7369</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7369</id><created>2014-01-28</created><authors><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author></authors><title>Linear Codes are Optimal for Index-Coding Instances with Five or Fewer
  Receivers</title><categories>cs.IT math.IT</categories><comments>submitted to the 2014 IEEE International Symposium on Information
  Theory (ISIT)</comments><journal-ref>Proceedings of the 2014 IEEE International Symposium on
  Information Theory (ISIT 2014), Honolulu, USA, pp. 491-495, June 29-July 4,
  2014</journal-ref><doi>10.1109/ISIT.2014.6874881</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study zero-error unicast index-coding instances, where each receiver must
perfectly decode its requested message set, and the message sets requested by
any two receivers do not overlap. We show that for all these instances with up
to five receivers, linear index codes are optimal. Although this class contains
9847 non-isomorphic instances, by using our recent results and by properly
categorizing the instances based on their graphical representations, we need to
consider only 13 non-trivial instances to solve the entire class. This work
complements the result by Arbabjolfaei et al. (ISIT 2013), who derived the
capacity region of all unicast index-coding problems with up to five receivers
in the diminishing-error setup. They employed random-coding arguments, which
require infinitely-long messages. We consider the zero-error setup; our
approach uses graph theory and combinatorics, and does not require long
messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7372</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7372</id><created>2014-01-28</created><authors><author><keyname>Eddelbuettel</keyname><forenames>Dirk</forenames></author><author><keyname>Stokely</keyname><forenames>Murray</forenames></author><author><keyname>Ooms</keyname><forenames>Jeroen</forenames></author></authors><title>RProtoBuf: Efficient Cross-Language Data Serialization in R</title><categories>stat.CO cs.MS cs.SE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Modern data collection and analysis pipelines often involve a sophisticated
mix of applications written in general purpose and specialized programming
languages. Many formats commonly used to import and export data between
different programs or systems, such as CSV or JSON, are verbose, inefficient,
not type-safe, or tied to a specific programming language. Protocol Buffers are
a popular method of serializing structured data between applications - while
remaining independent of programming languages or operating systems. They offer
a unique combination of features, performance, and maturity that seems
particularly well suited for data-driven applications and numerical computing.
The RProtoBuf package provides a complete interface to Protocol Buffers from
the R environment for statistical computing. This paper outlines the general
class of data serialization requirements for statistical computing, describes
the implementation of the RProtoBuf package, and illustrates its use with
example applications in large-scale data collection pipelines and web services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7374</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7374</id><created>2014-01-28</created><authors><author><keyname>Zhou</keyname><forenames>Zhiyi</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author><author><keyname>Honig</keyname><forenames>Michael L.</forenames></author></authors><title>A Message-Passing Approach to Combating Hidden Terminals in Wireless
  Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>12 pages. arXiv admin note: text overlap with arXiv:0901.1408</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collisions with hidden terminals is a major cause of performance degradation
in 802.11 and likewise wireless networks. Carrier sense multiple access with
collision avoidance (CSMA/CA) is utilized to avoid collisions at the cost of
spatial reuse. This report studies receiver design to mitigate interference
from hidden terminals. A wireless channel model with correlated fading in time
is assumed. A message-passing approach is proposed, in which a receiver can
successfully receive and decode partially overlapping transmissions from two
sources rather than treating undesired one as thermal noise. Numerical results
of both coded and uncoded systems show the advantage of the receiver over
conventional receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7375</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7375</id><created>2014-01-28</created><authors><author><keyname>Yang</keyname><forenames>Jaewon</forenames></author><author><keyname>McAuley</keyname><forenames>Julian</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Detecting Cohesive and 2-mode Communities in Directed and Undirected
  Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Published in the proceedings of WSDM '14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks are a general language for representing relational information among
objects. An effective way to model, reason about, and summarize networks, is to
discover sets of nodes with common connectivity patterns. Such sets are
commonly referred to as network communities. Research on network community
detection has predominantly focused on identifying communities of densely
connected nodes in undirected networks.
  In this paper we develop a novel overlapping community detection method that
scales to networks of millions of nodes and edges and advances research along
two dimensions: the connectivity structure of communities, and the use of edge
directedness for community detection. First, we extend traditional definitions
of network communities by building on the observation that nodes can be densely
interlinked in two different ways: In cohesive communities nodes link to each
other, while in 2-mode communities nodes link in a bipartite fashion, where
links predominate between the two partitions rather than inside them. Our
method successfully detects both 2-mode as well as cohesive communities, that
may also overlap or be hierarchically nested. Second, while most existing
community detection methods treat directed edges as though they were
undirected, our method accounts for edge directions and is able to identify
novel and meaningful community structures in both directed and undirected
networks, using data from social, biological, and ecological domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7377</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7377</id><created>2014-01-28</created><updated>2014-02-08</updated><authors><author><keyname>Nongpiur</keyname><forenames>R. C.</forenames></author></authors><title>Improved Robust Node Position Estimation in Wireless Sensor Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method for estimating the relative positions of location-unaware nodes
from the location-aware nodes and the received signal strength (RSS) between
the nodes, in a wireless sensor network (WSN), is proposed. In the method, a
regularization term is incorporated in the optimization problem leading to
significant improvement in the estimation accuracy even in the presence of
position errors of the location-aware nodes and distance errors between the
nodes. The regularization term is appropriated weighted on the basis of the
degree of connectivity between the nodes in the network. The method is
formulated as a convex optimization problem using the semidefinite relaxation
approach. Experimental comparisons with state-of-the-art competing methods show
that the proposed method yields node positions that are much more accurate even
in the presence of measurement errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7388</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7388</id><created>2014-01-28</created><authors><author><keyname>Rubinstein</keyname><forenames>J. Hyam</forenames></author><author><keyname>Rubinstein</keyname><forenames>Benjamin I. P.</forenames></author><author><keyname>Bartlett</keyname><forenames>Peter L.</forenames></author></authors><title>Bounding Embeddings of VC Classes into Maximum Classes</title><categories>cs.LG math.CO stat.ML</categories><comments>22 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the earliest conjectures in computational learning theory-the Sample
Compression conjecture-asserts that concept classes (equivalently set systems)
admit compression schemes of size linear in their VC dimension. To-date this
statement is known to be true for maximum classes---those that possess maximum
cardinality for their VC dimension. The most promising approach to positively
resolving the conjecture is by embedding general VC classes into maximum
classes without super-linear increase to their VC dimensions, as such
embeddings would extend the known compression schemes to all VC classes. We
show that maximum classes can be characterised by a local-connectivity property
of the graph obtained by viewing the class as a cubical complex. This geometric
characterisation of maximum VC classes is applied to prove a negative embedding
result which demonstrates VC-d classes that cannot be embedded in any maximum
class of VC dimension lower than 2d. On the other hand, we show that every VC-d
class C embeds in a VC-(d+D) maximum class where D is the deficiency of C,
i.e., the difference between the cardinalities of a maximum VC-d class and of
C. For VC-2 classes in binary n-cubes for 4 &lt;= n &lt;= 6, we give best possible
results on embedding into maximum classes. For some special classes of Boolean
functions, relationships with maximum classes are investigated. Finally we give
a general recursive procedure for embedding VC-d classes into VC-(d+k) maximum
classes for smallest k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7404</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7404</id><created>2014-01-28</created><authors><author><keyname>Asadi</keyname><forenames>Behzad</forenames></author><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>On Index Coding in Noisy Broadcast Channels with Receiver Message Side
  Information</title><categories>cs.IT math.IT</categories><comments>Authors' final version (to appear in IEEE Communications Letters)</comments><journal-ref>IEEE Communications Letters, Vol. 18, No. 4, pp. 640-643, Apr.
  2014</journal-ref><doi>10.1109/LCOMM.2014.020414.132589</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter investigates the role of index coding in the capacity of AWGN
broadcast channels with receiver message side information. We first show that
index coding is unnecessary where there are two receivers; multiplexing coding
and superposition coding are sufficient to achieve the capacity region. We next
show that, for more than two receivers, multiplexing coding and superposition
coding alone can be suboptimal. We give an example where these two coding
schemes alone cannot achieve the capacity region, but adding index coding can.
This demonstrates that, in contrast to the two-receiver case, multiplexing
coding cannot fulfill the function of index coding where there are three or
more receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7406</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7406</id><created>2014-01-28</created><authors><author><keyname>Tsang</keyname><forenames>Jeffrey</forenames></author></authors><title>The parametrized probabilistic finite-state transducer probe game player
  fingerprint model</title><categories>cs.GT cs.NE</categories><comments>17 pages, 35 figures</comments><journal-ref>IEEE Transactions on Computational Intelligence and AI in Games
  2(3):208-224, 2010</journal-ref><doi>10.1109/TCIAIG.2010.2062512</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fingerprinting operators generate functional signatures of game players and
are useful for their automated analysis independent of representation or
encoding. The theory for a fingerprinting operator which returns the
length-weighted probability of a given move pair occurring from playing the
investigated agent against a general parametrized probabilistic finite-state
transducer (PFT) is developed, applicable to arbitrary iterated games. Results
for the distinguishing power of the 1-state opponent model, uniform
approximability of fingerprints of arbitrary players, analyticity and Lipschitz
continuity of fingerprints for logically possible players, and equicontinuity
of the fingerprints of bounded-state probabilistic transducers are derived.
Algorithms for the efficient computation of special instances are given; the
shortcomings of a previous model, strictly generalized here from a simple
projection of the new model, are explained in terms of regularity condition
violations, and the extra power and functional niceness of the new fingerprints
demonstrated. The 2-state deterministic finite-state transducers (DFTs) are
fingerprinted and pairwise distances computed; using this the structure of DFTs
in strategy space is elucidated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7411</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7411</id><created>2014-01-28</created><authors><author><keyname>Ghosh</keyname><forenames>Subrata</forenames></author><author><keyname>Aswani</keyname><forenames>Krishna</forenames></author><author><keyname>Singh</keyname><forenames>Surabhi</forenames></author><author><keyname>Sahu</keyname><forenames>Satyajit</forenames></author><author><keyname>Fujita</keyname><forenames>Daisuke</forenames></author><author><keyname>Bandyopadhyay</keyname><forenames>Anirban</forenames></author></authors><title>Design and Construction of a Brain-Like Computer: A New Class of
  Frequency-Fractal Computing Using Wireless Communication in a Supramolecular
  Organic, Inorganic System</title><categories>cs.ET physics.bio-ph</categories><comments>73 pages, 23 figures</comments><journal-ref>Information 2014, 5, 28-100</journal-ref><doi>10.3390/info5010028</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Here, we introduce a new class of computer which does not use any circuit or
logic gate. In fact, no program needs to be written: it learns by itself and
writes its own program to solve a problem. Godels incompleteness argument is
explored here to devise an engine where an astronomically large number of
IfThen arguments are allowed to grow by self assembly, based on the basic set
of arguments written in the system, thus, we explore the beyond Turing path of
computing but following a fundamentally different route adopted in the last
half a century old non Turing adventures. Our hardware is a multilayered seed
structure. If we open the largest seed, which is the final hardware, we find
several computing seed structures inside, if we take any of them and open,
there are several computing seeds inside. We design and synthesize the smallest
seed, the entire multilayered architecture grows by itself. The electromagnetic
resonance band of each seed looks similar, but the seeds of any layer shares a
common region in its resonance band with inner and upper layer, hence a chain
of resonance bands is formed (frequency fractal) connecting the smallest to the
largest seed (hence the name invincible rhythm or Ajeya Chhandam in Sanskrit).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7413</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7413</id><created>2014-01-29</created><updated>2014-12-06</updated><authors><author><keyname>Lu</keyname><forenames>Canyi</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Smoothed Low Rank and Sparse Matrix Recovery by Iteratively Reweighted
  Least Squares Minimization</title><categories>cs.LG cs.CV stat.ML</categories><comments>IEEE Transactions on Image Processing 2015</comments><doi>10.1109/TIP.2014.2380155</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a general framework for solving the low rank and/or sparse
matrix minimization problems, which may involve multiple non-smooth terms. The
Iteratively Reweighted Least Squares (IRLS) method is a fast solver, which
smooths the objective function and minimizes it by alternately updating the
variables and their weights. However, the traditional IRLS can only solve a
sparse only or low rank only minimization problem with squared loss or an
affine constraint. This work generalizes IRLS to solve joint/mixed low rank and
sparse minimization problems, which are essential formulations for many tasks.
As a concrete example, we solve the Schatten-$p$ norm and $\ell_{2,q}$-norm
regularized Low-Rank Representation (LRR) problem by IRLS, and theoretically
prove that the derived solution is a stationary point (globally optimal if
$p,q\geq1$). Our convergence proof of IRLS is more general than previous one
which depends on the special properties of the Schatten-$p$ norm and
$\ell_{2,q}$-norm. Extensive experiments on both synthetic and real data sets
demonstrate that our IRLS is much more efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7416</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7416</id><created>2014-01-29</created><authors><author><keyname>P</keyname><forenames>Pandiselvam.</forenames></author><author><keyname>T</keyname><forenames>Marimuthu.</forenames></author><author><keyname>R</keyname><forenames>Lawrance.</forenames></author></authors><title>A Comparative Study on String Matching Algorithm of Biological Sequences</title><categories>cs.DS cs.CE</categories><comments>Selected For International Conference on Intelligent Computing</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  String matching algorithm plays the vital role in the Computational Biology.
The functional and structural relationship of the biological sequence is
determined by similarities on that sequence. For that, the researcher is
supposed to aware of similarities on the biological sequences. Pursuing of
similarity among biological sequences is an important research area of that can
bring insight into the evolutionary and genetic relationships among the genes.
In this paper, we have studied different kinds of string matching algorithms
and observed their time and space complexities. For this study, we have
assessed the performance of algorithms tested with biological sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7419</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7419</id><created>2014-01-29</created><updated>2014-03-19</updated><authors><author><keyname>Raz</keyname><forenames>Orit E.</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author><author><keyname>Solymosi</keyname><forenames>J&#xf3;zsef</forenames></author></authors><title>Polynomials vanishing on grids: The Elekes-R\'onyai problem revisited</title><categories>cs.CG math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we characterize real bivariate polynomials which have a small
range over large Cartesian products. We show that for every constant-degree
bivariate real polynomial $f$, either $|f(A,B)|=\Omega(n^{4/3})$, for every
pair of finite sets $A,B\subset{\mathbb R}$, with $|A|=|B|=n$ (where the
constant of proportionality depends on ${\rm deg} f$), or else $f$ must be of
one of the special forms $f(u,v)=h(\varphi(u)+\psi(v))$, or
$f(u,v)=h(\varphi(u)\cdot\psi(v))$, for some univariate polynomials
$\varphi,\psi,h$ over ${\mathbb R}$. This significantly improves a result of
Elekes and R\'onyai (2000).
  Our results are cast in a more general form, in which we give an upper bound
for the number of zeros of $z=f(x,y)$ on a triple Cartesian product $A\times
B\times C$, when the sizes $|A|$, $|B|$, $|C|$ need not be the same; the upper
bound is $O(n^{11/6})$ when $|A|=|B|=|C|=n$, where the constant of
proportionality depends on ${\rm deg} f$, unless $f$ has one of the
aforementioned special forms.
  This result provides a unified tool for improving bounds in various Erd\H
os-type problems in geometry and additive combinatorics. Several applications
of our results to problems of these kinds are presented. For example, we show
that the number of distinct distances between $n$ points lying on a
constant-degree parametric algebraic curve which does not contain a line, in
any dimension, is $\Omega(n^{4/3})$, extending the result of Pach and de Zeeuw
(2013) and improving the bound of Charalambides (2012), for the special case
where the curve under consideration has a polynomial parameterization. We also
derive improved lower bounds for several variants of the sum-product problem in
additive combinatorics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7425</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7425</id><created>2014-01-29</created><authors><author><keyname>Varga</keyname><forenames>Imre</forenames></author><author><keyname>N&#xe9;meth</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>Kocsis</keyname><forenames>Gergely</forenames></author></authors><title>A novel method of generating tunable underlying network topologies for
  social simulation</title><categories>cs.SI physics.soc-ph</categories><comments>Originally presented at the 2013 IEEE 4th International Conference on
  Cognitive Infocommunications (CogInfoCom)</comments><journal-ref>2013 IEEE 4th CogInfoCom, 2-5. Dec. 2013, Budapest, Hungary, pp.
  71-74</journal-ref><doi>10.1109/CogInfoCom.2013.6719189</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method of generating different scale-free networks, which has
several input parameters in order to adjust the structure, so that they can
serve as a basis for computer simulation of real-world phenomena. The
topological structure of these networks was studied to determine what kind of
networks can be produced and how can we give the appropriate values of
parameters to get a desired structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7426</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7426</id><created>2014-01-29</created><authors><author><keyname>Alkhateeb</keyname><forenames>Ahmed</forenames></author><author><keyname>Ayach</keyname><forenames>Omar El</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Channel Estimation and Hybrid Precoding for Millimeter Wave Cellular
  Systems</title><categories>cs.IT math.IT</categories><comments>36 pages, 10 figures, submitted to IEEE Journal of Selected Topics in
  Signal Processing</comments><doi>10.1109/JSTSP.2014.2334278</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mmWave) cellular systems will enable gigabit-per-second data
rates thanks to the large bandwidth available at mmWave frequencies. To realize
sufficient link margin, mmWave systems will employ directional beamforming with
large antenna arrays at both the transmitter and receiver. Due to the high cost
and power consumption of gigasample mixed-signal devices, mmWave precoding will
likely be divided among the analog and digital domains. The large number of
antennas and the presence of analog beamforming requires the development of
mmWave-specific channel estimation and precoding algorithms. This paper
develops an adaptive algorithm to estimate the mmWave channel parameters that
exploits the poor scattering nature of the channel. To enable the efficient
operation of this algorithm, a novel hierarchical multi-resolution codebook is
designed to construct training beamforming vectors with different beamwidths.
For single-path channels, an upper bound on the estimation error probability
using the proposed algorithm is derived, and some insights into the efficient
allocation of the training power among the adaptive stages of the algorithm are
obtained. The adaptive channel estimation algorithm is then extended to the
multi-path case relying on the sparse nature of the channel. Using the
estimated channel, this paper proposes a new hybrid analog/digital precoding
algorithm that overcomes the hardware constraints on the analog-only
beamforming, and approaches the performance of digital solutions. Simulation
results show that the proposed low-complexity channel estimation algorithm
achieves comparable precoding gains compared to exhaustive channel training
algorithms. The results also illustrate that the proposed algorithms can
approach the coverage probability achieved by perfect channel knowledge even in
the presence of interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7435</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7435</id><created>2014-01-29</created><authors><author><keyname>Jebessa</keyname><forenames>Naod Duga</forenames></author><author><keyname>Alemayehu</keyname><forenames>Henok Getachew</forenames></author></authors><title>Mobile Services and ICT4D, To the Network Economy - Bridging the Digital
  Divide, Ethiopia's Case</title><categories>cs.CY</categories><comments>3rd Scientific Conference on Electrical Engineering, Organized by
  Ethiopian Society of Electrical Engineers and Electrical and Computer
  Engineering Department, Addis Ababa University, 22-23 August 2009. Keywords:
  mobile services, ICT4D, appropriate technologies, telecom in Ethiopia,
  development, Parlay/JAIN, techno-economics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a development paradigm for Ethiopia, based on appropriate
services and innovative use of mobile communications technologies via
applications tailored for sectors like business, finance, healthcare,
governance, education and infotainment. The experience of other developing
countries like India and Kenya is cited so as to adapt those to the Ethiopian
context. Notable application areas in the aforementioned sectors have been
outlined. The ETC 'next generation network' is taken into consideration, with
an emphasis on mobile service offering by the Telco itself and/or third party
service providers. In addition, enabling technologies like mobile internet,
location-based systems, open interfaces to large telecom networks, specifically
service-oriented architecture (SOA), Parlay/JAIN and the like are discussed.
The paper points out possible endeavors by such stakeholders like: telecom
agencies and network operators; businesses, government and NGOs; entrepreneurs
and innovators; technology companies and professionals; as well as researchers
and academic institutions. ICT4D through mobile services and their role in
bridging the digital divide by building a virtual 'network economy' is
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7436</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7436</id><created>2014-01-29</created><authors><author><keyname>Rahmani</keyname><forenames>Rahim</forenames></author><author><keyname>Rahman</keyname><forenames>Hasibur</forenames></author><author><keyname>Kanter</keyname><forenames>Theo</forenames></author></authors><title>On Performance of Logical-Clustering Of Flow-Sensors</title><categories>cs.NI cs.DC</categories><comments>13 pages, 15 Figures and published on International Journal of
  Computer Science Issues, IJCSI Volume 10 Issue 5, September 2013</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In state-of-the-art Pervasive Computing, it is envisioned that unlimited
access to information will be facilitated for anyone and anything. Wireless
sensor networks will play a pivotal role in the stated vision. This reflects
the phenomena where any situation can be sensed and analyzed anywhere. It makes
heterogeneous context ubiquitous. Clustering context is one of the techniques
to manage ubiquitous context information efficiently to maximize its potential.
Logical-clustering is useful to share real-time context where sensors are
physically distributed but logically clustered. This paper investigates the
network performance of logical-clustering based on ns-3 simulations. In
particular reliability, scalability, and reachability in terms of delay,
jitter, and packet loss for the logically clustered network have been
investigated. The performance study shows that jitter demonstrates 40 % and 44
% fluctuation for 200 % increase in the node per cluster and 100 % increase in
the cluster size respectively. Packet loss exhibits only 18 % increase for 83 %
increase in the packet flow-rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7437</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7437</id><created>2014-01-29</created><authors><author><keyname>Kanter</keyname><forenames>Theo</forenames></author><author><keyname>Rahmani</keyname><forenames>Rahim</forenames></author><author><keyname>Mahmud</keyname><forenames>Arif</forenames></author></authors><title>Conceptual Framework for Internet of Things' Virtualization via OpenFlow
  in Context-aware Networks</title><categories>cs.NI cs.DC</categories><comments>12 pages, 19 Figures, 1 table</comments><journal-ref>International Journal of Computer Science Issues, ISSN: 1694-0784,
  1694-0814, November 2013 Issue (Volume 10, Issue 6)
  http://ijcsi.org/papers/IJCSI-10-6-1-16-27.pdf</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  A novel conceptual framework is presented in this paper with an aim to
standardize and virtualize Internet of Things(IoT) infrastructure through
deploying OpenFlow technology. The framework can receivee services based on
context information leaving the current infrastructure unchanged. This
framework allows the active collaboration of heterogeneous devices and
protocols. Moreover it is capable to model placement of physical objects,
manage the system and to collect information for services deployed on an IoT
infrastructure. Our proposed IoT virtualization is applicable to a random
topology scenario which makes it possible to 1) share flow sensors resources 2)
establish multioperational sensor networks, and 3) extend reachability within
the framework without establishing any further physical networks. Flow sensors
achieve better results comparable to the typical sensors with respect to packet
generation, reachability, simulation time, throughput, energy consumption point
of view. Even better results are possible through utilizing multicast groups in
large scale networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7444</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7444</id><created>2014-01-29</created><authors><author><keyname>Gilad</keyname><forenames>Yossi</forenames></author><author><keyname>Herzberg</keyname><forenames>Amir</forenames></author><author><keyname>Trachtenberg</keyname><forenames>Ari</forenames></author></authors><title>Securing Smartphones: A Micro-TCB Approach</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As mobile phones have evolved into `smartphones', with complex operating
systems running third- party software, they have become increasingly vulnerable
to malicious applications (malware). We introduce a new design for mitigating
malware attacks against smartphone users, based on a small trusted computing
base module, denoted uTCB. The uTCB manages sensitive data and sensors, and
provides core services to applications, independently of the operating system.
The user invokes uTCB using a simple secure attention key, which is pressed in
order to validate physical possession of the device and authorize a sensitive
action; this protects private information even if the device is infected with
malware. We present a proof-of-concept implementation of uTCB based on ARM's
TrustZone, a secure execution environment increasingly found in smartphones,
and evaluate our implementation using simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7457</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7457</id><created>2014-01-29</created><authors><author><keyname>Liu</keyname><forenames>Chi-Man</forenames></author><author><keyname>Luo</keyname><forenames>Ruibang</forenames></author><author><keyname>Lam</keyname><forenames>Tak-Wah</forenames></author></authors><title>GPU-Accelerated BWT Construction for Large Collection of Short Reads</title><categories>q-bio.GN cs.DC cs.DS q-bio.QM</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in DNA sequencing technology have stimulated the development of
algorithms and tools for processing very large collections of short strings
(reads). Short-read alignment and assembly are among the most well-studied
problems. Many state-of-the-art aligners, at their core, have used the
Burrows-Wheeler transform (BWT) as a main-memory index of a reference genome
(typical example, NCBI human genome). Recently, BWT has also found its use in
string-graph assembly, for indexing the reads (i.e., raw data from DNA
sequencers). In a typical data set, the volume of reads is tens of times of the
sequenced genome and can be up to 100 Gigabases. Note that a reference genome
is relatively stable and computing the index is not a frequent task. For reads,
the index has to computed from scratch for each given input. The ability of
efficient BWT construction becomes a much bigger concern than before. In this
paper, we present a practical method called CX1 for constructing the BWT of
very large string collections. CX1 is the first tool that can take advantage of
the parallelism given by a graphics processing unit (GPU, a relative cheap
device providing a thousand or more primitive cores), as well as simultaneously
the parallelism from a multi-core CPU and more interestingly, from a cluster of
GPU-enabled nodes. Using CX1, the BWT of a short-read collection of up to 100
Gigabases can be constructed in less than 2 hours using a machine equipped with
a quad-core CPU and a GPU, or in about 43 minutes using a cluster with 4 such
machines (the speedup is almost linear after excluding the first 16 minutes for
loading the reads from the hard disk). The previously fastest tool BRC is
measured to take 12 hours to process 100 Gigabases on one machine; it is
non-trivial how BRC can be parallelized to take advantage a cluster of
machines, let alone GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7463</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7463</id><created>2014-01-29</created><authors><author><keyname>Flener</keyname><forenames>Pierre</forenames></author><author><keyname>Pearson</keyname><forenames>Justin</forenames></author></authors><title>Propagators and Violation Functions for Geometric and Workload
  Constraints Arising in Airspace Sectorisation</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Airspace sectorisation provides a partition of a given airspace into sectors,
subject to geometric constraints and workload constraints, so that some cost
metric is minimised. We make a study of the constraints that arise in airspace
sectorisation. For each constraint, we give an analysis of what algorithms and
properties are required under systematic search and stochastic local search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7471</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7471</id><created>2014-01-29</created><updated>2015-09-03</updated><authors><author><keyname>Cafaro</keyname><forenames>Massimo</forenames></author><author><keyname>Pell&#xe8;</keyname><forenames>Piergiuseppe</forenames></author></authors><title>Space-efficient Verifiable Secret Sharing Using Polynomial Interpolation</title><categories>cs.CR cs.DS</categories><comments>Accepted for publication. To appear in IEEE Transactions on Cloud
  Computing</comments><doi>10.1109/TCC.2015.2396072</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Preserving data confidentiality in clouds is a key issue. Secret Sharing, a
cryptographic primitive for the distribution of a secret among a group of $n$
participants designed so that only subsets of shareholders of cardinality $0 &lt;
t \leq n$ are allowed to reconstruct the secret by pooling their shares, can
help mitigating and minimizing the problem. A desirable feature of Secret
Sharing schemes is cheater detection, i.e. the ability to detect one or more
malicious shareholders trying to reconstruct the secret by obtaining legal
shares from the other shareholders while providing them with fake shares.
Verifiable Secret Sharing schemes solve this problem by allowing shareholders
verifying the others' shares. We present new verification algorithms providing
arbitrary secret sharing schemes with cheater detection capabilities, and prove
their space efficiency with regard to other schemes appeared in the literature.
We also introduce, in one of our schemes, the Exponentiating Polynomial Root
Problem (EPRP), which is believed to be NP-Intermediate and therefore
difficult.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7474</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7474</id><created>2014-01-29</created><authors><author><keyname>Berthelot</keyname><forenames>Geoffroy</forenames></author></authors><title>The phenotypic expansion and its boundaries</title><categories>stat.OT cs.MA nlin.AO</categories><comments>Phd Thesis, 227 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of sport performances in the future is a subject of myth and
disagreement among experts. As arguments favoring and opposing such methodology
were discussed, other publications empirically showed that the past development
of performances followed a non linear trend. Other works, while deeply
exploring the conditions leading to world records, highlighted that performance
is tied to the economical and geopolitical context. Here we investigated the
following human boundaries: development of performances with time in Olympic
and non-Olympic events, development of sport performances with aging among
humans and others species (greyhounds, thoroughbreds, mice). Development of
performances from a broader point of view (demography &amp; lifespan) in a specific
sub-system centered on primary energy was also investigated. We show that the
physiological developments are limited with time. Three major and direct
determinants of sport performance are age, technology and climatic conditions
(temperature). However, all observed developments are related to the
international context including the efficient use of primary energies. This
last parameter is a major indirect propeller of performance development. We
show that when physiological and societal performance indicators such as
lifespan and population density depend on primary energies, the energy source,
competition and mobility are key parameters for achieving long term sustainable
trajectories. Otherwise, the vast majority (98.7%) of the studied trajectories
reaches 0 before 15 generations, due to the consumption of fossil energy and a
low mobility rate. This led us to consider that in the present turbulent
economical context and given the upcoming energy crisis, societal and physical
performances are not expected to grow continuously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7480</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7480</id><created>2014-01-29</created><updated>2014-02-21</updated><authors><author><keyname>Litow</keyname><forenames>Bruce</forenames></author></authors><title>NP is contained in DTIME(n^O(log^{gamma}))</title><categories>cs.CC</categories><comments>This paper has been withdrawn due to a fatal flaw in the proof of
  Lemma 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use existential Diophantine predicates carefully reinterpreted over the
reals and the time complexity of Tarski algebra to show that 3-CNF SAT is in
n^O(log^{gamma} n) time for an absolute positive constant gamma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7483</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7483</id><created>2014-01-29</created><authors><author><keyname>Krawczyk</keyname><forenames>Pawel</forenames></author></authors><title>Secure SAML validation to prevent XML signature wrapping attacks</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SAML assertions are becoming popular method for passing authentication and
authorisation information between identity providers and consumers using
various single sign-on protocols. However their practical security strongly
depends on correct implementation, especially on the consumer side. Somorovsky
and others have demonstrated a number of XML signature related vulnerabilities
in SAML assertion validation frameworks. This article demonstrates how bad
library documentation and examples can lead to vulnerable consumer code and how
this can be avoided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7485</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7485</id><created>2014-01-29</created><authors><author><keyname>D'yachkov</keyname><forenames>A.</forenames></author><author><keyname>Rykov</keyname><forenames>V.</forenames></author><author><keyname>Deppe</keyname><forenames>C.</forenames></author><author><keyname>Lebedev</keyname><forenames>V.</forenames></author></authors><title>Superimposed Codes and Threshold Group Testing</title><categories>cs.IT math.IT</categories><comments>19 pages, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We will discuss superimposed codes and non-adaptive group testing designs
arising from the potentialities of compressed genotyping models in molecular
biology. The given paper was motivated by the 30th anniversary of
D'yachkov-Rykov recurrent upper bound on the rate of superimposed codes
published in 1982. We were also inspired by recent results obtained for
non-adaptive threshold group testing which develop the theory of superimposed
codes
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7486</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7486</id><created>2014-01-29</created><authors><author><keyname>Rezaeiye</keyname><forenames>Payam Porkar</forenames></author><author><keyname>bazrafkan</keyname><forenames>mehrnoosh</forenames></author><author><keyname>movassagh</keyname><forenames>ali akbar</forenames></author><author><keyname>Fazli</keyname><forenames>Mojtaba Sedigh</forenames></author><author><keyname>bazyari</keyname><forenames>Gholam hossein</forenames></author></authors><title>Use HMM and KNN for classifying corneal data</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  These days to gain classification system with high accuracy that can classify
complicated pattern are so useful in medicine and industry. In this article a
process for getting the best classifier for Lasik data is suggested. However at
first it's been tried to find the best line and curve by this classifier in
order to gain classifier fitting, and in the end by using the Markov method a
classifier for topographies is gained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7492</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7492</id><created>2014-01-29</created><authors><author><keyname>Dyachkov</keyname><forenames>A.</forenames></author></authors><title>Lectures on DNA Codes</title><categories>cs.IT math.IT q-bio.QM</categories><comments>24 pages, 2 figures</comments><journal-ref>A. Dyachkov, P. Vilenkin, I. Ismagilov, R. Sarbayev, A. Macula, D.
  Torney, P. White, &quot;On dna codes&quot;// Problems of Information Transmission, vol.
  41, no. 4, pp. 349-367, 2005</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For $q$-ary $n$-sequences, we develop the concept of similarity functions
that can be used (for $q=4$) to model a thermodynamic similarity on DNA
sequences. A similarity function is identified by the length of a longest
common subsequence between two $q$-ary $n$-sequences. Codes based on similarity
functions are called DNA codes. DNA codes are important components in
biomolecular computing and other biotechnical applications that employ DNA
hybridization assays. The main aim of the given lecture notes -- to discuss
lower bounds on the rate of optimal DNA codes for a biologically motivated
similarity function called a block similarity and for the conventional deletion
similarity function used in the theory of error-correcting codes. We also
present constructions of suboptimal DNA codes based on the parity-check code
detecting one error in the Hamming metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7494</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7494</id><created>2014-01-29</created><authors><author><keyname>Hofmann</keyname><forenames>Johannes</forenames></author><author><keyname>Treibig</keyname><forenames>Jan</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>Comparing the Performance of Different x86 SIMD Instruction Sets for a
  Medical Imaging Application on Modern Multi- and Manycore Chips</title><categories>cs.DC cs.PF</categories><comments>arXiv admin note: text overlap with arXiv:1401.3615</comments><doi>10.1145/2568058.2568068</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single Instruction, Multiple Data (SIMD) vectorization is a major driver of
performance in current architectures, and is mandatory for achieving good
performance with codes that are limited by instruction throughput. We
investigate the efficiency of different SIMD-vectorized implementations of the
RabbitCT benchmark. RabbitCT performs 3D image reconstruction by back
projection, a vital operation in computed tomography applications. The
underlying algorithm is a challenge for vectorization because it consists,
apart from a streaming part, also of a bilinear interpolation requiring
scattered access to image data. We analyze the performance of SSE (128 bit),
AVX (256 bit), AVX2 (256 bit), and IMCI (512 bit) implementations on recent
Intel x86 systems. A special emphasis is put on the vector gather
implementation on Intel Haswell and Knights Corner microarchitectures. Finally
we discuss why GPU implementations perform much better for this specific
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7498</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7498</id><created>2014-01-29</created><authors><author><keyname>Saarela</keyname><forenames>Aleksi</forenames></author></authors><title>Systems of word equations, polynomials and linear algebra: A new
  approach</title><categories>math.CO cs.FL</categories><comments>19 pages, submitted to a journal, extended version of the conference
  paper arXiv:1108.3637</comments><msc-class>68R15</msc-class><journal-ref>European J. Combin. 47 (2015) 1-14</journal-ref><doi>10.1016/j.ejc.2015.01.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new tool, namely polynomial and linear algebraic methods, for
studying systems of word equations. We illustrate its usefulness by giving
essentially simpler proofs of several hard problems. At the same time we prove
extensions of these results. Finally, we obtain the first nontrivial upper
bounds for the fundamental problem of the maximal size of independent systems.
These bounds depend quadratically on the size of the shortest equation. No
methods of having such bounds have been known before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7499</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7499</id><created>2014-01-29</created><authors><author><keyname>Sharifzadeh</keyname><forenames>Manaf</forenames></author><author><keyname>aragy</keyname><forenames>saeid</forenames></author><author><keyname>Bashash</keyname><forenames>Kaveh</forenames></author><author><keyname>Bashokian</keyname><forenames>Shahram</forenames></author><author><keyname>gheisari</keyname><forenames>mehdi</forenames></author></authors><title>A Comparison with two semantic sensor data storages in total data
  transmission</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The creation of small and cheap sensors promoted the emergence of large scale
sensor networks. Sensor networks allow monitoring a variety of physical
phenomena, like weather conditions (temperature, humidity, atmospheric pressure
...), traffic levels on highways or rooms occupancy in public buildings. Some
of the sensors produce large volume of data such as weather temperature. These
data should be stored somewhere for user queries. In this paper two known
sensor data storage methods that store data semantically has been compared and
it has been shown that storing data in ontology form consumes more energy so
the lifetime of sensor network would decreases. The reason we choose them is
that they are useful and popular.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7505</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7505</id><created>2014-01-29</created><authors><author><keyname>D'yachkov</keyname><forenames>Arkadii G.</forenames></author></authors><title>Lectures on Designing Screening Experiments</title><categories>cs.IT math.IT</categories><comments>66 pages</comments><journal-ref>Lecture Note Series 10, Feb. 2004, Combinatorial and Computational
  Mathematics Center, Pohang University of Science and Technology (POSTECH),
  Korea Republic, (monograph, pp. 112)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing Screening Experiments (DSE) is a class of information - theoretical
models for multiple - access channels (MAC). We discuss the combinatorial model
of DSE called a disjunct channel model. This model is the most important for
applications and closely connected with the superimposed code concept. We give
a detailed survey of lower and upper bounds on the rate of superimposed codes.
The best known constructions of superimposed codes are considered in paper. We
also discuss the development of these codes (non-adaptive pooling designs)
intended for the clone - library screening problem. We obtain lower and upper
bounds on the rate of binary codes for the combinatorial model of DSE called an
adder channel model. We also consider the concept of universal decoding for the
probabilistic DSE model called a symmetric model of DSE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7508</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7508</id><created>2014-01-29</created><authors><author><keyname>D'yachkov</keyname><forenames>A. G.</forenames></author><author><keyname>Macula</keyname><forenames>A. J.</forenames></author><author><keyname>Torney</keyname><forenames>D. C.</forenames></author><author><keyname>Vilenkin</keyname><forenames>P. A.</forenames></author></authors><title>Two Models of Nonadaptive Group Testing for Designing Screening
  Experiments</title><categories>cs.IT math.IT</categories><comments>10 pages</comments><journal-ref>Proceedings of the 6th International Workshop on Model Oriented
  Design and Analysis, pp. 63-75, Physica-Verlag Heidelberg,
  Puchberg/Schneeberg, Austria, 2001</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss two non-standard models of nonadaptive combinatorial search which
develop the conventional disjunct search model for a small number of defective
elements contained in a finite ground set or a population. The first model is
called a search of defective supersets. The second model is called a search of
defective subsets in the presence of inhibitors. For these models, we study the
constructive search methods based on the known constructions for the disjunct
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7517</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7517</id><created>2014-01-29</created><authors><author><keyname>Kharinov</keyname><forenames>M.</forenames></author></authors><title>Information quantity in a pixel of digital image</title><categories>cs.CV cs.IT math.IT</categories><comments>11 pages, 4 figures, 1 definition, 5 formulas</comments><journal-ref>M.V. Kharinov, Information quantity in a pixel of digital image,
  Bulletin of the Buryat State University. Mathematics and Informatics. 2013.
  No 2, Ulan-Ude, Russia, pp. 95-104. ISSN 2304-5728 [in Russian]</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is devoted to the problem of integer-valued estimating of
information quantity in a pixel of digital image. The definition of an integer
estimation of information quantity based on constructing of the certain binary
hierarchy of pixel clusters is proposed. The methods for constructing
hierarchies of clusters and generating of hierarchical sequences of image
approximations that minimally differ from the image by a standard deviation are
developed. Experimental results on integer-valued estimation of information
quantity are compared with the results obtained by utilizing of the classical
formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7528</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7528</id><created>2014-01-29</created><updated>2014-03-11</updated><authors><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Asghari</keyname><forenames>Vahid</forenames></author><author><keyname>Moghaddam</keyname><forenames>Fereydoun Farrahi</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>Failure-aware Lifespan Performance Analysis of Network Fabric in Modular
  Data Centers: Toward Deployment in Canada's North</title><categories>cs.PF cs.DC cs.NI</categories><comments>36 Pages, 17 figures</comments><msc-class>62N05, 68M15, 90B25</msc-class><acm-class>B.4.3; C.2.1; B.4.5; C.4; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data centers have evolved from a passive element of compute infrastructure to
become an active and core part of any ICT solution. Modular data centers are a
promising design approach to improve resiliency of data centers, and they can
play a key role in deploying ICT infrastructure in remote and inhospitable
environments with low temperatures and hydro- and wind-electric capabilities.
Modular data centers can also survive even with lack of continuous physical
maintenance and support. Generally, the most critical part of a data center is
its network fabric that could impede the whole system even if all other
components are fully functional. In this work, a complete failure analysis of
modular data centers using failure models of various components including
servers, switches, and links is performed using a proposed Monte-Carlo
approach. This approach allows us to calculate the performance of a design
along its lifespan even at the terminal stages. A class of modified Tanh-Log
cumulative distribution function of failure is proposed for aforementioned
components in order to achieve a better fit on the real data. In this study,
the real experimental data from the lanl2005 database is used to calculate the
fitting parameters of the failure cumulative distributions. For the network
connectivity, various topologies, such as FatTree, BCube, MDCube, and their
modified topologies are considered. The performance and also the lifespan of
each topology in presence of failures in various components are studied against
the topology parameters using the proposed approach. Furthermore, these
topologies are compared against each other in a consistent settings in order to
determine what topology could deliver a higher performance and resiliency
subject to the scalability and agility requirements of a target data center
design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7532</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7532</id><created>2014-01-29</created><authors><author><keyname>Garcia-Morchon</keyname><forenames>Oscar</forenames></author><author><keyname>Rietman</keyname><forenames>Ronald</forenames></author><author><keyname>Tolhuizen</keyname><forenames>Ludo</forenames></author><author><keyname>Gomez</keyname><forenames>Domingo</forenames></author><author><keyname>Gutierrez</keyname><forenames>Jaime</forenames></author></authors><title>The MMO problem</title><categories>math.RA cs.CR cs.SC math.NT</categories><comments>Submitted to Interantaional Symposium on Symbolic and Algebraic
  Computation (ISSAC) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a two polynomials analogue of the polynomial interpolation
problem. Namely, we consider the Mixing Modular Operations (MMO) problem of
recovering two polynomials $f\in \Z_p[x]$ and $g\in \Z_q[x]$ of known degree,
where $p$ and $q$ are two (un)known positive integers, from the values of
$f(t)\bmod p + g(t)\bmod q$ at polynomially many points $t \in \Z$. We show
that if $p$ and $q$ are known, the MMO problem is equivalent to computing a
close vector in a lattice with respect to the infinity norm. We also
implemented in the SAGE system a heuristic polynomial-time algorithm. If $p$
and $q$ are kept secret, we do not know how to solve this problem. This problem
is motivated by several potential cryptographic applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7533</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7533</id><created>2014-01-29</created><updated>2015-10-08</updated><authors><author><keyname>Herzet</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Dr&#xe9;meau</keyname><forenames>Ang&#xe9;lique</forenames></author><author><keyname>Soussen</keyname><forenames>Charles</forenames></author></authors><title>Relaxed Recovery Conditions for OMP/OLS by Exploiting both Coherence and
  Decay</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose extended coherence-based conditions for exact sparse support
recovery using orthogonal matching pursuit (OMP) and orthogonal least squares
(OLS). Unlike standard uniform guarantees, we embed some information about the
decay of the sparse vector coefficients in our conditions. As a result, the
standard condition $\mu&lt;1/(2k-1)$ (where $\mu$ denotes the mutual coherence and
$k$ the sparsity level) can be weakened as soon as the non-zero coefficients
obey some decay, both in the noiseless and the bounded-noise scenarios.
Furthermore, the resulting condition is approaching $\mu&lt;1/k$ for strongly
decaying sparse signals. Finally, in the noiseless setting, we prove that the
proposed conditions, in particular the bound $\mu&lt;1/k$, are the tightest
achievable guarantees based on mutual coherence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7535</identifier>
 <datestamp>2014-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7535</id><created>2014-01-29</created><updated>2014-08-13</updated><authors><author><keyname>O'Callaghan</keyname><forenames>Derek</forenames></author><author><keyname>Prucha</keyname><forenames>Nico</forenames></author><author><keyname>Greene</keyname><forenames>Derek</forenames></author><author><keyname>Conway</keyname><forenames>Maura</forenames></author><author><keyname>Carthy</keyname><forenames>Joe</forenames></author><author><keyname>Cunningham</keyname><forenames>P&#xe1;draig</forenames></author></authors><title>Online Social Media in the Syria Conflict: Encompassing the Extremes and
  the In-Betweens</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>8 pages, 3 figures, 3 tables. Minor changes including additional
  references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Syria conflict has been described as the most socially mediated in
history, with online social media playing a particularly important role. At the
same time, the ever-changing landscape of the conflict leads to difficulties in
applying analytical approaches taken by other studies of online political
activism. Therefore, in this paper, we use an approach that does not require
strong prior assumptions or the proposal of an advance hypothesis to analyze
Twitter and YouTube activity of a range of protagonists to the conflict, in an
attempt to reveal additional insights into the relationships between them. By
means of a network representation that combines multiple data views, we uncover
communities of accounts falling into four categories that broadly reflect the
situation on the ground in Syria. A detailed analysis of selected communities
within the anti-regime categories is provided, focusing on their central
actors, preferred online platforms, and activity surrounding &quot;real world&quot;
events. Our findings indicate that social media activity in Syria is
considerably more convoluted than reported in many other studies of online
political activism, suggesting that alternative analytical approaches can play
an important role in this type of scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7538</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7538</id><created>2014-01-29</created><authors><author><keyname>Herzet</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Dr&#xe9;meau</keyname><forenames>Ang&#xe9;lique</forenames></author></authors><title>Bayesian Pursuit Algorithms</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the sparse representation (SR) problem within a general
Bayesian framework. We show that the Lagrangian formulation of the standard SR
problem, i.e., $\mathbf{x}^\star=\arg\min_\mathbf{x} \lbrace \|
\mathbf{y}-\mathbf{D}\mathbf{x} \|_2^2+\lambda\| \mathbf{x}\|_0 \rbrace$, can
be regarded as a limit case of a general maximum a posteriori (MAP) problem
involving Bernoulli-Gaussian variables. We then propose different tractable
implementations of this MAP problem that we refer to as &quot;Bayesian pursuit
algorithms&quot;. The Bayesian algorithms are shown to have strong connections with
several well-known pursuit algorithms of the literature (e.g., MP, OMP, StOMP,
CoSaMP, SP) and generalize them in several respects. In particular, i) they
allow for atom deselection; ii) they can include any prior information about
the probability of occurrence of each atom within the selection process; iii)
they can encompass the estimation of unkown model parameters into their
recursions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7539</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7539</id><created>2014-01-29</created><authors><author><keyname>Al-Khiaty</keyname><forenames>Mojeeb Al-Rhman</forenames></author><author><keyname>Ahmed</keyname><forenames>Moataz</forenames></author></authors><title>Automatic Reference Models Development: A Framework</title><categories>cs.SE</categories><doi>10.7321/jscse.v3.n3.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software reuse allows the software industry to simultaneously reduce
development cost and improve product quality. Reuse of early-stage artifacts
has been acknowledged to be more beneficial than reuse of later-stage
artifacts. In this regard, early-stage reference models have been considered as
good tools to allow reuse across applications within the same domain. However,
our literature survey reported in this paper reveals that the problem of
automatically developing reference models from given instances has not caught
enough researchers attention yet. Accordingly, in this paper we propose a
framework for building a reference model that captures the common and variable
analysis/design practices, across the different applications in a domain. The
framework considers multi-view models in assessing the commonalities and
variabilities among given instances. The proposed framework incorporates
learning capabilities to allow improving the quality and re-usability of the
reference model as it is being used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7540</identifier>
 <datestamp>2014-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7540</id><created>2014-01-29</created><updated>2014-08-20</updated><authors><author><keyname>Reidl</keyname><forenames>Felix</forenames></author><author><keyname>Rossmanith</keyname><forenames>Peter</forenames></author><author><keyname>Villaamil</keyname><forenames>Fernando Sanchez</forenames></author><author><keyname>Sikdar</keyname><forenames>Somnath</forenames></author></authors><title>A Faster Parameterized Algorithm for Treedepth</title><categories>cs.DS cs.DM</categories><comments>An extended abstract was published in ICALP 2014, Track A</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The width measure \emph{treedepth}, also known as vertex ranking, centered
coloring and elimination tree height, is a well-established notion which has
recently seen a resurgence of interest. We present an algorithm which---given
as input an $n$-vertex graph, a tree decomposition of the graph of width $w$,
and an integer $t$---decides Treedepth, i.e. whether the treedepth of the graph
is at most $t$, in time $2^{O(wt)} \cdot n$. If necessary, a witness structure
for the treedepth can be constructed in the same running time. In conjunction
with previous results we provide a simple algorithm and a fast algorithm which
decide treedepth in time $2^{2^{O(t)}} \cdot n$ and $2^{O(t^2)} \cdot n$,
respectively, which do not require a tree decomposition as part of their input.
The former answers an open question posed by Ossona de Mendez and Nesetril as
to whether deciding Treedepth admits an algorithm with a linear running time
(for every fixed $t$) that does not rely on Courcelle's Theorem or other heavy
machinery. For chordal graphs we can prove a running time of $2^{O(t \log
t)}\cdot n$ for the same algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7547</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7547</id><created>2014-01-25</created><authors><author><keyname>Arslan</keyname><forenames>Mehmet Lutfi</forenames></author><author><keyname>Seker</keyname><forenames>Sadi Evren</forenames></author></authors><title>Web Based Reputation Index of Turkish Universities</title><categories>cs.DL</categories><journal-ref>International Journal of E-Education E-Business E-Management and
  E-Learning (IJEEEE), Issn : 2010-3654, vol.4, is.3, pp.197-203, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper attempts to develop an online reputation index of Turkish
universities through their online impact and effectiveness. Using 16 different
web based parameters and employing normalization process of the results, we
have ranked websites of Turkish universities in terms of their web presence.
This index is first attempt to determine the tools of reputation of Turkish
academic websites and would be a basis for further studies to examine the
relation between reputation and the online effectiveness of the universities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7574</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7574</id><created>2014-01-29</created><updated>2015-05-16</updated><authors><author><keyname>Sun</keyname><forenames>Jie</forenames></author><author><keyname>Taylor</keyname><forenames>Dane</forenames></author><author><keyname>Bollt</keyname><forenames>Erik M.</forenames></author></authors><title>Causal Network Inference by Optimal Causation Entropy</title><categories>cs.IT math.IT</categories><msc-class>37N99, 62B10, 94A17</msc-class><journal-ref>SIAM J. Appl. Dyn. Syst. 14, 73-106 (2015)</journal-ref><doi>10.1137/140956166</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The broad abundance of time series data, which is in sharp contrast to
limited knowledge of the underlying network dynamic processes that produce such
observations, calls for a rigorous and efficient method of causal network
inference. Here we develop mathematical theory of causation entropy, an
information-theoretic statistic designed for model-free causality inference.
For stationary Markov processes, we prove that for a given node in the network,
its causal parents forms the minimal set of nodes that maximizes causation
entropy, a result we refer to as the optimal causation entropy principle.
Furthermore, this principle guides us to develop computational and data
efficient algorithms for causal network inference based on a two-step discovery
and removal algorithm for time series data for a network-couple dynamical
system. Validation in terms of analytical and numerical results for Gaussian
processes on large random networks highlight that inference by our algorithm
outperforms previous leading methods including conditioned Granger causality
and transfer entropy. Interestingly, our numerical results suggest that the
number of samples required for accurate inference depends strongly on network
characteristics such as the density of links and information diffusion rate and
not necessarily on the number of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7583</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7583</id><created>2014-01-28</created><authors><author><keyname>Kulesz</keyname><forenames>Daniel</forenames></author><author><keyname>Ostberg</keyname><forenames>Jan-Peter</forenames></author></authors><title>Practical Challenges with Spreadsheet Auditing Tools</title><categories>cs.SE</categories><comments>13 Pages. 3 Detailed Colour Figures, Proc. European Spreadsheet Risks
  Int. Grp. (EuSpRIG) 2013, ISBN: 978-1-9054045-1-3</comments><proxy>Grenville Croll</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Just like other software, spreadsheets can contain significant faults. Static
analysis is an accepted and well-established technique in software engineering
known for its capability to discover faults. In recent years, a growing number
of tool vendors started offering tools that allow casual end-users to run
various static analyses on spreadsheets as well. We supervised a study where
three undergraduate software engineering students examined a selection of 14
spreadsheet auditing tools, trying to give a concrete recommendation for an
industry partner. Reflecting on the study's results, we found that most of
these tools do provide useful aids in finding problems in spreadsheets, but we
have also spotted several areas where tools had significant issues. Some of
these issues could be remedied if spreadsheet auditing tool vendors would pick
up some ideas of static analysis tools for traditional software development and
adopt some of their solution approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7584</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7584</id><created>2014-01-28</created><authors><author><keyname>Kohlhase</keyname><forenames>Michael</forenames></author><author><keyname>Prodescu</keyname><forenames>Corneliu</forenames></author><author><keyname>Liguda</keyname><forenames>Christian</forenames></author></authors><title>XLSearch: A Search Engine for Spreadsheets</title><categories>cs.DB</categories><comments>12 Pages. 10 B&amp;W &amp; Colour Figures. Proc. European Spreadsheet Risks
  Int. Grp. (EuSpRIG) 2013, ISBN: 978-1-9054045-1-3</comments><proxy>Grenville Croll</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spreadsheets are end-user programs and domain models that are heavily
employed in administration, financial forecasting, education, and science
because of their intuitive, flexible, and direct approach to computation. As a
result, institutions are swamped by millions of spreadsheets that are becoming
increasingly difficult to manage, access, and control.
  This note presents the XLSearch system, a novel search engine for
spreadsheets. It indexes spreadsheet formulae and efficiently answers formula
queries via unification (a complex query language that allows metavariables in
both the query as well as the index). But a web-based search engine is only one
application of the underlying technology: Spreadsheet formula export to web
standards like MathML combined with formula indexing can be used to find
similar spreadsheets or common formula errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7586</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7586</id><created>2014-01-28</created><authors><author><keyname>O'Beirne</keyname><forenames>Patrick</forenames></author></authors><title>Excel 2013 Spreadsheet Inquire</title><categories>cs.SE</categories><comments>22 Pages with 23 large and highly detailed colour figures, Proc.
  European Spreadsheet Risks Int. Grp. (EuSpRIG) 2013, ISBN: 978-1-9054045-1-3</comments><proxy>Grenville Croll</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Excel 2013 (version 15) includes an add-in &quot;Inquire&quot; for auditing
spreadsheets. We describe the evolution of such tools in the third-party
marketplace and assess the usefulness of Microsoft's own add-in in this
context. We compare in detail the features of Inquire with similar products and
make suggestions for how it could be enhanced. We offer a free helper add-in
that in our opinion corrects one major shortcoming of Inquire.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7591</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7591</id><created>2014-01-29</created><authors><author><keyname>Demetrescu</keyname><forenames>Camil</forenames></author><author><keyname>Finocchi</keyname><forenames>Irene</forenames></author><author><keyname>Italiano</keyname><forenames>Giuseppe F.</forenames></author><author><keyname>Laura</keyname><forenames>Luigi</forenames></author></authors><title>Experimental Evaluation of Algorithms for the Food-Selection Problem</title><categories>cs.DS</categories><comments>This paper discuss the problem of eating good food in Rome :-) This
  is the latest version, the one that has been distributed to people
  participants of the SEA 2013 conference. Previous versions of this paper have
  been distributed to participants of FOCS 2004, CIAC 2006, ICTCS 2007, WEA
  2007, WINE 2009, OPODIS 2012, and WSDM 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe the result of our experiments on Algorithms for
the Food-Selection Problem, which is the fundamental problem first stated and
addressed in the seminal paper \cite{pigout}. Because the key aspect of any
experimental evaluation is the \textbf{reproducibility}, we detail deeply the
setup of all our experiments, thus leaving to the interested eater the
opportunity to reproduce all the results described in this paper. More
specifically, we describe all the answers we provided to the questions proposed
in \cite{pigout}: Where can I have dinner tonight? What is the typical Roman
cuisine that I should (not) miss? Where can I find the best coffee or gelato in
town?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7594</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7594</id><created>2014-01-29</created><authors><author><keyname>Lin</keyname><forenames>Ching-Chi</forenames></author><author><keyname>Tu</keyname><forenames>Hai-Lun</forenames></author></authors><title>Linear-Time Algorithms for the Paired-Domination Problem in Interval
  Graphs and Circular-Arc Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a graph $G$, a vertex subset $S\subseteq V(G)$ is said to be a dominating
set of $G$ if every vertex not in $S$ is adjacent to a vertex in $S$. A
dominating set $S$ of a graph $G$ is called a paired-dominating set if the
induced subgraph $G[S]$ contains a perfect matching. The paired-domination
problem involves finding a smallest paired-dominating set of $G$. Given an
intersection model of an interval graph $G$ with sorted endpoints, Cheng et al.
designed an $O(m+n)$-time algorithm for interval graphs and an $O(m(m+n))$-time
algorithm for circular-arc graphs. In this paper, to solve the
paired-domination problem in interval graphs, we propose an $O(n)$-time
algorithm that searches for a minimum paired-dominating set of $G$
incrementally in a greedy manner. Then, we extend the results to design an
algorithm for circular-arc graphs that also runs in $O(n)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7612</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7612</id><created>2014-01-12</created><updated>2014-09-30</updated><authors><author><keyname>Taylor-King</keyname><forenames>Jake P.</forenames></author><author><keyname>Franz</keyname><forenames>Benjamin</forenames></author><author><keyname>Yates</keyname><forenames>Christian A.</forenames></author><author><keyname>Erban</keyname><forenames>Radek</forenames></author></authors><title>Mathematical Modelling of Turning Delays in Swarm Robotics</title><categories>cs.RO cs.SY</categories><comments>Submitted to the IMA Journal of Applied Mathematics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the effect of turning delays on the behaviour of groups of
differential wheeled robots and show that the group-level behaviour can be
described by a transport equation with a suitably incorporated delay. The
results of our mathematical analysis are supported by numerical simulations and
experiments with e-puck robots. The experimental quantity we compare to our
revised model is the mean time for robots to find the target area in an unknown
environment. The transport equation with delay better predicts the mean time to
find the target than the standard transport equation without delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7616</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7616</id><created>2014-01-29</created><updated>2014-07-10</updated><authors><author><keyname>Mitzenmacher</keyname><forenames>Michael</forenames></author></authors><title>A New Approach to Analyzing Robin Hood Hashing</title><categories>cs.DS</categories><comments>19 pages, draft version. Updated from the previous version with some
  new proofs, in particular a full proof of the log log n + O(1) maximum age
  bound in the static setting, by converting the fluid limit argument into a
  layered induction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robin Hood hashing is a variation on open addressing hashing designed to
reduce the maximum search time as well as the variance in the search time for
elements in the hash table. While the case of insertions only using Robin Hood
hashing is well understood, the behavior with deletions has remained open. Here
we show that Robin Hood hashing can be analyzed under the framework of
finite-level finite-dimensional jump Markov chains. This framework allows us to
re-derive some past results for the insertion-only case with some new insight,
as well as provide a new analysis for a standard deletion model, where we
alternate between deleting a random old key and inserting a new one. In
particular, we show that a simple but apparently unstudied approach for
handling deletions with Robin Hood hashing offers good performance even under
high loads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7620</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7620</id><created>2014-01-29</created><authors><author><keyname>Ruiz</keyname><forenames>Francisco J. R.</forenames></author><author><keyname>Valera</keyname><forenames>Isabel</forenames></author><author><keyname>Blanco</keyname><forenames>Carlos</forenames></author><author><keyname>Perez-Cruz</keyname><forenames>Fernando</forenames></author></authors><title>Bayesian nonparametric comorbidity analysis of psychiatric disorders</title><categories>stat.ML cs.LG</categories><comments>Submitted to Journal of Machine Learning Research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of comorbidity is an open and complex research field in the
branch of psychiatry, where clinical experience and several studies suggest
that the relation among the psychiatric disorders may have etiological and
treatment implications. In this paper, we are interested in applying latent
feature modeling to find the latent structure behind the psychiatric disorders
that can help to examine and explain the relationships among them. To this end,
we use the large amount of information collected in the National Epidemiologic
Survey on Alcohol and Related Conditions (NESARC) database and propose to model
these data using a nonparametric latent model based on the Indian Buffet
Process (IBP). Due to the discrete nature of the data, we first need to adapt
the observation model for discrete random variables. We propose a generative
model in which the observations are drawn from a multinomial-logit distribution
given the IBP matrix. The implementation of an efficient Gibbs sampler is
accomplished using the Laplace approximation, which allows integrating out the
weighting factors of the multinomial-logit likelihood model. We also provide a
variational inference algorithm for this model, which provides a complementary
(and less expensive in terms of computational complexity) alternative to the
Gibbs sampler allowing us to deal with a larger number of data. Finally, we use
the model to analyze comorbidity among the psychiatric disorders diagnosed by
experts from the NESARC database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7623</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7623</id><created>2014-01-29</created><updated>2014-10-12</updated><authors><author><keyname>Aflalo</keyname><forenames>Yonathan</forenames></author><author><keyname>Bronstein</keyname><forenames>Alex</forenames></author><author><keyname>Kimmel</keyname><forenames>Ron</forenames></author></authors><title>Graph matching: relax or not?</title><categories>cs.DS cs.CG cs.CV math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of exact and inexact matching of weighted undirected
graphs, in which a bijective correspondence is sought to minimize a quadratic
weight disagreement. This computationally challenging problem is often relaxed
as a convex quadratic program, in which the space of permutations is replaced
by the space of doubly-stochastic matrices. However, the applicability of such
a relaxation is poorly understood. We define a broad class of friendly graphs
characterized by an easily verifiable spectral property. We prove that for
friendly graphs, the convex relaxation is guaranteed to find the exact
isomorphism or certify its inexistence. This result is further extended to
approximately isomorphic graphs, for which we develop an explicit bound on the
amount of weight disagreement under which the relaxation is guaranteed to find
the globally optimal approximate isomorphism. We also show that in many cases,
the graph matching problem can be further harmlessly relaxed to a convex
quadratic program with only n separable linear equality constraints, which is
substantially more efficient than the standard relaxation involving 2n equality
and n^2 inequality constraints. Finally, we show that our results are still
valid for unfriendly graphs if additional information in the form of seeds or
attributes is allowed, with the latter satisfying an easy to verify spectral
characteristic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7625</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7625</id><created>2014-01-29</created><authors><author><keyname>Mokhtari</keyname><forenames>Aryan</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>RES: Regularized Stochastic BFGS Algorithm</title><categories>cs.LG math.OC stat.ML</categories><comments>13 pages</comments><doi>10.1109/TSP.2014.2357775</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RES, a regularized stochastic version of the Broyden-Fletcher-Goldfarb-Shanno
(BFGS) quasi-Newton method is proposed to solve convex optimization problems
with stochastic objectives. The use of stochastic gradient descent algorithms
is widespread, but the number of iterations required to approximate optimal
arguments can be prohibitive in high dimensional problems. Application of
second order methods, on the other hand, is impracticable because computation
of objective function Hessian inverses incurs excessive computational cost.
BFGS modifies gradient descent by introducing a Hessian approximation matrix
computed from finite gradient differences. RES utilizes stochastic gradients in
lieu of deterministic gradients for both, the determination of descent
directions and the approximation of the objective function's curvature. Since
stochastic gradients can be computed at manageable computational cost RES is
realizable and retains the convergence rate advantages of its deterministic
counterparts. Convergence results show that lower and upper bounds on the
Hessian egeinvalues of the sample functions are sufficient to guarantee
convergence to optimal arguments. Numerical experiments showcase reductions in
convergence time relative to stochastic gradient descent algorithms and
non-regularized stochastic versions of BFGS. An application of RES to the
implementation of support vector machines is developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7631</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7631</id><created>2014-01-29</created><authors><author><keyname>Melnikova</keyname><forenames>N. B.</forenames></author><author><keyname>Jordan</keyname><forenames>D.</forenames></author><author><keyname>Krzhizhanovskaya</keyname><forenames>V. V.</forenames></author><author><keyname>Sloot</keyname><forenames>P. M. A.</forenames></author></authors><title>Slope Instability of the Earthen Levee in Boston, UK: Numerical
  Simulation and Sensor Data Analysis</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a slope stability analysis for a heterogeneous earthen
levee in Boston, UK, which is prone to occasional slope failures under tidal
loads. Dynamic behavior of the levee under tidal fluctuations was simulated
using a finite element model of variably saturated linear elastic perfectly
plastic soil. Hydraulic conductivities of the soil strata have been calibrated
according to piezometers readings, in order to obtain correct range of
hydraulic loads in tidal mode. Finite element simulation was complemented with
series of limit equilibrium analyses. Stability analyses have shown that slope
failure occurs with the development of a circular slip surface located in the
soft clay layer. Both models (FEM and LEM) confirm that the least stable
hydraulic condition is the combination of the minimum river levels at low tide
with the maximal saturation of soil layers. FEM results indicate that in winter
time the levee is almost at its limit state, at the margin of safety (strength
reduction factor values are 1.03 and 1.04 for the low-tide and high-tide
phases, respectively); these results agree with real-life observations. The
stability analyses have been implemented as real-time components integrated
into the UrbanFlood early warning system for flood protection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7635</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7635</id><created>2014-01-29</created><authors><author><keyname>Baudry</keyname><forenames>Benoit</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Allier</keyname><forenames>Simon</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Tailored Source Code Transformations to Synthesize Computationally
  Diverse Program Variants</title><categories>cs.SE</categories><proxy>ccsd</proxy><doi>10.1145/2610384.2610415</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The predictability of program execution provides attackers a rich source of
knowledge who can exploit it to spy or remotely control the program. Moving
target defense addresses this issue by constantly switching between many
diverse variants of a program, which reduces the certainty that an attacker can
have about the program execution. The effectiveness of this approach relies on
the availability of a large number of software variants that exhibit different
executions. However, current approaches rely on the natural diversity provided
by off-the-shelf components, which is very limited. In this paper, we explore
the automatic synthesis of large sets of program variants, called sosies.
Sosies provide the same expected functionality as the original program, while
exhibiting different executions. They are said to be computationally diverse.
This work addresses two objectives: comparing different transformations for
increasing the likelihood of sosie synthesis (densifying the search space for
sosies); demonstrating computation diversity in synthesized sosies. We
synthesized 30184 sosies in total, for 9 large, real-world, open source
applications. For all these programs we identified one type of program analysis
that systematically increases the density of sosies; we measured computation
diversity for sosies of 3 programs and found diversity in method calls or data
in more than 40% of sosies. This is a step towards controlled massive
unpredictability of software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7651</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7651</id><created>2014-01-29</created><authors><author><keyname>Yazici</keyname><forenames>Volkan</forenames></author><author><keyname>Sunay</keyname><forenames>M. Oguz</forenames></author><author><keyname>Ercan</keyname><forenames>Ali O.</forenames></author></authors><title>Controlling a Software-Defined Network via Distributed Controllers</title><categories>cs.NI</categories><comments>6 pages, 4 figures</comments><journal-ref>Proceedings of the 2012 NEM Summit, pp. 16-20, Istanbul, Turkey,
  October 16-18, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a distributed OpenFlow controller and an associated
coordination framework that achieves scalability and reliability even under
heavy data center loads. The proposed framework, which is designed to work with
all existing OpenFlow controllers with minimal or no required changes, provides
support for dynamic addition and removal of controllers to the cluster without
any interruption to the network operation. We demonstrate performance results
of the proposed framework implemented over an experimental testbed that uses
controllers running Beacon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7694</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7694</id><created>2014-01-29</created><updated>2014-04-17</updated><authors><author><keyname>Gross</keyname><forenames>Jason</forenames></author><author><keyname>Chlipala</keyname><forenames>Adam</forenames></author><author><keyname>Spivak</keyname><forenames>David I.</forenames></author></authors><title>Experience Implementing a Performant Category-Theory Library in Coq</title><categories>math.CT cs.LO</categories><comments>The final publication will be available at link.springer.com. This
  version includes a full bibliography which does not fit in the Springer
  version; other than the more complete references, this is the version
  submitted as a final copy to ITP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe our experience implementing a broad category-theory library in
Coq. Category theory and computational performance are not usually mentioned in
the same breath, but we have needed substantial engineering effort to teach Coq
to cope with large categorical constructions without slowing proof script
processing unacceptably. In this paper, we share the lessons we have learned
about how to represent very abstract mathematical objects and arguments in Coq
and how future proof assistants might be designed to better support such
reasoning. One particular encoding trick to which we draw attention allows
category-theoretic arguments involving duality to be internalized in Coq's
logic with definitional equality. Ours may be the largest Coq development to
date that uses the relatively new Coq version developed by homotopy type
theorists, and we reflect on which new features were especially helpful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7700</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7700</id><created>2014-01-29</created><updated>2015-06-19</updated><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author></authors><title>Random assignment with multi-unit demands</title><categories>cs.GT</categories><comments>17 pages</comments><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the multi-unit random assignment problem in which agents express
preferences over objects and objects are allocated to agents randomly based on
the preferences. The most well-established preference relation to compare
random allocations of objects is stochastic dominance (SD) which also leads to
corresponding notions of envy-freeness, efficiency, and weak strategyproofness.
We show that there exists no rule that is anonymous, neutral, efficient and
weak strategyproof. For single-unit random assignment, we show that there
exists no rule that is anonymous, neutral, efficient and weak
group-strategyproof. We then study a generalization of the PS (probabilistic
serial) rule called multi-unit-eating PS and prove that multi-unit-eating PS
satisfies envy-freeness, weak strategyproofness, and unanimity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7702</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7702</id><created>2014-01-29</created><updated>2014-10-22</updated><authors><author><keyname>Miller</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Beard</keyname><forenames>Michelle S.</forenames></author><author><keyname>Wolfe</keyname><forenames>Patrick J.</forenames></author><author><keyname>Bliss</keyname><forenames>Nadya T.</forenames></author></authors><title>A Spectral Framework for Anomalous Subgraph Detection</title><categories>cs.SI stat.ML</categories><comments>In submission to the IEEE, 16 pages, 8 figures</comments><journal-ref>IEEE Trans. Signal Process. 63 (2015) 4191-4206</journal-ref><doi>10.1109/TSP.2015.2437841</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide variety of application domains are concerned with data consisting of
entities and their relationships or connections, formally represented as
graphs. Within these diverse application areas, a common problem of interest is
the detection of a subset of entities whose connectivity is anomalous with
respect to the rest of the data. While the detection of such anomalous
subgraphs has received a substantial amount of attention, no
application-agnostic framework exists for analysis of signal detectability in
graph-based data. In this paper, we describe a framework that enables such
analysis using the principal eigenspace of a graph's residuals matrix, commonly
called the modularity matrix in community detection. Leveraging this analytical
tool, we show that the framework has a natural power metric in the spectral
norm of the anomalous subgraph's adjacency matrix (signal power) and of the
background graph's residuals matrix (noise power). We propose several
algorithms based on spectral properties of the residuals matrix, with more
computationally expensive techniques providing greater detection power.
Detection and identification performance are presented for a number of signal
and noise models, including clusters and bipartite foregrounds embedded into
simple random backgrounds as well as graphs with community structure and
realistic degree distributions. The trends observed verify intuition gleaned
from other signal processing areas, such as greater detection power when the
signal is embedded within a less active portion of the background. We
demonstrate the utility of the proposed techniques in detecting small, highly
anomalous subgraphs in real graphs derived from Internet traffic and product
co-purchases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7709</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7709</id><created>2014-01-29</created><authors><author><keyname>Chakrabarti</keyname><forenames>Deepayan</forenames></author><author><keyname>Funiak</keyname><forenames>Stanislav</forenames></author><author><keyname>Chang</keyname><forenames>Jonathan</forenames></author><author><keyname>Macskassy</keyname><forenames>Sofus A.</forenames></author></authors><title>Joint Inference of Multiple Label Types in Large Networks</title><categories>cs.LG cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tackle the problem of inferring node labels in a partially labeled graph
where each node in the graph has multiple label types and each label type has a
large number of possible labels. Our primary example, and the focus of this
paper, is the joint inference of label types such as hometown, current city,
and employers, for users connected by a social network. Standard label
propagation fails to consider the properties of the label types and the
interactions between them. Our proposed method, called EdgeExplain, explicitly
models these, while still enabling scalable inference under a distributed
message-passing architecture. On a billion-node subset of the Facebook social
network, EdgeExplain significantly outperforms label propagation for several
label types, with lifts of up to 120% for recall@1 and 60% for recall@3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7713</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7713</id><created>2014-01-29</created><authors><author><keyname>Liu</keyname><forenames>Lingqiao</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author></authors><title>A Generalized Probabilistic Framework for Compact Codebook Creation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compact and discriminative visual codebooks are preferred in many visual
recognition tasks. In the literature, a number of works have taken the approach
of hierarchically merging visual words of an initial large-sized codebook, but
implemented this approach with different merging criteria. In this work, we
propose a single probabilistic framework to unify these merging criteria, by
identifying two key factors: the function used to model class-conditional
distribution and the method used to estimate the distribution parameters. More
importantly, by adopting new distribution functions and/or parameter estimation
methods, our framework can readily produce a spectrum of novel merging
criteria. Three of them are specifically focused in this work. In the first
criterion, we adopt the multinomial distribution with Bayesian method; In the
second criterion, we integrate Gaussian distribution with maximum likelihood
parameter estimation. In the third criterion, which shows the best merging
performance, we propose a max-margin-based parameter estimation method and
apply it with multinomial distribution. Extensive experimental study is
conducted to systematically analyse the performance of the above three criteria
and compare them with existing ones. As demonstrated, the best criterion
obtained in our framework achieves the overall best merging performance among
the comparable merging criteria developed in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7714</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7714</id><created>2014-01-29</created><authors><author><keyname>Gall</keyname><forenames>Fran&#xe7;ois Le</forenames></author></authors><title>Powers of Tensors and Fast Matrix Multiplication</title><categories>cs.DS cs.CC cs.SC</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method to analyze the powers of a given trilinear form
(a special kind of algebraic constructions also called a tensor) and obtain
upper bounds on the asymptotic complexity of matrix multiplication. Compared
with existing approaches, this method is based on convex optimization, and thus
has polynomial-time complexity. As an application, we use this method to study
powers of the construction given by Coppersmith and Winograd [Journal of
Symbolic Computation, 1990] and obtain the upper bound $\omega&lt;2.3728639$ on
the exponent of square matrix multiplication, which slightly improves the best
known upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7715</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7715</id><created>2014-01-29</created><updated>2014-02-01</updated><authors><author><keyname>Shi</keyname><forenames>Jianing V.</forenames></author><author><keyname>Yin</keyname><forenames>Wotao</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Aswin C.</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Video Compressive Sensing for Dynamic MRI</title><categories>cs.CV math.OC</categories><comments>30 pages, 9 figures</comments><msc-class>90-08, 90C25, 65P99, 65K10, 93E10, 93E12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a video compressive sensing framework, termed kt-CSLDS, to
accelerate the image acquisition process of dynamic magnetic resonance imaging
(MRI). We are inspired by a state-of-the-art model for video compressive
sensing that utilizes a linear dynamical system (LDS) to model the motion
manifold. Given compressive measurements, the state sequence of an LDS can be
first estimated using system identification techniques. We then reconstruct the
observation matrix using a joint structured sparsity assumption. In particular,
we minimize an objective function with a mixture of wavelet sparsity and joint
sparsity within the observation matrix. We derive an efficient convex
optimization algorithm through alternating direction method of multipliers
(ADMM), and provide a theoretical guarantee for global convergence. We
demonstrate the performance of our approach for video compressive sensing, in
terms of reconstruction accuracy. We also investigate the impact of various
sampling strategies. We apply this framework to accelerate the acquisition
process of dynamic MRI and show it achieves the best reconstruction accuracy
with the least computational time compared with existing algorithms in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7717</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7717</id><created>2014-01-29</created><authors><author><keyname>Abou-zeid</keyname><forenames>Hatem</forenames></author><author><keyname>Hassanein</keyname><forenames>Hossam S.</forenames></author></authors><title>Predictive Green Wireless Access: Exploiting Mobility and Application
  Information</title><categories>cs.NI</categories><journal-ref>IEEE Wireless Communications Magazine, vol. 20, no. 5, pp. 92-99,
  Oct. 2013</journal-ref><doi>10.1109/MWC.2013.6664479</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ever increasing mobile data traffic and dense deployment of wireless
networks have made energy efficient radio access imperative. As networks are
designed to satisfy peak user demands, radio access energy can be reduced in a
number of ways at times of lower demand. This includes putting base stations
(BSs) to intermittent short sleep modes during low load, as well as adaptively
powering down select BSs completely where demand is low for prolonged time
periods. In order to fully exploit such energy conserving mechanisms, networks
should be aware of the user temporal and spatial traffic demands. To this end,
this article investigates the potential of utilizing predictions of user
location and application information as a means to energy saving. We discuss
the development of a predictive green wireless access (PreGWA) framework and
identify its key functional entities and their interaction. To demonstrate the
potential energy savings we then provide a case study on stored video streaming
and illustrate how exploiting predictions can minimize BS resource consumption
within a single cell, and across a network of cells. Finally, to emphasize the
practical potential of PreGWA, we present a distributed heuristic that reduces
resource consumption significantly without requiring considerable information
or signaling overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7727</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7727</id><created>2014-01-29</created><authors><author><keyname>Biggio</keyname><forenames>Battista</forenames></author><author><keyname>Corona</keyname><forenames>Igino</forenames></author><author><keyname>Nelson</keyname><forenames>Blaine</forenames></author><author><keyname>Rubinstein</keyname><forenames>Benjamin I. P.</forenames></author><author><keyname>Maiorca</keyname><forenames>Davide</forenames></author><author><keyname>Fumera</keyname><forenames>Giorgio</forenames></author><author><keyname>Giacinto</keyname><forenames>Giorgio</forenames></author><author><keyname>Roli</keyname><forenames>and Fabio</forenames></author></authors><title>Security Evaluation of Support Vector Machines in Adversarial
  Environments</title><categories>cs.LG cs.CR</categories><comments>47 pages, 9 figures; chapter accepted into book 'Support Vector
  Machine Applications'</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support Vector Machines (SVMs) are among the most popular classification
techniques adopted in security applications like malware detection, intrusion
detection, and spam filtering. However, if SVMs are to be incorporated in
real-world security systems, they must be able to cope with attack patterns
that can either mislead the learning algorithm (poisoning), evade detection
(evasion), or gain information about their internal parameters (privacy
breaches). The main contributions of this chapter are twofold. First, we
introduce a formal general framework for the empirical evaluation of the
security of machine-learning systems. Second, according to our framework, we
demonstrate the feasibility of evasion, poisoning and privacy attacks against
SVMs in real-world security problems. For each attack technique, we evaluate
its impact and discuss whether (and how) it can be countered through an
adversary-aware design of SVMs. Our experiments are easily reproducible thanks
to open-source code that we have made available, together with all the employed
datasets, on a public repository.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7733</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7733</id><created>2014-01-29</created><authors><author><keyname>Kumar</keyname><forenames>C. Sunil</forenames></author><author><keyname>Seetha</keyname><forenames>J.</forenames></author><author><keyname>Vinotha</keyname><forenames>S. R.</forenames></author></authors><title>Security Implications of Distributed Database Management System Models</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security features must be addressed when escalating a distributed database.
The choice between the object oriented and the relational data model, several
factors should be considered. The most important of these factors are single
and multilevel access controls (MAC), protection and integrity maintenance.
While determining which distributed database replica will be more secure for a
particular function, the choice should not be made exclusively on the basis of
available security features. One should also query the effectiveness and
efficiency of the delivery of these characteristics. In this paper, the
security strengths and weaknesses of both database models and the thorough
problems initiate in the distributed environment are conversed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7735</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7735</id><created>2014-01-29</created><authors><author><keyname>Tewari</keyname><forenames>Anuj</forenames></author><author><keyname>Goyal</keyname><forenames>Nitesh</forenames></author><author><keyname>Chan</keyname><forenames>Matthew K</forenames></author><author><keyname>Yau</keyname><forenames>Tina</forenames></author><author><keyname>Canny</keyname><forenames>John</forenames></author><author><keyname>Schroeder</keyname><forenames>Ulrik</forenames></author></authors><title>SPRING: speech and pronunciation improvement through games, for Hispanic
  children</title><categories>cs.HC</categories><comments>ACM ICTD 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lack of proper English pronunciations is a major problem for immigrant
population in developed countries like U.S. This poses various problems,
including a barrier to entry into mainstream society. This paper presents a
research study that explores the use of speech technologies merged with
activity-based and arcade-based games to do pronunciation feedback for Hispanic
children within the U.S. A 3-month long study with immigrant population in
California was used to investigate and analyze the effectiveness of computer
aided pronunciation feedback through games. In addition to quantitative
findings that point to statistically significant gains in pronunciation
quality, the paper also explores qualitative findings, interaction patterns and
challenges faced by the researchers in dealing with this community. It also
describes the issues involved in dealing with pronunciation as a competency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7739</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7739</id><created>2014-01-30</created><authors><author><keyname>Lanzon</keyname><forenames>A.</forenames></author><author><keyname>Petersen</keyname><forenames>I. R.</forenames></author></authors><title>Stability robustness of a feedback interconnection of systems with
  negative imaginary frequency response</title><categories>math.OC cs.SY</categories><journal-ref>IEEE Transactions on Automatic Control, vol 53, no 4, pp 1042-1046
  2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A necessary and sufficient condition, expressed simply as the DC loop gain
(ie the loop gain at zero frequency) being less than unity, is given in this
paper to guarantee the internal stability of a feedback interconnection of
Linear Time-Invariant (LTI) Multiple-Input Multiple-Output (MIMO) systems with
negative imaginary frequency response. Systems with negative imaginary
frequency response arise for example when considering transfer functions from
force actuators to co-located position sensors, and are commonly important in
for example lightly damped structures. The key result presented here has
similar application to the small-gain theorem, which refers to the stability of
feedback interconnections of contractive gain systems, and the passivity
theorem (or more precisely the positive real theorem in the LTI case), which
refers to the stability of feedback interconnections of positive real systems.
A complete state-space characterisation of systems with negative imaginary
frequency response is also given in this paper and also an example that
demonstrates the application of the key result is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7741</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7741</id><created>2014-01-30</created><authors><author><keyname>Bulut</keyname><forenames>Mevlut</forenames></author></authors><title>ReducedCBT and SuperCBT, Two New and Improved Complete Binary Tree
  Structures</title><categories>cs.DS</categories><comments>12 pages research paper, contains 6 figures and a table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Between the leaves and the nodes of a complete binary tree, a separate
parent-child-sister hierarchy is employed independent of the
parent-child-sister hierarchy used for the rest of the tree. Two different
versions of such a local hierarchy are introduced. The result of the first
proposed hierarchy is a faster and smaller footprint, while the second one
provides the size variation functionality without a significant computational
overhead. This novel approach brings considerable memory gains and performance
boosts to the complete binary tree based algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7743</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7743</id><created>2014-01-30</created><authors><author><keyname>Balaji</keyname><forenames>T.</forenames></author><author><keyname>Sumathi</keyname><forenames>Dr. M.</forenames></author></authors><title>Effective Features of Remote Sensing Image Classification Using
  Interactive Adaptive Thresholding Method</title><categories>cs.CV</categories><comments>5 pages,International Conference on Intelligent Computing
  Applications - ICICA 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Remote sensing image classification can be performed in many different ways
to extract meaningful features. One common approach is to perform edge
detection. A second approach is to try and detect whole shapes, given the fact
that these shapes usually tend to have distinctive properties such as object
foreground or background. To get optimal results, these two approaches can be
combined. This paper adopts a combinatorial optimization method to adaptively
select threshold based features to improve remote sensing image. Feature
selection is an important combinatorial optimization problem in the remote
sensing image classification. The feature selection method has to achieve three
characteristics: first the performance issues by facilitating data collection
and reducing storage space and classification time, second to perform semantics
analysis helping to understand the problem, and third to improve prediction
accuracy by avoiding the curse of dimensionality. The goal of this thresholding
an image is to classify pixels as either dark or light and evaluation of
classification results. Interactive adaptive thresholding is a form of
thresholding that takes into account spatial variations in illumination of
remote sensing image. We present a technique for remote sensing based adaptive
thresholding using the interactive satellite image of the input. However, our
solution is more robust to illumination changes in the remote sensing image.
Additionally, our method is simple and easy to implement but it is effective
algorithm to classify the image pixels. This technique is suitable for
preprocessing the remote sensing image classification, making it a valuable
tool for interactive remote based applications such as augmented reality of the
classification procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7745</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7745</id><created>2014-01-30</created><authors><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Lanzon</keyname><forenames>Alexander</forenames></author></authors><title>Feedback Control of Negative-Imaginary Systems: Large Flexible
  structures with colocated actuators and sensors</title><categories>cs.SY math.OC</categories><journal-ref>Control Systems Magazine, vol. 30, no. 5, pp. 54 - 72, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a survey of recent results on the theory of negative
imaginary systems. This theory can be applied to the robust control of large
flexible structures with colocated force actuators and position sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7758</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7758</id><created>2014-01-30</created><updated>2014-02-04</updated><authors><author><keyname>Elberzhager</keyname><forenames>Frank</forenames></author><author><keyname>Kremer</keyname><forenames>Stephan</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Assmann</keyname><forenames>Danilo</forenames></author></authors><title>Focusing Testing by Using Inspection and Product Metrics</title><categories>cs.SE</categories><comments>29 pages. The final publication is available at
  http://www.worldscientific.com/doi/abs/10.1142/s0218194013400093. arXiv admin
  note: substantial text overlap with arXiv:1312.0713</comments><journal-ref>International Journal of Software Engineering and Knowledge
  Engineering, 23(04):433-462, 2013</journal-ref><doi>10.1142/S0218194013400093</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A well-known approach for identifying defect-prone parts of software in order
to focus testing is to use different kinds of product metrics such as size or
complexity. Although this approach has been evaluated in many contexts, the
question remains if there are further opportunities to improve test focusing.
One idea is to identify other types of information that may indicate the
location of defect-prone software parts. Data from software inspections, in
particular, appear to be promising. This kind of data might already lead to
software parts that have inherent difficulties or programming challenges, and
in consequence might be defect-prone. This article first explains how
inspection and product metrics can be used to focus testing activities. Second,
we compare selected product and inspection metrics commonly used to predict
defect-prone parts (e.g., size and complexity metrics, inspection defect
content metrics, and defect density metrics). Based on initial experience from
two case studies performed in different environments, the suitability of
different metrics for predicting defect-prone parts is illustrated. The studies
revealed that inspection defect data seems to be a suitable predictor, and a
combination of certain inspection and product metrics led to the best
prioritizations in our contexts. In addition, qualitative experience is
presented, which substantiates the expected benefit of using inspection results
to optimize testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7772</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7772</id><created>2014-01-30</created><authors><author><keyname>Alaa</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Ismail</keyname><forenames>Mahmoud H.</forenames></author><author><keyname>Tawfik</keyname><forenames>Hazim</forenames></author></authors><title>Spectrum Sensing Via Reconfigurable Antennas: Is Cooperation of
  Secondary Users Indispensable?</title><categories>cs.NI cs.IT math.IT</categories><comments>Accepted for IEEE WCNC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents an analytical framework for characterizing the performance
of cooperative and noncooperative spectrum sensing schemes by figuring out the
tradeoff between the achieved diversity and coding gains in each scheme. Based
on this analysis, we try to answer the fundamental question: can we dispense
with SUs cooperation and still achieve an arbitrary diversity gain? It is shown
that this is indeed possible via a novel technique that can offer diversity
gain for a single SU using a single antenna. The technique is based on the
usage of a reconfigurable antenna that changes its propagation characteristics
over time, thus creating an artificial temporal diversity. It is shown that the
usage of reconfigurable antennas outperforms cooperative as well as
non-cooperative schemes at low and high Signal-to-Noise Ratios (SNRs).
Moreover, if the channel state information is available at the SU, an
additional SNR gain can also be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7799</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7799</id><created>2014-01-30</created><authors><author><keyname>Hawkins</keyname><forenames>Ted</forenames></author><author><keyname>Lemon</keyname><forenames>Andrew</forenames></author><author><keyname>Gibson</keyname><forenames>Alec</forenames></author></authors><title>Introducing Morphit, a new type of spreadsheet technology</title><categories>cs.SE</categories><comments>6 Pages, 4 colour figures, Proc. European Spreadsheet Risks Int. Grp.
  (EuSpRIG) 2013, ISBN: 978-1-9054045-1-3</comments><proxy>Grenville Croll</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new type of spreadsheet which mitigates the errors
caused by incorrect range referencing in formulae. This spreadsheet is composed
of structured worksheets called tables which contain a hierarchical
organization of fields. Formulae are defined at the field-level removing the
need for positional references. In addition, relationships can be defined
between fields in tables, allowing data to be modeled rather than simply
processed and providing a re-usable framework for authoring spreadsheets. We
shall describe the key features of tables with an emphasis on error detection
and avoidance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7807</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7807</id><created>2014-01-30</created><authors><author><keyname>Pitts</keyname><forenames>Andrew M.</forenames></author></authors><title>An Equivalent Presentation of the Bezem-Coquand-Huber Category of
  Cubical Sets</title><categories>cs.LO</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Staton has shown that there is an equivalence between the category of
presheaves on (the opposite of) finite sets and partial bijections and the
category of nominal restriction sets: see [2, Exercise 9.7]. The aim here is to
see that this extends to an equivalence between the category of cubical sets
introduced in [1] and a category of nominal sets equipped with a
&quot;01-substitution&quot; operation. It seems to me that presenting the topos in
question equivalently as 01-substitution sets rather than cubical sets will
make it easier (and more elegant) to carry out the constructions and
calculations needed to build the intended univalent model of intentional
constructive type theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7814</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7814</id><created>2014-01-30</created><authors><author><keyname>Vlootman</keyname><forenames>Henk</forenames></author><author><keyname>Hermans</keyname><forenames>Felienne</forenames></author></authors><title>A Maintainability Checklist for Spreadsheets</title><categories>cs.SE</categories><comments>10 pages, 2 tables, 1 colour figure, Proc. European Spreadsheet Risks
  Int. Grp. (EuSpRIG) 2013, ISBN: 978-1-9054045-1-3</comments><proxy>Grenville Croll</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spreadsheets are widely used in industry, because they are flexible and easy
to use. Often, they are even used for business-critical applications. It is
however difficult for spreadsheet users to correctly assess the maintainability
of spreadsheets. Maintainability of spreadsheets is important, since
spreadsheets often have a long lifespan, during which they are used by several
users. In this paper, we present a checklist aimed at measuring the
maintainability of a spreadsheet. This is achieved via asking several
questions, and can indicate whether the spreadsheet is safe to use now and in
the future. We demonstrate the applicability of our approach on 11 spreadsheets
from the EUSES corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7821</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7821</id><created>2014-01-30</created><authors><author><keyname>Allen</keyname><forenames>Stephen</forenames></author></authors><title>Spatial Modelling Techniques in Microsoft Excel</title><categories>cs.HC</categories><comments>9 pages, 5 colour figures, 1 table, Proc. European Spreadsheet Risks
  Int. Grp. (EuSpRIG) 2013, ISBN: 978-1-9054045-1-3</comments><proxy>Grenville Croll</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We begin by considering the expectations of the creators of VisiCalc, the
first spreadsheet. The emphasis is on the nature of the spreadsheet grid. The
grid is taken as a presentational method for showing a solution to a Sudoku
puzzle. We consider methods or approaches for the solution. We look at the
relationship between this model and academic papers on the methods for
describing and categorising end-user models generally. We consider whether the
type of model described here should be categorised separately. The complexity
of the model is reviewed in the context of commendations to minimise overly
sophisticated presentational constructs and formulae.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7826</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7826</id><created>2014-01-30</created><authors><author><keyname>Mansouri</keyname><forenames>Ali</forenames></author><author><keyname>bouhlel</keyname><forenames>Mohamed Salim</forenames></author></authors><title>The result for the grundy number on p4 classes</title><categories>cs.DM math.CO</categories><comments>5 pages</comments><journal-ref>International Journal of Next-Generation Networks (IJNGN) Vol.5,
  No.4, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our work becomes integrated into the general problem of the stability of the
network ad hoc. Some, works attacked (affected) this problem. Among these
works, we find the modelling of the network ad hoc in the form of a graph. We
can resume the problem of coherence of the network ad hoc of a problem of
allocation of frequency We study a new class of graphs, the fat-extended P4
graphs, and we give a polynomial time algorithm to calculate the Grundy number
of the graphs in this class. This result implies that the Grundy number can be
found in polynomial time for many graphs
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7828</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7828</id><created>2014-01-30</created><authors><author><keyname>Flaut</keyname><forenames>Cristina</forenames></author></authors><title>Codes over a subset of Octonion Integers</title><categories>cs.IT math.IT</categories><msc-class>94B15, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we define codes over some Octonion integers. We prove that in
some conditions these codes can correct up to two errors for a transmitted
vector and the code rate of the codes is grater than the code rate of the codes
defined on some subset of Quaternion integers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7838</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7838</id><created>2014-01-30</created><updated>2015-03-02</updated><authors><author><keyname>von Sivers</keyname><forenames>Isabella</forenames></author><author><keyname>K&#xf6;ster</keyname><forenames>Gerta</forenames></author></authors><title>Dynamic Stride Length Adaptation According to Utility And Personal Space</title><categories>physics.soc-ph cs.MA math.OC</categories><comments>Authors version, 30 Pages, accepted for publication in Transportation
  Research Part B: Methodological. Changes resulting from the publishing
  process, such as peer review, editing, corrections, structural formatting,
  and other quality control mechanisms may not be reflected in this document</comments><journal-ref>Transportation Research Part B: Methodological, 2015, Vol. 74,
  pages 104 - 117</journal-ref><doi>10.1016/j.trb.2015.01.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pedestrians adjust both speed and stride length when they navigate difficult
situations such as tight corners or dense crowds. They try to avoid collisions
and to preserve their personal space. State-of-the-art pedestrian motion models
automatically reduce speed in dense crowds simply because there is no space
where the pedestrians could go. The stride length and its correct adaptation,
however, are rarely considered. This leads to artefacts that impact macroscopic
observation parameters such as densities in front of bottlenecks and, through
this, flow. Hence modelling stride adaptation is important to increase the
predictive power of pedestrian models. To achieve this we reformulate the
problem as an optimisation problem on a disk around the pedestrian. Each
pedestrian seeks the position that is most attractive in a sense of balanced
goals between the search for targets, the need for individual space and the
need to keep a distance from obstacles. The need for space is modelled
according to findings from psychology defining zones around a person that, when
invaded, cause unease. The result is a fully automatic adjustment that allows
calibration through meaningful social parameters and that gives visually
natural results with an excellent fit to measured experimental data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7842</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7842</id><created>2014-01-30</created><authors><author><keyname>Bonelle</keyname><forenames>Jerome</forenames></author><author><keyname>Ern</keyname><forenames>Alexandre</forenames></author></authors><title>Analysis of Compatible Discrete Operator Schemes for the Stokes
  Equations on Polyhedral Meshes</title><categories>math.NA cs.CE cs.NA</categories><msc-class>65N15, 65N12, 65N08, 65N30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compatible Discrete Operator schemes preserve basic properties of the
continuous model at the discrete level. They combine discrete differential
operators that discretize exactly topological laws and discrete Hodge operators
that approximate constitutive relations. We devise and analyze two families of
such schemes for the Stokes equations in curl formulation, with the pressure
degrees of freedom located at either mesh vertices or cells. The schemes ensure
local mass and momentum conservation. We prove discrete stability by
establishing novel discrete Poincar\'e inequalities. Using commutators related
to the consistency error, we derive error estimates with first-order
convergence rates for smooth solutions. We analyze two strategies for
discretizing the external load, so as to deliver tight error estimates when the
external load has a large irrotational or divergence-free part. Finally,
numerical results are presented on three-dimensional polyhedral meshes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7846</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7846</id><created>2014-01-30</created><authors><author><keyname>Ropokis</keyname><forenames>George A.</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>Berberidis</keyname><forenames>Kostas</forenames></author></authors><title>Optimal power control in Cognitive MIMO systems with limited feedback</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of optimal power allocation in Cognitive Radio
(CR) Multiple Input Multiple Output (MIMO) systems is treated. The focus is on
providing limited feedback solutions aiming at maximizing the secondary system
rate subject to a constraint on the average interference caused to primary
communication. The limited feedback solutions are obtained by reducing the
information available at secondary transmitter (STx) for the link between STx
and the secondary receiver (SRx) as well as by limiting the level of available
information at STx that corresponds to the link between the STx and the primary
receiver PRx. Monte Carlo simulation results are given that allow to quanitfy
the performance achieved by the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7860</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7860</id><created>2014-01-30</created><authors><author><keyname>Panina</keyname><forenames>Gaiane</forenames></author><author><keyname>Siersma</keyname><forenames>Dirk</forenames></author></authors><title>Motion planning and control of a planar polygonal linkage</title><categories>math.MG cs.RO</categories><msc-class>57Q99 52C99 57Q55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a polygonal linkage, we produce a fast navigation algorithm on its
configuration space. The basic idea is to approximate $M(L)$ by the vertex-edge
graph of the cell decomposition of the configuration space discovered by the
first author. The algorithm has three aspects: (1) the number of navigation
steps does not exceed 14 (independent on the number of edges), (2) each step is
a disguised flex of a quadrilateral from one triangular configuration to
another, which can be ranged as well understood type of flexes, and (3) each
step can be performed in a mechanical way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7886</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7886</id><created>2014-01-30</created><updated>2014-06-13</updated><authors><author><keyname>Naves</keyname><forenames>Guyslain</forenames></author><author><keyname>Spiwack</keyname><forenames>Arnaud</forenames></author></authors><title>Balancing lists: a proof pearl</title><categories>cs.DS cs.LO</categories><comments>To appear in proceedings of Interactive Theorem Proving (2014)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Starting with an algorithm to turn lists into full trees which uses
non-obvious invariants and partial functions, we progressively encode the
invariants in the types of the data, removing most of the burden of a
correctness proof.
  The invariants are encoded using non-uniform inductive types which parallel
numerical representations in a style advertised by Okasaki, and a small amount
of dependent types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7890</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7890</id><created>2014-01-30</created><authors><author><keyname>Qin</keyname><forenames>Xiangju</forenames></author><author><keyname>Salter-Townshend</keyname><forenames>Michael</forenames></author><author><keyname>Cunningham</keyname><forenames>P&#xe1;draig</forenames></author></authors><title>Exploring the Relationship between Membership Turnover and Productivity
  in Online Communities</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the more disruptive reforms associated with the modern Internet is the
emergence of online communities working together on knowledge artefacts such as
Wikipedia and OpenStreetMap. Recently it has become clear that these
initiatives are vulnerable because of problems with membership turnover. This
study presents a longitudinal analysis of 891 WikiProjects where we model the
impact of member turnover and social capital losses on project productivity. By
examining social capital losses we attempt to provide a more nuanced analysis
of member turnover. In this context social capital is modelled from a social
network perspective where the loss of more central members has more impact. We
find that only a small proportion of WikiProjects are in a relatively healthy
state with low levels of membership turnover and social capital losses. The
results show that the relationship between social capital losses and project
performance is U-shaped, and that member withdrawal has significant negative
effect on project outcomes. The results also support the mediation of turnover
rate and network density on the curvilinear relationship.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7898</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7898</id><created>2014-01-30</created><authors><author><keyname>Kontorovich</keyname><forenames>Aryeh</forenames></author><author><keyname>Weiss</keyname><forenames>Roi</forenames></author></authors><title>Maximum Margin Multiclass Nearest Neighbors</title><categories>cs.LG math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a general framework for margin-based multicategory classification
in metric spaces. The basic work-horse is a margin-regularized version of the
nearest-neighbor classifier. We prove generalization bounds that match the
state of the art in sample size $n$ and significantly improve the dependence on
the number of classes $k$. Our point of departure is a nearly Bayes-optimal
finite-sample risk bound independent of $k$. Although $k$-free, this bound is
unregularized and non-adaptive, which motivates our main result: Rademacher and
scale-sensitive margin bounds with a logarithmic dependence on $k$. As the best
previous risk estimates in this setting were of order $\sqrt k$, our bound is
exponentially sharper. From the algorithmic standpoint, in doubling metric
spaces our classifier may be trained on $n$ examples in $O(n^2\log n)$ time and
evaluated on new points in $O(\log n)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7909</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7909</id><created>2014-01-30</created><authors><author><keyname>Morstatter</keyname><forenames>Fred</forenames></author><author><keyname>Pfeffer</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Liu</keyname><forenames>Huan</forenames></author></authors><title>When is it Biased? Assessing the Representativeness of Twitter's
  Streaming API</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter has captured the interest of the scientific community not only for
its massive user base and content, but also for its openness in sharing its
data. Twitter shares a free 1% sample of its tweets through the &quot;Streaming
API&quot;, a service that returns a sample of tweets according to a set of
parameters set by the researcher. Recently, research has pointed to evidence of
bias in the data returned through the Streaming API, raising concern in the
integrity of this data service for use in research scenarios. While these
results are important, the methodologies proposed in previous work rely on the
restrictive and expensive Firehose to find the bias in the Streaming API data.
In this work we tackle the problem of finding sample bias without the need for
&quot;gold standard&quot; Firehose data. Namely, we focus on finding time periods in the
Streaming API data where the trend of a hashtag is significantly different from
its trend in the true activity on Twitter. We propose a solution that focuses
on using an open data source to find bias in the Streaming API. Finally, we
assess the utility of the data source in sparse data situations and for users
issuing the same query from different regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7923</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7923</id><created>2014-01-30</created><updated>2014-07-07</updated><authors><author><keyname>Lelarge</keyname><forenames>Marc</forenames></author></authors><title>Loopy annealing belief propagation for vertex cover and matching:
  convergence, LP relaxation, correctness and Bethe approximation</title><categories>cs.DM cs.DS cs.IT math-ph math.IT math.MP math.PR</categories><comments>revised version, 23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the minimum cardinality vertex cover and maximum cardinality matching
problems, the max-product form of belief propagation (BP) is known to perform
poorly on general graphs. In this paper, we present an iterative loopy
annealing BP (LABP) algorithm which is shown to converge and to solve a Linear
Programming relaxation of the vertex cover or matching problem on general
graphs. LABP finds (asymptotically) a minimum half-integral vertex cover (hence
provides a 2-approximation) and a maximum fractional matching on any graph. We
also show that LABP finds (asymptotically) a minimum size vertex cover for any
bipartite graph and as a consequence compute the matching number of the graph.
Our proof relies on some subtle monotonicity arguments for the local iteration.
We also show that the Bethe free entropy is concave and that LABP maximizes it.
Using loop calculus, we also give an exact (also intractable for general
graphs) expression of the partition function for matching in term of the LABP
messages which can be used to improve mean-field approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7941</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7941</id><created>2014-01-30</created><updated>2015-12-09</updated><authors><author><keyname>Albrecht</keyname><forenames>Stefano V.</forenames></author><author><keyname>Ramamoorthy</keyname><forenames>Subramanian</forenames></author></authors><title>Exploiting Causality for Selective Belief Filtering in Dynamic Bayesian
  Networks</title><categories>cs.AI</categories><comments>43 pages, submitted to Journal of Artificial Intelligence Research
  (revised version)</comments><acm-class>I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic Bayesian networks (DBNs) are a general model for stochastic processes
with partially observed states. Belief filtering in DBNs is the task of
inferring the belief state (i.e. the probability distribution over process
states) based on incomplete and noisy observations. This can be a hard problem
in complex processes with large state space. In this article, we explore the
idea of accelerating the filtering task by automatically exploiting causality
in the process. We consider a specific type of causal relation, called
passivity, which pertains to how state variables cause changes in other
variables. We present a novel filtering method, called Passivity-based
Monitoring (PM), which maintains a factored belief representation and exploits
passivity to perform selective updates over the belief factors. PM produces
exact belief states under certain assumptions and approximate belief states
otherwise, where the approximation error is bounded by the degree of
uncertainty in the process. We show empirically, in synthetic processes with
varying sizes and degrees of passivity, that PM is faster than several
alternative methods while achieving competitive accuracy. Furthermore, we
demonstrate how passivity occurs naturally in a complex system such as a
multi-robot warehouse, and how PM can exploit this to accelerate the filtering
task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7944</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7944</id><created>2014-01-30</created><authors><author><keyname>Psomas</keyname><forenames>Constantinos</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Fragkiskos</forenames></author></authors><title>Performance Rescaling of Complex Networks</title><categories>cs.NI cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>To appear in IEEE Communications Letters</comments><journal-ref>IEEE Communications Letters, Vol. 18, Issue 4, pp. 684-687, 2014</journal-ref><doi>10.1109/LCOMM.2014.021014.132684</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent progress in network topology modeling [1], [2] has shown that it is
possible to create smaller-scale replicas of large complex networks, like the
Internet, while simultaneously preserving several important topological
properties. However, the constructed replicas do not include notions of
capacities and latencies, and the fundamental question of whether smaller
networks can reproduce the performance of larger networks remains unanswered.
We address this question in this letter, and show that it is possible to
predict the performance of larger networks from smaller replicas, as long as
the right link capacities and propagation delays are assigned to the replica's
links. Our procedure is inspired by techniques introduced in [2] and combines a
time-downscaling argument from [3]. We show that significant computational
savings can be achieved when simulating smaller-scale replicas with TCP and UDP
traffic, with simulation times being reduced by up to two orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7962</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7962</id><created>2014-01-29</created><authors><author><keyname>Drugarin</keyname><forenames>Anghel</forenames></author><author><keyname>Victoria</keyname><forenames>Cornelia</forenames></author></authors><title>Numerical application and Turbo C program using the Gauss-Jordan Method</title><categories>cs.MS</categories><comments>in Romanian</comments><journal-ref>Algebra liniara. Programare liniara, vol.1, Eftimie Murgu,Press
  Resita (2003). AGIR Press, vol.22, 2012, pp.171-176</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article presents the general notions and algorithm about the Gauss-Jordan
method. An eloquent example is given and the Turbo C program illustrated this
method. We conclude that we can obtain by this method the determinant, by
simple calculations and reducing the rounding errors
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.7970</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.7970</id><created>2014-01-30</created><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Mahini</keyname><forenames>Hamid</forenames></author><author><keyname>Malec</keyname><forenames>David L.</forenames></author><author><keyname>Raghavan</keyname><forenames>S.</forenames></author><author><keyname>Sawant</keyname><forenames>Anshul</forenames></author><author><keyname>Zadimoghadam</keyname><forenames>Morteza</forenames></author></authors><title>How to Influence People with Partial Incentives</title><categories>cs.GT cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the power of fractional allocations of resources to maximize
influence in a network. This work extends in a natural way the well-studied
model by Kempe, Kleinberg, and Tardos (2003), where a designer selects a
(small) seed set of nodes in a social network to influence directly, this
influence cascades when other nodes reach certain thresholds of neighbor
influence, and the goal is to maximize the final number of influenced nodes.
Despite extensive study from both practical and theoretical viewpoints, this
model limits the designer to a binary choice for each node, with no way to
apply intermediate levels of influence. This model captures some settings
precisely, e.g. exposure to an idea or pathogen, but it fails to capture very
relevant concerns in others, for example, a manufacturer promoting a new
product by distributing five &quot;20% off&quot; coupons instead of giving away one free
product.
  While fractional versions of problems tend to be easier to solve than
integral versions, for influence maximization, we show that the two versions
have essentially the same computational complexity. On the other hand, the two
versions can have vastly different solutions: the added flexibility of
fractional allocation can lead to significantly improved influence. Our main
theoretical contribution is to show how to adapt the major positive results
from the integral case to the fractional case. Specifically, Mossel and Roch
(2006) used the submodularity of influence to obtain their integral results; we
introduce a new notion of continuous submodularity, and use this to obtain
matching fractional results. We conclude that we can achieve the same greedy
$(1-1/e-\epsilon)$-approximation for the fractional case as the integral case.
In practice, we find that the fractional model performs substantially better
than the integral model, according to simulations on real-world social network
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8008</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8008</id><created>2014-01-30</created><authors><author><keyname>Hocking</keyname><forenames>Toby Dylan</forenames></author><author><keyname>Spanurattana</keyname><forenames>Supaporn</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Support vector comparison machines</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In ranking problems, the goal is to learn a ranking function from labeled
pairs of input points. In this paper, we consider the related comparison
problem, where the label indicates which element of the pair is better, or if
there is no significant difference. We cast the learning problem as a margin
maximization, and show that it can be solved by converting it to a standard
SVM. We use simulated nonlinear patterns and a real learning to rank sushi data
set to show that our proposed SVMcompare algorithm outperforms SVMrank when
there are equality pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8022</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8022</id><created>2014-01-30</created><updated>2014-02-11</updated><authors><author><keyname>Su</keyname><forenames>Lili</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>Synchronizing Rankings via Interactive Communication</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of exact synchronization of two rankings at remote
locations connected by a two-way channel. Such synchronization problems arise
when items in the data are distinguishable, as is the case for playlists,
tasklists, crowdvotes and recommender systems rankings. Our model accounts for
different constraints on the communication throughput of the forward and
feedback links, resulting in different anchoring, syndrome and checksum
computation strategies. Information editing is assumed of the form of
deletions, insertions, block deletions/insertions, translocations and
transpositions. The protocols developed under the given model are order-optimal
with respect to genie aided lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8023</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8023</id><created>2014-01-30</created><authors><author><keyname>Baetz</keyname><forenames>Bradley</forenames></author><author><keyname>Wood</keyname><forenames>David R.</forenames></author></authors><title>Brooks' Vertex-Colouring Theorem in Linear Time</title><categories>cs.DM</categories><report-no>Technical Report CS-AAG-2001-05, Basser Department of Computer
  Science, The University of Sydney, 2001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brooks' Theorem [R. L. Brooks, On Colouring the Nodes of a Network, Proc.
Cambridge Philos. Soc.} 37:194-197, 1941] states that every graph $G$ with
maximum degree $\Delta$, has a vertex-colouring with $\Delta$ colours, unless
$G$ is a complete graph or an odd cycle, in which case $\Delta+1$ colours are
required. Lov\'asz [L. Lov\'asz, Three short proofs in graph theory, J. Combin.
Theory Ser. 19:269-271, 1975] gives an algorithmic proof of Brooks' Theorem.
Unfortunately this proof is missing important details and it is thus unclear
whether it leads to a linear time algorithm. In this paper we give a complete
description of the proof of Lov\'asz, and we derive a linear time algorithm for
determining the vertex-colouring guaranteed by Brooks' Theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8030</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8030</id><created>2014-01-30</created><authors><author><keyname>Haque</keyname><forenames>Asif</forenames></author></authors><title>Transit Fare Arbitrage: Case Study of San Francisco Bay Area Rapid
  Transit (BART) System</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transit fare arbitrage is the scenario when two or more commuters agree to
swap tickets during travel in such a way that total cost is lower than
otherwise. Such arbitrage allows pricing inefficiencies to be explored and
exploited, leading to improved pricing models. In this paper we discuss the
basics of fare arbitrage through an intuitive pricing framework involving
population density. We then analyze the San Francisco Bay Area Rapid Transit
(BART) system to understand underlying inefficiencies. We also provide source
code and comprehensive list of pairs of trips with significant arbitrage gain
at github.com/asifhaque/transit-arbitrage. Finally, we point towards a uniform
payment interface for different kinds of transit systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8038</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8038</id><created>2014-01-30</created><authors><author><keyname>Chichin</keyname><forenames>Sergei</forenames></author><author><keyname>Vo</keyname><forenames>Quoc Bao</forenames></author><author><keyname>Kowalczyk</keyname><forenames>Ryszard</forenames></author></authors><title>Truthful Market-based Trading of Cloud Resources with Reservation Price</title><categories>cs.GT</categories><comments>10 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapidly growing demand for the cloud services, a need for efficient
methods to trade computing resources increases. Commonly used fixed-price model
is not always the best approach for trading cloud resources, because of its
inflexible and static nature. Dynamic trading systems, which make use of market
mechanisms, show promise for more efficient resource allocation and pricing in
the cloud. However, most of the existing mechanisms ignore the seller's costs
of providing the resources. In order to address it, we propose a single-sided
market mechanism for trading virtual machine instances in the cloud, where the
cloud provider can express the reservation prices for traded cloud services. We
investigate the theoretical properties of the proposed mechanism and prove that
it is truthful, i.e. the buyers do not have an incentive to lie about their
true valuation of the resources. We perform extensive experiments in order to
investigate the impact of the reserve price on the market outcome. Our
experiments show that the proposed mechanism yields near optimal allocations
and has a low execution time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8042</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8042</id><created>2014-01-30</created><authors><author><keyname>Tu</keyname><forenames>Kun</forenames></author><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames></author><author><keyname>Jiang</keyname><forenames>Hua</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author><author><keyname>Jensen</keyname><forenames>David</forenames></author><author><keyname>Liu</keyname><forenames>Benyuan</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Online Dating Recommendations: Matching Markets and Learning Preferences</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>6 pages, 4 figures, submission on 5th International Workshop on
  Social Recommender Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommendation systems for online dating have recently attracted much
attention from the research community. In this paper we proposed a two-side
matching framework for online dating recommendations and design an LDA model to
learn the user preferences from the observed user messaging behavior and user
profile features. Experimental results using data from a large online dating
website shows that two-sided matching improves significantly the rate of
successful matches by as much as 45%. Finally, using simulated matchings we
show that the the LDA model can correctly capture user preferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8044</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8044</id><created>2014-01-30</created><authors><author><keyname>Carlberg</keyname><forenames>Kevin</forenames></author><author><keyname>Tuminaro</keyname><forenames>Ray</forenames></author><author><keyname>Boggs</keyname><forenames>Paul</forenames></author></authors><title>Preserving Lagrangian structure in nonlinear model reduction with
  application to structural dynamics</title><categories>cs.CE math.NA</categories><comments>Submitted to SIAM Journal on Scientific Computing (SISC)</comments><journal-ref>SIAM Journal on Scientific Computing, Vol. 37, No. 2, p. B153-B184
  (2015)</journal-ref><doi>10.1137/140959602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a model-reduction methodology that preserves Lagrangian
structure (equivalently Hamiltonian structure) and achieves computational
efficiency in the presence of high-order nonlinearities and arbitrary parameter
dependence. As such, the resulting reduced-order model retains key properties
such as energy conservation and symplectic time-evolution maps. We focus on
parameterized simple mechanical systems subjected to Rayleigh damping and
external forces, and consider an application to nonlinear structural dynamics.
To preserve structure, the method first approximates the system's `Lagrangian
ingredients'---the Riemannian metric, the potential-energy function, the
dissipation function, and the external force---and subsequently derives
reduced-order equations of motion by applying the (forced) Euler--Lagrange
equation with these quantities. From the algebraic perspective, key
contributions include two efficient techniques for approximating parameterized
reduced matrices while preserving symmetry and positive definiteness: matrix
gappy POD and reduced-basis sparsification (RBS). Results for a parameterized
truss-structure problem demonstrate the importance of preserving Lagrangian
structure and illustrate the proposed method's merits: it reduces computation
time while maintaining high accuracy and stability, in contrast to existing
nonlinear model-reduction techniques that do not preserve structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8046</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8046</id><created>2014-01-30</created><updated>2014-02-28</updated><authors><author><keyname>Borges</keyname><forenames>Nerio</forenames><affiliation>Universidad Sim&#xf3;n Bol&#xed;var</affiliation></author><author><keyname>Bonet</keyname><forenames>Blai</forenames><affiliation>Universidad Sim&#xf3;n Bol&#xed;var</affiliation></author></authors><title>Universal First-Order Logic is Superfluous for NL, P, NP and coNP</title><categories>cs.LO cs.CC</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (March 3,
  2014) lmcs:1011</journal-ref><doi>10.2168/LMCS-10(1:15)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we continue the syntactic study of completeness that began with
the works of Immerman and Medina. In particular, we take a conjecture raised by
Medina in his dissertation that says if a conjunction of a second-order and a
first-order sentences defines an NP-complete problems via fops, then it must be
the case that the second-order conjoint alone also defines a NP-complete
problem. Although this claim looks very plausible and intuitive, currently we
cannot provide a definite answer for it. However, we can solve in the
affirmative a weaker claim that says that all ``consistent'' universal
first-order sentences can be safely eliminated without the fear of losing
completeness. Our methods are quite general and can be applied to complexity
classes other than NP (in this paper: to NLSPACE, PTIME, and coNP), provided
the class has a complete problem satisfying a certain combinatorial property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8053</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8053</id><created>2014-01-30</created><authors><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author></authors><title>Hallucinating optimal high-dimensional subspaces</title><categories>cs.CV</categories><comments>Pattern Recognition, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear subspace representations of appearance variation are pervasive in
computer vision. This paper addresses the problem of robustly matching such
subspaces (computing the similarity between them) when they are used to
describe the scope of variations within sets of images of different (possibly
greatly so) scales. A naive solution of projecting the low-scale subspace into
the high-scale image space is described first and subsequently shown to be
inadequate, especially at large scale discrepancies. A successful approach is
proposed instead. It consists of (i) an interpolated projection of the
low-scale subspace into the high-scale space, which is followed by (ii) a
rotation of this initial estimate within the bounds of the imposed
``downsampling constraint''. The optimal rotation is found in the closed-form
which best aligns the high-scale reconstruction of the low-scale subspace with
the reference it is compared to. The method is evaluated on the problem of
matching sets of (i) face appearances under varying illumination and (ii)
object appearances under varying viewpoint, using two large data sets. In
comparison to the naive matching, the proposed algorithm is shown to greatly
increase the separation of between-class and within-class similarities, as well
as produce far more meaningful modes of common appearance on which the match
score is based.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8064</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8064</id><created>2014-01-31</created><authors><author><keyname>Niu</keyname><forenames>Ben</forenames></author><author><keyname>Zhang</keyname><forenames>Tanran</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaoyan</forenames></author><author><keyname>Li</keyname><forenames>Hui</forenames></author><author><keyname>Lu</keyname><forenames>Zongqing</forenames></author></authors><title>Priority-Aware Private Matching Schemes for Proximity-Based Mobile
  Social Networks</title><categories>cs.CR</categories><comments>15 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid developments of mobile devices and online social networks have
resulted in increasing attention to Mobile Social Networking (MSN). The
explosive growth of mobile-connected and location-aware devices makes it
possible and meaningful to do the Proximity-based Mobile Social Networks
(PMSNs). Users can discover and make new social interactions easily with
physical-proximate mobile users through WiFi/Bluetooth interfaces embedded in
their smartphones. However, users enjoy these conveniences at the cost of their
growing privacy concerns. To address this problem, we propose a suit of
priority-aware private matching schemes to privately match the similarity with
potential friends in the vicinity. Unlike most existing work, our proposed
priority-aware matching scheme (P-match) achieves the privacy goal by combining
the commutative encryption function and the Tanimoto similarity coefficient
which considers both the number of common attributes between users as well as
the corresponding priorities on each common attribute. Further, based on the
newly constructed similarity function which takes the ratio of attributes
matched over all the input set into consideration, we design an enhanced
version to deal with some potential attacks such as unlimitedly inputting the
attribute set on either the initiator side or the responder side, etc. Finally,
our proposed E-match avoids the heavy cryptographic operations and improves the
system performance significantly by employing a novel use of the Bloom filter.
The security and communication/computation overhead of our schemes are
thoroughly analyzed and evaluated via detailed simulations and implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8069</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8069</id><created>2014-01-31</created><authors><author><keyname>N</keyname><forenames>Rashmi</forenames></author><author><keyname>V</keyname><forenames>Suma</forenames></author></authors><title>Defect Detection Efficiency A Combined Approach</title><categories>cs.SE</categories><comments>6 Pages, 2 Figures</comments><journal-ref>International Journal of Advanced Computer Research (ISSN (print):
  2249 - 7277 ISSN (online): 2277 - 7970) Volume - 3 Number - 3 Issue - 11
  September - 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Survival of IT industries depends much upon the development of high quality
and customer satisfied software products. Quality however can be viewed from
various perspectives such as deployment of the products within estimated
resources, constrains and also being defect free. Testing is one of the
promising techniques ever since the inception of software in the global market.
Though there are several testing techniques existing, the most widely accepted
is the conventional scripted testing. Despite of advancement in the technology,
achieving defect free deliverables is yet a challenge. This paper therefore
aims to enhance the existing testing techniques in order to achieve nearly zero
defect products through the combined approach of scripted and exploratory
testing. This approach thus enables the testing team to capture maximum defects
and thereby reduce the expensive nature of overheads. Further, it leads towards
generation of high quality products and assures the continued customer
satisfaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8071</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8071</id><created>2014-01-31</created><authors><author><keyname>ling</keyname><forenames>Miriam Kie&#xdf;</forenames></author><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author><author><keyname>Rambau</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>An exact column-generation approach for the lot-type design problem --
  extended abstract</title><categories>math.OC cs.DS</categories><comments>4 pages, 2 tables, presented at ISCO 2012</comments><msc-class>90C06, 90C11, 90B90</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a fashion discounter that supplies any of its many branches with
an integral multiple of lots whose size assortment structure stems from a set
of many applicable lot-types. We design a column generation algorithm for the
optimal approximation of the branch and size dependent demand by a supply using
a bounded number of lot-types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8072</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8072</id><created>2014-01-31</created><authors><author><keyname>Armbrust</keyname><forenames>Ove</forenames></author><author><keyname>Katahira</keyname><forenames>Masafumi</forenames></author><author><keyname>Miyamoto</keyname><forenames>Yuko</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Nakao</keyname><forenames>Haruka</forenames></author><author><keyname>Ocampo</keyname><forenames>Alexis</forenames></author></authors><title>Scoping Software Process Models - Initial Concepts and Experience from
  Defining Space Standards</title><categories>cs.SE</categories><comments>13 pages. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F978-3-540-79588-9_15</comments><journal-ref>Making Globally Distributed Software Development a Success Story,
  volume 5007 of Lecture Notes in Computer Science, pages 160-172. Springer
  Berlin Heidelberg, 2008</journal-ref><doi>10.1007/978-3-540-79588-9_15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Defining process standards by integrating, harmonizing, and standardizing
heterogeneous and often implicit processes is an important task, especially for
large development organizations. However, many challenges exist, such as
limiting the scope of process standards, coping with different levels of
process model abstraction, and identifying relevant process variabilities to be
included in the standard. On the one hand, eliminating process variability by
building more abstract models with higher degrees of interpretation has many
disadvantages, such as less control over the process. Integrating all kinds of
variability, on the other hand, leads to high process deployment costs. This
article describes requirements and concepts for determining the scope of
process standards based on a characterization of the potential products to be
produced in the future, the projects expected for the future, and the
respective process capabilities needed. In addition, the article sketches
experience from determining the scope of space process standards for satellite
software development. Finally, related work with respect to process model
scoping, conclusions, and an outlook on future work are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8074</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8074</id><created>2014-01-31</created><authors><author><keyname>Zawadzki</keyname><forenames>Erik</forenames></author><author><keyname>Lipson</keyname><forenames>Asher</forenames></author><author><keyname>Leyton-Brown</keyname><forenames>Kevin</forenames></author></authors><title>Empirically Evaluating Multiagent Learning Algorithms</title><categories>cs.GT cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There exist many algorithms for learning how to play repeated bimatrix games.
Most of these algorithms are justified in terms of some sort of theoretical
guarantee. On the other hand, little is known about the empirical performance
of these algorithms. Most such claims in the literature are based on small
experiments, which has hampered understanding as well as the development of new
multiagent learning (MAL) algorithms. We have developed a new suite of tools
for running multiagent experiments: the MultiAgent Learning Testbed (MALT).
These tools are designed to facilitate larger and more comprehensive
experiments by removing the need to build one-off experimental code. MALT also
provides baseline implementations of many MAL algorithms, hopefully eliminating
or reducing differences between algorithm implementations and increasing the
reproducibility of results. Using this test suite, we ran an experiment
unprecedented in size. We analyzed the results according to a variety of
performance metrics including reward, maxmin distance, regret, and several
notions of equilibrium convergence. We confirmed several pieces of conventional
wisdom, but also discovered some surprising results. For example, we found that
single-agent $Q$-learning outperformed many more complicated and more modern
MAL algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8079</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8079</id><created>2014-01-31</created><authors><author><keyname>Asratian</keyname><forenames>A. S.</forenames></author><author><keyname>Kamalian</keyname><forenames>R. R.</forenames></author></authors><title>Interval colorings of edges of a multigraph</title><categories>cs.DM math.CO</categories><journal-ref>Applied Mathematics 5, Yerevan State University, 1987, pp. 25--34</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G=(V_1(G),V_2(G),E(G))$ be a bipartite multigraph, and $R\subseteq
V_1(G)\cup V_2(G)$. A proper coloring of edges of $G$ with the colors
$1,\ldots,t$ is called interval (respectively, continuous) on $R$, if each
color is used for at least one edge and the edges incident with each vertex
$x\in R$ are colored by $d(x)$ consecutive colors (respectively, by the colors
$1,\ldots,d(x))$, where $d(x)$ is a degree of the vertex $x$. We denote by
$w_1(G)$ and $W_1(G)$, respectively, the least and the greatest values of $t$,
for which there exists an interval on $V_1(G)$ coloring of the multigraph $G$
with the colors $1,\ldots,t$.
  In the paper the following basic results are obtained.
  \textbf{Theorem 2.} For an arbitrary $k$, $w_1(G)\leq k\leq W_1(G)$, there is
an interval on $V_1(G)$ coloring of the multigraph $G$ with the colors
$1,\ldots,k$.
  \textbf{Theorem 3.} The problem of recognition of the existence of a
continuous on $V_1(G)$ coloring of the multigraph $G$ is $NP$-complete.
  \textbf{Theorem 4.} If for any edge $(x,y)\in E(G)$, where $x\in V_1(G)$, the
inequality $d(x)\geq d(y)$ holds then there is a continuous on $V_1(G)$
coloring of the multigraph $G$.
  \textbf{Theorem 1.} If $G$ has no multiple edges and triangles, and there is
an interval on $V(G)$ coloring of the graph $G$ with the colors $1,\ldots,k$,
then $k\leq|V(G)|-1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8090</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8090</id><created>2014-01-31</created><authors><author><keyname>Olmos</keyname><forenames>Markus Stinner Pablo M.</forenames></author></authors><title>Analyzing Finite-length Protograph-based Spatially Coupled LDPC Codes</title><categories>cs.IT math.IT</categories><comments>5 pages, 6 figures, submitted to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The peeling decoding for spatially coupled low-density parity-check (SC-LDPC)
codes is analyzed for a binary erasure channel. An analytical calculation of
the mean evolution of degree-one check nodes of protograph-based SC-LDPC codes
is given and an estimate for the covariance evolution of degree-one check nodes
is proposed in the stable decoding phase where the decoding wave propagates
along the chain of coupled codes. Both results are verified numerically.
Protograph-based SC-LDPC codes turn out to have a more robust behavior than
unstructured random SC-LDPC codes. Using the analytically calculated
parameters, the finite- length scaling laws for these constructions are given
and verified by numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8092</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8092</id><created>2014-01-31</created><updated>2014-06-30</updated><authors><author><keyname>Hansard</keyname><forenames>Miles</forenames></author><author><keyname>Evangelidis</keyname><forenames>Georgios</forenames></author><author><keyname>Pelorson</keyname><forenames>Quentin</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author></authors><title>Cross-calibration of Time-of-flight and Colour Cameras</title><categories>cs.CV cs.RO</categories><comments>18 pages, 12 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-of-flight cameras provide depth information, which is complementary to
the photometric appearance of the scene in ordinary images. It is desirable to
merge the depth and colour information, in order to obtain a coherent scene
representation. However, the individual cameras will have different viewpoints,
resolutions and fields of view, which means that they must be mutually
calibrated. This paper presents a geometric framework for this multi-view and
multi-modal calibration problem. It is shown that three-dimensional projective
transformations can be used to align depth and parallax-based representations
of the scene, with or without Euclidean reconstruction. A new evaluation
procedure is also developed; this allows the reprojection error to be
decomposed into calibration and sensor-dependent components. The complete
approach is demonstrated on a network of three time-of-flight and six colour
cameras. The applications of such a system, to a range of automatic
scene-interpretation problems, are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8096</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8096</id><created>2014-01-31</created><updated>2014-05-18</updated><authors><author><keyname>De Bacco</keyname><forenames>Caterina</forenames></author><author><keyname>Franz</keyname><forenames>Silvio</forenames></author><author><keyname>Saad</keyname><forenames>David</forenames></author><author><keyname>Yeung</keyname><forenames>Chi Ho</forenames></author></authors><title>Shortest node-disjoint paths on random graphs</title><categories>cond-mat.dis-nn cs.DS math.OC</categories><comments>22 pages, 14 figures</comments><journal-ref>J. Stat. Mech. (2014) P07009</journal-ref><doi>10.1088/1742-5468/2014/07/P07009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A localized method to distribute paths on random graphs is devised, aimed at
finding the shortest paths between given source/destination pairs while
avoiding path overlaps at nodes. We propose a method based on message-passing
techniques to process global information and distribute paths optimally.
Statistical properties such as scaling with system size and number of paths,
average path-length and the transition to the frustrated regime are analysed.
The performance of the suggested algorithm is evaluated through a comparison
against a greedy algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8120</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8120</id><created>2014-01-31</created><authors><author><keyname>Das</keyname><forenames>Suprita</forenames></author><author><keyname>Duvvuru</keyname><forenames>Rajesh</forenames></author><author><keyname>Raj</keyname><forenames>Sonal</forenames></author></authors><title>Novel Integrated Approach of Software Development and Design Pattern
  with X-CM</title><categories>cs.SE</categories><comments>8 Pages. IJCEA,2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expert evaluation in software life cycle is playing a vital role. In this
paper, we have discussed about the formal framework for selecting the best
model with some enhanced features. In our work we have introduced a novel
algorithm X Chain Model X-CM. Here XCM overcomes the limitation of Vmodel. In
XCM, we have concentrated on hierarchy of multiple independent sub-projects in
project development phase. XCM merges through the subprojects binding them at
their Integration Testing Phases. Hence combining binary modules at each stage
into a new module through chains of X till the final level in the hierarchy is
implemented. X-CM results in reducing the time and effort for software
developer. In addition to this, X-CM also rules out the dependency errors
between the modules at each stage of combination. Here we have tested our
proposed X-CM with Unit Testing and Integrating test. Finally, we proved that
X-CM Model works better than Spiral, Prototype and V-Model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8126</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8126</id><created>2014-01-31</created><updated>2015-05-19</updated><authors><author><keyname>Harandi</keyname><forenames>Mehrtash</forenames></author><author><keyname>Hartley</keyname><forenames>Richard</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Lovell</keyname><forenames>Brian</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author></authors><title>Extrinsic Methods for Coding and Dictionary Learning on Grassmann
  Manifolds</title><categories>cs.LG cs.CV stat.ML</categories><comments>Appearing in International Journal of Computer Vision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparsity-based representations have recently led to notable results in
various visual recognition tasks. In a separate line of research, Riemannian
manifolds have been shown useful for dealing with features and models that do
not lie in Euclidean spaces. With the aim of building a bridge between the two
realms, we address the problem of sparse coding and dictionary learning over
the space of linear subspaces, which form Riemannian structures known as
Grassmann manifolds. To this end, we propose to embed Grassmann manifolds into
the space of symmetric matrices by an isometric mapping. This in turn enables
us to extend two sparse coding schemes to Grassmann manifolds. Furthermore, we
propose closed-form solutions for learning a Grassmann dictionary, atom by
atom. Lastly, to handle non-linearity in data, we extend the proposed Grassmann
sparse coding and dictionary learning algorithms through embedding into Hilbert
spaces.
  Experiments on several classification tasks (gender recognition, gesture
classification, scene analysis, face recognition, action recognition and
dynamic texture classification) show that the proposed approaches achieve
considerable improvements in discrimination accuracy, in comparison to
state-of-the-art methods such as kernelized Affine Hull Method and
graph-embedding Grassmann discriminant analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8131</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8131</id><created>2014-01-31</created><authors><author><keyname>Gupta</keyname><forenames>Bhagvan Krishna</forenames></author><author><keyname>Mundra</keyname><forenames>Ankit</forenames></author><author><keyname>Rakesh</keyname><forenames>Nitin</forenames></author></authors><title>Failure Detection and Recovery in Hierarchical Network Using FTN
  Approach</title><categories>cs.DC cs.NI</categories><comments>8 pages, 7 figure. International Journal of Computer Science Issues,
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In current scenario several commercial and social organizations are using
computer networks for their business and management purposes. In order to meet
the business requirements networks are also grow. The growth of network also
promotes the handling capability of large networks because it counter raises
the possibilities of various faults in the network. A fault in network degrades
its performance by affecting parameters like throughput, delay, latency,
reliability etc. In hierarchical network models any possibility of fault may
collapse entire network. If a fault occurrence disables a device in
hierarchical network then it may distresses all the devices underneath. Thus it
affects entire networks performance. In this paper we propose Fault Tolerable
hierarchical Network (FTN) approach as a solution to the problems of
hierarchical networks. The proposed approach firstly detects possibilities of
fault in the network and accordingly provides specific recovery mechanism. We
have evaluated the performance of FTN approach in terms of delay and throughput
of network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8132</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8132</id><created>2014-01-31</created><authors><author><keyname>Bandini</keyname><forenames>Stefania</forenames></author><author><keyname>Crociani</keyname><forenames>Luca</forenames></author><author><keyname>Vizzari</keyname><forenames>Giuseppe</forenames></author></authors><title>Heterogeneous Speed Profiles in Discrete Models for Pedestrian
  Simulation</title><categories>cs.MA</categories><comments>Poster at the 93rd Transportation Research Board annual meeting,
  Washington, January 2014 - Committee number AHB45 - TRB Committee on Traffic
  Flow Theory and Characteristics</comments><msc-class>68U20</msc-class><acm-class>I.6.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrete pedestrian simulation models are viable alternatives to particle
based approaches based on a continuous spatial representation. The effects of
discretisation, however, also imply some difficulties in modelling certain
phenomena that can be observed in reality. This paper focuses on the
possibility to manage heterogeneity in the walking speed of the simulated
population of pedestrians by modifying an existing multi-agent model extending
the floor field approach. Whereas some discrete models allow pedestrians (or
cars, when applied to traffic modelling) to move more than a single cell per
time step, the present work proposes a maximum speed of one cell per step, but
we model lower speeds by having pedestrians yielding their movement in some
turns. Different classes of pedestrians are associated to different desired
walking speeds and we define a stochastic mechanism ensuring that they maintain
an average speed close to this threshold. In the paper we formally describe the
model and we show the results of its application in benchmark scenarios.
Finally, we also show how this approach can also support the definition of
slopes and stairs as elements reducing the walking speed of pedestrians
climbing them in a simulated scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8135</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8135</id><created>2014-01-31</created><authors><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author></authors><title>Competitive learning of monotone Boolean functions</title><categories>cs.DS cs.CC math.OC</categories><comments>5 pages, 2 figures, presented at APMOD 2012</comments><msc-class>68W27, 06E30, 68Q32</msc-class><journal-ref>Suhl, Leena; Mitra, Gautam et al. (ed.): Applied Mathematical
  Optimization and Modelling, APMOD 2012, DS&amp;OR Lab, Vol. 8, Pages 416-421</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply competitive analysis onto the problem of minimizing the number of
queries to an oracle to completely reconstruct a given monotone Boolean
function. Besides lower and upper bounds on the competitivity we determine
optimal deterministic online algorithms for the smallest problem instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8136</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8136</id><created>2014-01-31</created><authors><author><keyname>Arakistain</keyname><forenames>Ivan</forenames></author><author><keyname>Abascal</keyname><forenames>Jose Miguel</forenames></author><author><keyname>Munne</keyname><forenames>Oriol</forenames></author></authors><title>ICT technologies for the refurbishment of wooden structure buildings</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, one would think that after years of massive concrete and steel
construction in Spain, there are not many wood structure buildings left to be
refurbished except for some palaces or cathedrals. However, if we go for a walk
and have a look at the old part of any city, we will realize that still most of
the buildings have a wood structure. In spite of the fact that the majority of
urban regulations forbid their demolition, other bad practices such as casting
and overloading the wood structure are very common. Considering that we want to
reach a standard of sustainable construction, the economical and environmental
costs, which are implied by the deficient refurbishment makes it well worth a
previous study of the structure, which in most cases represents less than a 1%
of the total budget. The main goal of this paper is to present most relevant
parts of the whole process of diagnosis of a wood structure building by means
of Non-Destructive Testing Techniques. Among the ones to be considered, we
could mention the analysis of the building and its surroundings, on-site
inspection of the building, structural diagnosis, definition of the corrective
actions to be taken, definition of treatments, quality control and a
maintenance plan. For the on-site inspection of the building, in the paper we
will highlight the use of Non-Destructive methods such as resistograph
drilling, X-ray imaging, ultrasound-based testing or moisture measurement. We
will provide practical examples of all this. The aim of this paper is to give
the audience an overall idea on how a pre-assessment work can enhance the
refurbishment of a wood structure building while reducing costs and
environmental impact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8144</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8144</id><created>2014-01-31</created><authors><author><keyname>Rosales</keyname><forenames>David</forenames></author></authors><title>Cooperative Product Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I introduce cooperative product games (CPGs), a cooperative game where every
player has a weight, and the value of a coalition is the product of the weights
of the players in the coalition. I only look at games where the weights are at
least $2$.
  I show that no player in such a game can be a dummy. I show that the game is
convex, and therefore always has a non-empty core. I provide a simple method
for finding a payoff vector in the core.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8151</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8151</id><created>2014-01-31</created><authors><author><keyname>Darmann</keyname><forenames>Andreas</forenames></author><author><keyname>Elkind</keyname><forenames>Edith</forenames></author><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author><author><keyname>Lang</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Schauer</keyname><forenames>Joachim</forenames></author><author><keyname>Woeginger</keyname><forenames>Gerhard</forenames></author></authors><title>Group Activity Selection Problem</title><categories>cs.GT</categories><comments>13 pages, presented at WINE-2012 and COMSOC-2012</comments><msc-class>68Q25, 91A40</msc-class><journal-ref>Lecture Notes in Computer Science Vol. 7695 (2012), Pages 157-170</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a setting where one has to organize one or several group
activities for a set of agents. Each agent will participate in at most one
activity, and her preferences over activities depend on the number of
participants in the activity. The goal is to assign agents to activities based
on their preferences. We put forward a general model for this setting, which is
a natural generalization of anonymous hedonic games. We then focus on a special
case of our model, where agents' preferences are binary, i.e., each agent
classifies all pairs of the form &quot;(activity, group size)&quot; into ones that are
acceptable and ones that are not. We formulate several solution concepts for
this scenario, and study them from the computational point of view, providing
hardness results for the general case as well as efficient algorithms for
settings where agents' preferences satisfy certain natural constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8152</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8152</id><created>2014-01-31</created><updated>2014-02-13</updated><authors><author><keyname>Saha</keyname><forenames>Dibakar</forenames></author><author><keyname>Das</keyname><forenames>Nabanita</forenames></author></authors><title>Distributed Area Coverage by Connected Set Cover Partitioning in
  Wireless Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assuming a random uniform distribution of n sensor nodes over a virtual grid,
this paper addresses the problem of finding the maximum number of connected set
covers each ensuring 100% coverage of the query region. The connected sets
remain active one after another in a round robin fashion such that if there are
P such set covers, it can enhance the network lifetime P-fold. From
graph-theoretic point of view, a centralized O(n3) heuristic is proposed here
to maximize P. Next, for large self-organized sensor networks, a distributed
algorithm is developed. The proposed algorithm is to be executed just once,
during the initialization of the network. In case of failure, a distributed
recovery algorithm is executed to rearrange the partitions. Simulation studies
show that the performance of the proposed distributed algorithm is comparable
with that of the centralized algorithm in terms of number of partitions. Also,
comparison with earlier works shows significant improvement in terms of number
of partitions, message complexity and network lifetime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8168</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8168</id><created>2014-01-31</created><authors><author><keyname>Tomasoni</keyname><forenames>Alessandro</forenames></author><author><keyname>Bellini</keyname><forenames>Sandro</forenames></author><author><keyname>Ferrari</keyname><forenames>Marco</forenames></author></authors><title>Thresholds of absorbing sets in Low-Density-Parity-Check codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate absorbing sets, responsible of error floors in
Low Density Parity Check codes. We look for a concise, quantitative way to rate
the absorbing sets' dangerousness. Based on a simplified model for iterative
decoding evolution, we show that absorbing sets exhibit a threshold behavior.
An absorbing set with at least one channel log-likelihood-ratio below the
threshold can stop the convergence towards the right codeword. Otherwise
convergence is guaranteed. We show that absorbing sets with negative thresholds
can be deactivated simply using proper saturation levels. We propose an
efficient algorithm to compute thresholds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8170</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8170</id><created>2014-01-31</created><authors><author><keyname>Rauzy</keyname><forenames>Pablo</forenames><affiliation>LTCI</affiliation></author><author><keyname>Guilley</keyname><forenames>Sylvain</forenames></author></authors><title>A Formal Proof of Countermeasures against Fault Injection Attacks on
  CRT-RSA</title><categories>cs.CR</categories><comments>PROOFS, Santa Barbara, CA : United States (2013)</comments><proxy>ccsd</proxy><doi>10.1007/s13389-013-0065-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we describe a methodology that aims at either breaking or
proving the security of CRT-RSA implementations against fault injection
attacks. In the specific case-study of the BellCoRe attack, our work bridges a
gap between formal proofs and implementation-level attacks. We apply our
results to three implementations of CRT-RSA, namely the unprotected one, that
of Shamir, and that of Aum\&quot;uller et al. Our findings are that many attacks are
possible on both the unprotected and the Shamir implementations, while the
implementation of Aum\&quot;uller et al. is resistant to all single-fault attacks.
It is also resistant to double-fault attacks if we consider the less powerful
threat-model of its authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8172</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8172</id><created>2014-01-31</created><updated>2014-05-23</updated><authors><author><keyname>Rauzy</keyname><forenames>Pablo</forenames><affiliation>LTCI</affiliation></author><author><keyname>Guilley</keyname><forenames>Sylvain</forenames></author></authors><title>Formal Analysis of CRT-RSA Vigilant's Countermeasure Against the
  BellCoRe Attack: A Pledge for Formal Methods in the Field of Implementation
  Security</title><categories>cs.CR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1401.8170</comments><proxy>ccsd</proxy><journal-ref>Program Protection and Reverse Engineering Workshop 2014, San
  Diego, CA : United States (2014)</journal-ref><doi>10.1145/2556464.2556466</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our paper at PROOFS 2013, we formally studied a few known countermeasures
to protect CRT-RSA against the BellCoRe fault injection attack. However, we
left Vigilant's countermeasure and its alleged repaired version by Coron et al.
as future work, because the arithmetical framework of our tool was not
sufficiently powerful. In this paper we bridge this gap and then use the same
methodology to formally study both versions of the countermeasure. We obtain
surprising results, which we believe demonstrate the importance of formal
analysis in the field of implementation security. Indeed, the original version
of Vigilant's countermeasure is actually broken, but not as much as Coron et
al. thought it was. As a consequence, the repaired version they proposed can be
simplified. It can actually be simplified even further as two of the nine
modular verifications happen to be unnecessary. Fortunately, we could formally
prove the simplified repaired version to be resistant to the BellCoRe attack,
which was considered a &quot;challenging issue&quot; by the authors of the countermeasure
themselves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8173</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8173</id><created>2014-01-31</created><authors><author><keyname>Zaragoza</keyname><forenames>Daniel</forenames></author></authors><title>Modeling TCP Throughput with Random Packet Drops</title><categories>cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present report deals with the modeling of the long-term throughput,
a.k.a., send rate, of the Transmission Control Protocol (TCP) under the
following assumptions. (i) We consider a single 'infinite source' using a
network path from sender to receiver. (ii) Each TCP packet is randomly dropped
with probability p; independently of previous drops or any other
event/parameter. (iii) The - never changing - receiver window limits the amount
of outstanding data. (iv) The receiver acknowledges every packet. (v) The TCP
modeled here conforms to the publicly available standards (RFCs) as concerns
congestion control. We validate and determine the limits of the different
models proposed here using packet-level simulations. The contributions of the
present work are the following: (a) We determine three regimes, and their
conditions of applicability, depending on p: Linear law regime, square root law
regime, and timeout regime. (b) As concerns the relationship between the linear
and square root regimes, we give additional insights relatively to previously
published work. (c) We give the exact equations governing the TCP send rate in
any regime. (d) From the exact equation and under the further condition that
the path is not saturated, we give and discuss approximations for the send rate
of the NewReno variant of TCP. A by-product of these calculations is the
distribution of the sender window, independently of any timing or saturation
consideration. (e) These approximations give results that are accurate to a few
percent when compared to simulation results. Detailed comparison and sources of
errors between theory and simulations are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8175</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8175</id><created>2014-01-31</created><updated>2015-03-04</updated><authors><author><keyname>Suzuki</keyname><forenames>Toshio</forenames></author><author><keyname>Niida</keyname><forenames>Yoshinao</forenames></author></authors><title>Equilibrium Points of an AND-OR Tree: under Constraints on Probability</title><categories>cs.AI</categories><comments>13 pages, 3 figures</comments><msc-class>68T20, 68Q17, 03D15</msc-class><acm-class>I.2.8; F.2.2</acm-class><journal-ref>ANN PURE APPL LOGIC 166, pp. 1150--1164 (2015)</journal-ref><doi>10.1016/j.apal.2015.07.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a probability distribution d on the truth assignments to a uniform
binary AND-OR tree. Liu and Tanaka [2007, Inform. Process. Lett.] showed the
following: If d achieves the equilibrium among independent distributions (ID)
then d is an independent identical distribution (IID). We show a stronger form
of the above result. Given a real number r such that 0 &lt; r &lt; 1, we consider a
constraint that the probability of the root node having the value 0 is r. Our
main result is the following: When we restrict ourselves to IDs satisfying this
constraint, the above result of Liu and Tanaka still holds. The proof employs
clever tricks of induction. In particular, we show two fundamental
relationships between expected cost and probability in an IID on an OR-AND
tree: (1) The ratio of the cost to the probability (of the root having the
value 0) is a decreasing function of the probability x of the leaf. (2) The
ratio of derivative of the cost to the derivative of the probability is a
decreasing function of x, too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8176</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8176</id><created>2014-01-31</created><updated>2014-05-23</updated><authors><author><keyname>Colomer-de-Simon</keyname><forenames>Pol</forenames></author><author><keyname>Boguna</keyname><forenames>Marian</forenames></author></authors><title>Double percolation phase transition in clustered complex networks</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>Major improvement of the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The internal organization of complex networks often has striking consequences
on either their response to external perturbations or on their dynamical
properties. In addition to small-world and scale-free properties, clustering is
the most common topological characteristic observed in many real networked
systems. In this paper, we report an extensive numerical study on the effects
of clustering on the structural properties of complex networks. Strong
clustering in heterogeneous networks induces the emergence of a core-periphery
organization that has a critical effect on the percolation properties of the
networks. We observe a novel double phase transition with an intermediate phase
in which only the core of the network is percolated and a final phase in which
the periphery percolates regardless of the core. This result implies breaking
of the same symmetry at two different values of the control parameter, in stark
contrast to the modern theory of continuous phase transitions. Inspired by this
core-periphery organization, we introduce a simple model that allows us to
analytically prove that such an anomalous phase transition is in fact possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8192</identifier>
 <datestamp>2014-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8192</id><created>2014-01-31</created><updated>2014-08-06</updated><authors><author><keyname>Klos</keyname><forenames>Michal</forenames></author><author><keyname>Wawrzyniak</keyname><forenames>Karol</forenames></author><author><keyname>Jakubek</keyname><forenames>Marcin</forenames></author><author><keyname>Orynczak</keyname><forenames>Grzegorz</forenames></author></authors><title>The Scheme of a Novel Methodology for Zonal Division Based on Power
  Transfer Distribution Factors</title><categories>cs.CE cs.CY cs.SY</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the methodologies that carry out the division of the electrical grid
into zones is based on the aggregation of nodes characterized by similar Power
Transfer Distribution Factors (PTDFs). Here, we point out that satisfactory
clustering algorithm should take into account two aspects. First, nodes of
similar impact on cross-border lines should be grouped together. Second,
cross-border power flows should be relatively insensitive to differences
between real and assumed Generation Shift Key matrices. We introduce a
theoretical basis of a novel clustering algorithm (BubbleClust) that fulfills
these requirements and we perform a case study to illustrate social welfare
consequences of the division.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8199</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8199</id><created>2014-01-31</created><authors><author><keyname>Georg</keyname><forenames>S&#xf6;ren</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Matthias</forenames></author><author><keyname>Schulte</keyname><forenames>Horst</forenames></author></authors><title>Wind Turbine Model and Observer in Takagi-Sugeno Model Structure</title><categories>cs.SY</categories><comments>&quot;The Science of Making Torque from Wind&quot;, Oldenburg, Germany, October
  2012, European Academy of Wind Energy</comments><doi>10.1088/1742-6596/555/1/012042</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on a reduced-order, dynamic nonlinear wind turbine model in
Takagi-Sugeno (TS) model structure, a TS state observer is designed as a
disturbance observer to estimate the unknown effective wind speed. The TS
observer model is an exact representation of the underlying nonlinear model,
obtained by means of the sector-nonlinearity approach. The observer gain
matrices are obtained by means of a linear matrix inequality (LMI) design
approach for optimal fuzzy control, where weighting matrices for the individual
system states and outputs are included. The observer is tested in simulations
with the aero-elastic code FAST for the NREL 5 MW reference turbine, where it
shows a stable behaviour both for IEC wind gusts and turbulent wind input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8201</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8201</id><created>2014-01-31</created><updated>2014-11-28</updated><authors><author><keyname>Fletcher</keyname><forenames>George H. L.</forenames></author><author><keyname>Gyssens</keyname><forenames>Marc</forenames></author><author><keyname>Leinders</keyname><forenames>Dirk</forenames></author><author><keyname>Surinx</keyname><forenames>Dimitri</forenames></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author><author><keyname>Van Gucht</keyname><forenames>Dirk</forenames></author><author><keyname>Vansummeren</keyname><forenames>Stijn</forenames></author><author><keyname>Wu</keyname><forenames>Yuqing</forenames></author></authors><title>Relative Expressive Power of Navigational Querying on Graphs</title><categories>cs.DB cs.LO</categories><comments>An extended abstract announcing the results of this paper was
  presented at the 14th International Conference on Database Theory, Uppsala,
  Sweden, March 2011</comments><journal-ref>Information Sciences (2015), pp. 390-406</journal-ref><doi>10.1016/j.ins.2014.11.031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by both established and new applications, we study navigational
query languages for graphs (binary relations). The simplest language has only
the two operators union and composition, together with the identity relation.
We make more powerful languages by adding any of the following operators:
intersection; set difference; projection; coprojection; converse; and the
diversity relation. All these operators map binary relations to binary
relations. We compare the expressive power of all resulting languages. We do
this not only for general path queries (queries where the result may be any
binary relation) but also for boolean or yes/no queries (expressed by the
nonemptiness of an expression). For both cases, we present the complete Hasse
diagram of relative expressiveness. In particular the Hasse diagram for boolean
queries contains some nontrivial separations and a few surprising collapses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8206</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8206</id><created>2014-01-31</created><authors><author><keyname>Vishwakarma</keyname><forenames>Sanjay</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>Decode-and-Forward Relay Beamforming with Secret and Non-Secret Messages</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study beamforming in decode-and-forward (DF) relaying using
multiple relays, where the source node sends a secret message as well as a
non-secret message to the destination node in the presence of multiple
non-colluding eavesdroppers. The non-secret message is transmitted at a fixed
rate $R_{0}$ and requires no protection from the eavesdroppers, whereas the
secret message needs to be protected from the eavesdroppers. The source and
relays operate under a total power constraint. We find the optimum source
powers and weights of the relays for both secret and non-secret messages which
maximize the worst case secrecy rate for the secret message as well as meet the
information rate constraint $R_{0}$ for the non-secret message. We solve this
problem for the cases when ($i$) perfect channel state information (CSI) of all
links is known, and ($ii$) only the statistical CSI of the eavesdroppers links
and perfect CSI of other links are known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8209</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8209</id><created>2014-01-31</created><updated>2015-04-10</updated><authors><author><keyname>Haeusler</keyname><forenames>Edward Hermann</forenames><affiliation>PUC-Rio</affiliation></author></authors><title>Propositional Logics Complexity and the Sub-Formula Property</title><categories>cs.LO</categories><comments>In Proceedings DCM 2014, arXiv:1504.01927</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 179, 2015, pp. 1-16</journal-ref><doi>10.4204/EPTCS.179.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1979 Richard Statman proved, using proof-theory, that the purely
implicational fragment of Intuitionistic Logic (M-imply) is PSPACE-complete. He
showed a polynomially bounded translation from full Intuitionistic
Propositional Logic into its implicational fragment. By the PSPACE-completeness
of S4, proved by Ladner, and the Goedel translation from S4 into Intuitionistic
Logic, the PSPACE- completeness of M-imply is drawn. The sub-formula principle
for a deductive system for a logic L states that whenever F1,...,Fk proves A,
there is a proof in which each formula occurrence is either a sub-formula of A
or of some of Fi. In this work we extend Statman result and show that any
propositional (possibly modal) structural logic satisfying a particular
formulation of the sub-formula principle is in PSPACE. If the logic includes
the minimal purely implicational logic then it is PSPACE-complete. As a
consequence, EXPTIME-complete propositional logics, such as PDL and the
common-knowledge epistemic logic with at least 2 agents satisfy this particular
sub-formula principle, if and only if, PSPACE=EXPTIME. We also show how our
technique can be used to prove that any finitely many-valued logic has the set
of its tautologies in PSPACE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8212</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8212</id><created>2014-01-30</created><authors><author><keyname>Rasekh</keyname><forenames>Amin</forenames></author><author><keyname>Chen</keyname><forenames>Chien-An</forenames></author><author><keyname>Lu</keyname><forenames>Yan</forenames></author></authors><title>Human Activity Recognition using Smartphone</title><categories>cs.CY cs.HC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human activity recognition has wide applications in medical research and
human survey system. In this project, we design a robust activity recognition
system based on a smartphone. The system uses a 3-dimentional smartphone
accelerometer as the only sensor to collect time series signals, from which 31
features are generated in both time and frequency domain. Activities are
classified using 4 different passive learning methods, i.e., quadratic
classifier, k-nearest neighbor algorithm, support vector machine, and
artificial neural networks. Dimensionality reduction is performed through both
feature extraction and subset selection. Besides passive learning, we also
apply active learning algorithms to reduce data labeling expense. Experiment
results show that the classification rate of passive learning reaches 84.4% and
it is robust to common positions and poses of cellphone. The results of active
learning on real data demonstrate a reduction of labeling labor to achieve
comparable performance with passive learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8219</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8219</id><created>2014-01-31</created><updated>2014-06-20</updated><authors><author><keyname>Ku&#x142;akowski</keyname><forenames>Konrad</forenames></author></authors><title>On the Properties of the Priority Deriving Procedure in the Pairwise
  Comparisons Method</title><categories>cs.DM cs.GT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pairwise comparisons method is a convenient tool used when the relative
order of preferences among different concepts (alternatives) needs to be
determined. There are several popular implementations of this method, including
the Eigenvector Method, the Least Squares Method, the Chi Squares Method and
others. Each of the above methods comes with one or more inconsistency indices
that help to decide whether the consistency of input guarantees obtaining a
reliable output, thus taking the optimal decision. This article explores the
relationship between inconsistency of input and discrepancy of output. A global
ranking discrepancy describes to what extent the obtained results correspond to
the single expert's assessments. On the basis of the inconsistency and
discrepancy indices, two properties of the weight deriving procedure are
formulated. These properties are proven for Eigenvector Method and Koczkodaj's
Inconsistency Index. Several estimates using Koczkodaj's Inconsistency Index
for a principal eigenvalue, Saaty's inconsistency index and the Condition of
Order Preservation are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8226</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8226</id><created>2014-01-30</created><authors><author><keyname>Karunakaran</keyname><forenames>Prasanth</forenames></author><author><keyname>Wagner</keyname><forenames>Thomas</forenames></author><author><keyname>Scherb</keyname><forenames>Ansgar</forenames></author><author><keyname>Gerstacker</keyname><forenames>Wolfgang</forenames></author></authors><title>Sensing for Spectrum Sharing in Cognitive LTE-A Cellular Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present a case for dynamic spectrum sharing between different
operators in systems with carrier aggregation (CA) which is an important
feature in 3GPP LTE-A systems. Cross-carrier scheduling and sensing are
identified as key enablers for such spectrum sharing in LTE-A. Sensing is
classified as Type 1 sensing and Type 2 sensing and the role of each in the
system operation is discussed. The more challenging Type 2 sensing which
involves sensing the interfering signal in the presence of a desired signal is
studied for a single-input single-output system. Energy detection and the most
powerful test are formulated. The probability of false alarm and of detection
are analyzed for energy detectors. Performance evaluations show that reasonable
sensing performance can be achieved with the use of channel state information,
making such sensing practically viable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8230</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8230</id><created>2014-01-29</created><updated>2014-05-12</updated><authors><author><keyname>Demchik</keyname><forenames>Vadim</forenames></author><author><keyname>Gulov</keyname><forenames>Alexey</forenames></author></authors><title>Increasing precision of uniform pseudorandom number generators</title><categories>cs.MS cs.CR cs.DS</categories><comments>5 pages, 1 figure; additional description of algorithm is applied</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general method to produce uniformly distributed pseudorandom numbers with
extended precision by combining two pseudorandom numbers with lower precision
is proposed. In particular, this method can be used for pseudorandom number
generation with extended precision on graphics processing units (GPU), where
the performance of single and double precision operations can vary
significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8242</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8242</id><created>2014-01-31</created><updated>2015-05-06</updated><authors><author><keyname>Hirsch</keyname><forenames>Dan</forenames></author><author><keyname>Markstr&#xf6;m</keyname><forenames>Ingemar</forenames></author><author><keyname>Patterson</keyname><forenames>Meredith L</forenames></author><author><keyname>Sandberg</keyname><forenames>Anders</forenames></author><author><keyname>Vejdemo-Johansson</keyname><forenames>Mikael</forenames></author></authors><title>More ties than we thought</title><categories>cs.FL cs.CG math.CO math.GN</categories><comments>Accepted at PeerJ Computer Science 12 pages, 6 color photographs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the existing enumeration of neck tie-knots to include tie-knots
with a textured front, tied with the narrow end of a tie. These tie-knots have
gained popularity in recent years, based on reconstructions of a costume detail
from The Matrix Reloaded, and are explicitly ruled out in the enumeration by
Fink and Mao (2000).
  We show that the relaxed tie-knot description language that comprehensively
describes these extended tie-knot classes is context free. It has a regular
sub-language that covers all the knots that originally inspired the work.
  From the full language, we enumerate 266 682 distinct tie-knots that seem
tie-able with a normal neck-tie. Out of these 266 682, we also enumerate 24 882
tie-knots that belong to the regular sub-language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8244</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8244</id><created>2014-01-31</created><updated>2014-04-30</updated><authors><author><keyname>Meng</keyname><forenames>Chun</forenames></author><author><keyname>Chen</keyname><forenames>Minghua</forenames></author><author><keyname>Markopoulou</keyname><forenames>Athina</forenames></author></authors><title>On Routing-Optimal Network for Multiple Unicasts</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider networks with multiple unicast sessions.
Generally, non-linear network coding is needed to achieve the whole rate region
of network coding. Yet, there exist networks for which routing is sufficient to
achieve the whole rate region, and we refer to them as routing-optimal
networks. We identify a class of routing-optimal networks, which we refer to as
information-distributive networks, defined by three topological features. Due
to these features, for each rate vector achieved by network coding, there is
always a routing scheme such that it achieves the same rate vector, and the
traffic transmitted through the network is exactly the information transmitted
over the cut-sets between the sources and the sinks in the corresponding
network coding scheme. We present more examples of information-distributive
networks, including some examples from index coding and single unicast with
hard deadline constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8252</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8252</id><created>2014-01-31</created><authors><author><keyname>Mouzoune</keyname><forenames>Abdessamad</forenames></author><author><keyname>Taibi</keyname><forenames>Saoudi</forenames></author></authors><title>Introducing E-maintenance 2.0</title><categories>cs.OH</categories><comments>11 pages, 1 figure</comments><journal-ref>International Journal of Computer Science and Business
  Informatics, Vol. 9, No. 1, pp. 80-90</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While research literature is still debating e-maintenance definition, a new
reality is emerging in business world confirming the enterprise 2.0 model.
Executives are more and more forced to stop running against current trend
towards social media and instead envisage harnessing its power within the
enterprise. Maintenance can't be an exception for long and has to take
advantage of new opportunities created by social technological innovations. In
this paper, a combination of pure E perspective and 2.0 perspective is proposed
to avoid a lock-in and allow continous evolution of e-maintenance within the
new context of business: A combination of data centric models and people
oriented applications to form a collaborative environment in order to conceive
and achieve global goals of maintenance.New challenges are also to be expected
as to the effecient integration of enterprise 2.0 tools within current
e-maintenance platforms and futher research work is still to be done in this
area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8255</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8255</id><created>2014-01-31</created><authors><author><keyname>Carter</keyname><forenames>Kevin M.</forenames></author><author><keyname>Okhravi</keyname><forenames>Hamed</forenames></author><author><keyname>Riordan</keyname><forenames>James</forenames></author></authors><title>Quantitative Analysis of Active Cyber Defenses Based on Temporal
  Platform Diversity</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active cyber defenses based on temporal platform diversity have been proposed
as way to make systems more resistant to attacks. These defenses change the
properties of the platforms in order to make attacks more complicated.
Unfortunately, little work has been done on measuring the effectiveness of
these defenses. In this work, we use four different approaches to
quantitatively analyze these defenses; an abstract analysis studies the
algebraic models of a temporal platform diversity system; a set of experiments
on a test bed measures the metrics of interest for the system; a game theoretic
analysis studies the impact of preferential selection of platforms and derives
an optimal strategy; finally, a set of simulations evaluates the metrics of
interest on the models. Our results from these approaches all agree and yet are
counter-intuitive. We show that although platform diversity can mitigate some
attacks, it can be detrimental for others. We also illustrate that the benefit
from these systems heavily depends on their threat model and that the
preferential selection of platforms can achieve better protection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8257</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8257</id><created>2014-01-31</created><updated>2014-06-06</updated><authors><author><keyname>Gentile</keyname><forenames>Claudio</forenames></author><author><keyname>Li</keyname><forenames>Shuai</forenames></author><author><keyname>Zappella</keyname><forenames>Giovanni</forenames></author></authors><title>Online Clustering of Bandits</title><categories>cs.LG stat.ML</categories><comments>In E. Xing and T. Jebara (Eds.), Proceedings of 31st International
  Conference on Machine Learning, Journal of Machine Learning Research Workshop
  and Conference Proceedings, Vol.32 (JMLR W&amp;CP-32), Beijing, China, Jun.
  21-26, 2014 (ICML 2014), Submitted by Shuai Li
  (https://sites.google.com/site/shuailidotsli)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel algorithmic approach to content recommendation based on
adaptive clustering of exploration-exploitation (&quot;bandit&quot;) strategies. We
provide a sharp regret analysis of this algorithm in a standard stochastic
noise setting, demonstrate its scalability properties, and prove its
effectiveness on a number of artificial and real-world datasets. Our
experiments show a significant increase in prediction performance over
state-of-the-art methods for bandit problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8261</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8261</id><created>2014-01-28</created><authors><author><keyname>Ghiass</keyname><forenames>Reza Shoja</forenames></author><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author><author><keyname>Bendada</keyname><forenames>Hakim</forenames></author><author><keyname>Maldague</keyname><forenames>Xavier</forenames></author></authors><title>Infrared face recognition: a comprehensive review of methodologies and
  databases</title><categories>cs.CV</categories><comments>Pattern Recognition, 2014. arXiv admin note: substantial text overlap
  with arXiv:1306.1603</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic face recognition is an area with immense practical potential which
includes a wide range of commercial and law enforcement applications. Hence it
is unsurprising that it continues to be one of the most active research areas
of computer vision. Even after over three decades of intense research, the
state-of-the-art in face recognition continues to improve, benefitting from
advances in a range of different research fields such as image processing,
pattern recognition, computer graphics, and physiology. Systems based on
visible spectrum images, the most researched face recognition modality, have
reached a significant level of maturity with some practical success. However,
they continue to face challenges in the presence of illumination, pose and
expression changes, as well as facial disguises, all of which can significantly
decrease recognition accuracy. Amongst various approaches which have been
proposed in an attempt to overcome these limitations, the use of infrared (IR)
imaging has emerged as a particularly promising research direction. This paper
presents a comprehensive and timely review of the literature on this subject.
Our key contributions are: (i) a summary of the inherent properties of infrared
imaging which makes this modality promising in the context of face recognition,
(ii) a systematic review of the most influential approaches, with a focus on
emerging common trends as well as key differences between alternative
methodologies, (iii) a description of the main databases of infrared facial
images available to the researcher, and lastly (iv) a discussion of the most
promising avenues for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8265</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8265</id><created>2014-01-31</created><updated>2015-04-20</updated><authors><author><keyname>Gherekhloo</keyname><forenames>Soheil</forenames></author><author><keyname>Di</keyname><forenames>Chen</forenames></author><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Sub-optimality of Treating Interference as Noise in the Cellular Uplink
  with Weak Interference</title><categories>cs.IT math.IT</categories><comments>This paper is an extended version of the paper
  http://arxiv.org/abs/1105.5072, presented at ITG WSA, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the simplicity of the scheme of treating interference as noise (TIN),
it was shown to be sum-capacity optimal in the Gaussian interference channel
(IC) with very-weak (noisy) interference. In this paper, the 2-user IC is
altered by introducing an additional transmitter that wants to communicate with
one of the receivers of the IC. The resulting network thus consists of a
point-to-point channel interfering with a multiple access channel (MAC) and is
denoted PIMAC. The sum-capacity of the PIMAC is studied with main focus on the
optimality of TIN. It turns out that TIN in its naive variant, where all
transmitters are active and both receivers use TIN for decoding, is not the
best choice for the PIMAC. In fact, a scheme that combines both time division
multiple access and TIN (TDMA-TIN) strictly outperforms the naive-TIN scheme.
Furthermore, it is shown that in some regimes, TDMA-TIN achieves the
sum-capacity for the deterministic PIMAC and the sum-capacity within a constant
gap for the Gaussian PIMAC. Additionally, it is shown that, even for very-weak
interference, there are some regimes where a combination of interference
alignment with power control and treating interference as noise at the receiver
side outperforms TDMA-TIN. As a consequence, on the one hand treating
interference as noise in a cellular uplink is approximately optimal in certain
regimes. On the other hand those regimes cannot be simply described by the
strength of interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8269</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8269</id><created>2014-01-31</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames></author><author><keyname>Mohammad</keyname><forenames>Saif M.</forenames></author></authors><title>Experiments with Three Approaches to Recognizing Lexical Entailment</title><categories>cs.CL cs.AI cs.LG</categories><comments>to appear in Natural Language Engineering</comments><acm-class>H.3.1; I.2.6; I.2.7</acm-class><journal-ref>Natural Language Engineering, 21 (3), (2015), 437-476</journal-ref><doi>10.1017/S1351324913000387</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference in natural language often involves recognizing lexical entailment
(RLE); that is, identifying whether one word entails another. For example,
&quot;buy&quot; entails &quot;own&quot;. Two general strategies for RLE have been proposed: One
strategy is to manually construct an asymmetric similarity measure for context
vectors (directional similarity) and another is to treat RLE as a problem of
learning to recognize semantic relations using supervised machine learning
techniques (relation classification). In this paper, we experiment with two
recent state-of-the-art representatives of the two general strategies. The
first approach is an asymmetric similarity measure (an instance of the
directional similarity strategy), designed to capture the degree to which the
contexts of a word, a, form a subset of the contexts of another word, b. The
second approach (an instance of the relation classification strategy)
represents a word pair, a:b, with a feature vector that is the concatenation of
the context vectors of a and b, and then applies supervised learning to a
training set of labeled feature vectors. Additionally, we introduce a third
approach that is a new instance of the relation classification strategy. The
third approach represents a word pair, a:b, with a feature vector in which the
features are the differences in the similarities of a and b to a set of
reference words. All three approaches use vector space models (VSMs) of
semantics, based on word-context matrices. We perform an extensive evaluation
of the three approaches using three different datasets. The proposed new
approach (similarity differences) performs significantly better than the other
two approaches on some datasets and there is no dataset for which it is
significantly worse. Our results suggest it is beneficial to make connections
between the research in lexical entailment and the research in semantic
relation classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.8294</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.8294</id><created>2014-01-31</created><authors><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Evolution of extortion in structured populations</title><categories>physics.soc-ph cs.GT q-bio.PE</categories><comments>5 two-column pages, 4 figures; accepted for publication in Physical
  Review E</comments><journal-ref>Phys. Rev. E 89 (2014) 022804</journal-ref><doi>10.1103/PhysRevE.89.022804</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extortion strategies can dominate any opponent in an iterated prisoner's
dilemma game. But if players are able to adopt the strategies performing
better, extortion becomes widespread and evolutionary unstable. It may
sometimes act as a catalyst for the evolution of cooperation, and it can also
emerge in interactions between two populations, yet it is not the evolutionary
stable outcome. Here we revisit these results in the realm of spatial games. We
find that pairwise imitation and birth-death dynamics return known evolutionary
outcomes. Myopic best response strategy updating, on the other hand, reveals
new counterintuitive solutions. Defectors and extortioners coarsen
spontaneously, which allows cooperators to prevail even at prohibitively high
temptations to defect. Here extortion strategies play the role of a Trojan
horse. They may emerge among defectors by chance, and once they do, cooperators
become viable as well. These results are independent of the interaction
topology, and they highlight the importance of coarsening, checkerboard
ordering, and best response updating in evolutionary games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0009</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0009</id><created>2014-01-31</created><authors><author><keyname>McClelland</keyname><forenames>Mark</forenames></author><author><keyname>Estlin</keyname><forenames>Tara</forenames></author><author><keyname>Campbell</keyname><forenames>Mark</forenames></author></authors><title>Qualitative Relational Mapping and Navigation for Planetary Rovers</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel method for qualitative mapping of large scale
spaces. The proposed framework makes use of a graphical representation of the
world in order to build a map consisting of qualitative constraints on the
geometric relationships between landmark triplets. A novel measurement method
based on camera imagery is presented which extends previous work from the field
of Qualitative Spatial Reasoning. Measurements are fused into the map using a
deterministic, iterative graph update. A Branch-and-Bound approach is taken to
solve a set of non-convex feasibility problems required for generating on-line
measurements and off-line operator lookup tables. A navigation approach for
travel between distant landmarks is developed, using estimates of the Relative
Neighborhood Graph extracted from the qualitative map in order to generate a
sequence of landmark objectives based on proximity. Average and asymptotic
performance of the mapping algorithm is evaluated using Monte Carlo tests on
randomly generated maps, and data-driven simulation results are presented for a
robot traversing the Jet Propulsion Laboratory Mars Yard while building a
relational map. Simulation results demonstrate an initial rapid convergence of
qualitative state estimates for visible landmarks, followed by a slow tapering
as the remaining ambiguous states are removed from the map.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0013</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0013</id><created>2014-01-31</created><authors><author><keyname>Lim</keyname><forenames>Yeon-sup</forenames></author><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Classifying Latent Infection States in Complex Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms for identifying the infection states of nodes in a network are
crucial for understanding and containing infections. Often, however, only a
relatively small set of nodes have a known infection state. Moreover, the
length of time that each node has been infected is also unknown. This missing
data -- infection state of most nodes and infection time of the unobserved
infected nodes -- poses a challenge to the study of real-world cascades.
  In this work, we develop techniques to identify the latent infected nodes in
the presence of missing infection time-and-state data. Based on the likely
epidemic paths predicted by the simple susceptible-infected epidemic model, we
propose a measure (Infection Betweenness) for uncovering these unknown
infection states. Our experimental results using machine learning algorithms
show that Infection Betweenness is the most effective feature for identifying
latent infected nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0017</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0017</id><created>2014-01-31</created><updated>2015-02-15</updated><authors><author><keyname>Kourtellaris</keyname><forenames>Christos K.</forenames></author><author><keyname>Charalambous</keyname><forenames>Charalambos D.</forenames></author></authors><title>Capacity of Binary State Symmetric Channel with and without Feedback and
  Transmission Cost</title><categories>cs.IT math.IT</categories><doi>10.1109/ITW.2015.7133133</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a unit memory channel, called Binary State Symmetric Channel
(BSSC), in which the channel state is the modulo2 addition of the current
channel input and the previous channel output. We derive closed form
expressions for the capacity and corresponding channel input distribution, of
this BSSC with and without feedback and transmission cost. We also show that
the capacity of the BSSC is not increased by feedback, and it is achieved by a
first order symmetric Markov process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0024</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0024</id><created>2014-01-31</created><authors><author><keyname>Le</keyname><forenames>Van Bang</forenames></author><author><keyname>Oversberg</keyname><forenames>Andrea</forenames></author><author><keyname>Schaudt</keyname><forenames>Oliver</forenames></author></authors><title>Polynomial time recognition of squares of ptolemaic graphs and
  3-sun-free split graphs</title><categories>cs.DM math.CO</categories><msc-class>05C75, 05C85, 68R05, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The square of a graph $G$, denoted $G^2$, is obtained from $G$ by putting an
edge between two distinct vertices whenever their distance is two. Then $G$ is
called a square root of $G^2$. Deciding whether a given graph has a square root
is known to be NP-complete, even if the root is required to be a chordal graph
or even a split graph.
  We present a polynomial time algorithm that decides whether a given graph has
a ptolemaic square root. If such a root exists, our algorithm computes one with
a minimum number of edges.
  In the second part of our paper, we give a characterization of the graphs
that admit a 3-sun-free split square root. This characterization yields a
polynomial time algorithm to decide whether a given graph has such a root, and
if so, to compute one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0029</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0029</id><created>2014-01-31</created><authors><author><keyname>Acheson</keyname><forenames>Paulette</forenames></author><author><keyname>Dagli</keyname><forenames>Cihan</forenames></author><author><keyname>Kilicay-Ergin</keyname><forenames>Nil</forenames></author></authors><title>Fuzzy Decision Analysis in Negotiation between the System of Systems
  Agent and the System Agent in an Agent-Based Model</title><categories>cs.MA</categories><comments>5 pages, 5 figures, 2 tables, International Journal of Soft Computing
  and Software Engineering [JSCSE], Vol. 3, No. 3, 2013</comments><doi>10.7321/jscse.v3.n3.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous papers have described a computational approach to System of Systems
(SoS) development using an Agent-Based Model (ABM). This paper describes the
Fuzzy Decision Analysis used in the negotiation between the SoS agent and a
System agent in the ABM of an Acknowledged SoS development. An Acknowledged SoS
has by definition a limited influence on the development of the individual
Systems. The individual Systems have their own priorities, pressures, and
agenda which may or may not align with the goals of the SoS. The SoS has some
funding and deadlines which can be used to negotiate with the individual System
in order to illicit the required capability from that System. The Fuzzy
Decision Analysis determines how the SoS agent will adjust the funding and
deadlines for each of the Systems in order to achieve the desired SoS
architecture quality. The Fuzzy Decision Analysis has inputs of performance,
funding, and deadlines as well as weights for each capability. The performance,
funding, and deadlines are crisp values which are fuzzified. The fuzzified
values are then used with a Fuzzy Inference Engine to get the fuzzy outputs of
funding adjustment and deadline adjustment which must then be defuzzified
before being passed to the System agent. The first contribution of this paper
is the fuzzy decision analysis that represents the negotiation between the SoS
agent and the System agent. A second contribution of this paper is the method
of implementing the fuzzy decision analysis which provides a generalized fuzzy
decision analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0030</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0030</id><created>2014-01-31</created><updated>2014-06-04</updated><authors><author><keyname>Mnih</keyname><forenames>Andriy</forenames></author><author><keyname>Gregor</keyname><forenames>Karol</forenames></author></authors><title>Neural Variational Inference and Learning in Belief Networks</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Highly expressive directed latent variable models, such as sigmoid belief
networks, are difficult to train on large datasets because exact inference in
them is intractable and none of the approximate inference methods that have
been applied to them scale well. We propose a fast non-iterative approximate
inference method that uses a feedforward network to implement efficient exact
sampling from the variational posterior. The model and this inference network
are trained jointly by maximizing a variational lower bound on the
log-likelihood. Although the naive estimator of the inference model gradient is
too high-variance to be useful, we make it practical by applying several
straightforward model-independent variance reduction techniques. Applying our
approach to training sigmoid belief networks and deep autoregressive networks,
we show that it outperforms the wake-sleep algorithm on MNIST and achieves
state-of-the-art results on the Reuters RCV1 document dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0033</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0033</id><created>2014-01-31</created><authors><author><keyname>Grudzinska</keyname><forenames>Justyna</forenames></author><author><keyname>Zawadowski</keyname><forenames>Marek</forenames></author></authors><title>Generalized Quantifiers on Dependent Types: A System for Anaphora</title><categories>math.LO cs.LO</categories><comments>25 pages</comments><msc-class>03B65, 91F20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a system for the interpretation of anaphoric relationships between
unbound pronouns and quantifiers. The main technical contribution of our
proposal consists in combining generalized quantifiers with dependent types.
Empirically, our system allows a uniform treatment of all types of unbound
anaphora, including the notoriously difficult cases such as quantificational
subordination, cumulative and branching continuations, and 'donkey anaphora'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0049</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0049</id><created>2014-01-31</created><updated>2014-06-13</updated><authors><author><keyname>Liu</keyname><forenames>Yi-Kai</forenames></author></authors><title>Single-shot security for one-time memories in the isolated qubits model</title><categories>quant-ph cs.CR cs.IT math.IT</categories><comments>v2: to appear in CRYPTO 2014. 21 pages, 3 figures</comments><journal-ref>CRYPTO 2014, Part II, Lecture Notes in Computer Science Volume
  8617, pp.19-36 (2014)</journal-ref><doi>10.1007/978-3-662-44381-1_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One-time memories (OTM's) are simple, tamper-resistant cryptographic devices,
which can be used to implement sophisticated functionalities such as one-time
programs. Can one construct OTM's whose security follows from some physical
principle? This is not possible in a fully-classical world, or in a
fully-quantum world, but there is evidence that OTM's can be built using
&quot;isolated qubits&quot; -- qubits that cannot be entangled, but can be accessed using
adaptive sequences of single-qubit measurements.
  Here we present new constructions for OTM's using isolated qubits, which
improve on previous work in several respects: they achieve a stronger
&quot;single-shot&quot; security guarantee, which is stated in terms of the (smoothed)
min-entropy; they are proven secure against adversaries who can perform
arbitrary local operations and classical communication (LOCC); and they are
efficiently implementable.
  These results use Wiesner's idea of conjugate coding, combined with
error-correcting codes that approach the capacity of the q-ary symmetric
channel, and a high-order entropic uncertainty relation, which was originally
developed for cryptography in the bounded quantum storage model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0050</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0050</id><created>2014-01-31</created><updated>2014-04-28</updated><authors><author><keyname>Yin</keyname><forenames>Qiang</forenames></author><author><keyname>Fu</keyname><forenames>Yuxi</forenames></author><author><keyname>He</keyname><forenames>Chaodong</forenames></author><author><keyname>Huang</keyname><forenames>Mingzhang</forenames></author><author><keyname>Tao</keyname><forenames>Xiuting</forenames></author></authors><title>Branching Bisimilarity Checking for PRS</title><categories>cs.LO</categories><comments>18 pages, 6 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies reveal that branching bisimilarity is decidable for both nBPP
(normed Basic Parallel Process) and nBPA (normed Basic Process Algebra). These
results lead to the question if there are any other models in the hierarchy of
PRS (Process Rewrite System) whose branching bisimilarity is decidable. It is
shown in this paper that the branching bisimilarity for both nOCN (normed One
Counter Net) and nPA (normed Process Algebra) is undecidable. These results
essentially imply that the question has a negative answer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0051</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0051</id><created>2014-01-31</created><updated>2014-04-10</updated><authors><author><keyname>Atanasov</keyname><forenames>Nikolay A.</forenames></author><author><keyname>Ny</keyname><forenames>Jerome Le</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Distributed Algorithms for Stochastic Source Seeking with Mobile Robot
  Networks: Technical Report</title><categories>cs.MA cs.RO cs.SY</categories><comments>13 pages (two-column); 3 figures; Manuscript submitted to the ASME
  Journal on Dynamic Systems, Measurement and Control (JDSMC); In version 2
  typos in the text were corrected, the proofs were cleaned up, hyperlinks were
  added to the bibliography, several clarifications were added to the text, and
  some statements were made more precise</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous robot networks are an effective tool for monitoring large-scale
environmental fields. This paper proposes distributed control strategies for
localizing the source of a noisy signal, which could represent a physical
quantity of interest such as magnetic force, heat, radio signal, or chemical
concentration. We develop algorithms specific to two scenarios: one in which
the sensors have a precise model of the signal formation process and one in
which a signal model is not available. In the model-free scenario, a team of
sensors is used to follow a stochastic gradient of the signal field. Our
approach is distributed, robust to deformations in the group geometry, does not
necessitate global localization, and is guaranteed to lead the sensors to a
neighborhood of a local maximum of the field. In the model-based scenario, the
sensors follow the stochastic gradient of the mutual information between their
expected measurements and the location of the source in a distributed manner.
The performance is demonstrated in simulation using a robot sensor network to
localize the source of a wireless radio signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0052</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0052</id><created>2014-02-01</created><updated>2014-09-29</updated><authors><author><keyname>Gamarnik</keyname><forenames>David</forenames></author><author><keyname>Sudan</keyname><forenames>Madhu</forenames></author></authors><title>Performance of the Survey Propagation-guided decimation algorithm for
  the random NAE-K-SAT problem</title><categories>math.PR cond-mat.stat-mech cs.AI cs.CC cs.DS math.CO</categories><comments>25 pages</comments><msc-class>60C05, 82B20, 05C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the Survey Propagation-guided decimation algorithm fails to find
satisfying assignments on random instances of the &quot;Not-All-Equal-$K$-SAT&quot;
problem if the number of message passing iterations is bounded by a constant
independent of the size of the instance and the clause-to-variable ratio is
above $(1+o_K(1)){2^{K-1}\over K}\log^2 K$ for sufficiently large $K$. Our
analysis in fact applies to a broad class of algorithms described as
&quot;sequential local algorithms&quot;. Such algorithms iteratively set variables based
on some local information and then recurse on the reduced instance. Survey
Propagation-guided as well as Belief Propagation-guided decimation algorithms -
two widely studied message passing based algorithms, fall under this category
of algorithms provided the number of message passing iterations is bounded by a
constant. Another well-known algorithm falling into this category is the Unit
Clause algorithm. Our work constitutes the first rigorous analysis of the
performance of the SP-guided decimation algorithm.
  The approach underlying our paper is based on an intricate geometry of the
solution space of random NAE-$K$-SAT problem. We show that above the
$(1+o_K(1)){2^{K-1}\over K}\log^2 K$ threshold, the overlap structure of
$m$-tuples of satisfying assignments exhibit a certain clustering behavior
expressed in the form of constraints on distances between the $m$ assignments,
for appropriately chosen $m$. We further show that if a sequential local
algorithm succeeds in finding a satisfying assignment with probability bounded
away from zero, then one can construct an $m$-tuple of solutions violating
these constraints, thus leading to a contradiction. Along with (citation), this
result is the first work which directly links the clustering property of random
constraint satisfaction problems to the computational hardness of finding
satisfying assignments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0054</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0054</id><created>2014-02-01</created><authors><author><keyname>Abboud</keyname><forenames>Amir</forenames></author><author><keyname>Williams</keyname><forenames>Virginia Vassilevska</forenames></author></authors><title>Popular conjectures imply strong lower bounds for dynamic problems</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider several well-studied problems in dynamic algorithms and prove
that sufficient progress on any of them would imply a breakthrough on one of
five major open problems in the theory of algorithms:
  1. Is the 3SUM problem on $n$ numbers in $O(n^{2-\epsilon})$ time for some
$\epsilon&gt;0$?
  2. Can one determine the satisfiability of a CNF formula on $n$ variables in
$O((2-\epsilon)^n poly n)$ time for some $\epsilon&gt;0$?
  3. Is the All Pairs Shortest Paths problem for graphs on $n$ vertices in
$O(n^{3-\epsilon})$ time for some $\epsilon&gt;0$?
  4. Is there a linear time algorithm that detects whether a given graph
contains a triangle?
  5. Is there an $O(n^{3-\epsilon})$ time combinatorial algorithm for $n\times
n$ Boolean matrix multiplication?
  The problems we consider include dynamic versions of bipartite perfect
matching, bipartite maximum weight matching, single source reachability, single
source shortest paths, strong connectivity, subgraph connectivity, diameter
approximation and some nongraph problems such as Pagh's problem defined in a
recent paper by Patrascu [STOC 2010].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0060</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0060</id><created>2014-02-01</created><updated>2014-09-13</updated><authors><author><keyname>Luo</keyname><forenames>Xue</forenames></author><author><keyname>Yau</keyname><forenames>Stephen S. -T.</forenames></author><author><keyname>Zhang</keyname><forenames>Mingyi</forenames></author><author><keyname>Zuo</keyname><forenames>Huaiqing</forenames></author></authors><title>On Classification of Toric Surface Codes of Low Dimension</title><categories>cs.IT math.CO math.IT</categories><comments>18 pages, 4 figures, 8 tables</comments><journal-ref>Finite Fields Appl., Vol. 33, pp. 90-102, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is a natural continuation of our previous work \cite{yz}. In this
paper, we give a complete classification of toric surface codes of dimension
less than or equal to 6, except a special pair, $C_{P_6^{(4)}}$ and
$C_{P_6^{(5)}}$ over $\mathbb{F}_8$. Also, we give an example, $C_{P_6^{(5)}}$
and $C_{P_6^{(6)}}$ over $\mathbb{F}_7$, to illustrate that two monomially
equivalent toric codes can be constructed from two lattice non-equivalent
polygons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0062</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0062</id><created>2014-02-01</created><authors><author><keyname>Kumar</keyname><forenames>Gowtham Ramani</forenames></author><author><keyname>Li</keyname><forenames>Cheuk Ting</forenames></author><author><keyname>Gamal</keyname><forenames>Abbas El</forenames></author></authors><title>Exact Common Information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the notion of exact common information, which is the
minimum description length of the common randomness needed for the exact
distributed generation of two correlated random variables $(X,Y)$. We introduce
the quantity $G(X;Y)=\min_{X\to W \to Y} H(W)$ as a natural bound on the exact
common information and study its properties and computation. We then introduce
the exact common information rate, which is the minimum description rate of the
common randomness for the exact generation of a 2-DMS $(X,Y)$. We give a
multiletter characterization for it as the limit $\bar{G}(X;Y)=\lim_{n\to
\infty}(1/n)G(X^n;Y^n)$. While in general $\bar{G}(X;Y)$ is greater than or
equal to the Wyner common information, we show that they are equal for the
Symmetric Binary Erasure Source. We do not know, however, if the exact common
information rate has a single letter characterization in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0068</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0068</id><created>2014-02-01</created><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author><author><keyname>Vijayalakshmi</keyname><forenames>V. J.</forenames></author></authors><title>Radiation Pattern of Patch Antenna with Slits</title><categories>cs.IT math.IT</categories><comments>6 pages and 7 figures</comments><doi>10.5121/ijit.2014.3101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Microstrip antenna has been commercially used in many applications, such
as direct broadcast satellite service, mobile satellite communications, global
positioning system, medical hyperthermia usage, etc. The patch antenna of the
size reduction at a given operating frequency is obtained. Mobile personal
communication systems and wireless computer networks are most commonly used
nowadays and they are in need of antennas in different frequency bands. In
regulate to without difficulty incorporate these antennas into individual
systems, a micro strip scrap transmitter have been preferred and intended for a
convinced divergence. There is also an analysis of radiation pattern, Gain of
the antenna, Directivity of the antenna, Electric Far Field. The simulations
results are obtained by using electromagnetic simulation software called feko
software are presented and discussed
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0081</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0081</id><created>2014-02-01</created><authors><author><keyname>Heras</keyname><forenames>J&#xf3;nathan</forenames></author><author><keyname>Komendantskaya</keyname><forenames>Ekaterina</forenames></author></authors><title>Proof Pattern Search in Coq/SSReflect</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ML4PG is an extension of the Proof General interface, allowing the user to
invoke machine-learning algorithms and find proof similarities in Coq/SSReect
libraries. In this paper, we present three new improvements to ML4PG. First, a
new method of &quot;recurrent clustering&quot; is introduced to collect statistical
features from Coq terms. Now the user can receive suggestions about similar
definitions, types and lemma statements, in addition to proof strategies.
Second, Coq proofs are split into patches to capture proof strategies that
could arise at different stages of a proof. Finally, we improve ML4PG's output
introducing an automaton-shape representation for proof patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0087</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0087</id><created>2014-02-01</created><authors><author><keyname>Nia</keyname><forenames>Mehran Alidoost</forenames></author><author><keyname>Atani</keyname><forenames>Reza Ebrahimi</forenames></author></authors><title>A novel datatype architecture support for programming languages</title><categories>cs.PL</categories><comments>This paper is accepted and published in International journal of
  Programming Languages and applications</comments><journal-ref>International journal of Programming Languages and applications
  Vol. 4 No.1 2014</journal-ref><doi>10.5121/ijpla</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In programmers point of view, Datatypes in programming language level have a
simple description but inside hardware, huge machine codes are responsible to
describe type features. Datatype architecture design is a novel approach to
match programming features along with hardware design. In this paper a novel
Data type-Based Code Reducer (TYPELINE) architecture is proposed and
implemented according to significant data types (SDT) of programming languages.
TYPELINE uses TEUs for processing various SDT operations. This architecture
design leads to reducing the number of machine codes, and increases execution
speed, and also improves some parallelism level. This is because this
architecture supports some operation for the execution of Abstract Data Types
in parallel. Also it ensures to maintain data type features and entire
application level specifications using the proposed type conversion unit. This
framework includes compiler level identifying execution modes and memory
management unit for decreasing object read/write in heap memory by ISA support.
This energy-efficient architecture is completely compatible with object
oriented programming languages and in combination mode it can process complex
C++ data structures with respect to parallel TYPELINE architecture support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0092</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0092</id><created>2014-02-01</created><authors><author><keyname>Harremo&#xeb;s</keyname><forenames>Peter</forenames></author></authors><title>Mutual information of Contingency Tables and Related Inequalities</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>A version without the appendix has been submitted to a conference</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  For testing independence it is very popular to use either the
$\chi^{2}$-statistic or $G^{2}$-statistics (mutual information). Asymptotically
both are $\chi^{2}$-distributed so an obvious question is which of the two
statistics that has a distribution that is closest to the
$\chi^{2}$-distribution. Surprisingly the distribution of mutual information is
much better approximated by a $\chi^{2}$-distribution than the
$\chi^{2}$-statistic. For technical reasons we shall focus on the simplest case
with one degree of freedom. We introduce the signed log-likelihood and
demonstrate that its distribution function can be related to the distribution
function of a standard Gaussian by inequalities. For the hypergeometric
distribution we formulate a general conjecture about how close the signed
log-likelihood is to a standard Gaussian, and this conjecture gives much more
accurate estimates of the tail probabilities of this type of distribution than
previously published results. The conjecture has been proved numerically in all
cases relevant for testing independence and further evidence of its validity is
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0099</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0099</id><created>2014-02-01</created><authors><author><keyname>Kir&#xe1;ly</keyname><forenames>Franz J.</forenames></author><author><keyname>Kreuzer</keyname><forenames>Martin</forenames></author><author><keyname>Theran</keyname><forenames>Louis</forenames></author></authors><title>Dual-to-kernel learning with ideals</title><categories>stat.ML cs.LG math.AC math.AG math.ST stat.TH</categories><comments>15 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a theory which unifies kernel learning and symbolic
algebraic methods. We show that both worlds are inherently dual to each other,
and we use this duality to combine the structure-awareness of algebraic methods
with the efficiency and generality of kernels. The main idea lies in relating
polynomial rings to feature space, and ideals to manifolds, then exploiting
this generative-discriminative duality on kernel matrices. We illustrate this
by proposing two algorithms, IPCA and AVICA, for simultaneous manifold and
feature learning, and test their accuracy on synthetic and real world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0108</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0108</id><created>2014-02-01</created><updated>2014-05-02</updated><authors><author><keyname>Strobl</keyname><forenames>Eric V.</forenames></author><author><keyname>Visweswaran</keyname><forenames>Shyam</forenames></author></authors><title>Markov Blanket Ranking using Kernel-based Conditional Dependence
  Measures</title><categories>stat.ML cs.LG</categories><comments>10 pages, 4 figures, 2 algorithms, NIPS 2013 Workshop on Causality,
  code: github.com/ericstrobl/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing feature selection algorithms that move beyond a pure correlational
to a more causal analysis of observational data is an important problem in the
sciences. Several algorithms attempt to do so by discovering the Markov blanket
of a target, but they all contain a forward selection step which variables must
pass in order to be included in the conditioning set. As a result, these
algorithms may not consider all possible conditional multivariate combinations.
We improve on this limitation by proposing a backward elimination method that
uses a kernel-based conditional dependence measure to identify the Markov
blanket in a fully multivariate fashion. The algorithm is easy to implement and
compares favorably to other methods on synthetic and real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0116</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0116</id><created>2014-02-01</created><authors><author><keyname>Nizamani</keyname><forenames>Sehrish</forenames></author><author><keyname>Khoumbati</keyname><forenames>Khalil</forenames></author><author><keyname>Ismaili</keyname><forenames>Imdad Ali</forenames></author><author><keyname>Nizamani</keyname><forenames>Saad</forenames></author></authors><title>A Conceptual Framework for ERP Evaluation in Universities of Pakistan</title><categories>cs.CY</categories><acm-class>H.3.4</acm-class><journal-ref>Sindh University Research Journal (Science Series) Vol. 45
  (3)467-475 (2013)</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The higher education has been greatly impacted by worldwide trends. In a
result, the universities throughout the world are focusing to enhance
performance and efficiency in their workings. Therefore, the higher education
has moved their systems to Enterprise Resource Planning (ERP) systems to cope
with the needs of changing environment. However, the literature review
indicates that there is void on the evaluation of success or failure of ERP
systems in higher education Institutes in Pakistan. In overall, ERP systems
implementation in higher education of Pakistan has not been given appropriate
research focus. Thus, in this paper the authors have attempted to develop a
conceptual framework for ERP evaluation in Universities of Pakistan. This seeks
to expand the knowledge on ERP in higher educational institutes of Pakistan and
focuses on understanding the ERP related critical success factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0119</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0119</id><created>2014-02-01</created><updated>2014-05-13</updated><authors><author><keyname>Lopez-Paz</keyname><forenames>David</forenames></author><author><keyname>Sra</keyname><forenames>Suvrit</forenames></author><author><keyname>Smola</keyname><forenames>Alex</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Randomized Nonlinear Component Analysis</title><categories>stat.ML cs.LG</categories><comments>Appearing in ICML 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical methods such as Principal Component Analysis (PCA) and Canonical
Correlation Analysis (CCA) are ubiquitous in statistics. However, these
techniques are only able to reveal linear relationships in data. Although
nonlinear variants of PCA and CCA have been proposed, these are computationally
prohibitive in the large scale.
  In a separate strand of recent research, randomized methods have been
proposed to construct features that help reveal nonlinear patterns in data. For
basic tasks such as regression or classification, random features exhibit
little or no loss in performance, while achieving drastic savings in
computational requirements.
  In this paper we leverage randomness to design scalable new variants of
nonlinear PCA and CCA; our ideas extend to key multivariate analysis tools such
as spectral clustering or LDA. We demonstrate our algorithms through
experiments on real-world data, on which we compare against the
state-of-the-art. A simple R implementation of the presented algorithms is
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0121</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0121</id><created>2014-02-01</created><updated>2015-02-16</updated><authors><author><keyname>Maurer</keyname><forenames>Alexandre</forenames><affiliation>EPFL</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LINCS, NPA</affiliation></author><author><keyname>D&#xe9;fago</keyname><forenames>Xavier</forenames><affiliation>JAIST</affiliation></author></authors><title>Reliable Communication in a Dynamic Network in the Presence of Byzantine
  Faults</title><categories>cs.DC cs.DS cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following problem: two nodes want to reliably communicate in
a dynamic multihop network where some nodes have been compromised, and may have
a totally arbitrary and unpredictable behavior. These nodes are called
Byzantine. We consider the two cases where cryptography is available and not
available. We prove the necessary and sufficient condition (that is, the
weakest possible condition) to ensure reliable communication in this context.
Our proof is constructive, as we provide Byzantine-resilient algorithms for
reliable communication that are optimal with respect to our impossibility
results. In a second part, we investigate the impact of our conditions in three
case studies: participants interacting in a conference, robots moving on a grid
and agents in the subway. Our simulations indicate a clear benefit of using our
algorithms for reliable communication in those contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0126</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0126</id><created>2014-02-01</created><authors><author><keyname>Cranmer</keyname><forenames>Skyler J.</forenames></author><author><keyname>Menninga</keyname><forenames>Elizabeth J.</forenames></author><author><keyname>Mucha</keyname><forenames>Peter J.</forenames></author></authors><title>Kantian fractionalization predicts the conflict propensity of the
  international system</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>17 pages + 17 pages designed as supplementary online material</comments><doi>10.1073/pnas.1509423112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of complex social and political phenomena with the perspective and
methods of network science has proven fruitful in a variety of areas, including
applications in political science and more narrowly the field of international
relations. We propose a new line of research in the study of international
conflict by showing that the multiplex fractionalization of the international
system (which we label Kantian fractionalization) is a powerful predictor of
the propensity for violent interstate conflict, a key indicator of the system's
stability. In so doing, we also demonstrate the first use of multislice
modularity for community detection in a multiplex network application. Even
after controlling for established system-level conflict indicators, we find
that Kantian fractionalization contributes more to model fit for violent
interstate conflict than previously established measures. Moreover, evaluating
the influence of each of the constituent networks shows that joint democracy
plays little, if any, role in predicting system stability, thus challenging a
major empirical finding of the international relations literature. Lastly, a
series of Granger causal tests shows that the temporal variability of Kantian
fractionalization is consistent with a causal relationship with the prevalence
of conflict in the international system. This causal relationship has
real-world policy implications as changes in Kantian fractionalization could
serve as an early warning sign of international instability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0130</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0130</id><created>2014-02-01</created><authors><author><keyname>Shu</keyname><forenames>Qin</forenames></author><author><keyname>Sanfelice</keyname><forenames>Ricardo G.</forenames></author></authors><title>Dynamical Properties of a Two-gene Network with Hysteresis</title><categories>cs.SY math.DS</categories><comments>55 pages, 31 figures.Expanded version of paper in Special Issue on
  Hybrid Systems and Biology, Elsevier Information and Computation, 2014</comments><report-no>HDC-2014-001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mathematical model for a two-gene regulatory network is derived and several
of their properties analyzed. Due to the presence of mixed continuous/discrete
dynamics and hysteresis, we employ a hybrid systems model to capture the
dynamics of the system. The proposed model incorporates binary hysteresis with
different thresholds capturing the interaction between the genes. We analyze
properties of the solutions and asymptotic stability of equilibria in the
system as a function of its parameters. Our analysis reveals the presence of
limit cycles for a certain range of parameters, behavior that is associated
with hysteresis. The set of points defining the limit cycle is characterized
and its asymptotic stability properties are studied. Furthermore, the stability
property of the limit cycle is robust to small perturbations. Numerical
simulations are presented to illustrate the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0134</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0134</id><created>2014-02-01</created><updated>2014-06-11</updated><authors><author><keyname>Akbari</keyname><forenames>S.</forenames></author><author><keyname>Dalirrooyfard</keyname><forenames>M.</forenames></author><author><keyname>Davodpoor</keyname><forenames>S.</forenames></author><author><keyname>Ehsani</keyname><forenames>K.</forenames></author><author><keyname>Sherkati</keyname><forenames>R.</forenames></author></authors><title>On the Decision Number of Graphs</title><categories>cs.DM math.CO</categories><comments>17 pages, 7 figures</comments><msc-class>05C05, 05C38, 05CC69, 05C78</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a graph. A good function is a function $f:V(G)\rightarrow
\{-1,1\}$, satisfying $f(N(v))\geq 1$, for each $v\in V(G)$, where $
N(v)=\{u\in V(G)\, |\, uv\in E(G) \} $ and $f(S) = \sum_{u\in S} f(u)$ for
every $S \subseteq V(G) $. For every cubic graph $G$ of order $ n, $ we prove
that $ \gamma(G) \leq \frac{5n}{7} $ and show that this inequality is sharp. A
function $f:V(G)\rightarrow \{-1,1\}$ is called a nice function, if
$f(N[v])\le1$, for each $v\in V(G)$, where $ N[v]=\{v\} \cup N(v) $. Define
$\overline{\beta}(G)=max\{f(V(G))\}$, where $f$ is a nice function for $G$. We
show that $\overline\beta(G)\ge -\frac{3n}{7}$ for every cubic graph $G$ of
order $n$, which improves the best known bound $-\frac{n}{2}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0140</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0140</id><created>2014-02-01</created><authors><author><keyname>Halder</keyname><forenames>Abhishek</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Raktim</forenames></author></authors><title>Probabilistic Model Validation for Uncertain Nonlinear Systems</title><categories>cs.SY math.OC</categories><comments>24 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a probabilistic model validation methodology for
nonlinear systems in time-domain. The proposed formulation is simple,
intuitive, and accounts both deterministic and stochastic nonlinear systems
with parametric and nonparametric uncertainties. Instead of hard invalidation
methods available in the literature, a relaxed notion of validation in
probability is introduced. To guarantee provably correct inference, algorithm
for constructing probabilistically robust validation certificate is given along
with computational complexities. Several examples are worked out to illustrate
its use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0146</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0146</id><created>2014-02-01</created><authors><author><keyname>Cao</keyname><forenames>Zhengjun</forenames></author><author><keyname>Liu</keyname><forenames>Lihua</forenames></author></authors><title>Remarks on AKS Primality Testing Algorithm and A Flaw in the Definition
  of P</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We remark that the AKS primality testing algorithm [Annals of Mathematics 160
(2), 2004] needs about 1,000,000,000 G (gigabyte) storage space for a number of
1024 bits. The requirement is very hard to meet. The complexity class P which
contains all decision problems that can be solved by a deterministic Turing
machine using a polynomial amount of computation time, is generally believed to
be ``easy&quot;. We point out that the time is estimated only in terms of the amount
of arithmetic operations. It does not comprise the time for reading and writing
data on the tape in a Turing machine. The flaw makes some deterministic
polynomial time algorithms impractical, and humbles the importance of P=NP
question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0147</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0147</id><created>2014-02-01</created><authors><author><keyname>Halder</keyname><forenames>Abhishek</forenames></author><author><keyname>Lee</keyname><forenames>Kooktae</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Raktim</forenames></author></authors><title>A Probabilistic Method for Nonlinear Robustness Analysis of F-16
  Controllers</title><categories>cs.SY math.OC</categories><comments>27 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new framework for controller robustness verification
with respect to F-16 aircraft's closed-loop performance in longitudinal flight.
We compare the state regulation performance of a linear quadratic regulator
(LQR) and a gain-scheduled linear quadratic regulator (gsLQR), applied to
nonlinear open-loop dynamics of F-16, in presence of stochastic initial
condition and parametric uncertainties, as well as actuator disturbance. We
show that, in presence of initial condition uncertainties alone, both LQR and
gsLQR have comparable immediate and asymptotic performances, but the gsLQR
exhibits better transient performance at intermediate times. This remains true
in the presence of additional actuator disturbance. Also, gsLQR is shown to be
more robust than LQR, against parametric uncertainties. The probabilistic
framework proposed here, leverages transfer operator based density computation
in exact arithmetic and introduces optimal transport theoretic performance
validation and verification (V&amp;V) for nonlinear dynamical systems. Numerical
results from our proposed method, are in unison with Monte Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0157</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0157</id><created>2014-02-02</created><authors><author><keyname>Salami</keyname><forenames>Hamza Onoruoiza</forenames></author><author><keyname>Ahmed</keyname><forenames>Moataz A.</forenames></author></authors><title>UML Artifacts Reuse: State of the Art</title><categories>cs.SE</categories><comments>Conference Proceedings</comments><journal-ref>International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 3, No. 3, pp. 115-122, 2013</journal-ref><doi>10.7321/jscse.v3.n3.19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The benefits that can be derived from reusing software include accelerated
development, reduced cost, reduced risk and effective use of specialists. Reuse
of software artifacts during the initial stages of software development
increases reuse benefits, because it allows subsequent reuse of later stage
artifacts derived from earlier artifacts. UML is the de facto modeling language
used by software developers during the initial stages of software development
such as requirements engineering, architectural and detailed design. This
survey analyzes previous works on UML artifacts reuse. The analysis considers
four perspectives: retrieval method, artifact support, tool support and
experiments performed. As an outcome of the analysis, some suggestions for
future work on UML artifacts reuse are also provided
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0160</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0160</id><created>2014-02-02</created><authors><author><keyname>Salami</keyname><forenames>Hamza Onoruoiza</forenames></author><author><keyname>Ahmed</keyname><forenames>Moataz</forenames></author></authors><title>A framework for reuse of multi-view UML artifacts</title><categories>cs.SE</categories><comments>conference proceeding</comments><journal-ref>International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 3, No. 3, pp. 156-162, 2013, Doi: 10.7321/jscse.v3.n3.25</journal-ref><doi>10.7321/jscse.v3.n3.25</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software is typically modeled from different viewpoints such as structural
view, behavioral view and functional view. Few existing works can be considered
as applying multi-view retrieval approaches. A number of important issues
regarding mapping of entities during multi-view retrieval of UML models is
identified in this study. In response, we describe a framework for reusing UML
artifacts, and discuss how our retrieval approach tackles the identified
issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0170</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0170</id><created>2014-02-02</created><authors><author><keyname>Kong</keyname><forenames>Shu</forenames></author><author><keyname>Jiang</keyname><forenames>Zhuolin</forenames></author><author><keyname>Yang</keyname><forenames>Qiang</forenames></author></authors><title>Collaborative Receptive Field Learning</title><categories>cs.CV cs.LG cs.MM stat.ML</categories><comments>16 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The challenge of object categorization in images is largely due to arbitrary
translations and scales of the foreground objects. To attack this difficulty,
we propose a new approach called collaborative receptive field learning to
extract specific receptive fields (RF's) or regions from multiple images, and
the selected RF's are supposed to focus on the foreground objects of a common
category. To this end, we solve the problem by maximizing a submodular function
over a similarity graph constructed by a pool of RF candidates. However,
measuring pairwise distance of RF's for building the similarity graph is a
nontrivial problem. Hence, we introduce a similarity metric called
pyramid-error distance (PED) to measure their pairwise distances through
summing up pyramid-like matching errors over a set of low-level features.
Besides, in consistent with the proposed PED, we construct a simple
nonparametric classifier for classification. Experimental results show that our
method effectively discovers the foreground objects in images, and improves
classification performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0197</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0197</id><created>2014-02-02</created><authors><author><keyname>Zubillaga</keyname><forenames>Dario</forenames></author><author><keyname>Cruz</keyname><forenames>Geovany</forenames></author><author><keyname>Aguilar</keyname><forenames>Luis Daniel</forenames></author><author><keyname>Zapotecatl</keyname><forenames>Jorge</forenames></author><author><keyname>Fernandez</keyname><forenames>Nelson</forenames></author><author><keyname>Aguilar</keyname><forenames>Jose</forenames></author><author><keyname>Rosenblueth</keyname><forenames>David A.</forenames></author><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Measuring the Complexity of Self-organizing Traffic Lights</title><categories>nlin.AO cs.IT cs.SY math.IT nlin.CG physics.soc-ph</categories><comments>18 pages, 11 figures</comments><acm-class>F.1.1; D.2.8; F.1.3; J.2; H.1.1</acm-class><doi>10.3390/e16052384</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply measures of complexity, emergence and self-organization to an
abstract city traffic model for comparing a traditional traffic coordination
method with a self-organizing method in two scenarios: cyclic boundaries and
non-orientable boundaries. We show that the measures are useful to identify and
characterize different dynamical phases. It becomes clear that different
operation regimes are required for different traffic demands. Thus, not only
traffic is a non-stationary problem, which requires controllers to adapt
constantly. Controllers must also change drastically the complexity of their
behavior depending on the demand. Based on our measures, we can say that the
self-organizing method achieves an adaptability level comparable to a living
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0200</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0200</id><created>2014-02-02</created><authors><author><keyname>Burrows</keyname><forenames>Alison</forenames></author><author><keyname>Gooberman-Hill</keyname><forenames>Rachel</forenames></author><author><keyname>Craddock</keyname><forenames>Ian</forenames></author><author><keyname>Coyle</keyname><forenames>David</forenames></author></authors><title>SPHERE: Meaningful and Inclusive Sensor-Based Home Healthcare</title><categories>cs.HC cs.CY</categories><comments>Presented at the ACM CSCW 2014 workshop on Designing with Users for
  Domestic environments: Methods, Challenges, Lessons Learned</comments><acm-class>H.5.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given current demographic and health trends, and their economic implications,
home healthcare technology has become a fertile area for research and
development. Motivated by the need for a radical reform of healthcare
provision, SPHERE is a large-scale Interdisciplinary Research Collaboration
that aims to develop home sensor systems to monitor people's health and
wellbeing in the home. This paper outlines the unique circumstances of
designing healthcare technology for the home environment, with a particular
focus on how to ensure future systems are meaningful to and desirable for the
intended users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0214</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0214</id><created>2014-02-02</created><authors><author><keyname>Kesidis</keyname><forenames>George</forenames></author><author><keyname>Pang</keyname><forenames>Guodong</forenames></author></authors><title>Golden-rule capacity allocation for distributed delay management in
  peer-to-peer networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a distributed framework for resources management in peer-to-peer
networks leading to golden-rule reciprocity, a kind of one-versus-rest
tit-for-tat, so that the delays experienced by any given peer's messages in the
rest of the network are proportional to those experienced by others' messages
at that peer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0215</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0215</id><created>2014-02-02</created><updated>2014-12-23</updated><authors><author><keyname>Bianconi</keyname><forenames>Ginestra</forenames></author><author><keyname>Dorogovtsev</keyname><forenames>Sergey N.</forenames></author><author><keyname>Mendes</keyname><forenames>Jos&#xe9; F. F.</forenames></author></authors><title>Mutually connected component of network of networks with replica nodes</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>(9 pages, 2 figures )</comments><journal-ref>Phys. Rev. E. 91, 012804 (2015)</journal-ref><doi>10.1103/PhysRevE.91.012804</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the emergence of the giant mutually connected component in
networks of networks in which each node has a single replica node in any layer
and can be interdependent only on its replica nodes in the interdependent
layers. We prove that if in these networks, all the nodes of one network
(layer) are interdependent on the nodes of the same other interconnected layer,
then, remarkably, the mutually connected component does not depend on the
topology of the network of networks. This component coincides with the mutual
component of the fully connected network of networks constructed from the same
set of layers, i.e., a multiplex network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0225</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0225</id><created>2014-02-02</created><authors><author><keyname>Haeusler</keyname><forenames>Edward Hermann</forenames></author><author><keyname>Rademaker</keyname><forenames>Alexandre</forenames></author></authors><title>An Intuitionisticaly based Description Logic</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This article presents iALC, an intuitionistic version of the classical
description logic ALC, based on the framework for constructive modal logics
presented by Simpson \cite{simpson95} and related to description languages, via
hybrid logics, by dePaiva \cite{depaiva2003}. This article correcta and extends
the presentation of iALC appearing in \cite{PHR:2010}. It points out the
difference between iALC and the intuitionistic hybrid logic presented in
\cite{depaiva2003}. Completeness and soundness proofs are provided. A brief
discussion on the computacional complexity of iALC provability is taken. It is
worth mentioning that iALC is used to formalize legal knowledge
\cite{HPR:2010a,HPR:2010ab,Jurix, HPR:2011}, and in fact, was specifically
designed to this goal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0238</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0238</id><created>2014-02-02</created><authors><author><keyname>Kantarc&#x131;</keyname><forenames>Burcu</forenames></author><author><keyname>Labatut</keyname><forenames>Vincent</forenames></author></authors><title>Classification of Complex Networks Based on Topological Properties</title><categories>cs.SI physics.soc-ph</categories><proxy>ccsd</proxy><journal-ref>3rd Conference on Social Computing and its Applications, Karlsruhe
  : Germany (2013)</journal-ref><doi>10.1109/CGC.2013.54</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks are a powerful modeling tool, allowing the study of
countless real-world systems. They have been used in very different domains
such as computer science, biology, sociology, management, etc. Authors have
been trying to characterize them using various measures such as degree
distribution, transitivity or average distance. Their goal is to detect certain
properties such as the small-world or scale-free properties. Previous works
have shown some of these properties are present in many different systems,
while others are characteristic of certain types of systems only. However, each
one of these studies generally focuses on a very small number of topological
measures and networks. In this work, we aim at using a more systematic
approach. We first constitute a dataset of 152 publicly available networks,
spanning over 7 different domains. We then process 14 different topological
measures to characterize them in the most possible complete way. Finally, we
apply standard data mining tools to analyze these data. A cluster analysis
reveals it is possible to obtain two significantly distinct clusters of
networks, corresponding roughly to a bisection of the domains modeled by the
networks. On these data, the most discriminant measures are density,
modularity, average degree and transitivity, and at a lesser extent, closeness
and edgebetweenness centralities.Abstract--Complex networks are a powerful
modeling tool, allowing the study of countless real-world systems. They have
been used in very different domains such as computer science, biology,
sociology, management, etc. Authors have been trying to characterize them using
various measures such as degree distribution, transitivity or average distance.
Their goal is to detect certain properties such as the small-world or
scale-free properties. Previous works have shown some of these properties are
present in many different systems, while others are characteristic of certain
types of systems only. However, each one of these studies generally focuses on
a very small number of topological measures and networks. In this work, we aim
at using a more systematic approach. We first constitute a dataset of 152
publicly available networks, spanning over 7 different domains. We then process
14 different topological measures to characterize them in the most possible
complete way. Finally, we apply standard data mining tools to analyze these
data. A cluster analysis reveals it is possible to obtain two significantly
distinct clusters of networks, corresponding roughly to a bisection of the
domains modeled by the networks. On these data, the most discriminant measures
are density, modularity, average degree and transitivity, and at a lesser
extent, closeness and edgebetweenness centralities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0239</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0239</id><created>2014-02-02</created><authors><author><keyname>Lipinski</keyname><forenames>Bartosz</forenames></author><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Szczypiorski</keyname><forenames>Krzysztof</forenames></author></authors><title>Improving Hard Disk Contention-based Covert Channel in Cloud Computing
  Environment</title><categories>cs.CR cs.DC</categories><comments>8 pages, 9 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steganographic methods allow the covert exchange of secret data between
parties aware of the procedure. The cloud computing environment is a new and
hot target for steganographers, and currently not many solutions have been
proposed. This paper proposes CloudSteg which is a steganographic method that
allows the creation of a covert channel based on hard disk contention between
the two cloud instances that reside on the same physical machine. Experimental
results conducted using open source cloud environment OpenStack, show that
CloudSteg is able to achieve a bandwidth of about 0.1 bps which is 1000 times
higher than is known from the state-of-the-art version.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0240</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0240</id><created>2014-02-02</created><updated>2014-08-30</updated><authors><author><keyname>Jegelka</keyname><forenames>Stefanie</forenames><affiliation>UC Berkeley</affiliation></author><author><keyname>Bilmes</keyname><forenames>Jeff</forenames><affiliation>University of Washington</affiliation></author></authors><title>Graph Cuts with Interacting Edge Costs - Examples, Approximations, and
  Algorithms</title><categories>cs.DS cs.CV cs.DM math.OC</categories><comments>45 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an extension of the classical graph cut problem, wherein we replace
the modular (sum of edge weights) cost function by a submodular set function
defined over graph edges. Special cases of this problem have appeared in
different applications in signal processing, machine learning and computer
vision. In this paper, we connect these applications via the generic
formulation of &quot;cooperative graph cuts&quot;, for which we study complexity,
algorithms and connections to polymatroidal network flows. Finally, we compare
the proposed algorithms empirically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0246</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0246</id><created>2014-02-02</created><updated>2015-01-12</updated><authors><author><keyname>Li</keyname><forenames>Di</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Moura</keyname><forenames>Jose' M. F.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>Distributed Kalman Filtering over Massive Data Sets: Analysis Through
  Large Deviations of Random Riccati Equations</title><categories>cs.IT math.IT</categories><comments>22 pages, 4 figures. IEEE Transactions on Information Thoery, to
  appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the convergence of the estimation error process and the
characterization of the corresponding invariant measure in distributed Kalman
filtering for potentially unstable and large linear dynamic systems. A gossip
network protocol termed Modified Gossip Interactive Kalman Filtering (M-GIKF)
is proposed, where sensors exchange their filtered states (estimates and error
covariances) and propagate their observations via inter-sensor communications
of rate $\overline{\gamma}$; $\overline{\gamma}$ is defined as the averaged
number of inter-sensor message passages per signal evolution epoch. The
filtered states are interpreted as stochastic particles swapped through local
interaction. The paper shows that the conditional estimation error covariance
sequence at each sensor under M-GIKF evolves as a random Riccati equation (RRE)
with Markov modulated switching. By formulating the RRE as a random dynamical
system, it is shown that the network achieves weak consensus, i.e., the
conditional estimation error covariance at a randomly selected sensor converges
weakly (in distribution) to a unique invariant measure. Further, it is proved
that as $\overline{\gamma} \rightarrow \infty$ this invariant measure satisfies
the Large Deviation (LD) upper and lower bounds, implying that this measure
converges exponentially fast (in probability) to the Dirac measure
$\delta_{P^*}$, where $P^*$ is the stable error covariance of the centralized
(Kalman) filtering setup. The LD results answer a fundamental question on how
to quantify the rate at which the distributed scheme approaches the centralized
performance as the inter-sensor communication rate increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0247</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0247</id><created>2014-02-02</created><authors><author><keyname>Munir</keyname><forenames>Ms. Rumaisah</forenames></author></authors><title>Secure Debit Card Device Model</title><categories>cs.CE cs.CR</categories><comments>Royal Institute of Technology, KTH</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The project envisages the implementation of an e-payment system utilizing
FIPS-201 Smart Card. The system combines hardware and software modules. The
hardware module takes data insertions (e.g. currency notes), processes the data
and then creates connection with the smart card using serial/USB ports to
perform further mathematical manipulations. The hardware interacts with servers
at the back for authentication and identification of users and for data storage
pertaining to a particular user. The software module manages database, handles
identities, provide authentication and secure communication between the various
system components. It will also provide a component to the end users. This
component can be in the form of software for computer or executable binaries
for PoS devices. The idea is to receive data in the embedded system from data
reader and smart card. After manipulations, the updated data is imprinted on
smart card memory and also updated in the back end servers maintaining
database. The information to be sent to a server is sent through a PoS device
which has multiple transfer mediums involving wired and un-wired mediums. The
user device also acts as an updater; therefore, whenever the smart card is
inserted by user, it is automatically updated by synchronizing with back-end
database. The project required expertise in embedded systems, networks, java
and C++ (Optional).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0258</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0258</id><created>2014-02-02</created><updated>2015-07-27</updated><authors><author><keyname>Unal</keyname><forenames>Sinem</forenames></author><author><keyname>Wagner</keyname><forenames>Aaron B.</forenames></author></authors><title>A Rate-Distortion Approach to Index Coding</title><categories>cs.IT math.IT</categories><comments>Substantially extended version. Submitted to IEEE Transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We approach index coding as a special case of rate-distortion with multiple
receivers, each with some side information about the source. Specifically,
using techniques developed for the rate-distortion problem, we provide two
upper bounds and one lower bound on the optimal index coding rate. The upper
bounds involve specific choices of the auxiliary random variables in the best
existing scheme for the rate-distortion problem. The lower bound is based on a
new lower bound for the general rate-distortion problem. The bounds are shown
to coincide for a number of (groupcast) index coding instances, including all
instances for which the number of decoders does not exceed three.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0264</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0264</id><created>2014-02-02</created><authors><author><keyname>Haque</keyname><forenames>Sardar Anisul</forenames></author><author><keyname>Maza</keyname><forenames>Marc Moreno</forenames></author><author><keyname>Xie</keyname><forenames>Ning</forenames></author></authors><title>A Many-core Machine Model for Designing Algorithms with Minimum
  Parallelism Overheads</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a model of multithreaded computation, combining fork-join and
single-instruction-multiple-data parallelisms, with an emphasis on estimating
parallelism overheads of programs written for modern many-core architectures.
We establish a Graham-Brent theorem for this model so as to estimate execution
time of programs running on a given number of streaming multiprocessors. We
evaluate the benefits of our model with four fundamental algorithms from
scientific computing. In each case, our model is used to minimize parallelism
overheads by determining an appropriate value range for a given program
parameter; moreover experimentation confirms the model's prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0273</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0273</id><created>2014-02-02</created><authors><author><keyname>Salam</keyname><forenames>Sazilah</forenames></author><author><keyname>Mohamad</keyname><forenames>Siti Nurul Mahfuzah</forenames></author><author><keyname>Bakar</keyname><forenames>Norasiken</forenames></author><author><keyname>Sui</keyname><forenames>Linda Khoo Mei</forenames></author></authors><title>The Designing of Online Multiple Intelligence Tools for Lecturers at
  Polytechnic</title><categories>cs.CY cs.HC</categories><comments>7 pages, 4 figures, 1 table, International Journal of Soft Computing
  and Software Engineering [JSCSE], Vol. 3, No. 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the designing of Online Multiple Intelligence (MI)
Teaching Tools for Polytechnic lecturers. These teaching tools can assist
lecturers to create their own teaching materials without having any knowledge
of Information Technology (IT) especially in programming. The theory of MI is
used in this paper and this theory postulates that everybody has at least two
or more intelligences. Multiple approaches embedded into a series of activities
via online teaching tools must be implemented in order to achieve effective
teaching and learning in the classroom. The objectives of this paper are to
identify the relationship between the students self-perceived MI and their
academic achievement in Polytechnic, and design online MI tools for teaching at
Polytechnic. This paper also addressed the theoretical framework and MI
teaching activities. The instrument used for this study was Ujian Multiple
Intelligence (UMI). The results showed Polytechnic students have strength in
Interpersonal, Visual-Spatial and Verbal-Linguistic intelligences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0282</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0282</id><created>2014-02-02</created><authors><author><keyname>Zhang</keyname><forenames>Duo</forenames></author><author><keyname>Rubinstein</keyname><forenames>Benjamin I. P.</forenames></author><author><keyname>Gemmell</keyname><forenames>Jim</forenames></author></authors><title>Principled Graph Matching Algorithms for Integrating Multiple Data
  Sources</title><categories>cs.DB cs.LG stat.ML</categories><comments>14 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores combinatorial optimization for problems of max-weight
graph matching on multi-partite graphs, which arise in integrating multiple
data sources. Entity resolution-the data integration problem of performing
noisy joins on structured data-typically proceeds by first hashing each record
into zero or more blocks, scoring pairs of records that are co-blocked for
similarity, and then matching pairs of sufficient similarity. In the most
common case of matching two sources, it is often desirable for the final
matching to be one-to-one (a record may be matched with at most one other);
members of the database and statistical record linkage communities accomplish
such matchings in the final stage by weighted bipartite graph matching on
similarity scores. Such matchings are intuitively appealing: they leverage a
natural global property of many real-world entity stores-that of being nearly
deduped-and are known to provide significant improvements to precision and
recall. Unfortunately unlike the bipartite case, exact max-weight matching on
multi-partite graphs is known to be NP-hard. Our two-fold algorithmic
contributions approximate multi-partite max-weight matching: our first
algorithm borrows optimization techniques common to Bayesian probabilistic
inference; our second is a greedy approximation algorithm. In addition to a
theoretical guarantee on the latter, we present comparisons on a real-world ER
problem from Bing significantly larger than typically found in the literature,
publication data, and on a series of synthetic problems. Our results quantify
significant improvements due to exploiting multiple sources, which are made
possible by global one-to-one constraints linking otherwise independent
matching sub-problems. We also discover that our algorithms are complementary:
one being much more robust under noise, and the other being simple to implement
and very fast to run.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0288</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0288</id><created>2014-02-03</created><authors><author><keyname>Niu</keyname><forenames>Gang</forenames></author><author><keyname>Dai</keyname><forenames>Bo</forenames></author><author><keyname>Plessis</keyname><forenames>Marthinus Christoffel du</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Transductive Learning with Multi-class Volume Approximation</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a hypothesis space, the large volume principle by Vladimir Vapnik
prioritizes equivalence classes according to their volume in the hypothesis
space. The volume approximation has hitherto been successfully applied to
binary learning problems. In this paper, we extend it naturally to a more
general definition which can be applied to several transductive problem
settings, such as multi-class, multi-label and serendipitous learning. Even
though the resultant learning method involves a non-convex optimization
problem, the globally optimal solution is almost surely unique and can be
obtained in O(n^3) time. We theoretically provide stability and error analyses
for the proposed method, and then experimentally show that it is promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0289</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0289</id><created>2014-02-03</created><authors><author><keyname>Janney</keyname><forenames>Pranam</forenames></author><author><keyname>Geers</keyname><forenames>Glenn</forenames></author></authors><title>A Robust Framework for Moving-Object Detection and Vehicular Traffic
  Density Estimation</title><categories>cs.CV cs.RO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent machines require basic information such as moving-object
detection from videos in order to deduce higher-level semantic information. In
this paper, we propose a methodology that uses a texture measure to detect
moving objects in video. The methodology is computationally inexpensive,
requires minimal parameter fine-tuning and also is resilient to noise,
illumination changes, dynamic background and low frame rate. Experimental
results show that performance of the proposed approach is higher than those of
state-of-the-art approaches. We also present a framework for vehicular traffic
density estimation using the foreground object detection technique and present
a comparison between the foreground object detection-based framework and the
classical density state modelling-based framework for vehicular traffic density
estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0292</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0292</id><created>2014-02-03</created><authors><author><keyname>Basili</keyname><forenames>Victor</forenames></author><author><keyname>Heidrich</keyname><forenames>Jens</forenames></author><author><keyname>Lindvall</keyname><forenames>Mikael</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Regardie</keyname><forenames>Myrna</forenames></author><author><keyname>Rombach</keyname><forenames>Dieter</forenames></author><author><keyname>Seaman</keyname><forenames>Carolyn</forenames></author><author><keyname>Trendowicz</keyname><forenames>Adam</forenames></author></authors><title>GQM+Strategies: A Comprehensive Methodology for Aligning Business
  Strategies with Software Measurement</title><categories>cs.SE</categories><comments>14 pages</comments><journal-ref>Proceedings of the DASMA Software Metric Congress (MetriKon 2007):
  Magdeburger Schriften zum Empirischen Software Engineering, pages 253-266,
  Kaiserslautern, Germany, November 15-16 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In software-intensive organizations, an organizational management system will
not guarantee organizational success unless the business strategy can be
translated into a set of operational software goals. The Goal Question Metric
(GQM) approach has proven itself useful in a variety of industrial settings to
support quantitative software project management. However, it does not address
linking software measurement goals to higher-level goals of the organization in
which the software is being developed. This linkage is important, as it helps
to justify software measurement efforts and allows measurement data to
contribute to higher-level decisions. In this paper, we propose a
GQM+Strategies(R) measurement approach that builds on the GQM approach to plan
and implement software measurement. GQM+Strategies(R) provides mechanisms for
explicitly linking software measurement goals to higher-level goals for the
software organization, and further to goals and strategies at the level of the
entire business. The proposed method is illustrated in the context of an
example application of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0294</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0294</id><created>2014-02-03</created><authors><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Lamersdorf</keyname><forenames>Ansgar</forenames></author></authors><title>Systematic Task Allocation Evaluation in Distributed Software
  Development</title><categories>cs.SE</categories><comments>10 pages. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F978-3-642-05290-3_34</comments><journal-ref>On the Move to Meaningful Internet Systems: OTM 2009 Workshops,
  volume 5872 of Lecture Notes in Computer Science, pages 228-237. Springer
  Berlin Heidelberg, 2009</journal-ref><doi>10.1007/978-3-642-05290-3_34</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systematic task allocation to different development sites in global software
de- velopment projects can open business and engineering perspectives and help
to reduce risks and problems inherent in distributed development. Relying only
on a single evaluation criterion such as development cost when distributing
tasks to development sites has shown to be very risky and often does not lead
to successful solutions in the long run. Task allocation in global software
projects is challenging due to a multitude of impact factors and constraints.
Systematic allocation decisions require the ability to evaluate and compare
task allocation alternatives and to effectively establish customized task
allocation practices in an organization. In this article, we present a
customizable process for task allocation evaluation that is based on results
from a systematic interview study with practitioners. In this process, the
relevant criteria for evaluating task allocation alternatives are derived by
applying principles from goal-oriented measurement. In addition, the
customization of the process is demonstrated, related work and limitations are
sketched, and an outlook on future work is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0295</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0295</id><created>2014-02-03</created><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Performance Analysis and Optimization for Interference Alignment over
  MIMO Interference Channels with Limited Feedback</title><categories>cs.IT math.IT</categories><comments>11 pages, 6 figures, 1 table. IEEE Transactions on Signal Processing,
  2014</comments><doi>10.1109/TSP.2014.2304926</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of interference alignment (IA) over
MIMO interference channels with limited channel state information (CSI)
feedback based on quantization codebooks. Due to limited feedback and hence
imperfect IA, there are residual interferences across different links and
different data streams. As a result, the performance of IA is greatly related
to the CSI accuracy (namely number of feedback bits) and the number of data
streams (namely transmission mode). In order to improve the performance of IA,
it makes sense to optimize the system parameters according to the channel
conditions. Motivated by this, we first give a quantitative performance
analysis for IA under limited feedback, and derive a closed-form expression for
the average transmission rate in terms of feedback bits and transmission mode.
By maximizing the average transmission rate, we obtain an adaptive feedback
allocation scheme, as well as a dynamic mode selection scheme. Furthermore,
through asymptotic analysis, we obtain several clear insights on the system
performance, and provide some guidelines on the system design. Finally,
simulation results validate our theoretical claims, and show that obvious
performance gain can be obtained by adjusting feedback bits dynamically or
selecting transmission mode adaptively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0299</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0299</id><created>2014-02-03</created><updated>2015-02-07</updated><authors><author><keyname>&#xc9;sik</keyname><forenames>Zolt&#xe1;n</forenames></author><author><keyname>Rondogiannis</keyname><forenames>Panos</forenames></author></authors><title>A Fixed Point Theorem for Non-Monotonic Functions</title><categories>cs.LO math.LO</categories><comments>34 pages. Accepted in: Theoretical Computer Science (to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fixed point theorem for a class of (potentially) non-monotonic
functions over specially structured complete lattices. The theorem has as a
special case the Knaster-Tarski fixed point theorem when restricted to the case
of monotonic functions and Kleene's theorem when the functions are additionally
continuous. From the practical side, the theorem has direct applications in the
semantics of negation in logic programming. In particular, it leads to a more
direct and elegant proof of the least fixed point result of [Rondogiannis and
W.W.Wadge, ACM TOCL 6(2): 441-467 (2005)]. Moreover, the theorem appears to
have potential for possible applications outside the logic programming domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0313</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0313</id><created>2014-02-03</created><authors><author><keyname>Heindlmaier</keyname><forenames>Michael</forenames></author><author><keyname>Iscan</keyname><forenames>Onurcan</forenames></author></authors><title>Rate-Distortion Properties of Single-Layer Quantize-and-Forward for
  Two-Way Relaying</title><categories>cs.IT math.IT</categories><comments>Extended version of submission to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Quantize &amp; Forward (QF) scheme for two-way relaying is studied with a
focus on its rate-distortion properties. A sum rate maximization problem is
formulated and the associated quantizer optimization problem is investigated.
An algorithm to approximately solve the problem is proposed. Under certain
cases scalar quantizers maximize the sum rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0327</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0327</id><created>2014-02-03</created><authors><author><keyname>Kumar</keyname><forenames>Suman</forenames></author><author><keyname>Kalyani</keyname><forenames>Sheetal</forenames></author><author><keyname>Giridhar</keyname><forenames>K.</forenames></author></authors><title>Spectrum Allocation for ICIC Based Picocell</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we analytically study the impact of spectrum allocation scheme
in picocells on the coverage probability (CP) of the Pico User (PU), when the
macro base stations (MBSs) employ either fractional frequency reuse (FFR) or
soft frequency reuse (SFR). Assuming a fixed size for the picocell, the CP
expression is derived for a PU present in either a FFR or SFR based deployment,
and when the PU uses either the centre or the edge frequency resources. Based
on these expressions, we propose two possible frequency allocation schemes for
the picocell when FFR is employed by the macrocell. The CP and the average rate
expressions for both these schemes are derived, and it is shown that these
schemes outperform the conventional scheme where no inter-cell interference
coordination (ICIC) is assumed. The impact of both schemes on the macro-user
performance is also analysed. When SFR is used by the MBS, it is shown that the
CP is maximized when the PU uses the same frequency resources as used by the
centre region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0344</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0344</id><created>2014-02-03</created><authors><author><keyname>Arnault</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Formes quadratiques de discriminants embo\^it\'es</title><categories>math.NT cs.CR cs.DM</categories><comments>7 pages. In French</comments><msc-class>68R99, 11Y05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quadratic forms with embedded discriminants. Integral binary quadratic forms
have multiple applications, for example in factorization or cryptography. The
Nice family of cryptographic systems makes use of quadratic forms with
different discriminants $\pm p$, and $\pm pq^2$ where $p$, $q$ are large
primes. This paper shows the precise links between forms with $D$ discriminant
and forms with $Df^2$ discriminant, which are crucial in the analysis of the
systems Nice and theirs attacks. We also introduce the notion of
semi-equivalence of binary quadratic forms, and give some characterizations of
semi-equivalent forms, which are useful in the analysis of these attacks.
  -----
  Les formes quadratiques binaires fournissent un moyen explicite pour
manipuler des id\'eaux de corps quadratiques, et leurs applications pratiques
sont multiples. De nombreux algorithmes de factorisation les utilisent. Elle
sont aussi utilis\'ees en cryptographie, en particulier pour les syst\`emes
Nice. Les syst\`emes de chiffrement Nice utilisent des formes quadratiques de
discriminants $\pm p$ et $\pm pq^2$ o\`u $p$ et $q$ sont des nombres premiers.
Cet article pr\'ecise les liens entre les formes de discriminant $D$ et celles
de discriminant $Df^2$, ce qui est essentiel pour l'analyse de Nice et de ses
attaques. Il introduit aussi la notion de formes quadratiques
semi-\'equivalentes et en explicite plusieurs caract\'erisations, utiles pour
l'analyse de ces attaques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0349</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0349</id><created>2014-02-03</created><updated>2015-04-14</updated><authors><author><keyname>Cohen</keyname><forenames>G&#xe9;rard</forenames></author><author><keyname>Fachini</keyname><forenames>Emanuela</forenames></author><author><keyname>K&#xf6;rner</keyname><forenames>J&#xe1;nos</forenames></author></authors><title>Zero-error capacity of binary channels with memory</title><categories>math.CO cs.IT math.IT</categories><comments>10 pages. This paper is the revised version of our previous paper
  having the same title, published on ArXiV on February 3, 2014. We complete
  Theorem 2 of the previous version by showing here that our previous
  construction is asymptotically optimal. This proves that the isometric
  triangles yield different capacities. The new manuscript differs from the old
  one by the addition of one more page</comments><msc-class>05D05, 94A24</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We begin a systematic study of the problem of the zero--error capacity of
noisy binary channels with memory and solve some of the non--trivial cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0362</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0362</id><created>2014-02-03</created><updated>2014-05-27</updated><authors><author><keyname>Mathieu</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Louveaux</keyname><forenames>Quentin</forenames></author><author><keyname>Ernst</keyname><forenames>Damien</forenames></author><author><keyname>Corn&#xe9;lusse</keyname><forenames>Bertrand</forenames></author></authors><title>A quantitative analysis of the effect of flexible loads on reserve
  markets</title><categories>cs.GT cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and analyze a day-ahead reserve market model that handles bids
from flexible loads. This pool market model takes into account the fact that a
load modulation in one direction must usually be compensated later by a
modulation of the same magnitude in the opposite direction. Our analysis takes
into account the gaming possibilities of producers and retailers, controlling
load flexibility, in the day-ahead energy and reserve markets, and in imbalance
settlement. This analysis is carried out by an agent-based approach where, for
every round, each actor uses linear programs to maximize its profit according
to forecasts of the prices. The procurement of a reserve is assumed to be
determined, for each period, as a fixed percentage of the total consumption
cleared in the energy market for the same period. The results show that the
provision of reserves by flexible loads has a negligible impact on the energy
market prices but markedly decreases the cost of reserve procurement. However,
as the rate of flexible loads increases, the system operator has to rely more
and more on non-contracted reserves, which may cancel out the benefits made in
the procurement of reserves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0375</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0375</id><created>2014-02-03</created><updated>2015-09-27</updated><authors><author><keyname>S&#x142;omczy&#x144;ski</keyname><forenames>Wojciech</forenames></author><author><keyname>Szymusiak</keyname><forenames>Anna</forenames></author></authors><title>Highly symmetric POVMs and their informational power</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>40 pages, 3 figures</comments><msc-class>Primary 81P15, 94A17, Secondary 81R05, 94A40, 58D19, 58K05, 52B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the dependence of the Shannon entropy of normalized finite rank-1
POVMs on the choice of the input state, looking for the states that minimize
this quantity. To distinguish the class of measurements where the problem can
be solved analytically, we introduce the notion of highly symmetric POVMs and
classify them in dimension two (for qubits). In this case we prove that the
entropy is minimal, and hence the relative entropy (informational power) is
maximal, if and only if the input state is orthogonal to one of the states
constituting a POVM. The method used in the proof, employing the Michel theory
of critical points for group action, the Hermite interpolation and the
structure of invariant polynomials for unitary-antiunitary groups, can also be
applied in higher dimensions and for other entropy-like functions. The links
between entropy minimization and entropic uncertainty relations, the Wehrl
entropy and the quantum dynamical entropy are described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0391</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0391</id><created>2014-02-03</created><updated>2014-02-03</updated><authors><author><keyname>Gao</keyname><forenames>Hui</forenames></author><author><keyname>Lv</keyname><forenames>Tiejun</forenames></author><author><keyname>Fang</keyname><forenames>Di</forenames></author><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Limited Feedback-Based Interference Alignment for Interfering
  Multi-Access Channels</title><categories>cs.IT math.IT</categories><comments>4 pages, 4 figures, to appear in IEEE Communications Letters</comments><journal-ref>IEEE Communications Letters, Vol. 18, No. 4, pp. 540 - 543, April
  2014</journal-ref><doi>10.1109/LCOMM.2014.021214.132762</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A limited feedback-based interference alignment (IA) scheme is proposed for
the interfering multi-access channel (IMAC). By employing a novel
performance-oriented quantization strategy, the proposed scheme is able to
achieve the minimum overall residual inter-cell interference (ICI) with the
optimized transceivers under limited feedback. Consequently, the scheme
outperforms the existing counterparts in terms of system throughput. In
addition, the proposed scheme can be implemented with flexible antenna
configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0400</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0400</id><created>2014-02-03</created><authors><author><keyname>Lee</keyname><forenames>SeoungKyou</forenames></author><author><keyname>Becker</keyname><forenames>Aaron</forenames></author><author><keyname>Fekete</keyname><forenames>S&#xe1;ndor P.</forenames></author><author><keyname>Kr&#xf6;ller</keyname><forenames>Alexander</forenames></author><author><keyname>McLurkin</keyname><forenames>James</forenames></author></authors><title>Exploration via Structured Triangulation by a Multi-Robot System with
  Bearing-Only Low-Resolution Sensors</title><categories>cs.RO cs.CG</categories><comments>8 pages, 11 figures. To appear in ICRA 2014</comments><acm-class>I.2.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a distributed approach for exploring and triangulating an
unknown region using a multi- robot system. The objective is to produce a
covering of an unknown workspace by a fixed number of robots such that the
covered region is maximized, solving the Maximum Area Triangulation Problem
(MATP). The resulting triangulation is a physical data structure that is a
compact representation of the workspace; it contains distributed knowledge of
each triangle, adjacent triangles, and the dual graph of the workspace.
Algorithms can store information in this physical data structure, such as a
routing table for robot navigation Our algorithm builds a triangulation in a
closed environment, starting from a single location. It provides coverage with
a breadth-first search pattern and completeness guarantees. We show the
computational and communication requirements to build and maintain the
triangulation and its dual graph are small. Finally, we present a physical
navigation algorithm that uses the dual graph, and show that the resulting path
lengths are within a constant factor of the shortest-path Euclidean distance.
We validate our theoretical results with experiments on triangulating a region
with a system of low-cost robots. Analysis of the resulting quality of the
triangulation shows that most of the triangles are of high quality, and cover a
large area. Implementation of the triangulation, dual graph, and navigation all
use communication messages of fixed size, and are a practical solution for
large populations of low-cost robots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0402</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0402</id><created>2014-02-03</created><updated>2015-08-21</updated><authors><author><keyname>Dibbelt</keyname><forenames>Julian</forenames></author><author><keyname>Strasser</keyname><forenames>Ben</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author></authors><title>Customizable Contraction Hierarchies</title><categories>cs.DS cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of quickly computing shortest paths in weighted
graphs given auxiliary data derived in an expensive preprocessing phase. By
adding a fast weight-customization phase, we extend Contraction Hierarchies by
Geisberger et al to support the three-phase workflow introduced by Delling et
al. Our Customizable Contraction Hierarchies use nested dissection orders as
suggested by Bauer et al. We provide an in-depth experimental analysis on large
road and game maps that clearly shows that Customizable Contraction Hierarchies
are a very practicable solution in scenarios where edge weights often change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0410</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0410</id><created>2014-02-03</created><authors><author><keyname>Kuniavsky</keyname><forenames>Sergey</forenames></author></authors><title>Minimum Price in Search Model</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the effects of a low bound price. To do so, a popular
and empirically proven model (Stahl (89') \cite{Stahl89}) is used. The model is
extended to include an exogenously given bound on prices sellers can offer,
excluding prices below such bound. The finding are rather surprising - when the
bound is set sufficiently high expected price offered (EPO) by sellers drops
significantly. The result seem to be robust in the parameters of the model, and
driven by the information provided to consumers by such legislation step: when
the limitation is set at sufficiently high levels all consumers anticipate the
bound price, and searchers reject any price above it. As a result sellers offer
the bound price as a pure strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0412</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0412</id><created>2014-02-03</created><updated>2014-02-05</updated><authors><author><keyname>Steiner</keyname><forenames>Thomas</forenames></author></authors><title>Bots vs. Wikipedians, Anons vs. Logged-Ins</title><categories>cs.DL cs.CY cs.SI</categories><comments>Poster at the Web Science Track of the 23rd International World Wide
  Web Conference (WWW2014), Seoul, Korea</comments><doi>10.1145/2567948.2576948</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wikipedia is a global crowdsourced encyclopedia that at time of writing is
available in 287 languages. Wikidata is a likewise global crowdsourced
knowledge base that provides shared facts to be used by Wikipedias. In the
context of this research, we have developed an application and an underlying
Application Programming Interface (API) capable of monitoring realtime edit
activity of all language versions of Wikipedia and Wikidata. This application
allows us to easily analyze edits in order to answer questions such as &quot;Bots
vs. Wikipedians, who edits more?&quot;, &quot;Which is the most anonymously edited
Wikipedia?&quot;, or &quot;Who are the bots and what do they edit?&quot;. To the best of our
knowledge, this is the first time such an analysis could be done in realtime
for Wikidata and for really all Wikipedias--large and small. Our application is
available publicly online at the URL http://wikipedia-edits.herokuapp.com/, its
code has been open-sourced under the Apache 2.0 license.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0420</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0420</id><created>2014-02-03</created><authors><author><keyname>Bertini</keyname><forenames>Francesco</forenames></author><author><keyname>Mas</keyname><forenames>Lorenzo Dal</forenames></author><author><keyname>Vassio</keyname><forenames>Luca</forenames></author><author><keyname>Ampellio</keyname><forenames>Enrico</forenames></author></authors><title>Multidiscipinary Optimization For Gas Turbines Design</title><categories>math.OC cs.NE</categories><comments>12 pages, 6 figures. Presented at the XXII Italian Association of
  Aeronautics and Astronautics Conference (2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art aeronautic Low Pressure gas Turbines (LPTs) are already
characterized by high quality standards, thus they offer very narrow margins of
improvement. Typical design process starts with a Concept Design (CD) phase,
defined using mean-line 1D and other low-order tools, and evolves through a
Preliminary Design (PD) phase, which allows the geometric definition in
details. In this framework, multidisciplinary optimization is the only way to
properly handle the complicated peculiarities of the design. The authors
present different strategies and algorithms that have been implemented
exploiting the PD phase as a real-like design benchmark to illustrate results.
The purpose of this work is to describe the optimization techniques, their
settings and how to implement them effectively in a multidisciplinary
environment. Starting from a basic gradient method and a semi-random second
order method, the authors have introduced an Artificial Bee Colony-like
optimizer, a multi-objective Genetic Diversity Evolutionary Algorithm [1] and a
multi-objective response surface approach based on Artificial Neural Network,
parallelizing and customizing them for the gas turbine study. Moreover, speedup
and improvement arrangements are embedded in different hybrid strategies with
the aim at finding the best solutions for different kind of problems that arise
in this field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0422</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0422</id><created>2014-02-03</created><authors><author><keyname>Lancichinetti</keyname><forenames>Andrea</forenames></author><author><keyname>Sirer</keyname><forenames>M. Irmak</forenames></author><author><keyname>Wang</keyname><forenames>Jane X.</forenames></author><author><keyname>Acuna</keyname><forenames>Daniel</forenames></author><author><keyname>K&#xf6;rding</keyname><forenames>Konrad</forenames></author><author><keyname>Amaral</keyname><forenames>Lu&#xed;s A. Nunes</forenames></author></authors><title>A high-reproducibility and high-accuracy method for automated topic
  classification</title><categories>stat.ML cs.IR cs.LG physics.soc-ph</categories><comments>23 pages, 24 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much of human knowledge sits in large databases of unstructured text.
Leveraging this knowledge requires algorithms that extract and record metadata
on unstructured text documents. Assigning topics to documents will enable
intelligent search, statistical characterization, and meaningful
classification. Latent Dirichlet allocation (LDA) is the state-of-the-art in
topic classification. Here, we perform a systematic theoretical and numerical
analysis that demonstrates that current optimization techniques for LDA often
yield results which are not accurate in inferring the most suitable model
parameters. Adapting approaches for community detection in networks, we propose
a new algorithm which displays high-reproducibility and high-accuracy, and also
has high computational efficiency. We apply it to a large set of documents in
the English Wikipedia and reveal its hierarchical structure. Our algorithm
promises to make &quot;big data&quot; text analysis systems more reliable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0423</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0423</id><created>2014-02-03</created><authors><author><keyname>Sultanik</keyname><forenames>Evan A.</forenames></author></authors><title>A Bound on the Expected Optimality of Random Feasible Solutions to
  Combinatorial Optimization Problems</title><categories>cs.DS math.CO</categories><msc-class>60C05, 68W25, 68W40</msc-class><acm-class>G.2.1; G.3; F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates and bounds the expected solution quality of
combinatorial optimization problems when feasible solutions are chosen at
random. Loose general bounds are discovered, as well as families of
combinatorial optimization problems for which random feasible solutions are
expected to be a constant factor of optimal. One implication of this result is
that, for graphical problems, if the average edge weight in a feasible solution
is sufficiently small, then any randomly chosen feasible solution to the
problem will be a constant factor of optimal. For example, under certain
well-defined circumstances, the expected constant of approximation of a
randomly chosen feasible solution to the Steiner network problem is bounded
above by 3. Empirical analysis supports these bounds and actually suggest that
they might be tightened.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0429</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0429</id><created>2014-02-03</created><updated>2015-12-30</updated><authors><author><keyname>Ali</keyname><forenames>S. Tabrez</forenames></author></authors><title>Defmod - Parallel multiphysics finite element code for modeling crustal
  deformation during the earthquake/rifting cycle</title><categories>physics.geo-ph cs.CE physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we present Defmod, an open source, fully unstructured, two
or three dimensional, parallel finite element code for modeling crustal
deformation over time scales ranging from milliseconds to thousands of years.
Unlike existing public domain numerical codes, Defmod can simulate deformation
due to all major processes that make up the earthquake/rifting cycle, in
non-homogeneous media. Specifically, it can be used to model deformation due to
dynamic and quasistatic processes such as co-seismic slip or dike intrusion(s),
poroelastic rebound due to fluid flow and post-seismic or post-rifting
viscoelastic relaxation. It can also be used to model deformation due to
processes such as post-glacial rebound, hydrological (un)loading, injection
and/or withdrawal of fluids from subsurface reservoirs etc. Defmod is written
in Fortran 95 and uses PETSc's parallel sparse data structures and implicit
solvers. Problems can be solved using (stabilized) linear triangular,
quadrilateral, tetrahedral or hexahedral elements on shared or distributed
memory machines with hundreds or even thousands of processor cores. In the
current version of the code, prescribed loading is supported. Results are
written in ASCII VTK format for easy visualization. The source code is released
under the terms of GNU General Public License (v3.0) and is freely available
from https://bitbucket.org/stali/defmod/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0452</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0452</id><created>2014-02-03</created><authors><author><keyname>Mitra</keyname><forenames>Rangeet</forenames></author><author><keyname>Mishra</keyname><forenames>Amit Kumar</forenames></author><author><keyname>Choubisa</keyname><forenames>Tarun</forenames></author></authors><title>A Lower Bound for the Variance of Estimators for Nakagami m Distribution</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, we have proposed a maximum likelihood iterative algorithm for
estimation of the parameters of the Nakagami-m distribution. This technique
performs better than state of art estimation techniques for this distribution.
This could be of particular use in low data or block based estimation problems.
In these scenarios, the estimator should be able to give accurate estimates in
the mean square sense with less amounts of data. Also, the estimates should
improve with the increase in number of blocks received. In this paper, we see
through our simulations, that our proposal is well designed for such
requirements. Further, it is well known in the literature that an efficient
estimator does not exist for Nakagami-m distribution. In this paper, we derive
a theoretical expression for the variance of our proposed estimator. We find
that this expression clearly fits the experimental curve for the variance of
the proposed estimator. This expression is pretty close to the cramer-rao lower
bound(CRLB).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0453</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0453</id><created>2014-02-03</created><updated>2015-06-04</updated><authors><author><keyname>Qian</keyname><forenames>Qi</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Zhu</keyname><forenames>Shenghuo</forenames></author><author><keyname>Lin</keyname><forenames>Yuanqing</forenames></author></authors><title>Fine-Grained Visual Categorization via Multi-stage Metric Learning</title><categories>cs.CV cs.LG stat.ML</categories><comments>in CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fine-grained visual categorization (FGVC) is to categorize objects into
subordinate classes instead of basic classes. One major challenge in FGVC is
the co-occurrence of two issues: 1) many subordinate classes are highly
correlated and are difficult to distinguish, and 2) there exists the large
intra-class variation (e.g., due to object pose). This paper proposes to
explicitly address the above two issues via distance metric learning (DML). DML
addresses the first issue by learning an embedding so that data points from the
same class will be pulled together while those from different classes should be
pushed apart from each other; and it addresses the second issue by allowing the
flexibility that only a portion of the neighbors (not all data points) from the
same class need to be pulled together. However, feature representation of an
image is often high dimensional, and DML is known to have difficulty in dealing
with high dimensional feature vectors since it would require $\mathcal{O}(d^2)$
for storage and $\mathcal{O}(d^3)$ for optimization. To this end, we proposed a
multi-stage metric learning framework that divides the large-scale high
dimensional learning problem to a series of simple subproblems, achieving
$\mathcal{O}(d)$ computational complexity. The empirical study with FVGC
benchmark datasets verifies that our method is both effective and efficient
compared to the state-of-the-art FGVC approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0454</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0454</id><created>2014-02-03</created><authors><author><keyname>B&#xe9;n&#xe9;zit</keyname><forenames>F.</forenames></author><author><keyname>Elayoubi</keyname><forenames>S.</forenames></author><author><keyname>Indre</keyname><forenames>R-M.</forenames></author><author><keyname>Simonian</keyname><forenames>A.</forenames></author></authors><title>Modelling Load Balancing and Carrier Aggregation in Mobile Networks</title><categories>cs.NI cs.PF</categories><comments>8 pages, 6 figures, submitted to WiOpt2014</comments><msc-class>68M10, 68M20, 60K25, 60K30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the performance of multicarrier mobile networks.
Specifically, we analyze the flow-level performance of two inter-carrier load
balancing schemes and the gain engendered by Carrier Aggregation (CA). CA is
one of the most important features of HSPA+ and LTE-A networks; it allows
devices to be served simultaneously by several carriers. We propose two load
balancing schemes, namely Join the Fastest Queue (JFQ) and Volume Balancing
(VB), that allow the traffic of CA and non-CA users to be distributed over the
aggregated carriers. We then evaluate the performance of these schemes by means
of analytical modeling. We show that the proposed schemes achieve quasi-ideal
load balancing. We also investigate the impact of mixing traffic of CA and
non-CA users in the same cell and show that performance is practically
insensitive to the traffic mix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0459</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0459</id><created>2014-02-03</created><authors><author><keyname>Duan</keyname><forenames>Hubert Haoyang</forenames></author></authors><title>Applying Supervised Learning Algorithms and a New Feature Selection
  Method to Predict Coronary Artery Disease</title><categories>cs.LG stat.ML</categories><comments>This is a Master of Science in Mathematics thesis under the
  supervision of Dr. Vladimir Pestov and Dr. George Wells submitted on January
  31, 2014 at the University of Ottawa; 102 pages and 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From a fresh data science perspective, this thesis discusses the prediction
of coronary artery disease based on genetic variations at the DNA base pair
level, called Single-Nucleotide Polymorphisms (SNPs), collected from the
Ontario Heart Genomics Study (OHGS).
  First, the thesis explains two commonly used supervised learning algorithms,
the k-Nearest Neighbour (k-NN) and Random Forest classifiers, and includes a
complete proof that the k-NN classifier is universally consistent in any finite
dimensional normed vector space. Second, the thesis introduces two
dimensionality reduction steps, Random Projections, a known feature extraction
technique based on the Johnson-Lindenstrauss lemma, and a new method termed
Mass Transportation Distance (MTD) Feature Selection for discrete domains.
Then, this thesis compares the performance of Random Projections with the k-NN
classifier against MTD Feature Selection and Random Forest, for predicting
artery disease based on accuracy, the F-Measure, and area under the Receiver
Operating Characteristic (ROC) curve.
  The comparative results demonstrate that MTD Feature Selection with Random
Forest is vastly superior to Random Projections and k-NN. The Random Forest
classifier is able to obtain an accuracy of 0.6660 and an area under the ROC
curve of 0.8562 on the OHGS genetic dataset, when 3335 SNPs are selected by MTD
Feature Selection for classification. This area is considerably better than the
previous high score of 0.608 obtained by Davies et al. in 2010 on the same
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0471</identifier>
 <datestamp>2014-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0471</id><created>2014-02-03</created><authors><author><keyname>Auger</keyname><forenames>David</forenames><affiliation>MAGMAT</affiliation></author><author><keyname>COUCHENEY</keyname><forenames>Pierre</forenames><affiliation>PRISM</affiliation></author><author><keyname>Strozecki</keyname><forenames>Yann</forenames><affiliation>PRISM</affiliation></author></authors><title>Finding Optimal Strategies of Almost Acyclic Simple Stochatic Games</title><categories>cs.CC cs.DS cs.GT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimal value computation for turned-based stochastic games with
reachability objectives, also known as simple stochastic games, is one of the
few problems in $NP \cap coNP$ which are not known to be in $P$. However, there
are some cases where these games can be easily solved, as for instance when the
underlying graph is acyclic. In this work, we try to extend this tractability
to several classes of games that can be thought as &quot;almost&quot; acyclic. We give
some fixed-parameter tractable or polynomial algorithms in terms of different
parameters such as the number of cycles or the size of the minimal feedback
vertex set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0474</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0474</id><created>2014-02-03</created><authors><author><keyname>Amblard</keyname><forenames>Maxime</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Retor&#xe9;</keyname><forenames>Christian</forenames><affiliation>LaBRI</affiliation></author></authors><title>Normalization and sub-formula property for Lambek with product and PCMLL
  -- Partially Commutative Multiplicative Linear Logic</title><categories>cs.LO cs.FL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper establishes the normalisation of natural deduction or lambda
calculus formulation of Intuitionistic Non Commutative Logic --- which involves
both commutative and non commutative connectives. This calculus first
introduced by de Groote and as opposed to the classical version by Abrusci and
Ruet admits a full entropy which allow order to be relaxed into any suborder.
Our result also includes, as a special case, the normalisation of natural
deduction the Lambek calculus with product, which is unsurprising but yet
unproved. Regarding Intuitionistic Non Commutative Logic with full entropy does
not have up to now a proof net syntax, and that for linguistic applications,
sequent calculi which are only more or less equivalent to natural deduction,
are not convenient because they lack the standard Curry-Howard isomorphism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0480</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0480</id><created>2014-02-03</created><updated>2015-01-22</updated><authors><author><keyname>Kingma</keyname><forenames>Diederik P.</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Efficient Gradient-Based Inference through Transformations between Bayes
  Nets and Neural Nets</title><categories>cs.LG stat.ML</categories><journal-ref>Proceedings of The 31st International Conference on Machine
  Learning, pp. 1782-1790, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical Bayesian networks and neural networks with stochastic hidden
units are commonly perceived as two separate types of models. We show that
either of these types of models can often be transformed into an instance of
the other, by switching between centered and differentiable non-centered
parameterizations of the latent variables. The choice of parameterization
greatly influences the efficiency of gradient-based posterior inference; we
show that they are often complementary to eachother, we clarify when each
parameterization is preferred and show how inference can be made robust. In the
non-centered form, a simple Monte Carlo estimator of the marginal likelihood
can be used for learning the parameters. Theoretical results are supported by
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0485</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0485</id><created>2014-02-03</created><updated>2015-12-09</updated><authors><author><keyname>Rahman</keyname><forenames>Mustazee</forenames></author><author><keyname>Virag</keyname><forenames>Balint</forenames></author></authors><title>Local algorithms for independent sets are half-optimal</title><categories>math.PR cs.DC math.CO</categories><comments>Exposition has been clarified in the new version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the largest density of factor of i.i.d. independent sets on the
d-regular tree is asymptotically at most (log d)/d as d tends to infinity. This
matches the lower bound given by previous constructions. It follows that the
largest independent sets given by local algorithms on random d-regular graphs
have the same asymptotic density. In contrast, the density of the largest
independent sets on these graphs is asymptotically 2(log d)/d. We also prove
analogous results for Poisson-Galton-Watson trees, which yield bounds for local
algorithms on sparse Erdos-Renyi graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0491</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0491</id><created>2014-02-03</created><updated>2014-06-24</updated><authors><author><keyname>Chattopadhyay</keyname><forenames>Anupam</forenames></author><author><keyname>Chandak</keyname><forenames>Chander</forenames></author><author><keyname>Chakraborty</keyname><forenames>Kaushik</forenames></author></authors><title>Complexity Analysis of Reversible Logic Synthesis</title><categories>cs.ET</categories><comments>14 pages, 6 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible logic circuit is a necessary construction for achieving ultra low
power dissipation as well as for prominent post-CMOS computing technologies
such as Quantum computing. Consequently automatic synthesis of a Boolean
function using elementary reversible logic gates has received significant
research attention in recent times, creating the domain of reversible logic
synthesis. In this paper, we study the complexity of reversible logic
synthesis. The problem is separately studied for bounded-ancilla and
ancilla-free optimal synthesis approaches. The computational complexity for
both cases are linked to known/presumed hard problems. Finally, experiments are
performed with a shortest-path based reversible logic synthesis approach and a
(0-1) ILP-based formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0501</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0501</id><created>2014-02-03</created><authors><author><keyname>Hartmann</keyname><forenames>Alexander K.</forenames></author></authors><title>Large-deviation properties of resilience of transportation networks</title><categories>physics.soc-ph cs.SI physics.comp-ph</categories><comments>10 pages, 10 figures</comments><doi>10.1140/epjb/e2014-50078-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributions of the resilience of transport networks are studied
numerically, in particular the large-deviation tails. Thus, not only typical
quantities like average or variance but the distributions over the (almost)
full support can be studied. For a proof of principle, a simple transport model
based on the edge-betweenness and three abstract yet widely studied random
network ensembles are considered here: Erdoes-Renyi random networks with finite
connectivity, small world networks and spatial networks embedded in a
two-dimensional plane. Using specific numerical large-deviation techniques,
probability densities as small as 10^(-80) are obtained here. This allows one
to study typical but also the most and the least resilient networks. The
resulting distributions fulfill the mathematical large-deviation principle,
i.e., can be well described by rate functions in the thermodynamic limit. The
analysis of the limiting rate function reveals that the resilience follows an
exponential distribution almost everywhere. An analysis of the structure of the
network shows that the most-resilient networks can be obtained, as a rule of
thumb, by minimizing the diameter of a network. Also, trivially, by including
more links a network can typically be made more resilient. On the other hand,
the least-resilient networks are very rare and characterized by one (or few)
small core(s) to which all other nodes are connected. In total, the spatial
network ensemble turns out to be most suitable for obtaining and studying
resilience of real mostly finite-dimensional networks. Studying this ensemble
in combination with the presented large-deviation approach for more realistic,
in particular dynamic transport networks appears to be very promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0521</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0521</id><created>2014-02-03</created><updated>2015-06-01</updated><authors><author><keyname>Hakami</keyname><forenames>Vesal</forenames></author><author><keyname>Dehghan</keyname><forenames>Mehdi</forenames></author></authors><title>Cognitive Forwarding Control in Wireless Ad-Hoc Networks with Slow
  Fading Channels</title><categories>cs.NI</categories><comments>29 pages, 9 figures, 2 tables. Wireless Networks (Springer), Vol. XX,
  No. XX, 2015</comments><doi>10.1007/s11276-015-0919-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a decentralized stochastic control solution for the broadcast
message dissemination problem in wireless ad hoc networks with slow fading
channels. We formulate the control problem as a dynamic robust game which is
well justified by two key observations; first, the shared nature of the
wireless medium which inevitably cross-couples the nodes' forwarding decisions,
thus binding them together as strategic players; second, the stochastic
dynamics associated with the link qualities which renders the transmission
costs noisy, thus motivating a robust formulation. Given the non stationarity
induced by the fading process, an online solution for the formulated game would
then require an adaptive procedure capable of both convergence to and tracking
strategic equilibria as the environment changes. To this end, we deploy the
strategic and non stationary learning algorithm of regret tracking, the
temporally adaptive variant of the celebrated regret matching algorithm, to
guarantee the emergence and active tracking of the correlated equilibria in the
dynamic robust forwarding game. We also make provision for exploiting the
channel state information, when available, to enhance the convergence speed of
the learning algorithm by conducting an accurate transmission cost estimation.
This cost estimate can basically serve as a model which spares the algorithm
from extra action exploration, thus rendering the learning process more sample
efficient. Simulation results reveal that our proposed solution excels in terms
of both the number of transmissions and load distribution while also
maintaining near perfect delivery ratio, especially in dense crowded
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0525</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0525</id><created>2014-02-03</created><authors><author><keyname>Mehmetoglu</keyname><forenames>Mustafa</forenames></author><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Rose</keyname><forenames>Kenneth</forenames></author></authors><title>A Deterministic Annealing Approach to Witsenhausen's Counterexample</title><categories>cs.IT cs.SY math.IT math.OC</categories><comments>submitted to ISIT'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a numerical method, based on information theoretic ideas,
to a class of distributed control problems. As a particular test case, the
well-known and numerically &quot;over-mined&quot; problem of decentralized control and
implicit communication, commonly referred to as Witsenhausen's counterexample,
is considered. The method provides a small improvement over the best numerical
result so far for this benchmark problem. The key idea is to randomize the
zero-delay mappings. which become &quot;soft&quot;, probabilistic mappings to be
optimized in a deterministic annealing process, by incorporating a Shannon
entropy constraint in the problem formulation. The entropy of the mapping is
controlled and gradually lowered to zero to obtain deterministic mappings,
while avoiding poor local minima. Proposed method obtains new mappings that
shed light on the structure of the optimal solution, as well as achieving a
small improvement in total cost over the state of the art in numerical
approaches to this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0532</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0532</id><created>2014-02-03</created><authors><author><keyname>Bozkurt</keyname><forenames>Alican</forenames></author><author><keyname>Arslan</keyname><forenames>Musa Tun&#xe7;</forenames></author><author><keyname>Sevimli</keyname><forenames>Rasim Akin</forenames></author><author><keyname>Akbas</keyname><forenames>Cem Emre</forenames></author><author><keyname>&#xc7;etin</keyname><forenames>A. Enis</forenames></author></authors><title>Approximate Computation of DFT without Performing Any Multiplications:
  Applications to Radar Signal Processing</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many practical problems it is not necessary to compute the DFT in a
perfect manner including some radar problems. In this article a new
multiplication free algorithm for approximate computation of the DFT is
introduced. All multiplications $(a\times b)$ in DFT are replaced by an
operator which computes $sign(a\times b)(|a|+|b|)$. The new transform is
especially useful when the signal processing algorithm requires correlations.
Ambiguity function in radar signal processing requires high number of
multiplications to compute the correlations. This new additive operator is used
to decrease the number of multiplications. Simulation examples involving
passive radars are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0543</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0543</id><created>2014-02-03</created><authors><author><keyname>Koeman</keyname><forenames>Jan</forenames></author><author><keyname>Rea</keyname><forenames>William</forenames></author></authors><title>How Does Latent Semantic Analysis Work? A Visualisation Approach</title><categories>cs.CL cs.IR</categories><comments>13 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By using a small example, an analogy to photographic compression, and a
simple visualization using heatmaps, we show that latent semantic analysis
(LSA) is able to extract what appears to be semantic meaning of words from a
set of documents by blurring the distinctions between the words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0555</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0555</id><created>2014-02-03</created><updated>2014-10-13</updated><authors><author><keyname>Agarwal</keyname><forenames>Alekh</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Kale</keyname><forenames>Satyen</forenames></author><author><keyname>Langford</keyname><forenames>John</forenames></author><author><keyname>Li</keyname><forenames>Lihong</forenames></author><author><keyname>Schapire</keyname><forenames>Robert E.</forenames></author></authors><title>Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for the contextual bandit learning problem, where
the learner repeatedly takes one of $K$ actions in response to the observed
context, and observes the reward only for that chosen action. Our method
assumes access to an oracle for solving fully supervised cost-sensitive
classification problems and achieves the statistically optimal regret guarantee
with only $\tilde{O}(\sqrt{KT/\log N})$ oracle calls across all $T$ rounds,
where $N$ is the number of policies in the policy class we compete against. By
doing so, we obtain the most practical contextual bandit learning algorithm
amongst approaches that work for general policy classes. We further conduct a
proof-of-concept experiment which demonstrates the excellent computational and
prediction performance of (an online variant of) our algorithm relative to
several baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0556</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0556</id><created>2014-02-03</created><authors><author><keyname>Qazvinian</keyname><forenames>Vahed</forenames></author><author><keyname>Radev</keyname><forenames>Dragomir R.</forenames></author><author><keyname>Mohammad</keyname><forenames>Saif M.</forenames></author><author><keyname>Dorr</keyname><forenames>Bonnie</forenames></author><author><keyname>Zajic</keyname><forenames>David</forenames></author><author><keyname>Whidby</keyname><forenames>Michael</forenames></author><author><keyname>Moon</keyname><forenames>Taesun</forenames></author></authors><title>Generating Extractive Summaries of Scientific Paradigms</title><categories>cs.IR cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 46, pages
  165-201, 2013</journal-ref><doi>10.1613/jair.3732</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Researchers and scientists increasingly find themselves in the position of
having to quickly understand large amounts of technical material. Our goal is
to effectively serve this need by using bibliometric text mining and
summarization techniques to generate summaries of scientific literature. We
show how we can use citations to produce automatically generated, readily
consumable, technical extractive summaries. We first propose C-LexRank, a model
for summarizing single scientific articles based on citations, which employs
community detection and extracts salient information-rich sentences. Next, we
further extend our experiments to summarize a set of papers, which cover the
same scientific topic. We generate extractive summaries of a set of Question
Answering (QA) and Dependency Parsing (DP) papers, their abstracts, and their
citation sentences and show that citations have unique information amenable to
creating a summary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0557</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0557</id><created>2014-02-03</created><authors><author><keyname>Huang</keyname><forenames>Eric</forenames></author><author><keyname>Korf</keyname><forenames>Richard E.</forenames></author></authors><title>Optimal Rectangle Packing: An Absolute Placement Approach</title><categories>cs.AI cs.DS</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 46, pages
  47-87, 2013</journal-ref><doi>10.1613/jair.3735</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding all enclosing rectangles of minimum area
that can contain a given set of rectangles without overlap. Our rectangle
packer chooses the x-coordinates of all the rectangles before any of the
y-coordinates. We then transform the problem into a perfect-packing problem
with no empty space by adding additional rectangles. To determine the
y-coordinates, we branch on the different rectangles that can be placed in each
empty position. Our packer allows us to extend the known solutions for a
consecutive-square benchmark from 27 to 32 squares. We also introduce three new
benchmarks, avoiding properties that make a benchmark easy, such as rectangles
with shared dimensions. Our third benchmark consists of rectangles of
increasingly high precision. To pack them efficiently, we limit the rectangles
coordinates and the bounding box dimensions to the set of subset sums of the
rectangles dimensions. Overall, our algorithms represent the current
state-of-the-art for this problem, outperforming other algorithms by orders of
magnitude, depending on the benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0558</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0558</id><created>2014-02-03</created><authors><author><keyname>Ordyniak</keyname><forenames>Sebastian</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Parameterized Complexity Results for Exact Bayesian Network Structure
  Learning</title><categories>cs.AI cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 46, pages
  263-302, 2013</journal-ref><doi>10.1613/jair.3744</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian network structure learning is the notoriously difficult problem of
discovering a Bayesian network that optimally represents a given set of
training data. In this paper we study the computational worst-case complexity
of exact Bayesian network structure learning under graph theoretic restrictions
on the (directed) super-structure. The super-structure is an undirected graph
that contains as subgraphs the skeletons of solution networks. We introduce the
directed super-structure as a natural generalization of its undirected
counterpart. Our results apply to several variants of score-based Bayesian
network structure learning where the score of a network decomposes into local
scores of its nodes. Results: We show that exact Bayesian network structure
learning can be carried out in non-uniform polynomial time if the
super-structure has bounded treewidth, and in linear time if in addition the
super-structure has bounded maximum degree. Furthermore, we show that if the
directed super-structure is acyclic, then exact Bayesian network structure
learning can be carried out in quadratic time. We complement these positive
results with a number of hardness results. We show that both restrictions
(treewidth and degree) are essential and cannot be dropped without loosing
uniform polynomial time tractability (subject to a complexity-theoretic
assumption). Similarly, exact Bayesian network structure learning remains
NP-hard for &quot;almost acyclic&quot; directed super-structures. Furthermore, we show
that the restrictions remain essential if we do not search for a globally
optimal network but aim to improve a given network by means of at most k arc
additions, arc deletions, or arc reversals (k-neighborhood local search).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0559</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0559</id><created>2014-02-03</created><authors><author><keyname>Nightingale</keyname><forenames>Peter</forenames></author><author><keyname>Gent</keyname><forenames>Ian Philip</forenames></author><author><keyname>Jefferson</keyname><forenames>Christopher</forenames></author><author><keyname>Miguel</keyname><forenames>Ian</forenames></author></authors><title>Short and Long Supports for Constraint Propagation</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 46, pages
  1-45, 2013</journal-ref><doi>10.1613/jair.3749</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Special-purpose constraint propagation algorithms frequently make implicit
use of short supports -- by examining a subset of the variables, they can infer
support (a justification that a variable-value pair may still form part of an
assignment that satisfies the constraint) for all other variables and values
and save substantial work -- but short supports have not been studied in their
own right. The two main contributions of this paper are the identification of
short supports as important for constraint propagation, and the introduction of
HaggisGAC, an efficient and effective general purpose propagation algorithm for
exploiting short supports. Given the complexity of HaggisGAC, we present it as
an optimised version of a simpler algorithm ShortGAC. Although experiments
demonstrate the efficiency of ShortGAC compared with other general-purpose
propagation algorithms where a compact set of short supports is available, we
show theoretically and experimentally that HaggisGAC is even better. We also
find that HaggisGAC performs better than GAC-Schema on full-length supports. We
also introduce a variant algorithm HaggisGAC-Stable, which is adapted to avoid
work on backtracking and in some cases can be faster and have significant
reductions in memory use. All the proposed algorithms are excellent for
propagating disjunctions of constraints. In all experiments with disjunctions
we found our algorithms to be faster than Constructive Or and GAC-Schema by at
least an order of magnitude, and up to three orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0560</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0560</id><created>2014-02-03</created><authors><author><keyname>Garcia</keyname><forenames>Javier</forenames></author><author><keyname>Fernandez</keyname><forenames>Fernando</forenames></author></authors><title>Safe Exploration of State and Action Spaces in Reinforcement Learning</title><categories>cs.LG cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 45, pages
  515-564, 2012</journal-ref><doi>10.1613/jair.3761</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the important problem of safe exploration in
reinforcement learning. While reinforcement learning is well-suited to domains
with complex transition dynamics and high-dimensional state-action spaces, an
additional challenge is posed by the need for safe and efficient exploration.
Traditional exploration techniques are not particularly useful for solving
dangerous tasks, where the trial and error process may lead to the selection of
actions whose execution in some states may result in damage to the learning
system (or any other system). Consequently, when an agent begins an interaction
with a dangerous and high-dimensional state-action space, an important question
arises; namely, that of how to avoid (or at least minimize) damage caused by
the exploration of the state-action space. We introduce the PI-SRL algorithm
which safely improves suboptimal albeit robust behaviors for continuous state
and action control tasks and which efficiently learns from the experience
gained from the environment. We evaluate the proposed method in four complex
tasks: automatic car parking, pole-balancing, helicopter hovering, and business
management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0561</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0561</id><created>2014-02-03</created><authors><author><keyname>de Cooman</keyname><forenames>Gert</forenames></author><author><keyname>Miranda</keyname><forenames>Enrique</forenames></author></authors><title>Irrelevant and independent natural extension for sets of desirable
  gambles</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 45, pages
  601-640, 2012</journal-ref><doi>10.1613/jair.3770</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The results in this paper add useful tools to the theory of sets of desirable
gambles, a growing toolbox for reasoning with partial probability assessments.
We investigate how to combine a number of marginal coherent sets of desirable
gambles into a joint set using the properties of epistemic irrelevance and
independence. We provide formulas for the smallest such joint, called their
independent natural extension, and study its main properties. The independent
natural extension of maximal coherent sets of desirable gambles allows us to
define the strong product of sets of desirable gambles. Finally, we explore an
easy way to generalise these results to also apply for the conditional versions
of epistemic irrelevance and independence. Having such a set of tools that are
easily implemented in computer programs is clearly beneficial to fields, like
AI, with a clear interest in coherent reasoning under uncertainty using general
and robust uncertainty models that require no full specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0562</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0562</id><created>2014-02-03</created><updated>2014-05-19</updated><authors><author><keyname>Azar</keyname><forenames>Mohammad Gheshlaghi</forenames></author><author><keyname>Lazaric</keyname><forenames>Alessandro</forenames></author><author><keyname>Brunskill</keyname><forenames>Emma</forenames></author></authors><title>Online Stochastic Optimization under Correlated Bandit Feedback</title><categories>stat.ML cs.LG cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of online stochastic optimization of a
locally smooth function under bandit feedback. We introduce the high-confidence
tree (HCT) algorithm, a novel any-time $\mathcal{X}$-armed bandit algorithm,
and derive regret bounds matching the performance of existing state-of-the-art
in terms of dependency on number of steps and smoothness factor. The main
advantage of HCT is that it handles the challenging case of correlated rewards,
whereas existing methods require that the reward-generating process of each arm
is an identically and independent distributed (iid) random process. HCT also
improves on the state-of-the-art in terms of its memory requirement as well as
requiring a weaker smoothness assumption on the mean-reward function in compare
to the previous anytime algorithms. Finally, we discuss how HCT can be applied
to the problem of policy search in reinforcement learning and we report
preliminary empirical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0563</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0563</id><created>2014-02-03</created><authors><author><keyname>Costa-juss&#xe0;</keyname><forenames>Marta R.</forenames></author><author><keyname>Henr&#xed;quez</keyname><forenames>Carlos A.</forenames></author><author><keyname>Banchs</keyname><forenames>Rafael E.</forenames></author></authors><title>Evaluating Indirect Strategies for Chinese-Spanish Statistical Machine
  Translation</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 45, pages
  761-780, 2012</journal-ref><doi>10.1613/jair.3786</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although, Chinese and Spanish are two of the most spoken languages in the
world, not much research has been done in machine translation for this language
pair. This paper focuses on investigating the state-of-the-art of
Chinese-to-Spanish statistical machine translation (SMT), which nowadays is one
of the most popular approaches to machine translation. For this purpose, we
report details of the available parallel corpus which are Basic Traveller
Expressions Corpus (BTEC), Holy Bible and United Nations (UN). Additionally, we
conduct experimental work with the largest of these three corpora to explore
alternative SMT strategies by means of using a pivot language. Three
alternatives are considered for pivoting: cascading, pseudo-corpus and
triangulation. As pivot language, we use either English, Arabic or French.
Results show that, for a phrase-based SMT system, English is the best pivot
language between Chinese and Spanish. We propose a system output combination
using the pivot strategies which is capable of outperforming the direct
translation strategy. The main objective of this work is motivating and
involving the research community to work in this important pair of languages
given their demographic impact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0564</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0564</id><created>2014-02-03</created><authors><author><keyname>Coles</keyname><forenames>Amanda Jane</forenames></author><author><keyname>Coles</keyname><forenames>Andrew Ian</forenames></author><author><keyname>Fox</keyname><forenames>Maria</forenames></author><author><keyname>Long</keyname><forenames>Derek</forenames></author></authors><title>A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in
  Planning</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 46, pages
  343-412, 2013</journal-ref><doi>10.1613/jair.3788</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the use of metric fluents is fundamental to many practical planning
problems, the study of heuristics to support fully automated planners working
with these fluents remains relatively unexplored. The most widely used
heuristic is the relaxation of metric fluents into interval-valued variables
--- an idea first proposed a decade ago. Other heuristics depend on domain
encodings that supply additional information about fluents, such as capacity
constraints or other resource-related annotations. A particular challenge to
these approaches is in handling interactions between metric fluents that
represent exchange, such as the transformation of quantities of raw materials
into quantities of processed goods, or trading of money for materials. The
usual relaxation of metric fluents is often very poor in these situations,
since it does not recognise that resources, once spent, are no longer available
to be spent again. We present a heuristic for numeric planning problems
building on the propositional relaxed planning graph, but using a mathematical
program for numeric reasoning. We define a class of producer--consumer planning
problems and demonstrate how the numeric constraints in these can be modelled
in a mixed integer program (MIP). This MIP is then combined with a metric
Relaxed Planning Graph (RPG) heuristic to produce an integrated hybrid
heuristic. The MIP tracks resource use more accurately than the usual
relaxation, but relaxes the ordering of actions, while the RPG captures the
causal propositional aspects of the problem. We discuss how these two
components interact to produce a single unified heuristic and go on to explore
how further numeric features of planning problems can be integrated into the
MIP. We show that encoding a limited subset of the propositional problem to
augment the MIP can yield more accurate guidance, partly by exploiting
structure such as propositional landmarks and propositional resources. Our
results show that the use of this heuristic enhances scalability on problems
where numeric resource interaction is key in finding a solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0565</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0565</id><created>2014-02-03</created><authors><author><keyname>Taghipour</keyname><forenames>Nima</forenames></author><author><keyname>Fierens</keyname><forenames>Daan</forenames></author><author><keyname>Davis</keyname><forenames>Jesse</forenames></author><author><keyname>Blockeel</keyname><forenames>Hendrik</forenames></author></authors><title>Lifted Variable Elimination: Decoupling the Operators from the
  Constraint Language</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 47, pages
  393-439, 2013</journal-ref><doi>10.1613/jair.3793</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lifted probabilistic inference algorithms exploit regularities in the
structure of graphical models to perform inference more efficiently. More
specifically, they identify groups of interchangeable variables and perform
inference once per group, as opposed to once per variable. The groups are
defined by means of constraints, so the flexibility of the grouping is
determined by the expressivity of the constraint language. Existing approaches
for exact lifted inference use specific languages for (in)equality constraints,
which often have limited expressivity. In this article, we decouple lifted
inference from the constraint language. We define operators for lifted
inference in terms of relational algebra operators, so that they operate on the
semantic level (the constraints extension) rather than on the syntactic level,
making them language-independent. As a result, lifted inference can be
performed using more powerful constraint languages, which provide more
opportunities for lifting. We empirically demonstrate that this can improve
inference efficiency by orders of magnitude, allowing exact inference where
until now only approximate inference was feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0566</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0566</id><created>2014-02-03</created><authors><author><keyname>Oliehoek</keyname><forenames>Frans Adriaan</forenames></author><author><keyname>Spaan</keyname><forenames>Matthijs T. J.</forenames></author><author><keyname>Amato</keyname><forenames>Christopher</forenames></author><author><keyname>Whiteson</keyname><forenames>Shimon</forenames></author></authors><title>Incremental Clustering and Expansion for Faster Optimal Planning in
  Dec-POMDPs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 46, pages
  449-509, 2013</journal-ref><doi>10.1613/jair.3804</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents the state-of-the-art in optimal solution methods for
decentralized partially observable Markov decision processes (Dec-POMDPs),
which are general models for collaborative multiagent planning under
uncertainty. Building off the generalized multiagent A* (GMAA*) algorithm,
which reduces the problem to a tree of one-shot collaborative Bayesian games
(CBGs), we describe several advances that greatly expand the range of
Dec-POMDPs that can be solved optimally. First, we introduce lossless
incremental clustering of the CBGs solved by GMAA*, which achieves exponential
speedups without sacrificing optimality. Second, we introduce incremental
expansion of nodes in the GMAA* search tree, which avoids the need to expand
all children, the number of which is in the worst case doubly exponential in
the nodes depth. This is particularly beneficial when little clustering is
possible. In addition, we introduce new hybrid heuristic representations that
are more compact and thereby enable the solution of larger Dec-POMDPs. We
provide theoretical guarantees that, when a suitable heuristic is used, both
incremental clustering and incremental expansion yield algorithms that are both
complete and search equivalent. Finally, we present extensive empirical results
demonstrating that GMAA*-ICE, an algorithm that synthesizes these advances, can
optimally solve Dec-POMDPs of unprecedented size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0567</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0567</id><created>2014-02-03</created><authors><author><keyname>Michalak</keyname><forenames>Tomasz Pawel</forenames></author><author><keyname>Aadithya</keyname><forenames>Karthik V</forenames></author><author><keyname>Szczepanski</keyname><forenames>Piotr L.</forenames></author><author><keyname>Ravindran</keyname><forenames>Balaraman</forenames></author><author><keyname>Jennings</keyname><forenames>Nicholas R.</forenames></author></authors><title>Efficient Computation of the Shapley Value for Game-Theoretic Network
  Centrality</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 46, pages
  607-650, 2013</journal-ref><doi>10.1613/jair.3806</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Shapley value---probably the most important normative payoff division
scheme in coalitional games---has recently been advocated as a useful measure
of centrality in networks. However, although this approach has a variety of
real-world applications (including social and organisational networks,
biological networks and communication networks), its computational properties
have not been widely studied. To date, the only practicable approach to compute
Shapley value-based centrality has been via Monte Carlo simulations which are
computationally expensive and not guaranteed to give an exact answer. Against
this background, this paper presents the first study of the computational
aspects of the Shapley value for network centralities. Specifically, we develop
exact analytical formulae for Shapley value-based centrality in both weighted
and unweighted networks and develop efficient (polynomial time) and exact
algorithms based on them. We empirically evaluate these algorithms on two
real-life examples (an infrastructure network representing the topology of the
Western States Power Grid and a collaboration network from the field of
astrophysics) and demonstrate that they deliver significant speedups over the
Monte Carlo approach. For instance, in the case of unweighted networks our
algorithms are able to return the exact solution about 1600 times faster than
the Monte Carlo approximation, even if we allow for a generous 10% error margin
for the latter method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0568</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0568</id><created>2014-02-03</created><authors><author><keyname>Metodi</keyname><forenames>Amit</forenames></author><author><keyname>Codish</keyname><forenames>Michael</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter James</forenames></author></authors><title>Boolean Equi-propagation for Concise and Efficient SAT Encodings of
  Combinatorial Problems</title><categories>cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1206.3883</comments><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 46, pages
  303-341, 2013</journal-ref><doi>10.1613/jair.3809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to propagation-based SAT encoding of combinatorial
problems, Boolean equi-propagation, where constraints are modeled as Boolean
functions which propagate information about equalities between Boolean
literals. This information is then applied to simplify the CNF encoding of the
constraints. A key factor is that considering only a small fragment of a
constraint model at one time enables us to apply stronger, and even complete,
reasoning to detect equivalent literals in that fragment. Once detected,
equivalences apply to simplify the entire constraint model and facilitate
further reasoning on other fragments. Equi-propagation in combination with
partial evaluation and constraint simplification provide the foundation for a
powerful approach to SAT-based finite domain constraint solving. We introduce a
tool called BEE (Ben-Gurion Equi-propagation Encoder) based on these ideas and
demonstrate for a variety of benchmarks that our approach leads to a
considerable reduction in the size of CNF encodings and subsequent speed-ups in
SAT solving times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0569</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0569</id><created>2014-02-03</created><authors><author><keyname>Hariri</keyname><forenames>Babak Bagheri</forenames></author><author><keyname>Calvanese</keyname><forenames>Diego</forenames></author><author><keyname>Montali</keyname><forenames>Marco</forenames></author><author><keyname>De Giacomo</keyname><forenames>Giuseppe</forenames></author><author><keyname>De Masellis</keyname><forenames>Riccardo</forenames></author><author><keyname>Felli</keyname><forenames>Paolo</forenames></author></authors><title>Description Logic Knowledge and Action Bases</title><categories>cs.AI cs.LO</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 46, pages
  651-686, 2013</journal-ref><doi>10.1613/jair.3826</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Description logic Knowledge and Action Bases (KAB) are a mechanism for
providing both a semantically rich representation of the information on the
domain of interest in terms of a description logic knowledge base and actions
to change such information over time, possibly introducing new objects. We
resort to a variant of DL-Lite where the unique name assumption is not enforced
and where equality between objects may be asserted and inferred. Actions are
specified as sets of conditional effects, where conditions are based on
epistemic queries over the knowledge base (TBox and ABox), and effects are
expressed in terms of new ABoxes. In this setting, we address verification of
temporal properties expressed in a variant of first-order mu-calculus with
quantification across states. Notably, we show decidability of verification,
under a suitable restriction inspired by the notion of weak acyclicity in data
exchange.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0570</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0570</id><created>2014-02-03</created><authors><author><keyname>Wang</keyname><forenames>Guangtao</forenames></author><author><keyname>Song</keyname><forenames>Qinbao</forenames></author><author><keyname>Sun</keyname><forenames>Heli</forenames></author><author><keyname>Zhang</keyname><forenames>Xueying</forenames></author><author><keyname>Xu</keyname><forenames>Baowen</forenames></author><author><keyname>Zhou</keyname><forenames>Yuming</forenames></author></authors><title>A Feature Subset Selection Algorithm Automatic Recommendation Method</title><categories>cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 47, pages
  1-34, 2013</journal-ref><doi>10.1613/jair.3831</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many feature subset selection (FSS) algorithms have been proposed, but not
all of them are appropriate for a given feature selection problem. At the same
time, so far there is rarely a good way to choose appropriate FSS algorithms
for the problem at hand. Thus, FSS algorithm automatic recommendation is very
important and practically useful. In this paper, a meta learning based FSS
algorithm automatic recommendation method is presented. The proposed method
first identifies the data sets that are most similar to the one at hand by the
k-nearest neighbor classification algorithm, and the distances among these data
sets are calculated based on the commonly-used data set characteristics. Then,
it ranks all the candidate FSS algorithms according to their performance on
these similar data sets, and chooses the algorithms with best performance as
the appropriate ones. The performance of the candidate FSS algorithms is
evaluated by a multi-criteria metric that takes into account not only the
classification accuracy over the selected features, but also the runtime of
feature selection and the number of selected features. The proposed
recommendation method is extensively tested on 115 real world data sets with 22
well-known and frequently-used different FSS algorithms for five representative
classifiers. The results show the effectiveness of our proposed FSS algorithm
recommendation method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0571</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0571</id><created>2014-02-03</created><authors><author><keyname>Tesauro</keyname><forenames>Gerald</forenames></author><author><keyname>Gondek</keyname><forenames>David C.</forenames></author><author><keyname>Lenchner</keyname><forenames>Jonathan</forenames></author><author><keyname>Fan</keyname><forenames>James</forenames></author><author><keyname>Prager</keyname><forenames>John M.</forenames></author></authors><title>Analysis of Watson's Strategies for Playing Jeopardy!</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 47, pages
  205-251, 2013</journal-ref><doi>10.1613/jair.3834</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Major advances in Question Answering technology were needed for IBM Watson to
play Jeopardy! at championship level -- the show requires rapid-fire answers to
challenging natural language questions, broad general knowledge, high
precision, and accurate confidence estimates. In addition, Jeopardy! features
four types of decision making carrying great strategic importance: (1) Daily
Double wagering; (2) Final Jeopardy wagering; (3) selecting the next square
when in control of the board; (4) deciding whether to attempt to answer, i.e.,
&quot;buzz in.&quot; Using sophisticated strategies for these decisions, that properly
account for the game state and future event probabilities, can significantly
boost a players overall chances to win, when compared with simple &quot;rule of
thumb&quot; strategies. This article presents our approach to developing Watsons
game-playing strategies, comprising development of a faithful simulation model,
and then using learning and Monte-Carlo methods within the simulator to
optimize Watsons strategic decision-making. After giving a detailed description
of each of our game-strategy algorithms, we then focus in particular on
validating the accuracy of the simulators predictions, and documenting
performance improvements using our methods. Quantitative performance benefits
are shown with respect to both simple heuristic strategies, and actual human
contestant performance in historical episodes. We further extend our analysis
of human play to derive a number of valuable and counterintuitive examples
illustrating how human contestants may improve their performance on the show.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0572</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0572</id><created>2014-02-03</created><authors><author><keyname>Bachrach</keyname><forenames>Yoram</forenames></author><author><keyname>Porat</keyname><forenames>Ely Porat</forenames></author><author><keyname>Rosenschein</keyname><forenames>Jeffrey S.</forenames></author></authors><title>Sharing Rewards in Cooperative Connectivity Games</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 47, pages
  281-311, 2013</journal-ref><doi>10.1613/jair.3841</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider how selfish agents are likely to share revenues derived from
maintaining connectivity between important network servers. We model a network
where a failure of one node may disrupt communication between other nodes as a
cooperative game called the vertex Connectivity Game (CG). In this game, each
agent owns a vertex, and controls all the edges going to and from that vertex.
A coalition of agents wins if it fully connects a certain subset of vertices in
the graph, called the primary vertices. Power indices measure an agents ability
to affect the outcome of the game. We show that in our domain, such indices can
be used to both determine the fair share of the revenues an agent is entitled
to, and identify significant possible points of failure affecting the
reliability of communication in the network. We show that in general graphs,
calculating the Shapley and Banzhaf power indices is #P-complete, but suggest a
polynomial algorithm for calculating them in trees. We also investigate finding
stable payoff divisions of the revenues in CGs, captured by the game theoretic
solution of the core, and its relaxations, the epsilon-core and least core. We
show a polynomial algorithm for computing the core of a CG, but show that
testing whether an imputation is in the epsilon-core is coNP-complete. Finally,
we show that for trees, it is possible to test for epsilon-core imputations in
polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0573</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0573</id><created>2014-02-03</created><authors><author><keyname>Vesic</keyname><forenames>Srdjan</forenames></author></authors><title>Identifying the Class of Maxi-Consistent Operators in Argumentation</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 47, pages
  71-93, 2013</journal-ref><doi>10.1613/jair.3860</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dung's abstract argumentation theory can be seen as a general framework for
non-monotonic reasoning. An important question is then: what is the class of
logics that can be subsumed as instantiations of this theory? The goal of this
paper is to identify and study the large class of logic-based instantiations of
Dung's theory which correspond to the maxi-consistent operator, i.e. to the
function which returns maximal consistent subsets of an inconsistent knowledge
base. In other words, we study the class of instantiations where very extension
of the argumentation system corresponds to exactly one maximal consistent
subset of the knowledge base. We show that an attack relation belonging to this
class must be conflict-dependent, must not be valid, must not be
conflict-complete, must not be symmetric etc. Then, we show that some attack
relations serve as lower or upper bounds of the class (e.g. if an attack
relation contains canonical undercut then it is not a member of this class). By
using our results, we show for all existing attack relations whether or not
they belong to this class. We also define new attack relations which are
members of this class. Finally, we interpret our results and discuss more
general questions, like: what is the added value of argumentation in such a
setting? We believe that this work is a first step towards achieving our
long-term goal, which is to better understand the role of argumentation and,
particularly, the expressivity of logic-based instantiations of Dung-style
argumentation frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0574</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0574</id><created>2014-02-03</created><authors><author><keyname>Radinsky</keyname><forenames>Kira</forenames></author><author><keyname>Davidovich</keyname><forenames>Sagie</forenames></author><author><keyname>Markovitch</keyname><forenames>Shaul</forenames></author></authors><title>Learning to Predict from Textual Data</title><categories>cs.CL cs.AI cs.IR</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 45, pages
  641-684, 2012</journal-ref><doi>10.1613/jair.3865</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a current news event, we tackle the problem of generating plausible
predictions of future events it might cause. We present a new methodology for
modeling and predicting such future news events using machine learning and data
mining techniques. Our Pundit algorithm generalizes examples of causality pairs
to infer a causality predictor. To obtain precisely labeled causality examples,
we mine 150 years of news articles and apply semantic natural language modeling
techniques to headlines containing certain predefined causality patterns. For
generalization, the model uses a vast number of world knowledge ontologies.
Empirical evaluation on real news articles shows that our Pundit algorithm
performs as well as non-expert humans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0575</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0575</id><created>2014-02-03</created><authors><author><keyname>Calvanese</keyname><forenames>Diego</forenames></author><author><keyname>Ortiz</keyname><forenames>Magdalena</forenames></author><author><keyname>Simkus</keyname><forenames>Mantas</forenames></author><author><keyname>Stefanoni</keyname><forenames>Giorgio</forenames></author></authors><title>Reasoning about Explanations for Negative Query Answers in DL-Lite</title><categories>cs.AI cs.LO</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 48, pages
  635-669, 2013</journal-ref><doi>10.1613/jair.3870</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to meet usability requirements, most logic-based applications
provide explanation facilities for reasoning services. This holds also for
Description Logics, where research has focused on the explanation of both TBox
reasoning and, more recently, query answering. Besides explaining the presence
of a tuple in a query answer, it is important to explain also why a given tuple
is missing. We address the latter problem for instance and conjunctive query
answering over DL-Lite ontologies by adopting abductive reasoning; that is, we
look for additions to the ABox that force a given tuple to be in the result. As
reasoning tasks we consider existence and recognition of an explanation, and
relevance and necessity of a given assertion for an explanation. We
characterize the computational complexity of these problems for arbitrary,
subset minimal, and cardinality minimal explanations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0576</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0576</id><created>2014-02-03</created><authors><author><keyname>Kollia</keyname><forenames>Ilianna</forenames></author><author><keyname>Glimm</keyname><forenames>Birte</forenames></author></authors><title>Optimizing SPARQL Query Answering over OWL Ontologies</title><categories>cs.DB cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 48, pages
  253-303, 2013</journal-ref><doi>10.1613/jair.3872</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The SPARQL query language is currently being extended by the World Wide Web
Consortium (W3C) with so-called entailment regimes. An entailment regime
defines how queries are evaluated under more expressive semantics than SPARQLs
standard simple entailment, which is based on subgraph matching. The queries
are very expressive since variables can occur within complex concepts and can
also bind to concept or role names. In this paper, we describe a sound and
complete algorithm for the OWL Direct Semantics entailment regime. We further
propose several novel optimizations such as strategies for determining a good
query execution order, query rewriting techniques, and show how specialized OWL
reasoning tasks and the concept and role hierarchy can be used to reduce the
query execution time. For determining a good execution order, we propose a
cost-based model, where the costs are based on information about the instances
of concepts and roles that are extracted from a model abstraction built by an
OWL reasoner. We present two ordering strategies: a static and a dynamic one.
For the dynamic case, we improve the performance by exploiting an individual
clustering approach that allows for computing the cost functions based on one
individual sample from a cluster. We provide a prototypical implementation and
evaluate the efficiency of the proposed optimizations. Our experimental study
shows that the static ordering usually outperforms the dynamic one when
accurate statistics are available. This changes, however, when the statistics
are less accurate, e.g., due to nondeterministic reasoning decisions. For
queries that go beyond conjunctive instance queries we observe an improvement
of up to three orders of magnitude due to the proposed optimizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0577</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0577</id><created>2014-02-03</created><authors><author><keyname>Mourad</keyname><forenames>Rapha&#xeb;l</forenames></author><author><keyname>Sinoquet</keyname><forenames>Christine</forenames></author><author><keyname>Zhang</keyname><forenames>Nevin L.</forenames></author><author><keyname>Liu</keyname><forenames>Tengfei</forenames></author><author><keyname>Leray</keyname><forenames>Philippe</forenames></author></authors><title>A Survey on Latent Tree Models and Applications</title><categories>cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 47, pages
  157-203, 2013</journal-ref><doi>10.1613/jair.3879</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In data analysis, latent variables play a central role because they help
provide powerful insights into a wide variety of phenomena, ranging from
biological to human sciences. The latent tree model, a particular type of
probabilistic graphical models, deserves attention. Its simple structure - a
tree - allows simple and efficient inference, while its latent variables
capture complex relationships. In the past decade, the latent tree model has
been subject to significant theoretical and methodological developments. In
this review, we propose a comprehensive study of this model. First we summarize
key ideas underlying the model. Second we explain how it can be efficiently
learned from data. Third we illustrate its use within three types of
applications: latent structure discovery, multidimensional clustering, and
probabilistic inference. Finally, we conclude and give promising directions for
future researches in this field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0578</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0578</id><created>2014-02-03</created><authors><author><keyname>Alabbas</keyname><forenames>Maytham</forenames></author><author><keyname>Ramsay</keyname><forenames>Allan</forenames></author></authors><title>Natural Language Inference for Arabic Using Extended Tree Edit Distance
  with Subtrees</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 48, pages
  1-22, 2013</journal-ref><doi>10.1613/jair.3892</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many natural language processing (NLP) applications require the computation
of similarities between pairs of syntactic or semantic trees. Many researchers
have used tree edit distance for this task, but this technique suffers from the
drawback that it deals with single node operations only. We have extended the
standard tree edit distance algorithm to deal with subtree transformation
operations as well as single nodes. The extended algorithm with subtree
operations, TED+ST, is more effective and flexible than the standard algorithm,
especially for applications that pay attention to relations among nodes (e.g.
in linguistic trees, deleting a modifier subtree should be cheaper than the sum
of deleting its components individually). We describe the use of TED+ST for
checking entailment between two Arabic text snippets. The preliminary results
of using TED+ST were encouraging when compared with two string-based approaches
and with the standard algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0579</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0579</id><created>2014-02-03</created><authors><author><keyname>Ono</keyname><forenames>Masahiro</forenames></author><author><keyname>Williams</keyname><forenames>Brian C.</forenames></author><author><keyname>Blackmore</keyname><forenames>L.</forenames></author></authors><title>Probabilistic Planning for Continuous Dynamic Systems under Bounded Risk</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 46, pages
  511-577, 2013</journal-ref><doi>10.1613/jair.3893</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a model-based planner called the Probabilistic Sulu
Planner or the p-Sulu Planner, which controls stochastic systems in a goal
directed manner within user-specified risk bounds. The objective of the p-Sulu
Planner is to allow users to command continuous, stochastic systems, such as
unmanned aerial and space vehicles, in a manner that is both intuitive and
safe. To this end, we first develop a new plan representation called a
chance-constrained qualitative state plan (CCQSP), through which users can
specify the desired evolution of the plant state as well as the acceptable
level of risk. An example of a CCQSP statement is go to A through B within 30
minutes, with less than 0.001% probability of failure.&quot; We then develop the
p-Sulu Planner, which can tractably solve a CCQSP planning problem. In order to
enable CCQSP planning, we develop the following two capabilities in this paper:
1) risk-sensitive planning with risk bounds, and 2) goal-directed planning in a
continuous domain with temporal constraints. The first capability is to ensures
that the probability of failure is bounded. The second capability is essential
for the planner to solve problems with a continuous state space such as vehicle
path planning. We demonstrate the capabilities of the p-Sulu Planner by
simulations on two real-world scenarios: the path planning and scheduling of a
personal aerial vehicle as well as the space rendezvous of an autonomous cargo
spacecraft.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0580</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0580</id><created>2014-02-03</created><authors><author><keyname>Betzler</keyname><forenames>Nadja</forenames></author><author><keyname>Slinko</keyname><forenames>Arkadii</forenames></author><author><keyname>Uhlmann</keyname><forenames>Johannes</forenames></author></authors><title>On the Computation of Fully Proportional Representation</title><categories>cs.GT cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 47, pages
  475-519, 2013</journal-ref><doi>10.1613/jair.3896</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate two systems of fully proportional representation suggested by
Chamberlin Courant and Monroe. Both systems assign a representative to each
voter so that the &quot;sum of misrepresentations&quot; is minimized. The winner
determination problem for both systems is known to be NP-hard, hence this work
aims at investigating whether there are variants of the proposed rules and/or
specific electorates for which these problems can be solved efficiently. As a
variation of these rules, instead of minimizing the sum of misrepresentations,
we considered minimizing the maximal misrepresentation introducing effectively
two new rules. In the general case these &quot;minimax&quot; versions of classical rules
appeared to be still NP-hard. We investigated the parameterized complexity of
winner determination of the two classical and two new rules with respect to
several parameters. Here we have a mixture of positive and negative results:
e.g., we proved fixed-parameter tractability for the parameter the number of
candidates but fixed-parameter intractability for the number of winners. For
single-peaked electorates our results are overwhelmingly positive: we provide
polynomial-time algorithms for most of the considered problems. The only rule
that remains NP-hard for single-peaked electorates is the classical Monroe
rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0581</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0581</id><created>2014-02-03</created><authors><author><keyname>Snooke</keyname><forenames>Neal Andrew</forenames></author><author><keyname>Lee</keyname><forenames>Mark H</forenames></author></authors><title>Qualitative Order of Magnitude Energy-Flow-Based Failure Modes and
  Effects Analysis</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 46, pages
  413-447, 2013</journal-ref><doi>10.1613/jair.3898</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a structured power and energy-flow-based qualitative
modelling approach that is applicable to a variety of system types including
electrical and fluid flow. The modelling is split into two parts. Power flow is
a global phenomenon and is therefore naturally represented and analysed by a
network comprised of the relevant structural elements from the components of a
system. The power flow analysis is a platform for higher-level behaviour
prediction of energy related aspects using local component behaviour models to
capture a state-based representation with a global time. The primary
application is Failure Modes and Effects Analysis (FMEA) and a form of
exaggeration reasoning is used, combined with an order of magnitude
representation to derive the worst case failure modes. The novel aspects of the
work are an order of magnitude(OM) qualitative network analyser to represent
any power domain and topology, including multiple power sources, a feature that
was not required for earlier specialised electrical versions of the approach.
Secondly, the representation of generalised energy related behaviour as
state-based local models is presented as a modelling strategy that can be more
vivid and intuitive for a range of topologically complex applications than
qualitative equation-based representations.The two-level modelling strategy
allows the broad system behaviour coverage of qualitative simulation to be
exploited for the FMEA task, while limiting the difficulties of qualitative
ambiguity explanation that can arise from abstracted numerical models. We have
used the method to support an automated FMEA system with examples of an
aircraft fuel system and domestic a heating system discussed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0582</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0582</id><created>2014-02-03</created><authors><author><keyname>Bajestani</keyname><forenames>Maliheh Aramon</forenames></author><author><keyname>Beck</keyname><forenames>J. Christopher</forenames></author></authors><title>Scheduling a Dynamic Aircraft Repair Shop with Limited Repair Resources</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 47, pages
  35-70, 2013</journal-ref><doi>10.1613/jair.3902</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address a dynamic repair shop scheduling problem in the context of
military aircraft fleet management where the goal is to maintain a full
complement of aircraft over the long-term. A number of flights, each with a
requirement for a specific number and type of aircraft, are already scheduled
over a long horizon. We need to assign aircraft to flights and schedule repair
activities while considering the flights requirements, repair capacity, and
aircraft failures. The number of aircraft awaiting repair dynamically changes
over time due to failures and it is therefore necessary to rebuild the repair
schedule online. To solve the problem, we view the dynamic repair shop as
successive static repair scheduling sub-problems over shorter time periods. We
propose a complete approach based on the logic-based Benders decomposition to
solve the static sub-problems, and design different rescheduling policies to
schedule the dynamic repair shop. Computational experiments demonstrate that
the Benders model is able to find and prove optimal solutions on average four
times faster than a mixed integer programming model. The rescheduling approach
having both aspects of scheduling over a longer horizon and quickly adjusting
the schedule increases aircraft available in the long term by 10% compared to
the approaches having either one of the aspects alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0583</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0583</id><created>2014-02-03</created><authors><author><keyname>Cigler</keyname><forenames>Ludek</forenames></author><author><keyname>Faltings</keyname><forenames>Boi</forenames></author></authors><title>Decentralized Anti-coordination Through Multi-agent Learning</title><categories>cs.GT cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 47, pages
  441-473, 2013</journal-ref><doi>10.1613/jair.3904</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To achieve an optimal outcome in many situations, agents need to choose
distinct actions from one another. This is the case notably in many resource
allocation problems, where a single resource can only be used by one agent at a
time. How shall a designer of a multi-agent system program its identical agents
to behave each in a different way? From a game theoretic perspective, such
situations lead to undesirable Nash equilibria. For example consider a resource
allocation game in that two players compete for an exclusive access to a single
resource. It has three Nash equilibria. The two pure-strategy NE are efficient,
but not fair. The one mixed-strategy NE is fair, but not efficient. Aumanns
notion of correlated equilibrium fixes this problem: It assumes a correlation
device that suggests each agent an action to take. However, such a &quot;smart&quot;
coordination device might not be available. We propose using a randomly chosen,
&quot;stupid&quot; integer coordination signal. &quot;Smart&quot; agents learn which action they
should use for each value of the coordination signal. We present a multi-agent
learning algorithm that converges in polynomial number of steps to a correlated
equilibrium of a channel allocation game, a variant of the resource allocation
game. We show that the agents learn to play for each coordination signal value
a randomly chosen pure-strategy Nash equilibrium of the game. Therefore, the
outcome is an efficient correlated equilibrium. This CE becomes more fair as
the number of the available coordination signal values increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0584</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0584</id><created>2014-02-03</created><authors><author><keyname>Cai</keyname><forenames>Shaowei</forenames></author><author><keyname>Su</keyname><forenames>Kaile</forenames></author><author><keyname>Luo</keyname><forenames>Chuan</forenames></author><author><keyname>Sattar</keyname><forenames>Abdul</forenames></author></authors><title>NuMVC: An Efficient Local Search Algorithm for Minimum Vertex Cover</title><categories>cs.AI cs.DS</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 46, pages
  687-716, 2013</journal-ref><doi>10.1613/jair.3907</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Minimum Vertex Cover (MVC) problem is a prominent NP-hard combinatorial
optimization problem of great importance in both theory and application. Local
search has proved successful for this problem. However, there are two main
drawbacks in state-of-the-art MVC local search algorithms. First, they select a
pair of vertices to exchange simultaneously, which is time-consuming. Secondly,
although using edge weighting techniques to diversify the search, these
algorithms lack mechanisms for decreasing the weights. To address these issues,
we propose two new strategies: two-stage exchange and edge weighting with
forgetting. The two-stage exchange strategy selects two vertices to exchange
separately and performs the exchange in two stages. The strategy of edge
weighting with forgetting not only increases weights of uncovered edges, but
also decreases some weights for each edge periodically. These two strategies
are used in designing a new MVC local search algorithm, which is referred to as
NuMVC. We conduct extensive experimental studies on the standard benchmarks,
namely DIMACS and BHOSLIB. The experiment comparing NuMVC with state-of-the-art
heuristic algorithms show that NuMVC is at least competitive with the nearest
competitor namely PLS on the DIMACS benchmark, and clearly dominates all
competitors on the BHOSLIB benchmark. Also, experimental results indicate that
NuMVC finds an optimal solution much faster than the current best exact
algorithm for Maximum Clique on random instances as well as some structured
ones. Moreover, we study the effectiveness of the two strategies and the
run-time behaviour through experimental analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0585</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0585</id><created>2014-02-03</created><authors><author><keyname>Fernandez</keyname><forenames>Jose David</forenames></author><author><keyname>Vico</keyname><forenames>Francisco</forenames></author></authors><title>AI Methods in Algorithmic Composition: A Comprehensive Survey</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 48, pages
  513-582, 2013</journal-ref><doi>10.1613/jair.3908</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithmic composition is the partial or total automation of the process of
music composition by using computers. Since the 1950s, different computational
techniques related to Artificial Intelligence have been used for algorithmic
composition, including grammatical representations, probabilistic methods,
neural networks, symbolic rule-based systems, constraint programming and
evolutionary algorithms. This survey aims to be a comprehensive account of
research on algorithmic composition, presenting a thorough view of the field
for researchers in Artificial Intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0586</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0586</id><created>2014-02-03</created><authors><author><keyname>Joty</keyname><forenames>Shafiq Rayhan</forenames></author><author><keyname>Carenini</keyname><forenames>Giuseppe</forenames></author><author><keyname>Ng</keyname><forenames>Raymond T</forenames></author></authors><title>Topic Segmentation and Labeling in Asynchronous Conversations</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 47, pages
  521-573, 2013</journal-ref><doi>10.1613/jair.3940</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topic segmentation and labeling is often considered a prerequisite for
higher-level conversation analysis and has been shown to be useful in many
Natural Language Processing (NLP) applications. We present two new corpora of
email and blog conversations annotated with topics, and evaluate annotator
reliability for the segmentation and labeling tasks in these asynchronous
conversations. We propose a complete computational framework for topic
segmentation and labeling in asynchronous conversations. Our approach extends
state-of-the-art methods by considering a fine-grained structure of an
asynchronous conversation, along with other conversational features by applying
recent graph-based methods for NLP. For topic segmentation, we propose two
novel unsupervised models that exploit the fine-grained conversational
structure, and a novel graph-theoretic supervised model that combines lexical,
conversational and topic features. For topic labeling, we propose two novel
(unsupervised) random walk models that respectively capture conversation
specific clues from two different sources: the leading sentences and the
fine-grained conversational structure. Empirical evaluation shows that the
segmentation and the labeling performed by our best models beat the
state-of-the-art, and are highly correlated with human annotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0587</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0587</id><created>2014-02-03</created><authors><author><keyname>Grinshpoun</keyname><forenames>Tal</forenames></author><author><keyname>Grubshtein</keyname><forenames>Alon</forenames></author><author><keyname>Zivan</keyname><forenames>Roie</forenames></author><author><keyname>Netzer</keyname><forenames>Arnon</forenames></author><author><keyname>Meisels</keyname><forenames>Amnon</forenames></author></authors><title>Asymmetric Distributed Constraint Optimization Problems</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 47, pages
  613-647, 2013</journal-ref><doi>10.1613/jair.3945</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed Constraint Optimization (DCOP) is a powerful framework for
representing and solving distributed combinatorial problems, where the
variables of the problem are owned by different agents. Many multi-agent
problems include constraints that produce different gains (or costs) for the
participating agents. Asymmetric gains of constrained agents cannot be
naturally represented by the standard DCOP model. The present paper proposes a
general framework for Asymmetric DCOPs (ADCOPs). In ADCOPs different agents may
have different valuations for constraints that they are involved in. The new
framework bridges the gap between multi-agent problems which tend to have
asymmetric structure and the standard symmetric DCOP model. The benefits of the
proposed model over previous attempts to generalize the DCOP model are
discussed and evaluated. Innovative algorithms that apply to the special
properties of the proposed ADCOP model are presented in detail. These include
complete algorithms that have a substantial advantage in terms of runtime and
network load over existing algorithms (for standard DCOPs) which use
alternative representations. Moreover, standard incomplete algorithms (i.e.,
local search algorithms) are inapplicable to the existing DCOP representations
of asymmetric constraints and when they are applied to the new ADCOP framework
they often fail to converge to a local optimum and yield poor results. The
local search algorithms proposed in the present paper converge to high quality
solutions. The experimental evidence that is presented reveals that the
proposed local search algorithms for ADCOPs achieve high quality solutions
while preserving a high level of privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0588</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0588</id><created>2014-02-03</created><authors><author><keyname>B&#xe4;ckstr&#xf6;m</keyname><forenames>Christer</forenames></author><author><keyname>Jonsson</keyname><forenames>Peter</forenames></author></authors><title>A Refined View of Causal Graphs and Component Sizes: SP-Closed Graph
  Classes and Beyond</title><categories>cs.AI cs.DS</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 47, pages
  575-611, 2013</journal-ref><doi>10.1613/jair.3968</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The causal graph of a planning instance is an important tool for planning
both in practice and in theory. The theoretical studies of causal graphs have
largely analysed the computational complexity of planning for instances where
the causal graph has a certain structure, often in combination with other
parameters like the domain size of the variables. Chen and Gimand#233;nez
ignored even the structure and considered only the size of the weakly connected
components. They proved that planning is tractable if the components are
bounded by a constant and otherwise intractable. Their intractability result
was, however, conditioned by an assumption from parameterised complexity theory
that has no known useful relationship with the standard complexity classes. We
approach the same problem from the perspective of standard complexity classes,
and prove that planning is NP-hard for classes with unbounded components under
an additional restriction we refer to as SP-closed. We then argue that most
NP-hardness theorems for causal graphs are difficult to apply and, thus, prove
a more general result; even if the component sizes grow slowly and the class is
not densely populated with graphs, planning still cannot be tractable unless
the polynomial hierachy collapses. Both these results still hold when
restricted to the class of acyclic causal graphs. We finally give a partial
characterization of the borderline between NP-hard and NP-intermediate classes,
giving further insight into the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0589</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0589</id><created>2014-02-03</created><updated>2014-07-12</updated><authors><author><keyname>Leaute</keyname><forenames>Thomas</forenames></author><author><keyname>Faltings</keyname><forenames>Boi</forenames></author></authors><title>Protecting Privacy through Distributed Computation in Multi-agent
  Decision Making</title><categories>cs.AI cs.CR cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 47, pages
  649-695, 2013</journal-ref><doi>10.1613/jair.3983</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As large-scale theft of data from corporate servers is becoming increasingly
common, it becomes interesting to examine alternatives to the paradigm of
centralizing sensitive data into large databases. Instead, one could use
cryptography and distributed computation so that sensitive data can be supplied
and processed in encrypted form, and only the final result is made known. In
this paper, we examine how such a paradigm can be used to implement constraint
satisfaction, a technique that can solve a broad class of AI problems such as
resource allocation, planning, scheduling, and diagnosis. Most previous work on
privacy in constraint satisfaction only attempted to protect specific types of
information, in particular the feasibility of particular combinations of
decisions. We formalize and extend these restricted notions of privacy by
introducing four types of private information, including the feasibility of
decisions and the final decisions made, but also the identities of the
participants and the topology of the problem. We present distributed algorithms
that allow computing solutions to constraint satisfaction problems while
maintaining these four types of privacy. We formally prove the privacy
properties of these algorithms, and show experiments that compare their
respective performance on benchmark problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0590</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0590</id><created>2014-02-03</created><authors><author><keyname>Roijers</keyname><forenames>Diederik Marijn</forenames></author><author><keyname>Vamplew</keyname><forenames>Peter</forenames></author><author><keyname>Whiteson</keyname><forenames>Shimon</forenames></author><author><keyname>Dazeley</keyname><forenames>Richard</forenames></author></authors><title>A Survey of Multi-Objective Sequential Decision-Making</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 48, pages
  67-113, 2013</journal-ref><doi>10.1613/jair.3987</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequential decision-making problems with multiple objectives arise naturally
in practice and pose unique challenges for research in decision-theoretic
planning and learning, which has largely focused on single-objective settings.
This article surveys algorithms designed for sequential decision-making
problems with multiple objectives. Though there is a growing body of literature
on this subject, little of it makes explicit under what circumstances special
methods are needed to solve multi-objective problems. Therefore, we identify
three distinct scenarios in which converting such a problem to a
single-objective one is impossible, infeasible, or undesirable. Furthermore, we
propose a taxonomy that classifies multi-objective methods according to the
applicable scenario, the nature of the scalarization function (which projects
multi-objective values to scalar ones), and the type of policies considered. We
show how these factors determine the nature of an optimal solution, which can
be a single policy, a convex hull, or a Pareto front. Using this taxonomy, we
survey the literature on multi-objective methods for planning and learning.
Finally, we discuss key applications of such methods and outline opportunities
for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0591</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0591</id><created>2014-02-03</created><authors><author><keyname>Costa</keyname><forenames>Paulo Roberto</forenames></author><author><keyname>Botelho</keyname><forenames>Lu&#xed;s Miguel</forenames></author></authors><title>Learning by Observation of Agent Software Images</title><categories>cs.AI cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 47, pages
  313-349, 2013</journal-ref><doi>10.1613/jair.3989</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning by observation can be of key importance whenever agents sharing
similar features want to learn from each other. This paper presents an agent
architecture that enables software agents to learn by direct observation of the
actions executed by expert agents while they are performing a task. This is
possible because the proposed architecture displays information that is
essential for observation, making it possible for software agents to observe
each other. The agent architecture supports a learning process that covers all
aspects of learning by observation, such as discovering and observing experts,
learning from the observed data, applying the acquired knowledge and evaluating
the agents progress. The evaluation provides control over the decision to
obtain new knowledge or apply the acquired knowledge to new problems. We
combine two methods for learning from the observed information. The first one,
the recall method, uses the sequence on which the actions were observed to
solve new problems. The second one, the classification method, categorizes the
information in the observed data and determines to which set of categories the
new problems belong. Results show that agents are able to learn in conditions
where common supervised learning algorithms fail, such as when agents do not
know the results of their actions a priori or when not all the effects of the
actions are visible. The results also show that our approach provides better
results than other learning methods since it requires shorter learning periods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0595</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0595</id><created>2014-02-03</created><authors><author><keyname>Seyedhosseini</keyname><forenames>Mojtaba</forenames></author><author><keyname>Tasdizen</keyname><forenames>Tolga</forenames></author></authors><title>Scene Labeling with Contextual Hierarchical Models</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scene labeling is the problem of assigning an object label to each pixel. It
unifies the image segmentation and object recognition problems. The importance
of using contextual information in scene labeling frameworks has been widely
realized in the field. We propose a contextual framework, called contextual
hierarchical model (CHM), which learns contextual information in a hierarchical
framework for scene labeling. At each level of the hierarchy, a classifier is
trained based on downsampled input images and outputs of previous levels. Our
model then incorporates the resulting multi-resolution contextual information
into a classifier to segment the input image at original resolution. This
training strategy allows for optimization of a joint posterior probability at
multiple resolutions through the hierarchy. Contextual hierarchical model is
purely based on the input image patches and does not make use of any fragments
or shape examples. Hence, it is applicable to a variety of problems such as
object segmentation and edge detection. We demonstrate that CHM outperforms
state-of-the-art on Stanford background and Weizmann horse datasets. It also
outperforms state-of-the-art edge detection methods on NYU depth dataset and
achieves state-of-the-art on Berkeley segmentation dataset (BSDS 500).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0599</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0599</id><created>2014-02-03</created><authors><author><keyname>Han</keyname><forenames>Duo</forenames></author><author><keyname>Mo</keyname><forenames>Yilin</forenames></author><author><keyname>Wu</keyname><forenames>Junfeng</forenames></author><author><keyname>Weerakkody</keyname><forenames>Sean</forenames></author><author><keyname>Sinopoli</keyname><forenames>Bruno</forenames></author><author><keyname>Shi</keyname><forenames>Ling</forenames></author></authors><title>Stochastic Event-triggered Sensor Schedule for Remote State Estimation</title><categories>cs.IT math.IT</categories><doi>10.1109/TAC.2015.2406975</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an open-loop and a closed-loop stochastic event-triggered sensor
schedule for remote state estimation. Both schedules overcome the essential
difficulties of existing schedules in recent literature works where, through
introducing a deterministic event-triggering mechanism, the Gaussian property
of the innovation process is destroyed which produces a challenging nonlinear
filtering problem that cannot be solved unless approximation techniques are
adopted. The proposed stochastic event-triggered sensor schedules eliminate
such approximations. Under these two schedules, the MMSE estimator and its
estimation error covariance matrix at the remote estimator are given in a
closed-form. Simulation studies demonstrate that the proposed schedules have
better performance than periodic ones with the same sensor-to-estimator
communication rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0601</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0601</id><created>2014-02-03</created><authors><author><keyname>Cassez</keyname><forenames>Franck</forenames></author><author><keyname>van der Meyden</keyname><forenames>Ron</forenames></author><author><keyname>Zhang</keyname><forenames>Chenyi</forenames></author></authors><title>The Complexity of Synchronous Notions of Information Flow Security</title><categories>cs.CR</categories><comments>Extended version of a paper in FOSSACS'10 with all proofs included</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper considers the complexity of verifying that a finite state system
satisfies a number of definitions of information flow security. The systems
model considered is one in which agents operate synchronously with awareness of
the global clock. This enables timing based attacks to be captured, whereas
previous work on this topic has dealt primarily with asynchronous systems.
Versions of the notions of nondeducibility on inputs, nondeducibility on
strategies, and an unwinding based notion are formulated for this model. All
three notions are shown to be decidable, and their computational complexity is
characterised.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0608</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0608</id><created>2014-02-03</created><updated>2015-10-07</updated><authors><author><keyname>Kostina</keyname><forenames>Victoria</forenames></author><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author><author><keyname>Verd&#xfa;</keyname><forenames>Sergio</forenames></author></authors><title>Variable-length compression allowing errors</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Information Theory, vol. 61, no. 9, pp.
  4316-4330, Aug. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the fundamental limits of the minimum average length of
lossless and lossy variable-length compression, allowing a nonzero error
probability $\epsilon$, for lossless compression. We give non-asymptotic bounds
on the minimum average length in terms of Erokhin's rate-distortion function
and we use those bounds to obtain a Gaussian approximation on the speed of
approach to the limit which is quite accurate for all but small blocklengths:
$$(1 - \epsilon) k H(\mathsf S) - \sqrt{\frac{k V(\mathsf S)}{2 \pi} } e^{-
\frac {(Q^{-1}(\epsilon))^2} 2 }$$ where $Q^{-1}(\cdot)$ is the functional
inverse of the standard Gaussian complementary cdf, and $V(\mathsf S)$ is the
source dispersion. A nonzero error probability thus not only reduces the
asymptotically achievable rate by a factor of $1 - \epsilon$, but this
asymptotic limit is approached from below, i.e. larger source dispersions and
shorter blocklengths are beneficial. Variable-length lossy compression under an
excess distortion constraint is shown to exhibit similar properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0614</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0614</id><created>2014-02-03</created><updated>2015-01-16</updated><authors><author><keyname>Bai</keyname><forenames>Jingwen</forenames></author><author><keyname>Dick</keyname><forenames>Chris</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Vector Bin-and-Cancel for MIMO Distributed Full-Duplex</title><categories>cs.IT math.IT</categories><comments>60 pages, Submitted to IEEE Transactions on Information Theory (under
  revision), Jan 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multi-input multi-output (MIMO) full-duplex network, where an in-band
full-duplex infrastruc- ture node communicates with two half-duplex mobiles
supporting simultaneous up- and downlink flows, the inter-mobile interference
between the up- and downlink mobiles limits the system performance. We study
the impact of leveraging an out-of-band side-channel between mobiles in such
network under different channel models. For time-invariant channels, we aim to
characterize the generalized degrees- of-freedom (GDoF) of the side-channel
assisted MIMO full-duplex network. For slow-fading channels, we focus on the
diversity-multiplexing tradeoff (DMT) of the system with various assumptions as
to the availability of channel state information at the transmitter (CSIT). The
key to the optimal performance is a vector bin-and-cancel strategy leveraging
Han-Kobayashi message splitting, which is shown to achieve the system capacity
region to within a constant bit. We quantify how the side-channel improve the
GDoF and DMT compared to a system without the extra orthogonal spectrum. The
insights gained from our analysis reveal: i) the tradeoff between spatial
resources from multiple antennas at different nodes and spectral resources of
the side-channel, and ii) the interplay between the channel uncertainty at the
transmitter and use of the side-channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0622</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0622</id><created>2014-02-04</created><authors><author><keyname>Strzebonski</keyname><forenames>Adam</forenames></author></authors><title>Divide-And-Conquer Computation of Cylindrical Algebraic Decomposition</title><categories>cs.SC cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a divide-and-conquer version of the Cylindrical Algebraic
Decomposition (CAD) algorithm. The algorithm represents the input as a Boolean
combination of subformulas, computes cylindrical algebraic decompositions of
solution sets of the subformulas, and combines the results. We propose a
graph-based heuristic to find a suitable partitioning of the input and present
empirical comparison with direct CAD computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0630</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0630</id><created>2014-02-04</created><authors><author><keyname>Abdulhamid</keyname><forenames>Shafii Muhammad</forenames></author><author><keyname>Ibrahim</keyname><forenames>Fasilat Folagbayo</forenames></author></authors><title>Controlling Citizens Cyber Viewing Using Enhanced Internet Content
  Filters</title><categories>cs.CY</categories><comments>8 pages</comments><journal-ref>I.J. Information Technology and Computer Science, 2013, 12, 56-63</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information passing through internet is generally unrestricted and
uncontrollable and a good web content filter acts very much like a sieve. This
paper looks at how citizens internet viewing can be controlled using content
filters to prevent access to illegal sites and malicious contents in Nigeria.
Primary data were obtained by administering 100 questionnaires. The data was
analyzed by a software package called Statistical Package for Social Sciences
(SPSS). The result of the study shows that 66.4% of the respondents agreed that
the internet is been abused and the abuse can be controlled by the use of
content filters. The PHP, MySQL and Apache were used to design a content filter
program. It was recommended that a lot still need to be done by public
organizations, academic institutions, government and its agencies especially
the Economic and Financial Crime Commission (EFCC) in Nigeria to control the
internet abuse by the under aged and criminals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0631</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0631</id><created>2014-02-04</created><authors><author><keyname>Bhalgama</keyname><forenames>S. R.</forenames></author><author><keyname>Kavar</keyname><forenames>C. C.</forenames></author><author><keyname>Parmar</keyname><forenames>S. S.</forenames></author></authors><title>LWRP: Low Power Consumption Weighting Replacement Policy using Buffer
  Memory</title><categories>cs.OS</categories><comments>4 pages, 2 figures, Published with International Journal of Computer
  Trends and Technology (IJCTT)</comments><journal-ref>IJCTT 7(3):147-150, January 2014. Published by Seventh Sense
  Research Group</journal-ref><doi>10.14445/22312803/IJCTT-V7P142</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As the performance gap between memory and processors has increased, then it
leads to the poor performance. Efficient virtual memory can overcome this
problem. And the efficiency of virtual memory depends on the replacement policy
used for cache. In this paper, our algorithm not only based on the time to last
access and frequency index but, we also consider the power consumption. We show
that Low Power Consumption Weighting Replacement Policy (LWRP) has better
performance and low power consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0635</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0635</id><created>2014-02-04</created><updated>2016-02-15</updated><authors><author><keyname>Osband</keyname><forenames>Ian</forenames></author><author><keyname>Van Roy</keyname><forenames>Benjamin</forenames></author><author><keyname>Wen</keyname><forenames>Zheng</forenames></author></authors><title>Generalization and Exploration via Randomized Value Functions</title><categories>stat.ML cs.AI cs.LG cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1307.4847</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose randomized least-squares value iteration (RLSVI) -- a new
reinforcement learning algorithm designed to explore and generalize efficiently
via linearly parameterized value functions. We explain why versions of
least-squares value iteration that use Boltzmann or epsilon-greedy exploration
can be highly inefficient, and we present computational results that
demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish
an upper bound on the expected regret of RLSVI that demonstrates
near-optimality in a tabula rasa learning context. More broadly, our results
suggest that randomized value functions offer a promising approach to tackling
a critical challenge in reinforcement learning: synthesizing efficient
exploration and effective generalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0642</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0642</id><created>2014-02-04</created><authors><author><keyname>Wentworth</keyname><forenames>Thomas</forenames></author><author><keyname>Ipsen</keyname><forenames>Ilse</forenames></author></authors><title>kappa_SQ: A Matlab package for randomized sampling of matrices with
  orthonormal columns</title><categories>math.NA cs.NA</categories><comments>Kappa_SQ can be downloaded from :
  http://www4.ncsu.edu/~tawentwo/kappaSQ_02042014.zip</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The kappa_SQ software package is designed to assist researchers working on
randomized row sampling. The package contains a collection of Matlab functions
along with a GUI that ties them all together and provides a platform for the
user to perform experiments. In particular, kappa_SQ is designed to do
experiments related to the two-norm condition number of a sampled matrix,
$\kappa(SQ)$, where $S$ is a row sampling matrix and $Q$ is a tall and skinny
matrix with orthonormal columns. Via a simple GUI, kappa_SQ can generate test
matrices, perform various types of row sampling, measure $\kappa(SQ)$,
calculate bounds and produce high quality plots of the results. All of the
important codes are written in separate Matlab function files in a standard
format which makes it easy for a user to either use the codes by themselves or
incorporate their own codes into the kappa_SQ package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0643</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0643</id><created>2014-02-04</created><updated>2015-02-13</updated><authors><author><keyname>Chowdhury</keyname><forenames>Muhammad F. I.</forenames><affiliation>ORCCA</affiliation></author><author><keyname>Jeannerod</keyname><forenames>Claude-Pierre</forenames><affiliation>LIP, Inria Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme</affiliation></author><author><keyname>Neiger</keyname><forenames>Vincent</forenames><affiliation>ORCCA, LIP, Inria Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme</affiliation></author><author><keyname>Schost</keyname><forenames>Eric</forenames><affiliation>ORCCA</affiliation></author><author><keyname>Villard</keyname><forenames>Gilles</forenames><affiliation>LIP, Inria Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme</affiliation></author></authors><title>Faster Algorithms for Multivariate Interpolation with Multiplicities and
  Simultaneous Polynomial Approximations</title><categories>cs.IT cs.SC math.IT</categories><comments>Version 2: Generalized our results about Problem 1 to distinct
  multiplicities. Added Section 4 which details several applications of our
  results to the decoding of Reed-Solomon codes (list-decoding with re-encoding
  technique, Wu algorithm, and soft-decoding). Reorganized the sections, added
  references and corrected typos</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interpolation step in the Guruswami-Sudan algorithm is a bivariate
interpolation problem with multiplicities commonly solved in the literature
using either structured linear algebra or basis reduction of polynomial
lattices. This problem has been extended to three or more variables; for this
generalization, all fast algorithms proposed so far rely on the lattice
approach. In this paper, we reduce this multivariate interpolation problem to a
problem of simultaneous polynomial approximations, which we solve using fast
structured linear algebra. This improves the best known complexity bounds for
the interpolation step of the list-decoding of Reed-Solomon codes,
Parvaresh-Vardy codes, and folded Reed-Solomon codes. In particular, for
Reed-Solomon list-decoding with re-encoding, our approach has complexity
$\mathcal{O}\tilde{~}(\ell^{\omega-1}m^2(n-k))$, where $\ell,m,n,k$ are the
list size, the multiplicity, the number of sample points and the dimension of
the code, and $\omega$ is the exponent of linear algebra; this accelerates the
previously fastest known algorithm by a factor of $\ell / m$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0645</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0645</id><created>2014-02-04</created><authors><author><keyname>Meier</keyname><forenames>Franziska</forenames></author><author><keyname>Hennig</keyname><forenames>Philipp</forenames></author><author><keyname>Schaal</keyname><forenames>Stefan</forenames></author></authors><title>Local Gaussian Regression</title><categories>cs.LG cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locally weighted regression was created as a nonparametric learning method
that is computationally efficient, can learn from very large amounts of data
and add data incrementally. An interesting feature of locally weighted
regression is that it can work with spatially varying length scales, a
beneficial property, for instance, in control problems. However, it does not
provide a generative model for function values and requires training and test
data to be generated identically, independently. Gaussian (process) regression,
on the other hand, provides a fully generative model without significant formal
requirements on the distribution of training data, but has much higher
computational cost and usually works with one global scale per input dimension.
Using a localising function basis and approximate inference techniques, we take
Gaussian (process) regression to increasingly localised properties and toward
the same computational complexity class as locally weighted regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0648</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0648</id><created>2014-02-04</created><authors><author><keyname>Das</keyname><forenames>Abhik Kumar</forenames></author><author><keyname>Banerjee</keyname><forenames>Siddhartha</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Linear Network Coding for Multiple Groupcast Sessions: An Interference
  Alignment Approach</title><categories>cs.IT math.IT</categories><comments>5 pages, appeared in ITW 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of linear network coding over communication networks,
representable by directed acyclic graphs, with multiple groupcast sessions: the
network comprises of multiple destination nodes, each desiring messages from
multiple sources. We adopt an interference alignment perspective, providing new
insights into designing practical network coding schemes as well as the impact
of network topology on the complexity of the alignment scheme. In particular,
we show that under certain (polynomial-time checkable) constraints on networks
with $K$ sources, it is possible to achieve a rate of $1/(L+d+1)$ per source
using linear network coding coupled with interference alignment, where each
destination receives messages from $L$ sources ($L &lt; K$), and $d$ is a
parameter, solely dependent on the network topology, that satisfies $0 \leq d &lt;
K-L$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0649</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0649</id><created>2014-02-04</created><updated>2014-07-08</updated><authors><author><keyname>Pajarinen</keyname><forenames>Joni</forenames></author><author><keyname>Kyrki</keyname><forenames>Ville</forenames></author></authors><title>Robotic manipulation of multiple objects as a POMDP</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates manipulation of multiple unknown objects in a crowded
environment. Because of incomplete knowledge due to unknown objects and
occlusions in visual observations, object observations are imperfect and action
success is uncertain, making planning challenging. We model the problem as a
partially observable Markov decision process (POMDP), which allows a general
reward based optimization objective and takes uncertainty in temporal evolution
and partial observations into account. In addition to occlusion dependent
observation and action success probabilities, our POMDP model also
automatically adapts object specific action success probabilities. To cope with
the changing system dynamics and performance constraints, we present a new
online POMDP method based on particle filtering that produces compact policies.
The approach is validated both in simulation and in physical experiments in a
scenario of moving dirty dishes into a dishwasher. The results indicate that:
1) a greedy heuristic manipulation approach is not sufficient, multi-object
manipulation requires multi-step POMDP planning, and 2) on-line planning is
beneficial since it allows the adaptation of the system dynamics model based on
actual experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0660</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0660</id><created>2014-02-04</created><updated>2014-04-02</updated><authors><author><keyname>Wang</keyname><forenames>Guoming</forenames></author></authors><title>Quantum Algorithms for Curve Fitting</title><categories>quant-ph cs.DS</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present quantum algorithms for estimating the best-fit parameters and the
quality of least-square curve fitting. The running times of these algorithms
are polynomial in $\log{n}$, $d$, $\kappa$, $\nu$, $\chi$, $1/\Phi$ and
$1/\epsilon$, where $n$ is the number of data points to be fitted, $d$ is the
dimension of feature vectors, $\kappa$ is the condition number of the design
matrix, $\nu$ and $\chi$ are some parameters reflecting the variances of the
design matrix and response vector, $\Phi$ is the fit quality, and $\epsilon$ is
the tolerable error. Different from previous quantum algorithms for these
tasks, our algorithms do not require the design matrix to be sparse, and they
do completely determine the fitted curve. They are developed by combining phase
estimation and the density matrix exponentiation technique for dense
Hamiltonian simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0670</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0670</id><created>2014-02-04</created><authors><author><keyname>Bambarasinghe</keyname><forenames>B. A. N. M.</forenames></author><author><keyname>Huruggamuwa</keyname><forenames>H. M. S.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author><author><keyname>Radhakrishnan</keyname><forenames>S.</forenames></author></authors><title>Axis2UNO: Web Services Enabled Openoffice.org</title><categories>cs.SE</categories><comments>6 pages, 4th International Conference on Information and Automation
  for Sustainability, 2008. ICIAFS 2008</comments><journal-ref>ICIAFS 2008. 437-442, 12-14 Dec. 2008</journal-ref><doi>10.1109/ICIAFS.2008.4783956</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Openoffice.org is a popular, free and open source office product. This
product is used by millions of people and developed, maintained and extended by
thousands of developers worldwide. Playing a dominant role in the web, web
services technology is serving millions of people every day. Axis2 is one of
the most popular, free and open source web service engines. The framework
presented in this paper, Axis2UNO, a combination of such two technologies is
capable of making a new era in office environment. Two other attempts to
enhance web services functionality in office products are Excel Web Services
and UNO Web Service Proxy. Excel Web Services is combined with Microsoft
SharePoint technology and exposes information sharing in a different
perspective within the proprietary Microsoft office products. UNO Web Service
Proxy is implemented with Java Web Services Developer Pack and enables basic
web services related functionality in Openoffice.org. However, the work
presented here is the first one to combine Openoffice.org and Axis2 and we
expect it to outperform the other efforts with the community involvement and
feature richness in those products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0671</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0671</id><created>2014-02-04</created><authors><author><keyname>Navarathna</keyname><forenames>Rajitha</forenames></author><author><keyname>Radhakrishnan</keyname><forenames>Swarnalatha</forenames></author><author><keyname>Ragel</keyname><forenames>Roshan</forenames></author></authors><title>Loop Unrolling in Multi-pipeline ASIP Design</title><categories>cs.PL</categories><comments>6 pages</comments><journal-ref>Navarathna, H. M R D B; Radhakrishnan, S.; Ragel, R.G., &quot;Loop
  unrolling in multi-pipeline ASIP design,&quot; Industrial and Information Systems
  (ICIIS), 2009 International Conference on , pp.306-311, 28-31 Dec. 2009</journal-ref><doi>10.1109/ICIINFS.2009.5429845</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Application Specific Instruction-set Processor (ASIP) is one of the popular
processor design techniques for embedded systems which allows customizability
in processor design without overly hindering design flexibility. Multi-pipeline
ASIPs were proposed to improve the performance of such systems by compromising
between speed and processor area. One of the problems in the multi-pipeline
design is the limited inherent instruction level parallelism (ILP) available in
applications. The ILP of application programs can be improved via a compiler
optimization technique known as loop unrolling. In this paper, we present how
loop unrolling effects the performance of multi-pipeline ASIPs. The
improvements in performance average around 15% for a number of benchmark
applications with the maximum improvement of around 30%. In addition, we
analyzed the variable of performance against loop unrolling factor, which is
the amount of unrolling we perform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0672</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0672</id><created>2014-02-04</created><authors><author><keyname>Karunathilake</keyname><forenames>A. K. B.</forenames></author><author><keyname>Balasuriya</keyname><forenames>B. M. D.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author></authors><title>User Friendly Line CAPTCHAs</title><categories>cs.HC cs.AI</categories><comments>6 pages</comments><journal-ref>Industrial and Information Systems (ICIIS), 2009 International
  Conference on , pp.210,215, 28-31 Dec. 2009</journal-ref><doi>10.1109/ICIINFS.2009.5429864</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CAPTCHAs or reverse Turing tests are real-time assessments used by programs
(or computers) to tell humans and machines apart. This is achieved by assigning
and assessing hard AI problems that could only be solved easily by human but
not by machines. Applications of such assessments range from stopping spammers
from automatically filling online forms to preventing hackers from performing
dictionary attack. Today, the race between makers and breakers of CAPTCHAs is
at a juncture, where the CAPTCHAs proposed are not even answerable by humans.
We consider such CAPTCHAs as non user friendly. In this paper, we propose a
novel technique for reverse Turing test - we call it the Line CAPTCHAs - that
mainly focuses on user friendliness while not compromising the security aspect
that is expected to be provided by such a system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0693</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0693</id><created>2014-02-04</created><authors><author><keyname>Shah</keyname><forenames>Kinjal N.</forenames></author><author><keyname>Rathod</keyname><forenames>Kirit R.</forenames></author><author><keyname>Agravat</keyname><forenames>Shardul J.</forenames></author></authors><title>A survey on Human Computer Interaction Mechanism Using Finger Tracking</title><categories>cs.HC</categories><comments>4 pages, 8 figures, International Journal of Computer Trends and
  Technology (IJCTT)</comments><journal-ref>IJCTT 7(3):174-177, January 2014</journal-ref><doi>10.14445/22312803/IJCTT-V7P148</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Human Computer Interaction (HCI) is a field in which developer makes a user
friendly system. User can interact with a computer system without using any
conventional peripheral devices. Marker is used to recognize hand movement
accurately &amp; successfully. Researchers establish the mechanism to interact with
computer system using computer vision. The interaction is better than normal
static keyboard and mouse. This paper represents most of innovative mechanisms
of the finger tracking used to interact with a computer system using computer
vision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0696</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0696</id><created>2014-02-04</created><authors><author><keyname>Abdulhamid</keyname><forenames>Shafii Muhammad</forenames></author><author><keyname>Latiff</keyname><forenames>Muhammad Shafie Abd</forenames></author><author><keyname>Bashir</keyname><forenames>Mohammed Bakri</forenames></author></authors><title>On-Demand Grid Provisioning Using Cloud Infrastructures and Related
  Virtualization Tools: A Survey and Taxonomy</title><categories>cs.DC</categories><comments>11 page, 6 figures, 1 table</comments><journal-ref>International Journal of Advanced Studies in Computer Science and
  Engineering IJASCSE, Volume 3, Issue 1, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent researches have shown that grid resources can be accessed by client
on-demand, with the help of virtualization technology in the Cloud. The virtual
machines hosted by the hypervisors are being utilized to build the grid network
within the cloud environment. The aim of this study is to survey some concepts
used for the on-demand grid provisioning using Infrastructure as a Service
Cloud and the taxonomy of its related components. This paper, discusses the
different approaches for on-demand grid using infrastructural Cloud, the issues
it tries to address and the implementation tools. The paper also, proposed an
extended classification for the virtualization technology used and a new
classification for the Grid-Cloud integration which was based on the
architecture, communication flow and the user demand for the Grid resources.
This survey, tools and taxonomies presented here will contribute as a guide in
the design of future architectures for further researches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0698</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0698</id><created>2014-02-04</created><authors><author><keyname>Dogra</keyname><forenames>D. P.</forenames></author><author><keyname>Nandam</keyname><forenames>K.</forenames></author><author><keyname>Majumdar</keyname><forenames>A. K.</forenames></author><author><keyname>Suralt</keyname><forenames>S.</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>J.</forenames></author><author><keyname>Majumdar</keyname><forenames>B.</forenames></author><author><keyname>Singh</keyname><forenames>A.</forenames></author><author><keyname>Mukherjee</keyname><forenames>S.</forenames></author></authors><title>User Friendly Implementation for Efficiently Conducting Hammersmith
  Infant Neurological Examination</title><categories>cs.CY</categories><journal-ref>IEEE Proceedings of the 12th International Conference on E-Health
  Networking, Application and Services (Healthcom), Lyon, France, pp.374-378,
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this work is to design a semi-automatic application that can be
used as an aid by the doctors for smoothly conducting Hammersmith Infant
Neurological Examination (IDNE). A simplified version of the examination which
provides a quantitative neurological assessment is used to design the
application. The application includes a methodology of conducting IDNE
examination suited to inexperienced staff, applicable to both neonatal and
post-neonatal infants. It also provides a facility to go through the previous
records of a patient that can help in diagnosing patients with high risk of
neurological disorder. A semi-automatic approach is proposed for skeleton
generation. The application has been installed in hospitals and currently in
operation. It is expected to increase the efficiency of conducting HINE using
the proposed application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0705</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0705</id><created>2014-02-04</created><updated>2015-06-19</updated><authors><author><keyname>Schmitz</keyname><forenames>Sylvain</forenames></author></authors><title>Implicational Relevance Logic is 2-EXPTIME-Complete</title><categories>cs.LO</categories><acm-class>F.2.2; F.4.1</acm-class><journal-ref>Proceedings of RTA-TLCA 2014, Lecture Notes in Computer Science
  8560, pp. 395--409, Springer, 2014</journal-ref><doi>10.1007/978-3-319-08918-8_27</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that provability in the implicational fragment of relevance logic is
complete for doubly exponential time, using reductions to and from coverability
in branching vector addition systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0708</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0708</id><created>2014-02-04</created><authors><author><keyname>Ulker</keyname><forenames>Ezgi Deniz</forenames></author><author><keyname>Ulker</keyname><forenames>Sadik</forenames></author></authors><title>Microstrip Coupler Design Using Bat Algorithm</title><categories>cs.NE</categories><comments>7 pages, 4 figures, 1 table</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), vol. 5, no. 1, January 2014, pp. 127-133</journal-ref><doi>10.5121/ijaia.2014.5110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary and swarm algorithms have found many applications in design
problems since todays computing power enables these algorithms to find
solutions to complicated design problems very fast. Newly proposed hybrid
algorithm, bat algorithm, has been applied for the design of microwave
microstrip couplers for the first time. Simulation results indicate that the
bat algorithm is a very fast algorithm and it produces very reliable results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0710</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0710</id><created>2014-02-04</created><updated>2014-09-09</updated><authors><author><keyname>Soltoggio</keyname><forenames>Andrea</forenames></author></authors><title>Short-term plasticity as cause-effect hypothesis testing in distal
  reward learning</title><categories>cs.NE q-bio.NC</categories><comments>Biological Cybernetics, September 2014</comments><doi>10.1007/s00422-014-0628-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asynchrony, overlaps and delays in sensory-motor signals introduce ambiguity
as to which stimuli, actions, and rewards are causally related. Only the
repetition of reward episodes helps distinguish true cause-effect relationships
from coincidental occurrences. In the model proposed here, a novel plasticity
rule employs short and long-term changes to evaluate hypotheses on cause-effect
relationships. Transient weights represent hypotheses that are consolidated in
long-term memory only when they consistently predict or cause future rewards.
The main objective of the model is to preserve existing network topologies when
learning with ambiguous information flows. Learning is also improved by biasing
the exploration of the stimulus-response space towards actions that in the past
occurred before rewards. The model indicates under which conditions beliefs can
be consolidated in long-term memory, it suggests a solution to the
plasticity-stability dilemma, and proposes an interpretation of the role of
short-term plasticity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0728</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0728</id><created>2014-02-04</created><updated>2014-05-08</updated><authors><author><keyname>Kowald</keyname><forenames>Dominik</forenames></author><author><keyname>Seitlinger</keyname><forenames>Paul</forenames></author><author><keyname>Trattner</keyname><forenames>Christoph</forenames></author><author><keyname>Ley</keyname><forenames>Tobias</forenames></author></authors><title>Forgetting the Words but Remembering the Meaning: Modeling Forgetting in
  a Verbal and Semantic Tag Recommender</title><categories>cs.IR</categories><acm-class>H.2.8; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We assume that recommender systems are more successful, when they are based
on a thorough understanding of how people process information. In the current
paper we test this assumption in the context of social tagging systems.
Cognitive research on how people assign tags has shown that they draw on two
interconnected levels of knowledge in their memory: on a conceptual level of
semantic fields or topics, and on a lexical level that turns patterns on the
semantic level into words. Another strand of tagging research reveals a strong
impact of time dependent forgetting on users' tag choices, such that recently
used tags have a higher probability being reused than &quot;older&quot; tags. In this
paper, we align both strands by implementing a computational theory of human
memory that integrates the two-level conception and the process of forgetting
in form of a tag recommender and test it in three large-scale social tagging
datasets (drawn from BibSonomy, CiteULike and Flickr).
  As expected, our results reveal a selective effect of time: forgetting is
much more pronounced on the lexical level of tags. Second, an extensive
evaluation based on this observation shows that a tag recommender
interconnecting both levels and integrating time dependent forgetting on the
lexical level results in high accuracy predictions and outperforms other
well-established algorithms, such as Collaborative Filtering, Pairwise
Interaction Tensor Factorization, FolkRank and two alternative time dependent
approaches. We conclude that tag recommenders can benefit from going beyond the
manifest level of word co-occurrences, and from including forgetting processes
on the lexical level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0729</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0729</id><created>2014-02-04</created><authors><author><keyname>Pappas</keyname><forenames>Nikolaos</forenames></author><author><keyname>Ephremides</keyname><forenames>Anthony</forenames></author><author><keyname>Traganitis</keyname><forenames>Apostolos</forenames></author></authors><title>Stability and Performance Issues of a Relay Assisted Multiple Access
  Scheme with MPR Capabilities</title><categories>cs.IT cs.NI math.IT</categories><comments>Accepted for publication in Elsevier Computer Communications. arXiv
  admin note: substantial text overlap with arXiv:1105.0452</comments><doi>10.1016/j.comcom.2014.01.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the impact of a relay node to a network with a finite
number of users-sources and a destination node. We assume that the users have
saturated queues and the relay node does not have packets of its own; we have
random access of the medium and the time is slotted. The relay node stores a
source packet that it receives successfully in its queue when the transmission
to the destination node has failed. The relay and the destination nodes have
multi-packet reception capabilities. We obtain analytical equations for the
characteristics of the relay's queue such as average queue length, stability
conditions etc. We also study the throughput per user and the aggregate
throughput for the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0736</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0736</id><created>2014-02-04</created><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author><author><keyname>S</keyname><forenames>Geethanjali.</forenames></author><author><keyname>M</keyname><forenames>Mekala.</forenames></author><author><keyname>T</keyname><forenames>Deepika.</forenames></author></authors><title>Water Eminence Scrutinizing Scheme Based On Zigbee and Wireless Antenna
  Expertise - A Study</title><categories>cs.NI</categories><comments>8 FIGURES AND 5 PAGES</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Network is the essential structure of a water eminence
monitoring by means of wireless sensor network technology To scrutinize water
quality greater than different sites as a synchronized application an estimable
system structural design constituted by spread sensor nodes and a base station
is suggested The nodes and base stations are linked using WSN technology like
Zigbee Base stations are related via Ethernet. Design and execution of a
prototype using WSN technology are the exigent work. Data are identified by
means of dissimilar sensors at the node plane to compute the parameters like
turbidity and oxygen quantity is transmitted via WSN to the support station
Information unruffled from the distant location is capable of displayed in
diagram setup as well as it is able to be calculated using dissimilar
replication tools at the supporting station. The recent methods have benefits
such as null amount carbon emission low power utilization more stretchy to put
together at distant locations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0746</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0746</id><created>2014-02-04</created><updated>2014-03-29</updated><authors><author><keyname>Zankl</keyname><forenames>Harald</forenames><affiliation>University of Innsbruck</affiliation></author><author><keyname>Korp</keyname><forenames>Martin</forenames><affiliation>University of Innsbruck</affiliation></author></authors><title>Modular Complexity Analysis for Term Rewriting</title><categories>cs.LO</categories><comments>33 pages; Special issue of RTA 2010</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (April 1,
  2014) lmcs:749</journal-ref><doi>10.2168/LMCS-10(1:19)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All current investigations to analyze the derivational complexity of term
rewrite systems are based on a single termination method, possibly preceded by
transformations. However, the exclusive use of direct criteria is problematic
due to their restricted power. To overcome this limitation the article
introduces a modular framework which allows to infer (polynomial) upper bounds
on the complexity of term rewrite systems by combining different criteria.
Since the fundamental idea is based on relative rewriting, we study how matrix
interpretations and match-bounds can be used and extended to measure complexity
for relative rewriting, respectively. The modular framework is proved strictly
more powerful than the conventional setting. Furthermore, the results have been
implemented and experiments show significant gains in power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0761</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0761</id><created>2014-02-04</created><authors><author><keyname>Sojakova</keyname><forenames>Kristina</forenames></author></authors><title>Higher Inductive Types as Homotopy-Initial Algebras</title><categories>cs.LO math.CT math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Homotopy Type Theory is a new field of mathematics based on the surprising
and elegant correspondence between Martin-Lofs constructive type theory and
abstract homotopy theory. We have a powerful interplay between these
disciplines - we can use geometric intuition to formulate new concepts in type
theory and, conversely, use type-theoretic machinery to verify and often
simplify existing mathematical proofs. A crucial ingredient in this new system
are higher inductive types, which allow us to represent objects such as
spheres, tori, pushouts, and quotients. We investigate a variant of higher
inductive types whose computational behavior is determined up to a higher path.
We show that in this setting, higher inductive types are characterized by the
universal property of being a homotopy-initial algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0779</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0779</id><created>2014-02-04</created><updated>2014-03-12</updated><authors><author><keyname>Perraudin</keyname><forenames>Nathanael</forenames></author><author><keyname>Shuman</keyname><forenames>David</forenames></author><author><keyname>Puy</keyname><forenames>Gilles</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>UNLocBoX A matlab convex optimization toolbox using proximal splitting
  methods</title><categories>cs.LG stat.ML</categories><comments>Draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays the trend to solve optimization problems is to use specific
algorithms rather than very general ones. The UNLocBoX provides a general
framework allowing the user to design his own algorithms. To do so, the
framework try to stay as close from the mathematical problem as possible. More
precisely, the UNLocBoX is a Matlab toolbox designed to solve convex
optimization problem of the form $$ \min_{x \in \mathcal{C}} \sum_{n=1}^K
f_n(x), $$ using proximal splitting techniques. It is mainly composed of
solvers, proximal operators and demonstration files allowing the user to
quickly implement a problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0780</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0780</id><created>2014-02-04</created><authors><author><keyname>Yaghubi</keyname><forenames>Setare</forenames></author><author><keyname>modiri</keyname><forenames>Nasser</forenames></author><author><keyname>Rafighi</keyname><forenames>Masoud</forenames></author></authors><title>Model performance indicators ERP systems</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Implementation process ERP is complex and expensive process. Typically always
be faced with many failures. Successfully implemented in an organization has
many challenges. Organizations in the deployment and success of the system
depends on several factors.One of the key factors in the successful deployment
of systems methodology is the implementation process. Methodology has several
indicators for successful implementation of ERP systems, we have examined. And
indicators for each of the methodologies have identified. The proposed method
is also an important indicator of the success of security controls and
indicators to be monitored and controlled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0785</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0785</id><created>2014-02-04</created><authors><author><keyname>Jiang</keyname><forenames>Hong</forenames></author><author><keyname>Huang</keyname><forenames>Gang</forenames></author><author><keyname>Wilford</keyname><forenames>Paul</forenames></author></authors><title>Signal to Noise Ratio in Lensless Compressive Imaging</title><categories>cs.CV</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the signal to noise ratio (SNR) in a lensless compressive imaging
(LCI) architecture. The architecture consists of a sensor of a single detecting
element and an aperture assembly of an array of programmable elements. LCI can
be used in conjunction with compressive sensing to capture images in a
compressed form of compressive measurements. In this paper, we perform SNR
analysis of the LCI and compare it with imaging with a pinhole or a lens. We
will show that the SNR in the LCI is independent of the image resolution, while
the SNR in either pinhole aperture imaging or lens aperture imaging decreases
as the image resolution increases. Consequently, the SNR in the LCI is much
higher if the image resolution is large enough.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0790</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0790</id><created>2014-02-04</created><updated>2014-06-04</updated><authors><author><keyname>Singer</keyname><forenames>Philipp</forenames></author><author><keyname>Helic</keyname><forenames>Denis</forenames></author><author><keyname>Taraghi</keyname><forenames>Behnam</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author></authors><title>Detecting Memory and Structure in Human Navigation Patterns Using Markov
  Chain Models of Varying Order</title><categories>cs.SI physics.soc-ph</categories><journal-ref>PLoS ONE, vol 9(7), 2014</journal-ref><doi>10.1371/journal.pone.0102070</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most frequently used models for understanding human navigation on
the Web is the Markov chain model, where Web pages are represented as states
and hyperlinks as probabilities of navigating from one page to another.
Predominantly, human navigation on the Web has been thought to satisfy the
memoryless Markov property stating that the next page a user visits only
depends on her current page and not on previously visited ones. This idea has
found its way in numerous applications such as Google's PageRank algorithm and
others. Recently, new studies suggested that human navigation may better be
modeled using higher order Markov chain models, i.e., the next page depends on
a longer history of past clicks. Yet, this finding is preliminary and does not
account for the higher complexity of higher order Markov chain models which is
why the memoryless model is still widely used. In this work we thoroughly
present a diverse array of advanced inference methods for determining the
appropriate Markov chain order. We highlight strengths and weaknesses of each
method and apply them for investigating memory and structure of human
navigation on the Web. Our experiments reveal that the complexity of higher
order models grows faster than their utility, and thus we confirm that the
memoryless model represents a quite practical model for human navigation on a
page level. However, when we expand our analysis to a topical level, where we
abstract away from specific page transitions to transitions between topics, we
find that the memoryless assumption is violated and specific regularities can
be observed. We report results from experiments with two types of navigational
datasets (goal-oriented vs. free form) and observe interesting structural
differences that make a strong argument for more contextual studies of human
navigation in future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0794</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0794</id><created>2014-02-04</created><authors><author><keyname>Anand</keyname><forenames>S.</forenames></author><author><keyname>Arazy</keyname><forenames>Ofer</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan</forenames></author><author><keyname>Nov</keyname><forenames>Oded</forenames></author></authors><title>A Game Theoretic Analysis of Collaboration in Wikipedia</title><categories>cs.GT cs.SI physics.soc-ph</categories><comments>16 pages 5 figures 2 tables</comments><journal-ref>Proceedings of the 4th International Conference on Decision and
  Game Theory for Security (GameSec 2013), LNCS 8252, pp. 29-44, Springer 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer production projects such as Wikipedia or open-source software
development allow volunteers to collectively create knowledge based products.
The inclusive nature of such projects poses difficult challenges for ensuring
trustworthiness and combating vandalism. Prior studies in the area deal with
descriptive aspects of peer production, failing to capture the idea that while
contributors collaborate, they also compete for status in the community and for
imposing their views on the product. In this paper we investigate collaborative
authoring in Wikipedia where contributors append and overwrite previous
contributions to a page. We assume that a contributors goal is to maximize
ownership of content sections such that content owned (or originated) by her
survived the most recent revision of the page. We model contributors
interactions to increase their content ownership as a noncooperative game where
a players utility is associated with content owned and cost is a function of
effort expended. Our results capture several real life aspects of contributors
interactions within peer production projects. We show that at the Nash
equilibrium there is an inverse relationship between the effort required to
make a contribution and the survival of a contributors content. In other words
majority of the content that survives is necessarily contributed by experts who
expend relatively less effort than non experts. An empirical analysis of
Wikipedia articles provides support for our models predictions. Implications
for research and practice are discussed in the context of trustworthy
collaboration as well as vandalism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0796</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0796</id><created>2014-02-04</created><authors><author><keyname>Lacoste</keyname><forenames>Alexandre</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author><author><keyname>Laviolette</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Marchand</keyname><forenames>Mario</forenames></author></authors><title>Sequential Model-Based Ensemble Optimization</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most tedious tasks in the application of machine learning is model
selection, i.e. hyperparameter selection. Fortunately, recent progress has been
made in the automation of this process, through the use of sequential
model-based optimization (SMBO) methods. This can be used to optimize a
cross-validation performance of a learning algorithm over the value of its
hyperparameters. However, it is well known that ensembles of learned models
almost consistently outperform a single model, even if properly selected. In
this paper, we thus propose an extension of SMBO methods that automatically
constructs such ensembles. This method builds on a recently proposed ensemble
construction paradigm known as agnostic Bayesian learning. In experiments on 22
regression and 39 classification data sets, we confirm the success of this
proposed approach, which is able to outperform model selection with SMBO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0804</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0804</id><created>2014-02-04</created><authors><author><keyname>Arjona</keyname><forenames>Jordi</forenames></author><author><keyname>Chatzipapas</keyname><forenames>Angelos</forenames></author><author><keyname>Anta</keyname><forenames>Antonio Fernandez</forenames></author><author><keyname>Mancuso</keyname><forenames>Vincenzo</forenames></author></authors><title>A Measurement-based Analysis of the Energy Consumption of Data Center
  Servers</title><categories>cs.DC cs.PF</categories><acm-class>B.8.2; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy consumption is a growing issue in data centers, impacting their
economic viability and their public image. In this work we empirically
characterize the power and energy consumed by different types of servers. In
particular, in order to understand the behavior of their energy and power
consumption, we perform measurements in different servers. In each of them, we
exhaustively measure the power consumed by the CPU, the disk, and the network
interface under different configurations, identifying the optimal operational
levels. One interesting conclusion of our study is that the curve that defines
the minimal CPU power as a function of the load is neither linear nor purely
convex as has been previously assumed. Moreover, we find that the efficiency of
the various server components can be maximized by tuning the CPU frequency and
the number of active cores as a function of the system and network load, while
the block size of I/O operations should be always maximized by applications. We
also show how to estimate the energy consumed by an application as a function
of some simple parameters, like the CPU load, and the disk and network
activity. We validate the proposed approach by accurately estimating the energy
of a map-reduce computation in a Hadoop platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0808</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0808</id><created>2014-02-03</created><authors><author><keyname>Jarollahi</keyname><forenames>Hooman</forenames></author><author><keyname>Onizawa</keyname><forenames>Naoya</forenames></author><author><keyname>Hanyu</keyname><forenames>Takahiro</forenames></author><author><keyname>Gross</keyname><forenames>Warren J.</forenames></author></authors><title>Associative Memories Based on Multiple-Valued Sparse Clustered Networks</title><categories>cs.NE</categories><comments>6 pages, Accepted in IEEE ISMVL 2014 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Associative memories are structures that store data patterns and retrieve
them given partial inputs. Sparse Clustered Networks (SCNs) are
recently-introduced binary-weighted associative memories that significantly
improve the storage and retrieval capabilities over the prior state-of-the art.
However, deleting or updating the data patterns result in a significant
increase in the data retrieval error probability. In this paper, we propose an
algorithm to address this problem by incorporating multiple-valued weights for
the interconnections used in the network. The proposed algorithm lowers the
error rate by an order of magnitude for our sample network with 60% deleted
contents. We then investigate the advantages of the proposed algorithm for
hardware implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0812</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0812</id><created>2014-02-04</created><authors><author><keyname>Antone</keyname><forenames>Alexandru Florin</forenames></author><author><keyname>Arsinte</keyname><forenames>Radu</forenames></author></authors><title>A Study on the Optimal Implementation of Statistical Multiplexing in DVB
  Distribution Systems</title><categories>cs.MM</categories><comments>8 pages, 17 figures</comments><journal-ref>Informatics and IT Today, Volume 1, July 2013, pp.19-27</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an overview of the main methods used to improve the
efficiency of DVB systems, based on multiplexing, through a study on the impact
of the multiplexing methods used in DVB, having as a final goal a better usage
of the data capacity and the possibility to insert new services into the
original DVB Transport Stream. This study revealed that not all DVB providers
are using statistical multiplexing. Based on this study, we were able to
propose a method to improve the original DVB stream, originated from DVB-S or
DVB-T providers. This method is proposing the detection of null packets,
removal and reinserting a new service, with a VBR content. The method developed
in this research can be implemented even in optimized statistical multiplexing
systems, due to a residual use of null packets for data rate adjustment. There
is no need to have access in the original stream multiplexer, since the method
allows the implementation on the fly, near to the end user. The proposed method
is proposed to be applied in DVB-S to DVB-C translation, using the computing
power of a PC or in a FPGA implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0815</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0815</id><created>2014-02-04</created><updated>2014-02-05</updated><authors><author><keyname>Matou&#x161;ek</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>Sedgwick</keyname><forenames>Eric</forenames></author><author><keyname>Tancer</keyname><forenames>Martin</forenames></author><author><keyname>Wagner</keyname><forenames>Uli</forenames></author></authors><title>Embeddability in the 3-sphere is decidable</title><categories>math.GT cs.CG</categories><comments>54 pages, 26 figures; few faulty references to figures in the first
  version fixed</comments><msc-class>57N10 (57M27, 57Q35, 05E45, 68U05, 68W99)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the following algorithmic problem is decidable: given a
$2$-dimensional simplicial complex, can it be embedded (topologically, or
equivalently, piecewise linearly) in $\mathbf{R}^3$? By a known reduction, it
suffices to decide the embeddability of a given triangulated 3-manifold $X$
into the 3-sphere $S^3$. The main step, which allows us to simplify $X$ and
recurse, is in proving that if $X$ can be embedded in $S^3$, then there is also
an embedding in which $X$ has a short meridian, i.e., an essential curve in the
boundary of $X$ bounding a disk in $S^3\setminus X$ with length bounded by a
computable function of the number of tetrahedra of $X$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0836</identifier>
 <datestamp>2014-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0836</id><created>2014-02-04</created><authors><author><keyname>Dasgupta</keyname><forenames>Sakyasingha</forenames></author></authors><title>Cognitive Aging as Interplay between Hebbian Learning and Criticality</title><categories>nlin.AO cs.NE q-bio.NC</categories><comments>Concise version of MSc thesis, Neural Models of the Ageing Brain,
  University of Edinburgh, 2010. Supervisor Dr. J. Michael Herrmann. 64 pages,
  20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive ageing seems to be a story of global degradation. As one ages there
are a number of physical, chemical and biological changes that take place.
Therefore it is logical to assume that the brain is no exception to this
phenomenon. The principle purpose of this project is to use models of neural
dynamics and learning based on the underlying principle of self-organised
criticality, to account for the age related cognitive effects. In this regard
learning in neural networks can serve as a model for the acquisition of skills
and knowledge in early development stages i.e. the ageing process and
criticality in the network serves as the optimum state of cognitive abilities.
Possible candidate mechanisms for ageing in a neural network are loss of
connectivity and neurons, increase in the level of noise, reduction in white
matter or more interestingly longer learning history and the competition among
several optimization objectives. In this paper we are primarily interested in
the affect of the longer learning history on memory and thus the optimality in
the brain. Hence it is hypothesized that prolonged learning in the form of
associative memory patterns can destroy the state of criticality in the
network. We base our model on Tsodyks and Markrams [49] model of dynamic
synapses, in the process to explore the effect of combining standard Hebbian
learning with the phenomenon of Self-organised criticality. The project mainly
consists of evaluations and simulations of networks of integrate and
fire-neurons that have been subjected to various combinations of neural-level
ageing effects, with the aim of establishing the primary hypothesis and
understanding the decline of cognitive abilities due to ageing, using one of
its important characteristics, a longer learning history.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0851</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0851</id><created>2014-02-04</created><updated>2014-07-12</updated><authors><author><keyname>van Bevern</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Mnich</keyname><forenames>Matthias</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author><author><keyname>Weller</keyname><forenames>Mathias</forenames></author></authors><title>Interval scheduling and colorful independent sets</title><categories>cs.DM cs.DS math.CO</categories><comments>This revision does not contain Theorem 7 of the first revision, whose
  proof contained an error</comments><msc-class>68M20</msc-class><acm-class>F.2.2; I.2.8; G.2.1; G.2.2</acm-class><journal-ref>Journal of Scheduling 18(5):449-469, 2015</journal-ref><doi>10.1007/s10951-014-0398-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous applications in scheduling, such as resource allocation or steel
manufacturing, can be modeled using the NP-hard Independent Set problem (given
an undirected graph and an integer k, find a set of at least k pairwise
non-adjacent vertices). Here, one encounters special graph classes like 2-union
graphs (edge-wise unions of two interval graphs) and strip graphs (edge-wise
unions of an interval graph and a cluster graph), on which Independent Set
remains NP-hard but admits constant-ratio approximations in polynomial time. We
study the parameterized complexity of Independent Set on 2-union graphs and on
subclasses like strip graphs. Our investigations significantly benefit from a
new structural &quot;compactness&quot; parameter of interval graphs and novel problem
formulations using vertex-colored interval graphs. Our main contributions are:
  1. We show a complexity dichotomy: restricted to graph classes closed under
induced subgraphs and disjoint unions, Independent Set is polynomial-time
solvable if both input interval graphs are cluster graphs, and is NP-hard
otherwise.
  2. We chart the possibilities and limits of effective polynomial-time
preprocessing (also known as kernelization).
  3. We extend Halld\'orsson and Karlsson (2006)'s fixed-parameter algorithm
for Independent Set on strip graphs parameterized by the structural parameter
&quot;maximum number of live jobs&quot; to show that the problem (also known as Job
Interval Selection) is fixed-parameter tractable with respect to the parameter
k and generalize their algorithm from strip graphs to 2-union graphs.
Preliminary experiments with random data indicate that Job Interval Selection
with up to fifteen jobs and 5*10^5 intervals can be solved optimally in less
than five minutes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0856</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0856</id><created>2014-02-04</created><authors><author><keyname>Huang</keyname><forenames>Hong</forenames></author><author><keyname>Al-Azzawi</keyname><forenames>Hussein</forenames></author><author><keyname>Brani</keyname><forenames>Hajar</forenames></author></authors><title>Network Traffic Anomaly Detection</title><categories>cs.CR</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a tutorial for network anomaly detection, focusing on
non-signature-based approaches. Network traffic anomalies are unusual and
significant changes in the traffic of a network. Networks play an important
role in today's social and economic infrastructures. The security of the
network becomes crucial, and network traffic anomaly detection constitutes an
important part of network security. In this paper, we present three major
approaches to non-signature-based network detection: PCA-based, sketch-based,
and signal-analysis-based. In addition, we introduce a framework that subsumes
the three approaches and a scheme for network anomaly extraction. We believe
network anomaly detection will become more important in the future because of
the increasing importance of network security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0858</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0858</id><created>2014-02-04</created><authors><author><keyname>Franek</keyname><forenames>Peter</forenames></author><author><keyname>Krcal</keyname><forenames>Marek</forenames></author></authors><title>Robust Satisfiability of Systems of Equations</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of \emph{robust satisfiability} of systems of nonlinear
equations, namely, whether for a given continuous function
$f:\,K\to\mathbb{R}^n$ on a~finite simplicial complex $K$ and $\alpha&gt;0$, it
holds that each function $g:\,K\to\mathbb{R}^n$ such that $\|g-f\|_\infty \leq
\alpha$, has a root in $K$. Via a reduction to the extension problem of maps
into a sphere, we particularly show that this problem is decidable in
polynomial time for every fixed $n$, assuming $\dim K \le 2n-3$. This is a
substantial extension of previous computational applications of
\emph{topological degree} and related concepts in numerical and interval
analysis. Via a reverse reduction we prove that the problem is undecidable when
$\dim K\ge 2n-2$, where the threshold comes from the \emph{stable range} in
homotopy theory. For the lucidity of our exposition, we focus on the setting
when $f$ is piecewise linear. Such functions can approximate general continuous
functions, and thus we get approximation schemes and undecidability of the
robust satisfiability in other possible settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0859</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0859</id><created>2014-02-04</created><updated>2015-03-07</updated><authors><author><keyname>Jampani</keyname><forenames>Varun</forenames></author><author><keyname>Nowozin</keyname><forenames>Sebastian</forenames></author><author><keyname>Loper</keyname><forenames>Matthew</forenames></author><author><keyname>Gehler</keyname><forenames>Peter V.</forenames></author></authors><title>The Informed Sampler: A Discriminative Approach to Bayesian Inference in
  Generative Computer Vision Models</title><categories>cs.CV cs.LG stat.ML</categories><comments>Appearing in Computer Vision and Image Understanding Journal (Special
  Issue on Generative Models in Computer Vision)</comments><doi>10.1016/j.cviu.2015.03.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer vision is hard because of a large variability in lighting, shape,
and texture; in addition the image signal is non-additive due to occlusion.
Generative models promised to account for this variability by accurately
modelling the image formation process as a function of latent variables with
prior beliefs. Bayesian posterior inference could then, in principle, explain
the observation. While intuitively appealing, generative models for computer
vision have largely failed to deliver on that promise due to the difficulty of
posterior inference. As a result the community has favoured efficient
discriminative approaches. We still believe in the usefulness of generative
models in computer vision, but argue that we need to leverage existing
discriminative or even heuristic computer vision methods. We implement this
idea in a principled way with an &quot;informed sampler&quot; and in careful experiments
demonstrate it on challenging generative models which contain renderer programs
as their components. We concentrate on the problem of inverting an existing
graphics rendering engine, an approach that can be understood as &quot;Inverse
Graphics&quot;. The informed sampler, using simple discriminative proposals based on
existing computer vision technology, achieves significant improvements of
inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0862</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0862</id><created>2014-02-04</created><authors><author><keyname>Landau</keyname><forenames>Zeph</forenames></author><author><keyname>Su</keyname><forenames>Francis Edward</forenames></author></authors><title>Fair Division and Redistricting</title><categories>cs.GT math.CO</categories><comments>20 pages; to appear, Contemporary Mathematics</comments><msc-class>Primary 91F10, Secondary 91B32, 91B12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Landau, Reid and Yershov provided a novel solution to the problem
of redistricting. Instead of trying to ensure fairness by restricting the shape
of the possible maps or by assigning the power to draw the map to nonbiased
entities, the solution ensures fairness by balancing competing interests
against each other. This kind of solution is an example of what are known as
&quot;fair division&quot; solutions--- such solutions involve the preferences of all
parties and are accompanied by rigorous guarantees of a specified well-defined
notion of fairness. In this expository article, we give an introduction to the
ideas of fair division in the context of this redistricting solution. Through
examples and discussion we clarify how fair division methods can play an
important role in a realistic redistricting solution by introducing an
interactive step that incorporates a certain kind of fairness that can be used
in concert with, and not a substitute for, other necessary or desired criteria
for a good redistricting solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0886</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0886</id><created>2014-02-04</created><authors><author><keyname>Mahmoodi</keyname><forenames>Maryam</forenames></author><author><keyname>Varnamkhasti</keyname><forenames>Mohammad Mahmoodi</forenames></author></authors><title>A Secure Communication in Mobile Agent System</title><categories>cs.CR</categories><comments>3 pages, Published with International Journal of Engineering Trends
  and Technology (IJETT)</comments><journal-ref>International Journal of Engineering Trends and Technology (IJETT)
  V6(4):186-188, december 2013. Published by Seventh Sense Research Group</journal-ref><doi>10.14445/22315381/IJETT-V6N4P133</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mobile agent is a software code with mobility which can be move from a
computer into another computers through network. The mobile agent paradigm
provides many benefits in developments of distributed application at the same
time introduce new requirements for security issues with these systems. In this
article we present a solution for protection agent from other agents attacks
with loging patterns of malicious agent and useing this log for communication.
We implemented our resolution by JADE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0897</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0897</id><created>2014-02-04</created><updated>2014-08-14</updated><authors><author><keyname>Boja&#x144;czyk</keyname><forenames>Miko&#x142;aj</forenames><affiliation>University of Warsaw</affiliation></author><author><keyname>Klin</keyname><forenames>Bartek</forenames><affiliation>University of Warsaw</affiliation></author><author><keyname>Lasota</keyname><forenames>S&#x142;awomir</forenames><affiliation>University of Warsaw</affiliation></author></authors><title>Automata theory in nominal sets</title><categories>cs.LO cs.FL</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (August
  15, 2014) lmcs:1157</journal-ref><doi>10.2168/LMCS-10(3:4)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study languages over infinite alphabets equipped with some structure that
can be tested by recognizing automata. We develop a framework for studying such
alphabets and the ensuing automata theory, where the key role is played by an
automorphism group of the alphabet. In the process, we generalize nominal sets
due to Gabbay and Pitts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0898</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0898</id><created>2014-02-04</created><authors><author><keyname>Wu</keyname><forenames>Rui</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author><author><keyname>Wang</keyname><forenames>Yi</forenames></author></authors><title>Interference Channels with Half-Duplex Source Cooperation</title><categories>cs.IT math.IT</categories><comments>final version submitted to IEEE Transaction on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance gain by allowing half-duplex source cooperation is studied
for Gaussian interference channels. The source cooperation is {\em in-band},
meaning that each source can listen to the other source's transmission, but
there is no independent (or orthogonal) channel between the sources. The
half-duplex constraint supposes that at each time instant the sources can
either transmit or listen, but not do both.
  Our main result is a characterization of the sum capacity when the
cooperation is bidirectional and the channel gains are symmetric. With
unidirectional cooperation, we essentially have a cognitive radio channel. By
requiring the primary to achieve a rate close to its link capacity, the best
possible rate for the secondary is characterized within a constant. Novel inner
and outer bounds are derived as part of these characterizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0911</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0911</id><created>2014-02-04</created><updated>2014-08-09</updated><authors><author><keyname>Meier</keyname><forenames>Rich</forenames></author><author><keyname>Cotilla-Sanchez</keyname><forenames>Eduardo</forenames></author><author><keyname>Fern</keyname><forenames>Alan</forenames></author></authors><title>A Policy Switching Approach to Consolidating Load Shedding and Islanding
  Protection Schemes</title><categories>cs.SY physics.soc-ph</categories><comments>Full Paper Accepted to PSCC 2014 - IEEE Co-Sponsored Conference. 7
  Pages, 2 Figures, 2 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years there have been many improvements in the reliability of
critical infrastructure systems. Despite these improvements, the power systems
industry has seen relatively small advances in this regard. For instance, power
quality deficiencies, a high number of localized contingencies, and large
cascading outages are still too widespread. Though progress has been made in
improving generation, transmission, and distribution infrastructure, remedial
action schemes (RAS) remain non-standardized and are often not uniformly
implemented across different utilities, ISOs, and RTOs. Traditionally, load
shedding and islanding have been successful protection measures in restraining
propagation of contingencies and large cascading outages. This paper proposes a
novel, algorithmic approach to selecting RAS policies to optimize the operation
of the power network during and after a contingency. Specifically, we use
policy-switching to consolidate traditional load shedding and islanding
schemes. In order to model and simulate the functionality of the proposed power
systems protection algorithm, we conduct Monte-Carlo, time-domain simulations
using Siemens PSS/E. The algorithm is tested via experiments on the IEEE-39
topology to demonstrate that the proposed approach achieves optimal power
system performance during emergency situations, given a specific set of RAS
policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0914</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0914</id><created>2014-02-04</created><authors><author><keyname>Linderman</keyname><forenames>Scott W.</forenames></author><author><keyname>Adams</keyname><forenames>Ryan P.</forenames></author></authors><title>Discovering Latent Network Structure in Point Process Data</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks play a central role in modern data analysis, enabling us to reason
about systems by studying the relationships between their parts. Most often in
network analysis, the edges are given. However, in many systems it is difficult
or impossible to measure the network directly. Examples of latent networks
include economic interactions linking financial instruments and patterns of
reciprocity in gang violence. In these cases, we are limited to noisy
observations of events associated with each node. To enable analysis of these
implicit networks, we develop a probabilistic model that combines
mutually-exciting point processes with random graph models. We show how the
Poisson superposition principle enables an elegant auxiliary variable
formulation and a fully-Bayesian, parallel inference algorithm. We evaluate
this new model empirically on several datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0915</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0915</id><created>2014-02-04</created><authors><author><keyname>Rippel</keyname><forenames>Oren</forenames></author><author><keyname>Gelbart</keyname><forenames>Michael A.</forenames></author><author><keyname>Adams</keyname><forenames>Ryan P.</forenames></author></authors><title>Learning Ordered Representations with Nested Dropout</title><categories>stat.ML cs.LG</categories><comments>11 pages, 5 figures. Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study ordered representations of data in which different
dimensions have different degrees of importance. To learn these representations
we introduce nested dropout, a procedure for stochastically removing coherent
nested sets of hidden units in a neural network. We first present a sequence of
theoretical results in the simple case of a semi-linear autoencoder. We
rigorously show that the application of nested dropout enforces identifiability
of the units, which leads to an exact equivalence with PCA. We then extend the
algorithm to deep models and demonstrate the relevance of ordered
representations to a number of applications. Specifically, we use the ordered
property of the learned codes to construct hash-based data structures that
permit very fast retrieval, achieving retrieval in time logarithmic in the
database size and independent of the dimensionality of the representation. This
allows codes that are hundreds of times longer than currently feasible for
retrieval. We therefore avoid the diminished quality associated with short
codes, while still performing retrieval that is competitive in speed with
existing methods. We also show that ordered representations are a promising way
to learn adaptive compression for efficient online data reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0916</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0916</id><created>2014-02-04</created><authors><author><keyname>Tamo</keyname><forenames>Itzhak</forenames></author><author><keyname>Barg</keyname><forenames>Alexander</forenames></author></authors><title>Bounds on Locally Recoverable Codes with Multiple Recovering Sets</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A locally recoverable code (LRC code) is a code over a finite alphabet such
that every symbol in the encoding is a function of a small number of other
symbols that form a recovering set. Bounds on the rate and distance of such
codes have been extensively studied in the literature. In this paper we derive
upper bounds on the rate and distance of codes in which every symbol has $t\geq
1$ disjoint recovering sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0918</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0918</id><created>2014-02-04</created><authors><author><keyname>Doostmohammadian</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Khan</keyname><forenames>Usman A.</forenames></author></authors><title>Graphic-theoretic distributed inference in social networks</title><categories>cs.SI cs.MA</categories><comments>submitted for journal publication</comments><doi>10.1109/JSTSP.2014.2314512</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider distributed inference in social networks where a phenomenon of
interest evolves over a given social interaction graph, referred to as the
\emph{social digraph}. For inference, we assume that a network of agents
monitors certain nodes in the social digraph and no agent may be able to
perform inference within its neighborhood; the agents must rely on inter-agent
communication. The key contributions of this paper include: (i) a novel
construction of the distributed estimator and distributed observability from
the first principles; (ii) a graph-theoretic agent classification that
establishes the importance and role of each agent towards inference; (iii)
characterizing the necessary conditions, based on the classification in (ii),
on the agent network to achieve distributed observability. Our results are
based on structured systems theory and are applicable to any parameter choice
of the underlying system matrix as long as the social digraph remains fixed. In
other words, any social phenomena that evolves (linearly) over a
structure-invariant social digraph may be considered--we refer to such systems
as Liner Structure-Invariant (LSI). The aforementioned contributions,
(i)--(iii), thus, only require the knowledge of the social digraph (topology)
and are independent of the social phenomena. We show the applicability of the
results to several real-wold social networks, i.e. social influence among
monks, networks of political blogs and books, and a co-authorship graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0921</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0921</id><created>2014-02-04</created><authors><author><keyname>Adebayo</keyname><forenames>Olawale</forenames></author><author><keyname>Abdulhamid</keyname><forenames>Shafii Muhammad</forenames></author></authors><title>E- Exams System for Nigerian Universities with Emphasis on Security and
  Result Integrity</title><categories>cs.CY</categories><comments>12 pages, 3 figures, 1 table. arXiv admin note: text overlap with
  arXiv:1202.3466, arXiv:1202.2516 by other authors</comments><report-no>ISSN 0858-7027</report-no><journal-ref>International Journal of the Computer, the Internet and Management
  (IJCIM), Volume 18, Number 2, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent employment and eventual widespread acceptance of electronic test
in examining students and various classes in Nigeria has created a significant
impact in the trends of educational history in the country. In this paper, we
examined the impacts, associated challenges and security lapses of the existing
electronic-examination system with the aim of ameliorating and developing a new
acceptable e-Exam system that takes care of the existing systems challenges and
security lapses. Six Universities that are already conducting e- Examination
were selected across the country for this research work. Twenty students that
participated in the e-exams and five members of staff were selected for
interview and questionnaire. Based on the analysis of the interviews and study
of the existing electronic examination system, some anomalies were discovered
and a new e-exams system was developed to eradicate these anomalies. The new
system uses data encryption in order to protect the questions sent to the
e-Examination center through the internet or intranet and a biometric
fingerprint authentication to screen the stakeholders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0925</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0925</id><created>2014-02-04</created><authors><author><keyname>Limal</keyname><forenames>Nicolas</forenames></author></authors><title>An Information Identity for State-dependent Channels with Feedback</title><categories>cs.IT math.IT</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this technical note, we investigate information quantities of
state-dependent communication channels with corrupted information fed back from
the receiver. We derive an information identity which can be interpreted as a
law of conservation of information flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0928</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0928</id><created>2014-02-04</created><updated>2014-10-05</updated><authors><author><keyname>Ainsworth</keyname><forenames>Scott G.</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author><author><keyname>Van de Sompel</keyname><forenames>Herbert</forenames></author></authors><title>A Framework for Evaluation of Composite Memento Temporal Coherence</title><categories>cs.DL</categories><comments>10 pages, 12 figures, 6 tables. Version 3 corrects predicates 13, 14,
  16, 17, 19, and 20</comments><acm-class>H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most archived HTML pages embed other web resources, such as images and
stylesheets. Playback of the archived web pages typically provides only the
capture date (or Memento-Datetime) of the root resource and not the
Memento-Datetime of the embedded resources. In the course of our research, we
have discovered that the Memento-Datetime of embedded resources can be up to
several years in the future or past, relative to the Memento-Datetime of the
embedding root resource. We introduce a framework for assessing temporal
coherence between a root resource and its embedded resource depending on
Memento-Datetime, Last-Modified datetime, and entity body.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0929</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0929</id><created>2014-02-04</created><updated>2014-06-11</updated><authors><author><keyname>Snoek</keyname><forenames>Jasper</forenames></author><author><keyname>Swersky</keyname><forenames>Kevin</forenames></author><author><keyname>Zemel</keyname><forenames>Richard S.</forenames></author><author><keyname>Adams</keyname><forenames>Ryan P.</forenames></author></authors><title>Input Warping for Bayesian Optimization of Non-stationary Functions</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian optimization has proven to be a highly effective methodology for the
global optimization of unknown, expensive and multimodal functions. The ability
to accurately model distributions over functions is critical to the
effectiveness of Bayesian optimization. Although Gaussian processes provide a
flexible prior over functions which can be queried efficiently, there are
various classes of functions that remain difficult to model. One of the most
frequently occurring of these is the class of non-stationary functions. The
optimization of the hyperparameters of machine learning algorithms is a problem
domain in which parameters are often manually transformed a priori, for example
by optimizing in &quot;log-space,&quot; to mitigate the effects of spatially-varying
length scale. We develop a methodology for automatically learning a wide family
of bijective transformations or warpings of the input space using the Beta
cumulative distribution function. We further extend the warping framework to
multi-task Bayesian optimization so that multiple tasks can be warped into a
jointly stationary space. On a set of challenging benchmark optimization tasks,
we observe that the inclusion of warping greatly improves on the
state-of-the-art, producing better results faster and more reliably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0932</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0932</id><created>2014-02-04</created><authors><author><keyname>Rakhshan</keyname><forenames>Ali</forenames></author><author><keyname>Pishro-Nik</keyname><forenames>Hossein</forenames></author><author><keyname>Fisher</keyname><forenames>Donald L.</forenames></author><author><keyname>Nekoui</keyname><forenames>Mohammad</forenames></author></authors><title>Tuning Collision Warning Algorithms to Individual Drivers for Design of
  Active Safety Systems</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every year, many people are killed and injured in highway traffic accidents.
In order to reduce such casualties, collisions warning systems has been studied
extensively. These systems are built by taking the driver reaction times into
account. However, most of the existing literature focuses on characterizing how
driver reaction times vary across an entire population. Therefore, many of the
warnings that are given turn out to be false alarms. A false alarm occurs
whenever a warning is sent, but it is not needed. This would nagate any safety
benefit of the system, and could even reduce the overall safety if warnings
become a distraction. In this paper, we propose our solution to address the
described problem; First, we briefly describe our method for estimating the
distribution of brake response times for a particular driver using data from a
Vehicular Ad-Hoc Network (VANET) system. Then, we investigate how brake
response times of individual drivers can be used in collision warning
algorithms to reduce false alarm rates while still maintaining a high level of
safety. This will yield a system that is overall more reliable and trustworthy
for drivers, which could lead to wider adoption and applicability for V2V/V2I
communication systems. Moreover, we show how false alarm rate varies with
respect to probability of accident. Our simulation results show that by
individualizing collision warnings the number of false alarms can be reduced
more than $50\%$. Then, we conclude safety applications could potentially take
full advantage of being customized to an individual's characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0936</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0936</id><created>2014-02-05</created><updated>2014-03-27</updated><authors><author><keyname>Baghaie</keyname><forenames>Ahmadreza</forenames></author><author><keyname>Yu</keyname><forenames>Zeyun</forenames></author></authors><title>An Optimization Method For Slice Interpolation Of Medical Images</title><categories>cs.CV cs.CE</categories><comments>4 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Slice interpolation is a fast growing field in medical image processing.
Intensity-based interpolation and object-based interpolation are two major
groups of methods in the literature. In this paper, we describe an
object-oriented, optimization method based on a modified version of
curvature-based image registration, in which a displacement field is computed
for the missing slice between two known slices and used to interpolate the
intensities of the missing slice. The proposed approach is evaluated
quantitatively by using the Mean Squared Difference (MSD) as a metric. The
produced results also show visual improvement in preserving sharp edges in
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0945</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0945</id><created>2014-02-05</created><authors><author><keyname>Eslami</keyname><forenames>Seyed Gholamreza</forenames></author><author><keyname>Peiravi</keyname><forenames>Ali</forenames></author><author><keyname>Molavi</keyname><forenames>Behzad</forenames></author></authors><title>A Survey on Factors Affecting Iran's Fuel Rationing Smart Card User
  Acceptance and Security</title><categories>cs.CY</categories><comments>5 pages, 1 figure, 5 tables</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Smart card technology has resulted in vast developments in many aspects of
modern human life. User acceptance of fuel rationing smart cards based on
adoption model involves many factors such as: satisfaction, security, external
variables, attitude toward using, etc. In this study, user acceptance and
security factors for fuel rationing smart cards in Iran have been evaluated
based on an adoption model by distributing a questionnaire among UTM
(University Technology Malaysia) Iranian students, MMU (Multimedia University)
Iranian students, either asking by e-mail from people who are not available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0972</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0972</id><created>2014-02-05</created><updated>2014-03-05</updated><authors><author><keyname>Berger</keyname><forenames>Thierry P.</forenames></author></authors><title>Construction of dyadic MDS matrices for cryptographic applications</title><categories>cs.CR cs.IT math.IT</categories><comments>This paper has been withdrawn. Indeed, similar results to those
  presented in this paper have been obtained in [1]. [1] A. M. Youssef, S.
  Mister, and S. E. Tavares, &quot;On the design of linear transformations for
  substitution permutation encryption networks,&quot; SAC'97, 1997, pp. 40--48</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many recent block ciphers use Maximum Distance Separable (MDS) matrices in
their diffusion layer. The main objective of this operation is to spread as
much as possible the differences between the outputs of nonlinear Sboxes. So
they generally act at nibble or at byte level. The MDS matrices are associated
to MDS codes of ratio 1/2. The most famous example is the MixColumns operation
of the AES block cipher.
  In this example, the MDS matrix was carefully chosen to obtain compact and
efficient implementations. However, this MDS matrix is dedicated to 8-bit
words, and is not always adapted to lightweight applications. Recently, several
studies have been devoted to the construction of recursive diffusion layers.
Such a method allows to apply an MDS matrix using an iterative process which
looks like a Feistel network with linear functions instead of nonlinear.
  Our approach is quite different. We present a generic construction of
classical MDS matrices that are not recursively computed, but that are strong
symmetric in order to either accelerate their evaluation with a minimal number
of look-up tables, or to perform this evaluation with a minimal number of gates
in a circuit. We call this particular kind of matrices &quot;dyadic matrices&quot;, since
they are related to dyadic codes. We study some basic properties of such
matrices. We introduce a generic construction of involutive dyadic MDS matrices
from Reed Solomon codes. Finally, we discuss the implementation aspects of
these dyadic MDS matrices in order to build efficient block ciphers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0978</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0978</id><created>2014-02-05</created><authors><author><keyname>Zarezade</keyname><forenames>Ali</forenames></author><author><keyname>Rabiee</keyname><forenames>Hamid R.</forenames></author><author><keyname>Soltani-Farani</keyname><forenames>Ali</forenames></author><author><keyname>Khajenezhad</keyname><forenames>Ahmad</forenames></author></authors><title>Patchwise Joint Sparse Tracking with Occlusion Detection</title><categories>cs.CV</categories><doi>10.1109/TIP.2014.2346029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a robust tracking approach to handle challenges such as
occlusion and appearance change. Here, the target is partitioned into a number
of patches. Then, the appearance of each patch is modeled using a dictionary
composed of corresponding target patches in previous frames. In each frame, the
target is found among a set of candidates generated by a particle filter, via a
likelihood measure that is shown to be proportional to the sum of
patch-reconstruction errors of each candidate. Since the target's appearance
often changes slowly in a video sequence, it is assumed that the target in the
current frame and the best candidates of a small number of previous frames,
belong to a common subspace. This is imposed using joint sparse representation
to enforce the target and previous best candidates to have a common sparsity
pattern. Moreover, an occlusion detection scheme is proposed that uses
patch-reconstruction errors and a prior probability of occlusion, extracted
from an adaptive Markov chain, to calculate the probability of occlusion per
patch. In each frame, occluded patches are excluded when updating the
dictionary. Extensive experimental results on several challenging sequences
shows that the proposed method outperforms state-of-the-art trackers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0988</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0988</id><created>2014-02-05</created><authors><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author></authors><title>The inverse problem for power distributions in committees</title><categories>cs.GT</categories><comments>46 pages, 2 tables</comments><msc-class>91B12, 94C10</msc-class><doi>10.1007/s00355-015-0946-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several power indices have been introduced in the literature in order to
measure the influence of individual committee members on the aggregated
decision. Here we ask the inverse question and aim to design voting rules for a
committee such that a given desired power distribution is met as closely as
possible. We present an exact algorithm for a large class of different power
indices based on integer linear programming. With respect to negative
approximation results we generalize the approach of Alon and Edelman who
studied power distributions for the Banzhaf index, where most of the power is
concentrated on few coordinates. It turned out that each Banzhaf vector of an
n-member committee that is near to such a desired power distribution, has to be
also near to the Banzhaf vector of a k-member committee. We show that such
Alon-Edelman type results are possible for other power indices like e.g. the
Public Good index or the Coleman index to prevent actions, while they are
principally impossible for e.g. the Johnston index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0993</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0993</id><created>2014-02-05</created><authors><author><keyname>Alaa</keyname><forenames>Ahmed M.</forenames></author></authors><title>Defeating the Eavesdropper: On the Achievable Secrecy Capacity using
  Reconfigurable Antennas</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the transmission of confidential messages over
slow fading wireless channels in the presence of an eavesdropper. We propose a
transmission scheme that employs a single reconfigurable antenna at each of the
legitimate partners, whereas the eavesdropper uses a single conventional
antenna. A reconfigurable antenna can switch its propagation characteristics
over time and thus it perceives different fading channels. It is shown that
without channel side information (CSI) at the legitimate partners, the main
channel can be transformed into an ergodic regime offering a \textit{secrecy
capacity} gain for strict outage constraints. If the legitimate partners have
partial or full channel side information (CSI), a sort of selection diversity
can be applied boosting the maximum secret communication rate. In this case,
fading acts as a friend not a foe.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0997</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0997</id><created>2014-02-05</created><authors><author><keyname>Yu</keyname><forenames>Liliana Pasquale. Yijun</forenames></author><author><keyname>Cavallaro</keyname><forenames>Luca</forenames></author><author><keyname>Salehie</keyname><forenames>Mazeiar</forenames></author><author><keyname>Tun</keyname><forenames>Thein Than</forenames></author><author><keyname>Nuseibeh</keyname><forenames>Bashar</forenames></author></authors><title>Engineering Adaptive Digital Investigations using Forensics Requirements</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A digital forensic investigation aims to collect and analyse the evidence
necessary to demonstrate a potential hypothesis of a digital crime. Despite the
availability of several digital forensics tools, investigators still approach
each crime case from scratch, postulating potential hypotheses and analysing
large volumes of data. This paper proposes to explicitly model forensic
requirements in order to engineer software systems that are forensic-ready and
guide the activities of a digital investigation. Forensic requirements relate
some speculative hypotheses of a crime to the evidence that should be collected
and analysed in a crime scene. In contrast to existing approaches, we propose
to perform proactive activities to preserve important - potentially ephemeral -
evidence, depending on the risk of a crime to take place. Once an investigation
starts, the evidence collected proactively is analysed to assess if some of the
speculative hypotheses of a crime hold and what further evidence is necessary
to support them. For each hypothesis that is satisfied, a structured argument
is generated to demonstrate how the evidence collected supports that
hypothesis. Our evaluation results suggest that the approach provides correct
investigative findings and reduces significantly the amount of evidence to be
collected and the hypotheses to be analysed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.0998</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.0998</id><created>2014-02-05</created><authors><author><keyname>Reiss</keyname><forenames>Julius</forenames></author></authors><title>A family of energy stable, skew-symmetric finite difference schemes on
  collocated grids</title><categories>physics.flu-dyn cs.NA math.NA</categories><msc-class>76D05, 35L65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple scheme for incompressible, constant density flows is presented,
which avoids odd-even decoupling for the Laplacian on a collocated grids.
Energy stability is implied by maintaining strict energy conservation. Momentum
is conserved. Arbitrary order in space and time can easily be obtained. The
conservation properties hold on transformed grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1001</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1001</id><created>2014-02-05</created><authors><author><keyname>Nicolau</keyname><forenames>Hugo</forenames></author><author><keyname>Guerreiro</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Guerreiro</keyname><forenames>Tiago</forenames></author></authors><title>Stressing the Boundaries of Mobile Accessibility</title><categories>cs.HC</categories><comments>3 pages, two figures, ACM CHI 2013 Mobile Accessibility Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile devices gather the communication capabilities as no other gadget.
Plus, they now comprise a wider set of applications while still maintaining
reduced size and weight. They have started to include accessibility features
that enable the inclusion of disabled people. However, these inclusive efforts
still fall short considering the possibilities of such devices. This is mainly
due to the lack of interoperability and extensibility of current mobile
operating systems (OS). In this paper, we present a case study of a
multi-impaired person where access to basic mobile applications was provided in
an applicational basis. We outline the main flaws in current mobile OS and
suggest how these could further empower developers to provide accessibility
components. These could then be compounded to provide system-wide inclusion to
a wider range of (multi)-impairments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1010</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1010</id><created>2014-02-05</created><updated>2014-07-22</updated><authors><author><keyname>Sandberg</keyname><forenames>Henrik</forenames></author><author><keyname>Delvenne</keyname><forenames>Jean-Charles</forenames></author><author><keyname>Newton</keyname><forenames>Nigel J.</forenames></author><author><keyname>Mitter</keyname><forenames>Sanjoy K.</forenames></author></authors><title>Maximum work extraction and implementation costs for non-equilibrium
  Maxwell's demons</title><categories>cond-mat.stat-mech cs.SY math.OC</categories><journal-ref>Phys. Rev. E 90, 042119, October 2014</journal-ref><doi>10.1103/PhysRevE.90.042119</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this theoretical study, we determine the maximum amount of work
extractable in finite time by a demon performing continuous measurements on a
quadratic Hamiltonian system subjected to thermal fluctuations, in terms of the
information extracted from the system. This is in contrast to many recent
studies that focus on demons' maximizing the extracted work over received
information, and operate close to equilibrium. The maximum work demon is found
to apply a high-gain continuous feedback using a Kalman-Bucy estimate of the
system state. A simple and concrete electrical implementation of the feedback
protocol is proposed, which allows for analytic expressions of the flows of
energy and entropy inside the demon. This let us show that any implementation
of the demon must necessarily include an external power source, which we prove
both from classical thermodynamics arguments and from a version of Landauer's
memory erasure argument extended to non-equilibrium linear systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1012</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1012</id><created>2014-02-05</created><authors><author><keyname>Ishak</keyname><forenames>Siti Nurul Hayatie</forenames></author><author><keyname>Nordin</keyname><forenames>Ariza</forenames></author></authors><title>Sequencing Participatory Action Research and i* Modeling Framework in
  Capturing Multiple Roles Requirements</title><categories>cs.SE</categories><comments>Siti Nurul Hayatie Ishak, Ariza Nordin&quot;Sequencing Participatory
  Action Research and i* Modeling Framework in Capturing Multiple Roles
  Requirements&quot;, International Journal of Soft Computing and Software
  Engineering [JSCSE], Vol. 3, No. 3, pp. 123-130, 2013, Doi:
  10.7321/jscse.v3.n3.20</comments><doi>10.7321/jscse.v3.n3.20</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents the conceptual framework for sequencing of Participatory
Action Research (PAR) methodology with the implementation of i* modeling
framework in capturing multiple roles requirements. There are multiple roles
involved in the development of information system, thus it involves with
difference users requirements and preferences, context as well as the demands
which become a challenge in development of system. This is due to these roles
where information of the project monitoring is perceived in accordance to their
role and domain. In the development of information systems, requirement
engineering is a vital methodology. Requirement engineering (RE) consists of
several phases which elicitation is a crucial phase in RE since it requires
researcher to gather the requirement from the users. Methods of eliciting
requirements are now more co-operative. Based on the preliminary study of
construction-based in Malaysia, evidence of dynamic requirements has been
observed according to the environments, economic, technology and manpower
involved in the construction project. An adaptive design for project monitoring
is needed which allow the physical system to self-adapt in response to the
changing environments. Adaptive design requires selecting the right techniques
of requirements elicitation. The conceptual framework defined shall be used to
elicit requirements from a local construction company.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1027</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1027</id><created>2014-02-05</created><updated>2015-06-01</updated><authors><author><keyname>Hakami</keyname><forenames>Vesal</forenames></author><author><keyname>Dehghan</keyname><forenames>Mehdi</forenames></author></authors><title>Learning Stationary Correlated Equilibria in Constrained General-Sum
  Stochastic Games</title><categories>cs.GT cs.MA</categories><comments>25 pages, 2 figures, 5 tables in IEEE Transactions on Cybernetics,
  Vol. XX, No. XX, 2015</comments><doi>10.1109/TCYB.2015.2453165</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study constrained general-sum stochastic games with unknown Markovian
dynamics. A distributed constrained no-regret Q-learning scheme (CNRQ) is
presented to guarantee convergence to the set of stationary correlated
equilibria of the game. Prior art addresses the unconstrained case only, is
structured with nested control loops, and has no convergence result. CNRQ is
cast as a single-loop three-timescale asynchronous stochastic approximation
algorithm with set-valued update increments. A rigorous convergence analysis
with differential inclusion arguments is given which draws on recent extensions
of the theory of stochastic approximation to the case of asynchronous recursive
inclusions with set-valued mean fields. Numerical results are given for the
exemplary application of CNRQ to decentralized resource control in
heterogeneous wireless networks (HetNets).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1036</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1036</id><created>2014-02-05</created><authors><author><keyname>Guerreiro</keyname><forenames>Tiago</forenames></author></authors><title>User-Sensitive Mobile Interfaces: Accounting for Individual Differences
  amongst the Blind</title><categories>cs.HC</categories><comments>330 pages, PhD Thesis, Technical University of Lisbon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile phones pervade our daily lives and play ever expanding roles in many
contexts. Their ubiquitousness makes them pivotal in empowering disabled
people. However, if no inclusive approaches are provided, it becomes a strong
vehicle of exclusion. Even though current solutions try to compensate for the
lack of sight, not all information reaches the blind user. Good spatial ability
is still required to make sense of the device and its interface, as well as the
need to memorize positions on screen or keys and associated actions in a
keypad. Those problems are compounded by many individual attributes such as
age, age of blindness onset or tactile sensitivity which often are forgotten by
designers. Worse, the entire blind population is recurrently thought of as
homogeneous (often stereotypically so). Thus all users face the same solutions,
ignoring their specific capabilities and needs. We usually ignore this
diversity as we have the ability to adapt and become experts in interfaces that
were probably maladjusted to begin with. This adaptation is not always within
reach. Interaction with mobile devices is highly visually demanding which
widens this gap amongst blind people. It is paramount to understand the impact
of individual differences and their relationship with demands to enable the
deployment of more inclusive solutions. We explore individual differences among
blind people and assess how they are related with mobile interface demands,
both at low (e.g. performing an on-screen gesture) and high level (text-entry)
tasks. Results confirmed that different ability levels have significant impact
on the performance attained by a blind person. Particularly, otherwise ignored
attributes like tactile acuity, pressure sensitivity, spatial ability or verbal
IQ have shown to be matched with specific mobile demands and parametrizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1037</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1037</id><created>2014-02-05</created><authors><author><keyname>Guerreiro</keyname><forenames>Tiago</forenames></author><author><keyname>Nicolau</keyname><forenames>Hugo</forenames></author><author><keyname>Oliveira</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Jorge</keyname><forenames>Joaquim</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Daniel</forenames></author></authors><title>Understanding Individual Differences: Towards Effective Mobile Interface
  Design and Adaptation for the Blind</title><categories>cs.HC</categories><comments>3 pages, CHI 2011 Workshop on Dynamic Accessibility</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  No two people are alike. We usually ignore this diversity as we have the
capability to adapt and, without noticing, become experts in interfaces that
were probably misadjusted to begin with. This adaptation is not always at the
user's reach. One neglected group is the blind. Spatial ability, memory, and
tactile sensitivity are some characteristics that diverge between users.
Regardless, all are presented with the same methods ignoring their capabilities
and needs. Interaction with mobile devices is highly visually demanding which
widens the gap between blind people. Our research goal is to identify the
individual attributes that influence mobile interaction, considering the blind,
and match them with mobile interaction modalities in a comprehensive and
extensible design space. We aim to provide knowledge both for device design,
device prescription and interface adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1047</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1047</id><created>2014-02-05</created><authors><author><keyname>Feige</keyname><forenames>Uriel</forenames></author></authors><title>On robustly asymmetric graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  O'Donnell, Wright, Wu and Zhou [SODA 2014] introduced the notion of robustly
asymmetric graphs. Roughly speaking, these are graphs in which for every $0 \le
\rho \le 1$, every permutation that permutes a $\rho$ fraction of the vertices
maps a $\Theta(\rho)$ fraction of the edges to non-edges. We show that there
are graphs for which the constant hidden in the $\Theta$ notation is roughly~1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1051</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1051</id><created>2014-02-05</created><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author><author><keyname>Duval</keyname><forenames>Dominique</forenames><affiliation>LJK</affiliation></author><author><keyname>Reynaud</keyname><forenames>Jean-Claude</forenames><affiliation>RC</affiliation></author></authors><title>Breaking a monad-comonad symmetry between computational effects</title><categories>cs.LO math.CT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1310.0605</comments><proxy>ccsd</proxy><journal-ref>Mathematical Structures in Computer Science 22, 4 (2012) p.719-722</journal-ref><doi>10.1017/S0960129511000752</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational effects may often be interpreted in the Kleisli category of a
monad or in the coKleisli category of a comonad. The duality between monads and
comonads corresponds, in general, to a symmetry between construction and
observation, for instance between raising an exception and looking up a state.
Thanks to the properties of adjunction one may go one step further: the
coKleisli-on-Kleisli category of a monad provides a kind of observation with
respect to a given construction, while dually the Kleisli-on-coKleisli category
of a comonad provides a kind of construction with respect to a given
observation. In the previous examples this gives rise to catching an exception
and updating a state. However, the interpretation of computational effects is
usually based on a category which is not self-dual, like the category of sets.
This leads to a breaking of the monad-comonad duality. For instance, in a
distributive category the state effect has much better properties than the
exception effect. This remark provides a novel point of view on the usual
mechanism for handling exceptions. The aim of this paper is to build an
equational semantics for handling exceptions based on the coKleisli-on-Kleisli
category of the monad of exceptions. We focus on n-ary functions and
conditionals. We propose a programmer's language for exceptions and we prove
that it has the required behaviour with respect to n-ary functions and
conditionals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1072</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1072</id><created>2014-02-05</created><authors><author><keyname>Sayin</keyname><forenames>Muhammed O.</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author></authors><title>Compressive Diffusion Strategies Over Distributed Networks for Reduced
  Communication Load</title><categories>cs.SY cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2014.2347917</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the compressive diffusion strategies over distributed networks based
on the diffusion implementation and adaptive extraction of the information from
the compressed diffusion data. We demonstrate that one can achieve a comparable
performance with the full information exchange configurations, even if the
diffused information is compressed into a scalar or a single bit. To this end,
we provide a complete performance analysis for the compressive diffusion
strategies. We analyze the transient, steady-state and tracking performance of
the configurations in which the diffused data is compressed into a scalar or a
single-bit. We propose a new adaptive combination method improving the
convergence performance of the compressive diffusion strategies further. In the
new method, we introduce one more freedom-of-dimension in the combination
matrix and adapt it by using the conventional mixture approach in order to
enhance the convergence performance for any possible combination rule used for
the full diffusion configuration. We demonstrate that our theoretical analysis
closely follow the ensemble averaged results in our simulations. We provide
numerical examples showing the improved convergence performance with the new
adaptive combination method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1076</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1076</id><created>2014-02-05</created><updated>2014-06-20</updated><authors><author><keyname>Bohy</keyname><forenames>Aaron</forenames></author><author><keyname>Bruy&#xe8;re</keyname><forenames>V&#xe9;ronique</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Symblicit algorithms for optimal strategy synthesis in monotonic Markov
  decision processes (extended version)</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When treating Markov decision processes (MDPs) with large state spaces, using
explicit representations quickly becomes unfeasible. Lately, Wimmer et al. have
proposed a so-called symblicit algorithm for the synthesis of optimal
strategies in MDPs, in the quantitative setting of expected mean-payoff. This
algorithm, based on the strategy iteration algorithm of Howard and Veinott,
efficiently combines symbolic and explicit data structures, and uses binary
decision diagrams as symbolic representation. The aim of this paper is to show
that the new data structure of pseudo-antichains (an extension of antichains)
provides another interesting alternative, especially for the class of monotonic
MDPs. We design efficient pseudo-antichain based symblicit algorithms (with
open source implementations) for two quantitative settings: the expected
mean-payoff and the stochastic shortest path. For two practical applications
coming from automated planning and LTL synthesis, we report promising
experimental results w.r.t. both the run time and the memory consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1088</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1088</id><created>2014-02-05</created><authors><author><keyname>Yousefbeiki</keyname><forenames>Mohsen</forenames></author><author><keyname>Alrabadi</keyname><forenames>Osama N.</forenames></author><author><keyname>Perruisseau-Carrier</keyname><forenames>Julien</forenames></author></authors><title>Efficient MIMO Transmission of PSK Signals With a Single-Radio
  Reconfigurable Antenna</title><categories>cs.IT math.IT</categories><comments>30 pages, 6 figures. IEEE Transactions on Communications, 2014</comments><doi>10.1109/TCOMM.2013.122113.130481</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crucial developments to the recently introduced signal-space approach for
multiplexing multiple data symbols using a single-radio switched antenna are
presented. First, we introduce a general framework for expressing the spatial
multiplexing relation of the transmit signals only from the antenna scattering
parameters and the modulating reactive loading. This not only avoids tedious
far-field calculations, but more importantly provides an efficient and
practical strategy for spatially multiplexing PSK signals of any modulation
order. The proposed approach allows ensuring a constant impedance matching at
the input of the driving antenna for all symbol combinations, and as
importantly uses only passive reconfigurable loads. This obviates the use of
reconfigurable matching networks and active loads, respectively, thereby
overcoming stringent limitations of previous single-feed MIMO techniques in
terms of complexity, efficiency, and power consumption. The proposed approach
is illustrated by the design of a realistic very compact antenna system
optimized for multiplexing QPSK signals. The results show that the proposed
approach can bring the MIMO benefits to the low-end user terminals at a reduced
RF complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1092</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1092</id><created>2014-02-05</created><authors><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>M&#xf6;nich</keyname><forenames>Ullrich J.</forenames></author></authors><title>Signal and System Approximation from General Measurements</title><categories>cs.IT math.CV math.FA math.IT</categories><comments>This paper will be published as part of the book &quot;New Perspectives on
  Approximation and Sampling Theory - Festschrift in honor of Paul Butzer's
  85th birthday&quot; in the Applied and Numerical Harmonic Analysis Series,
  Birkhauser (Springer-Verlag). Parts of this work have been presented at the
  IEEE International Conference on Acoustics, Speech, and Signal Processing
  2014 (ICASSP 2014)</comments><msc-class>30D10, 30E05, 30H05, 94A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze the behavior of system approximation processes for
stable linear time-invariant (LTI) systems and signals in the Paley-Wiener
space PW_\pi^1. We consider approximation processes, where the input signal is
not directly used to generate the system output, but instead a sequence of
numbers is used that is generated from the input signal by measurement
functionals. We consider classical sampling which corresponds to a pointwise
evaluation of the signal, as well as several more general measurement
functionals. We show that a stable system approximation is not possible for
pointwise sampling, because there exist signals and systems such that the
approximation process diverges. This remains true even with oversampling.
However, if more general measurement functionals are considered, a stable
approximation is possible if oversampling is used. Further, we show that
without oversampling we have divergence for a large class of practically
relevant measurement procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1099</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1099</id><created>2014-02-05</created><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author></authors><title>Levels of Abstraction and the Apparent Contradictory Philosophical
  Legacy of Turing and Shannon</title><categories>cs.GL</categories><comments>A version of the essay published in the Q, the AISB Quarterly
  bulletin of the Society for the Study of Artificial Intelligence and
  Simulation of Behaviour</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent article, Luciano Floridi explains his view of Turing's legacy in
connection to the philosophy of information. I will very briefly survey one of
Turing's other contributions to the philosophy of information and computation,
including similarities to Shannon's own methodological approach to information
through communication, showing how crucial they are and have been as
methodological strategies to understanding key aspects of these concepts. While
Floridi's concept of Levels of Abstraction is related to the novel methodology
of Turing's imitation game for tackling the question of machine intelligence,
Turing's other main contribution to the philosophy of information runs contrary
to it. Indeed, the seminal concept of computation universality strongly
suggests the deletion of fundamental differences among seemingly different
levels of description. How might we reconcile these apparently contradictory
contributions? I will argue that Turing's contribution should prompt us to plot
some directions for a philosophy of information and computation, one that
closely parallels the most important developments in computer science, one that
understands the profound implications of the works of Turing, Shannon and
others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1107</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1107</id><created>2014-02-05</created><authors><author><keyname>Pal</keyname><forenames>Arindam</forenames></author></authors><title>Approximation Algorithms for Covering and Packing Problems on Paths</title><categories>cs.DS cs.DM math.CO</categories><comments>Ph.D. thesis of Arindam Pal. 100 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Routing and scheduling problems are fundamental problems in combinatorial
optimization, and also have many applications. Most variations of these
problems are NP-Hard, so we need to use heuristics to solve these problems on
large instances, which are fast and yet come close to the optimal value. In
this thesis, we study the design and analysis of approximation algorithms for
such problems. We focus on two important class of problems. The first is the
Unsplittable Flow Problem and some of its variants and the second is the
Resource Allocation for Job Scheduling Problem and some of its variants. The
first is a packing problem, whereas the second is a covering problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1128</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1128</id><created>2014-02-05</created><authors><author><keyname>Sak</keyname><forenames>Ha&#x15f;im</forenames></author><author><keyname>Senior</keyname><forenames>Andrew</forenames></author><author><keyname>Beaufays</keyname><forenames>Fran&#xe7;oise</forenames></author></authors><title>Long Short-Term Memory Based Recurrent Neural Network Architectures for
  Large Vocabulary Speech Recognition</title><categories>cs.NE cs.CL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Long Short-Term Memory (LSTM) is a recurrent neural network (RNN)
architecture that has been designed to address the vanishing and exploding
gradient problems of conventional RNNs. Unlike feedforward neural networks,
RNNs have cyclic connections making them powerful for modeling sequences. They
have been successfully used for sequence labeling and sequence prediction
tasks, such as handwriting recognition, language modeling, phonetic labeling of
acoustic frames. However, in contrast to the deep neural networks, the use of
RNNs in speech recognition has been limited to phone recognition in small scale
tasks. In this paper, we present novel LSTM based RNN architectures which make
more effective use of model parameters to train acoustic models for large
vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at
various numbers of parameters and configurations. We show that LSTM models
converge quickly and give state of the art speech recognition performance for
relatively small sized models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1137</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1137</id><created>2014-02-05</created><authors><author><keyname>Akin</keyname><forenames>Sami</forenames></author></authors><title>Security in Cognitive Radio Networks</title><categories>cs.IT cs.CR math.IT</categories><comments>Submitted to CISS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the information-theoretic security by modeling
a cognitive radio wiretap channel under quality-of-service (QoS) constraints
and interference power limitations inflicted on primary users (PUs). We
initially define four different transmission scenarios regarding channel
sensing results and their correctness. We provide effective secure transmission
rates at which a secondary eavesdropper is refrained from listening to a
secondary transmitter (ST). Then, we construct a channel state transition
diagram that characterizes this channel model. We obtain the effective secure
capacity which describes the maximum constant buffer arrival rate under given
QoS constraints. We find out the optimal transmission power policies that
maximize the effective secure capacity, and then, we propose an algorithm that,
in general, converges quickly to these optimal policy values. Finally, we show
the performance levels and gains obtained under different channel conditions
and scenarios. And, we emphasize, in particular, the significant effect of
hidden-terminal problem on information-theoretic security in cognitive radios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1141</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1141</id><created>2014-02-05</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Carlos Pedro</forenames></author></authors><title>Quantum Cybernetics and Complex Quantum Systems Science - A Quantum
  Connectionist Exploration</title><categories>cs.NE cond-mat.dis-nn quant-ph</categories><msc-class>92B20, 81P68, 82C32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum cybernetics and its connections to complex quantum systems science is
addressed from the perspective of complex quantum computing systems. In this
way, the notion of an autonomous quantum computing system is introduced in
regards to quantum artificial intelligence, and applied to quantum artificial
neural networks, considered as autonomous quantum computing systems, which
leads to a quantum connectionist framework within quantum cybernetics for
complex quantum computing systems. Several examples of quantum feedforward
neural networks are addressed in regards to Boolean functions' computation,
multilayer quantum computation dynamics, entanglement and quantum
complementarity. The examples provide a framework for a reflection on the role
of quantum artificial neural networks as a general framework for addressing
complex quantum systems that perform network-based quantum computation,
possible consequences are drawn regarding quantum technologies, as well as
fundamental research in complex quantum systems science and quantum biology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1151</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1151</id><created>2014-02-05</created><authors><author><keyname>Biega&#x144;ski</keyname><forenames>Wojciech</forenames></author><author><keyname>Kasi&#x144;ski</keyname><forenames>Andrzej</forenames></author></authors><title>Image Acquisition in an Underwater Vision System with NIR and VIS
  Illumination</title><categories>cs.CV</categories><journal-ref>Computer Science &amp; Information Technology, Volume 4, Number 1,
  2014, pp. 215-224</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes the image acquisition system able to capture images in
two separated bands of light, used to underwater autonomous navigation. The
channels are: the visible light spectrum and near infrared spectrum. The
characteristics of natural, underwater environment were also described together
with the process of the underwater image creation. The results of an experiment
with comparison of selected images acquired in these channels are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1156</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1156</id><created>2014-02-05</created><updated>2014-02-06</updated><authors><author><keyname>Naumov</keyname><forenames>Pavel</forenames></author><author><keyname>Protzman</keyname><forenames>Margaret</forenames></author></authors><title>Equilibria Interchangeability in Cellular Games</title><categories>cs.GT math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of interchangeability has been introduced by John Nash in one of
his original papers on equilibria. This paper studies properties of Nash
equilibria interchangeability in cellular games that model behavior of infinite
chain of homogeneous economic agents. The paper shows that there are games in
which strategy of any given player is interchangeable with strategies of
players in an arbitrary large neighborhood of the given player, but is not
interchangeable with the strategy of a remote player outside of the
neighborhood. The main technical result is a sound and complete logical system
describing universal properties of interchangeability common to all cellular
games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1188</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1188</id><created>2014-02-05</created><authors><author><keyname>Kuhn</keyname><forenames>Adrian</forenames></author><author><keyname>DeLine</keyname><forenames>Robert</forenames></author></authors><title>On Designing Better Tools for Learning APIs</title><categories>cs.SE</categories><comments>Published at ICSE 2012</comments><doi>10.1109/SUITE.2012.6225476</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern software development requires a large investment in learning
application programming interfaces (APIs). Recent research found that the
learning materials themselves are often inadequate: developers struggle to find
answers beyond simple usage scenarios. Solving these problems requires a large
investment in tool and search engine development. To understand where further
investment would be most useful, we ran a study with 19 professional developers
to understand what a solution might look like, free of technical constraints.
In this paper, we report on design implications of tools for API learning,
grounded in the reality of the professional developers themselves. The
reoccurring themes in the participants' feedback were trustworthiness,
confidentiality, information overload and the need for code examples as
first-class documentation artifacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1191</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1191</id><created>2014-02-05</created><updated>2015-05-12</updated><authors><author><keyname>Cai</keyname><forenames>Xing Shi</forenames></author><author><keyname>Devroye</keyname><forenames>Luc</forenames></author></authors><title>The Analysis of Kademlia for random IDs</title><categories>cs.DS cs.NI</categories><comments>15 pages. 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kademlia is the de facto standard searching algorithm for P2P (peer-to-peer)
networks on the Internet. In our earlier work, we introduced two slightly
different models for Kademlia and studied how many steps it takes to search for
a target node by using Kademlia's searching algorithm. The first model, in
which nodes of the network are labelled with deterministic IDs, had been
discussed in that paper. The second one, in which nodes are labelled with
random IDs, which we call the Random ID Model, was only briefly mentioned.
Refined results with detailed proofs for this model are given in this paper.
Our analysis shows that with high probability it takes about $c \log n$ steps
to locate any node, where $n$ is the total number of nodes in the network and
$c$ is a constant that does not depend on $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1194</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1194</id><created>2014-02-05</created><authors><author><keyname>R&#xe9;tv&#xe1;ri</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Tapolcai</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>K&#x151;r&#xf6;si</keyname><forenames>Attila</forenames></author><author><keyname>Majd&#xe1;n</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>Heszberger</keyname><forenames>Zal&#xe1;n</forenames></author></authors><title>Compressing IP Forwarding Tables: Towards Entropy Bounds and Beyond</title><categories>cs.DS</categories><acm-class>C.2.1; E.4</acm-class><journal-ref>ACM SIGCOMM 2013. 111-122</journal-ref><doi>10.1145/2486001.2486009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lately, there has been an upsurge of interest in compressed data structures,
aiming to pack ever larger quantities of information into constrained memory
without sacrificing the efficiency of standard operations, like random access,
search, or update. The main goal of this paper is to demonstrate how data
compression can benefit the networking community, by showing how to squeeze the
IP Forwarding Information Base (FIB), the giant table consulted by IP routers
to make forwarding decisions, into information-theoretical entropy bounds, with
essentially zero cost on longest prefix match and FIB update. First, we adopt
the state-of-the-art in compressed data structures, yielding a static
entropy-compressed FIB representation with asymptotically optimal lookup. Then,
we re-design the venerable prefix tree, used commonly for IP lookup for at
least 20 years in IP routers, to also admit entropy bounds and support lookup
in optimal time and update in nearly optimal time. Evaluations on a Linux
kernel prototype indicate that our compressors encode a FIB comprising more
than 440K prefixes to just about 100--400 KBytes of memory, with a threefold
increase in lookup throughput and no penalty on FIB updates.
  This technical report contains a number of important corrections and
revisions to the original manuscript.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1213</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1213</id><created>2014-02-05</created><authors><author><keyname>Ickowicz</keyname><forenames>Adrien</forenames></author></authors><title>A Statistical Modelling Approach to Detecting Community in Networks</title><categories>cs.SI stat.AP</categories><comments>23 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:physics/0512106, arXiv:1207.0865 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been considerable recent interest in algorithms for finding
communities in networks - groups of vertex within which connections are dense
(frequent), but between which connections are sparser (rare). Most of the
current literature advocates an heuristic approach to the removal of the edges
(i.e., removing the links that are less significant using a well-designed
function). In this article, we will investigate a technique for uncovering
latent communities using a new modelling approach, based on how information
spread within a network. It will prove to be easy to use, robust and scalable.
It makes supplementary information related to the network/community structure
(different communications, consecutive observations) easier to integrate. We
will demonstrate the efficiency of our approach by providing some illustrating
real-world applications, like the famous Zachary karate club, or the Amazon
political books buyers network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1216</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1216</id><created>2014-02-05</created><authors><author><keyname>Sun</keyname><forenames>Jingchao</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Zhang</keyname><forenames>Jinxue</forenames></author><author><keyname>Zhang</keyname><forenames>Yanchao</forenames></author></authors><title>TouchIn: Sightless Two-factor Authentication on Multi-touch Mobile
  Devices</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile authentication is indispensable for preventing unauthorized access to
multi-touch mobile devices. Existing mobile authentication techniques are often
cumbersome to use and also vulnerable to shoulder-surfing and smudge attacks.
This paper focuses on designing, implementing, and evaluating TouchIn, a
two-factor authentication system on multi-touch mobile devices. TouchIn works
by letting a user draw on the touchscreen with one or multiple fingers to
unlock his mobile device, and the user is authenticated based on the geometric
properties of his drawn curves as well as his behavioral and physiological
characteristics. TouchIn allows the user to draw on arbitrary regions on the
touchscreen without looking at it. This nice sightless feature makes TouchIn
very easy to use and also robust to shoulder-surfing and smudge attacks.
Comprehensive experiments on Android devices confirm the high security and
usability of TouchIn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1219</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1219</id><created>2014-02-05</created><authors><author><keyname>Tierney</keyname><forenames>Brian B.</forenames></author><author><keyname>Grbic</keyname><forenames>Anthony</forenames></author></authors><title>Planar Shielded-Loop Resonators</title><categories>cs.OH</categories><doi>10.1109/TAP.2014.2314305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design and analysis of planar shielded-loop resonators for use in
wireless non-radiative power transfer systems is presented. The difficulties
associated with coaxial shielded-loop resonators for wireless power transfer
are discussed and planar alternatives are proposed. The currents along these
planar structures are analyzed and first-order design equations are presented
in the form of a circuit model. In addition, the planar structures are
simulated and fabricated. Planar shielded-loop resonators are compact and
simple to fabricate. Moreover, they are well-suited for printed circuit board
designs or integrated circuits
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1241</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1241</id><created>2014-02-05</created><authors><author><keyname>Abdulhamid</keyname><forenames>Shafii Muhammad</forenames></author><author><keyname>Idris</keyname><forenames>Ismaila</forenames></author></authors><title>Design Evaluation of Some Nigerian University Portals: A Programmer's
  Point of View</title><categories>cs.CY</categories><comments>8 pages. Computer Science and Telecommunications 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, Nigerian Universities feel pressured to get a portal up and running
dynamic, individualized web systems have become essential for institutions of
higher learning. As a result, most of the Nigerian University portals nowadays
do not meet up to standard. In this paper, ten Nigerian University portals were
selected and their design evaluated in accordance with the international best
practices. The result was revealing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1242</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1242</id><created>2014-02-05</created><authors><author><keyname>Idris</keyname><forenames>Ismaila</forenames></author><author><keyname>Abdulhamid</keyname><forenames>Shafii Muhammad</forenames></author></authors><title>An Improved AIS Based E-mail Classification Technique for Spam Detection</title><categories>cs.CR</categories><comments>6 pages, 2 figures, 1 table</comments><journal-ref>The Eighth International Conference on eLearning for
  Knowledge-Based Society, 23-24 February 2012, Thailand</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An improved email classification method based on Artificial Immune System is
proposed in this paper to develop an immune based system by using the immune
learning, immune memory in solving complex problems in spam detection. An
optimized technique for e-mail classification is accomplished by distinguishing
the characteristics of spam and non-spam that is been acquired from trained
data set. These extracted features of spam and non-spam are then combined to
make a single detector, therefore reducing the false rate. (Non-spam that were
wrongly classified as spam). Effectiveness of our technique in decreasing the
false rate shall be demonstrated by the result that will be acquired.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1243</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1243</id><created>2014-02-05</created><authors><author><keyname>Abdulhamid</keyname><forenames>Shafii Muhammad</forenames></author><author><keyname>Usman</keyname><forenames>Gana</forenames></author></authors><title>Destination Information Management System for Tourist</title><categories>cs.HC</categories><comments>8 pages. Computer Science and Telecommunications 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of information and communication technology in our day to day
activities is now unavoidable. In tourism developments, destination information
and management systems are used to guide visitors and provide information to
both visitors and management of the tour sites. In this paper, information and
navigation system was designed for tourists, taking some Niger state of Nigeria
tourism destinations into account. The information management system was
designed using Java Applet (NetBeans IDE 6.1), Hypertext MarkUp Language
(HTML), Personal Home Page (PHP), Java script and MySQL as the back-end
integration database. Two different MySQL servers were used, the MySQL query
browser and the WAMP5 server to compare the effectiveness of the system
developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1245</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1245</id><created>2014-02-06</created><authors><author><keyname>R</keyname><forenames>Ms. Aruna. G.</forenames></author><author><keyname>SivanArulSelvan</keyname><forenames>Mr.</forenames></author></authors><title>A Survey on Delay-Aware Network Structure for Wireless Sensor Networks
  with Consecutive Data Collection Processes</title><categories>cs.NI</categories><comments>4 pages,3 figures, &quot;Published with International Journal of
  Engineering Trends and Technology (IJETT)&quot;</comments><journal-ref>IJETT, 7(4),198-201, 2014 published by seventh sense research
  group</journal-ref><doi>10.14445/22315381/IJETT-V7P240</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Wireless Sensor Network (WSN) consists of spatially distributed autonomous
sensors to monitor physical or environmental conditions, such as temperature,
sound, pressure,etc. In sensing applications, data packets are flowing from
sensor nodes to base station. In data collection processes, bottom up approach
is used. In bottom up approach, all nodes send their sensed data packets to
base station directly. In this approach will lead to increased delay, which
will lead to higher energy consumption. To reduce the energy consumption of
sensor nodes,Clustering Algorithm and Low-Energy Adaptive Clustering Hierarchy
(LEACH) are being used. Efficient gathering in Wireless Sensor information
systems, Power Efficient Gathering in Sensor Information System (PEGASIS) is
being used. There are lot of research issues in Wireless Sensor Networks such
as delay, lifetime of network, energy dissipation which needs to be resolved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1246</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1246</id><created>2014-02-06</created><authors><author><keyname>R</keyname><forenames>Ms. Rubia.</forenames></author><author><keyname>SivanArulSelvan</keyname><forenames>Mr.</forenames></author></authors><title>A Survey on Mobile Data Gathering in Wireless Sensor Networks - Bounded
  Relay</title><categories>cs.NI</categories><comments>4 pages, 1 figure, &quot;Published with International Journal of
  Engineering Trends and Technology (IJETT)&quot;</comments><journal-ref>IJETT, 7(5),205-208,2014 published by seventh sense research group</journal-ref><doi>10.14445/22315381/IJETT-V7P247</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the wireless sensor networks consist of static sensors, which can be
deployed in a wide environment for monitoring applications. While transmitting
the data from source to static sink, the amount of energy consumption of the
sensor node is high. It results in reduced lifetime of the network.Some of the
WSN architectures have been proposed based on Mobile Elements. There is large
number of approaches to resolve the above problem. It is found those two
approaches, namely Single Hop Data Gathering problem (SHDGP) and mobile Data
Gathering, which is used to increase the lifetime of the network. Single Hop
Data Gathering Problem is used to achieve the uniform energy consumption. The
mobile Data Gathering algorithm is used to find the minimal set of points in
the sensor network, which serves as data gathering points for mobile network.
Even after so many decades of research, there are some unresolved problems like
non uniform energy consumption, increased latency, which needs to be resolved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1252</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1252</id><created>2014-02-06</created><authors><author><keyname>K</keyname><forenames>Manojkumar. M.</forenames></author><author><keyname>D</keyname><forenames>Sathya.</forenames></author></authors><title>A Survey on an Effective Defense Mechanism against Reactive Jamming
  Attacks in WSN</title><categories>cs.NI</categories><comments>4 pages,3 figures,&quot;Published with International Journal of Computer
  Trends and Technology (IJCTT)&quot;</comments><journal-ref>IJCTT 7(3):143-146, January 2014. Published by Seventh Sense
  Research Group</journal-ref><doi>10.14445/22312803/IJCTT-V7P129</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Wireless Sensor Network (WSN) is a self-configure network of sensor nodes
communicate among themselves using radio signals and deployed in quantity to
sense, monitor and to understand the physical world. A jammer is an entity
which interferes with the physical transmission and reception of wireless
communications. Reactive jamming attack is a major security problem in the
wireless sensor network. The reactive jammer stays quiet when the channel is
idle. The jammer starts transmitting a radio signal as soon as it senses
activity on the channel. The reactive jammer nodes will be deactivated by
identifying all the trigger nodes, at the same time a jammer node is localized
by exploiting the changes in the neighbor nodes. The affected node can be
identified, by analyzing the changes in its communication range, compared to
its neighbors. The paper proposes a survey on trigger node identification and a
detailed survey on techniques to identify trigger nodes and highly concentrated
on the reactive jammer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1257</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1257</id><created>2014-02-06</created><updated>2014-02-07</updated><authors><author><keyname>Vadnere</keyname><forenames>Nishant</forenames></author><author><keyname>Mehta</keyname><forenames>R. G.</forenames></author><author><keyname>Rana</keyname><forenames>D. P.</forenames></author><author><keyname>Mistry</keyname><forenames>N. J.</forenames></author><author><keyname>Raghuwanshi</keyname><forenames>M. M.</forenames></author></authors><title>Incremental classification using Feature Tree</title><categories>cs.DB</categories><comments>5 Pages, 1 figure, International conference on recent trends and
  innovations in engineering and technology - 2013 (ICRTIET-2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, stream data have become an immensely growing area of
research for the database, computer science and data mining communities. Stream
data is an ordered sequence of instances. In many applications of data stream
mining data can be read only once or a small number of times using limited
computing and storage capabilities. Some of the issues occurred in classifying
stream data that have significant impact in algorithm development are size of
database, online streaming, high dimensionality and concept drift. The concept
drift occurs when the properties of the historical data and target variable
change over time abruptly in such a case that the predictions will become
inaccurate as time passes. In this paper the framework of incremental
classification is proposed to solve the issues for the classification of stream
data. The Trie structure based incremental feature tree, Trie structure based
incremental FP (Frequent Pattern) growth tree and tree based incremental
classification algorithm are introduced in the proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1258</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1258</id><created>2014-02-06</created><authors><author><keyname>Gupta</keyname><forenames>Mohit Kumar</forenames></author><author><keyname>Verma</keyname><forenames>Vishal</forenames></author><author><keyname>Verma</keyname><forenames>Megha Singh</forenames></author></authors><title>In-Memory Database Systems - A Paradigm Shift</title><categories>cs.DB</categories><comments>Pages-4, Figures-1, Published with International Journal of
  Engineering Trends and Technology (IJETT)</comments><journal-ref>International Journal of Engineering Trends and Technology (IJETT)
  6(6):333-336, Dec. 2013. Published by Seventh Sense Research Group</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today world, organizations like Google, Yahoo, Amazon, Facebook etc. are
facing drastic increase in data. This leads to the problem of capturing,
storing, managing and analyzing terabytes or petabytes of data, stored in
multiple formats, from different internal and external sources. Moreover, new
applications scenarios like weather forecasting, trading, artificial
intelligence etc. need huge data processing in real time. These requirements
exceed the processing capacity of traditional on-disk database management
systems to manage this data and to give speedy real time results. Therefore,
data management needs new solutions for coping with the challenges of data
volumes and processing data in real-time. An in-memory database system (IMDS)
is a latest breed of database management system which is becoming answer to
above challenges with other supporting technologies. IMDS is capable to process
massive data distinctly faster. This paper explores IMDS approach and its
associated design issues and challenges. It also investigates some famous
commercial and open-source IMDS solutions available in the market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1263</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1263</id><created>2014-02-06</created><authors><author><keyname>Meirom</keyname><forenames>Eli A.</forenames></author><author><keyname>Milling</keyname><forenames>Chris</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author><author><keyname>Orda</keyname><forenames>Ariel</forenames></author><author><keyname>Shakkottai</keyname><forenames>Sanjay</forenames></author></authors><title>Localized epidemic detection in networks with overwhelming noise</title><categories>cs.SI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of detecting an epidemic in a population where
individual diagnoses are extremely noisy. The motivation for this problem is
the plethora of examples (influenza strains in humans, or computer viruses in
smartphones, etc.) where reliable diagnoses are scarce, but noisy data
plentiful. In flu/phone-viruses, exceedingly few infected people/phones are
professionally diagnosed (only a small fraction go to a doctor) but less
reliable secondary signatures (e.g., people staying home, or
greater-than-typical upload activity) are more readily available. These
secondary data are often plagued by unreliability: many people with the flu do
not stay home, and many people that stay home do not have the flu. This paper
identifies the precise regime where knowledge of the contact network enables
finding the needle in the haystack: we provide a distributed, efficient and
robust algorithm that can correctly identify the existence of a spreading
epidemic from highly unreliable local data. Our algorithm requires only
local-neighbor knowledge of this graph, and in a broad array of settings that
we describe, succeeds even when false negatives and false positives make up an
overwhelming fraction of the data available. Our results show it succeeds in
the presence of partial information about the contact network, and also when
there is not a single &quot;patient zero&quot;, but rather many (hundreds, in our
examples) of initial patient-zeroes, spread across the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1270</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1270</id><created>2014-02-06</created><authors><author><keyname>Amine</keyname><forenames>Abderrahim Mohammed El</forenames></author></authors><title>Vers une interface pour l enrichissement des requetes en arabe dans un
  systeme de recherche d information</title><categories>cs.IR</categories><comments>9 pages, in French</comments><journal-ref>CIIA'2009 : 2eme Conference Internationale sur Informatique et ses
  Applications, Saida - Algerie, 03 -04 Mai 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This presentation focuses on the automatic expansion of Arabic request using
morphological analyzer and Arabic Wordnet. The expanded request is sent to
Google.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1283</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1283</id><created>2014-02-06</created><authors><author><keyname>Zaidi</keyname><forenames>Abdallah</forenames></author><author><keyname>Rokbani</keyname><forenames>Nizar</forenames></author><author><keyname>Alimi</keyname><forenames>Adel. M.</forenames></author></authors><title>A Hierarchical fuzzy controller for a biped robot</title><categories>cs.RO</categories><comments>4 pages- 9 figures. Proc of The 2013 International Conference on
  Individual and Collective Behaviors in Robotics (ICBR 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the investigation is placed on the hierarchic neuro-fuzzy
systems as a possible solution for biped control. An hierarchic controller for
biped is presented, it includes several sub-controllers and the whole structure
is generated using the adaptive Neuro-fuzzy method. The proposed hierarchic
system focus on the key role that the centre of mass position plays in biped
robotics, the system sub-controllers generate their outputs taken into
consideration the position of that key point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1285</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1285</id><created>2014-02-06</created><authors><author><keyname>Gonz&#xe1;lez-Dom&#xed;nguez</keyname><forenames>Jorge</forenames></author><author><keyname>Georganas</keyname><forenames>Evangelos</forenames></author><author><keyname>Zheng</keyname><forenames>Yili</forenames></author><author><keyname>Mart&#xed;n</keyname><forenames>Mar&#xed;a J.</forenames></author></authors><title>Constructing Performance Models for Dense Linear Algebra Algorithms on
  Cray XE Systems</title><categories>cs.DC cs.MS cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hiding or minimizing the communication cost is key in order to obtain good
performance on large-scale systems. While communication overlapping attempts to
hide communications cost, 2.5D communication avoiding algorithms improve
performance scalability by reducing the volume of data transfers at the cost of
extra memory usage. Both approaches can be used together or separately and the
best choice depends on the machine, the algorithm and the problem size. Thus,
the development of performance models is crucial to determine the best option
for each scenario. In this paper, we present a methodology for constructing
performance models for parallel numerical routines on Cray XE systems. Our
models use portable benchmarks that measure computational cost and network
characteristics, as well as performance degradation caused by simultaneous
accesses to the network. We validate our methodology by constructing the
performance models for the 2D and 2.5D approaches, with and without
overlapping, of two matrix multiplication algorithms (Cannon's and SUMMA),
triangular solve (TRSM) and Cholesky. We compare the estimations provided by
these models with the experimental results using up to 24,576 cores of a Cray
XE6 system and predict the performance of the algorithms on larger systems.
Results prove that the estimations significantly improve when taking into
account network contention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1287</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1287</id><created>2014-02-06</created><authors><author><keyname>Kahanwal</keyname><forenames>Brijender</forenames></author></authors><title>Towards High Performance Computing (Hpc) Through Parallel Programming
  Paradigms and Their Principles</title><categories>cs.PL</categories><comments>11 pages, 2 figures. International Journal of Programming Languages
  and Applications (IJPLA) 2014</comments><msc-class>AIRCC Publishing Corporation</msc-class><doi>10.5121/ijpla.2014.4104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, we are to find out solutions to huge computing problems very
rapidly. It brings the idea of parallel computing in which several machines or
processors work cooperatively for computational tasks. In the past decades,
there are a lot of variations in perceiving the importance of parallelism in
computing machines. And it is observed that the parallel computing is a
superior solution to many of the computing limitations like speed and density;
non-recurring and high cost; and power consumption and heat dissipation etc.
The commercial multiprocessors have emerged with lower prices than the
mainframe machines and supercomputers machines. In this article the high
performance computing (HPC) through parallel programming paradigms (PPPs) are
discussed with their constructs and design approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1296</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1296</id><created>2014-02-06</created><authors><author><keyname>Gamboa</keyname><forenames>Ricardo Jo&#xe3;o Silveira Santos</forenames></author></authors><title>Mnemonical Body Shortcuts: Gestural Interface for Mobile Devices</title><categories>cs.HC</categories><comments>124 pages, MSc Thesis, Technical University of Lisbon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile devices' user interfaces are still quite similar to traditional
interfaces offered by desktop computers, but those can be highly problematic
when used in a mobile context. Human gesture recognition in mobile interaction
appears as an important area to provide suitable on-the-move usability. We
present a body space based approach to improve mobile device interaction and
mobile performance, which we named as Mnemonical Body Shortcuts. The human body
is presented as a rich repository of meaningful relations which are always
available to interact with. These body-based gestures allow the user to
naturally interact with mobile devices with no movement limitations.
Preliminary studies using Radio Frequency Identification (RFID) technology were
performed, validating Mnemonical Body Shortcuts as an appropriate new mobile
interaction mechanism. Following those studies, we developed inertial sensing
prototypes using an accelerometer, ending in the construction and user testing
of a gestural interface for mobile devices capable of properly recognizing
Mnemonical Body Shortcuts and also providing suitable user control mechanisms
and audio, visual and haptic feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1298</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1298</id><created>2014-02-06</created><updated>2015-01-31</updated><authors><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>M&#xe9;zard</keyname><forenames>Marc</forenames></author><author><keyname>Sakata</keyname><forenames>Ayaka</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Phase transitions and sample complexity in Bayes-optimal matrix
  factorization</title><categories>cs.NA cond-mat.stat-mech cs.IT cs.LG math.IT stat.ML</categories><comments>48 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse the matrix factorization problem. Given a noisy measurement of a
product of two matrices, the problem is to estimate back the original matrices.
It arises in many applications such as dictionary learning, blind matrix
calibration, sparse principal component analysis, blind source separation, low
rank matrix completion, robust principal component analysis or factor analysis.
We use the tools of statistical mechanics - the cavity and replica methods - to
analyze the achievability and tractability of the inference problems in the
setting of Bayes-optimal inference, which amounts to assuming that the two
matrices have random independent elements generated from some known
distribution, and this information is available to the inference algorithm. In
this setting, we compute the minimal mean-squared-error achievable in principle
in any computational time, and the error that can be achieved by an efficient
approximate message passing algorithm. The computation is based on the
asymptotic state-evolution analysis of the algorithm. The performance that our
analysis predicts, both in terms of the achieved mean-squared-error, and in
terms of sample complexity, is extremely promising and motivating for a further
development of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1309</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1309</id><created>2014-02-06</created><authors><author><keyname>Ngoko</keyname><forenames>Yanik</forenames></author><author><keyname>C&#xe9;rin</keyname><forenames>Christophe</forenames></author><author><keyname>Goldman</keyname><forenames>Alfredo</forenames></author><author><keyname>Milojicic</keyname><forenames>Dejan</forenames></author></authors><title>Backtracking algorithms for service selection</title><categories>cs.DC</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore the automation of services' compositions. We focus
on the service selection problem. In the formulation that we consider, the
problem's inputs are constituted by a behavioral composition whose abstract
services must be bound to concrete ones. The objective is to find the binding
that optimizes the {\it utility} of the composition under some services level
agreements. We propose a complete solution. Firstly, we show that the service
selection problem can be mapped onto a Constraint Satisfaction Problem (CSP).
The benefit of this mapping is that the large know-how in the resolution of the
CSP can be used for the service selection problem. Among the existing
techniques for solving CSP, we consider the backtracking. Our second
contribution is to propose various backtracking-based algorithms for the
service selection problem. The proposed variants are inspired by existing
heuristics for the CSP. We analyze the runtime gain of our framework over an
intuitive resolution based on exhaustive search. Our last contribution is an
experimental evaluation in which we demonstrate that there is an effective gain
in using backtracking instead of some comparable approaches. The experiments
also show that our proposal can be used for finding in real time, optimal
solutions on small and medium services' compositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1314</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1314</id><created>2014-02-06</created><authors><author><keyname>Selvakumar</keyname><forenames>A. Arul Lawernce</forenames><affiliation>Member IAENG</affiliation></author><author><keyname>Ratastogi</keyname><forenames>R. S.</forenames><affiliation>Member IAENG</affiliation></author></authors><title>Study the function of building blocks in SHA Family</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we analyse the role of some of the building blocks in SHA-256.
We show that the disturbance correction strategy is applicable to the SHA-256
architecture and we prove that functions $\Sigma$, $\sigma$ are vital for the
security of SHA-256 by showing that for a variant without them it is possible
to find collisions with complexity 2 64 hash operations. As a step towards an
analysis of the full function, we present the results of our experiments on
Hamming weights of expanded messages for different variants of the message
expansion and show that there exist low-weight expanded messages for
XOR-linearised variants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1324</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1324</id><created>2014-02-06</created><authors><author><keyname>Rafael</keyname><forenames>Ivo</forenames></author></authors><title>UCAT: Ubiquitous Context Awareness Tools for The Blind</title><categories>cs.HC</categories><comments>93 pages, MSc Thesis, University of Lisbon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visually impaired people are often confronted with new environments and they
find themselves face to face with an innumerous amount of difficulties when
facing these environments. Having to surpass and deal with these difficulties
that arise with their condition is something that we can help diminish. They
are one sense down when trying to understand their surrounding environments and
gather information about what is happening around them. Nowadays, mobile
devices present significant computing and technological capacity which has been
increasing to the point where it is very common for most people to have access
to a device with Bluetooth, GPS, Wi-Fi, and both high processing and storage
capacities. This allows us to think of applications that can do so much to help
people with difficulties. In the particular case of blind people, the lack of
visual information can be bypassed with other contextual information retrieved
by their own personal devices. Our goal is to provide information to blind
users, be able to give them information about the context that surrounds them.
We wanted to provide the blind users with the tools to create information and
be able to share this information between each other, information about people,
locations or objects. Our approach was to split the project into a data and
information gathering phase where we did our field search and interviewed and
elaborated on how is the situation of environment perception for blind users,
followed by a technical phase where we implement a system based on the first
stage. Our results gathered from both the collecting phase and our implementing
phase showed that there is potential to use these tools in the blind community
and that they welcome the possibilities and horizons that it opens them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1327</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1327</id><created>2014-02-06</created><authors><author><keyname>Zala</keyname><forenames>Mr. Rushirajsinh L.</forenames></author><author><keyname>Mehta</keyname><forenames>Mr. Brijesh B.</forenames></author><author><keyname>Zala</keyname><forenames>Mr. Mahipalsinh R.</forenames></author></authors><title>A Survey on Spatial Co-location Patterns Discovery from Spatial Datasets</title><categories>cs.DB</categories><comments>6 pages,8 figures</comments><journal-ref>IJCTT 7(3):137-142, January 2014</journal-ref><doi>10.14445/22312803/IJCTT-V7P140</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Spatial data mining or Knowledge discovery in spatial database is the
extraction of implicit knowledge, spatial relations and spatial patterns that
are not explicitly stored in databases. Co-location patterns discovery is the
process of finding the subsets of features that are frequently located together
in the same geographic area. In this paper, we discuss the different approaches
like Rule based approach, Join-less approach, Partial Join approach and
Constraint neighborhood based approach for finding co-location patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1331</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1331</id><created>2014-02-06</created><authors><author><keyname>Bhattacharya</keyname><forenames>Abhishek</forenames></author><author><keyname>Chatterjee</keyname><forenames>Tanusree</forenames></author></authors><title>An Estimation Method of Measuring Image Quality for Compressed Images of
  Human Face</title><categories>cs.CV</categories><comments>6 pages</comments><doi>10.14445/22312803/IJCTT-V7P144</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays digital image compression and decompression techniques are very much
important. So our aim is to calculate the quality of face and other regions of
the compressed image with respect to the original image. Image segmentation is
typically used to locate objects and boundaries (lines, curves etc.)in images.
After segmentation the image is changed into something which is more meaningful
to analyze. Using Universal Image Quality Index(Q),Structural Similarity
Index(SSIM) and Gradient-based Structural Similarity Index(G-SSIM) it can be
shown that face region is less compressed than any other region of the image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1347</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1347</id><created>2014-02-06</created><authors><author><keyname>Praboo</keyname><forenames>N. N.</forenames></author><author><keyname>Bhaba</keyname><forenames>P. K.</forenames></author></authors><title>Simulation work on Fractional Order PI{\lambda} Control Strategy for
  speed control of DC motor based on stability boundary locus method</title><categories>cs.SY</categories><comments>7 pages, 13 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)</comments><journal-ref>IJETT, 4(8):3403-309, August 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the design of Fractional Order Proportional Integral
(FO-PI{\lambda}) controller for the speed control of DC motor. A mathematical
model of DC motor control system is derived and based on this model fractional
order PI{\lambda} controller is designed using stability boundary locus method
to satisfy required gain margin (GM) and phase margin (PM) of the system. Servo
and Regulatory tracking simulation runs are carried out for the speed control
of DC motor. The performance of the fractional order PI{\lambda}
(FO-PI{\lambda}) controller is compared with Integer Order Relay Feedback
Proportional Integral (IO-RFPI) controller. Finally the stability of both
control system is considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1348</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1348</id><created>2014-02-06</created><authors><author><keyname>Nayak</keyname><forenames>Deepak Ranjan</forenames></author><author><keyname>Sahu</keyname><forenames>Sumit Kumar</forenames></author><author><keyname>Mohammed</keyname><forenames>Jahangir</forenames></author></authors><title>A Cellular Automata based Optimal Edge Detection Technique using
  Twenty-Five Neighborhood Model</title><categories>cs.CV</categories><comments>7 pages, 9 figures</comments><journal-ref>International Journal of Computer Applications, Volume 84, Number
  10, Year of Publication: 2013</journal-ref><doi>10.5120/14614-2869</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular Automata (CA) are common and most simple models of parallel
computations. Edge detection is one of the crucial task in image processing,
especially in processing biological and medical images. CA can be successfully
applied in image processing. This paper presents a new method for edge
detection of binary images based on two dimensional twenty five neighborhood
cellular automata. The method considers only linear rules of CA for extraction
of edges under null boundary condition. The performance of this approach is
compared with some existing edge detection techniques. This comparison shows
that the proposed method to be very promising for edge detection of binary
images. All the algorithms and results used in this paper are prepared in
MATLAB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1349</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1349</id><created>2014-02-06</created><authors><author><keyname>Cheplygina</keyname><forenames>Veronika</forenames></author><author><keyname>Tax</keyname><forenames>David M. J.</forenames></author><author><keyname>Loog</keyname><forenames>Marco</forenames></author></authors><title>Dissimilarity-based Ensembles for Multiple Instance Learning</title><categories>stat.ML cs.LG</categories><comments>Submitted to IEEE Transactions on Neural Networks and Learning
  Systems, Special Issue on Learning in Non-(geo)metric Spaces</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multiple instance learning, objects are sets (bags) of feature vectors
(instances) rather than individual feature vectors. In this paper we address
the problem of how these bags can best be represented. Two standard approaches
are to use (dis)similarities between bags and prototype bags, or between bags
and prototype instances. The first approach results in a relatively
low-dimensional representation determined by the number of training bags, while
the second approach results in a relatively high-dimensional representation,
determined by the total number of instances in the training set. In this paper
a third, intermediate approach is proposed, which links the two approaches and
combines their strengths. Our classifier is inspired by a random subspace
ensemble, and considers subspaces of the dissimilarity space, defined by
subsets of instances, as prototypes. We provide guidelines for using such an
ensemble, and show state-of-the-art performances on a range of multiple
instance learning problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1359</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1359</id><created>2014-02-06</created><authors><author><keyname>Berger</keyname><forenames>Kai</forenames></author><author><keyname>Thiyagalingam</keyname><forenames>Jeyarajan</forenames></author></authors><title>Real-time Pedestrian Surveillance with Top View Cumulative Grids</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manuscript presents an efficient approach to map pedestrian surveillance
footage to an aerial view for global assessment of features. The analysis of
the footages relies on low level computer vision and enable real-time
surveillance. While we neglect object tracking, we introduce cumulative grids
on top view scene flow visualization to highlight situations of interest in the
footage. Our approach is tested on multiview footage both from RGB cameras and,
for the first time in the field, on RGB-D-sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1361</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1361</id><created>2014-02-06</created><authors><author><keyname>Fages</keyname><forenames>Jean-Guillaume</forenames></author><author><keyname>Chabert</keyname><forenames>Gilles</forenames></author><author><keyname>Prud'homme</keyname><forenames>Charles</forenames></author></authors><title>Combining finite and continuous solvers</title><categories>cs.AI</categories><comments>Presented at Workshop TRICS in conference CP'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Combining efficiency with reliability within CP systems is one of the main
concerns of CP developers. This paper presents a simple and efficient way to
connect Choco and Ibex, two CP solvers respectively specialised on finite and
continuous domains. This enables to take advantage of the most recent advances
of the continuous community within Choco while saving development and
maintenance resources, hence ensuring a better software quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1368</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1368</id><created>2014-02-06</created><authors><author><keyname>Csirmaz</keyname><forenames>Laszlo</forenames></author><author><keyname>Tardos</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>On-line secret sharing</title><categories>cs.IT cs.CR math.IT</categories><msc-class>05C85, 05C15, 94A62</msc-class><journal-ref>Des. Codes Cryptopgr (2012) 63: 127-147</journal-ref><doi>10.1007/s10623-011-9540-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an on-line secret sharing scheme the dealer assigns shares in the order
the participants show up, knowing only those qualified subsets whose all
members she has seen. We assume that the overall access structure is known and
only the order of the participants is unknown. On-line secret sharing is a
useful primitive when the set of participants grows in time, and redistributing
the secret is too expensive. In this paper we start the investigation of
unconditionally secure on-line secret sharing schemes. The complexity of a
secret sharing scheme is the size of the largest share a single participant can
receive over the size of the secret. The infimum of this amount in the on-line
or off-line setting is the on-line or off-line complexity of the access
structure, respectively. For paths on at most five vertices and cycles on at
most six vertices the on-line and offline complexities are equal, while for
other paths and cycles these values differ. We show that the gap between these
values can be arbitrarily large even for graph based access structures. We
present a general on-line secret sharing scheme that we call first-fit. Its
complexity is the maximal degree of the access structure. We show, however,
that this on-line scheme is never optimal: the on-line complexity is always
strictly less than the maximal degree. On the other hand, we give examples
where the first-fit scheme is almost optimal, namely, the on-line complexity
can be arbitrarily close to the maximal degree. The performance ratio is the
ratio of the on-line and off-line complexities of the same access structure. We
show that for graphs the performance ratio is smaller than the number of
vertices, and for an infinite family of graphs the performance ratio is at
least constant times the square root of the number of vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1371</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1371</id><created>2014-02-06</created><authors><author><keyname>Tax</keyname><forenames>David M. J.</forenames></author><author><keyname>Cheplygina</keyname><forenames>Veronika</forenames></author><author><keyname>Loog</keyname><forenames>Marco</forenames></author></authors><title>Quantile Representation for Indirect Immunofluorescence Image
  Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the diagnosis of autoimmune diseases, an important task is to classify
images of slides containing several HEp-2 cells. All cells from one slide share
the same label, and by classifying cells from one slide independently, some
information on the global image quality and intensity is lost. Considering one
whole slide as a collection (a bag) of feature vectors, however, poses the
problem of how to handle this bag. A simple, and surprisingly effective,
approach is to summarize the bag of feature vectors by a few quantile values
per feature. This characterizes the full distribution of all instances, thereby
assuming that all instances in a bag are informative. This representation is
particularly useful when each bag contains many feature vectors, which is the
case in the classification of the immunofluorescence images. Experiments on the
classification of indirect immunofluorescence images show the usefulness of
this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1377</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1377</id><created>2014-02-06</created><updated>2014-04-14</updated><authors><author><keyname>de Vasconcelos</keyname><forenames>Davi Romero</forenames></author><author><keyname>Haeusler</keyname><forenames>Edward Hermann</forenames></author></authors><title>Reasoning about Games via a First-order Modal Model Checking Approach</title><categories>cs.LO</categories><comments>Extended version of article published in the SBMF 2007. Accepted to
  ENTCS. Withdrawn from ENTCS in 2014 in virtue to submission to other venue</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present a logic based on first-order CTL, namely Game
Analysis Logic (GAL), in order to reason about games. We relate models and
solution concepts of Game Theory as models and formulas of GAL, respectively.
Precisely, we express extensive games with perfect in- formation as models of
GAL, and Nash equilibrium and subgame perfect equilibrium by means of formulas
of GAL. From a practical point of view, we provide a GAL model checker in order
to analyze games automatically. We use our model checker in at least two
directions: to find solution con- cepts of Game Theory; and, to analyze players
that are based on standard algorithms of the AI community, such as the minimax
procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1379</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1379</id><created>2014-02-06</created><authors><author><keyname>Fu</keyname><forenames>Zhang-Hua</forenames></author><author><keyname>Hao</keyname><forenames>Jin-Kao</forenames></author></authors><title>A Three-Phase Search Approach for the Quadratic Minimum Spanning Tree
  Problem</title><categories>cs.DS cs.NE</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph with costs associated with each edge as well as
each pair of edges, the quadratic minimum spanning tree problem (QMSTP)
consists of determining a spanning tree of minimum total cost. This problem can
be used to model many real-life network design applications, in which both
routing and interference costs should be considered. For this problem, we
propose a three-phase search approach named TPS, which integrates 1) a
descent-based neighborhood search phase using two different move operators to
reach a local optimum from a given starting solution, 2) a local optima
exploring phase to discover nearby local optima within a given regional search
area, and 3) a perturbation-based diversification phase to jump out of the
current regional search area. Additionally, we introduce dedicated techniques
to reduce the neighborhood to explore and streamline the neighborhood
evaluations. Computational experiments based on hundreds of representative
benchmarks show that TPS produces highly competitive results with respect to
the best performing approaches in the literature by improving the best known
results for 31 instances and matching the best known results for the remaining
instances only except two cases. Critical elements of the proposed algorithms
are analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1384</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1384</id><created>2014-02-06</created><authors><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Manoel</keyname><forenames>Andre</forenames></author><author><keyname>Tramel</keyname><forenames>Eric W.</forenames></author><author><keyname>Zdeborova</keyname><forenames>Lenka</forenames></author></authors><title>Variational Free Energies for Compressed Sensing</title><categories>cs.IT cond-mat.stat-mech math.IT</categories><comments>5 pages, 3 figures</comments><journal-ref>Information Theory Proceedings (ISIT), 2014 IEEE International
  Symposium on, page(s) 1499 - 1503</journal-ref><doi>10.1109/ISIT.2014.6875083</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the variational free energy approach for compressed sensing. We
first show that the na\&quot;ive mean field approach performs remarkably well when
coupled with a noise learning procedure. We also notice that it leads to the
same equations as those used for iterative thresholding. We then discuss the
Bethe free energy and how it corresponds to the fixed points of the approximate
message passing algorithm. In both cases, we test numerically the direct
optimization of the free energies as a converging sparse-estimationalgorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1386</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1386</id><created>2014-02-06</created><updated>2014-06-23</updated><authors><author><keyname>Singer</keyname><forenames>Philipp</forenames></author><author><keyname>Fl&#xf6;ck</keyname><forenames>Fabian</forenames></author><author><keyname>Meinhart</keyname><forenames>Clemens</forenames></author><author><keyname>Zeitfogel</keyname><forenames>Elias</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author></authors><title>Evolution of Reddit: From the Front Page of the Internet to a
  Self-referential Community?</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Published in the proceedings of WWW'14 companion</comments><acm-class>H.3.5</acm-class><doi>10.1145/2567948.2576943</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past few years, Reddit -- a community-driven platform for submitting,
commenting and rating links and text posts -- has grown exponentially, from a
small community of users into one of the largest online communities on the Web.
To the best of our knowledge, this work represents the most comprehensive
longitudinal study of Reddit's evolution to date, studying both (i) how user
submissions have evolved over time and (ii) how the community's allocation of
attention and its perception of submissions have changed over 5 years based on
an analysis of almost 60 million submissions. Our work reveals an
ever-increasing diversification of topics accompanied by a simultaneous
concentration towards a few selected domains both in terms of posted
submissions as well as perception and attention. By and large, our
investigations suggest that Reddit has transformed itself from a dedicated
gateway to the Web to an increasingly self-referential community that focuses
on and reinforces its own user-generated image- and textual content over
external sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1389</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1389</id><created>2014-02-06</created><updated>2014-09-29</updated><authors><author><keyname>Gal</keyname><forenames>Yarin</forenames></author><author><keyname>van der Wilk</keyname><forenames>Mark</forenames></author><author><keyname>Rasmussen</keyname><forenames>Carl E.</forenames></author></authors><title>Distributed Variational Inference in Sparse Gaussian Process Regression
  and Latent Variable Models</title><categories>stat.ML cs.LG</categories><comments>9 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian processes (GPs) are a powerful tool for probabilistic inference over
functions. They have been applied to both regression and non-linear
dimensionality reduction, and offer desirable properties such as uncertainty
estimates, robustness to over-fitting, and principled ways for tuning
hyper-parameters. However the scalability of these models to big datasets
remains an active topic of research. We introduce a novel re-parametrisation of
variational inference for sparse GP regression and latent variable models that
allows for an efficient distributed algorithm. This is done by exploiting the
decoupling of the data given the inducing points to re-formulate the evidence
lower bound in a Map-Reduce setting. We show that the inference scales well
with data and computational resources, while preserving a balanced distribution
of the load among the nodes. We further demonstrate the utility in scaling
Gaussian processes to big data. We show that GP performance improves with
increasing amounts of data in regression (on flight data with 2 million
records) and latent variable modelling (on MNIST). The results show that GPs
perform better than many common models often used for big data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1399</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1399</id><created>2014-02-06</created><authors><author><keyname>Toloza</keyname><forenames>Juan</forenames></author><author><keyname>Acosta</keyname><forenames>Nelson</forenames></author><author><keyname>Kornuta</keyname><forenames>Carlos</forenames></author></authors><title>WiFiPos: An In/Out-Door Positioning Tool</title><categories>cs.NI</categories><comments>7 pages, 6 figures, &quot;Published with International Journal of Computer
  Trends and Technology (IJCTT)&quot;</comments><journal-ref>International Journal of Computer Trends and Technology
  (IJCTT.6(2).1-7.December 2013.Published by Seventh Sense Research Group</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Geographical location solutions have a wide diversity of applications,
ranging from emergency services to access to tourist and entertainment
services. GPS (Global Positioning System) is the most widely used system for
outdoor areas. Since it requires a direct line of sight with the satellites, it
cannot be used indoors. For indoor positioning, the most commonly used
technology to calculate the position of mobile devices is Wi-Fi. In this
article, we present a positioning tool called WiFiPos based on Wi-Fi signal
processing. This tool uses different variations of the Fingerprint technique to
analyze the performance and accuracy of the system both indoors and outdoors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1425</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1425</id><created>2014-02-06</created><authors><author><keyname>Nevries</keyname><forenames>Ragnar</forenames></author><author><keyname>Rosenke</keyname><forenames>Christian</forenames></author></authors><title>Towards a Characterization of Leaf Powers by Clique Arrangements</title><categories>cs.DM math.CO</categories><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The class ${\cal L}_k$ of $k$-leaf powers consists of graphs $G=(V,E)$ that
have a $k$-leaf root, that is, a tree $T$ with leaf set $V$, where $xy \in E$,
if and only if the $T$-distance between $x$ and $y$ is at most $k$. Structure
and linear time recognition algorithms have been found for $2$-, $3$-, $4$-,
and, to some extent, $5$-leaf powers, and it is known that the union of all
$k$-leaf powers, that is, the graph class ${\cal L} = \bigcup_{k=2}^\infty
{\cal L}_k$, forms a proper subclass of strongly chordal graphs. Despite from
that, no essential progress has been made lately. In this paper, we use the new
notion of clique arrangements to suggest that leaf powers are a natural special
case of strongly chordal graphs. The clique arrangement ${\cal A}(G)$ of a
chordal graph $G$ is a directed graph that represents the intersections between
maximal cliques of $G$ by nodes and the mutual inclusion of these vertex
subsets by arcs. Recently, strongly chordal graphs have been characterized as
the graphs that have a clique arrangement without bad $k$-cycles for $k \geq
3$. We show that the clique arrangement of every graph of ${\cal L}$ is free of
bad $2$-cycles. The question whether this characterizes the class ${\cal L}$
exactly remains open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1429</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1429</id><created>2014-02-06</created><authors><author><keyname>Gaubert</keyname><forenames>Stephane</forenames></author><author><keyname>Qu</keyname><forenames>Zheng</forenames></author></authors><title>Checking the strict positivity of Kraus maps is NP-hard</title><categories>cs.CC cs.IT math.IT math.OA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Basic properties in Perron-Frobenius theory are strict positivity,
primitivityand irreducibility. Whereas for nonnegative matrices, these
properties are equivalent to elementary graph properties which can be checked
in polynomial time, we show that for Kraus maps- the noncommutative
generalization of stochastic matrices - checking strict positivity (whether the
map sends the cone to its interior) is NP-hard. The proof proceeds by reducing
to the latter problem the existence of a non-zero solution of a special system
of bilinear equations. The complexity of irreducibility and primitivity is also
discussed in the noncommutative setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1438</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1438</id><created>2014-02-05</created><authors><author><keyname>Candlot</keyname><forenames>Alexandre</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Perry</keyname><forenames>Nicolas</forenames><affiliation>LGM2B</affiliation></author><author><keyname>Bernard</keyname><forenames>Alain</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Ammar-Khodja</keyname><forenames>Samar</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Deployment of an Innovative Resource Choice Method for Process Planning</title><categories>cs.OH</categories><proxy>ccsd</proxy><journal-ref>CIRP Journal of Manufacturing Systems 35, 5 (2006) 487-506</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designers, process planners and manufacturers naturally consider different
concepts for a same object. The stiffness of production means and the design
specification requirements mark out process planners as responsible of the
coherent integration of all constraints. First, this paper details an
innovative solution of resource choice, applied for aircraft manufacturing
parts. In a second part, key concepts are instanced for the considered
industrial domain. Finally, a digital mock up validates the solution viability
and demonstrates the possibility of an in-process knowledge capitalisation and
use. Formalising the link between Design and Manufacturing allows to hope
enhancements of simultaneous Product / Process developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1450</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1450</id><created>2014-02-06</created><updated>2014-10-22</updated><authors><author><keyname>Bortolussi</keyname><forenames>Luca</forenames></author><author><keyname>Milios</keyname><forenames>Dimitrios</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Guido</forenames></author></authors><title>Smoothed Model Checking for Uncertain Continuous Time Markov Chains</title><categories>cs.LO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing the satisfaction probability of a
formula for stochastic models with parametric uncertainty. We show that this
satisfaction probability is a smooth function of the model parameters. This
enables us to devise a novel Bayesian statistical algorithm which performs
statistical model checking simultaneously for all values of the model
parameters from observations of truth values of the formula over individual
runs of the model at isolated parameter values. This is achieved by exploiting
the smoothness of the satisfaction function: by modelling explicitly
correlations through a prior distribution over a space of smooth functions (a
Gaussian Process), we can condition on observations at individual parameter
values to construct an analytical approximation of the function itself.
Extensive experiments on non-trivial case studies show that the approach is
accurate and several orders of magnitude faster than naive parameter
exploration with standard statistical model checking methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1454</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1454</id><created>2014-02-06</created><authors><author><keyname>P</keyname><forenames>Sarath Chandar A</forenames></author><author><keyname>Lauly</keyname><forenames>Stanislas</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author><author><keyname>Khapra</keyname><forenames>Mitesh M.</forenames></author><author><keyname>Ravindran</keyname><forenames>Balaraman</forenames></author><author><keyname>Raykar</keyname><forenames>Vikas</forenames></author><author><keyname>Saha</keyname><forenames>Amrita</forenames></author></authors><title>An Autoencoder Approach to Learning Bilingual Word Representations</title><categories>cs.CL cs.LG stat.ML</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross-language learning allows us to use training data from one language to
build models for a different language. Many approaches to bilingual learning
require that we have word-level alignment of sentences from parallel corpora.
In this work we explore the use of autoencoder-based methods for cross-language
learning of vectorial word representations that are aligned between two
languages, while not relying on word-level alignments. We show that by simply
learning to reconstruct the bag-of-words representations of aligned sentences,
within and between languages, we can in fact learn high-quality representations
and do without word alignments. Since training autoencoders on word
observations presents certain computational issues, we propose and compare
different variations adapted to this setting. We also propose an explicit
correlation maximizing regularizer that leads to significant improvement in the
performance. We empirically investigate the success of our approach on the
problem of cross-language test classification, where a classifier trained on a
given language (e.g., English) must learn to generalize to a different language
(e.g., German). These experiments demonstrate that our approaches are
competitive with the state-of-the-art, achieving up to 10-14 percentage point
improvements over the best reported results on this task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1467</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1467</id><created>2014-02-06</created><authors><author><keyname>Nikulchev</keyname><forenames>Evgeny</forenames></author></authors><title>Reconstruction Models for Attractors in the Technical and Economic
  Processes</title><categories>cs.CE</categories><comments>5 pages</comments><msc-class>94-02</msc-class><journal-ref>International Journal of Computer Trends and Technology. 2013. V.6
  N.3. P.171-175</journal-ref><doi>10.14445/22312803/IJCTT-V6N3P128</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article discusses building models based on the reconstructed attractors
of the time series. Discusses the use of the properties of dynamical chaos,
namely to identify the strange attractors structure models. Here is used the
group properties of differential equations, which consist in the symmetry of
particular solutions. Examples of modeling engineering systems are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1469</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1469</id><created>2014-02-06</created><authors><author><keyname>Pluzhnik</keyname><forenames>Evgeniy</forenames></author><author><keyname>Nikulchev</keyname><forenames>Evgeny</forenames></author></authors><title>Use of Dynamical Systems Modeling to Hybrid Cloud Database</title><categories>cs.DB</categories><comments>8 pages</comments><journal-ref>International Journal of Communications, Network and System
  Sciences.2013.V.6. N.12. PP. 505-512</journal-ref><doi>10.4236/ijcns.2013.612054</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the article, an experiment is aimed at clarifying the transfer efficiency
of the database in the cloud infrastructure. The system was added to the
control unit, which has guided the database search in the local part or in the
cloud. It is shown that the time data acquisition remains unchanged as a result
of modification. Suggestions have been made about the use of the theory of
dynamic systems to hybrid cloud database. The present work is aimed at
attracting the attention of spe-cialists in the field of cloud database to the
apparatus control theory. The experiment presented in this article allows the
use of the description of the known methods for solving important practical
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1473</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1473</id><created>2014-02-06</created><authors><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Guibas</keyname><forenames>Leonidas J.</forenames></author><author><keyname>Huang</keyname><forenames>Qi-Xing</forenames></author></authors><title>Near-Optimal Joint Object Matching via Convex Relaxation</title><categories>cs.LG cs.CV cs.IT math.IT math.OC stat.ML</categories><journal-ref>31st International Conference on Machine Learning, vol. 32, pp.
  100 - 108, June 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joint matching over a collection of objects aims at aggregating information
from a large collection of similar instances (e.g. images, graphs, shapes) to
improve maps between pairs of them. Given multiple matches computed between a
few object pairs in isolation, the goal is to recover an entire collection of
maps that are (1) globally consistent, and (2) close to the provided maps ---
and under certain conditions provably the ground-truth maps. Despite recent
advances on this problem, the best-known recovery guarantees are limited to a
small constant barrier --- none of the existing methods find theoretical
support when more than $50\%$ of input correspondences are corrupted. Moreover,
prior approaches focus mostly on fully similar objects, while it is practically
more demanding to match instances that are only partially similar to each
other.
  In this paper, we develop an algorithm to jointly match multiple objects that
exhibit only partial similarities, given a few pairwise matches that are
densely corrupted. Specifically, we propose to recover the ground-truth maps
via a parameter-free convex program called MatchLift, following a spectral
method that pre-estimates the total number of distinct elements to be matched.
Encouragingly, MatchLift exhibits near-optimal error-correction ability, i.e.
in the asymptotic regime it is guaranteed to work even when a dominant fraction
$1-\Theta\left(\frac{\log^{2}n}{\sqrt{n}}\right)$ of the input maps behave like
random outliers. Furthermore, MatchLift succeeds with minimal input complexity,
namely, perfect matching can be achieved as soon as the provided maps form a
connected map graph. We evaluate the proposed algorithm on various benchmark
data sets including synthetic examples and real-world examples, all of which
confirm the practical applicability of MatchLift.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1484</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1484</id><created>2014-02-06</created><authors><author><keyname>Emiris</keyname><forenames>Ioannis Z.</forenames></author><author><keyname>Psarros</keyname><forenames>Ioannis</forenames></author></authors><title>Counting Euclidean embeddings of rigid graphs</title><categories>cs.CG math.CO</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is called (generically) rigid in $\mathbb{R}^d$ if, for any choice of
sufficiently generic edge lengths, it can be embedded in $\mathbb{R}^d$ in a
finite number of distinct ways, modulo rigid transformations. Here we deal with
the problem of determining the maximum number of planar Euclidean embeddings as
a function of the number of the vertices. We obtain polynomial systems which
totally capture the structure of a given graph, by exploiting distance geometry
theory. Consequently, counting the number of Euclidean embeddings of a given
rigid graph, reduces to the problem of counting roots of the corresponding
polynomial system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1485</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1485</id><created>2014-02-06</created><authors><author><keyname>S&#xfd;kora</keyname><forenames>Jan</forenames></author><author><keyname>Ku&#x10d;erov&#xe1;</keyname><forenames>Anna</forenames></author></authors><title>Uncertainty Propagation in Elasto-Plastic Material</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Macroscopically heterogeneous materials, characterised mostly by comparable
heterogeneity lengthscale and structural sizes, can no longer be modelled by
deterministic approach instead. It is convenient to introduce stochastic
approach with uncertain material parameters quantified as random fields and/or
random variables. The present contribution is devoted to propagation of these
uncertainties in mechanical modelling of inelastic behaviour. In such case the
Monte Carlo method is the traditional approach for solving the proposed
problem. Nevertheless, convergence rate is relatively slow, thus new methods
(e.g. stochastic Galerkin method, stochastic collocation approach, etc.) have
been recently developed to offer fast convergence for sufficiently smooth
solution in the probability space. Our goal is to accelerate the uncertainty
propagation using a polynomial chaos expansion based on stochastic collocation
method. The whole concept is demonstrated on a simple numerical example of
uniaxial test at a material point where interesting phenomena can be clearly
understood.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1500</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1500</id><created>2014-02-06</created><updated>2014-05-15</updated><authors><author><keyname>Shaham</keyname><forenames>Eran</forenames></author><author><keyname>Sarne</keyname><forenames>David</forenames></author><author><keyname>Ben-Moshe</keyname><forenames>Boaz</forenames></author></authors><title>Co-clustering of Fuzzy Lagged Data</title><categories>cs.AI</categories><comments>Under consideration for publication in Knowledge and Information
  Systems. The final publication is available at Springer via
  http://dx.doi.org/10.1007/s10115-014-0758-7</comments><doi>10.1007/s10115-014-0758-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper focuses on mining patterns that are characterized by a fuzzy lagged
relationship between the data objects forming them. Such a regulatory mechanism
is quite common in real life settings. It appears in a variety of fields:
finance, gene expression, neuroscience, crowds and collective movements are but
a limited list of examples. Mining such patterns not only helps in
understanding the relationship between objects in the domain, but assists in
forecasting their future behavior. For most interesting variants of this
problem, finding an optimal fuzzy lagged co-cluster is an NP-complete problem.
We thus present a polynomial-time Monte-Carlo approximation algorithm for
mining fuzzy lagged co-clusters. We prove that for any data matrix, the
algorithm mines a fuzzy lagged co-cluster with fixed probability, which
encompasses the optimal fuzzy lagged co-cluster by a maximum 2 ratio columns
overhead and completely no rows overhead. Moreover, the algorithm handles
noise, anti-correlations, missing values and overlapping patterns. The
algorithm was extensively evaluated using both artificial and real datasets.
The results not only corroborate the ability of the algorithm to efficiently
mine relevant and accurate fuzzy lagged co-clusters, but also illustrate the
importance of including the fuzziness in the lagged-pattern model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1503</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1503</id><created>2014-02-06</created><authors><author><keyname>Arif</keyname><forenames>Omar</forenames></author><author><keyname>Sundaramoorthi</keyname><forenames>Ganesh</forenames></author><author><keyname>Hong</keyname><forenames>Byung-Woo</forenames></author><author><keyname>Yezzi</keyname><forenames>Anthony</forenames></author></authors><title>Tracking via Motion Estimation with Physically Motivated Inter-Region
  Constraints</title><categories>cs.CV</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a method for tracking structures (e.g., ventricles
and myocardium) in cardiac images (e.g., magnetic resonance) by propagating
forward in time a previous estimate of the structures via a new deformation
estimation scheme that is motivated by physical constraints of fluid motion.
The method employs within structure motion estimation (so that differing
motions among different structures are not mixed) while simultaneously
satisfying the physical constraint in fluid motion that at the interface
between a fluid and a medium, the normal component of the fluid's motion must
match the normal component of the motion of the medium. We show how to estimate
the motion according to the previous considerations in a variational framework,
and in particular, show that these conditions lead to PDEs with boundary
conditions at the interface that resemble Robin boundary conditions and induce
coupling between structures. We illustrate the use of this motion estimation
scheme in propagating a segmentation across frames and show that it leads to
more accurate segmentation than traditional motion estimation that does not
make use of physical constraints. Further, the method is naturally suited to
interactive segmentation methods, which are prominently used in practice in
commercial applications for cardiac analysis, where typically a segmentation
from the previous frame is used to predict a segmentation in the next frame. We
show that our propagation scheme reduces the amount of user interaction by
predicting more accurate segmentations than commonly used and recent
interactive commercial techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1515</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1515</id><created>2014-02-06</created><updated>2014-12-07</updated><authors><author><keyname>Chen</keyname><forenames>Jianshu</forenames></author><author><keyname>Towfic</keyname><forenames>Zaid J.</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Dictionary Learning over Distributed Models</title><categories>cs.LG cs.DC</categories><comments>16 pages, 8 figures. To appear in IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2014.2385045</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider learning dictionary models over a network of
agents, where each agent is only in charge of a portion of the dictionary
elements. This formulation is relevant in Big Data scenarios where large
dictionary models may be spread over different spatial locations and it is not
feasible to aggregate all dictionaries in one location due to communication and
privacy considerations. We first show that the dual function of the inference
problem is an aggregation of individual cost functions associated with
different agents, which can then be minimized efficiently by means of diffusion
strategies. The collaborative inference step generates dual variables that are
used by the agents to update their dictionaries without the need to share these
dictionaries or even the coefficient models for the training data. This is a
powerful property that leads to an effective distributed procedure for learning
dictionaries over large networks (e.g., hundreds of agents in our experiments).
Furthermore, the proposed learning strategy operates in an online manner and is
able to respond to streaming data, where each data sample is presented to the
network once.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1519</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1519</id><created>2014-02-06</created><authors><author><keyname>Barik</keyname><forenames>Somsubhra</forenames></author><author><keyname>Vikalo</keyname><forenames>Haris</forenames></author></authors><title>Sparsity-aware sphere decoding: Algorithms and complexity analysis</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2014.2307836</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integer least-squares problems, concerned with solving a system of equations
where the components of the unknown vector are integer-valued, arise in a wide
range of applications. In many scenarios the unknown vector is sparse, i.e., a
large fraction of its entries are zero. Examples include applications in
wireless communications, digital fingerprinting, and array-comparative genomic
hybridization systems. Sphere decoding, commonly used for solving integer
least-squares problems, can utilize the knowledge about sparsity of the unknown
vector to perform computationally efficient search for the solution. In this
paper, we formulate and analyze the sparsity-aware sphere decoding algorithm
that imposes $\ell_0$-norm constraint on the admissible solution. Analytical
expressions for the expected complexity of the algorithm for alphabets typical
of sparse channel estimation and source allocation applications are derived and
validated through extensive simulations. The results demonstrate superior
performance and speed of sparsity-aware sphere decoder compared to the
conventional sparsity-unaware sphere decoding algorithm. Moreover, variance of
the complexity of the sparsity-aware sphere decoding algorithm for binary
alphabets is derived. The search space of the proposed algorithm can be further
reduced by imposing lower bounds on the value of the objective function. The
algorithm is modified to allow for such a lower bounding technique and
simulations illustrating efficacy of the method are presented. Performance of
the algorithm is demonstrated in an application to sparse channel estimation,
where it is shown that sparsity-aware sphere decoder performs close to
theoretical lower limits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1523</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1523</id><created>2014-02-06</created><authors><author><keyname>Fabris</keyname><forenames>Antonio Elias</forenames></author><author><keyname>Nascimento</keyname><forenames>Marcelo Zanchetta do</forenames></author><author><keyname>Batista</keyname><forenames>Val&#xe9;rio Ramos</forenames></author></authors><title>Programming plantation lines on driverless tractors</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in Agricultural Engineering include image processing,
robotics and geographic information systems (GIS). Some tasks are still
accomplished manually, like drawing plantation lines that optimize
productivity. Herewith we present an algorithm to find the optimal plantation
lines in linear time. The algorithm is based upon classical results of Geometry
which enabled a source code with only 573 lines. We have implemented it in
Matlab for sugar cane, and it can be easily adapted to other crops like coffee,
maize and soy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1526</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1526</id><created>2014-02-06</created><updated>2015-11-18</updated><authors><author><keyname>Gaboardi</keyname><forenames>Marco</forenames></author><author><keyname>Arias</keyname><forenames>Emilio Jes&#xfa;s Gallego</forenames></author><author><keyname>Hsu</keyname><forenames>Justin</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Wu</keyname><forenames>Zhiwei Steven</forenames></author></authors><title>Dual Query: Practical Private Query Release for High Dimensional Data</title><categories>cs.DS cs.CR cs.DB cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a practical, differentially private algorithm for answering a
large number of queries on high dimensional datasets. Like all algorithms for
this task, ours necessarily has worst-case complexity exponential in the
dimension of the data. However, our algorithm packages the computationally hard
step into a concisely defined integer program, which can be solved
non-privately using standard solvers. We prove accuracy and privacy theorems
for our algorithm, and then demonstrate experimentally that our algorithm
performs well in practice. For example, our algorithm can efficiently and
accurately answer millions of queries on the Netflix dataset, which has over
17,000 attributes; this is an improvement on the state of the art by multiple
orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1530</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1530</id><created>2014-02-06</created><authors><author><keyname>Compagnoni</keyname><forenames>Marco</forenames></author><author><keyname>Notari</keyname><forenames>Roberto</forenames></author></authors><title>TDOA--based localization in two dimensions: the bifurcation curve</title><categories>cs.SD gr-qc math.AC</categories><comments>11 pages, 3 figures, to appear in Fundamenta Informaticae</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we complete the study of the geometry of the TDOA map that
encodes the noiseless model for the localization of a source from the range
differences between three receivers in a plane, by computing the Cartesian
equation of the bifurcation curve in terms of the positions of the receivers.
From that equation, we can compute its real asymptotic lines. The present
manuscript completes the analysis of [Inverse Problems, Vol. 30, Number 3,
Pages 035004]. Our result is useful to check if a source belongs or is closed
to the bifurcation curve, where the localization in a noisy scenario is
ambiguous.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1535</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1535</id><created>2014-02-06</created><authors><author><keyname>Fernandes</keyname><forenames>R. Q. A</forenames></author><author><keyname>Haeusler</keyname><forenames>E. H.</forenames></author><author><keyname>Pereira</keyname><forenames>L. C. P. D</forenames></author></authors><title>PUC-Logic</title><categories>cs.LO</categories><comments>33 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a logic for Proximity-based Understanding of Conditionals
(PUC-Logic) that unifies the Counterfactual and Deontic logics proposed by
David Lewis. We also propose a natural deduction system (PUC-ND) associated to
this new logic. This inference system is proven to be sound, complete,
normalizing and decidable. The relative completeness for the $\boldsymbol{V}$
and $\boldsymbol{CO}$ logics is shown to emphasize the unified approach over
the work of Lewis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1545</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1545</id><created>2014-02-06</created><authors><author><keyname>Niznik</keyname><forenames>Dr. Carol A.</forenames></author></authors><title>The Tactical Optimal Strategy Game (TOSG) Protocol Cockpit Software
  Control For Massive Ordnance Penetrator Release</title><categories>cs.CY cs.GT</categories><comments>e-ISSN: 2251-7545</comments><journal-ref>The International Journal of Soft Computing and Software
  Engineering [JSCSE], Vol. 3, No. 3, 2013, pp.474-480</journal-ref><doi>10.7321/jscse.v3.n3.72</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The Massive Ordnance Penetrator(MOP) has been developed to destroy deeply
buried nuclear components by controlled release from a B2 or B52 airplane. This
type of release must be cockpit software controlled by the Tactical Optimal
Strategy Game(TOSG) Protocol to optimally determine the war game aspects of the
dueling from other countries' MOP releases, and the depth at which the MOP
explosions can occur for maximal safety and risk concerns. The TOSG Protocol
characteristics of games of strategy, games of optimal strategy and tactical
games are defined initially by the game of strategy as a certain series of
events, each of which must have a finite number of distinct results. The
outcome of a game of strategy, in some cases, depends on chance. All other
events depend on the free decision of the players. A game has a solution if
there exist two strategies, which become optimal strategies when each
mathematically attains the value of the game. The TOSG Protocol war game
tactical problem for a class of games can be mathematically modeled as a combat
between two airplanes, each carrying a MOP as the specification of the accuracy
of the firing machinery and the total amount of ammunition that each plane
carries. This silent duel occurs, because each MOP bomber is unable to
determine the number of times its opponent has missed. The TOSG Protocol
realizes a game theory solution of the tactical optimal strategy game utilizing
the theory of games of timing, games of pursuit, games of time lag, games of
sequence, games of maneuvering, games of search, games of positioning and games
of aiming and evasion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1546</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1546</id><created>2014-02-06</created><authors><author><keyname>Song</keyname><forenames>Renchu</forenames></author><author><keyname>Sun</keyname><forenames>Weiwei</forenames></author><author><keyname>Zheng</keyname><forenames>Baihua</forenames></author><author><keyname>Zheng</keyname><forenames>Yu</forenames></author></authors><title>PRESS: A Novel Framework of Trajectory Compression in Road Networks</title><categories>cs.DB</categories><comments>27 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location data becomes more and more important. In this paper, we focus on the
trajectory data, and propose a new framework, namely PRESS (Paralleled
Road-Network-Based Trajectory Compression), to effectively compress trajectory
data under road network constraints. Different from existing work, PRESS
proposes a novel representation for trajectories to separate the spatial
representation of a trajectory from the temporal representation, and proposes a
Hybrid Spatial Compression (HSC) algorithm and error Bounded Temporal
Compression (BTC) algorithm to compress the spatial and temporal information of
trajectories respectively. PRESS also supports common spatial-temporal queries
without fully decompressing the data. Through an extensive experimental study
on real trajectory dataset, PRESS significantly outperforms existing approaches
in terms of saving storage cost of trajectory data with bounded errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1557</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1557</id><created>2014-02-07</created><authors><author><keyname>Zhang</keyname><forenames>Xinchen</forenames></author><author><keyname>Haenggi</keyname><forenames>Martin</forenames></author></authors><title>The Performance of Successive Interference Cancellation in Random
  Wireless Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a unified framework to study the performance of
successive interference cancellation (SIC) in wireless networks with arbitrary
fading distribution and power-law path loss. An analytical characterization of
the performance of SIC is given as a function of different system parameters.
The results suggest that the marginal benefit of enabling the receiver to
successively decode k users diminishes very fast with k, especially in networks
of high dimensions and small path loss exponent. On the other hand, SIC is
highly beneficial when the users are clustered around the receiver and/or very
low-rate codes are used. Also, with multiple packet reception, a lower per-user
information rate always results in higher aggregate throughput in
interference-limited networks. In contrast, there exists a positive optimal
per-user rate that maximizes the aggregate throughput in noisy networks.
  The analytical results serve as useful tools to understand the potential gain
of SIC in heterogeneous cellular networks (HCNs). Using these tools, this paper
quantifies the gain of SIC on the coverage probability in HCNs with
non-accessible base stations. An interesting observation is that, for
contemporary narrow-band systems (e.g., LTE and WiFi), most of the gain of SIC
is achieved by canceling a single interferer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1563</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1563</id><created>2014-02-07</created><authors><author><keyname>Lamersdorf</keyname><forenames>Ansgar</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Rombach</keyname><forenames>Dieter</forenames></author></authors><title>Towards a Multi-criteria Development Distribution Model: An Analysis of
  Existing Task Distribution Approaches</title><categories>cs.SE</categories><comments>10 pages. The final publication is available at
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4638658</comments><journal-ref>IEEE International Conference on Global Software Engineering
  (ICGSE 2008), pages 109-118, 2008</journal-ref><doi>10.1109/ICGSE.2008.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributing development tasks in the context of global software development
bears both many risks and many opportunities. Nowadays, distributed development
is often driven by only a few factors or even just a single factor such as
workforce costs. Risks and other relevant factors such as workforce
capabilities, the innovation potential of different regions, or cultural
factors are often not recognized sufficiently. This could be improved by using
empirically-based multi-criteria distribution models. Currently, there is a
lack of such decision models for distributing software development work. This
article focuses on mechanisms for such decision support. First, requirements
for a distribution model are formulated based on needs identified from
practice. Then, distribution models from different domains are surveyed,
compared, and analyzed in terms of suitability. Finally, research questions and
directions for future work are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1572</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1572</id><created>2014-02-07</created><authors><author><keyname>Cardone</keyname><forenames>Martina</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Knopp</keyname><forenames>Raymond</forenames></author><author><keyname>Salim</keyname><forenames>Umer</forenames></author></authors><title>New Outer Bounds for the Interference Channel with Unilateral Source
  Cooperation</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE International Symposium on Information Theory
  (ISIT) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the two-user interference channel with unilateral source
cooperation, which consists of two source-destination pairs that share the same
channel and where one full-duplex source can overhear the other source through
a noisy in-band link. Novel outer bounds of the type 2Rp+Rc/Rp+2Rc are
developed for the class of injective semi-deterministic channels with
independent noises at the different source-destination pairs. The bounds are
then specialized to the Gaussian noise case. Interesting insights are provided
about when these types of bounds are active, or in other words, when unilateral
cooperation is too weak and leaves &quot;holes&quot; in the system resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1587</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1587</id><created>2014-02-07</created><authors><author><keyname>Bonsma</keyname><forenames>Paul</forenames></author></authors><title>Independent Set Reconfiguration in Cographs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the following independent set reconfiguration problem, called
TAR-Reachability: given two independent sets $I$ and $J$ of a graph $G$, both
of size at least $k$, is it possible to transform $I$ into $J$ by adding and
removing vertices one-by-one, while maintaining an independent set of size at
least $k$ throughout? This problem is known to be PSPACE-hard in general. For
the case that $G$ is a cograph (i.e. $P_4$-free graph) on $n$ vertices, we show
that it can be solved in time $O(n^2)$, and that the length of a shortest
reconfiguration sequence from $I$ to $J$ is bounded by $4n-2k$, if such a
sequence exists.
  More generally, we show that if $X$ is a graph class for which (i)
TAR-Reachability can be solved efficiently, (ii) maximum independent sets can
be computed efficiently, and which satisfies a certain additional property,
then the problem can be solved efficiently for any graph that can be obtained
from a collection of graphs in $X$ using disjoint union and complete join
operations. Chordal graphs are given as an example of such a class $X$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1605</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1605</id><created>2014-02-07</created><updated>2015-09-10</updated><authors><author><keyname>Wahls</keyname><forenames>Sander</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Fast Numerical Nonlinear Fourier Transforms</title><categories>cs.IT math.IT math.NA nlin.SI physics.comp-ph</categories><comments>Minor changes</comments><journal-ref>IEEE Transactions on Information Theory, vol. 61, no. 12, pp.
  6957-6974, Dec. 2015</journal-ref><doi>10.1109/TIT.2015.2485944</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nonlinear Fourier transform, which is also known as the forward
scattering transform, decomposes a periodic signal into nonlinearly interacting
waves. In contrast to the common Fourier transform, these waves no longer have
to be sinusoidal. Physically relevant waveforms are often available for the
analysis instead. The details of the transform depend on the waveforms
underlying the analysis, which in turn are specified through the implicit
assumption that the signal is governed by a certain evolution equation. For
example, water waves generated by the Korteweg-de Vries equation can be
expressed in terms of cnoidal waves. Light waves in optical fiber governed by
the nonlinear Schr\&quot;odinger equation (NSE) are another example. Nonlinear
analogs of classic problems such as spectral analysis and filtering arise in
many applications, with information transmission in optical fiber, as proposed
by Yousefi and Kschischang, being a very recent one. The nonlinear Fourier
transform is eminently suited to address them -- at least from a theoretical
point of view. Although numerical algorithms are available for computing the
transform, a &quot;fast&quot; nonlinear Fourier transform that is similarly effective as
the fast Fourier transform is for computing the common Fourier transform has
not been available so far. The goal of this paper is to address this problem.
Two fast numerical methods for computing the nonlinear Fourier transform with
respect to the NSE are presented. The first method achieves a runtime of
$O(D^2)$ floating point operations, where $D$ is the number of sample points.
The second method applies only to the case where the NSE is defocusing, but it
achieves an $O(D\log^2D)$ runtime. Extensions of the results to other evolution
equations are discussed as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1607</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1607</id><created>2014-02-07</created><authors><author><keyname>Liu</keyname><forenames>Kangqi</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Xiang</keyname><forenames>Zhengzheng</forenames></author><author><keyname>Long</keyname><forenames>Xin</forenames></author></authors><title>Generalized Signal Alignment For MIMO Two-Way X Relay Channels</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures, to appear in IEEE ICC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the degrees of freedom (DoF) of MIMO two-way X relay channels.
Previous work studied the case $N &lt; 2M$, where $N$ and $M$ denote the number of
antennas at the relay and each source, respectively, and showed that the
maximum DoF of $2N$ is achievable when $N \leq \lfloor\frac{8M}{5}\rfloor$ by
applying signal alignment (SA) for network coding and interference cancelation.
This work considers the case $N&gt;2M$ where the performance is limited by the
number of antennas at each source node and conventional SA is not feasible. We
propose a \textit{generalized signal alignment} (GSA) based transmission
scheme. The key is to let the signals to be exchanged between every source node
align in a transformed subspace, rather than the direct subspace, at the relay
so as to form network-coded signals. This is realized by jointly designing the
precoding matrices at all source nodes and the processing matrix at the relay.
Moreover, the aligned subspaces are orthogonal to each other. By applying the
GSA, we show that the DoF upper bound $4M$ is achievable when $M \leq
\lfloor\frac{2N}{5}\rfloor$ ($M$ is even) or $M \leq
\lfloor\frac{2N-1}{5}\rfloor$ ($M$ is odd). Numerical results also demonstrate
that our proposed transmission scheme is feasible and effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1614</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1614</id><created>2014-02-07</created><updated>2014-02-26</updated><authors><author><keyname>samadieh</keyname><forenames>Mehdi</forenames></author><author><keyname>Gholami</keyname><forenames>Mohammad</forenames></author></authors><title>New LDPC Codes Using Permutation Matrices with Higher Girth than QC-LDPC
  Codes Constructed by Fossorier</title><categories>cs.IT cs.DM math.CO math.IT</categories><comments>This paper has been withdrawn by the authors. Some conclusions in the
  submitted paper are wrong and the authors decided to withdraw the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the literatures, it is well-known that Fossorier code has the girth among
LDPC codes. In this paper, we introduce a new class of low-density parity-check
(LDPC) codes, with higher girth than other previous constructed codes.
Especially we proposed a new method to construct LDPC codes using non ?xed
shift permutation matrices and full based matrices with higher girth than codes
constructed by Fossorier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1616</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1616</id><created>2014-02-07</created><authors><author><keyname>Vil&#xe0;</keyname><forenames>Mariona</forenames></author><author><keyname>Pereira</keyname><forenames>Jordi</forenames></author></authors><title>A note on 'Exact and approximate methods for a one-dimensional minimax
  bin-packing problem' [Annals of Operations Research (2013) 206:611-626]</title><categories>cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent paper, Brusco, K\&quot;ohn and Steinley [Ann. Oper. Res. 206:611-626
(2013)] conjecture that the 2 bins special case of the one-dimensional minimax
bin-packing problem with bin size constraints might be solvable in polynomial
time. In this note, we show that this problem is NP-hard for the special case
and that it is strongly NP-hard for the general problem. We also propose a
pseudo-polynomial algorithm for the special case and a constructive heuristic
for the general problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1617</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1617</id><created>2014-02-07</created><updated>2015-03-29</updated><authors><author><keyname>Yemini</keyname><forenames>Michal</forenames></author><author><keyname>Somekh-Baruch</keyname><forenames>Anelia</forenames></author><author><keyname>Leshem</keyname><forenames>Amir</forenames></author></authors><title>Asynchronous Transmission over Single-User State-Dependent Channels</title><categories>cs.IT math.IT</categories><comments>The paper &quot;On channels with asynchronous side information&quot; was split
  into two separate papers: the enclosed paper which considers only
  point-to-point channels and an additional paper named &quot;On the multiple access
  channel with asynchronous cognition&quot; which discusses the multiuser setups</comments><doi>10.1109/TIT.2015.2476477</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several channels with asynchronous side information are introduced. We first
consider single-user state-dependent channels with asynchronous side
information at the transmitter. It is assumed that the state information
sequence is a possibly delayed version of the state sequence, and that the
encoder and the decoder are aware of the fact that the state information might
be delayed. It is additionally assumed that an upper bound on the delay is
known to both encoder and decoder, but other than that, they are ignorant of
the actual delay. We consider both the causal and the noncausal cases and
present achievable rates for these channels, and the corresponding coding
schemes. We find the capacity of the asynchronous Gel'fand-Pinsker channel with
feedback. Finally, we consider a memoryless state dependent channel with
asynchronous side information at both the transmitter and receiver, and
establish a single-letter expression for its capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1624</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1624</id><created>2014-02-07</created><authors><author><keyname>Janetzko</keyname><forenames>Dietmar</forenames></author></authors><title>Using Twitter to Model the EUR/USD Exchange Rate</title><categories>q-fin.ST cs.CY</categories><comments>35 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast, global, and sensitively reacting to political, economic and social
events of any kind, these are attributes that social media like Twitter share
with foreign exchange markets. The leading assumption of this paper is that
information which can be distilled from public debates on Twitter has
predictive content for exchange rate movements. This assumption prompted a
Twitter-based exchange rate model that harnesses regARIMA analyses for
short-term out-of-sample ex post forecasts of the daily closing prices of
EUR/USD spot exchange rates. The analyses used Tweet counts collected from
January 1, 2012 - September 27, 2013. To identify concepts mentioned on Twitter
with a predictive potential the analysis followed a 2-step selection. Firstly,
a heuristic qualitative analysis assembled a long list of 594 concepts, e.g.,
Merkel, Greece, Cyprus, crisis, chaos, growth, unemployment expected to covary
with the ups and downs of the EUR/USD exchange rate. Secondly, cross-validation
using window averaging with a fixed-sized rolling origin was deployed to select
concepts and corresponding univariate time series that had error scores below
chance level as defined by the random walk model. With regard to a short list
of 17 concepts (covariates), in particular SP (Standard &amp; Poor's) and risk, the
out-of-sample predictive accuracy of the Twitter-based regARIMA model was found
to be repeatedly better than that obtained from both the random walk model and
a random noise covariate in 1-step ahead forecasts of the EUR/USD exchange
rate. This advantage was evident on the level of forecast error metrics (MSFE,
MAE) when a majority vote over different estimation windows was conducted. The
results challenge the semi-strong form of the efficient market hypothesis
(Fama, 1970, 1991) which when applied to the FX market maintains that all
publicly available information is already integrated into exchange rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1635</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1635</id><created>2014-02-07</created><authors><author><keyname>Samarathunga</keyname><forenames>Wasantha</forenames></author><author><keyname>Seki</keyname><forenames>Masatoshi</forenames></author><author><keyname>Saito</keyname><forenames>Hidenobu</forenames></author><author><keyname>Ichiryu</keyname><forenames>Ken</forenames></author><author><keyname>Ohyama</keyname><forenames>Yasuhiro</forenames></author></authors><title>Product Evaluation In Elliptical Helical Pipe Bending</title><categories>cs.CE</categories><journal-ref>International Journal of Computer Trends and Technology, volume 4
  Issue 10 Oct 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research proposes a computation approach to address the evaluation of
end product machining accuracy in elliptical surfaced helical pipe bending
using 6dof parallel manipulator as a pipe bender. The target end product is
wearable metal muscle supporters used in build-to-order welfare product
manufacturing. This paper proposes a product testing model that mainly corrects
the surface direction estimation errors of existing least squares ellipse
fittings, followed by arc length and central angle evaluations. This
post-machining modelling requires combination of reverse rotations and
translations to a specific location before accuracy evaluation takes place,
i.e. the reverse comparing to pre-machining product modelling. This specific
location not only allows us to compute surface direction but also the amount of
excessive surface twisting as a rotation angle about a specified axis, i.e.
quantification of surface torsion. At first we experimented three ellipse
fitting methods such as, two least-squares fitting methods with Bookstein
constraint and Trace constraint, and one non- linear least squares method using
Gauss-Newton algorithm. From fitting results, we found that using Trace
constraint is more reliable and designed a correction filter for surface
torsion observation. Finally we apply 2D total least squares line fitting
method with a rectification filter for surface direction detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1636</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1636</id><created>2014-02-07</created><authors><author><keyname>Vabishchevich</keyname><forenames>Petr N.</forenames></author></authors><title>Numerical solving the boundary value problem for fractional powers of
  elliptic operators</title><categories>cs.NA math.NA</categories><comments>18 pages, 17 figures</comments><msc-class>35R11, 65F60, 65M06, 65N22</msc-class><doi>10.1016/j.jcp.2014.11.022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A boundary value problem for a fractional power of the second-order elliptic
operator is considered. It is solved numerically using a time-dependent problem
for a pseudo-parabolic equation. For the auxiliary Cauchy problem, the standard
two-level schemes with weights are applied. Stability conditions are obtained
for the fully discrete schemes under the consideration. The numerical results
are presented for a model two-dimensional boundary value problem wit a
fractional power of an elliptic operator. The dependence of accuracy on grids
in time and in space is studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1637</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1637</id><created>2014-02-07</created><authors><author><keyname>Samarathunga</keyname><forenames>Wasantha</forenames></author><author><keyname>Seki</keyname><forenames>Masatoshi</forenames></author><author><keyname>Saito</keyname><forenames>Hidenobu</forenames></author><author><keyname>Ichiryu</keyname><forenames>Ken</forenames></author><author><keyname>Ohyama</keyname><forenames>Yasuhiro</forenames></author></authors><title>Vertical Clustering of 3D Elliptical Helical Data</title><categories>cs.CE</categories><journal-ref>International Journal of Computer Trends and Technology, volume 6
  number 2,Dec 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research proposes an effective vertical clustering strategy of 3D data
in an elliptical helical shape based on 2D geometry. The clustering object is
an elliptical cross-sectioned metal pipe which is been bended in to an
elliptical helical shape which is used in wearable muscle support designing for
welfare industry. The aim of this proposed method is to maximize the vertical
clustering (vertical partitioning) ability of surface data in order to run the
product evaluation process addressed in research [2]. The experiment results
prove that the proposed method outperforms the existing threshold no of
clusters that preserves the vertical shape than applying the conventional 3D
data. This research also proposes a new product testing strategy that provides
the flexibility in computer aided testing by not restricting the sequence
depending measurements which apply weight on measuring process. The clustering
algorithms used for the experiments in this research are self-organizing map
(SOM) and K-medoids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1652</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1652</id><created>2014-02-07</created><authors><author><keyname>Roca</keyname><forenames>Vidal</forenames></author><author><keyname>Torres</keyname><forenames>Vicente</forenames></author><author><keyname>Kretz</keyname><forenames>Tobias</forenames></author><author><keyname>Lehmann</keyname><forenames>Karsten</forenames></author><author><keyname>Hofs&#xe4;&#xdf;</keyname><forenames>Ingmar</forenames></author></authors><title>How to Apply Assignment Methods that were Developed for Vehicular
  Traffic to Pedestrian Microsimulations</title><categories>cs.CE cs.MA physics.soc-ph</categories><comments>contribution to PANAM 2014 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applying assignment methods to compute user-equilibrium route choice is very
common in traffic planning. It is common sense that vehicular traffic arranges
in a user-equilibrium based on generalized costs in which travel time is a
major factor. Surprisingly travel time has not received much attention for the
route choice of pedestrians. In microscopic simulations of pedestrians the
vastly dominating paradigm for the computation of the preferred walking
direction is set into the direction of the (spatially) shortest path. For
situations where pedestrians have travel time as primary determinant for their
walking behavior it would be desirable to also have an assignment method in
pedestrian simulations. To apply existing (road traffic) assignment methods
with simulations of pedestrians one has to reduce the nondenumerably many
possible pedestrian trajectories to a small subset of routes which represent
the main, relevant, and significantly distinguished routing alternatives. All
except one of these routes will mark detours, i.e. not the shortest connection
between origin and destination. The proposed assignment method is intended to
work with common operational models of pedestrian dynamics. These - as
mentioned before - usually send pedestrians into the direction of the spatially
shortest path. Thus, all detouring routes have to be equipped with intermediate
destinations, such that pedestrians can do a detour as a piecewise connection
of segments on which they walk into the direction of the shortest path. One has
then to take care that the transgression from one segment to the following one
no artifacts are introduced into the pedestrian trajectory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1660</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1660</id><created>2014-02-07</created><updated>2016-01-25</updated><authors><author><keyname>Igamberdiev</keyname><forenames>Alexander</forenames></author><author><keyname>Schulz</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>A Duality Transform for Constructing Small Grid Embeddings of 3d
  Polytopes</title><categories>cs.CG math.MG</categories><comments>Full version of the Graph Drawing 2013 conference version, 23 pages,
  5 figures</comments><msc-class>05C62, 52B10, 52B20, 68R10</msc-class><acm-class>I.3.5; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of how to obtain an integer realization of a 3d polytope
when an integer realization of its dual polytope is given. We focus on grid
embeddings with small coordinates and develop novel techniques based on Colin
de Verdi\`ere matrices and the Maxwell-Cremona lifting method. We show that
every truncated 3d polytope with n vertices can be realized on a grid of size
O(n^{9log(6)+1}). Moreover, for every simplicial 3d polytope with n vertices
with maximal vertex degree {\Delta} and vertices placed on an L x L x L grid, a
dual polytope can be realized on an integer grid of size O(n L^{3\Delta + 9}).
This implies that for a class C of simplicial 3d polytopes with bounded vertex
degree and polynomial size grid embedding, the dual polytopes of C can be
realized on a polynomial size grid as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1661</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1661</id><created>2014-02-07</created><authors><author><keyname>Kudelka</keyname><forenames>Milos</forenames></author><author><keyname>Zehnalova</keyname><forenames>Sarka</forenames></author><author><keyname>Platos</keyname><forenames>Jan</forenames></author></authors><title>Network Sampling Based on NN Representatives</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The amount of large-scale real data around us increase in size very quickly
and so does the necessity to reduce its size by obtaining a representative
sample. Such sample allows us to use a great variety of analytical methods,
whose direct application on original data would be infeasible. There are many
methods used for different purposes and with different results. In this paper
we outline a simple and straightforward approach based on analyzing the nearest
neighbors (NN) that is generally applicable. This feature is illustrated on
experiments with weighted networks and vector data. The properties of the
representative sample show that the presented approach maintains very well
internal data structures (e.g. clusters and density). Key technical parameters
of the approach is low complexity and high scalability. This allows the
application of this approach to the area of big data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1668</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1668</id><created>2014-02-07</created><authors><author><keyname>Osborne</keyname><forenames>John David</forenames></author><author><keyname>Gyawali</keyname><forenames>Binod</forenames></author><author><keyname>Solorio</keyname><forenames>Thamar</forenames></author></authors><title>Evaluation of YTEX and MetaMap for clinical concept recognition</title><categories>cs.IR cs.CL</categories><comments>6 pages, working notes to the ShareClef eHealth 2013 Shared Task</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We used MetaMap and YTEX as a basis for the construc- tion of two separate
systems to participate in the 2013 ShARe/CLEF eHealth Task 1[9], the
recognition of clinical concepts. No modifications were directly made to these
systems, but output concepts were filtered using stop concepts, stop concept
text and UMLS semantic type. Con- cept boundaries were also adjusted using a
small collection of rules to increase precision on the strict task. Overall
MetaMap had better per- formance than YTEX on the strict task, primarily due to
a 20% perfor- mance improvement in precision. In the relaxed task YTEX had
better performance in both precision and recall giving it an overall F-Score
4.6% higher than MetaMap on the test data. Our results also indicated a 1.3%
higher accuracy for YTEX in UMLS CUI mapping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1670</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1670</id><created>2014-02-07</created><authors><author><keyname>Busseniers</keyname><forenames>Evo</forenames></author></authors><title>Hierarchical organization versus self-organization</title><categories>cs.MA cs.SI</categories><comments>15 pages +figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we try to define the difference between hierarchical
organization and self-organization. Organization is defined as a structure with
a function. So we can define the difference between hierarchical organization
and self-organization both on the structure as on the function. In the next two
chapters these two definitions are given. For the structure we will use some
existing definitions in graph theory, for the function we will use existing
theory on (self-)organization. In the third chapter we will look how these two
definitions agree. Finally we give a conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1673</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1673</id><created>2014-02-07</created><updated>2015-02-16</updated><authors><author><keyname>Tichavsky</keyname><forenames>Petr</forenames></author><author><keyname>Phan</keyname><forenames>Anh Huy</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Non-Orthogonal Tensor Diagonalization, a Tool for Block Tensor
  Decompositions</title><categories>cs.NA stat.OT</categories><comments>The manuscript was revised according to reviewers'comments,
  shortened, and a new analysis is included</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents algorithms for non-orthogonal tensor diagonalization,
which can be used for block tensor decomposition. The diagonalization can be
performed along two or more tensor dimensions simultaneously. The method seeks
for one diagonalizing matrix of determinant 1 for each mode that together
convert a given tensor into a tensor that meets a block revealing condition.
Perturbation analysis of the algorithm showing how small changes in the tensor
translate in small changes of the diagonalization outcome is provided.
  The algorithm has a low computational complexity, comparable to complexity of
the fastest available canonical polyadic decomposition algorithms. For example,
the diagonalization of order-3 tensor of the size NxNxN has the complexity of
the order O(N^4) per iteration. If the tensor has a different shape, a Tucker
compression should be applied prior to the diagonalization.
  The algorithm can be applied in cumulant-based independent subspace
decomposition or for tensor deconvolution and feature extraction using the
convolutive model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1674</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1674</id><created>2014-02-06</created><authors><author><keyname>Feng</keyname><forenames>Xiaojun</forenames></author><author><keyname>Zhang</keyname><forenames>Qian</forenames></author><author><keyname>Zhang</keyname><forenames>Jin</forenames></author></authors><title>A Hybrid Pricing Framework for TV White Space Database</title><categories>cs.NI</categories><comments>12 pages, 9 figures, accepted by IEEE Transactions on Wireless
  Communications in Jan. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to the recent rulings of the Federal Communications Commission
(FCC), TV white spaces (TVWS) can now be accessed by secondary users (SUs)
after a list of vacant TV channels is obtained via a geo-location database.
Proper business models are therefore essential for database operators to manage
geo-location databases. Database access can be simultaneously priced under two
different schemes: the registration scheme and the service plan scheme. In the
registration scheme, the database reserves part of the TV bandwidth for
registered White Space Devices (WSDs). In the service plan scheme, the WSDs are
charged according to their queries. In this paper, we investigate the business
model for the TVWS database under a hybrid pricing scheme. We consider the
scenario where a database operator employs both the registration scheme and the
service plan scheme to serve the SUs. The SUs' choices of different pricing
schemes are modeled as a non-cooperative game and we derive distributed
algorithms to achieve Nash Equilibrium (NE). Considering the NE of the SUs, the
database operator optimally determines pricing parameters for both pricing
schemes in terms of bandwidth reservation, registration fee and query plans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1682</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1682</id><created>2014-02-07</created><authors><author><keyname>Khabbazibasmenj</keyname><forenames>Arash</forenames></author><author><keyname>Hassanien</keyname><forenames>Aboulnasr</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author></authors><title>How Many Beamforming Vectors Generate the Same Beampattern?</title><categories>cs.IT math.IT</categories><comments>12 pages, 3 figures, 2 tables, Submitted to the IEEE Signal
  Processing Letters in February 2014</comments><journal-ref>IEEE Signal Processing Letters, vol. 22, no. 10, pp. 1609-1613,
  Oct. 2015</journal-ref><doi>10.1109/LSP.2015.2417220</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we address the fundamental question of how many beamforming
vectors exist which generate the same beampattern? The question is relevant to
many fields such as, for example, array processing, radar, wireless
communications, data compression, dimensionality reduction, and biomedical
engineering. The desired property of having the same beampattern for different
columns of a beamspace transformation matrix (beamforming vectors) often plays
a key importance in practical applications. The result is that at most
2^{M-1}-1 beamforming vectors with the same beampattern can be generated from
any given beamforming vector. Here M is the dimension of the beamforming
vector. At the constructive side, the answer to this question allows for
computationally efficient techniques for the beamspace transformation design.
Indeed, one can start with a single beamforming vector, which gives a desired
beampattern, and generate a number of other beamforming vectors, which give
absolutely the same beampattern, in a computationally efficient way. We call
the initial beamforming vector as the mother beamforming vector. One possible
procedure for generating all possible new beamforming vectors with the same
beampattern from the mother beamforming vector is proposed. The application of
the proposed analysis to the transmit beamspace design in multiple-input
multiple-output radar is also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1683</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1683</id><created>2014-02-07</created><authors><author><keyname>Niznik</keyname><forenames>Dr. Carol A.</forenames></author></authors><title>Homeland Defense and Security Universal Interface Software (HDUIS)
  Protocol Communication Gateway UIS Protocol Enhancements, Alterations and
  Attachments</title><categories>cs.CY cs.CR</categories><journal-ref>The International Journal of Soft Computing and Software
  Engineering [JSCSE], Vol. 3, No. 3, pp.517-523, 2013</journal-ref><doi>10.7321/jscse.v3.n3.79</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The Universal Interface Software(UIS) Protocol was a Theater Missile Defense
Gateway Protocol which linked the Strategic Defense Initiative(SDI)
Architecture Killer Satellite Software Protocol to the National Test Bed
Simulation Software Protocol to enable neural network shock loop operation when
ICBMS were approaching the SDI Shield. A Gateway Software is required for
Homeland Defense and Security Systems to communicate the sensor information
from hardware and software boxes at airports and government buildings and other
locations to the Global Information Grid(GIG). Therefore, a Homeland Defense
and Security UIS(HDSUIS) Protocol is achieved by UIS conversion to HDSUIS for
Thresholds Stabilization and GIG and terrorist sensor Enhancements, Homeland
Defense and Security Lagrangian equation and GIG simulation facility timing
chart Alterations, and two Catastrophe Theory Protocol Attachments to the UIS
Geometric software structure inner cube. This UIS Protocol conversion to the
HDSUIS Protocol will track and provide a Congestion Controlled, i.e.,prevention
of deadlock and livelock, communication of (1) Shoe bombers and copycat shoe
bombers, (2) deeply buried and imbedded boxes with explosives, (3) damage to
lase1 equipment, (4) shoulder missile fired armament, and (5) surface to air
missiles from their sensor equipment to the Global Information Grid with
Theater Missile Defense Characteristics. The Homeland Defense and Security
GNNO(Geometric Neural Network Overlay) Protocol will be derived as a conversion
of the UIS GNNO Protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1690</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1690</id><created>2014-02-07</created><authors><author><keyname>Ramos</keyname><forenames>A.</forenames></author><author><keyname>Ramos</keyname><forenames>F.</forenames></author></authors><title>Heliostat blocking and shadowing efficiency in the video-game era</title><categories>math.OC cs.GR</categories><comments>LaTeX. 8 pages, 8 figures</comments><report-no>DESY 14-009</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blocking and shadowing is one of the key effects in designing and evaluating
a thermal central receiver solar tower plant. Therefore it is convenient to
develop efficient algorithms to compute the area of an heliostat blocked or
shadowed by the rest of the field. In this paper we explore the possibility of
using very efficient clipping algorithms developed for the video game and
imaging industry to compute the blocking and shadowing efficiency of a solar
thermal plant layout. We propose an algorithm valid for arbitrary position,
orientation and size of the heliostats. This algorithm turns out to be very
accurate, free of assumptions and fast. We show the feasibility of the use of
this algorithm to the optimization of a solar plant by studying a couple of
examples in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1693</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1693</id><created>2014-02-07</created><authors><author><keyname>Chavhan</keyname><forenames>Rahul D.</forenames></author><author><keyname>Chavhan</keyname><forenames>Sachin U.</forenames></author><author><keyname>Chavan</keyname><forenames>Ganesh B.</forenames></author></authors><title>Real Time Industrial Monitoring System</title><categories>cs.OH</categories><comments>5 pages 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Industries are the biggest workplace all over the world, also there are large
number of peoples involves as a worker and most of them are work as a machine
operator. There are many systems developed for industrial work place, some of
them, monitors machine processes and some do monitoring and control of machine
parameters. Such as speed, temperature, production batch count etc. However
there is no such system available that provides monitoring of operator during
their work is in progress at workplace. This paper proposes the monitoring of
the operators and the machines, by Real time Operator -Machine Allocation and
monitoring system (Omams). Omams allocates a work machine to worker at entry
point itself. It uses automation with RFID and one of the standards of wireless
communication method. The system can be industry specific. Through this
research paper our approach is to make fair allocation of machine to the
operator in industry and reduce hassle for efficiency calculations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1697</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1697</id><created>2014-02-07</created><authors><author><keyname>Halder</keyname><forenames>Abhishek</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Raktim</forenames></author></authors><title>Geodesic Density Tracking with Applications to Data Driven Modeling</title><categories>cs.SY math.OC</categories><comments>8 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many problems in dynamic data driven modeling deals with distributed rather
than lumped observations. In this paper, we show that the Monge-Kantorovich
optimal transport theory provides a unifying framework to tackle such problems
in the systems-control parlance. Specifically, given distributional
measurements at arbitrary instances of measurement availability, we show how to
derive dynamical systems that interpolate the observed distributions along the
geodesics. We demonstrate the framework in the context of three specific
problems: (i) \emph{finding a feedback control} to track observed ensembles
over finite-horizon, (ii) \emph{finding a model} whose prediction matches the
observed distributional data, and (iii) \emph{refining a baseline model} that
results a distribution-level prediction-observation mismatch. We emphasize how
the three problems can be posed as variants of the optimal transport problem,
but lead to different types of numerical methods depending on the problem
context. Several examples are given to elucidate the ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1699</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1699</id><created>2014-02-07</created><updated>2015-02-04</updated><authors><author><keyname>Jaskelioff</keyname><forenames>Mauro</forenames></author><author><keyname>O'Connor</keyname><forenames>Russell</forenames></author></authors><title>A Representation Theorem for Second-Order Functionals</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representation theorems relate seemingly complex objects to concrete, more
tractable ones.
  In this paper, we take advantage of the abstraction power of category theory
and provide a general representation theorem for a wide class of second-order
functionals which are polymorphic over a class of functors. Types polymorphic
over a class of functors are easily representable in languages such as Haskell,
but are difficult to analyse and reason about. The concrete representation
provided by the theorem is easier to analyse, but it might not be as convenient
to implement. Therefore, depending on the task at hand, the change of
representation may prove valuable in one direction or the other.
  We showcase the usefulness of the representation theorem with a range of
examples. Concretely, we show how the representation theorem can be used to
show that traversable functors are finitary containers, how parameterised
coalgebras relate to very well-behaved lenses, and how algebraic effects might
be implemented in a functional language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1713</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1713</id><created>2014-02-07</created><authors><author><keyname>Ma</keyname><forenames>Liang</forenames><affiliation>IRCCyN, DIE</affiliation></author><author><keyname>Zhang</keyname><forenames>Wei</forenames><affiliation>DIE</affiliation></author><author><keyname>Hu</keyname><forenames>Bo</forenames><affiliation>DIE</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Bennis</keyname><forenames>Fouad</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Guillaume</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Determination of subject-specific muscle fatigue rates under static
  fatiguing operations</title><categories>cs.RO</categories><proxy>ccsd</proxy><journal-ref>Ergonomics 56, 12 (2013) 1889-1900</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cumulative local muscle fatigue may lead to potential musculoskeletal
disorder (MSD) risks {\color{red}, and subject-specific muscle fatigability
needs to be considered to reduce potential MSD risks.} This study was conducted
to determine local muscle fatigue rate at shoulder joint level based on an
exponential function derived from a muscle fatigue model. Forty male subjects
participated in a fatiguing operation under a static posture with a range of
relative force levels (14% - 33%). Remaining maximum muscle strengths were
measured after different fatiguing sessions. The time course of strength
decline was fitted to the exponential function. Subject-specific fatigue rates
of shoulder joint moment strength were determined. Good correspondence
($R^2&gt;0.8$) was found in the regression of the majority (35 out of 40
subjects). Substantial inter-individual variability in fatigue rate was found
and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1718</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1718</id><created>2014-01-28</created><updated>2014-12-02</updated><authors><author><keyname>Courtois</keyname><forenames>Nicolas T.</forenames></author><author><keyname>Bahack</keyname><forenames>Lear</forenames></author></authors><title>On Subversive Miner Strategies and Block Withholding Attack in Bitcoin
  Digital Currency</title><categories>cs.CR cs.CE cs.SI</categories><comments>not published elsewhere</comments><acm-class>D.4.6; K.4.1; K.4.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bitcoin is a &quot;crypto currency&quot;, a decentralized electronic payment scheme
based on cryptography. Bitcoin economy grows at an incredibly fast rate and is
now worth some 10 billions of dollars. Bitcoin mining is an activity which
consists of creating (minting) the new coins which are later put into
circulation. Miners spend electricity on solving cryptographic puzzles and they
are also gatekeepers which validate bitcoin transactions of other people.
Miners are expected to be honest and have some incentives to behave well.
However. In this paper we look at the miner strategies with particular
attention paid to subversive and dishonest strategies or those which could put
bitcoin and its reputation in danger. We study in details several recent
attacks in which dishonest miners obtain a higher reward than their relative
contribution to the network. In particular we revisit the concept of block
withholding attacks and propose a new concrete and practical block withholding
attack which we show to maximize the advantage gained by rogue miners.
  RECENT EVENTS: it seems that the attack was recently executed, see Section
XI-A.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1720</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1720</id><created>2014-02-07</created><authors><author><keyname>Schultze</keyname><forenames>Blake</forenames></author><author><keyname>Witt</keyname><forenames>Micah</forenames></author><author><keyname>Censor</keyname><forenames>Yair</forenames></author><author><keyname>Schulte</keyname><forenames>Reinhard</forenames></author><author><keyname>Schubert</keyname><forenames>Keith Evan</forenames></author></authors><title>Performance of Hull-Detection Algorithms For Proton Computed Tomography
  Reconstruction</title><categories>cs.CV physics.med-ph</categories><comments>Contemporary Mathematics, accepted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proton computed tomography (pCT) is a novel imaging modality developed for
patients receiving proton radiation therapy. The purpose of this work was to
investigate hull-detection algorithms used for preconditioning of the large and
sparse linear system of equations that needs to be solved for pCT image
reconstruction. The hull-detection algorithms investigated here included
silhouette/space carving (SC), modified silhouette/space carving (MSC), and
space modeling (SM). Each was compared to the cone-beam version of filtered
backprojection (FBP) used for hull-detection. Data for testing these algorithms
included simulated data sets of a digital head phantom and an experimental data
set of a pediatric head phantom obtained with a pCT scanner prototype at Loma
Linda University Medical Center. SC was the fastest algorithm, exceeding the
speed of FBP by more than 100 times. FBP was most sensitive to the presence of
noise. Ongoing work will focus on optimizing threshold parameters in order to
define a fast and efficient method for hull-detection in pCT image
reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1726</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1726</id><created>2014-02-07</created><authors><author><keyname>Gilbert</keyname><forenames>Anna C.</forenames></author><author><keyname>Li</keyname><forenames>Yi</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author><author><keyname>Strauss</keyname><forenames>Martin J.</forenames></author></authors><title>For-all Sparse Recovery in Near-Optimal Time</title><categories>cs.DS cs.IT math.IT</categories><acm-class>F.2.2; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An approximate sparse recovery system in $\ell_1$ norm consists of parameters
$k$, $\epsilon$, $N$, an $m$-by-$N$ measurement $\Phi$, and a recovery
algorithm, $\mathcal{R}$. Given a vector, $\mathbf{x}$, the system approximates
$x$ by $\widehat{\mathbf{x}} = \mathcal{R}(\Phi\mathbf{x})$, which must satisfy
$\|\widehat{\mathbf{x}}-\mathbf{x}\|_1 \leq
(1+\epsilon)\|\mathbf{x}-\mathbf{x}_k\|_1$. We consider the 'for all' model, in
which a single matrix $\Phi$, possibly 'constructed' non-explicitly using the
probabilistic method, is used for all signals $\mathbf{x}$. The best existing
sublinear algorithm by Porat and Strauss (SODA'12) uses $O(\epsilon^{-3}
k\log(N/k))$ measurements and runs in time $O(k^{1-\alpha}N^\alpha)$ for any
constant $\alpha &gt; 0$.
  In this paper, we improve the number of measurements to $O(\epsilon^{-2} k
\log(N/k))$, matching the best existing upper bound (attained by super-linear
algorithms), and the runtime to $O(k^{1+\beta}\textrm{poly}(\log
N,1/\epsilon))$, with a modest restriction that $\epsilon \leq (\log k/\log
N)^{\gamma}$, for any constants $\beta,\gamma &gt; 0$. When $k\leq \log^c N$ for
some $c&gt;0$, the runtime is reduced to $O(k\textrm{poly}(N,1/\epsilon))$. With
no restrictions on $\epsilon$, we have an approximation recovery system with $m
= O(k/\epsilon \log(N/k)((\log N/\log k)^\gamma + 1/\epsilon))$ measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1732</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1732</id><created>2014-02-07</created><updated>2015-01-03</updated><authors><author><keyname>Franck</keyname><forenames>Christian</forenames></author></authors><title>Dining Cryptographers with 0.924 Verifiable Collision Resolution</title><categories>cs.CR</categories><comments>11 pages, 3 figures</comments><journal-ref>Annales UMCS, Informatica. Volume 14, Issue 1, Pages 49-59, ISSN
  (Online) 2083-3628, October 2014</journal-ref><doi>10.2478/umcsinfo-2014-0007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dining cryptographers protocol implements a multiple access channel in
which senders and recipients are anonymous. A problem is that a malicious
participant can disrupt communication by deliberately creating collisions. We
propose a computationally secure dining cryptographers protocol with collision
resolution that achieves a maximum stable throughput of 0.924 messages per
round and which allows to easily detect disruptors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1736</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1736</id><created>2014-01-29</created><authors><author><keyname>Cameron</keyname><forenames>Maria</forenames></author><author><keyname>Vanden-Eijnden</keyname><forenames>Eric</forenames></author></authors><title>Flows in Complex Networks: Theory, Algorithms, and Application to
  Lennard-Jones Cluster Rearrangement</title><categories>cond-mat.stat-mech cs.CE</categories><comments>32 pages, 13 figures</comments><doi>10.1007/s10955-014-0997-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A set of analytical and computational tools based on transition path theory
(TPT) is proposed to analyze flows in complex networks. Specifically, TPT is
used to study the statistical properties of the reactive trajectories by which
transitions occur between specific groups of nodes on the network. Sampling
tools are built upon the outputs of TPT that allow to generate these reactive
trajectories directly, or even transition paths that travel from one group of
nodes to the other without making any detour and carry the same probability
current as the reactive trajectories. These objects permit to characterize the
mechanism of the transitions, for example by quantifying the width of the tubes
by which these transitions occur, the location and distribution of their
dynamical bottlenecks, etc. These tools are applied to a network modeling the
dynamics of the Lennard-Jones cluster with 38 atoms (LJ38) and used to
understand the mechanism by which this cluster rearranges itself between its
two most likely states at various temperatures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1754</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1754</id><created>2014-02-07</created><updated>2015-01-26</updated><authors><author><keyname>Szabo</keyname><forenames>Zoltan</forenames></author><author><keyname>Gretton</keyname><forenames>Arthur</forenames></author><author><keyname>Poczos</keyname><forenames>Barnabas</forenames></author><author><keyname>Sriperumbudur</keyname><forenames>Bharath</forenames></author></authors><title>Two-stage Sampled Learning Theory on Distributions</title><categories>math.ST cs.LG math.FA stat.ML stat.TH</categories><comments>v6: accepted at AISTATS-2015 for oral presentation; final version;
  code: https://bitbucket.org/szzoli/ite/; extension to the misspecified and
  vector-valued case: http://arxiv.org/abs/1411.2066</comments><msc-class>62G08, 46E22, 47B32</msc-class><acm-class>G.3; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the distribution regression problem: regressing to a real-valued
response from a probability distribution. Although there exist a large number
of similarity measures between distributions, very little is known about their
generalization performance in specific learning tasks. Learning problems
formulated on distributions have an inherent two-stage sampled difficulty: in
practice only samples from sampled distributions are observable, and one has to
build an estimate on similarities computed between sets of points. To the best
of our knowledge, the only existing method with consistency guarantees for
distribution regression requires kernel density estimation as an intermediate
step (which suffers from slow convergence issues in high dimensions), and the
domain of the distributions to be compact Euclidean. In this paper, we provide
theoretical guarantees for a remarkably simple algorithmic alternative to solve
the distribution regression problem: embed the distributions to a reproducing
kernel Hilbert space, and learn a ridge regressor from the embeddings to the
outputs. Our main contribution is to prove the consistency of this technique in
the two-stage sampled setting under mild conditions (on separable, topological
domains endowed with kernels). For a given total number of observations, we
derive convergence rates as an explicit function of the problem difficulty. As
a special case, we answer a 15-year-old open question: we establish the
consistency of the classical set kernel [Haussler, 1999; Gartner et. al, 2002]
in regression, and cover more recent kernels on distributions, including those
due to [Christmann and Steinwart, 2010].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1757</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1757</id><created>2014-02-07</created><authors><author><keyname>Mao</keyname><forenames>Tao</forenames></author><author><keyname>Ray</keyname><forenames>Laura</forenames></author></authors><title>Frequency-Based Patrolling with Heterogeneous Agents and Limited
  Communication</title><categories>cs.MA cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates multi-agent frequencybased patrolling of
intersecting, circle graphs under conditions where graph nodes have non-uniform
visitation requirements and agents have limited ability to communicate. The
task is modeled as a partially observable Markov decision process, and a
reinforcement learning solution is developed. Each agent generates its own
policy from Markov chains, and policies are exchanged only when agents occupy
the same or adjacent nodes. This constraint on policy exchange models sparse
communication conditions over large, unstructured environments. Empirical
results provide perspectives on convergence properties, agent cooperation, and
generalization of learned patrolling policies to new instances of the task. The
emergent behavior indicates learned coordination strategies between
heterogeneous agents for patrolling large, unstructured regions as well as the
ability to generalize to dynamic variation in node visitation requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1759</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1759</id><created>2014-02-07</created><authors><author><keyname>Jolania</keyname><forenames>Smita</forenames></author><author><keyname>Toshniwal</keyname><forenames>Sandeep</forenames></author></authors><title>Performance Improvement of OFDM System Using Iterative Signal Clipping
  With Various Window Techniques for PAPR Reduction</title><categories>cs.IT math.IT</categories><comments>5 pages,7 figures,Published with &quot;International Journal of
  Engineering Trends and Technology (IJETT)&quot;</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  OFDM signals demonstrates high fluctuations termed as Peak to Average Power
Ratio (PAPR).The problem of OFDM is the frequent occurrence of high Peaks in
the time domain signal which in turn reduces the efficiency of transmit high
power amplifier.In this paper we discussed clipping and filtering technique
which is easy to implement and reduces the amount of PAPR by clipping the peak
of the maximum power signal.This technique clips the OFDM signal to a
predefined threshold and uses a filter to eliminate the out-of-band
radiation.Moreover, analysis of PAPR is given by varying different filters.The
study is focused to reduce PAPR by iterative clipping and filtering method. The
symbol error rate performances for different modulation techniques have been
countered.Each clipping noise sample is multiplied by a window
function(e.g.Hanning,Kaiser, or Hamming) to suppress the out-of-band noise.It
is shown that clipping and different filtering techniques for improvement in
the SER performance and provides further reduction in PAPR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1761</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1761</id><created>2014-02-07</created><authors><author><keyname>Zeger</keyname><forenames>Linda</forenames></author><author><keyname>M&#xe9;dard</keyname><forenames>Muriel</forenames></author></authors><title>On Scalability of Wireless Networks: A Practical Primer for Large Scale
  Cooperation</title><categories>cs.IT cs.NI math.IT</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An intuitive overview of the scalability of a variety of types of wireless
networks is presented. Simple heuris- tic arguments are demonstrated here for
scaling laws presented in other works, as well as for conditions not previously
considered in the literature. Unicast and multicast messages, topology,
hierarchy, and e?ects of reliability protocols are discussed. We show how two
key factors, bottlenecks and erasures, can often domi- nate the network scaling
behavior. Scaling of through- put or delay with the number of transmitting
nodes, the number of receiving nodes, and the ?le size is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1774</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1774</id><created>2014-02-07</created><updated>2014-09-29</updated><authors><author><keyname>Makhdoumi</keyname><forenames>Ali</forenames></author><author><keyname>Salamatian</keyname><forenames>Salman</forenames></author><author><keyname>Fawaz</keyname><forenames>Nadia</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>From the Information Bottleneck to the Privacy Funnel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the privacy-utility trade-off encountered by users who wish to
disclose some information to an analyst, that is correlated with their private
data, in the hope of receiving some utility. We rely on a general privacy
statistical inference framework, under which data is transformed before it is
disclosed, according to a probabilistic privacy mapping. We show that when the
log-loss is introduced in this framework in both the privacy metric and the
distortion metric, the privacy leakage and the utility constraint can be
reduced to the mutual information between private data and disclosed data, and
between non-private data and disclosed data respectively. We justify the
relevance and generality of the privacy metric under the log-loss by proving
that the inference threat under any bounded cost function can be upper-bounded
by an explicit function of the mutual information between private data and
disclosed data. We then show that the privacy-utility tradeoff under the
log-loss can be cast as the non-convex Privacy Funnel optimization, and we
leverage its connection to the Information Bottleneck, to provide a greedy
algorithm that is locally optimal. We evaluate its performance on the US census
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1777</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1777</id><created>2014-02-07</created><updated>2014-10-17</updated><authors><author><keyname>Figueiredo</keyname><forenames>Flavio</forenames></author><author><keyname>Almeida</keyname><forenames>Jussara M.</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Marcos Andr&#xe9;</forenames></author><author><keyname>Benevenuto</keyname><forenames>Fabr&#xed;cio</forenames></author></authors><title>On the Dynamics of Social Media Popularity: A YouTube Case Study</title><categories>cs.SI physics.soc-ph</categories><comments>Extended version of a paper published in ACM WSDM 2011. Pre-print of
  the paper accepted for publication on the ACM Transactions on Internet
  Tecnology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the factors that impact the popularity dynamics of social media
can drive the design of effective information services, besides providing
valuable insights to content generators and online advertisers. Taking YouTube
as case study, we analyze how video popularity evolves since upload, extracting
popularity trends that characterize groups of videos. We also analyze the
referrers that lead users to videos, correlating them, features of the video
and early popularity measures with the popularity trend and total observed
popularity the video will experience. Our findings provide fundamental
knowledge about popularity dynamics and its implications for services such as
advertising and search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1778</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1778</id><created>2014-02-07</created><authors><author><keyname>Agreste</keyname><forenames>Santa</forenames></author><author><keyname>De Meo</keyname><forenames>Pasquale</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Piccolo</keyname><forenames>Sebastiano</forenames></author><author><keyname>Provetti</keyname><forenames>Alessandro</forenames></author></authors><title>Analysis of a heterogeneous social network of humans and cultural
  objects</title><categories>cs.SI cs.CY physics.data-an physics.soc-ph</categories><comments>12 pages, 9 figures - Transactions on Systems, Man and Cybernetics:
  Systems - under review</comments><journal-ref>IEEE Transactions on Systems, Man, and Cybernetics: Systems,
  vol.45, no.4, pp.559,570, April 2015</journal-ref><doi>10.1109/TSMC.2014.2378215</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern online social platforms enable their members to be involved in a broad
range of activities like getting friends, joining groups, posting/commenting
resources and so on. In this paper we investigate whether a correlation emerges
across the different activities a user can take part in. To perform our
analysis we focused on aNobii, a social platform with a world-wide user base of
book readers, who like to post their readings, give ratings, review books and
discuss them with friends and fellow readers. aNobii presents a heterogeneous
structure: i) part social network, with user-to-user interactions, ii) part
interest network, with the management of book collections, and iii) part
folksonomy, with books that are tagged by the users. We analyzed a complete and
anonymized snapshot of aNobii and we focused on three specific activities a
user can perform, namely her tagging behavior, her tendency to join groups and
her aptitude to compile a wishlist reporting the books she is planning to read.
In this way each user is associated with a tag-based, a group-based and a
wishlist-based profile. Experimental analysis carried out by means of
Information Theory tools like entropy and mutual information suggests that
tag-based and group-based profiles are in general more informative than
wishlist-based ones. Furthermore, we discover that the degree of correlation
between the three profiles associated with the same user tend to be small.
Hence, user profiling cannot be reduced to considering just any one type of
user activity (although important) but it is crucial to incorporate multiple
dimensions to effectively describe users preferences and behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1780</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1780</id><created>2014-02-07</created><authors><author><keyname>Soltan</keyname><forenames>Saleh</forenames></author><author><keyname>Mazauric</keyname><forenames>Dorian</forenames></author><author><keyname>Zussman</keyname><forenames>Gil</forenames></author></authors><title>Cascading Failures in Power Grids - Analysis and Algorithms</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on cascading line failures in the transmission system of
the power grid. Recent large-scale power outages demonstrated the limitations
of percolation- and epid- emic-based tools in modeling cascades. Hence, we
study cascades by using computational tools and a linearized power flow model.
We first obtain results regarding the Moore-Penrose pseudo-inverse of the power
grid admittance matrix. Based on these results, we study the impact of a single
line failure on the flows on other lines. We also illustrate via simulation the
impact of the distance and resistance distance on the flow increase following a
failure, and discuss the difference from the epidemic models. We then study the
cascade properties, considering metrics such as the distance between failures
and the fraction of demand (load) satisfied after the cascade (yield). We use
the pseudo-inverse of admittance matrix to develop an efficient algorithm to
identify the cascading failure evolution, which can be a building block for
cascade mitigation. Finally, we show that finding the set of lines whose
removal has the most significant impact (under various metrics) is NP-Hard and
introduce a simple heuristic for the minimum yield problem. Overall, the
results demonstrate that using the resistance distance and the pseudo-inverse
of admittance matrix provides important insights and can support the
development of efficient algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1783</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1783</id><created>2014-02-07</created><updated>2014-02-13</updated><authors><author><keyname>Xiong</keyname><forenames>Caiming</forenames></author><author><keyname>Johnson</keyname><forenames>David</forenames></author><author><keyname>Corso</keyname><forenames>Jason J.</forenames></author></authors><title>Active Clustering with Model-Based Uncertainty Reduction</title><categories>cs.LG cs.CV stat.ML</categories><comments>14 pages, 8 figures, submitted to TPAMI (second version just fixes a
  missing reference and format)</comments><msc-class>62H30</msc-class><acm-class>H.2.8; H.3.3; I.5.3; I.4.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semi-supervised clustering seeks to augment traditional clustering methods by
incorporating side information provided via human expertise in order to
increase the semantic meaningfulness of the resulting clusters. However, most
current methods are \emph{passive} in the sense that the side information is
provided beforehand and selected randomly. This may require a large number of
constraints, some of which could be redundant, unnecessary, or even detrimental
to the clustering results. Thus in order to scale such semi-supervised
algorithms to larger problems it is desirable to pursue an \emph{active}
clustering method---i.e. an algorithm that maximizes the effectiveness of the
available human labor by only requesting human input where it will have the
greatest impact. Here, we propose a novel online framework for active
semi-supervised spectral clustering that selects pairwise constraints as
clustering proceeds, based on the principle of uncertainty reduction. Using a
first-order Taylor expansion, we decompose the expected uncertainty reduction
problem into a gradient and a step-scale, computed via an application of matrix
perturbation theory and cluster-assignment entropy, respectively. The resulting
model is used to estimate the uncertainty reduction potential of each sample in
the dataset. We then present the human user with pairwise queries with respect
to only the best candidate sample. We evaluate our method using three different
image datasets (faces, leaves and dogs), a set of common UCI machine learning
datasets and a gene dataset. The results validate our decomposition formulation
and show that our method is consistently superior to existing state-of-the-art
techniques, as well as being robust to noise and to unknown numbers of
clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1786</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1786</id><created>2014-02-07</created><authors><author><keyname>Chen</keyname><forenames>Yong</forenames></author><author><keyname>Patankar</keyname><forenames>Neelesh A.</forenames></author></authors><title>Fluctuating Immersed Material (FIMAT) Dynamics for Fully Resolved
  Simulation of the Brownian Motion of Particles</title><categories>physics.flu-dyn cs.NA math.NA physics.comp-ph</categories><comments>37 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fluctuating hydrodynamics based techniques have been developed in recent
years for the simulation of Brownian motion of particles. These mesoscale
simulation tools are viable approaches for problems where molecular dynamics
simulations may be deemed expensive. We have developed a rigid constraint-based
formulation where the key idea is to assume that the entire domain is a
fluctuating fluid. Rigid motion constraints are then imposed in regions that
are occupied by rigid particles. The resulting solution gives the Brownian
motion of the particles. This approach is shown to be viable for the simulation
of long time scale diffusive behavior as well as for short time scale dynamics
by using two separate solution techniques. Test cases are reported to validate
the approach and to establish its efficacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1792</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1792</id><created>2014-02-07</created><authors><author><keyname>Mahdavi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author></authors><title>Binary Excess Risk for Smooth Convex Surrogates</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In statistical learning theory, convex surrogates of the 0-1 loss are highly
preferred because of the computational and theoretical virtues that convexity
brings in. This is of more importance if we consider smooth surrogates as
witnessed by the fact that the smoothness is further beneficial both
computationally- by attaining an {\it optimal} convergence rate for
optimization, and in a statistical sense- by providing an improved {\it
optimistic} rate for generalization bound. In this paper we investigate the
smoothness property from the viewpoint of statistical consistency and show how
it affects the binary excess risk. We show that in contrast to optimization and
generalization errors that favor the choice of smooth surrogate loss, the
smoothness of loss function may degrade the binary excess risk. Motivated by
this negative result, we provide a unified analysis that integrates
optimization error, generalization bound, and the error in translating convex
excess risk into a binary excess risk when examining the impact of smoothness
on the binary excess risk. We show that under favorable conditions appropriate
choice of smooth convex loss will result in a binary excess risk that is better
than $O(1/\sqrt{n})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1794</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1794</id><created>2014-02-07</created><authors><author><keyname>Meyer</keyname><forenames>Jesse G.</forenames></author></authors><title>In silico Proteome Cleavage Reveals Iterative Digestion Strategy for
  High Sequence Coverage</title><categories>q-bio.GN cs.CE</categories><comments>10 pages of text/references followed by figure/table legends, six
  figures, and one table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the post-genome era, biologists have sought to measure the complete
complement of proteins, termed proteomics. Currently, the most effective method
to measure the proteome is with shotgun, or bottom-up, proteomics, in which the
proteome is digested into peptides that are identified followed by protein
inference. Despite continuous improvements to all steps of the shotgun
proteomics workflow, observed proteome coverage is often low; some proteins are
identified by a single peptide sequence. Complete proteome sequence coverage
would allow comprehensive characterization of RNA splicing variants and all
post translational modifications, which would drastically improve the accuracy
of biological models. There are many reasons for the sequence coverage deficit,
but ultimately peptide length determines sequence observability. Peptides that
are too short are lost because they match many protein sequences and their true
origin is ambiguous. The maximum observable peptide length is determined by
several analytical challenges. This paper explores computationally how peptide
lengths produced from several common proteome digestion methods limit
observable proteome coverage. Iterative proteome cleavage strategies are also
explored. These simulations reveal that maximized proteome coverage can be
achieved by use of an iterative digestion protocol involving multiple proteases
and chemical cleavages that theoretically allow 91.1% proteome coverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1801</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1801</id><created>2014-02-07</created><authors><author><keyname>Hashemi</keyname><forenames>SayedMasoud</forenames></author><author><keyname>Beheshti</keyname><forenames>Soosan</forenames></author><author><keyname>Gill</keyname><forenames>Patrick R.</forenames></author><author><keyname>Paul</keyname><forenames>Narinder S.</forenames></author><author><keyname>Cobbold</keyname><forenames>Richard S. C.</forenames></author></authors><title>Efficient Low Dose X-ray CT Reconstruction through Sparsity-Based MAP
  Modeling</title><categories>stat.AP cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ultra low radiation dose in X-ray Computed Tomography (CT) is an important
clinical objective in order to minimize the risk of carcinogenesis. Compressed
Sensing (CS) enables significant reductions in radiation dose to be achieved by
producing diagnostic images from a limited number of CT projections. However,
the excessive computation time that conventional CS-based CT reconstruction
typically requires has limited clinical implementation. In this paper, we first
demonstrate that a thorough analysis of CT reconstruction through a Maximum a
Posteriori objective function results in a weighted compressive sensing
problem. This analysis enables us to formulate a low dose fan beam and helical
cone beam CT reconstruction. Subsequently, we provide an efficient solution to
the formulated CS problem based on a Fast Composite Splitting Algorithm-Latent
Expected Maximization (FCSA-LEM) algorithm. In the proposed method we use
pseudo polar Fourier transform as the measurement matrix in order to decrease
the computational complexity; and rebinning of the projections to parallel rays
in order to extend its application to fan beam and helical cone beam scans. The
weight involved in the proposed weighted CS model, denoted by Error Adaptation
Weight (EAW), is calculated based on the statistical characteristics of CT
reconstruction and is a function of Poisson measurement noise and rebinning
interpolation error. Simulation results show that low computational complexity
of the proposed method made the fast recovery of the CT images possible and
using EAW reduces the reconstruction error by one order of magnitude. Recovery
of a high quality 512$\times$ 512 image was achieved in less than 20 sec on a
desktop computer without numerical optimizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1807</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1807</id><created>2014-02-07</created><updated>2015-04-19</updated><authors><author><keyname>Jaffer</keyname><forenames>Aubrey</forenames></author></authors><title>Recurrence for Pandimensional Space-Filling Functions</title><categories>cs.CG</categories><comments>19 pages, 20 figures</comments><acm-class>I.3.5; G.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A space-filling function is a bijection from the unit line segment to the
unit square, cube, or hypercube. The function from the unit line segment is
continuous. The inverse function, while well-defined, is not continuous.
Space-filling curves, the finite approximations to space-filling functions,
have found application in global optimization, database indexing, and dimension
reduction among others. For these applications the desired transforms are
mapping a scalar to multidimensional coordinates and mapping multidimensional
coordinates to a scalar.
  Presented are recurrences which produce space-filling functions and curves of
any rank $d\ge2$ based on serpentine Hamiltonian paths on $({\bf Z}\bmod s)^d$
where $s\ge2$. The recurrences for inverse space-filling functions are also
presented. Both Peano and Hilbert curves and functions and their
generalizations to higher dimensions are produced by these recurrences. The
computations of these space-filling functions and their inverse functions are
absolutely convergent geometric series.
  The space-filling functions are constructed as limits of integer recurrences
and equivalently as non-terminating real recurrences. Scaling relations are
given which enable the space-filling functions and curves and their inverses to
extend beyond the unit area or volume and even to all of $d$-space.
  This unification of pandimensional space-filling curves facilitates
quantitative comparison of curves generated from different Hamiltonian paths.
The isotropy and performance in dimension reduction of a variety of
space-filling curves are analyzed.
  For dimension reduction it is found that Hilbert curves perform somewhat
better than Peano curves and their isotropic variants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1810</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1810</id><created>2014-02-07</created><authors><author><keyname>F&#xfc;rer</keyname><forenames>Martin</forenames></author></authors><title>A Natural Generalization of Bounded Tree-Width and Bounded Clique-Width</title><categories>cs.DS cs.CC cs.DM</categories><comments>To appear in the proceedings of Latin 2014. Springer LNCS 8392</comments><msc-class>05C85, 68R10</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a new width parameter, the fusion-width of a graph. It is a
natural generalization of the tree-width, yet strong enough that not only
graphs of bounded tree-width, but also graphs of bounded clique-width,
trivially have bounded fusion-width. In particular, there is no exponential
growth between tree-width and fusion-width, as is the case between tree-width
and clique-width. The new parameter gives a good intuition about the
relationship between tree-width and clique-width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1811</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1811</id><created>2014-02-07</created><authors><author><keyname>F&#xfc;rer</keyname><forenames>Martin</forenames></author></authors><title>How Fast Can We Multiply Large Integers on an Actual Computer?</title><categories>cs.CC cs.DS</categories><comments>To appear in the proceedings of Latin 2014. Springer LNCS 8392</comments><msc-class>03D15, 68Q25, 11Y16</msc-class><acm-class>F.1.3; F.2.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide two complexity measures that can be used to measure the running
time of algorithms to compute multiplications of long integers. The random
access machine with unit or logarithmic cost is not adequate for measuring the
complexity of a task like multiplication of long integers. The Turing machine
is more useful here, but fails to take into account the multiplication
instruction for short integers, which is available on physical computing
devices. An interesting outcome is that the proposed refined complexity
measures do not rank the well known multiplication algorithms the same way as
the Turing machine model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1813</identifier>
 <datestamp>2014-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1813</id><created>2014-02-07</created><updated>2014-08-05</updated><authors><author><keyname>Postle</keyname><forenames>Luke</forenames></author><author><keyname>Thomas</keyname><forenames>Robin</forenames></author></authors><title>Five-list-coloring graphs on surfaces I. Two lists of size two in planar
  graphs</title><categories>math.CO cs.DM</categories><comments>8 pages, minor revision based on referee report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be a plane graph with outer cycle C, let u,v be vertices of C and let
(L(x):x in V(G)) be a family of sets such that |L(u)|=|L(v)|=2, L(x) has at
least three elements for every vertex x of C-{u,v} and L(x) has at least five
elements for every vertex x of G-V(C). We prove a conjecture of Hutchinson that
G has a (proper) coloring f such that f(x) belongs to L(x) for every vertex x
of G. We will use this as a lemma in subsequent papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1814</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1814</id><created>2014-02-07</created><authors><author><keyname>Tanna</keyname><forenames>Prof. Paresh</forenames></author><author><keyname>Ghodasara</keyname><forenames>Dr. Yogesh</forenames></author></authors><title>Foundation for Frequent Pattern Mining Algorithms Implementation</title><categories>cs.DB</categories><comments>5 pages, Published with International Journal of Computer Trends and
  Technology (IJCTT)</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  4(7):2159-2163, July 2013</journal-ref><doi>10.14445/2231-2803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As with the development of the IT technologies, the amount of accumulated
data is also increasing. Thus the role of data mining comes into picture.
Association rule mining becomes one of the significant responsibilities of
descriptive technique which can be defined as discovering meaningful patterns
from large collection of data. The frequent pattern mining algorithms determine
the frequent patterns from a database. Mining frequent itemset is very
fundamental part of association rule mining. Many algorithms have been proposed
from last many decades including majors are Apriori, Direct Hashing and
Pruning, FP-Growth, ECLAT etc. The aim of this study is to analyze the existing
techniques for mining frequent patterns and evaluate the performance of them by
comparing Apriori and DHP algorithms in terms of candidate generation, database
and transaction pruning. This creates a foundation to develop newer algorithm
for frequent pattern mining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1815</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1815</id><created>2014-02-07</created><updated>2015-06-19</updated><authors><author><keyname>Hong</keyname><forenames>Song-Nam</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>On the Performance of Optimized Dense Device-to-Device Wireless Networks</title><categories>cs.IT math.IT</categories><comments>Revised and resubmitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a D2D wireless network where $n$ users are densely deployed in a
squared planar region and communicate with each other without the help of a
wired infrastructure. For this network, we examine the 3-phase hierarchical
cooperation (HC) scheme and the 2-phase improved HC scheme based on the concept
of {\em network multiple access}. Exploiting recent results on the optimality
of treating interference as noise in Gaussian interference channels, we
optimize the achievable average per-link rate and not just its scaling law. In
addition, we provide further improvements on both the previously proposed
hierarchical cooperation schemes by a more efficient use of TDMA and spatial
reuse. Thanks to our explicit achievable rate expressions, we can compare HC
scheme with multihop routing (MR), where the latter can be regarded as the
current practice of D2D wireless networks. Our results show that the improved
and optimized HC schemes yield very significant rate gains over MR in realistic
conditions of channel propagation exponents, signal to noise ratio, and number
of users. This sheds light on the long-standing question about the real
advantage of HC scheme over MR beyond the well-known scaling laws analysis. In
contrast, we also show that our rate optimization is non-trivial, since when HC
is applied with off-the-shelf choice of the system parameters, no significant
rate gain with respect to MR is achieved. We also show that for large pathloss
exponent the sum rate is a nearly linear function of the number of users $n$ in
the range of networks of practical size. This also sheds light on a
long-standing dispute on the effective achievability of linear sum rate scaling
with HC. Finally, we notice that the achievable sum rate for large $\alpha$ is
much larger than for small $\alpha$. This suggests that HC scheme may be a very
effective approach for networks operating at mm-waves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1819</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1819</id><created>2014-02-07</created><authors><author><keyname>Vigita</keyname><forenames>Sahaya Rose</forenames></author><author><keyname>Julie</keyname><forenames>Golden</forenames></author></authors><title>Reliable Link-Based Routing Protocol for Highly Dynamic Mobile Adhoc
  Networks</title><categories>cs.NI</categories><comments>7 pages, 8 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)</comments><journal-ref>International Journal of Engineering Trends and Technology
  (IJETT)-Volume 4 Issue 5/ May 2013, Pages 1636 - 1642</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional topology-based MANET routing protocols use stateful routing which
increases the processing,communication and memory overheads. The high mobility
of nodes in MANETs makes it difficult to maintain a deterministic route. To
overcome this, stateless geographic routing protocols which ensure reliable
data delivery have been proposed. It is found that link instability can be a
major factor for unreliable data delivery. Driven by this issue, Link and
Position based Opportunistic Routing (L-POR) protocol which chooses a forwarder
based on the reception power of a node has been proposed. A back-up scheme is
also proposed to handle communication holes. Simulation results show that the
proposed protocol achieves excellent performance even under high node mobility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1834</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1834</id><created>2014-02-08</created><authors><author><keyname>Frery</keyname><forenames>Alejandro C.</forenames></author><author><keyname>de Almeida</keyname><forenames>Eliana S.</forenames></author><author><keyname>Rosso</keyname><forenames>Osvaldo A.</forenames></author></authors><title>The Generalized Statistical Complexity of PolSAR Data</title><categories>cs.IT math.IT</categories><comments>Proceedings of The 4th Asia-Pacific Conference on Synthetic Aperture
  Radar (APSAR), 2013, 100-103</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents and discusses the use of a new feature for PolSAR
imagery: the Generalized Statistical Complexity. This measure is able to
capture the disorder of the data by means of the entropy, as well as its
departure from a reference distribution. The latter component is obtained by
measuring a stochastic distance between two models: the $\mathcal G^0$ and the
Gamma laws. Preliminary results on the intensity components of AIRSAR image of
San Francisco are encouraging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1841</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1841</id><created>2014-02-08</created><authors><author><keyname>Reddy</keyname><forenames>G. Nikhita</forenames></author><author><keyname>Reddy</keyname><forenames>G. J. Ugander</forenames></author></authors><title>Study of Cloud Computing in HealthCare Industry</title><categories>cs.CY cs.DC</categories><comments>4 pages</comments><journal-ref>International Journal of Science and Engineering Research - France
  - ISSN : 2229-5518 Volume 4 Issue 9 - September 2013 Pages: 68-71</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Todays real world technology has become a domiant crucial component in
every industry including healthcare industry. The benefits of storing
electronically the records of patients have increased the productivity of
patient care and easy accessibility and usage. The recent technological
innovations in the health care is the invention of cloud based Technology. But
many fears and security measures regarding patient records storing remotely is
a concern for many in health care industry. One needs to understand the
benefits and fears of implementation of cloud computing its advantages and
disadvantages of this new technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1842</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1842</id><created>2014-02-08</created><authors><author><keyname>Reddy</keyname><forenames>G. Nikhita</forenames></author><author><keyname>Reddy</keyname><forenames>G. J. Ugander</forenames></author></authors><title>A Study Of Cyber Security Challenges And Its Emerging Trends On Latest
  Technologies</title><categories>cs.CR cs.CY</categories><comments>5 pages</comments><journal-ref>International Journal of Engineering and Technology - UK ISSN:
  2049-3444, Volume 4 No.1 January 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyber Security plays an important role in the field of information technology
.Securing the information have become one of the biggest challenges in the
present day. When ever we think about the cyber security the first thing that
comes to our mind is cyber crimes which are increasing immensely day by day.
Various Governments and companies are taking many measures in order to prevent
these cyber crimes. Besides various measures cyber security is still a very big
concern to many. This paper mainly focuses on challenges faced by cyber
security on the latest technologies .It also focuses on latest about the cyber
security techniques, ethics and the trends changing the face of cyber security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1862</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1862</id><created>2014-02-08</created><authors><author><keyname>Yang</keyname><forenames>Tao</forenames></author><author><keyname>Meng</keyname><forenames>Ziyang</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Periodic Behaviors in Constrained Multi-agent Systems</title><categories>cs.SY cs.MA</categories><comments>15 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide two discrete-time multi-agent models which generate
periodic behaviors. The first one is a multi-agent system of identical double
integrators with input saturation constraints, while the other one is a
multi-agent system of identical neutrally stable system with input saturation
constraints. In each case, we show that if the feedback gain parameters of the
local controller satisfy a certain condition, the multi-agent system exhibits a
periodic solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1864</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1864</id><created>2014-02-08</created><updated>2014-06-07</updated><authors><author><keyname>Maurer</keyname><forenames>Andreas</forenames></author><author><keyname>Pontil</keyname><forenames>Massimiliano</forenames></author><author><keyname>Romera-Paredes</keyname><forenames>Bernardino</forenames></author></authors><title>An Inequality with Applications to Structured Sparsity and Multitask
  Dictionary Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From concentration inequalities for the suprema of Gaussian or Rademacher
processes an inequality is derived. It is applied to sharpen existing and to
derive novel bounds on the empirical Rademacher complexities of unit balls in
various norms appearing in the context of structured sparsity and multitask
dictionary learning or matrix factorization. A key role is played by the
largest eigenvalue of the data covariance matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1865</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1865</id><created>2014-02-08</created><authors><author><keyname>Hakuta</keyname><forenames>Keisuke</forenames></author><author><keyname>Sato</keyname><forenames>Hisayoshi</forenames></author><author><keyname>Takagi</keyname><forenames>Tsuyoshi</forenames></author></authors><title>Some properties of ${\tau}$-adic expansions on hyperelliptic Koblitz
  curves</title><categories>math.NT cs.CR</categories><comments>100 pages</comments><msc-class>11A63 (Primary), 94A60 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores two techniques on a family of hyperelliptic curves that
have been proposed to accelerate computation of scalar multiplication for
hyperelliptic curve cryptosystems. In elliptic curve cryptosystems, it is known
that Koblitz curves admit fast scalar multiplication, namely, the ${\tau}$-adic
non-adjacent form ($\tau$-NAF). It is shown that the $\tau$-NAF has the three
properties: (1) existence, (2) uniqueness, and (3) minimality of the Hamming
weight. These properties are not only of intrinsic mathematical interest, but
also desirable in some cryptographic applications. On the other hand,
G{\&quot;u}nther, Lange, and Stein have proposed two generalizations of $\tau$-NAF
for a family of hyperelliptic curves, called \emph{hyperelliptic Koblitz
curves}. However, to our knowledge, it is not known whether the three
properties are true or not. We provide an answer to the question. Our
investigation shows that the first one has only the existence and the second
one has the existence and uniqueness. Furthermore, we shall prove that there
exist 16 digit sets so that one can achieve the second one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1869</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1869</id><created>2014-02-08</created><updated>2014-06-07</updated><authors><author><keyname>Mont&#xfa;far</keyname><forenames>Guido</forenames></author><author><keyname>Pascanu</keyname><forenames>Razvan</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>On the Number of Linear Regions of Deep Neural Networks</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of functions computable by deep feedforward neural
networks with piecewise linear activations in terms of the symmetries and the
number of linear regions that they have. Deep networks are able to sequentially
map portions of each layer's input-space to the same output. In this way, deep
models compute functions that react equally to complicated patterns of
different inputs. The compositional structure of these functions enables them
to re-use pieces of computation exponentially often in terms of the network's
depth. This paper investigates the complexity of such compositional maps and
contributes new theoretical results regarding the advantage of depth for neural
networks with piecewise linear activation functions. In particular, our
analysis is not specific to a single family of models, and as an example, we
employ it for rectifier and maxout networks. We improve complexity bounds from
pre-existing work and investigate the behavior of units in higher layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1870</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1870</id><created>2014-02-08</created><authors><author><keyname>De</keyname><forenames>Nilanjan</forenames></author><author><keyname>Nayeem</keyname><forenames>Sk. Md. Abu</forenames></author><author><keyname>Pal</keyname><forenames>Anita</forenames></author></authors><title>Bounds for the modified eccentric connectivity index</title><categories>math.CO cs.DM</categories><comments>10 pages</comments><msc-class>Primary: 05C35, Secondary: 05C07, 05C40</msc-class><acm-class>J.2; G.2.m</acm-class><journal-ref>Advanced Modeling and Optimization, 16(1) (2014), pp. 133 - 142</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modified eccentric connectivity index of a graph is defined as the sum of
the products of eccentricity with the total degree of neighboring vertices,
over all vertices of the graph. This is a generalization of eccentric
connectivity index. In this paper, we derive some upper and lower bounds for
the modified eccentric connectivity index in terms of some graph parameters
such as number of vertices, number of edges, radius, minimum degree, maximum
degree, total eccentricity, the first and second Zagreb indices, Weiner index
etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1879</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1879</id><created>2014-02-08</created><authors><author><keyname>Zhuang</keyname><forenames>Liansheng</forenames></author><author><keyname>Chan</keyname><forenames>Tsung-Han</forenames></author><author><keyname>Yang</keyname><forenames>Allen Y.</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author><author><keyname>Ma</keyname><forenames>Yi</forenames></author></authors><title>Sparse Illumination Learning and Transfer for Single-Sample Face
  Recognition with Image Corruption and Misalignment</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single-sample face recognition is one of the most challenging problems in
face recognition. We propose a novel algorithm to address this problem based on
a sparse representation based classification (SRC) framework. The new algorithm
is robust to image misalignment and pixel corruption, and is able to reduce
required gallery images to one sample per class. To compensate for the missing
illumination information traditionally provided by multiple gallery images, a
sparse illumination learning and transfer (SILT) technique is introduced. The
illumination in SILT is learned by fitting illumination examples of auxiliary
face images from one or more additional subjects with a sparsely-used
illumination dictionary. By enforcing a sparse representation of the query
image in the illumination dictionary, the SILT can effectively recover and
transfer the illumination and pose information from the alignment stage to the
recognition stage. Our extensive experiments have demonstrated that the new
algorithms significantly outperform the state of the art in the single-sample
regime and with less restrictions. In particular, the single-sample face
alignment accuracy is comparable to that of the well-known Deformable SRC
algorithm using multiple gallery images per class. Furthermore, the face
recognition accuracy exceeds those of the SRC and Extended SRC algorithms using
hand labeled alignment initialization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1880</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1880</id><created>2014-02-08</created><authors><author><keyname>Ismaeel</keyname><forenames>Ayad Ghany</forenames></author><author><keyname>Mikhail</keyname><forenames>Dina Y.</forenames></author></authors><title>Design of Locally E-management System for Technical Education
  Foundation- Erbil</title><categories>cs.CY</categories><comments>11 pages, 11 figures, 1 tables</comments><journal-ref>PolyTechnic, Vol. 1, No. 1, Oct. 2011, National Journal,
  Erbil-IRAQ</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Until now, there is no e-management and automation necessary for the
operations or procedures of the departments in the Technical Education
Foundation Erbil, and the foundation like any other organization in Kurdistan
region is not connected to the network, because there is not infrastructure for
that purpose. To solve this problem, comes the proposal DLMS4TEF, which
requirements are divided into hardware and software, as hardware will need
Fast-Ethernet (LAN) technology to connect the departments of the Foundation via
Client-Server network later, when an infrastructure is established for
e-governments or e-management, it may be extended to the campus network. The
software is represented by installing windows server to implement the proposal
design of DLMS4TEF, PHP script is used as web programming that supports the
server, where as the HTML and JavaScript are used to support the client side.
The dynamic DLMS4TEF will be based on relational database, which is created by
using MySQL, to support processing hundreds of queries per second, and the
Kurdish Unicode to support Kurdish fonts of GUI's, Moreover, for security
DLMS4TEF allows each department in the Foundation to enter its own section and
prevent accessing other sections by using HTAccessible program which allows the
user to access by using his IP address and his computer only. The important
conclusions and advantages of applying DLMS4TEF are making backup to DLMS4TEF's
databases using the option (zipped) which allows them to reach the size of (3%)
of the original database size, sufficient security techniques, through
achieving levels of security, hidden access to the administrator section, and
finally DLMS4TEF, when compared with the traditional methods and project of
Oman, shows the same efficiency of some, if not better, features of Oman.
Keywords- E-management, Client-Server network, Fast-Ethernet, PHP, MySQL
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1881</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1881</id><created>2014-02-08</created><authors><author><keyname>Zhou</keyname><forenames>Shuyu</forenames></author><author><keyname>Zhang</keyname><forenames>Xiandong</forenames></author><author><keyname>Chen</keyname><forenames>Bo</forenames></author><author><keyname>van de Velde</keyname><forenames>Steef</forenames></author></authors><title>Tactical Fixed Job Scheduling with Spread-Time Constraints</title><categories>cs.DS cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the tactical fixed job scheduling problem with spread-time
constraints. In such a problem, there are a fixed number of classes of machines
and a fixed number of groups of jobs. Jobs of the same group can only be
processed by machines of a given set of classes. All jobs have their fixed
start and end times. Each machine is associated with a cost according to its
machine class. Machines have spread-time constraints, with which each machine
is only available for $L$ consecutive time units from the start time of the
earliest job assigned to it. The objective is to minimize the total cost of the
machines used to process all the jobs. For this strongly NP-hard problem, we
develop a branch-and-price algorithm, which solves instances with up to $300$
jobs, as compared with CPLEX, which cannot solve instances of $100$ jobs. We
further investigate the influence of machine flexibility by computational
experiments. Our results show that limited machine flexibility is sufficient in
most situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1882</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1882</id><created>2014-02-08</created><updated>2014-05-05</updated><authors><author><keyname>Huang</keyname><forenames>Hong</forenames></author><author><keyname>Barani</keyname><forenames>Hajar</forenames></author><author><keyname>Al-Azzawi</keyname><forenames>Hussein</forenames></author></authors><title>Network Multiple-Input and Multiple-Output for Wireless Local Area
  Networks</title><categories>cs.NI</categories><comments>This paper has been withdraw by the authors due to a crucial error in
  algorithm 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a tutorial for network multiple-input and multiple-output
(netMIMO) in wireless local area networks (WLAN). Wireless traffic demand is
growing exponentially. NetMIMO allows access points (APs) in a WLAN to
cooperate in their transmissions as if the APs form a single virtual MIMO node.
NetMIMO can significantly increase network capacity by reducing interferences
and contentions through the cooperation of the APs. This paper covers a few
representative netMIMO methods, ranging from interference alignment and
cancelation, channel access protocol to allow MIMO nodes to join ongoing
transmissions, distributed synchronization, to interference and contention
mitigation in multiple contention domains. We believe the netMIMO methods
described here are just the beginning of the new technologies to address the
challenge of ever-increasing wireless traffic demand, and the future will see
even more new developments in this field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1892</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1892</id><created>2014-02-08</created><updated>2014-05-13</updated><authors><author><keyname>Lipton</keyname><forenames>Zachary Chase</forenames></author><author><keyname>Elkan</keyname><forenames>Charles</forenames></author><author><keyname>Narayanaswamy</keyname><forenames>Balakrishnan</forenames></author></authors><title>Thresholding Classifiers to Maximize F1 Score</title><categories>stat.ML cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides new insight into maximizing F1 scores in the context of
binary classification and also in the context of multilabel classification. The
harmonic mean of precision and recall, F1 score is widely used to measure the
success of a binary classifier when one class is rare. Micro average, macro
average, and per instance average F1 scores are used in multilabel
classification. For any classifier that produces a real-valued output, we
derive the relationship between the best achievable F1 score and the
decision-making threshold that achieves this optimum. As a special case, if the
classifier outputs are well-calibrated conditional probabilities, then the
optimal threshold is half the optimal F1 score. As another special case, if the
classifier is completely uninformative, then the optimal behavior is to
classify all examples as positive. Since the actual prevalence of positive
examples typically is low, this behavior can be considered undesirable. As a
case study, we discuss the results, which can be surprising, of applying this
procedure when predicting 26,853 labels for Medline documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1896</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1896</id><created>2014-02-08</created><updated>2014-12-14</updated><authors><author><keyname>Yu</keyname><forenames>Jingjin</forenames></author><author><keyname>Schwager</keyname><forenames>Mac</forenames></author><author><keyname>Rus</keyname><forenames>Daniela</forenames></author></authors><title>Correlated Orienteering Problem and it Application to Persistent
  Monitoring Tasks</title><categories>cs.RO</categories><comments>Extended version, 18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel non-linear extension to the Orienteering Problem (OP),
called the Correlated Orienteering Problem (COP). We use COP to model the
planning of informative tours for the persistent monitoring of a spatiotemporal
field with time-invariant spatial correlations, in which the tours are
constrained to have limited length. Our focus in this paper is QCOP a quadratic
COP formulation that only looks at correlations between neighboring nodes in a
node network. The main feature of QCOP is a quadratic utility function
capturing the said spatial correlation. QCOP may be solved using mixed integer
quadratic programming (MIQP), with the resulting anytime algorithm capable of
planning multiple disjoint tours that maximize the quadratic utility. In
particular, our algorithm can quickly plan a near-optimal tour over a network
with up to $150$ nodes. Besides performing extensive simulation studies to
verify the algorithm's correctness and characterize its performance, we also
successfully applied it to two realistic persistent monitoring tasks: (i)
estimation over a synthetic spatiotemporal field, and (ii) estimating the
temperature distribution in the state of Massachusetts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1899</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1899</id><created>2014-02-08</created><updated>2014-12-20</updated><authors><author><keyname>Bako</keyname><forenames>Laurent</forenames></author><author><keyname>Ohlsson</keyname><forenames>Henrik</forenames></author></authors><title>A Nonsmooth Optimization Approach to Robust Estimation</title><categories>cs.SY math.OC</categories><comments>28 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of identifying a linear map from
measurements which are subject to intermittent and arbitarily large errors.
This is a fundamental problem in many estimation-related applications such as
fault detection, state estimation in lossy networks, hybrid system
identification, robust estimation, etc. The problem is hard because it exhibits
some intrinsic combinatorial features. Therefore, obtaining an effective
solution necessitates relaxations that are both solvable at a reasonable cost
and effective in the sense that they can return the true parameter vector. The
current paper discusses a nonsmooth convex optimization approach. In
particular, it is shown that under appropriate conditions on the data, an exact
estimate can be recovered from data corrupted by a large (even infinite) number
of gross errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1918</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1918</id><created>2014-02-09</created><updated>2014-05-21</updated><authors><author><keyname>Zhang</keyname><forenames>Yuchen</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Lower bounds on the performance of polynomial-time algorithms for sparse
  linear regression</title><categories>math.ST cs.CC stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under a standard assumption in complexity theory (NP not in P/poly), we
demonstrate a gap between the minimax prediction risk for sparse linear
regression that can be achieved by polynomial-time algorithms, and that
achieved by optimal algorithms. In particular, when the design matrix is
ill-conditioned, the minimax prediction loss achievable by polynomial-time
algorithms can be substantially greater than that of an optimal algorithm. This
result is the first known gap between polynomial and optimal algorithms for
sparse linear regression, and does not depend on conjectures in average-case
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1919</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1919</id><created>2014-02-09</created><authors><author><keyname>Bappah</keyname><forenames>Abubakar Sadiq</forenames></author></authors><title>Appraisal of Social Learning Potentials in Some Trending Mobile
  Computing Applications</title><categories>cs.CY</categories><comments>5 pages, Published with International Journal of Computer Trends and
  Technology (IJCTT)</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  volume 4 Issue 7 July 2013 pp 2017 to 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New Mobile technologies have created a new social dimension where individuals
can develop increased levels of their social awareness by keeping in touch with
old friends, making new friends, dispense new data or product, and getting
information in many more aspects of everyday lives, making one to become more
knowledgeable which is very beneficial especially for students. Social
networks, in particular enable users to share and discuss common interests and
provide infrastructures for integrating various user experiences: synchronous
and asynchronous communication, game-playing, sharing links and files. The
trend of using social networks and social media to deliver and exchange
knowledge could bring a new era of social learning in which learners make use
all four language skills of reading, writing, listening and speaking. Unlike a
traditional e-leaning paradigm with pre-defined curriculum and standard
textbooks, social knowledge could be aggregated on demand, just in time, and in
context of engaging challenges from social networks, making learning more
exciting, social and, game-like experience. Social learning environment engages
the learners in discussion, collaboration, exploration, production, discovery
and creation. Schools should harness this to develop group collaboration skills
and even project based learning activities that span subjects and grade levels.
This paper focuses on technologies and avenues of using mobile phones in
supporting open social learning among undergraduate students in Nigeria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1921</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1921</id><created>2014-02-09</created><authors><author><keyname>Shi</keyname><forenames>Qinfeng</forenames></author><author><keyname>Reid</keyname><forenames>Mark</forenames></author><author><keyname>Caetano</keyname><forenames>Tiberio</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Wang</keyname><forenames>Zhenhua</forenames></author></authors><title>A Hybrid Loss for Multiclass and Structured Prediction</title><categories>cs.LG cs.AI cs.CV</categories><comments>12 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1009.3346</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel hybrid loss for multiclass and structured prediction
problems that is a convex combination of a log loss for Conditional Random
Fields (CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs).
We provide a sufficient condition for when the hybrid loss is Fisher consistent
for classification. This condition depends on a measure of dominance between
labels--specifically, the gap between the probabilities of the best label and
the second best label. We also prove Fisher consistency is necessary for
parametric consistency when learning models such as CRFs. We demonstrate
empirically that the hybrid loss typically performs least as well as--and often
better than--both of its constituent losses on a variety of tasks, such as
human action recognition. In doing so we also provide an empirical comparison
of the efficacy of probabilistic and margin based approaches to multiclass and
structured prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1922</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1922</id><created>2014-02-09</created><updated>2014-03-14</updated><authors><author><keyname>Hofmann</keyname><forenames>Martin</forenames></author><author><keyname>Moser</keyname><forenames>Georg</forenames></author></authors><title>Amortised Resource Analysis and Typed Polynomial Interpretations
  (extended version)</title><categories>cs.LO cs.PL</categories><comments>25 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We introduce a novel resource analysis for typed term rewrite systems based
on a potential-based type system. This type system gives rise to polynomial
bounds on the innermost runtime complexity. We relate the thus obtained
amortised resource analysis to polynomial interpretations and obtain the
perhaps surprising result that whenever a rewrite system R can be well-typed,
then there exists a polynomial interpretation that orients R. For this we
adequately adapt the standard notion of polynomial interpretations to the typed
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1925</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1925</id><created>2014-02-09</created><authors><author><keyname>V</keyname><forenames>Priyadharshini.</forenames></author><author><keyname>A</keyname><forenames>Malathi.</forenames></author></authors><title>Survey on software testing techniques in cloud computing</title><categories>cs.SE</categories><comments>5 pages, 2 figures. volume 4 issue 8 august 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is the next stage of the internet evolution. It relies on
sharing of resources to achieve coherence on a network. It is emerged as new
computing standard that impacts several different research fields, including
software testing. There are various software techniques used for testing
application. It not only changes the way of obtaining computing resources but
also changes the way of managing and delivering computing services,
technologies and solutions, meanwhile it causes new issues, challenges and
needs in software testing. Software testing in cloud can reduce the need for
hardware and software resources and offer a flexible and efficient alternative
to the traditional software testing process. This paper provides an overview
regarding trends, oppurtunities, challenges, issues, and needs in cloud testing
and cloud based application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1931</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1931</id><created>2014-02-09</created><authors><author><keyname>Ahmed</keyname><forenames>Rashid</forenames></author><author><keyname>Avaritsiotis</keyname><forenames>John A.</forenames></author></authors><title>MCA Learning Algorithm for Incident Signals Estimation: A Review</title><categories>cs.NE</categories><comments>5 pages,8 figures, 1 table. International Journal of Computer Trends
  and Technology (IJCTT),Feb 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently there has been many works on adaptive subspace filtering in the
signal processing literature. Most of them are concerned with tracking the
signal subspace spanned by the eigenvectors corresponding to the eigenvalues of
the covariance matrix of the signal plus noise data. Minor Component Analysis
(MCA) is important tool and has a wide application in telecommunications,
antenna array processing, statistical parametric estimation, etc. As an
important feature extraction technique, MCA is a statistical method of
extracting the eigenvector associated with the smallest eigenvalue of the
covariance matrix. In this paper, we will present a MCA learning algorithm to
extract minor component from input signals, and the learning rate parameter is
also presented, which ensures fast convergence of the algorithm, because it has
direct effect on the convergence of the weight vector and the error level is
affected by this value. MCA is performed to determine the estimated DOA.
Simulation results will be furnished to illustrate the theoretical results
achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1932</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1932</id><created>2014-02-09</created><authors><author><keyname>Malhotra</keyname><forenames>Dr. Rahul</forenames></author><author><keyname>Jain</keyname><forenames>Prince</forenames></author></authors><title>An EMUSIM Technique and its Components in Cloud Computing- A Review</title><categories>cs.DC</categories><comments>6 pages, 3 figures and &quot;Published with International Journal of
  Computer Trends and Technology (IJCTT)&quot;</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  4(8): 2435-2440 August 2013</journal-ref><doi>10.14445/2231-2803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent efforts to design and develop Cloud technologies focus on defining
novel methods, policies and mechanisms for efficiently managing Cloud
infrastructures. One key challenge potential Cloud customers have before
renting resources is to know how their services will behave in a set of
resources and the costs involved when growing and shrinking their resource
pool. Most of the studies in this area rely on simulation-based experiments,
which consider simplified modeling of applications and computing environment.
In order to better predict service's behavior on Cloud platforms, an integrated
architecture that is based on both simulation and emulation. The proposed
architecture, named EMUSIM, automatically extracts information from application
behavior via emulation and then uses this information to generate the
corresponding simulation model. This paper presents brief overview of the
EMUSIM technique and its components. The work in this paper focuses on
architecture and operation details of Automated Emulation Framework (AEF),
QAppDeployer and proposes Cloud Sim Application for Simulation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1936</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1936</id><created>2014-02-09</created><authors><author><keyname>Larsson</keyname><forenames>N. Jesper</forenames></author></authors><title>Integer Set Compression and Statistical Modeling</title><categories>cs.IT cs.DS math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compression of integer sets and sequences has been extensively studied for
settings where elements follow a uniform probability distribution. In addition,
methods exist that exploit clustering of elements in order to achieve higher
compression performance. In this work, we address the case where enumeration of
elements may be arbitrary or random, but where statistics is kept in order to
estimate probabilities of elements. We present a recursive subset-size encoding
method that is able to benefit from statistics, explore the effects of
permuting the enumeration order based on element probabilities, and discuss
general properties and possibilities for this class of compression problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1939</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1939</id><created>2014-02-09</created><updated>2015-03-29</updated><authors><author><keyname>Yan</keyname><forenames>Xiao-Yong</forenames></author><author><keyname>Minnhagen</keyname><forenames>Petter</forenames></author></authors><title>Maximum Entropy, Word-Frequency, Chinese Characters, and Multiple
  Meanings</title><categories>physics.soc-ph cs.CL</categories><comments>15 pages, 10 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The word-frequency distribution of a text written by an author is well
accounted for by a maximum entropy distribution, the RGF (random group
formation)-prediction. The RGF-distribution is completely determined by the a
priori values of the total number of words in the text (M), the number of
distinct words (N) and the number of repetitions of the most common word
(k_max). It is here shown that this maximum entropy prediction also describes a
text written in Chinese characters. In particular it is shown that although the
same Chinese text written in words and Chinese characters have quite
differently shaped distributions, they are nevertheless both well predicted by
their respective three a priori characteristic values. It is pointed out that
this is analogous to the change in the shape of the distribution when
translating a given text to another language. Another consequence of the
RGF-prediction is that taking a part of a long text will change the input
parameters (M, N, k_max) and consequently also the shape of the frequency
distribution. This is explicitly confirmed for texts written in Chinese
characters. Since the RGF-prediction has no system-specific information beyond
the three a priori values (M, N, k_max), any specific language characteristic
has to be sought in systematic deviations from the RGF-prediction and the
measured frequencies. One such systematic deviation is identified and, through
a statistical information theoretical argument and an extended RGF-model, it is
proposed that this deviation is caused by multiple meanings of Chinese
characters. The effect is stronger for Chinese characters than for Chinese
words. The relation between Zipf's law, the Simon-model for texts and the
present results are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1940</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1940</id><created>2014-02-09</created><authors><author><keyname>Verde</keyname><forenames>Nino Vincenzo</forenames></author><author><keyname>Ateniese</keyname><forenames>Giuseppe</forenames></author><author><keyname>Gabrielli</keyname><forenames>Emanuele</forenames></author><author><keyname>Mancini</keyname><forenames>Luigi Vincenzo</forenames></author><author><keyname>Spognardi</keyname><forenames>Angelo</forenames></author></authors><title>No NAT'd User left Behind: Fingerprinting Users behind NAT from NetFlow
  Records alone</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is generally recognized that the traffic generated by an individual
connected to a network acts as his biometric signature. Several tools exploit
this fact to fingerprint and monitor users. Often, though, these tools assume
to access the entire traffic, including IP addresses and payloads. This is not
feasible on the grounds that both performance and privacy would be negatively
affected. In reality, most ISPs convert user traffic into NetFlow records for a
concise representation that does not include, for instance, any payloads. More
importantly, large and distributed networks are usually NAT'd, thus a few IP
addresses may be associated to thousands of users. We devised a new
fingerprinting framework that overcomes these hurdles. Our system is able to
analyze a huge amount of network traffic represented as NetFlows, with the
intent to track people. It does so by accurately inferring when users are
connected to the network and which IP addresses they are using, even though
thousands of users are hidden behind NAT. Our prototype implementation was
deployed and tested within an existing large metropolitan WiFi network serving
about 200,000 users, with an average load of more than 1,000 users
simultaneously connected behind 2 NAT'd IP addresses only. Our solution turned
out to be very effective, with an accuracy greater than 90%. We also devised
new tools and refined existing ones that may be applied to other contexts
related to NetFlow analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1943</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1943</id><created>2014-02-09</created><authors><author><keyname>Kumar</keyname><forenames>G. Vijay</forenames></author><author><keyname>Raykundaliya</keyname><forenames>Ravikumar S.</forenames></author><author><keyname>Prasad</keyname><forenames>Dr. P. Naga</forenames></author></authors><title>Proactive Web Server Protocol for Complaint Assessment</title><categories>cs.NI cs.CR</categories><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  6(40):4-6, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vulnerability Discovery with attack Injection security threats are increasing
for the server software, when software is developed, the software tested for
the functionality. Due to unawareness of software vulnerabilities most of the
software before pre-Release the software should be thoroughly tested for not
only functionality reliability, but should be tested for the security flows
(or) vulnerabilities. The approaches such as fuzzers, Fault injection,
vulnerabilities scanners, static vulnerabilities analyzers, Run time prevention
mechanisms and software Rejuvenation are identifying the un-patched software
which is open for security threats address to solve the problem &quot;security
testing&quot;. These techniques are useful for generating attacks but cannot be
extendable for the new land of attacks. The system called proactive
vulnerability attack injection tool is suitable for adding new attacks
injection vectors, methods to define new protocol states (or) Specification
using the interface of tool includes Network server protocol specification
using GUI, Attacks generator, Attack injector, monitoring module at the victim
injector, monitoring module at the victim machine and the attacks injection
report generation. This tool can address most of the vulnerabilities (or)
security flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1946</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1946</id><created>2014-02-09</created><authors><author><keyname>Thakare</keyname><forenames>Prajwal R</forenames></author><author><keyname>Rao</keyname><forenames>K. Hanumantha</forenames></author></authors><title>Anomaly Detection Based on Access Behavior and Document Rank Algorithm</title><categories>cs.NI cs.CR cs.IR</categories><comments>6 pages, 3 figures</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  6(39):4-6, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed denial of service(DDos) attack is ongoing dangerous threat to the
Internet. Commonly, DDos attacks are carried out at the network layer, e.g. SYN
flooding, ICMP flooding and UDP flooding, which are called Distributed denial
of service attacks. The intention of these DDos attacks is to utilize the
network bandwidth and deny service to authorize users of the victim systems.
Obtain from the low layers, new application-layer-based DDos attacks utilizing
authorize HTTP requests to overload victim resources are more undetectable.
When these are taking place during crowd events of any popular website, this is
the case is very serious. The state-of-art approaches cannot handle the
situation where there is no considerable deviation between the normal and the
attackers activity. The page rank and proximity graph representation of online
web accesses takes much time in practice. There should be less computational
complexity, than of proximity graph search. Hence proposing Web Access Table
mechanism to hold the data such as &quot;who accessed what and how many times, and
their rank on average&quot; to find the anomalous web access behavior. The system
takes less computational complexity and may produce considerable time
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1947</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1947</id><created>2014-02-09</created><authors><author><keyname>Arslan</keyname><forenames>Farrukh</forenames></author></authors><title>Classification Tree Diagrams in Health Informatics Applications</title><categories>cs.IR cs.CV cs.LG</categories><comments>In the Proceedings of 7th International Conference on the Theory and
  Application of Diagrams 2012. 7th International Conference on the Theory and
  Application of Diagrams 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Health informatics deal with the methods used to optimize the acquisition,
storage and retrieval of medical data, and classify information in healthcare
applications. Healthcare analysts are particularly interested in various
computer informatics areas such as; knowledge representation from data, anomaly
detection, outbreak detection methods and syndromic surveillance applications.
Although various parametric and non-parametric approaches are being proposed to
classify information from data, classification tree diagrams provide an
interactive visualization to analysts as compared to other methods. In this
work we discuss application of classification tree diagrams to classify
information from medical data in healthcare applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1956</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1956</id><created>2014-02-09</created><authors><author><keyname>Jabbour</keyname><forenames>Said</forenames></author><author><keyname>Lonlac</keyname><forenames>Jerry</forenames></author><author><keyname>Sais</keyname><forenames>Lakhdar</forenames></author><author><keyname>Salhi</keyname><forenames>Yakoub</forenames></author></authors><title>Revisiting the Learned Clauses Database Reduction Strategies</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revisit an important issue of CDCL-based SAT solvers,
namely the learned clauses database management policies. Our motivation takes
its source from a simple observation on the remarkable performances of both
random and size-bounded reduction strategies. We first derive a simple
reduction strategy, called Size-Bounded Randomized strategy (in short SBR),
that combines maintaing short clauses (of size bounded by k), while deleting
randomly clauses of size greater than k. The resulting strategy outperform the
state-of-the-art, namely the LBD based one, on SAT instances taken from the
last SAT competition. Reinforced by the interest of keeping short clauses, we
propose several new dynamic variants, and we discuss their performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1958</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1958</id><created>2014-02-09</created><authors><author><keyname>Guez</keyname><forenames>Arthur</forenames></author><author><keyname>Silver</keyname><forenames>David</forenames></author><author><keyname>Dayan</keyname><forenames>Peter</forenames></author></authors><title>Better Optimism By Bayes: Adaptive Planning with Rich Models</title><categories>cs.AI cs.LG stat.ML</categories><comments>11 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computational costs of inference and planning have confined Bayesian
model-based reinforcement learning to one of two dismal fates: powerful
Bayes-adaptive planning but only for simplistic models, or powerful, Bayesian
non-parametric models but using simple, myopic planning strategies such as
Thompson sampling. We ask whether it is feasible and truly beneficial to
combine rich probabilistic models with a closer approximation to fully Bayesian
planning. First, we use a collection of counterexamples to show formal problems
with the over-optimism inherent in Thompson sampling. Then we leverage
state-of-the-art techniques in efficient Bayes-adaptive planning and
non-parametric Bayesian methods to perform qualitatively better than both
existing conventional algorithms and Thompson sampling on two contextual
bandit-like problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1971</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1971</id><created>2014-02-09</created><updated>2014-02-18</updated><authors><author><keyname>Javed</keyname><forenames>Mohammed</forenames></author><author><keyname>Nagabhushan</keyname><forenames>P.</forenames></author><author><keyname>Chaudhuri</keyname><forenames>B. B.</forenames></author></authors><title>Direct Processing of Run Length Compressed Document Image for
  Segmentation and Characterization of a Specified Block</title><categories>cs.CV</categories><comments>7 Pages and Published with International Journal of Computer
  Applications (IJCA)</comments><journal-ref>IJCA 83(15):1-6, December 2013</journal-ref><doi>10.5120/14521-2926</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extracting a block of interest referred to as segmenting a specified block in
an image and studying its characteristics is of general research interest, and
could be a challenging if such a segmentation task has to be carried out
directly in a compressed image. This is the objective of the present research
work. The proposal is to evolve a method which would segment and extract a
specified block, and carry out its characterization without decompressing a
compressed image, for two major reasons that most of the image archives contain
images in compressed format and decompressing an image indents additional
computing time and space. Specifically in this research work, the proposal is
to work on run-length compressed document images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1973</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1973</id><created>2014-02-09</created><updated>2014-10-02</updated><authors><author><keyname>Fawzi</keyname><forenames>Alhussein</forenames></author><author><keyname>Davies</keyname><forenames>Mike</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Dictionary learning for fast classification based on soft-thresholding</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classifiers based on sparse representations have recently been shown to
provide excellent results in many visual recognition and classification tasks.
However, the high cost of computing sparse representations at test time is a
major obstacle that limits the applicability of these methods in large-scale
problems, or in scenarios where computational power is restricted. We consider
in this paper a simple yet efficient alternative to sparse coding for feature
extraction. We study a classification scheme that applies the soft-thresholding
nonlinear mapping in a dictionary, followed by a linear classifier. A novel
supervised dictionary learning algorithm tailored for this low complexity
classification architecture is proposed. The dictionary learning problem, which
jointly learns the dictionary and linear classifier, is cast as a difference of
convex (DC) program and solved efficiently with an iterative DC solver. We
conduct experiments on several datasets, and show that our learning algorithm
that leverages the structure of the classification problem outperforms generic
learning procedures. Our simple classifier based on soft-thresholding also
competes with the recent sparse coding classifiers, when the dictionary is
learned appropriately. The adopted classification scheme further requires less
computational time at the testing stage, compared to other classifiers. The
proposed scheme shows the potential of the adequately trained soft-thresholding
mapping for classification and paves the way towards the development of very
efficient classification methods for vision problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1974</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1974</id><created>2014-02-09</created><authors><author><keyname>Chauhan</keyname><forenames>Rajesh R</forenames></author><author><keyname>Kumar</keyname><forenames>G S Praveen</forenames></author></authors><title>A Novel Approach to Detect Spam Worms Propagation with Monitoring the
  Footprinting</title><categories>cs.NI cs.CR</categories><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  6(23):3-6, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the key security threats on the Internet are the compromised machines
that can be used to launch various security attacks such as spamming and
spreading malware, accessing useful information and DDoS. Attackers for
spamming activity are volunteer by large number of compromised machines. Our
main focus is on detection of the compromised machines in a network that may be
or are involved in the spamming activities; these machines are commonly known
as spam zombies. Activities such as port scan, DB scan and so on are treated as
malicious activity within the network. So to overcome that we develop one of
the most effective spam zombie detection system within the network based on the
behavior of other systems as if performing the above activities are treated as
zombies machines. If any system within the network try's to gather some
information about any other system then this is treated as a malicious activity
and should be not allowed to do so. SYN packets are used in order to initiate
communication within the network so as to establish connection. If any system
try's to flood the network with these packets we can make an assumption that
the system is trying to gather the information about other system. This is what
called footprinting. So we will try to detect any system involved in
footprinting and report to the administrator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1978</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1978</id><created>2014-02-09</created><updated>2014-12-31</updated><authors><author><keyname>Klimek</keyname><forenames>Radoslaw</forenames></author></authors><title>A System for Deduction-based Formal Verification of Workflow-oriented
  Software Models</title><categories>cs.SE</categories><comments>International Journal of Applied Mathematics and Computer Science</comments><journal-ref>2014, vol. 24, no. 4, pp. 941-956</journal-ref><doi>10.2478/amcs-2014-0069</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work concerns formal verification of workflow-oriented software models
using deductive approach. The formal correctness of a model's behaviour is
considered. Manually building logical specifications, which are considered as a
set of temporal logic formulas, seems to be the significant obstacle for an
inexperienced user when applying the deductive approach. A system, and its
architecture, for the deduction-based verification of workflow-oriented models
is proposed. The process of inference is based on the semantic tableaux method
which has some advantages when compared to traditional deduction strategies.
The algorithm for an automatic generation of logical specifications is
proposed. The generation procedure is based on the predefined workflow patterns
for BPMN, which is a standard and dominant notation for the modeling of
business processes. The main idea for the approach is to consider patterns,
defined in terms of temporal logic,as a kind of (logical) primitives which
enable the transformation of models to temporal logic formulas constituting a
logical specification. Automation of the generation process is crucial for
bridging the gap between intuitiveness of the deductive reasoning and the
difficulty of its practical application in the case when logical specifications
are built manually. This approach has gone some way towards supporting,
hopefully enhancing our understanding of, the deduction-based formal
verification of workflow-oriented models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1985</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1985</id><created>2014-02-09</created><authors><author><keyname>Klimek</keyname><forenames>Radoslaw</forenames></author></authors><title>Generating Logical Specifications from Requirements Models for
  Deduction-based Formal Verification</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work concerns automatic generation of logical specifications from
requirements models. Logical specifications obtained in such a way can be
subjected to formal verification using deductive reasoning. Formal verification
concerns correctness of a model behaviour. Reliability of the requirements
engineering is essential for all phases of software development processes.
Deductive reasoning is an important alternative among other formal methods.
However, logical specifications, considered as sets of temporal logic formulas,
are difficult to specify manually by inexperienced users and this fact can be
regarded as a significant obstacle to practical use of deduction-based
verification tools. A method of building requirements models using some UML
diagrams, including their logical specifications, is presented step by step.
Organizing activity diagrams into predefined workflow patterns enables
automated extraction of logical specifications. The crucial aspect of the
presented approach is integrating the requirements engineering phase and the
automatic generation of logical specifications. A system of the deduction-based
verification is proposed. The reasoning process could be based on the semantic
tableaux method. A simple yet illustrative example of the requirements
elicitation and verification is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1986</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1986</id><created>2014-02-09</created><authors><author><keyname>Bouneffouf</keyname><forenames>Djallel</forenames></author></authors><title>Recommandation mobile, sensible au contexte de contenus \'evolutifs:
  Contextuel-E-Greedy</title><categories>cs.AI</categories><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce in this paper an algorithm named Contextuel-E-Greedy that
tackles the dynamicity of the user's content. It is based on dynamic
exploration/exploitation tradeoff and can adaptively balance the two aspects by
deciding which situation is most relevant for exploration or exploitation. The
experimental results demonstrate that our algorithm outperforms surveyed
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1987</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1987</id><created>2014-02-09</created><authors><author><keyname>Wang</keyname><forenames>Qi</forenames></author><author><keyname>Taylor</keyname><forenames>John E.</forenames></author></authors><title>Quantifying Human Mobility Perturbation and Resilience in Natural
  Disasters</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages, 3 figures</comments><doi>10.1371/journal.pone.0112608</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human mobility is influenced by environmental change and natural disasters.
Researchers have used trip distance distribution, radius of gyration of
movements, and individuals' visited locations to understand and capture human
mobility patterns and trajectories. However, our knowledge of human movements
during natural disasters is limited owing to both a lack of empirical data and
the low precision of available data. Here, we studied human mobility using
high-resolution movement data from individuals in New York City during and for
several days after Hurricane Sandy in 2012. We found the human movements
followed truncated power-law distributions during and after Hurricane Sandy,
although the {\beta} value was noticeably larger during the first 24 hours
after the storm struck. Also, we examined two parameters: the center of mass
and the radius of gyration of each individual's movements. We found that their
values during perturbation states and steady states are highly correlated,
suggesting human mobility data obtained in steady states can possibly predict
the perturbation state. Our results demonstrate that human movement
trajectories experienced significant perturbations during hurricanes, but also
exhibited high resilience. We expect the study will stimulate future research
on the perturbation and inherent resilience of human mobility under the
influence of natural disasters. For example, mobility patterns in coastal urban
areas could be examined as tropical cyclones approach, gain or dissipate in
strength, and as the path of the storm changes. Understanding nuances of human
mobility under the influence of disasters will enable more effective
evacuation, emergency response planning and development of strategies and
policies to reduce fatality, injury, and economic loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.1992</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.1992</id><created>2014-02-09</created><authors><author><keyname>Chen</keyname><forenames>Mingmin</forenames></author><author><keyname>Yu</keyname><forenames>Shizhuo</forenames></author><author><keyname>Franz</keyname><forenames>Nico</forenames></author><author><keyname>Bowers</keyname><forenames>Shawn</forenames></author><author><keyname>Lu\''asher</keyname><forenames>Bertram</forenames></author></authors><title>Euler/X: A Toolkit for Logic-based Taxonomy Integration</title><categories>cs.LO cs.DB</categories><comments>8 pages, 14 figures, WFLP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Euler/X, a toolkit for logic-based taxonomy integration. Given
two taxonomies and a set of alignment constraints between them, Euler/X
provides tools for detecting, explaining, and reconciling inconsistencies;
finding all possible merges between (consistent) taxonomies; and visualizing
merge results. Euler/X employs a number of different underlying reasoning
systems, including first-order reasoners (Prover9 and Mace4), answer set
programming (DLV and Potassco), and RCC reasoners (PyRCC8). We demonstrate the
features of Euler/X and provide experimental results showing its feasibility on
various synthetic and real-world examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2003</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2003</id><created>2014-02-09</created><updated>2014-03-11</updated><authors><author><keyname>Matei</keyname><forenames>Sorin Adam</forenames></author><author><keyname>Rauh</keyname><forenames>Nicholas K.</forenames></author><author><keyname>Kansa</keyname><forenames>Eric C.</forenames></author></authors><title>A New Approach to Reporting Archaeological Surveys: Connecting Rough
  Cilicia, Visible Past and Open Context through loose coupling and 3d codes</title><categories>cs.DL</categories><comments>A previous version of this paper was presented at the 2012 Conference
  of the Association of Ancient Historians, May 3-5, Chapell Hill, North
  Carolina</comments><journal-ref>Journal of Data Mining &amp; Digital Humanities, 2014 (June 24, 2014)
  jdmdh:12</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The project presents the strategy adopted by the Rough Cilicia Archaeological
Survey team for publishing its primary data and reports via three potentially
transformative strategies for digital humanities: Loose coupling of digital
data curation and publishing platforms. In loosely coupled systems, components
share only a limited set of simple assumptions, which enables systems to evolve
dynamically. Collaborative creation of map based narrative content. Connecting
print scholarship (book, reports, article) to online resources via
two-dimensional barcodes (2D codes) that can be printed on paper and can call
up hyperlinks when scanned with a Smartphone. The three strategies are made
possible by loosely coupling two autonomous services: Visible Past, dedicated
to web collaboration and digital-print publishing and Open Context, which is a
geo-historical data archiving and publishing service. The Rough Cilicia
Archaeological Survey, Visible Past, and Open Context work together to
illustrate a new genre of scholarship, which combine qualitative narratives and
quantitative representations of space and social phenomena. The project
provides tools for collaborative creation of rich scholarly narratives that are
spatially located and for connecting print publications to the digital realm.
The project is a case study for utilizing the three new strategies for creating
and publishing spatial humanities scholarship more broadly for ancient
historians.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2009</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2009</id><created>2014-02-09</created><authors><author><keyname>Rosenfeld</keyname><forenames>Meni</forenames></author></authors><title>Analysis of Hashrate-Based Double Spending</title><categories>cs.CR</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bitcoin is the world's first decentralized digital currency. Its main
technical innovation is the use of a blockchain and hash-based proof of work to
synchronize transactions and prevent double-spending the currency. While the
qualitative nature of this system is well understood, there is widespread
confusion about its quantitative aspects and how they relate to attack vectors
and their countermeasures. In this paper we take a look at the stochastic
processes underlying typical attacks and their resulting probabilities of
success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2011</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2011</id><created>2014-02-09</created><authors><author><keyname>Rawat</keyname><forenames>Ankit Singh</forenames></author><author><keyname>Papailiopoulos</keyname><forenames>Dimitris S.</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Locality and Availability in Distributed Storage</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of code symbol availability: a code symbol is
said to have $(r, t)$-availability if it can be reconstructed from $t$ disjoint
groups of other symbols, each of size at most $r$. For example, $3$-replication
supports $(1, 2)$-availability as each symbol can be read from its $t= 2$ other
(disjoint) replicas, i.e., $r=1$. However, the rate of replication must vanish
like $\frac{1}{t+1}$ as the availability increases.
  This paper shows that it is possible to construct codes that can support a
scaling number of parallel reads while keeping the rate to be an arbitrarily
high constant. It further shows that this is possible with the minimum distance
arbitrarily close to the Singleton bound. This paper also presents a bound
demonstrating a trade-off between minimum distance, availability and locality.
Our codes match the aforementioned bound and their construction relies on
combinatorial objects called resolvable designs.
  From a practical standpoint, our codes seem useful for distributed storage
applications involving hot data, i.e., the information which is frequently
accessed by multiple processes in parallel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2013</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2013</id><created>2014-02-09</created><authors><author><keyname>Yu</keyname><forenames>Xintong</forenames></author><author><keyname>Liu</keyname><forenames>Xiaohan</forenames></author><author><keyname>Chen</keyname><forenames>Yisong</forenames></author></authors><title>Foreground segmentation based on multi-resolution and matting</title><categories>cs.CV</categories><comments>5 pages. 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a foreground segmentation algorithm that does foreground
extraction under different scales and refines the result by matting. First, the
input image is filtered and resampled to 5 different resolutions. Then each of
them is segmented by adaptive figure-ground classification and the best
segmentation is automatically selected by an evaluation score that maximizes
the difference between foreground and background. This segmentation is
upsampled to the original size, and a corresponding trimap is built.
Closed-form matting is employed to label the boundary region, and the result is
refined by a final figure-ground classification. Experiments show the success
of our method in treating challenging images with cluttered background and
adapting to loose initial bounding-box.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2016</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2016</id><created>2014-02-09</created><updated>2014-03-07</updated><authors><author><keyname>Liu</keyname><forenames>Wenxi</forenames></author><author><keyname>Chan</keyname><forenames>Antoni B.</forenames></author><author><keyname>Lau</keyname><forenames>Rynson W. H.</forenames></author><author><keyname>Manocha</keyname><forenames>Dinesh</forenames></author></authors><title>Leveraging Long-Term Predictions and Online-Learning in Agent-based
  Multiple Person Tracking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a multiple-person tracking algorithm, based on combining particle
filters and RVO, an agent-based crowd model that infers collision-free
velocities so as to predict pedestrian's motion. In addition to position and
velocity, our tracking algorithm can estimate the internal goals (desired
destination or desired velocity) of the tracked pedestrian in an online manner,
thus removing the need to specify this information beforehand. Furthermore, we
leverage the longer-term predictions of RVO by deriving a higher-order particle
filter, which aggregates multiple predictions from different prior time steps.
This yields a tracker that can recover from short-term occlusions and spurious
noise in the appearance model. Experimental results show that our tracking
algorithm is suitable for predicting pedestrians' behaviors online without
needing scene priors or hand-annotated goal information, and improves tracking
in real-world crowded scenes under low frame rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2018</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2018</id><created>2014-02-09</created><authors><author><keyname>&#x15e;tef&#x103;nescu</keyname><forenames>R&#x103;zvan</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author><author><keyname>Navon</keyname><forenames>Ionel M.</forenames></author></authors><title>Comparison of POD reduced order strategies for the nonlinear 2D Shallow
  Water Equations</title><categories>cs.NA math.NA</categories><comments>23 pages, 8 figures, 5 tables</comments><report-no>TR 2/2014</report-no><doi>10.1002/fld.3946</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces tensorial calculus techniques in the framework of
Proper Orthogonal Decomposition (POD) to reduce the computational complexity of
the reduced nonlinear terms. The resulting method, named tensorial POD, can be
applied to polynomial nonlinearities of any degree $p$. Such nonlinear terms
have an on-line complexity of $\mathcal{O}(k^{p+1})$, where $k$ is the
dimension of POD basis, and therefore is independent of full space dimension.
However it is efficient only for quadratic nonlinear terms since for higher
nonlinearities standard POD proves to be less time consuming once the POD basis
dimension $k$ is increased. Numerical experiments are carried out with a two
dimensional shallow water equation (SWE) test problem to compare the
performance of tensorial POD, standard POD, and POD/Discrete Empirical
Interpolation Method (DEIM). Numerical results show that tensorial POD
decreases by $76\times$ times the computational cost of the on-line stage of
standard POD for configurations using more than $300,000$ model variables. The
tensorial POD SWE model was only $2-8\times$ slower than the POD/DEIM SWE model
but the implementation effort is considerably increased. Tensorial calculus was
again employed to construct a new algorithm allowing POD/DEIM shallow water
equation model to compute its off-line stage faster than the standard and
tensorial POD approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2019</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2019</id><created>2014-02-09</created><authors><author><keyname>Leiva-Mederos</keyname><forenames>Amed</forenames></author><author><keyname>Senso</keyname><forenames>Jose A.</forenames></author><author><keyname>Dominguez-Velasco</keyname><forenames>Sandor</forenames></author><author><keyname>Hipola</keyname><forenames>Pedro</forenames></author></authors><title>Authoris: a tool for authority control in the semantic web</title><categories>cs.DL</categories><comments>27 pages, 8 figures</comments><journal-ref>Library Hi Tech, Vol. 31 Iss: 3, pp.536 - 553</journal-ref><doi>10.1108/LHT-12-20112-0135</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: The purpose of this paper is to propose a tool that generates
authority files to be integrated with linked data by means of learning rules.
AUTHORIS is software developed to enhance authority control and information
exchange among bibliographic and non-bibliographic entities.
  Design / methodology / approach: The article analyzes different methods
previously developed for authority control as well as IFLA and ALA standards
for managing bibliographic records. Semantic Web technologies are also
evaluated. AUTHORIS relies on Drupal and incorporates the protocols of Dublin
Core, SIOC, SKOS and FOAF. The tool has also taken into account the
obsolescence of MARC and its substitution by FRBR and RDA. Its effectiveness
was evaluated applying a learning test proposed by RDA. Over 80 percent of the
actions were carried out correctly.
  Findings: The use of learning rules and the facilities of linked data make it
easier for information organizations to reutilize products for authority
control and distribute them in a fair and efficient manner.
  Research limitations / implications: The ISAD-G records were the ones
presenting most errors. EAD was found to be second in the number of errors
produced. The rest of the formats --MARC 21, Dublin Core, FRAD, RDF, OWL, XBRL
and FOAF-- showed fewer than 20 errors in total.
  Practical implications: AUTHORIS offers institutions the means of sharing
data with a high level of stability, helping to detect records that are
duplicated and contributing to lexical disambiguation and data enrichment.
  Originality / value: The software combines the facilities of linked data, the
potency of the algorithms for converting bibliographic data, and the precision
of learning rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2020</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2020</id><created>2014-02-09</created><authors><author><keyname>Zhang</keyname><forenames>Kang</forenames></author><author><keyname>Li</keyname><forenames>Jiyang</forenames></author><author><keyname>Li</keyname><forenames>Yijing</forenames></author><author><keyname>Hu</keyname><forenames>Weidong</forenames></author><author><keyname>Sun</keyname><forenames>Lifeng</forenames></author><author><keyname>Yang</keyname><forenames>Shiqiang</forenames></author></authors><title>Binary Stereo Matching</title><categories>cs.CV</categories><comments>Pattern Recognition (ICPR), 2012 21st International Conference on</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel binary-based cost computation and
aggregation approach for stereo matching problem. The cost volume is
constructed through bitwise operations on a series of binary strings. Then this
approach is combined with traditional winner-take-all strategy, resulting in a
new local stereo matching algorithm called binary stereo matching (BSM). Since
core algorithm of BSM is based on binary and integer computations, it has a
higher computational efficiency than previous methods. Experimental results on
Middlebury benchmark show that BSM has comparable performance with
state-of-the-art local stereo methods in terms of both quality and speed.
Furthermore, experiments on images with radiometric differences demonstrate
that BSM is more robust than previous methods under these changes, which is
common under real illumination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2022</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2022</id><created>2014-02-09</created><authors><author><keyname>Tamta</keyname><forenames>Pawan</forenames></author><author><keyname>Pande</keyname><forenames>Bhagwati Prasad</forenames></author><author><keyname>Dhami</keyname><forenames>H. S</forenames></author></authors><title>Reduction of Maximum Flow Network Interdiction Problem from The Clique
  Problem</title><categories>cs.DS math.OC</categories><comments>10 pages,3 figures. arXiv admin note: substantial text overlap with
  arXiv:1312.6492</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum Flow Network Interdiction Problem (MFNIP) is known to be strongly
NP-hard problem. We solve a simple form of MFNIP in polynomial time. We review
the reduction of MFNIP from the clique problem. We propose a polynomial time
solution to the Clique Problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2025</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2025</id><created>2014-02-09</created><updated>2015-09-23</updated><authors><author><keyname>Ohkubo</keyname><forenames>Jun</forenames></author></authors><title>Nonlinear Kalman filter based on duality relations between continuous
  and discrete-state stochastic processes</title><categories>cs.SY cond-mat.stat-mech</categories><comments>11 pages, 3 figures</comments><journal-ref>Phys. Rev. E 92, 043302 (2015)</journal-ref><doi>10.1103/PhysRevE.92.043302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new application of duality relations of stochastic processes is
demonstrated. Although conventional usages of the duality relations need
analytical solutions for the dual processes, we here employ numerical solutions
of the dual processes and investigate the usefulness. As a demonstration,
estimation problems of hidden variables in stochastic differential equations
are discussed. Employing algebraic probability theory, a little complicated
birth-death process is derived from the stochastic differential equations, and
an estimation method based on the ensemble Kalman filter is proposed. As a
result, the possibility for making faster computational algorithms based on the
duality concepts is shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2029</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2029</id><created>2014-02-09</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>Classical mathematical structures within topological graph theory</title><categories>math.GN cs.DM</categories><comments>28 pages</comments><msc-class>68-xx, 51-xx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite simple graphs are a playground for classical areas of mathematics. We
illustrate this by looking at some theorems. These are slightly enhanced
preparation notes for a talk given at the joint AMS meeting of January 16, 2014
in Baltimore.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2031</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2031</id><created>2014-02-09</created><authors><author><keyname>Wang</keyname><forenames>Wen</forenames></author><author><keyname>Cui</keyname><forenames>Zhen</forenames></author><author><keyname>Chang</keyname><forenames>Hong</forenames></author><author><keyname>Shan</keyname><forenames>Shiguang</forenames></author><author><keyname>Chen</keyname><forenames>Xilin</forenames></author></authors><title>Deeply Coupled Auto-encoder Networks for Cross-view Classification</title><categories>cs.CV cs.LG cs.NE</categories><comments>11 pages, 3 figures, 3 tables</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The comparison of heterogeneous samples extensively exists in many
applications, especially in the task of image classification. In this paper, we
propose a simple but effective coupled neural network, called Deeply Coupled
Autoencoder Networks (DCAN), which seeks to build two deep neural networks,
coupled with each other in every corresponding layers. In DCAN, each deep
structure is developed via stacking multiple discriminative coupled
auto-encoders, a denoising auto-encoder trained with maximum margin criterion
consisting of intra-class compactness and inter-class penalty. This single
layer component makes our model simultaneously preserve the local consistency
and enhance its discriminative capability. With increasing number of layers,
the coupled networks can gradually narrow the gap between the two views.
Extensive experiments on cross-view image classification tasks demonstrate the
superiority of our method over state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2032</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2032</id><created>2014-02-09</created><authors><author><keyname>Shirani</keyname><forenames>Farhad</forenames></author><author><keyname>Pradhan</keyname><forenames>Sandeep</forenames></author></authors><title>An Achievable Rate-Distortion Region for the Multiple Descriptions
  Problem</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multiple-descriptions (MD) coding strategy is proposed and an inner bound
to the achievable rate-distortion region is derived. The scheme utilizes linear
codes. It is shown in two different MD set-ups that the linear coding scheme
achieves a larger rate-distortion region than previously known random coding
strategies. Furthermore, it is shown via an example that the best known random
coding scheme for the set-up can be improved by including additional randomly
generated codebooks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2034</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2034</id><created>2014-02-09</created><authors><author><keyname>Albert</keyname><forenames>Michael</forenames></author><author><keyname>Bouvel</keyname><forenames>Mathilde</forenames></author></authors><title>Operators of equivalent sorting power and related Wilf-equivalences</title><categories>math.CO cs.DM</categories><comments>18 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study sorting operators $\mathbf{A}$ on permutations that are obtained
composing Knuth's stack sorting operator $\mathbf{S}$ and the reversal operator
$\mathbf{R}$, as many times as desired. For any such operator $\mathbf{A}$, we
provide a size-preserving bijection between the set of permutations sorted by
$\mathbf{S} \circ \mathbf{A}$ and the set of those sorted by $\mathbf{S} \circ
\mathbf{R} \circ \mathbf{A}$, proving that these sets are enumerated by the
same sequence, but also that many classical permutation statistics are
equidistributed across these two sets. The description of this family of
bijections is based on a bijection between the set of permutations avoiding the
pattern $231$ and the set of those avoiding $132$ which preserves many
permutation statistics. We also present other properties of this bijection, in
particular for finding pairs of Wilf-equivalent permutation classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2042</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2042</id><created>2014-02-10</created><updated>2014-07-28</updated><authors><author><keyname>Jeong</keyname><forenames>Cheol</forenames></author><author><keyname>Shin</keyname><forenames>Won-Yong</forenames></author></authors><title>Ad Hoc Networking With Cost-Effective Infrastructure: Generalized
  Capacity Scaling</title><categories>cs.IT math.IT</categories><comments>27 pages, 11 figures, Submitted to IEEE/ACM Transactions on
  Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Capacity scaling of a large hybrid network with unit node density, consisting
of $n$ wireless ad hoc nodes, base stations (BSs) equipped with multiple
antennas, and one remote central processor (RCP), is analyzed when wired
backhaul links between the BSs and the RCP are rate-limited. We deal with a
general scenario where the number of BSs, the number of antennas at each BS,
and the backhaul link rate can scale at arbitrary rates relative to $n$ (i.e.,
we introduce three scaling parameters). We first derive the minimum backhaul
link rate required to achieve the same capacity scaling law as in the
infinite-capacity backhaul link case. Assuming an arbitrary rate scaling of
each backhaul link, a generalized achievable throughput scaling law is then
analyzed in the network based on using one of pure multihop, hierarchical
cooperation, and two infrastructure-supported routing protocols, and moreover,
three-dimensional information-theoretic operating regimes are explicitly
identified according to the three scaling parameters. In particular, we show
the case where our network having a power limitation is also fundamentally in
the degrees-of-freedom- or infrastructure-limited regime, or both. In addition,
a generalized cut-set upper bound under the network model is derived by cutting
not only the wireless connections but also the wired connections. It is shown
that our upper bound matches the achievable throughput scaling even under
realistic network conditions such that each backhaul link rate scales slower
than the aforementioned minimum-required backhaul link rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2043</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2043</id><created>2014-02-10</created><authors><author><keyname>Mannor</keyname><forenames>Shie</forenames><affiliation>EE-Technion</affiliation></author><author><keyname>Perchet</keyname><forenames>Vianney</forenames><affiliation>LPMA</affiliation></author><author><keyname>Stoltz</keyname><forenames>Gilles</forenames><affiliation>GREGH</affiliation></author></authors><title>Approachability in unknown games: Online learning meets multi-objective
  optimization</title><categories>stat.ML cs.LG math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the standard setting of approachability there are two players and a target
set. The players play a repeated vector-valued game where one of them wants to
have the average vector-valued payoff converge to the target set which the
other player tries to exclude. We revisit the classical setting and consider
the setting where the player has a preference relation between target sets: she
wishes to approach the smallest (&quot;best&quot;) set possible given the observed
average payoffs in hindsight. Moreover, as opposed to previous works on
approachability, and in the spirit of online learning, we do not assume that
there is a known game structure with actions for two players. Rather, the
player receives an arbitrary vector-valued reward vector at every round. We
show that it is impossible, in general, to approach the best target set in
hindsight. We further propose a concrete strategy that approaches a non-trivial
relaxation of the best-in-hindsight given the actual rewards. Our approach does
not require projection onto a target set and amounts to switching between
scalar regret minimization algorithms that are performed in episodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2044</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2044</id><created>2014-02-10</created><authors><author><keyname>Gaillard</keyname><forenames>Pierre</forenames><affiliation>GREGH</affiliation></author><author><keyname>Stoltz</keyname><forenames>Gilles</forenames><affiliation>GREGH</affiliation></author><author><keyname>Van Erven</keyname><forenames>Tim</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author></authors><title>A Second-order Bound with Excess Losses</title><categories>stat.ML cs.LG math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study online aggregation of the predictions of experts, and first show new
second-order regret bounds in the standard setting, which are obtained via a
version of the Prod algorithm (and also a version of the polynomially weighted
average algorithm) with multiple learning rates. These bounds are in terms of
excess losses, the differences between the instantaneous losses suffered by the
algorithm and the ones of a given expert. We then demonstrate the interest of
these bounds in the context of experts that report their confidences as a
number in the interval [0,1] using a generic reduction to the standard setting.
We conclude by two other applications in the standard setting, which improve
the known bounds in case of small excess losses and show a bounded regret
against i.i.d. sequences of losses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2056</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2056</id><created>2014-02-10</created><authors><author><keyname>Feng</keyname><forenames>Tao</forenames></author><author><keyname>Wu</keyname><forenames>Qianqian</forenames></author><author><keyname>Zhang</keyname><forenames>Haipeng</forenames></author></authors><title>Key parameters generation of the navigation data of GPS Simulator</title><categories>cs.IT math.IT</categories><comments>9 pages, 5 figures, 28 equations, 12 references</comments><journal-ref>International Journal of Soft Computing And Software Engineering
  (JSCSE), Vol.2,No.10, 2012,Published online: Oct 25, 2012</journal-ref><doi>10.7321/jscse.v2.n10.2</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The development of the GPS (Global Positioning System) signal simulator
involving to a number of key technologies, in which the generation of
navigation message has important significance. Based on analysis of the
structure of GPS navigation data, the paper researches the production of
telemetry word and handover word, parity check code, time parameters and star
clock. Using disturbing force equation and Lagrange planetary motion equation
extrapolate ephemeris parameters whose feasibility is verified through the
Matlab software finally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2058</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2058</id><created>2014-02-10</created><updated>2014-10-15</updated><authors><author><keyname>Hennig</keyname><forenames>Philipp</forenames></author></authors><title>Probabilistic Interpretation of Linear Solvers</title><categories>math.OC cs.LG cs.NA math.NA math.PR stat.ML</categories><comments>final version, in press at SIAM J Optimization</comments><msc-class>90C53, 65F10,</msc-class><acm-class>F.2.1; G.1.2; G.1.3; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manuscript proposes a probabilistic framework for algorithms that
iteratively solve unconstrained linear problems $Bx = b$ with positive definite
$B$ for $x$. The goal is to replace the point estimates returned by existing
methods with a Gaussian posterior belief over the elements of the inverse of
$B$, which can be used to estimate errors. Recent probabilistic interpretations
of the secant family of quasi-Newton optimization algorithms are extended.
Combined with properties of the conjugate gradient algorithm, this leads to
uncertainty-calibrated methods with very limited cost overhead over conjugate
gradients, a self-contained novel interpretation of the quasi-Newton and
conjugate gradient algorithms, and a foundation for new nonlinear optimization
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2071</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2071</id><created>2014-02-10</created><authors><author><keyname>Belohlavek</keyname><forenames>Radim</forenames></author><author><keyname>Vychodil</keyname><forenames>Vilem</forenames></author></authors><title>Attribute Dependencies for Data with Grades</title><categories>cs.LO cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines attribute dependencies in data that involve grades, such
as a grade to which an object is red or a grade to which two objects are
similar. We thus extend the classical agenda by allowing graded, or fuzzy,
attributes instead of Boolean attributes in case of attribute implications, and
allowing approximate match based on degrees of similarity instead of exact
match in case of functional dependencies. In a sense, we move from bivalence,
inherently present in the now-available theories of dependencies, to a more
flexible setting that involves grades. Such a shift has far-reaching
consequences. We argue that a reasonable theory of dependencies may be
developed by making use of mathematical fuzzy logic. Namely, the theory of
dependencies is then based on a solid logic calculus the same way the classical
dependencies are based on classical logic. For instance, rather than handling
degrees of similarity in an ad hoc manner, we consistently treat them as truth
values, the same way as true (match) and false (mismatch) are treated in
classical theories. In addition, several notions intuitively embraced in the
presence of grades, such as a degree of validity of a particular dependence or
a degree of entailment, naturally emerge and receive a conceptually clean
treatment in the presented approach. In the paper, we discuss motivations,
provide basic notions of syntax and semantics, and develop basic results which
include entailment of dependencies, associated closure structures, a logic of
dependencies with two versions of completeness theorem, results and algorithms
regarding complete non-redundant sets of dependencies, relationship to and a
possible reductionist interface to classical dependencies, and relationship to
functional dependencies over domains with similarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2073</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2073</id><created>2014-02-10</created><authors><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author><author><keyname>Nagy</keyname><forenames>Mate Levente</forenames></author><author><keyname>Luong</keyname><forenames>ThaiBinh</forenames></author><author><keyname>Krauthammer</keyname><forenames>Michael</forenames></author></authors><title>Mining Images in Biomedical Publications: Detection and Analysis of Gel
  Diagrams</title><categories>cs.IR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1209.1481</comments><journal-ref>Journal of Biomedical Semantics 2014, 5:10</journal-ref><doi>10.1186/2041-1480-5-10</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Authors of biomedical publications use gel images to report experimental
results such as protein-protein interactions or protein expressions under
different conditions. Gel images offer a concise way to communicate such
findings, not all of which need to be explicitly discussed in the article text.
This fact together with the abundance of gel images and their shared common
patterns makes them prime candidates for automated image mining and parsing. We
introduce an approach for the detection of gel images, and present a workflow
to analyze them. We are able to detect gel segments and panels at high
accuracy, and present preliminary results for the identification of gene names
in these images. While we cannot provide a complete solution at this point, we
present evidence that this kind of image mining is feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2079</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2079</id><created>2014-02-10</created><authors><author><keyname>Coleman</keyname><forenames>Martin A.</forenames></author></authors><title>Freedom From Restriction, Freedom Of A Restriction: A Comparison Of Some
  Open Source Software Licenses</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a multitude of licenses out there for a software developer to choose
from, but a lot of programmers would prefer to not have to have a legal degree
in order to understand them and would rather just have their code out in the
public being used. This paper examines a few of the more popular and common
software licenses available and compares their conditions from the perspective
that a developer wanted to scratch their proverbial itch, have their code be
used and maybe even get credit for it. This paper attempts to clarify which
license a developer would benefit from the most if they just want to have their
code used, have their name out there and not be sued if anything breaks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2086</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2086</id><created>2014-02-10</created><authors><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Guaranteed Non-quadratic Performance for Quantum Systems with Nonlinear
  Uncertainties</title><categories>quant-ph cs.SY math.OC</categories><comments>A version of this paper is to appear in the Proceedings of the 2014
  American Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a robust performance analysis result for a class of
uncertain quantum systems containing sector bounded nonlinearities arising from
perturbations to the system Hamiltonian. An LMI condition is given for
calculating a guaranteed upper bound on a non-quadratic cost function. This
result is illustrated with an example involving a Josephson junction in an
electromagnetic cavity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2088</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2088</id><created>2014-02-10</created><authors><author><keyname>Tofighi</keyname><forenames>Mohammad</forenames></author><author><keyname>Kose</keyname><forenames>Kivanc</forenames></author><author><keyname>Cetin</keyname><forenames>A. Enis</forenames></author></authors><title>Signal Reconstruction Framework Based On Projections Onto Epigraph Set
  Of A Convex Cost Function (PESC)</title><categories>math.OC cs.CV</categories><comments>Submitted to IEEE Transactions on Image Processing on 7th Jan 2014.
  arXiv admin note: substantial text overlap with arXiv:1309.0700,
  arXiv:1306.2516</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new signal processing framework based on making orthogonal Projections onto
the Epigraph Set of a Convex cost function (PESC) is developed. In this way it
is possible to solve convex optimization problems using the well-known
Projections onto Convex Set (POCS) approach. In this algorithm, the dimension
of the minimization problem is lifted by one and a convex set corresponding to
the epigraph of the cost function is defined. If the cost function is a convex
function in $R^N$, the corresponding epigraph set is also a convex set in
R^{N+1}. The PESC method provides globally optimal solutions for
total-variation (TV), filtered variation (FV), L_1, L_2, and entropic cost
function based convex optimization problems. In this article, the PESC based
denoising and compressive sensing algorithms are developed. Simulation examples
are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2090</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2090</id><created>2014-02-10</created><authors><author><keyname>Skowron</keyname><forenames>Piotr</forenames></author><author><keyname>Rzadca</keyname><forenames>Krzysztof</forenames></author></authors><title>We Are Impatient: Algorithms for Geographically Distributed Load
  Balancing with (Almost) Arbitrary Load Functions</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In geographically-distributed systems, communication latencies are
non-negligible. The perceived processing time of a request is thus composed of
the time needed to route the request to the server and the true processing
time. Once a request reaches a target server, the processing time depends on
the total load of that server; this dependency is described by a load function.
We consider a broad class of load functions; we just require that they are
convex and two times differentiable. In particular our model can be applied to
heterogeneous systems in which every server has a different load function. This
approach allows us not only to generalize results for queuing theory and for
batches of requests, but also to use empirically-derived load functions,
measured in a system under stress-testing. The optimal assignment of requests
to servers is communication-balanced, i.e. for any pair of non
perfectly-balanced servers, the reduction of processing time resulting from
moving a single request from the overloaded to underloaded server is smaller
than the additional communication latency. We present a centralized and a
decentralized algorithm for optimal load balancing. We prove bounds on the
algorithms' convergence. To the best of our knowledge these bounds were not
known even for the special cases studied previously (queuing theory and batches
of requests). Both algorithms are any-time algorithms. In the decentralized
algorithm, each server balances the load with a randomly chosen peer. Such
algorithm is very robust to failures. We prove that the decentralized algorithm
performs locally-optimal steps. Our work extends the currently known results by
considering a broad class of load functions and by establishing theoretical
bounds on the algorithms' convergence. These results are applicable for servers
whose characteristics under load cannot be described by a standard mathematical
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2091</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2091</id><created>2014-02-10</created><authors><author><keyname>Liu</keyname><forenames>Shuiyin</forenames></author><author><keyname>Hong</keyname><forenames>Yi</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Artificial Noise Revisited</title><categories>cs.IT math.IT</categories><comments>4 figures, submitted to IEEE Trans on Wireless Commun</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The artificial noise (AN) scheme, proposed by Goel and Negi, is being
considered as one of the key enabling technology for secure communications over
multiple-input multiple-output (MIMO) wiretap channels. However, the decrease
in secrecy rate due to the increase in the number of Eve's antennas is not well
understood. In this paper, we develop an analytical framework to characterize
the secrecy rate of the AN scheme as a function of Eve's signal-to-noise ratio
(SNR), Bob's SNR, the number of antennas in each terminal, and the power
allocation scheme. We first derive a closed-form expression for the average
secrecy rate. We then derive a closed-form expression for the asymptotic
instantaneous secrecy rate with large number of antennas at all terminals.
Finally, we derive simple lower and upper bounds on the average and
instantaneous secrecy rate that provide a tool for the system design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2092</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2092</id><created>2014-02-10</created><updated>2014-03-07</updated><authors><author><keyname>Singla</keyname><forenames>Adish</forenames></author><author><keyname>Bogunovic</keyname><forenames>Ilija</forenames></author><author><keyname>Bart&#xf3;k</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Karbasi</keyname><forenames>Amin</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author></authors><title>Near-Optimally Teaching the Crowd to Classify</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How should we present training examples to learners to teach them
classification rules? This is a natural problem when training workers for
crowdsourcing labeling tasks, and is also motivated by challenges in
data-driven online education. We propose a natural stochastic model of the
learners, modeling them as randomly switching among hypotheses based on
observed feedback. We then develop STRICT, an efficient algorithm for selecting
examples to teach to workers. Our solution greedily maximizes a submodular
surrogate objective function in order to select examples to show to the
learners. We prove that our strategy is competitive with the optimal teaching
policy. Moreover, for the special case of linear separators, we prove that an
exponential reduction in error probability can be achieved. Our experiments on
simulated workers as well as three real image annotation tasks on Amazon
Mechanical Turk show the effectiveness of our teaching algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2097</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2097</id><created>2014-02-10</created><authors><author><keyname>Benson</keyname><forenames>Gary</forenames></author><author><keyname>Levy</keyname><forenames>Avivit</forenames></author><author><keyname>Shalom</keyname><forenames>Riva</forenames></author></authors><title>Longest Common Subsequence in k-length substrings</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we define a new problem, motivated by computational biology,
$LCSk$ aiming at finding the maximal number of $k$ length $substrings$,
matching in both input strings while preserving their order of appearance. The
traditional LCS definition is a special case of our problem, where $k = 1$. We
provide an algorithm, solving the general case in $O(n^2)$ time, where $n$ is
the length of the input strings, equaling the time required for the special
case of $k=1$. The space requirement of the algorithm is $O(kn)$. %, however,
in order to enable %backtracking of the solution, $O(n^2)$ space is needed.
  We also define a complementary $EDk$ distance measure and show that
$EDk(A,B)$ can be computed in $O(nm)$ time and $O(km)$ space, where $m$, $n$
are the lengths of the input sequences $A$ and $B$ respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2102</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2102</id><created>2014-02-10</created><authors><author><keyname>Konecny</keyname><forenames>Filip</forenames></author></authors><title>PTIME Computation of Transitive Closures of Octagonal Relations</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing transitive closures of integer relations is the key to finding
precise invariants of integer programs. In this paper, we study difference
bounds and octagonal relations and prove that their transitive closure is a
PTIME-computable formula in the existential fragment of Presburger arithmetic.
This result marks a significant complexity improvement, as the known algorithms
have EXPTIME worst case complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2107</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2107</id><created>2014-02-10</created><authors><author><keyname>Pastorelli</keyname><forenames>Mario</forenames></author><author><keyname>Dell'Amico</keyname><forenames>Matteo</forenames></author><author><keyname>Michiardi</keyname><forenames>Pietro</forenames></author></authors><title>OS-Assisted Task Preemption for Hadoop</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces a new task preemption primitive for Hadoop, that allows
tasks to be suspended and resumed exploiting existing memory management
mechanisms readily available in modern operating systems. Our technique fills
the gap that exists between the two extremes cases of killing tasks (which
waste work) or waiting for their completion (which introduces latency):
experimental results indicate superior performance and very small overheads
when compared to existing alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2108</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2108</id><created>2014-02-10</created><authors><author><keyname>Alnumay</keyname><forenames>Waleed S.</forenames></author><author><keyname>Ghosh</keyname><forenames>Uttam</forenames></author></authors><title>Secure Routing and Data Transmission in Mobile Ad Hoc Networks</title><categories>cs.NI cs.CR</categories><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.6, No.1, January 2014</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we present an identity (ID) based protocol that secures AODV
and TCP so that it can be used in dynamic and attack prone environments of
mobile ad hoc networks. The proposed protocol protects AODV using Sequential
Aggregate Signatures (SAS) based on RSA. It also generates a session key for
each pair of source-destination nodes of a MANET for securing the end-to-end
transmitted data. Here each node has an ID which is evaluated from its public
key and the messages that are sent are authenticated with a signature/ MAC. The
proposed scheme does not allow a node to change its ID throughout the network
lifetime. Thus it makes the network secure against attacks that target AODV and
TCP in MANET. We present performance analysis to validate our claim.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2110</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2110</id><created>2014-02-10</created><authors><author><keyname>Pai</keyname><forenames>Smitha N.</forenames></author><author><keyname>Shet</keyname><forenames>K. C.</forenames></author><author><keyname>Mruthyunjaya</keyname><forenames>H. S</forenames></author></authors><title>In-network Aggregation using Efficient Routing Techniques for Event
  Driven Sensor Network</title><categories>cs.NI</categories><comments>20 pages 16 figures, 7 tables</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.6, No.1, January 2014</journal-ref><doi>10.5121/ijcnc.2014.6110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensors used in applications such as agriculture, weather, etc., monitoring
physical parameters like soil moisture, temperature, humidity, will have to
sustain their battery power for long intervals of time. In order to accomplish
this, parameter which assists in reducing the consumption of power from battery
need to be attended to. One of the factors affecting the consumption of energy
is transmit and receive power. This energy consumption can be reduced by
avoiding unnecessary transmission and reception. Efficient routing techniques
and incorporating aggregation whenever possible can save considerable amount of
energy. Aggregation reduces repeated transmission of relative values and also
reduces lot of computation at the base station. In this paper, the benefits of
aggregation over direct transmission in saving the amount of energy consumed is
discussed. Routing techniques which assist aggregation are incorporated.
Aspects like transmission of average value of sensed data around an area of the
network, minimum value in the whole of the network, triggering of event when
there is low battery are assimilated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2114</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2114</id><created>2014-02-10</created><authors><author><keyname>Kumar</keyname><forenames>Shiu</forenames></author></authors><title>Ubiquitous Smart Home System Using Android Application</title><categories>cs.CY cs.SY</categories><comments>11 pages, 10 figures</comments><msc-class>68N19</msc-class><journal-ref>International Journal of Computer Networks &amp; Communications, vol.
  6(1), pp. 33-43, 2014</journal-ref><doi>10.5121/ijcnc.2014.6103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a flexible standalone, low-cost smart home system, which
is based on the Android app communicating with the micro-web server providing
more than the switching functionalities. The Arduino Ethernet is used to
eliminate the use of a personal computer (PC) keeping the cost of the overall
system to a minimum while voice activation is incorporated for switching
functionalities. Devices such as light switches, power plugs, temperature
sensors, humidity sensors, current sensors, intrusion detection sensors,
smoke/gas sensors and sirens have been integrated in the system to demonstrate
the feasibility and effectiveness of the proposed smart home system. The smart
home app is tested and it is able to successfully perform the smart home
operations such as switching functionalities, automatic environmental control
and intrusion detection, in the later case where an email is generated and the
siren goes on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2127</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2127</id><created>2014-02-10</created><updated>2014-02-11</updated><authors><author><keyname>Iosif</keyname><forenames>Radu</forenames></author><author><keyname>Rogalewicz</keyname><forenames>Adam</forenames></author><author><keyname>Vojnar</keyname><forenames>Tomas</forenames></author></authors><title>Deciding Entailments in Inductive Separation Logic with Tree Automata</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separation Logic (SL) with inductive definitions is a natural formalism for
specifying complex recursive data structures, used in compositional
verification of programs manipulating such structures. The key ingredient of
any automated verification procedure based on SL is the decidability of the
entailment problem. In this work, we reduce the entailment problem for a
non-trivial subset of SL describing trees (and beyond) to the language
inclusion of tree automata (TA). Our reduction provides tight complexity bounds
for the problem and shows that entailment in our fragment is EXPTIME-complete.
For practical purposes, we leverage from recent advances in automata theory,
such as inclusion checking for non-deterministic TA avoiding explicit
determinization. We implemented our method and present promising preliminary
experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2135</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2135</id><created>2014-02-10</created><updated>2015-05-10</updated><authors><author><keyname>Bonomo</keyname><forenames>Flavia</forenames></author><author><keyname>Grippo</keyname><forenames>Luciano N.</forenames></author><author><keyname>Milani&#x10d;</keyname><forenames>Martin</forenames></author><author><keyname>Safe</keyname><forenames>Mart&#xed;n D.</forenames></author></authors><title>Graph classes with and without powers of bounded clique-width</title><categories>math.CO cs.DM</categories><comments>23 pages, 4 figures</comments><journal-ref>Discrete Applied Mathematics 199 (2016): 3-15</journal-ref><doi>10.1016/j.dam.2015.06.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the study of graph classes of power-bounded clique-width, that
is, graph classes for which there exist integers $k$ and $\ell$ such that the
$k$-th powers of the graphs are of clique-width at most $\ell$. We give
sufficient and necessary conditions for this property. As our main results, we
characterize graph classes of power-bounded clique-width within classes defined
by either one forbidden induced subgraph, or by two connected forbidden induced
subgraphs. We also show that for every positive integer $k$, there exists a
graph class such that the $k$-th powers of graphs in the class form a class of
bounded clique-width, while this is not the case for any smaller power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2136</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2136</id><created>2014-02-10</created><updated>2014-05-01</updated><authors><author><keyname>van Iersel</keyname><forenames>Leo</forenames></author><author><keyname>Kelk</keyname><forenames>Steven</forenames></author><author><keyname>Leki&#x107;</keyname><forenames>Nela</forenames></author><author><keyname>Whidden</keyname><forenames>Chris</forenames></author><author><keyname>Zeh</keyname><forenames>Norbert</forenames></author></authors><title>Hybridization Number on Three Trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phylogenetic networks are leaf-labelled directed acyclic graphs that are used
to describe non-treelike evolutionary histories and are thus a generalization
of phylogenetic trees. The hybridization number of a phylogenetic network is
the sum of all indegrees minus the number of nodes plus one. The Hybridization
Number problem takes as input a collection of phylogenetic trees and asks to
construct a phylogenetic network that contains an embedding of each of the
input trees and has a smallest possible hybridization number. We present an
algorithm for the Hybridization Number problem on three binary trees on $n$
leaves, which runs in time $O(c^k poly(n))$, with $k$ the hybridization number
of an optimal network and $c$ a constant. For two trees, an algorithm with
running time $O(3.18^k n)$ was proposed before whereas an algorithm with
running time $O(c^k poly(n))$ for more than two trees had prior to this article
remained elusive. The algorithm for two trees uses the close connection to
acyclic agreement forests to achieve a linear exponent in the running time,
while previous algorithms for more than two trees (explicitly or implicitly)
relied on a brute force search through all possible underlying network
topologies, leading to running times that are not $O(c^k poly(n))$ for any $c$.
The connection to acyclic agreement forests is much weaker for more than two
trees, so even given the right agreement forest, reconstructing the network
poses major challenges. We prove novel structural results that allow us to
reconstruct a network without having to guess the underlying topology. Our
techniques generalize to more than three input trees with the exception of one
key lemma that maps nodes in the network to tree nodes and, thus, minimizes the
amount of guessing involved in constructing the network. The main open problem
therefore is to establish a similar mapping for more than three trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2137</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2137</id><created>2014-02-10</created><authors><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Jones</keyname><forenames>Mark</forenames></author><author><keyname>Sheng</keyname><forenames>Bin</forenames></author><author><keyname>Wahlstrom</keyname><forenames>Magnus</forenames></author></authors><title>Parameterized Directed $k$-Chinese Postman Problem and $k$ Arc-Disjoint
  Cycles Problem on Euler Digraphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Directed $k$-Chinese Postman Problem ($k$-DCPP), we are given a
connected weighted digraph $G$ and asked to find $k$ non-empty closed directed
walks covering all arcs of $G$ such that the total weight of the walks is
minimum. Gutin, Muciaccia and Yeo (Theor. Comput. Sci. 513 (2013) 124--128)
asked for the parameterized complexity of $k$-DCPP when $k$ is the parameter.
We prove that the $k$-DCPP is fixed-parameter tractable.
  We also consider a related problem of finding $k$ arc-disjoint directed
cycles in an Euler digraph, parameterized by $k$. Slivkins (ESA 2003) showed
that this problem is W[1]-hard for general digraphs. Generalizing another
result by Slivkins, we prove that the problem is fixed-parameter tractable for
Euler digraphs. The corresponding problem on vertex-disjoint cycles in Euler
digraphs remains W[1]-hard even for Euler digraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2140</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2140</id><created>2014-02-10</created><authors><author><keyname>Kavalci</keyname><forenames>Vedat</forenames></author><author><keyname>Ural</keyname><forenames>Aybars</forenames></author><author><keyname>Dagdeviren</keyname><forenames>Orhan</forenames></author></authors><title>Distributed Vertex Cover Algorithms For Wireless Sensor Networks</title><categories>cs.DC cs.NI</categories><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.6, No.1, January 2014</journal-ref><doi>10.5121/ijcnc.2014.6107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vertex covering has important applications for wireless sensor networks such
as monitoring link failures, facility location, clustering, and data
aggregation. In this study, we designed three algorithms for constructing
vertex cover in wireless sensor networks. The first algorithm, which is an
adaption of the Parnas &amp; Ron's algorithm, is a greedy approach that finds a
vertex cover by using the degrees of the nodes. The second algorithm finds a
vertex cover from graph matching where Hoepman's weighted matching algorithm is
used. The third algorithm firstly forms a breadth-first search tree and then
constructs a vertex cover by selecting nodes with predefined levels from
breadth-first tree. We show the operation of the designed algorithms, analyze
them, and provide the simulation results in the TOSSIM environment. Finally we
have implemented, compared and assessed all these approaches. The transmitted
message count of the first algorithm is smallest among other algorithms where
the third algorithm has turned out to be presenting the best results in vertex
cover approximation ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2143</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2143</id><created>2014-02-10</created><updated>2014-06-10</updated><authors><author><keyname>Fahrenberg</keyname><forenames>Uli</forenames></author><author><keyname>Legay</keyname><forenames>Axel</forenames></author><author><keyname>Traonouez</keyname><forenames>Louis-Marie</forenames></author></authors><title>Structural Refinement for the Modal nu-Calculus</title><categories>cs.LO</categories><comments>Accepted at ICTAC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new notion of structural refinement, a sound abstraction of
logical implication, for the modal nu-calculus. Using new translations between
the modal nu-calculus and disjunctive modal transition systems, we show that
these two specification formalisms are structurally equivalent.
  Using our translations, we also transfer the structural operations of
composition and quotient from disjunctive modal transition systems to the modal
nu-calculus. This shows that the modal nu-calculus supports composition and
decomposition of specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2144</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2144</id><created>2014-02-10</created><authors><author><keyname>Abufouda</keyname><forenames>Mohammed</forenames></author></authors><title>A Framework for Enhancing Performance and Handling Run-Time Uncertainty
  in Self-Adaptive Systems</title><categories>cs.SE</categories><journal-ref>International Journal of Software Engineering &amp; Applications
  (IJSEA), Vol.5, No.1, January 2014</journal-ref><doi>10.5121/ijsea.2014.5106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-adaptivity allows software systems to autonomously adjust their behavior
during run-time to reduce the cost complexities caused by manual maintenance.
In this paper, a framework for building an external adaptation engine for
self-adaptive software systems is proposed. In order to improve the quality of
self-adaptive software systems, this research addresses two challenges in
self-adaptive software systems. The first challenge is to provide better
performance of the adaptation engine by managing the complexity of the
adaptation space efficiently and the second challenge is handling run-time
uncertainty that hinders the adaptation process. This research utilizes
Case-based Reasoning as an adaptation engine along with utility functions for
realizing the managed system's requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2145</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2145</id><created>2014-02-10</created><updated>2014-02-13</updated><authors><author><keyname>Rastin</keyname><forenames>Niloofar</forenames></author><author><keyname>Jahromi</keyname><forenames>Mansoor Zolghadri</forenames></author></authors><title>Using content features to enhance performance of user-based
  collaborative filtering performance of user-based collaborative filtering</title><categories>cs.IR</categories><comments>10 pages, journal</comments><journal-ref>International journal of artificial intelligence and applications,
  vol. 5, no. 1, pp. 53-62, January 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content-based and collaborative filtering methods are the most successful
solutions in recommender systems. Content based method is based on items
attributes. This method checks the features of users favourite items and then
proposes the items which have the most similar characteristics with those
items. Collaborative filtering method is based on the determination of similar
items or similar users, which are called item-based and user-based
collaborative filtering, respectively.In this paper we propose a hybrid method
that integrates collaborative filtering and content-based methods. The proposed
method can be viewed as user-based Collaborative filtering technique. However
to find users with similar taste with active user, we used content features of
the item under investigation to put more emphasis on users rating for similar
items. In other words two users are similar if their ratings are similar on
items that have similar context. This is achieved by assigning a weight to each
rating when calculating the similarity of two users.We used movielens data set
to access the performance of the proposed method in comparison with basic
user-based collaborative filtering and other popular methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2149</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2149</id><created>2014-02-10</created><authors><author><keyname>Khayut</keyname><forenames>Ben</forenames></author><author><keyname>Fabri</keyname><forenames>Lina</forenames></author><author><keyname>Abukhana</keyname><forenames>Maya</forenames></author></authors><title>Intelligent User Interface in Fuzzy Environment</title><categories>cs.HC</categories><comments>15 pages, 4 figures</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol. 5, No. 1, January 2014, pp. 63-78</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human-Computer Interaction with the traditional User Interface is done using
a specified in advance script dialog menu, mainly based on human intellect and
unproductive use of navigation. This approach does not lead to making
qualitative decision in control systems, where the situations and processes
cannot be structured in advance. Any dynamic changes in the controlled business
process (as example, in organizational unit of the information fuzzy control
system) make it necessary to modify the script dialogue in User Interface. This
circumstance leads to a redesign of the components of the User Interface and of
the entire control system. In the Intelligent User Interface, where the dialog
situations are unknown in advance, fuzzy structured and artificial intelligence
is crucial, the redesign described above is impossible. To solve this and other
problems, we propose the data, information and knowledge based technology of
Smart/ Intelligent User Interface (IUI) design, which interacts with users and
systems in natural and other languages, utilizing the principles of Situational
Control and Fuzzy Logic theories, Artificial Intelligence, Linguistics,
Knowledge Base technologies and others. The proposed technology of IUI design
is defined by multi-agents of Situational Control and of data, information and
knowledge, modelling of Fuzzy Logic Inference, Generalization, Representation
and Explanation of knowledge, Planning and Decision-making, Dialog Control,
Reasoning and Systems Thinking, Fuzzy Control of organizational unit in
real-time, fuzzy conditions, heterogeneous domains, and multi-lingual
communication under uncertainty and in Fuzzy Environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2175</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2175</id><created>2014-02-10</created><authors><author><keyname>Yoshida</keyname><forenames>Yuichi</forenames></author></authors><title>A Characterization of Locally Testable Affine-Invariant Properties via
  Decomposition Theorems</title><categories>cs.CC</categories><comments>27 pages, appearing in STOC 2014. arXiv admin note: text overlap with
  arXiv:1306.0649, arXiv:1212.3849 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathcal{P}$ be a property of function $\mathbb{F}_p^n \to \{0,1\}$ for
a fixed prime $p$. An algorithm is called a tester for $\mathcal{P}$ if, given
a query access to the input function $f$, with high probability, it accepts
when $f$ satisfies $\mathcal{P}$ and rejects when $f$ is &quot;far&quot; from satisfying
$\mathcal{P}$. In this paper, we give a characterization of affine-invariant
properties that are (two-sided error) testable with a constant number of
queries. The characterization is stated in terms of decomposition theorems,
which roughly claim that any function can be decomposed into a structured part
that is a function of a constant number of polynomials, and a pseudo-random
part whose Gowers norm is small. We first give an algorithm that tests whether
the structured part of the input function has a specific form. Then we show
that an affine-invariant property is testable with a constant number of queries
if and only if it can be reduced to the problem of testing whether the
structured part of the input function is close to one of a constant number of
candidates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2184</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2184</id><created>2014-02-10</created><updated>2014-02-17</updated><authors><author><keyname>Konev</keyname><forenames>Boris</forenames></author><author><keyname>Lisitsa</keyname><forenames>Alexei</forenames></author></authors><title>A SAT Attack on the Erdos Discrepancy Conjecture</title><categories>cs.DM math.CO math.NT</categories><comments>8 pages. The description of the automata is clarified</comments><acm-class>F.2.2; I.2.3; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1930s Paul Erdos conjectured that for any positive integer C in any
infinite +1 -1 sequence (x_n) there exists a subsequence x_d, x_{2d}, ... ,
x_{kd} for some positive integers k and d, such that |x_d + x_{2d} + ... +
x_{kd}|&gt; C. The conjecture has been referred to as one of the major open
problems in combinatorial number theory and discrepancy theory. For the
particular case of C=1 a human proof of the conjecture exists; for C=2 a
bespoke computer program had generated sequences of length 1124 having
discrepancy 2, but the status of the conjecture remained open even for such a
small bound. We show that by encoding the problem into Boolean satisfiability
and applying the state of the art SAT solvers, one can obtain a sequence of
length 1160 with discrepancy 2 and a proof of the Erdos discrepancy conjecture
for C=2, claiming that no sequence of length 1161 and discrepancy 2 exists. We
also present our partial results for the case of C=3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2187</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2187</id><created>2014-02-10</created><authors><author><keyname>Rodrigues</keyname><forenames>Carlo Kleber Da Silva</forenames></author></authors><title>Analyzing Peer Selection Policies for BitTorrent Multimedia On-Demand
  Streaming Systems in Internet</title><categories>cs.NI cs.MM</categories><comments>19 PAGES</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.6, No.1, January 2014</journal-ref><doi>10.5121/ijcnc.2014.6114</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The adaptation of the BitTorrent protocol to multimedia on-demand streaming
systems essentially lies on the modification of its two core algorithms, namely
the piece and the peer selection policies, respectively. Much more attention
has though been given to the piece selection policy. Within this context, this
article proposes three novel peer selection policies for the design of
BitTorrent-like protocols targeted at that type of systems: Select Balanced
Neighbour Policy (SBNP), Select Regular Neighbour Policy (SRNP), and Select
Optimistic Neighbour Policy (SONP). These proposals are validated through a
competitive analysis based on simulations which encompass a variety of
multimedia scenarios, defined in function of important characterization
parameters such as content type, content size, and client interactivity
profile. Service time, number of clients served and efficiency retrieving
coefficient are the performance metrics assessed in the analysis. The final
results mainly show that the novel proposals constitute scalable solutions that
may be considered for real project designs. Lastly, future work is included in
the conclusion of this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2188</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2188</id><created>2014-02-10</created><authors><author><keyname>Chacko</keyname><forenames>Anitha Mary M. O.</forenames></author><author><keyname>Dhanya</keyname><forenames>P. M</forenames></author></authors><title>Handwritten Character Recognition In Malayalam Scripts- A Review</title><categories>cs.CV</categories><comments>11 pages,4 figures,2 tables</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol. 5, No. 1, January 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Handwritten character recognition is one of the most challenging and ongoing
areas of research in the field of pattern recognition. HCR research is matured
for foreign languages like Chinese and Japanese but the problem is much more
complex for Indian languages. The problem becomes even more complicated for
South Indian languages due to its large character set and the presence of
vowels modifiers and compound characters. This paper provides an overview of
important contributions and advances in offline as well as online handwritten
character recognition of Malayalam scripts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2190</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2190</id><created>2014-02-10</created><authors><author><keyname>El-Latif</keyname><forenames>Yasser M. Abd</forenames></author></authors><title>Surfaces Representation with Sharp Features Using Sqrt(3) and Loop
  Subdivision Schemes</title><categories>cs.CG cs.GR</categories><comments>14 pages, 9 figures</comments><acm-class>I.3.5; I.3.7</acm-class><journal-ref>IJCGA Vol.4, No.1, January 2014 International Journal of Computer
  Graphics &amp; Animation</journal-ref><doi>10.5121/ijcga.2014.4104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a hybrid algorithm that combines features form both
Sqrt(3) and Loop Subdivision schemes. The algorithm aims at preserving sharp
features and trim regions, during the surfaces subdivision, using a set of
rules. The implementation is nontrivial due to the computational, topological,
and smoothness constraints, which should be satisfied by the underlying
surface. The fundamental innovation, in this research work, is the ability to
preserve sharp features anywhere on a surface. In addition, the resulting
representation remains within the multiresolution subdivision framework.
Preserving the original representation has a core advantage that all the
applicable operations to the multiresolution subdivision surfaces can
subsequently be applied to the edited model. Experimental results, including
surfaces coarsening and smoothing, were performed using the proposed algorithm
for validation purposes, and the results revealed that the proposed algorithm
outperforms the other recent state of the art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2204</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2204</id><created>2014-02-10</created><authors><author><keyname>Saibharath</keyname><forenames>S.</forenames></author><author><keyname>Aarthi</keyname><forenames>J.</forenames></author></authors><title>Virtual Backbone Trees for Most Minimal Energy Consumption and
  Increasing Network Lifetime In WSNs</title><categories>cs.NI</categories><comments>10 pages, 4 figures, International Journal of Computer Networks &amp;
  Communications (IJCNC) Vol.6, No.1, January 2014</comments><msc-class>68M10</msc-class><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.6, No.1, January 2014</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Virtual backbone trees have been used for efficient communication between
sink node and any other node in the deployed area. But all the proposed virtual
backbone trees are not fully energy efficient and EVBTs have few flaws
associated with them. In this paper two such virtual backbones are proposed.
The motive behind the first algorithm, Most Minimal Energy Virtual Backbone
Tree (MMEVBT), is to minimise the energy consumption when packets are
transmitted between sink and a target sensor node. The energy consumption is
most minimal and optimal and it is shown why it always has minimal energy
consumption during any transfer of packet between every node with the sink
node. For every node, route path with most minimal energy consumption is
identified and a new tree node is elected only when a better minimal energy
consumption route is identified for a node to communicate with the sink and
vice versa. By moving sink periodically it is ensured the battery of the nodes
near sink is not completely drained out. Another backbone construction
algorithm is proposed which maximises the network lifetime by increasing the
lifetime of all tree nodes. Simulations are done in NS2 to practically test the
algorithms and the results are discussed in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2206</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2206</id><created>2014-02-10</created><authors><author><keyname>Nyagudi</keyname><forenames>Nyagudi Musandu</forenames></author></authors><title>Humanitarian Algorithms : A Codified Key Safety Switch Protocol for
  Lethal Autonomy</title><categories>cs.CR</categories><comments>14 pages, 11 references and 1 diagram</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the deployment of lethal autonomous weapons, there is the requirement
that any such platform complies with the precepts of International Humanitarian
Law. Humanitarian Algorithms[9: p. 9] ensure that lethal autonomous weapon
systems perform military/security operations, within the confines of
International Humanitarian Law. Unlike other existing techniques of regulating
lethal autonomy this scheme advocates for an approach that enables Machine
Learning. Lethal autonomous weapons must be equipped with appropriate fail-safe
mechanisms that locks them if they malfunction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2217</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2217</id><created>2014-02-10</created><authors><author><keyname>Aggarwal</keyname><forenames>Akshai</forenames></author><author><keyname>Gandhi</keyname><forenames>Savita</forenames></author><author><keyname>Chaubey</keyname><forenames>Nirbhay</forenames></author></authors><title>Performance Analysis of AODV, DSDV and DSR in MANETs</title><categories>cs.NI cs.CR</categories><comments>11 Pages, 8 Figure and 2 Tables</comments><doi>10.5121/ijdps.2011.2615</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Ad hoc Networks (MANETs) are considered as a new paradigm of
infrastructure-less mobile wireless communication systems. MANETs are being
widely studied and it is the technology that is attracting a large variety of
applications. Routing in MANETs is considered a challenging task due to the
unpredictable changes in the network topology, resulting from the random and
frequent movement of the nodes and due to the absence of any centralized
control [1][2]. In this paper, we evaluate the performance of reactive routing
protocols, Ad hoc On demand Distance Vector (AODV) and Dynamic Source Routing
(DSR) and proactive routing protocol Destination Sequenced Distance Vector
(DSDV). The major goal of this study is to analyze the performance of well
known MANETs routing protocol in high mobility case under low, medium and high
density scenario. Unlike military applications, most of the other applications
of MANETs require moderate to high mobility. Hence it becomes important to
study the impact of high mobility on the performance of these routing
protocols. The performance is analyzed with respect to Average End-to-End
Delay, Normalized Routing Load (NRL), Packet Delivery Fraction (PDF) and
Throughput. Simulation results verify that AODV gives better performance as
compared to DSR and DSDV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2224</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2224</id><created>2014-02-10</created><authors><author><keyname>Beimel</keyname><forenames>Amos</forenames></author><author><keyname>Nissim</keyname><forenames>Kobbi</forenames></author><author><keyname>Stemmer</keyname><forenames>Uri</forenames></author></authors><title>Characterizing the Sample Complexity of Private Learners</title><categories>cs.CR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2008, Kasiviswanathan et al. defined private learning as a combination of
PAC learning and differential privacy. Informally, a private learner is applied
to a collection of labeled individual information and outputs a hypothesis
while preserving the privacy of each individual. Kasiviswanathan et al. gave a
generic construction of private learners for (finite) concept classes, with
sample complexity logarithmic in the size of the concept class. This sample
complexity is higher than what is needed for non-private learners, hence
leaving open the possibility that the sample complexity of private learning may
be sometimes significantly higher than that of non-private learning.
  We give a combinatorial characterization of the sample size sufficient and
necessary to privately learn a class of concepts. This characterization is
analogous to the well known characterization of the sample complexity of
non-private learning in terms of the VC dimension of the concept class. We
introduce the notion of probabilistic representation of a concept class, and
our new complexity measure RepDim corresponds to the size of the smallest
probabilistic representation of the concept class.
  We show that any private learning algorithm for a concept class C with sample
complexity m implies RepDim(C)=O(m), and that there exists a private learning
algorithm with sample complexity m=O(RepDim(C)). We further demonstrate that a
similar characterization holds for the database size needed for privately
computing a large class of optimization problems and also for the well studied
problem of private data release.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2228</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2228</id><created>2014-02-06</created><authors><author><keyname>John</keyname><forenames>Wolfgang</forenames></author><author><keyname>Devlic</keyname><forenames>Alisa</forenames></author><author><keyname>Ding</keyname><forenames>Zhemin</forenames></author><author><keyname>Jocha</keyname><forenames>David</forenames></author><author><keyname>Kern</keyname><forenames>Andras</forenames></author><author><keyname>Kind</keyname><forenames>Mario</forenames></author><author><keyname>K&#xf6;psel</keyname><forenames>Andreas</forenames></author><author><keyname>Nordell</keyname><forenames>Viktor</forenames></author><author><keyname>Sharma</keyname><forenames>Sachin</forenames></author><author><keyname>Sk&#xf6;ldstr&#xf6;m</keyname><forenames>Pontus</forenames></author><author><keyname>Staessens</keyname><forenames>Dimitri</forenames></author><author><keyname>Takacs</keyname><forenames>Attila</forenames></author><author><keyname>Topp</keyname><forenames>Steffen</forenames></author><author><keyname>Westphal</keyname><forenames>F. -Joachim</forenames></author><author><keyname>Woesner</keyname><forenames>Hagen</forenames></author><author><keyname>Gladisch</keyname><forenames>Andreas</forenames></author></authors><title>Split Architecture for Large Scale Wide Area Networks</title><categories>cs.NI</categories><comments>This is the public deliverable D3.3 of the EU FP7 SPARC project
  (ICT-258457) - &quot;Split Architecture for Carrier Grade Networks&quot;. Original
  Deliverable published at http://www.fp7-sparc.eu/</comments><acm-class>C.2.1; C.2.2; C.2.3; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report defines a carrier-grade split architecture based on requirements
identified during the SPARC project. It presents the SplitArchitecture
proposal, the SPARC concept for Software Defined Networking (SDN) introduced
for large-scale wide area networks such as access/aggregation networks, and
evaluates technical issues against architectural trade-offs. First we present
the control and management architecture of the proposed SplitArchitecture.
Here, we discuss a recursive control architecture consisting of hierarchically
stacked control planes and provide initial considerations regarding network
management integration to SDN in general and SplitArchitecture in particular.
Next, OpenFlow extensions to support the carrier-grade SplitArchitecture are
discussed. These are: a) Openness &amp; Extensibility; b) Virtualization; c) OAM;
d) Resiliency approaches; e) Bootstrapping and topology discovery; f) Service
creation; g) Energy-efficient networking; h) QoS aspects; and i) Multilayer
aspects. In addition, we discuss selected deployment and adoption scenarios
faced by modern operator networks, such as service creation scenarios and
peering aspects, i.e., how to interconnect with legacy networks. Finally, we
indicate how our SplitArchitecture approach meets carrier grade scalability
requirements in access/aggregation network scenarios
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2230</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2230</id><created>2014-02-10</created><authors><author><keyname>Tripathi</keyname><forenames>Nikhita</forenames></author><author><keyname>Saxena</keyname><forenames>Nikhil</forenames></author><author><keyname>Soni</keyname><forenames>Sonal</forenames></author></authors><title>Design of an Amplifier through Second Generation Current Conveyor</title><categories>cs.ET</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the architecture of first and second generation current
conveyor (CCI and CCII respectively) and designing an amplifier using second
generation current conveyor. The designed amplifier through CCII+ can be used
in various analog computation circuits and is superior in performance than the
classical opamp. It provides better gain with higher accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2231</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2231</id><created>2014-02-10</created><authors><author><keyname>Laska</keyname><forenames>J. N.</forenames></author><author><keyname>Bradley</keyname><forenames>W. F.</forenames></author><author><keyname>Rondeau</keyname><forenames>T. W.</forenames></author><author><keyname>Nolan</keyname><forenames>K. E.</forenames></author><author><keyname>Vigoda</keyname><forenames>B.</forenames></author></authors><title>Compressive sensing for dynamic spectrum access networks: Techniques and
  tradeoffs</title><categories>cs.NI cs.IT math.IT</categories><comments>2011 IEEE Symposium on New Frontiers in Dynamic Spectrum, May 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the practical costs and benefits of CS for dynamic spectrum access
(DSA) networks. Firstly, we review several fast and practical techniques for
energy detection without full reconstruction and provide theoretical
guarantees. We also define practical metrics to measure the performance of
these techniques. Secondly, we perform comprehensive experiments comparing the
techniques on real signals captured over the air. Our results show that we can
significantly compressively acquire the signal while still accurately
determining spectral occupancy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2232</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2232</id><created>2014-02-10</created><authors><author><keyname>Rajakumar</keyname><forenames>V</forenames></author><author><keyname>Bopche</keyname><forenames>Vipeen V</forenames></author></authors><title>Image Search Reranking</title><categories>cs.IR cs.CV</categories><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  6(41):5-6, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existing methods for image search reranking suffer from the
unfaithfulness of the assumptions under which the text-based images search
result. The resulting images contain more irrelevant images. Hence the re
ranking concept arises to re rank the retrieved images based on the text around
the image and data of data of image and visual feature of image. A number of
methods are differentiated for this re-ranking. The high ranked images are used
as noisy data and a k means algorithm for classification is learned to rectify
the ranking further. We are study the affect ability of the cross validation
method to this training data. The pre eminent originality of the overall method
is in collecting text/metadata of image and visual features in order to achieve
an automatic ranking of the images. Supervision is initiated to learn the model
weights offline, previous to reranking process. While model learning needs
manual labeling of the results for a some limited queries, the resulting model
is query autonomous and therefore applicable to any other query .Examples are
given for a selection of other classes like vehicles, animals and other
classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2237</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2237</id><created>2014-02-10</created><updated>2014-10-30</updated><authors><author><keyname>Bailis</keyname><forenames>Peter</forenames></author><author><keyname>Fekete</keyname><forenames>Alan</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Ghodsi</keyname><forenames>Ali</forenames></author><author><keyname>Hellerstein</keyname><forenames>Joseph M.</forenames></author><author><keyname>Stoica</keyname><forenames>Ion</forenames></author></authors><title>Coordination Avoidance in Database Systems (Extended Version)</title><categories>cs.DB</categories><comments>Extended version of paper appearing in PVLDB Vol. 8, No. 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimizing coordination, or blocking communication between concurrently
executing operations, is key to maximizing scalability, availability, and high
performance in database systems. However, uninhibited coordination-free
execution can compromise application correctness, or consistency. When is
coordination necessary for correctness? The classic use of serializable
transactions is sufficient to maintain correctness but is not necessary for all
applications, sacrificing potential scalability. In this paper, we develop a
formal framework, invariant confluence, that determines whether an application
requires coordination for correct execution. By operating on application-level
invariants over database states (e.g., integrity constraints), invariant
confluence analysis provides a necessary and sufficient condition for safe,
coordination-free execution. When programmers specify their application
invariants, this analysis allows databases to coordinate only when anomalies
that might violate invariants are possible. We analyze the invariant confluence
of common invariants and operations from real-world database systems (i.e.,
integrity constraints) and applications and show that many are invariant
confluent and therefore achievable without coordination. We apply these results
to a proof-of-concept coordination-avoiding database prototype and demonstrate
sizable performance gains compared to serializable execution, notably a 25-fold
improvement over prior TPC-C New-Order performance on a 200 server cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2238</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2238</id><created>2014-02-10</created><updated>2014-05-03</updated><authors><author><keyname>Deshpande</keyname><forenames>Yash</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>Information-theoretically Optimal Sparse PCA</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>5 pages, 1 figure, conference</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Sparse Principal Component Analysis (PCA) is a dimensionality reduction
technique wherein one seeks a low-rank representation of a data matrix with
additional sparsity constraints on the obtained representation. We consider two
probabilistic formulations of sparse PCA: a spiked Wigner and spiked Wishart
(or spiked covariance) model. We analyze an Approximate Message Passing (AMP)
algorithm to estimate the underlying signal and show, in the high dimensional
limit, that the AMP estimates are information-theoretically optimal. As an
immediate corollary, our results demonstrate that the posterior expectation of
the underlying signal, which is often intractable to compute, can be obtained
using a polynomial-time scheme. Our results also effectively provide a
single-letter characterization of the sparse PCA problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2245</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2245</id><created>2014-02-10</created><updated>2014-02-12</updated><authors><author><keyname>Lombardi</keyname><forenames>Carlos</forenames></author><author><keyname>R&#xed;os</keyname><forenames>Alejandro</forenames></author><author><keyname>de Vrijer</keyname><forenames>Roel</forenames></author></authors><title>Proof terms for infinitary rewriting, progress report</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the notion of proof term to the realm of transfinite reduction.
Proof terms represent reductions in the first-order term format, thereby
facilitating their formal analysis. We show that any transfinite reduction can
be faithfully represented as an infinitary proof term, which is unique up to,
infinitary, associativity.
  Our main use of proof terms is in a definition of permutation equivalence for
transfinite reductions, on the basis of permutation equations. This definition
involves a variant of equational logic, adapted for dealing with infinite
objects.
  A proof of the compression property via proof terms is presented, which
establishes permutation equivalence between the original and the compressed
reductions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2255</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2255</id><created>2014-02-10</created><updated>2014-03-24</updated><authors><author><keyname>Mroueh</keyname><forenames>Youssef</forenames></author></authors><title>Robust Phase Retrieval and Super-Resolution from One Bit Coded
  Diffraction Patterns</title><categories>cs.IT math.IT math.ST stat.AP stat.TH</categories><comments>fixed notations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study a realistic setup for phase retrieval, where the
signal of interest is modulated or masked and then for each modulation or mask
a diffraction pattern is collected, producing a coded diffraction pattern (CDP)
[CLM13]. We are interested in the setup where the resolution of the collected
CDP is limited by the Fraunhofer diffraction limit of the imaging system.
  We investigate a novel approach based on a geometric quantization scheme of
phase-less linear measurements into (one-bit) coded diffraction patterns, and a
corresponding recovery scheme. The key novelty in this approach consists in
comparing pairs of coded diffractions patterns across frequencies: the one bit
measurements obtained rely on the order statistics of the un-quantized
measurements rather than their values . This results in a robust phase
recovery, and unlike currently available methods, allows to efficiently perform
phase recovery from measurements affected by severe (possibly unknown) non
linear, rank preserving perturbations, such as distortions. Another important
feature of this approach consists in the fact that it enables also
super-resolution and blind-deconvolution, beyond the diffraction limit of a
given imaging system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2266</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2266</id><created>2014-02-10</created><authors><author><keyname>Sedjelmaci</keyname><forenames>Sidi Mohamed</forenames><affiliation>LIPN</affiliation></author><author><keyname>Lavault</keyname><forenames>Christian</forenames><affiliation>LIPN</affiliation></author></authors><title>Improvements on the accelerated integer GCD algorithm</title><categories>cs.DC cs.DM math.NT</categories><comments>6 pages</comments><proxy>ccsd</proxy><journal-ref>Information Processing Letters 61, 1 (1997) 31--36</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present paper analyses and presents several improvements to the algorithm
for finding the $(a,b)$-pairs of integers used in the $k$-ary reduction of the
right-shift $k$-ary integer GCD algorithm. While the worst-case complexity of
Weber's &quot;Accelerated integer GCD algorithm&quot; is $\cO\l(\log_\phi(k)^2\r)$, we
show that the worst-case number of iterations of the while loop is exactly
$\tfrac 12 \l\lfloor \log_{\phi}(k)\r\rfloor$, where $\phi := \tfrac 12
\l(1+\sqrt{5}\r)$.\par We suggest improvements on the average complexity of the
latter algorithm and also present two new faster residual algorithms: the
sequential and the parallel one. A lower bound on the probability of avoiding
the while loop in our parallel residual algorithm is also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2269</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2269</id><created>2014-02-10</created><updated>2015-01-03</updated><authors><author><keyname>Franck</keyname><forenames>Christian</forenames></author><author><keyname>van de Graaf</keyname><forenames>Jeroen</forenames></author></authors><title>Dining Cryptographers are Practical</title><categories>cs.CR</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dining cryptographers protocol provides information-theoretically secure
sender and recipient untraceability. However, the protocol is considered to be
impractical because a malicious participant may disrupt the communication. We
propose an implementation which provides information-theoretical security for
senders and recipients, and in which a disruptor with limited computational
capabilities can easily be detected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2271</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2271</id><created>2014-02-10</created><authors><author><keyname>Rostami</keyname><forenames>Narges Hesami</forenames></author><author><keyname>Kheirkhah</keyname><forenames>Esmaeil</forenames></author><author><keyname>Jalali</keyname><forenames>Mehrdad</forenames></author></authors><title>An Optimized Semantic Web Service Composition Method Based on Clustering
  and Ant Colony Algorithm</title><categories>cs.SE</categories><comments>8 pages, 2 figure, International Journal of Web &amp; Semantic Technology
  (IJWesT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's Web, Web Services are created and updated on the fly. For
answering complex needs of users, the construction of new web services based on
existing ones is required. It has received a great attention from different
communities. This problem is known as web services composition. However, it is
one of big challenge problems of recent years in a distributed and dynamic
environment. Web services can be composed manually but it is a time consuming
task. The automatic web service composition is one of the key features for
future the semantic web. The various approaches in field of web service
compositions proposed by the researchers. In this paper, we propose a novel
architecture for semantic web service composition using clustering and Ant
colony algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2297</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2297</id><created>2014-02-10</created><authors><author><keyname>Varol</keyname><forenames>Onur</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author></authors><title>Connecting Dream Networks Across Cultures</title><categories>cs.SI physics.soc-ph</categories><comments>6 pages, 3 figures</comments><doi>10.1145/2567948.2579697</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many species dream, yet there remain many open research questions in the
study of dreams. The symbolism of dreams and their interpretation is present in
cultures throughout history. Analysis of online data sources for dream
interpretation using network science leads to understanding symbolism in dreams
and their associated meaning. In this study, we introduce dream interpretation
networks for English, Chinese and Arabic that represent different cultures from
various parts of the world. We analyze communities in these networks, finding
that symbols within a community are semantically related. The central nodes in
communities give insight about cultures and symbols in dreams. The community
structure of different networks highlights cultural similarities and
differences. Interconnections between different networks are also identified by
translating symbols from different languages into English. Structural
correlations across networks point out relationships between cultures.
Similarities between network communities are also investigated by analysis of
sentiment in symbol interpretations. We find that interpretations within a
community tend to have similar sentiment. Furthermore, we cluster communities
based on their sentiment, yielding three main categories of positive, negative,
and neutral dream symbols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2300</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2300</id><created>2014-02-10</created><authors><author><keyname>Karper</keyname><forenames>Aaron</forenames></author></authors><title>Feature and Variable Selection in Classification</title><categories>cs.LG cs.AI stat.ML</categories><comments>Part of master seminar in document analysis held by Marcus
  Eichenberger-Liwicki</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The amount of information in the form of features and variables avail- able
to machine learning algorithms is ever increasing. This can lead to classifiers
that are prone to overfitting in high dimensions, high di- mensional models do
not lend themselves to interpretable results, and the CPU and memory resources
necessary to run on high-dimensional datasets severly limit the applications of
the approaches. Variable and feature selection aim to remedy this by finding a
subset of features that in some way captures the information provided best. In
this paper we present the general methodology and highlight some specific
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2308</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2308</id><created>2014-02-10</created><authors><author><keyname>Kallus</keyname><forenames>Nathan</forenames></author></authors><title>Predicting Crowd Behavior with Big Public Data</title><categories>cs.SI physics.soc-ph</categories><doi>10.1145/2567948.2579233</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With public information becoming widely accessible and shared on today's web,
greater insights are possible into crowd actions by citizens and non-state
actors such as large protests and cyber activism. We present efforts to predict
the occurrence, specific timeframe, and location of such actions before they
occur based on public data collected from over 300,000 open content web sources
in 7 languages, from all over the world, ranging from mainstream news to
government publications to blogs and social media. Using natural language
processing, event information is extracted from content such as type of event,
what entities are involved and in what role, sentiment and tone, and the
occurrence time range of the event discussed. Statements made on Twitter about
a future date from the time of posting prove particularly indicative. We
consider in particular the case of the 2013 Egyptian coup d'etat. The study
validates and quantifies the common intuition that data on social media (beyond
mainstream news sources) are able to predict major events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2324</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2324</id><created>2014-02-10</created><updated>2014-07-11</updated><authors><author><keyname>Bhojanapalli</keyname><forenames>Srinadh</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author></authors><title>Universal Matrix Completion</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>22 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of low-rank matrix completion has recently generated a lot of
interest leading to several results that offer exact solutions to the problem.
However, in order to do so, these methods make assumptions that can be quite
restrictive in practice. More specifically, the methods assume that: a) the
observed indices are sampled uniformly at random, and b) for every new matrix,
the observed indices are sampled afresh. In this work, we address these issues
by providing a universal recovery guarantee for matrix completion that works
for a variety of sampling schemes. In particular, we show that if the set of
sampled indices come from the edges of a bipartite graph with large spectral
gap (i.e. gap between the first and the second singular value), then the
nuclear norm minimization based method exactly recovers all low-rank matrices
that satisfy certain incoherence properties. Moreover, we also show that under
certain stricter incoherence conditions, $O(nr^2)$ uniformly sampled entries
are enough to recover any rank-$r$ $n\times n$ matrix, in contrast to the
$O(nr\log n)$ sample complexity required by other matrix completion algorithms
as well as existing analyses of the nuclear norm method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2327</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2327</id><created>2014-02-10</created><updated>2014-07-16</updated><authors><author><keyname>Lipi&#x144;ski</keyname><forenames>Z.</forenames></author></authors><title>On the role of symmetry in solving maximum lifetime problem in
  two-dimensional sensor networks</title><categories>cs.NI</categories><comments>14 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a continuous and discrete symmetries of the maximum lifetime
problem in two dimensional sensor networks. We show, how a symmetry of the
network and invariance of the problem under a given transformation group $G$
can be utilized to simplify its solution. We prove, that for a $G$-invariant
maximum lifetime problem there exists a $G$-invariant solution. Constrains
which follow from the $G$-invariance allow to reduce the problem and its
solution to a subset, an optimal fundamental region of the sensor network. We
analyze in detail solutions of the maximum lifetime problem invariant under a
group of isometry transformations of a two dimensional Euclidean plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2331</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2331</id><created>2014-02-10</created><updated>2014-04-10</updated><authors><author><keyname>Hardt</keyname><forenames>Moritz</forenames></author><author><keyname>Meka</keyname><forenames>Raghu</forenames></author><author><keyname>Raghavendra</keyname><forenames>Prasad</forenames></author><author><keyname>Weitz</keyname><forenames>Benjamin</forenames></author></authors><title>Computational Limits for Matrix Completion</title><categories>cs.CC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix Completion is the problem of recovering an unknown real-valued
low-rank matrix from a subsample of its entries. Important recent results show
that the problem can be solved efficiently under the assumption that the
unknown matrix is incoherent and the subsample is drawn uniformly at random.
Are these assumptions necessary?
  It is well known that Matrix Completion in its full generality is NP-hard.
However, little is known if make additional assumptions such as incoherence and
permit the algorithm to output a matrix of slightly higher rank. In this paper
we prove that Matrix Completion remains computationally intractable even if the
unknown matrix has rank $4$ but we are allowed to output any constant rank
matrix, and even if additionally we assume that the unknown matrix is
incoherent and are shown $90%$ of the entries. This result relies on the
conjectured hardness of the $4$-Coloring problem. We also consider the positive
semidefinite Matrix Completion problem. Here we show a similar hardness result
under the standard assumption that $\mathrm{P}\ne \mathrm{NP}.$
  Our results greatly narrow the gap between existing feasibility results and
computational lower bounds. In particular, we believe that our results give the
first complexity-theoretic justification for why distributional assumptions are
needed beyond the incoherence assumption in order to obtain positive results.
On the technical side, we contribute several new ideas on how to encode hard
combinatorial problems in low-rank optimization problems. We hope that these
techniques will be helpful in further understanding the computational limits of
Matrix Completion and related problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2333</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2333</id><created>2014-02-10</created><authors><author><keyname>Michalski</keyname><forenames>Vincent</forenames></author><author><keyname>Memisevic</keyname><forenames>Roland</forenames></author><author><keyname>Konda</keyname><forenames>Kishore</forenames></author></authors><title>Modeling sequential data using higher-order relational features and
  predictive training</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bi-linear feature learning models, like the gated autoencoder, were proposed
as a way to model relationships between frames in a video. By minimizing
reconstruction error of one frame, given the previous frame, these models learn
&quot;mapping units&quot; that encode the transformations inherent in a sequence, and
thereby learn to encode motion. In this work we extend bi-linear models by
introducing &quot;higher-order mapping units&quot; that allow us to encode
transformations between frames and transformations between transformations.
  We show that this makes it possible to encode temporal structure that is more
complex and longer-range than the structure captured within standard bi-linear
models. We also show that a natural way to train the model is by replacing the
commonly used reconstruction objective with a prediction objective which forces
the model to correctly predict the evolution of the input multiple steps into
the future. Learning can be achieved by back-propagating the multi-step
prediction through time. We test the model on various temporal prediction
tasks, and show that higher-order mappings and predictive training both yield a
significant improvement over bi-linear models in terms of prediction accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2335</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2335</id><created>2014-02-10</created><authors><author><keyname>Carrillo</keyname><forenames>Rafael E.</forenames></author><author><keyname>McEwen</keyname><forenames>Jason D.</forenames></author><author><keyname>Wiaux</keyname><forenames>Yves</forenames></author></authors><title>Sparsity averaging for radio-interferometric imaging</title><categories>astro-ph.IM cs.CV</categories><comments>1 page, 1 figure, Proceedings of the Biomedical and Astronomical
  Signal Processing Frontiers (BASP) workshop 2013, Related journal
  publications available at http://arxiv.org/abs/arXiv:1208.2330 and
  http://arxiv.org/abs/1307.4370</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel regularization method for compressive imaging in the
context of the compressed sensing (CS) theory with coherent and redundant
dictionaries. Natural images are often complicated and several types of
structures can be present at once. It is well known that piecewise smooth
images exhibit gradient sparsity, and that images with extended structures are
better encapsulated in wavelet frames. Therefore, we here conjecture that
promoting average sparsity or compressibility over multiple frames rather than
single frames is an extremely powerful regularization prior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2343</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2343</id><created>2014-02-10</created><authors><author><keyname>Goparaju</keyname><forenames>Sreechakra</forenames></author><author><keyname>Rouayheb</keyname><forenames>Salim El</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author></authors><title>New Codes and Inner Bounds for Exact Repair in Distributed Storage
  Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE International Symposium on Information Theory
  (ISIT) 2014. This draft contains 8 pages and 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the exact-repair tradeoff between storage and repair bandwidth in
distributed storage systems (DSS). We give new inner bounds for the tradeoff
region and provide code constructions that achieve these bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2351</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2351</id><created>2014-02-10</created><updated>2016-02-14</updated><authors><author><keyname>Figueiredo</keyname><forenames>Flavio</forenames></author><author><keyname>Almeida</keyname><forenames>Jussara M.</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Marcos Andr&#xe9;</forenames></author><author><keyname>Benevenuto</keyname><forenames>Fabr&#xed;cio</forenames></author></authors><title>TrendLearner: Early Prediction of Popularity Trends of User Generated
  Content</title><categories>cs.SI cs.IR</categories><comments>To appear at Elsevier Information Sciences Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We here focus on the problem of predicting the popularity trend of user
generated content (UGC) as early as possible. Taking YouTube videos as case
study, we propose a novel two-step learning approach that: (1) extracts
popularity trends from previously uploaded objects, and (2) predicts trends for
new content. Unlike previous work, our solution explicitly addresses the
inherent tradeoff between prediction accuracy and remaining interest in the
content after prediction, solving it on a per-object basis. Our experimental
results show great improvements of our solution over alternatives, and its
applicability to improve the accuracy of state-of-the-art popularity prediction
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2359</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2359</id><created>2014-02-10</created><updated>2014-05-28</updated><authors><author><keyname>Kaliszyk</keyname><forenames>Cezary</forenames></author><author><keyname>Urban</keyname><forenames>Josef</forenames></author><author><keyname>Vysko&#x10d;il</keyname><forenames>Ji&#x159;&#xed;</forenames></author></authors><title>Machine Learner for Automated Reasoning 0.4 and 0.5</title><categories>cs.LG cs.AI cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine Learner for Automated Reasoning (MaLARea) is a learning and reasoning
system for proving in large formal libraries where thousands of theorems are
available when attacking a new conjecture, and a large number of related
problems and proofs can be used to learn specific theorem-proving knowledge.
The last version of the system has by a large margin won the 2013 CASC LTB
competition. This paper describes the motivation behind the methods used in
MaLARea, discusses the general approach and the issues arising in evaluation of
such system, and describes the Mizar@Turing100 and CASC'24 versions of MaLARea.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2363</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2363</id><created>2014-02-10</created><authors><author><keyname>Shingade</keyname><forenames>Ashish</forenames></author><author><keyname>Ghotkar</keyname><forenames>Archana</forenames></author></authors><title>Animation of 3D Human Model Using Markerless Motion Capture Applied To
  Sports</title><categories>cs.GR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markerless motion capture is an active research in 3D virtualization. In
proposed work we presented a system for markerless motion capture for 3D human
character animation, paper presents a survey on motion and skeleton tracking
techniques which are developed or are under development. The paper proposed a
method to transform the motion of a performer to a 3D human character (model),
the 3D human character performs similar movements as that of a performer in
real time. In the proposed work, human model data will be captured by Kinect
camera, processed data will be applied on 3D human model for animation. 3D
human model is created using open source software (MakeHuman). Anticipated
dataset for sport activity is considered as input which can be applied to any
HCI application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2371</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2371</id><created>2014-02-10</created><updated>2014-07-24</updated><authors><author><keyname>Blekherman</keyname><forenames>Grigoriy</forenames></author><author><keyname>Teitler</keyname><forenames>Zach</forenames></author></authors><title>On Maximum, Typical and Generic Ranks</title><categories>math.AG cs.CC</categories><comments>v1: 8pp. v2: 9pp. Corrected gap in Theorem 6, other minor
  corrections. v3: 10pp. Minor changes</comments><msc-class>15A21, 15A69, 14N15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for several notions of rank including tensor rank, Waring rank,
and generalized rank with respect to a projective variety, the maximum value of
rank is at most twice the generic rank. We show that over the real numbers, the
maximum value of the real rank is at most twice the smallest typical rank,
which is equal to the (complex) generic rank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2372</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2372</id><created>2014-02-10</created><authors><author><keyname>S.</keyname><forenames>Poornima. U.</forenames></author><author><keyname>V</keyname><forenames>Suma.</forenames></author><author><keyname>H</keyname><forenames>Vasanth Kumar.</forenames></author></authors><title>Design Patterns as Quality Influencing Factor in Object Oriented Design
  Approach</title><categories>cs.SE</categories><comments>4 Pages, 6 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object Oriented Design methodology is an emerging software development
approach for complex systems with huge set of requirements. Unlike procedural
approach, it captures the requirements as a set of data rather than services,
encapsulated as a single entity. The success such a project relies on major
factors like design patterns framework, key principles, metric standards and
best practices adapted by the industry. The patterns are key structures for
recursive problem bits in the problem domain. The combination of design
patterns forms a framework which suits the problem statement in hand. The
pattern includes static design and dynamic behavior of different types of
entities which can be mapped as a functional diagram with cardinalities between
them. The degree of cardinality represents the coupling factor which the
industry perceives and measures for software design quality. The organization
specific design principles and rich repository of on-the-shelf patterns are the
major design-quality-influencing-factor contribute to software success. These
are the asset of an industry to deliver a quality product to sustain itself in
the competitive market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2373</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2373</id><created>2014-02-11</created><authors><author><keyname>S.</keyname><forenames>Poornima. U.</forenames></author><author><keyname>V</keyname><forenames>Suma.</forenames></author></authors><title>Visualization of Object Oriented Modeling from the Perspective of Set
  theory</title><categories>cs.SE</categories><comments>Pages 5, 2 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Language is a medium for communication of our thoughts. Natural language is
too wide to conceive and formulate the thoughts and ideas in a precise way. As
science and technology grows, the necessity of languages arouses through which
the thoughts are expressed in a better manner. Set Theory is such a
mathematical language for expressing the thought of interest in a realistic
way. It is well suited for presenting object oriented solution model, since
this implementation methodology analyzes and modulates the requirements in a
realistic way. Since the design flaws are one of the factors for software
failure, industries are focusing on minimizing the design defects through
better solution modeling techniques and quality assessment practices. The
Object Oriented (OO) solution space can be visualized using the language of Set
theory with which the design architecture of modules can be well defined. It
provides a strong base to quantify the relationships within and between the
modules, which is a mode for measuring the complexity of solution design of any
software projects. This paper provides a visualization of OO modeling from the
perspective of Set theory. Thereby, it paves the path for the designers to
effectively design the application which is one of the challenges of a project
development. Further, this mode of visualization enables one to effectively
measure and controls the design complexity leading towards reducing the design
flaws and enhanced software quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2374</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2374</id><created>2014-02-11</created><authors><author><keyname>S.</keyname><forenames>Poornima U.</forenames></author><author><keyname>V</keyname><forenames>Suma.</forenames></author></authors><title>Factors Modulating Software Design Quality</title><categories>cs.SE</categories><comments>7 Pages, 2 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object oriented approach is one of the popular software development approach
for managing complex systems with massive set of requirements. Unlike
procedural approach, this approach captures the requirements as set of data
rather than services. Further, class is considered as a key unit of the
solution-domain with data and services wrapped together, representing
architectural design of a basic module. Thus, system complexity is directly
related to the number of modules and the degree of interaction between them.
This could be mapped as a functional diagram with cardinalities between the
modules. However, complexity is always a threat to quality at each stage of
software development. Design phase is therefore one of the core influencing
phases during development that selects the right architecture based on the
problem statement which is bound to be measured for quality. Hence, software
industries adapts several organization- specific principles, domain-specific
patterns, metric standards and best practices to improve and measure the
quality of both process and product. The paper highlights the factors which
influence the overall design quality and metrics implication in improving the
quality of final product. It also presents the solution domain as an
interdependent layered architecture which has a greater impact on concluding
the quality of the end product. This approach of design is a unique
contribution to the domain of Object Oriented approach of software development.
It also focuses on design metrics which ensures the implementation of right
choice of design towards the retention of quality of the product.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2375</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2375</id><created>2014-02-11</created><authors><author><keyname>S.</keyname><forenames>Poornima U.</forenames></author><author><keyname>V</keyname><forenames>Suma.</forenames></author></authors><title>Significance of Coupling and Cohesion on Design Quality</title><categories>cs.SE</categories><comments>6 Pages, 2 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, the complexity of the software is increasing due to
automation of every segment of application. Software is nowhere remained as
one-time development product since its architectural dimension is increasing
with addition of new requirements over a short duration. Object Oriented
Development (OOD) methodology is a popular development approach for such
systems which perceives and models the requirements as real world entities.
Classes and Objects logically represent the entities in the solution space and
quality of the software is directly depending on the design quality of these
logical entities. Cohesion and Coupling (C&amp;C) are two major design decisive
factors in OOD which impacts the design of a class and dependency between them
in complex software. It is also most significant to measure C&amp;C for software to
control the complexity level as requirements increases. Several metrics are in
practice to quantify C&amp;C which plays a major role in measuring the design
quality. The software industries are focusing on increasing and measuring the
quality of the product through quality design to continue their market image in
the competitive world. As a part of our research, this paper highlights on the
impact of C&amp;C on design quality of a complex system and its measures to
quantify the overall quality of software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2376</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2376</id><created>2014-02-11</created><authors><author><keyname>Gupta</keyname><forenames>Sangita</forenames></author><author><keyname>V</keyname><forenames>Suma</forenames></author></authors><title>Prediction of Human Performance Capability during Software Development
  using Classification</title><categories>cs.SE</categories><comments>7 Pages, 1 Figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quality of human capital is crucial for software companies to maintain
competitive advantages in knowledge economy era. Software companies recognize
superior talent as a business advantage. They increasingly recognize the
critical linkage between effective talent and business success. However,
software companies suffering from high turnover rates often find it hard to
recruit the right talents. There is an urgent need to develop a personnel
selection mechanism to find the talents who are the most suitable for their
software projects. Data mining techniques assures exploring the information
from the historical projects depending on which the project manager can make
decisions for producing high quality software. This study aims to fill the gap
by developing a data mining framework based on decision tree and association
rules to refocus on criteria for personnel selection. An empirical study was
conducted in a software company to support their hiring decision for project
members. The results demonstrated that there is a need to refocus on selection
criteria for quality objectives. Better selection criteria was identified by
patterns obtained from data mining models by integrating knowledge from
software project database and authors research techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2377</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2377</id><created>2014-02-11</created><authors><author><keyname>Gupta</keyname><forenames>Sangita</forenames></author><author><keyname>V</keyname><forenames>Suma.</forenames></author></authors><title>Empirical Study on Selection of Team Members for Software Projects -
  Data Mining Approach</title><categories>cs.SE</categories><comments>6 Pages, 1 Figure. arXiv admin note: text overlap with
  arXiv:1201.3417 by other authors</comments><journal-ref>International Journal of Computer Science and Informatics, ISSN
  PRINT: 2231 5292, Volume 3, Issue 2, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the essential requisites of any software industry is the development
of customer satisfied products. However, accomplishing the aforesaid business
objective depends upon the depth of quality of product that is engineered in
the organization. Thus, generation of high quality depends upon process, which
is in turn depends upon the people. Existing scenario in IT industries demands
a requirement for deploying the right personnel for achieving desirable quality
in the product through the existing process. The goal of this paper is to
identify the criteria which will be used in industrial practice to select
members of a software project team, and to look for relationships between these
criteria and project success. Using semi-structured interviews and qualitative
methods for data analysis and synthesis, a set of team building criteria was
identified from project managers in industry. The findings show that the
consistent use of the set of criteria correlated significantly with project
success, and the criteria related to human factors present strong correlations
with software quality and thereby project success. This knowledge enables
decision making for project managers in allocation of right personnel to
realize desired level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2379</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2379</id><created>2014-02-11</created><authors><author><keyname>Gupta</keyname><forenames>Sangita</forenames></author><author><keyname>V</keyname><forenames>Suma</forenames></author></authors><title>Enhancing Human Aspect of Software Engineering using Bayesian Classifier</title><categories>cs.SE</categories><comments>5 Pages, 2 Figures. arXiv admin note: substantial text overlap with
  arXiv:1104.4163, arXiv:1201.3418 by other authors</comments><journal-ref>International Journal of Cognitive Science, Engineering, and
  Technology Volume 1, Issue 1, November 2013 ISSN 2347 8047</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IT industries in current scenario have to struggle effectively in terms of
cost, quality, service or innovation for their subsistence in the global
market. Due to the swift transformation of technology, software industries owe
to manage a large set of data having precious information hidden. Data mining
technique enables one to effectively cope with this hidden information where it
can be applied to code optimization, fault prediction and other domains which
modulates the success nature of software projects. Additionally, the efficiency
of the product developed further depends upon the quality of the project
personnel. The position of the paper therefore is to explore potentials of
project personnel in terms of their competency and skill set and its influence
on quality of project. The above mentioned objective is accomplished using a
Bayesian classifier in order to capture the pattern of human performance. By
this means, the hidden and valuable knowledge discovered in the related
databases will be summarized in the statistical structure. This mode of
predictive study enables the project managers to reduce the failure ratio to a
significant level and improve the performance of the project using the right
choice of project personnel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2389</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2389</id><created>2014-02-11</created><authors><author><keyname>Trendowicz</keyname><forenames>Adam</forenames></author><author><keyname>Heidrich</keyname><forenames>Jens</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Supporting Process Maturation with the Enhanced CoBRA Method</title><categories>cs.SE</categories><comments>14 pages. Proceedings of the DASMA Software Metric Congress (MetriKon
  2006), Magdeburger Schriften zum Empirischen Software Engineering, pages
  3-16, Potsdam, Germany, November 2-3 2006. Shaker Verlag</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cost estimation is a very crucial field for software developing companies. In
the context of learning organizations, estimation applicability and accuracy
are not the only acceptance criteria. The contribution of an estimation
technique to the understanding and maturing of related organizational processes
(such as identification of cost and productivity factors, measurement, data
validation, model validation, model maintenance) has recently been gaining
increasing importance. Yet, most of the proposed cost modeling approaches
provide software engineers with hardly any assistance in supporting related
processes. Insufficient support is provided for validating created cost models
(including underlying data collection processes) or, if valid models are
obtained, for applying them to achieve an organization's objectives such as
improved productivity or reduced schedule. This paper presents an enhancement
of the CoBRA(R) cost modeling method by systematically including additional
quantitative methods into iterative analysis-feedback cycles. Applied at Oki
Electric Industry Co., Ltd., Japan, the CoBRA(R) method contributed to the
achievement of the following objectives, including: (1) maturation of existing
measurement processes, (2) increased expertise of Oki software project decision
makers regarding cost-related software processes, and, finally, (3) reduction
of initial estimation error from an initial 120% down to 14%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2390</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2390</id><created>2014-02-11</created><authors><author><keyname>Li</keyname><forenames>Shuqin</forenames></author><author><keyname>Cai</keyname><forenames>Liyu</forenames></author></authors><title>A Distributed Optimization Framework For Multi-channel Multi-user Small
  Cell Networks</title><categories>cs.NI</categories><comments>Technical report of the paper with the same name to be published in
  IEEE communication letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small cell enchantment is emerging as the key technique for wireless network
evolution. One challenging problem for small cell enhancement is how to achieve
high data rate with as-low-as-possible control and computation overheads. As a
solution, we propose a low-complexity distributed optimization framework in
this paper. Our solution includes two parts. One is a novel implicit
information exchange mechanism that enables channel-aware opportunistic
scheduling and resource allocation among links. The other is the sub-gradient
based algorithm with a polynomial-time complexity. What is more, for large
scale systems, we design an improved distributed algorithm based on insights
obtained from the problem structure. This algorithm achieves a close-to-optimal
performance with a much lower complexity. Our numerical evaluations validate
the analytical results and show the advantage of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2394</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2394</id><created>2014-02-11</created><authors><author><keyname>Xin</keyname><forenames>Reynold S.</forenames></author><author><keyname>Crankshaw</keyname><forenames>Daniel</forenames></author><author><keyname>Dave</keyname><forenames>Ankur</forenames></author><author><keyname>Gonzalez</keyname><forenames>Joseph E.</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Stoica</keyname><forenames>Ion</forenames></author></authors><title>GraphX: Unifying Data-Parallel and Graph-Parallel Analytics</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From social networks to language modeling, the growing scale and importance
of graph data has driven the development of numerous new graph-parallel systems
(e.g., Pregel, GraphLab). By restricting the computation that can be expressed
and introducing new techniques to partition and distribute the graph, these
systems can efficiently execute iterative graph algorithms orders of magnitude
faster than more general data-parallel systems. However, the same restrictions
that enable the performance gains also make it difficult to express many of the
important stages in a typical graph-analytics pipeline: constructing the graph,
modifying its structure, or expressing computation that spans multiple graphs.
As a consequence, existing graph analytics pipelines compose graph-parallel and
data-parallel systems using external storage systems, leading to extensive data
movement and complicated programming model.
  To address these challenges we introduce GraphX, a distributed graph
computation framework that unifies graph-parallel and data-parallel
computation. GraphX provides a small, core set of graph-parallel operators
expressive enough to implement the Pregel and PowerGraph abstractions, yet
simple enough to be cast in relational algebra. GraphX uses a collection of
query optimization techniques such as automatic join rewrites to efficiently
implement these graph-parallel operators. We evaluate GraphX on real-world
graphs and workloads and demonstrate that GraphX achieves comparable
performance as specialized graph computation systems, while outperforming them
in end-to-end graph pipelines. Moreover, GraphX achieves a balance between
expressiveness, performance, and ease of use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2404</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2404</id><created>2014-02-11</created><authors><author><keyname>Sneha</keyname><forenames>J. M.</forenames></author><author><keyname>Nagaraja</keyname><forenames>G. S.</forenames></author></authors><title>Virtual Learning Environments-A Survey</title><categories>cs.CY</categories><comments>5 pages, IJCTT Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is based on the study of existing literature, highlights the
current state of the work proposed to implement technically enhanced learning.
  Technology developments and network infrastructure improvements, specifically
the world wide web) are providing exciting opportunities for the use of
computers in all areas. These developments have fit together with an evolving
role for education as more students wish to study at a distance, part time, or
wish to integrate their education with their professional career. With the
market becoming increasingly mature, e learning has almost become a major plank
in both national and institutional strategies.
  At the same time, virtual learning system is also gaining its popularity
among its users. It has brought in a great revolution in itself. In the
advanced learning strategy, virtual learning systems depends on level and
sector of working, usage of functions, purpose of usage, required online
resources to perform computationally intensive operations such as information
sharing and collaborative work. Often the institutions prefer the latest and
the best technology which is cost effective and provide the best features which
meet up all requirements. Virtual learning environment enables operator
professional to bring together in one place a variety of prevailing resources,
suchlike tasks and formative feedback, and links to law reports, statutes and
journal articles, all intended at summing up value to student learning managing
learning experience without the burden of communication and providing
successful delivery of education and training with flexibility.
  In spite of the hype achieved by organizations in technically enhanced
learning, the growth of the virtual learning systems users is still below
expectations due to the risks associated with the implementation strategy and
provision of technical support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2409</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2409</id><created>2014-02-11</created><updated>2014-08-02</updated><authors><author><keyname>Chen</keyname><forenames>Shaoshi</forenames></author><author><keyname>Kauers</keyname><forenames>Manuel</forenames></author><author><keyname>Koutschan</keyname><forenames>Christoph</forenames></author></authors><title>A Generalized Apagodu-Zeilberger Algorithm</title><categories>cs.SC</categories><acm-class>I.1.2</acm-class><journal-ref>Proceedings of the International Symposium on Symbolic and
  Algebraic Computation (ISSAC 2014), pages 107-114, 2014. ACM, New York, USA,
  ISBN 978-1-4503-2501-1</journal-ref><doi>10.1145/2608628.2608641</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Apagodu-Zeilberger algorithm can be used for computing annihilating
operators for definite sums over hypergeometric terms, or for definite
integrals over hyperexponential functions. In this paper, we propose a
generalization of this algorithm which is applicable to arbitrary
$\partial$-finite functions. In analogy to the hypergeometric case, we
introduce the notion of proper $\partial$-finite functions. We show that the
algorithm always succeeds for these functions, and we give a tight a priori
bound for the order of the output operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2410</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2410</id><created>2014-02-11</created><updated>2014-06-23</updated><authors><author><keyname>Lonsing</keyname><forenames>Florian</forenames></author><author><keyname>Egly</keyname><forenames>Uwe</forenames></author></authors><title>Incremental QBF Solving</title><categories>cs.LO</categories><comments>revision (camera-ready, to appear in the proceedings of CP 2014,
  LNCS, Springer)</comments><doi>10.1007/978-3-319-10428-7_38</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of incrementally solving a sequence of quantified
Boolean formulae (QBF). Incremental solving aims at using information learned
from one formula in the process of solving the next formulae in the sequence.
Based on a general overview of the problem and related challenges, we present
an approach to incremental QBF solving which is application-independent and
hence applicable to QBF encodings of arbitrary problems. We implemented this
approach in our incremental search-based QBF solver DepQBF and report on
implementation details. Experimental results illustrate the potential benefits
of incremental solving in QBF-based workflows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2415</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2415</id><created>2014-02-11</created><updated>2014-12-16</updated><authors><author><keyname>Singla</keyname><forenames>Pradeep</forenames></author><author><keyname>Gautam</keyname><forenames>Devraj</forenames></author></authors><title>Reversible Squaring Circuit For Low Power Digital Signal Processing</title><categories>cs.AR</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in design fig. 3(b)</comments><journal-ref>International Journal of Electronics, Computer &amp; Communication
  Technology, Volume 4, Issue- 2, Jan-2014</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  With the high demand of low power digital systems, energy dissipation in the
digital system is one of the limiting factors. Reversible logic is one of the
alternate to reduce heat/energy dissipation in the digital circuits and have a
very significant importance in bioinformatics, optical information processing,
CMOS design etc. In this paper the authors propose the design of new 2- bit
binary Squaring circuit used in most of the digital signal processing hardware
using Feynman &amp; MUX gate. The proposed squaring circuit having less garbage
outputs, constant inputs, Quantum cost and Total logical calculation i.e. less
delay as compared to the traditional method of squaring operation by reversible
multiplier. The simulating results and quantized results are also shown in the
paper which shows the greatest improvement in the design against the previous
methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2420</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2420</id><created>2014-02-11</created><authors><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Gao</keyname><forenames>Jhih-Rong</forenames></author><author><keyname>Pan</keyname><forenames>David Z.</forenames></author></authors><title>L-Shape based Layout Fracturing for E-Beam Lithography</title><categories>cs.AR</categories><doi>10.1109/ASPDAC.2013.6509604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Layout fracturing is a fundamental step in mask data preparation and e-beam
lithography (EBL) writing. To increase EBL throughput, recently a new L-shape
writing strategy is proposed, which calls for new L-shape fracturing, versus
the conventional rectangular fracturing. Meanwhile, during layout fracturing,
one must minimize very small/narrow features, also called slivers, due to
manufacturability concern. This paper addresses this new research problem of
how to perform L-shaped fracturing with sliver minimization. We propose two
novel algorithms. The first one, rectangular merging (RM), starts from a set of
rectangular fractures and merges them optimally to form L-shape fracturing. The
second algorithm, direct L-shape fracturing (DLF), directly and effectively
fractures the input layouts into L-shapes with sliver minimization. The
experimental results show that our algorithms are very effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2425</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2425</id><created>2014-02-11</created><authors><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Gao</keyname><forenames>Jhih-Rong</forenames></author><author><keyname>Pan</keyname><forenames>David Z.</forenames></author></authors><title>Triple Patterning Lithography (TPL) Layout Decomposition using
  End-Cutting</title><categories>cs.AR</categories><doi>10.1117/12.2011355</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Triple patterning lithography (TPL) is one of the most promising techniques
in the 14nm logic node and beyond. However, traditional LELELE type TPL
technology suffers from native conflict and overlapping problems. Recently
LELEEC process was proposed to overcome the limitations, where the third mask
is used to generate the end-cuts. In this paper we propose the first study for
LELEEC layout decomposition. Conflict graphs and end-cut graphs are constructed
to extract all the geometrical relationships of input layout and end-cut
candidates. Based on these graphs, integer linear programming (ILP) is
formulated to minimize the conflict number and the stitch number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2426</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2426</id><created>2014-02-11</created><authors><author><keyname>Dillon</keyname><forenames>Keith</forenames></author><author><keyname>Fainman</keyname><forenames>Yeshaiahu</forenames></author></authors><title>Imaging with Rays: Microscopy, Medical Imaging, and Computer Vision</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we broadly consider techniques which utilize projections on
rays for data collection, with particular emphasis on optical techniques. We
formulate a variety of imaging techniques as either special cases or extensions
of tomographic reconstruction. We then consider how the techniques must be
extended to describe objects containing occlusion, as with a self-occluding
opaque object. We formulate the reconstruction problem as a regularized
nonlinear optimization problem to simultaneously solve for object brightness
and attenuation, where the attenuation can become infinite. We demonstrate
various simulated examples for imaging opaque objects, including sparse point
sources, a conventional multiview reconstruction technique, and a
super-resolving technique which exploits occlusion to resolve an image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2427</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2427</id><created>2014-02-11</created><authors><author><keyname>Hauffa</keyname><forenames>Jan</forenames></author><author><keyname>Lichtenberg</keyname><forenames>Tobias</forenames></author><author><keyname>Groh</keyname><forenames>Georg</forenames></author></authors><title>An evaluation of keyword extraction from online communication for the
  characterisation of social relations</title><categories>cs.SI cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The set of interpersonal relationships on a social network service or a
similar online community is usually highly heterogenous. The concept of tie
strength captures only one aspect of this heterogeneity. Since the unstructured
text content of online communication artefacts is a salient source of
information about a social relationship, we investigate the utility of keywords
extracted from the message body as a representation of the relationship's
characteristics as reflected by the conversation topics. Keyword extraction is
performed using standard natural language processing methods. Communication
data and human assessments of the extracted keywords are obtained from Facebook
users via a custom application. The overall positive quality assessment
provides evidence that the keywords indeed convey relevant information about
the relationship.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2435</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2435</id><created>2014-02-11</created><authors><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Yuan</keyname><forenames>Kun</forenames></author><author><keyname>Gao</keyname><forenames>Jhih-Rong</forenames></author><author><keyname>Pan</keyname><forenames>David Z.</forenames></author></authors><title>E-BLOW: E-Beam Lithography Overlapping aware Stencil Planning for MCC
  System</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electron beam lithography (EBL) is a promising maskless solution for the
technology beyond 14nm logic node. To overcome its throughput limitation,
recently the traditional EBL system is extended into MCC system. %to further
improve the throughput. In this paper, we present E-BLOW, a tool to solve the
overlapping aware stencil planning (OSP) problems in MCC system. E-BLOW is
integrated with several novel speedup techniques, i.e., successive relaxation,
dynamic programming and KD-Tree based clustering, to achieve a good performance
in terms of runtime and solution quality. Experimental results show that,
compared with previous works, E-BLOW demonstrates better performance for both
conventional EBL system and MCC system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2437</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2437</id><created>2014-02-11</created><updated>2015-01-19</updated><authors><author><keyname>Krawczyk</keyname><forenames>Tomasz</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>On-line approach to off-line coloring problems on graphs with geometric
  representations</title><categories>cs.DS cs.CG cs.DM math.CO</categories><comments>New title, significant improvements and extensions (e.g. K_k-free
  colorings), figures</comments><msc-class>05C15, 05C62, 68W27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main goal of this paper is to formalize and explore a connection between
chromatic properties of graphs with geometric representations and competitive
analysis of on-line algorithms, which became apparent after the recent
construction of triangle-free geometric intersection graphs with arbitrarily
large chromatic number due to Pawlik et al. We show that on-line graph coloring
problems give rise to classes of game graphs with a natural geometric
interpretation. We use this concept to estimate the chromatic number of graphs
with geometric representations by finding, for appropriate simpler graphs,
on-line coloring algorithms using few colors or proving that no such algorithms
exist.
  We derive upper and lower bounds on the maximum chromatic number that
rectangle overlap graphs, subtree overlap graphs, and interval filament graphs
(all of which generalize interval overlap graphs) can have when their clique
number is bounded. The bounds are absolute for interval filament graphs and
asymptotic of the form $(\log\log n)^{f(\omega)}$ for rectangle and subtree
overlap graphs. In particular, we provide the first construction of geometric
intersection graphs with bounded clique number and with chromatic number
asymptotically greater than $\log\log n$.
  We also introduce a concept of $K_k$-free colorings and show that for some
geometric representations, the $K_3$-free chromatic number can be bounded in
terms of the clique number although the ordinary ($K_2$-free) chromatic number
cannot. Such a result for segment intersection graphs would imply a well-known
conjecture that $k$-quasi-planar geometric graphs have linearly many edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2440</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2440</id><created>2014-02-11</created><authors><author><keyname>Ammer</keyname><forenames>Regina</forenames></author><author><keyname>Markl</keyname><forenames>Matthias</forenames></author><author><keyname>J&#xfc;chter</keyname><forenames>Vera</forenames></author><author><keyname>K&#xf6;rner</keyname><forenames>Carolin</forenames></author><author><keyname>R&#xfc;de</keyname><forenames>Ulrich</forenames></author></authors><title>Validation Experiments for LBM Simulations of Electron Beam Melting</title><categories>cs.CE</categories><comments>submitted to &quot;International Journal of Modern Physics C&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper validates 3D simulation results of electron beam melting (EBM)
processes comparing experimental and numerical data. The physical setup is
presented which is discretized by a three dimensional (3D) thermal lattice
Boltzmann method (LBM). An experimental process window is used for the
validation depending on the line energy injected into the metal powder bed and
the scan velocity of the electron beam. In the process window the EBM products
are classified into the categories, porous, good and swelling, depending on the
quality of the surface. The same parameter sets are used to generate a
numerical process window. A comparison of numerical and experimental process
windows shows a good agreement. This validates the EBM model and justifies
simulations for future improvements of EBM processes. In particular numerical
simulations can be used to explain future process window scenarios and find the
best parameter set for a good surface quality and dense products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2442</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2442</id><created>2014-02-11</created><authors><author><keyname>Gao</keyname><forenames>Jhih-Rong</forenames></author><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Huang</keyname><forenames>Ru</forenames></author><author><keyname>Pan</keyname><forenames>David Z.</forenames></author></authors><title>Self-Aligned Double Patterning Friendly Configuration for Standard Cell
  Library Considering Placement</title><categories>cs.AR</categories><doi>10.1117/12.2011660</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-aligned double patterning (SADP) has become a promising technique to
push pattern resolution limit to sub-22nm technology node. Although SADP
provides good overlay controllability, it encounters many challenges in
physical design stages to obtain conflict-free layout decomposition. In this
paper, we study the impact on placement by different standard cell layout
decomposition strategies. We propose a SADP friendly standard cell
configuration which provides pre-coloring results for standard cells. These
configurations are brought into the placement stage to help ensure layout
decomposability and save the extra effort for solving conflicts in later
stages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2446</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2446</id><created>2014-02-11</created><updated>2014-05-20</updated><authors><author><keyname>Bouzid</keyname><forenames>Zohir</forenames></author><author><keyname>Gafni</keyname><forenames>Eli</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Petr</forenames></author></authors><title>Strong Equivalence Relations for Iterated Models</title><categories>cs.DC cs.DS</categories><acm-class>C.2.4; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Iterated Immediate Snapshot model (IIS), due to its elegant geometrical
representation, has become standard for applying topological reasoning to
distributed computing. Its modular structure makes it easier to analyze than
the more realistic (non-iterated) read-write Atomic-Snapshot memory model (AS).
It is known that AS and IIS are equivalent with respect to \emph{wait-free
task} computability: a distributed task is solvable in AS if and only if it
solvable in IIS. We observe, however, that this equivalence is not sufficient
in order to explore solvability of tasks in \emph{sub-models} of AS (i.e.
proper subsets of its runs) or computability of \emph{long-lived} objects, and
a stronger equivalence relation is needed. In this paper, we consider
\emph{adversarial} sub-models of AS and IIS specified by the sets of processes
that can be \emph{correct} in a model run. We show that AS and IIS are
equivalent in a strong way: a (possibly long-lived) object is implementable in
AS under a given adversary if and only if it is implementable in IIS under the
same adversary. %This holds whether the object is one-shot or long-lived.
Therefore, the computability of any object in shared memory under an
adversarial AS scheduler can be equivalently investigated in IIS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2447</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2447</id><created>2014-02-11</created><updated>2014-04-09</updated><authors><author><keyname>Br&#xfc;mmer</keyname><forenames>Niko</forenames></author><author><keyname>Swart</keyname><forenames>Albert</forenames></author><author><keyname>van Leeuwen</keyname><forenames>David</forenames></author></authors><title>A comparison of linear and non-linear calibrations for speaker
  recognition</title><categories>stat.ML cs.LG</categories><comments>accepted for Odyssey 2014: The Speaker and Language Recognition
  Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent work on both generative and discriminative score to
log-likelihood-ratio calibration, it was shown that linear transforms give good
accuracy only for a limited range of operating points. Moreover, these methods
required tailoring of the calibration training objective functions in order to
target the desired region of best accuracy. Here, we generalize the linear
recipes to non-linear ones. We experiment with a non-linear, non-parametric,
discriminative PAV solution, as well as parametric, generative,
maximum-likelihood solutions that use Gaussian, Student's T and
normal-inverse-Gaussian score distributions. Experiments on NIST SRE'12 scores
suggest that the non-linear methods provide wider ranges of optimal accuracy
and can be trained without having to resort to objective function tailoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2453</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2453</id><created>2014-02-11</created><authors><author><keyname>Toraci</keyname><forenames>Cristian</forenames></author><author><keyname>Zaccaria</keyname><forenames>Gabriele</forenames></author><author><keyname>Ceriani</keyname><forenames>Stefano</forenames></author><author><keyname>Wilson</keyname><forenames>David</forenames></author><author><keyname>Fato</keyname><forenames>Marco</forenames></author><author><keyname>Piana</keyname><forenames>Michele</forenames></author></authors><title>Sliding window and compressive sensing for low-field dynamic magnetic
  resonance imaging</title><categories>cs.CE physics.med-ph</categories><comments>Submitted to IEEE Transactions on Medical Imaging</comments><msc-class>68U10, 65R32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an acquisition/processing procedure for image reconstruction in
dynamic Magnetic Resonance Imaging (MRI). The approach requires sliding window
to record a set of trajectories in the k-space, standard regularization to
reconstruct an estimate of the object and compressed sensing to recover image
residuals. We validated this approach in the case of specific simulated
experiments and, in the case of real measurements, we showed that the procedure
is reliable even in the case of data acquired by means of a low-field scanner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2455</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2455</id><created>2014-02-11</created><authors><author><keyname>Helou</keyname><forenames>E. S.</forenames></author><author><keyname>Censor</keyname><forenames>Y.</forenames></author><author><keyname>Chen</keyname><forenames>T. -B.</forenames></author><author><keyname>Chern</keyname><forenames>I-L.</forenames></author><author><keyname>De Pierro</keyname><forenames>&#xc1;. R.</forenames></author><author><keyname>Jiang</keyname><forenames>M.</forenames></author><author><keyname>Lu</keyname><forenames>H. H. -S.</forenames></author></authors><title>String-Averaging Expectation-Maximization for Maximum Likelihood
  Estimation in Emission Tomography</title><categories>physics.med-ph cs.CY math.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the maximum likelihood model in emission tomography and propose a
new family of algorithms for its solution, called String-Averaging
Expectation-Maximization (SAEM). In the String-Averaging algorithmic regime,
the index set of all underlying equations is split into subsets, called
&quot;strings,&quot; and the algorithm separately proceeds along each string, possibly in
parallel. Then, the end-points of all strings are averaged to form the next
iterate. SAEM algorithms with several strings presents better practical merits
than the classical Row-Action Maximum-Likelihood Algorithm (RAMLA). We present
numerical experiments showing the effectiveness of the algorithmic scheme in
realistic situations. Performance is evaluated from the computational cost and
reconstruction quality viewpoints. A complete convergence theory is also
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2459</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2459</id><created>2014-02-11</created><authors><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Yuan</keyname><forenames>Kun</forenames></author><author><keyname>Zhang</keyname><forenames>Boyang</forenames></author><author><keyname>Ding</keyname><forenames>Duo</forenames></author><author><keyname>Pan</keyname><forenames>David Z.</forenames></author></authors><title>Layout decomposition for triple patterning lithography</title><categories>cs.AR</categories><doi>10.1109/ICCAD.2011.6105297</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As minimum feature size and pitch spacing further decrease, triple patterning
lithography (TPL) is a possible 193nm extension along the paradigm of double
patterning lithography (DPL). However, there is very little study on TPL layout
decomposition. In this paper, we show that TPL layout decomposition is a more
difficult problem than that for DPL. We then propose a general integer linear
programming formulation for TPL layout decomposition which can simultaneously
minimize conflict and stitch numbers. Since ILP has very poor scalability, we
propose three acceleration techniques without sacrificing solution quality:
independent component computation, layout graph simplification, and bridge
computation. For very dense layouts, even with these speedup techniques, ILP
formulation may still be too slow. Therefore, we propose a novel vector
programming formulation for TPL decomposition, and solve it through effective
semidefinite programming (SDP) approximation. Experimental results show that
the ILP with acceleration techniques can reduce 82% runtime compared to the
baseline ILP. Using SDP based algorithm, the runtime can be further reduced by
42% with some tradeoff in the stitch number (reduced by 7%) and the conflict
(9% more). However, for very dense layouts, SDP based algorithm can achieve
140x speed-up even compared with accelerated ILP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2460</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2460</id><created>2014-02-11</created><authors><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Dong</keyname><forenames>Sheqin</forenames></author><author><keyname>Ma</keyname><forenames>Yuchun</forenames></author><author><keyname>Lin</keyname><forenames>Tao</forenames></author><author><keyname>Wang</keyname><forenames>Yu</forenames></author><author><keyname>Chen</keyname><forenames>Song</forenames></author><author><keyname>Goto</keyname><forenames>Satoshi</forenames></author></authors><title>Network flow-based simultaneous retiming and slack budgeting for low
  power design</title><categories>cs.AR</categories><doi>10.1109/ASPDAC.2011.5722236</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low power design has become one of the most significant requirements when
CMOS technology entered the nanometer era. Therefore, timing budget is often
performed to slow down as many components as possible so that timing slacks can
be applied to reduce the power consumption while maintaining the performance of
the whole design. Retiming is a procedure that involves the relocation of
flip-flops (FFs) across logic gates to achieve faster clocking speed. In this
paper we show that the retiming and slack budgeting problem can be formulated
to a convex cost dual network flow problem. Both the theoretical analysis and
experimental results show the efficiency of our approach which can not only
reduce power consumption by 8.9%, but also speedup previous work by 500 times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2461</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2461</id><created>2014-02-11</created><authors><author><keyname>Yu</keyname><forenames>Zhenhua</forenames></author><author><keyname>Baxley</keyname><forenames>Robert J.</forenames></author><author><keyname>Zhou</keyname><forenames>G. Tong</forenames></author></authors><title>Distributions of Upper PAPR and Lower PAPR of OFDM Signals in Visible
  Light Communications</title><categories>cs.IT math.IT</categories><comments>acceptted by IEEE ICASSP 2014. arXiv admin note: text overlap with
  arXiv:1304.0193</comments><doi>10.1109/ICASSP.2014.6853617</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonal frequency-division multiplexing (OFDM) in visible light
communications (VLC) inherits the disadvantage of high peak-to-average power
ratio (PAPR) from OFDM in radio frequency (RF) communications. The upper peak
power and lower peak power of real-valued VLC-OFDM signals are both limited by
the dynamic constraints of light emitting diodes (LEDs). The efficiency and
transmitted electrical power are directly related with the upper PAPR (UPAPR)
and lower PAPR (LPAPR) of VLC-OFDM. In this paper, we will derive the
complementary cumulative distribution function (CCDF) of UPAPR and LPAPR, and
investigate the joint distribution of UPAPR and LPAPR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2462</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2462</id><created>2014-02-11</created><authors><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Dong</keyname><forenames>Sheqin</forenames></author><author><keyname>Chen</keyname><forenames>Song</forenames></author><author><keyname>Goto</keyname><forenames>Satoshi</forenames></author></authors><title>Floorplanning and Topology Generation for Application-Specific
  Network-on-Chip</title><categories>cs.AR</categories><doi>10.1109/ASPDAC.2010.5419825</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network-on-chip (NoC) architectures have been proposed as a promising
alternative to classical bus-based communication architectures. In this paper,
we propose a two phases framework to solve application-specific NoCs topology
generation problem. At floorplanning phase, we carry out partition driven
floorplanning. At post-floorplanning phase, a heuristic method and a min-cost
max-flow algorithm is used to insert switches and network interfaces. Finally,
we allocate paths to minimize power consumption. The experimental results show
our algorithm is effective for power saving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2474</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2474</id><created>2014-02-11</created><authors><author><keyname>Hetzl</keyname><forenames>Stefan</forenames></author><author><keyname>Leitsch</keyname><forenames>Alexander</forenames></author><author><keyname>Reis</keyname><forenames>Giselle</forenames></author><author><keyname>Tapolczai</keyname><forenames>Janos</forenames></author><author><keyname>Weller</keyname><forenames>Daniel</forenames></author></authors><title>Introducing Quantified Cuts in Logic with Equality</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cut-introduction is a technique for structuring and compressing formal
proofs. In this paper we generalize our cut-introduction method for the
introduction of quantified lemmas of the form $\forall x.A$ (for
quantifier-free $A$) to a method generating lemmas of the form $\forall
x_1\ldots\forall x_n.A$. Moreover, we extend the original method to predicate
logic with equality. The new method was implemented and applied to the TSTP
proof database. It is shown that the extension of the method to handle equality
and quantifier-blocks leads to a substantial improvement of the old algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2479</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2479</id><created>2014-02-11</created><authors><author><keyname>Zhang</keyname><forenames>Zengfeng</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author></authors><title>Coalitional Games with Overlapping Coalitions for Interference
  Management in Small Cell Networks</title><categories>cs.GT cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of cooperative interference management in
an OFDMA two-tier small cell network. In particular, we propose a novel
approach for allowing the small cells to cooperate, so as to optimize their
sum-rate, while cooperatively satisfying their maximum transmit power
constraints. Unlike existing work which assumes that only disjoint groups of
cooperative small cells can emerge, we formulate the small cells' cooperation
problem as a coalition formation game with overlapping coalitions. In this
game, each small cell base station can choose to participate in one or more
cooperative groups (or coalitions) simultaneously, so as to optimize the
tradeoff between the benefits and costs associated with cooperation. We study
the properties of the proposed overlapping coalition formation game and we show
that it exhibits negative externalities due to interference. Then, we propose a
novel decentralized algorithm that allows the small cell base stations to
interact and self-organize into a stable overlapping coalitional structure.
Simulation results show that the proposed algorithm results in a notable
performance advantage in terms of the total system sum-rate, relative to the
noncooperative case and the classical algorithms for coalitional games with
non-overlapping coalitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2482</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2482</id><created>2014-02-11</created><updated>2014-06-20</updated><authors><author><keyname>Kryvasheyeu</keyname><forenames>Yury</forenames></author><author><keyname>Chen</keyname><forenames>Haohui</forenames></author><author><keyname>Moro</keyname><forenames>Esteban</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author><author><keyname>Cebrian</keyname><forenames>Manuel</forenames></author></authors><title>Performance of Social Network Sensors During Hurricane Sandy</title><categories>cs.SI physics.soc-ph</categories><doi>10.1371/journal.pone.0117288</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information flow during catastrophic events is a critical aspect of disaster
management. Modern communication platforms, in particular online social
networks, provide an opportunity to study such flow, and a mean to derive
early-warning sensors, improving emergency preparedness and response.
Performance of the social networks sensor method, based on topological and
behavioural properties derived from the &quot;friendship paradox&quot;, is studied here
for over 50 million Twitter messages posted before, during, and after Hurricane
Sandy. We find that differences in user's network centrality effectively
translate into moderate awareness advantage (up to 26 hours); and that
geo-location of users within or outside of the hurricane-affected area plays
significant role in determining the scale of such advantage. Emotional response
appears to be universal regardless of the position in the network topology, and
displays characteristic, easily detectable patterns, opening a possibility of
implementing a simple &quot;sentiment sensing&quot; technique to detect and locate
disasters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2487</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2487</id><created>2014-02-11</created><authors><author><keyname>Ghosh</keyname><forenames>Partha</forenames></author><author><keyname>Sen</keyname><forenames>Soumya</forenames></author></authors><title>Materialized View Replacement using Markovs Analysis</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Materialized view is used in large data centric applications to expedite
query processing. The efficiency of materialized view depends on degree of
result found against the queries over the existing materialized views.
Materialized views are constructed following different methodologies. Thus the
efficacy of the materialized views depends on the methodology based on which
these are formed. Construction of materialized views are often time consuming
and moreover after a certain time the performance of the materialized views
degrade when the nature of queries change. In this situation either new
materialized views could be constructed from scratch or the existing views
could be upgraded. Fresh construction of materialized views has higher time
complexity hence the modification of the existing views is a better
solution.Modification process of materialized view is classified under
materialized view maintenance scheme. Materialized view maintenance is a
continuous process and the system could be tuned to ensure a constant rate of
performance. If a materialized view construction process is not supported by
materialized view maintenance scheme that system would suffer from performance
degradation. In this paper a new materialized view maintenance scheme is
proposed using markovs analysis to ensure consistent performance. Markovs
analysis is chosen here to predict steady state probability over initial
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2489</identifier>
 <datestamp>2014-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2489</id><created>2014-02-11</created><updated>2014-07-29</updated><authors><author><keyname>Zhou</keyname><forenames>Yingjie</forenames></author><author><keyname>Maxemchuk</keyname><forenames>Nicholas</forenames></author><author><keyname>Qian</keyname><forenames>Xiangying</forenames></author><author><keyname>Wang</keyname><forenames>Chen</forenames></author></authors><title>The Fair Distribution of Power to Electric Vehicles: An Alternative to
  Pricing</title><categories>cs.NI cs.SY</categories><comments>accepted in IEEE Smartgridcomm'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the popularity of electric vehicles increases, the demand for more power
can increase more rapidly than our ability to install additional generating
capacity. In the long term we expect that the supply and demand will become
balanced. However, in the interim the rate at which electric vehicles can be
deployed will depend on our ability to charge these vehicles without
inconveniencing their owners. In this paper, we investigate using fairness
mechanisms to distribute power to electric vehicles on a smart grid. We assume
that during peak demand there is insufficient power to charge all the vehicles
simultaneously. In each five minute interval of time we select a subset of the
vehicles to charge, based upon information about the vehicles. We evaluate the
selection mechanisms using published data on the current demand for electric
power as a function of time of day, current driving habits for commuting, and
the current rates at which electric vehicles can be charged on home outlets. We
found that conventional selection strategies, such as first-come-first-served
or round robin, may delay a significant fraction of the vehicles by more than
two hours, even when the total available power over the course of a day is two
or three times the power required by the vehicles. However, a selection
mechanism that minimizes the maximum delay can reduce the delays to a few
minutes, even when the capacity available for charging electric vehicles
exceeds their requirements by as little as 5%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2491</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2491</id><created>2014-02-11</created><authors><author><keyname>Banu</keyname><forenames>M. Uthaya</forenames></author><author><keyname>Saravanan</keyname><forenames>K.</forenames></author></authors><title>Optimizing the Cost for Resource Subscription Policy in IaaS Cloud</title><categories>cs.DC</categories><comments>6 pages,8 figures,&quot;Published with International Journal of
  Engineering Trends and Technology (IJETT)&quot;. M.Uthaya Banu, K.Saravanan.
  Article:Optimizing the Cost for Resource Subscription Policy in IaaS Cloud</comments><journal-ref>International Journal of Engineering Trends and Technology (IJETT)
  6(6):296-301, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing allow the users to efficiently and dynamically provision
computing resource to meet their IT needs. Cloud Provider offers two
subscription plan to the customer namely reservation and on-demand. The
reservation plan is typically cheaper than on-demand plan. If the actual
computing demand is known in advance reserving the resource would be
straightforward. The challenge is how to make properly resource provisioning
and how the customers efficiently purchase the provisioning options under
reservation and on-demand. To address this issue, two-phase algorithm are
proposed to minimize service provision cost in both reservation and on-demand
plan. To reserve the correct and optimal amount of resources during
reservation, proposed a mathematical formulae in the first phase. To predict
resource demand, use kalman filter in the second phase. The evaluation result
shows that the two-phase algorithm can significantly reduce the provision cost
and the prediction is of reasonable accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2496</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2496</id><created>2014-02-11</created><authors><author><keyname>Blin</keyname><forenames>L&#xe9;lia</forenames></author><author><keyname>Fraigniaud</keyname><forenames>Pierre</forenames></author></authors><title>Polynomial-Time Space-Optimal Silent Self-Stabilizing Minimum-Degree
  Spanning Tree Construction</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by applications to sensor networks, as well as to many other areas,
this paper studies the construction of minimum-degree spanning trees. We
consider the classical node-register state model, with a weakly fair scheduler,
and we present a space-optimal \emph{silent} self-stabilizing construction of
minimum-degree spanning trees in this model. Computing a spanning tree with
minimum degree is NP-hard. Therefore, we actually focus on constructing a
spanning tree whose degree is within one from the optimal. Our algorithm uses
registers on $O(\log n)$ bits, converges in a polynomial number of rounds, and
performs polynomial-time computation at each node. Specifically, the algorithm
constructs and stabilizes on a special class of spanning trees, with degree at
most $OPT+1$. Indeed, we prove that, unless NP $=$ coNP, there are no
proof-labeling schemes involving polynomial-time computation at each node for
the whole family of spanning trees with degree at most $OPT+1$. Up to our
knowledge, this is the first example of the design of a compact silent
self-stabilizing algorithm constructing, and stabilizing on a subset of optimal
solutions to a natural problem for which there are no time-efficient
proof-labeling schemes. On our way to design our algorithm, we establish a set
of independent results that may have interest on their own. In particular, we
describe a new space-optimal silent self-stabilizing spanning tree
construction, stabilizing on \emph{any} spanning tree, in $O(n)$ rounds, and
using just \emph{one} additional bit compared to the size of the labels used to
certify trees. We also design a silent loop-free self-stabilizing algorithm for
transforming a tree into another tree. Last but not least, we provide a silent
self-stabilizing algorithm for computing and certifying the labels of a
NCA-labeling scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2507</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2507</id><created>2014-02-11</created><authors><author><keyname>Lasagni</keyname><forenames>Matteo</forenames></author><author><keyname>R&#xf6;mer</keyname><forenames>Kay</forenames></author></authors><title>Force-Guiding Particle Chains for Shape-Shifting Displays</title><categories>cs.RO</categories><comments>6 pages, 5 figure, submitted to IROS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present design and implementation of a chain of particles that can be
programmed to fold the chain into a given curve. The particles guide an
external force to fold, therefore the particles are simple and amenable for
miniaturization. A chain can consist of a large number of such particles. Using
multiple of these chains, a shape-shifting display can be constructed that
folds its initially flat surface to approximate a given 3D shape that can be
touched and modified by users, for example, enabling architects to
interactively view, touch, and modify a 3D model of a building.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2508</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2508</id><created>2014-02-11</created><authors><author><keyname>G&#xf6;rzig</keyname><forenames>Steffen</forenames></author></authors><title>Data Compaction - Compression without Decompression</title><categories>cs.DS</categories><comments>11 pages</comments><msc-class>68P30</msc-class><acm-class>E.1; E.2; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data compaction is a new approach for lossless and lossy compression of
read-only array data. The biggest advantage over existing approaches is the
possibility to access compressed data without any decompression. This makes
data compaction most suitable for systems that could currently not apply
compression techniques due to real-time or memory constraints. This is true for
the majority of all computers, i.e. a wide range of embedded systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2509</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2509</id><created>2014-02-11</created><authors><author><keyname>Subha</keyname><forenames>M.</forenames></author><author><keyname>Saravanan</keyname><forenames>K.</forenames></author></authors><title>Achieve Better Ranking Accuracy Using CloudRank Framework for Cloud
  Services</title><categories>cs.DC cs.IR</categories><comments>6 pages, 10 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)</comments><journal-ref>International Journal of Engineering Trends and Technology (IJETT)
  6(6):307-312, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building high quality cloud applications becomes an urgently required
research problem. Nonfunctional performance of cloud services is usually
described by quality-of-service (QoS). In cloud applications, cloud services
are invoked remotely by internet connections. The QoS Ranking of cloud services
for a user cannot be transferred directly to another user, since the locations
of the cloud applications are quite different. Personalized QoS Ranking is
required to evaluate all candidate services at the user - side but it is
impractical in reality. To get QoS values, the service candidates are usually
required and it is very expensive. To avoid time consuming and expensive
realworld service invocations, this paper proposes a CloudRank framework which
predicts the QoS ranking directly without predicting the corresponding QoS
values. This framework provides an accurate ranking but the QoS values are same
in both algorithms so, an optimal VM allocation policy is used to improve the
QoS performance of cloud services and it also provides better ranking accuracy
than CloudRank2 algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2511</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2511</id><created>2014-02-11</created><authors><author><keyname>Sironi</keyname><forenames>Eugenia</forenames></author></authors><title>Type Theory in Ludics</title><categories>math.LO cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1307.1028 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present some first steps in the more general setting of the interpretation
of dependent type theory in Ludics. The framework is the following: a
(Martin-Lof) type A is represented by a behaviour (which corresponds to a
formula) in such a way that canonical elements of A are interpreted in a set
that is principal for the behaviour, where principal means in some way a
minimal generator. We introduce some notions on Ludics and the interpretation
of Martin-Lof rules. Then we propose a representation for simple types in
Ludics, i.e., natural numbers, lists, the arrow construction and the usual
constructors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2531</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2531</id><created>2014-02-11</created><updated>2015-02-09</updated><authors><author><keyname>Jyothi</keyname><forenames>Sangeetha Abdu</forenames></author><author><keyname>Singla</keyname><forenames>Ankit</forenames></author><author><keyname>Godfrey</keyname><forenames>P. Brighten</forenames></author><author><keyname>Kolla</keyname><forenames>Alexandra</forenames></author></authors><title>Measuring and Understanding Throughput of Network Topologies</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High throughput is a fundamental goal of network design and is of particular
interest in data center and high performance computing networks. Although
myriad network topologies have been recently proposed, a broad head-to-head
comparison of these proposals is absent, and the right way to compare
worst-case throughput performance turns out to be a subtle problem.
  In this paper, we present a framework to benchmark the throughput of network
topologies. First, we show that cut-based metrics such as bisection bandwidth
are the wrong measure: they yield incorrect conclusions about throughput
performance. Next, we show how to generate a near-worst-case traffic matrix for
a given topology, which empirically approaches a theoretical lower bound.
Finally, we employ this metric, along with other traffic matrices, to analyze
the throughput performance of a variety of networks proposed for data centers
and high performance computing. Our evaluation code is freely available to
facilitate future reproducible work on rigorously designing and evaluating
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2536</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2536</id><created>2014-02-11</created><authors><author><keyname>Singh</keyname><forenames>Amandeep</forenames></author><author><keyname>Singh</keyname><forenames>Balwinder</forenames></author></authors><title>Design and Implementation of Bit Transition Counter</title><categories>cs.AR</categories><comments>International Journal Paper</comments><journal-ref>Circuits and Systems: An International Journal (CSIJ), Vol. 1, No.
  1, January 2014</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In today VLSI system design, power consumption is gaining more attention as
compared to performance and area. This is due to battery life in portable
devices and operating frequency of the design. Power consumption mainly
consists of static power, dynamic power, leakage power and short circuit power.
Dynamic power is dominant among all which depends on many factors viz. power
supply, load capacitance and frequency. Switching activity also affects dynamic
power consumption of bus which is determined by calculating the number of bit
transitions on bus. The purpose of this paper is to design a bit transition
counter which can be used to calculate the switching activity of the circuit
nodes. The novel feature is that it can be inserted at any node of the circuit,
thus helpful for calculating power consumption of bus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2543</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2543</id><created>2014-02-11</created><authors><author><keyname>Hirvonen</keyname><forenames>Juho</forenames></author><author><keyname>Rybicki</keyname><forenames>Joel</forenames></author><author><keyname>Schmid</keyname><forenames>Stefan</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author></authors><title>Large Cuts with Local Algorithms on Triangle-Free Graphs</title><categories>cs.DC cs.DM cs.DS</categories><comments>1+17 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of finding large cuts in $d$-regular triangle-free
graphs. In prior work, Shearer (1992) gives a randomised algorithm that finds a
cut of expected size $(1/2 + 0.177/\sqrt{d})m$, where $m$ is the number of
edges. We give a simpler algorithm that does much better: it finds a cut of
expected size $(1/2 + 0.28125/\sqrt{d})m$. As a corollary, this shows that in
any $d$-regular triangle-free graph there exists a cut of at least this size.
  Our algorithm can be interpreted as a very efficient randomised distributed
algorithm: each node needs to produce only one random bit, and the algorithm
runs in one synchronous communication round. This work is also a case study of
applying computational techniques in the design of distributed algorithms: our
algorithm was designed by a computer program that searched for optimal
algorithms for small values of $d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2549</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2549</id><created>2014-02-11</created><authors><author><keyname>Hilke</keyname><forenames>Miikka</forenames></author><author><keyname>Lenzen</keyname><forenames>Christoph</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author></authors><title>Local Approximability of Minimum Dominating Set on Planar Graphs</title><categories>cs.DC cs.DS</categories><comments>3 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there is no deterministic local algorithm (constant-time
distributed graph algorithm) that finds a $(7-\epsilon)$-approximation of a
minimum dominating set on planar graphs, for any positive constant $\epsilon$.
In prior work, the best lower bound on the approximation ratio has been
$5-\epsilon$; there is also an upper bound of $52$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2551</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2551</id><created>2014-02-11</created><authors><author><keyname>U</keyname><forenames>Aishwarya B</forenames></author><author><keyname>A</keyname><forenames>Mohammed Saaqib</forenames></author><author><keyname>R</keyname><forenames>Rajashree H</forenames></author><author><keyname>B</keyname><forenames>Vigasini</forenames></author></authors><title>Modeling European Options</title><categories>cs.CE</categories><comments>arXiv admin note: substantial text overlap with arXiv:1311.0438 by
  other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Option contracts can be valued by using the Black-Scholes equation, a partial
differential equation with initial conditions. An exact solution for European
style options is known. The computation time and the error need to be minimized
simultaneously. In this paper, the authors have solved the Black-Scholes
equation by employing a reasonably accurate implicit method. Options with known
analytic solutions have been evaluated. Furthermore, an overall second order
accurate space and time discretization has been accomplished in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2552</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2552</id><created>2014-02-11</created><authors><author><keyname>Laurinharju</keyname><forenames>Juhana</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author></authors><title>Linial's Lower Bound Made Easy</title><categories>cs.DC</categories><comments>3 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linial's seminal result shows that any deterministic distributed algorithm
that finds a $3$-colouring of an $n$-cycle requires at least $\log^*(n)/2 - 1$
communication rounds. We give a new simpler proof of this theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2561</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2561</id><created>2014-01-18</created><authors><author><keyname>Flati</keyname><forenames>Tiziano</forenames></author><author><keyname>Navigli</keyname><forenames>Roberto</forenames></author></authors><title>The CQC Algorithm: Cycling in Graphs to Semantically Enrich and Enhance
  a Bilingual Dictionary</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  135-171, 2012</journal-ref><doi>10.1613/jair.3456</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bilingual machine-readable dictionaries are knowledge resources useful in
many automatic tasks. However, compared to monolingual computational lexicons
like WordNet, bilingual dictionaries typically provide a lower amount of
structured information, such as lexical and semantic relations, and often do
not cover the entire range of possible translations for a word of interest. In
this paper we present Cycles and Quasi-Cycles (CQC), a novel algorithm for the
automated disambiguation of ambiguous translations in the lexical entries of a
bilingual machine-readable dictionary. The dictionary is represented as a
graph, and cyclic patterns are sought in the graph to assign an appropriate
sense tag to each translation in a lexical entry. Further, we use the
algorithms output to improve the quality of the dictionary itself, by
suggesting accurate solutions to structural problems such as misalignments,
partial alignments and missing entries. Finally, we successfully apply CQC to
the task of synonym extraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2562</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2562</id><created>2014-02-10</created><authors><author><keyname>Chaignaud</keyname><forenames>Nathalie</forenames><affiliation>LITIS</affiliation></author><author><keyname>Delavigne</keyname><forenames>Val&#xe9;rie</forenames><affiliation>LiDiFra</affiliation></author><author><keyname>Holzem</keyname><forenames>Maryvonne</forenames><affiliation>LiDiFra</affiliation></author><author><keyname>Kotowicz</keyname><forenames>Jean-Philippe</forenames><affiliation>LITIS</affiliation></author><author><keyname>Loisel</keyname><forenames>Alain</forenames><affiliation>LITIS</affiliation></author></authors><title>\'Etude cognitive des processus de construction d'une requ\^ete dans un
  syst\`eme de gestion de connaissances m\'edicales</title><categories>cs.IR cs.CL</categories><comments>29 pages</comments><proxy>ccsd</proxy><journal-ref>Revue Revue Technique et Science Informatiques 29 (2010) 991-1021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents the Cogni-CISMeF project, which aims at improving
medical information search in the CISMeF system (Catalog and Index of
French-language health resources) by including a conversational agent to
interact with the user in natural language. To study the cognitive processes
involved during the information search, a bottom-up methodology was adopted.
Experimentation has been set up to obtain human dialogs between a user (playing
the role of patient) dealing with medical information search and a CISMeF
expert refining the request. The analysis of these dialogs underlined the use
of discursive evidence: vocabulary, reformulation, implicit or explicit
expression of user intentions, conversational sequences, etc. A model of
artificial agent is proposed. It leads the user in its information search by
proposing to him examples, assistance and choices. This model was implemented
and integrated in the CISMeF system. ---- Cet article d\'ecrit le projet
Cogni-CISMeF qui propose un module de dialogue Homme-Machine \`a int\'egrer
dans le syst\`eme d'indexation de connaissances m\'edicales CISMeF (Catalogue
et Index des Sites M\'edicaux Francophones). Nous avons adopt\'e une d\'emarche
de mod\'elisation cognitive en proc\'edant \`a un recueil de corpus de
dialogues entre un utilisateur (jouant le r\^ole d'un patient) d\'esirant une
information m\'edicale et un expert CISMeF af inant cette demande pour
construire la requ\^ete. Nous avons analys\'e la structure des dialogues ainsi
obtenus et avons \'etudi\'e un certain nombre d'indices discursifs :
vocabulaire employ\'e, marques de reformulation, commentaires m\'eta et
\'epilinguistiques, expression implicite ou explicite des intentions de
l'utilisateur, encha\^inement conversationnel, etc. De cette analyse, nous
avons construit un mod\`ele d'agent artificiel dot\'e de capacit\'es cognitives
capables d'aider l'utilisateur dans sa t\^ache de recherche d'information. Ce
mod\`ele a \'et\'e impl\'ement\'e et int\'egr\'e dans le syst\`eme CISMeF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2583</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2583</id><created>2014-02-10</created><authors><author><keyname>Meng</keyname><forenames>Ziyang</forenames></author><author><keyname>Yang</keyname><forenames>Tao</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Coordinated Output Regulation of Heterogeneous Linear Systems under
  Switching Topologies</title><categories>cs.SY cs.MA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper constructs a framework to describe and study the coordinated
output regulation problem for multiple heterogeneous linear systems. Each agent
is modeled as a general linear multiple-input multiple-output system with an
autonomous exosystem which represents the individual offset from the group
reference for the agent. The multi-agent system as a whole has a group
exogenous state which represents the tracking reference for the whole group.
Under the constraints that the group exogenous output is only locally available
to each agent and that the agents have only access to their neighbors'
information, we propose observer-based feedback controllers to solve the
coordinated output regulation problem using output feedback information. A
high-gain approach is used and the information interactions are allowed to be
switched over a finite set of fixed networks containing both graphs that have a
directed spanning tree and graphs that do not. The fundamental relationship
between the information interactions, the dwell time, the non-identical
dynamics of different agents, and the high-gain parameters is given.
Simulations are shown to validate the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2589</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2589</id><created>2014-02-11</created><updated>2014-02-14</updated><authors><author><keyname>van Bevern</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Bredereck</keyname><forenames>Robert</forenames></author><author><keyname>Bulteau</keyname><forenames>Laurent</forenames></author><author><keyname>Chen</keyname><forenames>Jiehua</forenames></author><author><keyname>Froese</keyname><forenames>Vincent</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author><author><keyname>Woeginger</keyname><forenames>Gerhard J.</forenames></author></authors><title>Star Partitions of Perfect Graphs</title><categories>cs.DM cs.DS math.CO</categories><comments>Compared to the previous revision, makes the figures compatible with
  the TeX version used on arXiv and contains minor language corrections</comments><msc-class>05C70</msc-class><acm-class>G.2.2; F.2.2; G.2.1; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The partition of graphs into nice subgraphs is a central algorithmic problem
with strong ties to matching theory. We study the partitioning of undirected
graphs into stars, a problem known to be NP-complete even for the case of stars
on three vertices. We perform a thorough computational complexity study of the
problem on subclasses of perfect graphs and identify several polynomial-time
solvable and NP-hard cases, for example, on interval graphs, grid graphs, and
bipartite permutation graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2594</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2594</id><created>2014-02-11</created><authors><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>Online Nonparametric Regression</title><categories>stat.ML cs.LG math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish optimal rates for online regression for arbitrary classes of
regression functions in terms of the sequential entropy introduced in (Rakhlin,
Sridharan, Tewari, 2010). The optimal rates are shown to exhibit a phase
transition analogous to the i.i.d./statistical learning case, studied in
(Rakhlin, Sridharan, Tsybakov 2013). In the frequently encountered situation
when sequential entropy and i.i.d. empirical entropy match, our results point
to the interesting phenomenon that the rates for statistical learning with
squared loss and online nonparametric regression are the same.
  In addition to a non-algorithmic study of minimax regret, we exhibit a
generic forecaster that enjoys the established optimal rates. We also provide a
recipe for designing online regression algorithms that can be computationally
efficient. We illustrate the techniques by deriving existing and new
forecasters for the case of finite experts and for online linear regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2601</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2601</id><created>2014-02-11</created><updated>2014-07-24</updated><authors><author><keyname>Giryes</keyname><forenames>Raja</forenames></author><author><keyname>Needell</keyname><forenames>Deanna</forenames></author></authors><title>Near Oracle Performance and Block Analysis of Signal Space Greedy
  Methods</title><categories>math.NA cs.IT math.IT</categories><msc-class>94A20, 94A12, 62H12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sampling (CoSa) is a new methodology which demonstrates that
sparse signals can be recovered from a small number of linear measurements.
Greedy algorithms like CoSaMP have been designed for this recovery, and
variants of these methods have been adapted to the case where sparsity is with
respect to some arbitrary dictionary rather than an orthonormal basis. In this
work we present an analysis of the so-called Signal Space CoSaMP method when
the measurements are corrupted with mean-zero white Gaussian noise. We
establish near-oracle performance for recovery of signals sparse in some
arbitrary dictionary. In addition, we analyze the block variant of the method
for signals whose supports obey a block structure, extending the method into
the model-based compressed sensing framework. Numerical experiments confirm
that the block method significantly outperforms the standard method in these
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2603</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2603</id><created>2014-02-11</created><updated>2015-12-09</updated><authors><author><keyname>Li</keyname><forenames>Boyu</forenames></author><author><keyname>Zhu</keyname><forenames>Dengkui</forenames></author><author><keyname>Liang</keyname><forenames>Ping</forenames></author></authors><title>Small Cell In-Band Wireless Backhaul in Massive MIMO Systems: A
  Cooperation of Next-Generation Techniques</title><categories>cs.IT math.IT</categories><comments>accepted to journal</comments><journal-ref>IEEE TWC, Vol. 14, No. 12, Pages 7057-7069, Dec. 2015</journal-ref><doi>10.1109/TWC.2015.2464299</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multiple-inputmultiple-output (MIMO) systems, dense small-cells
(SCs), and full duplex are three candidate techniques for next-generation
communication systems. The cooperation of next-generation techniques could
offer more benefits, e.g., SC in-band wireless backhaul in massive MIMO
systems. In this paper, three strategies of SC in-band wireless backhaul in
massive MIMO systems are introduced and compared, i.e., complete time-division
duplex (CTDD), zero-division duplex (ZDD), and ZDD with interference rejection
(ZDD-IR). Simulation results demonstrate that SC in-band wireless backhaul has
the potential to improve the throughput for massive MIMO systems. Specifically,
among the three strategies, CTDD is the simplest one and could achieve decent
throughput improvement. Depending on conditions, with the self-interference
cancellation capability at SCs, ZDD could achieve better throughput than CTDD,
even with residual self-interference. Moreover, ZDD-IR requires the additional
interference rejection process at the BS compared to ZDD, but it could
generally achieve better throughput than CTDD and ZDD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2606</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2606</id><created>2014-02-11</created><authors><author><keyname>Mukherjee</keyname><forenames>Dibyendu</forenames></author></authors><title>A Fast Two Pass Multi-Value Segmentation Algorithm based on Connected
  Component Analysis</title><categories>cs.CV</categories><comments>9 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Connected component analysis (CCA) has been heavily used to label binary
images and classify segments. However, it has not been well-exploited to
segment multi-valued natural images. This work proposes a novel multi-value
segmentation algorithm that utilizes CCA to segment color images. A user
defined distance measure is incorporated in the proposed modified CCA to
identify and segment similar image regions. The raw output of the algorithm
consists of distinctly labelled segmented regions. The proposed algorithm has a
unique design architecture that provides several benefits: 1) it can be used to
segment any multi-channel multi-valued image; 2) the distance
measure/segmentation criteria can be application-specific and 3) an absolute
linear-time implementation allows easy extension for real-time video
segmentation. Experimental demonstrations of the aforesaid benefits are
presented along with the comparison results on multiple datasets with current
benchmark algorithms. A number of possible application areas are also
identified and results on real-time video segmentation has been presented to
show the promise of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2611</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2611</id><created>2014-02-11</created><authors><author><keyname>Abufouda</keyname><forenames>Mohammed</forenames></author></authors><title>Quality-aware Approach for Engineering Self-adaptive Software Systems</title><categories>cs.SE</categories><comments>Third International Conference on Information Technology Convergence
  and Services (ITCS 2014), January 2 ~ 4, 2014, Zurich, Switzerland. arXiv
  admin note: substantial text overlap with arXiv:1402.2144</comments><doi>10.5121/csit.2014.4117</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-adaptivity allows software systems to autonomously adjust their behavior
during run-time to reduce the cost complexities caused by manual maintenance.
In this paper, an approach for building an external adaptation engine for
self-adaptive software systems is proposed. In order to improve the quality of
self-adaptive software systems, this research addresses two challenges in
self-adaptive software systems. The first challenge is managing the complexity
of the adaptation space efficiently and the second is handling the run-time
uncertainty that hinders the adaptation process. This research utilizes
Case-based Reasoning as an adaptation engine along with utility functions for
realizing the managed system's requirements and handling uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2622</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2622</id><created>2014-02-11</created><authors><author><keyname>Baccelli</keyname><forenames>Emmanuel</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Hahm</keyname><forenames>Oliver</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>W&#xe4;hlisch</keyname><forenames>Matthias</forenames></author></authors><title>Spontaneous Wireless Networking to Counter Pervasive Monitoring</title><categories>cs.NI</categories><comments>W3C / IAB workshop on Strengthening the Internet Against Pervasive
  Monitoring (STRINT) (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several approaches can be employed to counter pervasive monitoring at large
scale on the Internet. One category of approaches aims to harden the current
Internet architecture and to increase the security of high profile targets
(data centers, exchange points etc.). Another category of approaches aims
instead for target dispersal, i.e. disabling systematic mass surveillance via
the elimination of existing vantage points, thus forcing surveillance efforts
to be more specific and personalized. This paper argues how networking
approaches that do not rely on central entities -- but rather on spontaneous
interaction, as locally as possible, between autonomous peer entities -- can
help realize target dispersal and thus counter pervasive monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2626</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2626</id><created>2014-02-11</created><updated>2014-05-13</updated><authors><author><keyname>Verschelde</keyname><forenames>Jan</forenames></author><author><keyname>Yu</keyname><forenames>Xiangcheng</forenames></author></authors><title>GPU acceleration of Newton's method for large systems of polynomial
  equations in double double and quad double arithmetic</title><categories>cs.DC cs.MS cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to compensate for the higher cost of double double and quad double
arithmetic when solving large polynomial systems, we investigate the
application of NVIDIA Tesla K20C general purpose graphics processing unit. The
focus on this paper is on Newton's method, which requires the evaluation of the
polynomials, their derivatives, and the solution of a linear system to compute
the update to the current approximation for the solution. The reverse mode of
algorithmic differentiation for a product of variables is rewritten in a binary
tree fashion so all threads in a block can collaborate in the computation. For
double arithmetic, the evaluation and differentiation problem is memory bound,
whereas for complex quad double arithmetic the problem is compute bound. With
acceleration we can double the dimension and get results that are twice as
accurate in about the same time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2634</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2634</id><created>2014-02-10</created><authors><author><keyname>Meng</keyname><forenames>Ziyang</forenames></author><author><keyname>Yang</keyname><forenames>Tao</forenames></author><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author><author><keyname>Hong</keyname><forenames>Yiguang</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Cooperative Set Aggregation for Multiple Lagrangian Systems</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the cooperative set tracking problem for a group of
Lagrangian systems. Each system observes a convex set as its local target. The
intersection of these local sets is the group aggregation target. We first
propose a control law based on each system's own target sensing and information
exchange with neighbors. With necessary connectivity for both cases of fixed
and switching communication graphs, multiple Lagrangian systems are shown to
achieve rendezvous on the intersection of all the local target sets while the
vectors of generalized coordinate derivatives are driven to zero. Then, we
introduce the collision avoidance control term into set aggregation control to
ensure group dispersion. By defining an ultimate bound on the final generalized
coordinate between each system and the intersection of all the local target
sets, we show that multiple Lagrangian systems approach a bounded region near
the intersection of all the local target sets while the collision avoidance is
guaranteed during the movement. In addition, the vectors of generalized
coordinate derivatives of all the mechanical systems are shown to be driven to
zero. Simulation results are given to validate the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2635</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2635</id><created>2014-02-11</created><authors><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Xu</keyname><forenames>Xiaoqing</forenames></author><author><keyname>Gao</keyname><forenames>Jhih-Rong</forenames></author><author><keyname>Pan</keyname><forenames>David Z.</forenames></author></authors><title>Methodology for standard cell compliance and detailed placement for
  triple patterning lithography</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the feature size of semiconductor process further scales to sub-16nm
technology node, triple patterning lithography (TPL) has been regarded one of
the most promising lithography candidates. M1 and contact layers, which are
usually deployed within standard cells, are most critical and complex parts for
modern digital designs. Traditional design flow that ignores TPL in early
stages may limit the potential to resolve all the TPL conflicts. In this paper,
we propose a coherent framework, including standard cell compliance and
detailed placement to enable TPL friendly design. Considering TPL constraints
during early design stages, such as standard cell compliance, improves the
layout decomposability. With the pre-coloring solutions of standard cells, we
present a TPL aware detailed placement, where the layout decomposition and
placement can be resolved simultaneously. Our experimental results show that,
with negligible impact on critical path delay, our framework can resolve the
conflicts much more easily, compared with the traditional physical design flow
and followed layout decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2637</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2637</id><created>2014-02-11</created><updated>2014-11-07</updated><authors><author><keyname>Choudhary</keyname><forenames>Sunav</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Identifiability Scaling Laws in Bilinear Inverse Problems</title><categories>cs.IT math.IT</categories><comments>25 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of ill-posed inverse problems in signal processing, like blind
deconvolution, matrix factorization, dictionary learning and blind source
separation share the common characteristic of being bilinear inverse problems
(BIPs), i.e. the observation model is a function of two variables and
conditioned on one variable being known, the observation is a linear function
of the other variable. A key issue that arises for such inverse problems is
that of identifiability, i.e. whether the observation is sufficient to
unambiguously determine the pair of inputs that generated the observation.
Identifiability is a key concern for applications like blind equalization in
wireless communications and data mining in machine learning. Herein, a unifying
and flexible approach to identifiability analysis for general conic prior
constrained BIPs is presented, exploiting a connection to low-rank matrix
recovery via lifting. We develop deterministic identifiability conditions on
the input signals and examine their satisfiability in practice for three
classes of signal distributions, viz. dependent but uncorrelated, independent
Gaussian, and independent Bernoulli. In each case, scaling laws are developed
that trade-off probability of robust identifiability with the complexity of the
rank two null space. An added appeal of our approach is that the rank two null
space can be partly or fully characterized for many bilinear problems of
interest (e.g. blind deconvolution). We present numerical experiments involving
variations on the blind deconvolution problem that exploit a characterization
of the rank two null space and demonstrate that the scaling laws offer good
estimates of identifiability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2641</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2641</id><created>2014-02-11</created><authors><author><keyname>Grippo</keyname><forenames>Luciano N.</forenames></author><author><keyname>Safe</keyname><forenames>Mart&#xed;n D.</forenames></author></authors><title>On circular-arc graphs having a model with no three arcs covering the
  circle</title><categories>cs.DM math.CO</categories><comments>15 pages, 3 figures. This paper originally appeared in proceedings of
  the XVI Congreso Latino-Iberoamericano de Investigaci\'on Operativa and the
  XLIV Simp\'osio Brasileiro de Pesquisa Operacional, September 24-28, 2012,
  Rio de Janeiro, Brazil. Anais do XLIV Simposio Brasileiro de Pesquisa
  Operacional, SOBRAPO, Rio de Janeiro, Brazil, 2012, pages 4093--4104.
  http://www.din.uem.br/sbpo/sbpo2012/pdf/arq0518.pdf</comments><msc-class>05C62, 05C75</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An interval graph is the intersection graph of a finite set of intervals on a
line and a circular-arc graph is the intersection graph of a finite set of arcs
on a circle. While a forbidden induced subgraph characterization of interval
graphs was found fifty years ago, finding an analogous characterization for
circular-arc graphs is a long-standing open problem. In this work, we study the
intersection graphs of finite sets of arcs on a circle no three of which cover
the circle, known as normal Helly circular-arc graphs. Those circular-arc
graphs which are minimal forbidden induced subgraphs for the class of normal
Helly circular-arc graphs were identified by Lin, Soulignac, and Szwarcfiter,
who also posed the problem of determining the remaining minimal forbidden
induced subgraphs. In this work, we solve their problem, obtaining the complete
list of minimal forbidden induced subgraphs for the class of normal Helly
circular-arc graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2642</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2642</id><created>2014-02-10</created><authors><author><keyname>Compagnoni</keyname><forenames>Marco</forenames></author><author><keyname>Notari</keyname><forenames>Roberto</forenames></author><author><keyname>Antonacci</keyname><forenames>Fabio</forenames></author><author><keyname>Sarti</keyname><forenames>Augusto</forenames></author></authors><title>A comprehensive analysis of the geometry of TDOA maps in localisation
  problems</title><categories>math-ph cs.CE cs.SD gr-qc math.AC math.MP</categories><comments>51 pages (3 appendices of 12 pages), 12 figures</comments><journal-ref>Inverse Problems, Vol. 30, Number 3, Pages 035004, 2014</journal-ref><doi>10.1088/0266-5611/30/3/035004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this manuscript we consider the well-established problem of TDOA-based
source localization and propose a comprehensive analysis of its solutions for
arbitrary sensor measurements and placements. More specifically, we define the
TDOA map from the physical space of source locations to the space of range
measurements (TDOAs), in the specific case of three receivers in 2D space. We
then study the identifiability of the model, giving a complete analytical
characterization of the image of this map and its invertibility. This analysis
has been conducted in a completely mathematical fashion, using many different
tools which make it valid for every sensor configuration. These results are the
first step towards the solution of more general problems involving, for
example, a larger number of sensors, uncertainty in their placement, or lack of
synchronization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2648</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2648</id><created>2014-02-10</created><authors><author><keyname>Fan</keyname><forenames>Deliang</forenames></author><author><keyname>Sharad</keyname><forenames>Mrigank</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Design and Synthesis of Ultra Low Energy Spin-Memristor Threshold Logic</title><categories>cs.ET cond-mat.mes-hall</categories><comments>this paper is submitted to IEEE Transactions on Nanotechnology. It is
  currently under review</comments><journal-ref>IEEE Transactions on Nanotechnology, Volume:13, Issue: 3, 2014</journal-ref><doi>10.1109/TNANO.2014.2312177</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A threshold logic gate (TLG) performs weighted sum of multiple inputs and
compares the sum with a threshold. We propose Spin-Memeristor Threshold Logic
(SMTL) gates, which employ memristive cross-bar array (MCA) to perform
current-mode summation of binary inputs, whereas, the low-voltage
fast-switching spintronic threshold devices (STD) carry out the threshold
operation in an energy efficient manner. Field programmable SMTL gate arrays
can operate at a small terminal voltage of ~50mV, resulting in ultra-low power
consumption in gates as well as programmable interconnect networks. We evaluate
the performance of SMTL using threshold logic synthesis. Results for common
benchmarks show that SMTL based programmable logic hardware can be more than
100x energy efficient than state of the art CMOS FPGA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2664</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2664</id><created>2014-02-11</created><updated>2015-03-15</updated><authors><author><keyname>van Bevern</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Bredereck</keyname><forenames>Robert</forenames></author><author><keyname>Chen</keyname><forenames>Jiehua</forenames></author><author><keyname>Froese</keyname><forenames>Vincent</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author><author><keyname>Woeginger</keyname><forenames>Gerhard J.</forenames></author></authors><title>Network-Based Vertex Dissolution</title><categories>cs.DM cs.DS cs.SI math.CO</categories><comments>Version accepted at SIAM Journal on Discrete Mathematics</comments><msc-class>05C21</msc-class><acm-class>G.2.1; G.2.2; F.2.2</acm-class><journal-ref>SIAM Journal on Discrete Mathematics 29(2):888-914, 2015</journal-ref><doi>10.1137/140978880</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a graph-theoretic vertex dissolution model that applies to a
number of redistribution scenarios such as gerrymandering in political
districting or work balancing in an online situation. The central aspect of our
model is the deletion of certain vertices and the redistribution of their load
to neighboring vertices in a completely balanced way.
  We investigate how the underlying graph structure, the knowledge of which
vertices should be deleted, and the relation between old and new vertex loads
influence the computational complexity of the underlying graph problems. Our
results establish a clear borderline between tractable and intractable cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2667</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2667</id><created>2014-02-11</created><authors><author><keyname>Liang</keyname><forenames>Tengyuan</forenames></author><author><keyname>Narayanan</keyname><forenames>Hariharan</forenames></author><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author></authors><title>On Zeroth-Order Stochastic Convex Optimization via Random Walks</title><categories>cs.LG stat.ML</categories><comments>10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for zeroth order stochastic convex optimization that
attains the suboptimality rate of $\tilde{\mathcal{O}}(n^{7}T^{-1/2})$ after
$T$ queries for a convex bounded function $f:{\mathbb R}^n\to{\mathbb R}$. The
method is based on a random walk (the \emph{Ball Walk}) on the epigraph of the
function. The randomized approach circumvents the problem of gradient
estimation, and appears to be less sensitive to noisy function evaluations
compared to noiseless zeroth order methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2671</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2671</id><created>2014-02-11</created><authors><author><keyname>Bild</keyname><forenames>David R.</forenames></author><author><keyname>Liu</keyname><forenames>Yue</forenames></author><author><keyname>Dick</keyname><forenames>Robert P.</forenames></author><author><keyname>Mao</keyname><forenames>Z. Morley</forenames></author><author><keyname>Wallach</keyname><forenames>Dan S.</forenames></author></authors><title>Aggregate Characterization of User Behavior in Twitter and Analysis of
  the Retweet Graph</title><categories>cs.SI physics.soc-ph</categories><comments>17 pages, 21 figures</comments><journal-ref>ACM Trans. Internet Technol. 15, 1, Article 4 (February 2015), 24
  pages</journal-ref><doi>10.1145/2700060</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most previous analysis of Twitter user behavior is focused on individual
information cascades and the social followers graph. We instead study aggregate
user behavior and the retweet graph with a focus on quantitative descriptions.
We find that the lifetime tweet distribution is a type-II discrete Weibull
stemming from a power law hazard function, the tweet rate distribution,
although asymptotically power law, exhibits a lognormal cutoff over finite
sample intervals, and the inter-tweet interval distribution is power law with
exponential cutoff. The retweet graph is small-world and scale-free, like the
social graph, but is less disassortative and has much stronger clustering.
These differences are consistent with it better capturing the real-world social
relationships of and trust between users. Beyond just understanding and
modeling human communication patterns and social networks, applications for
alternative, decentralized microblogging systems-both predicting real-word
performance and detecting spam-are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2673</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2673</id><created>2014-02-11</created><authors><author><keyname>Nalepa</keyname><forenames>Jakub</forenames></author><author><keyname>Kawulok</keyname><forenames>Michal</forenames></author></authors><title>Real-Time Hand Shape Classification</title><categories>cs.CV</categories><comments>11 pages</comments><doi>10.1007/978-3-319-06932-6_35</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of hand shape classification is challenging since a hand is
characterized by a large number of degrees of freedom. Numerous shape
descriptors have been proposed and applied over the years to estimate and
classify hand poses in reasonable time. In this paper we discuss our parallel
framework for real-time hand shape classification applicable in real-time
applications. We show how the number of gallery images influences the
classification accuracy and execution time of the parallel algorithm. We
present the speedup and efficiency analyses that prove the efficacy of the
parallel implementation. Noteworthy, different methods can be used at each step
of our parallel framework. Here, we combine the shape contexts with the
appearance-based techniques to enhance the robustness of the algorithm and to
increase the classification score. An extensive experimental study proves the
superiority of the proposed approach over existing state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2676</identifier>
 <datestamp>2014-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2676</id><created>2014-02-11</created><updated>2014-08-21</updated><authors><author><keyname>Yun</keyname><forenames>Hyokun</forenames></author><author><keyname>Raman</keyname><forenames>Parameswaran</forenames></author><author><keyname>Vishwanathan</keyname><forenames>S. V. N.</forenames></author></authors><title>Ranking via Robust Binary Classification and Parallel Parameter
  Estimation in Large-Scale Data</title><categories>stat.ML cs.DC cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose RoBiRank, a ranking algorithm that is motivated by observing a
close connection between evaluation metrics for learning to rank and loss
functions for robust classification. The algorithm shows a very competitive
performance on standard benchmark datasets against other representative
algorithms in the literature. On the other hand, in large scale problems where
explicit feature vectors and scores are not given, our algorithm can be
efficiently parallelized across a large number of machines; for a task that
requires 386,133 x 49,824,519 pairwise interactions between items to be ranked,
our algorithm finds solutions that are of dramatically higher quality than that
can be found by a state-of-the-art competitor algorithm, given the same amount
of wall-clock time for computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2680</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2680</id><created>2014-02-11</created><authors><author><keyname>Manzano</keyname><forenames>Marc</forenames></author><author><keyname>Fagertun</keyname><forenames>Anna Manolova</forenames></author><author><keyname>Ruepp</keyname><forenames>Sarah</forenames></author><author><keyname>Calle</keyname><forenames>Eusebi</forenames></author><author><keyname>Scoglio</keyname><forenames>Caterina</forenames></author><author><keyname>Sydney</keyname><forenames>Ali</forenames></author><author><keyname>de la Oliva</keyname><forenames>Antonio</forenames></author><author><keyname>Mu&#xf1;oz</keyname><forenames>Alfonso</forenames></author></authors><title>Unveiling Potential Failure Propagation Scenarios in Core Transport
  Networks</title><categories>cs.NI cs.DC</categories><comments>Submitted to IEEE Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The contemporary society has become more dependent on telecommunication
networks. Novel services and technologies supported by such networks, such as
cloud computing or e-Health, hold a vital role in modern day living.
Large-scale failures are prone to occur, thus being a constant threat to
business organizations and individuals. To the best of our knowledge, there are
no publicly available reports regarding failure propagation in core transport
networks. Furthermore, Software Defined Networking (SDN) is becoming more
prevalent in our society and we can envision more SDN-controlled Backbone
Transport Networks (BTNs) in the future. For this reason, we investigate the
main motivations that could lead to epidemic-like failures in BTNs and SDNTNs.
To do so, we enlist the expertise of several research groups with significant
background in epidemics, network resiliency, and security. In addition, we
consider the experiences of three network providers. Our results illustrate
that Dynamic Transport Networks (DTNs) are prone to epidemic-like failures.
Moreover, we propose different situations in which a failure can propagate in
SDNTNs. We believe that the key findings will aid network engineers and the
scientific community to predict this type of disastrous failure scenario and
plan adequate survivability strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2681</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2681</id><created>2014-02-11</created><updated>2014-04-13</updated><authors><author><keyname>Zheng</keyname><forenames>Liang</forenames></author><author><keyname>Wang</keyname><forenames>Shengjin</forenames></author><author><keyname>Liu</keyname><forenames>Ziqiong</forenames></author><author><keyname>Tian</keyname><forenames>Qi</forenames></author></authors><title>Packing and Padding: Coupled Multi-index for Accurate Image Retrieval</title><categories>cs.CV</categories><comments>8 pages, 7 figures, 6 tables. Accepted to CVPR 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In Bag-of-Words (BoW) based image retrieval, the SIFT visual word has a low
discriminative power, so false positive matches occur prevalently. Apart from
the information loss during quantization, another cause is that the SIFT
feature only describes the local gradient distribution. To address this
problem, this paper proposes a coupled Multi-Index (c-MI) framework to perform
feature fusion at indexing level. Basically, complementary features are coupled
into a multi-dimensional inverted index. Each dimension of c-MI corresponds to
one kind of feature, and the retrieval process votes for images similar in both
SIFT and other feature spaces. Specifically, we exploit the fusion of local
color feature into c-MI. While the precision of visual match is greatly
enhanced, we adopt Multiple Assignment to improve recall. The joint cooperation
of SIFT and color features significantly reduces the impact of false positive
matches.
  Extensive experiments on several benchmark datasets demonstrate that c-MI
improves the retrieval accuracy significantly, while consuming only half of the
query time compared to the baseline. Importantly, we show that c-MI is well
complementary to many prior techniques. Assembling these methods, we have
obtained an mAP of 85.8% and N-S score of 3.85 on Holidays and Ukbench
datasets, respectively, which compare favorably with the state-of-the-arts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2683</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2683</id><created>2014-02-11</created><updated>2014-03-20</updated><authors><author><keyname>Deleforge</keyname><forenames>Antoine</forenames></author><author><keyname>Forbes</keyname><forenames>Florence</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author></authors><title>Acoustic Space Learning for Sound Source Separation and Localization on
  Binaural Manifolds</title><categories>cs.SD</categories><comments>19 pages, 9 figures, 3 tables</comments><journal-ref>International Journal of Neural Systems 25(1) 2015</journal-ref><doi>10.1142/S0129065714400036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problems of modeling the acoustic space
generated by a full-spectrum sound source and of using the learned model for
the localization and separation of multiple sources that simultaneously emit
sparse-spectrum sounds. We lay theoretical and methodological grounds in order
to introduce the binaural manifold paradigm. We perform an in-depth study of
the latent low-dimensional structure of the high-dimensional interaural
spectral data, based on a corpus recorded with a human-like audiomotor robot
head. A non-linear dimensionality reduction technique is used to show that
these data lie on a two-dimensional (2D) smooth manifold parameterized by the
motor states of the listener, or equivalently, the sound source directions. We
propose a probabilistic piecewise affine mapping model (PPAM) specifically
designed to deal with high-dimensional data exhibiting an intrinsic piecewise
linear structure. We derive a closed-form expectation-maximization (EM)
procedure for estimating the model parameters, followed by Bayes inversion for
obtaining the full posterior density function of a sound source direction. We
extend this solution to deal with missing data and redundancy in real world
spectrograms, and hence for 2D localization of natural sound sources such as
speech. We further generalize the model to the challenging case of multiple
sound sources and we propose a variational EM framework. The associated
algorithm, referred to as variational EM for source separation and localization
(VESSL) yields a Bayesian estimation of the 2D locations and time-frequency
masks of all the sources. Comparisons of the proposed approach with several
existing methods reveal that the combination of acoustic-space learning with
Bayesian inference enables our method to outperform state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2695</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2695</id><created>2014-02-11</created><authors><author><keyname>Deal</keyname><forenames>Laura</forenames></author></authors><title>Visualizing Digital Collections</title><categories>cs.DL cs.IR</categories><comments>30 pages, 4 figures. This is a preprint of an article submitted for
  consideration in the Technical Services Quarterly (2014); Technical Services
  Quarterly is available online at:
  http://www.tandfonline.com/toc/wtsq20/current</comments><doi>10.1080/07317131.2015.972871</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data visualizations can greatly enhance search in digital collections by
providing information about the scope and context of a collection and allowing
users to more easily browse and explore the contents. This article discusses
the benefits of incorporating visualizations into digital collections based on
the experiences of the Cold War International History Project (CWIHP) in
developing a user-friendly tool for searching and visualizing the project's
complex set of historical documents. The paper concludes with a tutorial on
using the free Library of Congress tool Viewshare to create visualizations
based on real data from the CWIHP Digital Archive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2696</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2696</id><created>2014-02-11</created><authors><author><keyname>Standish</keyname><forenames>Russell K.</forenames></author></authors><title>Information Based Complexity of Networks</title><categories>nlin.AO cs.SI</categories><comments>arXiv admin note: substantial text overlap with arXiv:0805.0685</comments><msc-class>68Q19, 68Q30, 05C99</msc-class><acm-class>F.1.3</acm-class><journal-ref>in {\em Advances in Network Complexity}, Dehmer et al. (eds)
  (2013), Wiley-VCH, pp 209--247</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Review article of various complexity measures of networks
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2698</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2698</id><created>2014-02-11</created><authors><author><keyname>Oliveira</keyname><forenames>Mateus de Oliveira</forenames></author></authors><title>Automated Verification, Synthesis and Correction of Concurrent Systems
  via MSO Logic</title><categories>cs.LO cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we provide algorithmic solutions to five fundamental problems
concerning the verification, synthesis and correction of concurrent systems
that can be modeled by bounded p/t-nets. We express concurrency via partial
orders and assume that behavioral specifications are given via monadic second
order logic. A c-partial-order is a partial order whose Hasse diagram can be
covered by c paths. For a finite set T of transitions, we let P(c,T,\phi)
denote the set of all T-labelled c-partial-orders satisfying \phi. If N=(P,T)
is a p/t-net we let P(N,c) denote the set of all c-partially-ordered runs of N.
A (b, r)-bounded p/t-net is a b-bounded p/t-net in which each place appears
repeated at most r times. We solve the following problems:
  1. Verification: given an MSO formula \phi and a bounded p/t-net N determine
whether P(N,c)\subseteq P(c,T,\phi), whether P(c,T,\phi)\subseteq P(N,c), or
whether P(N,c)\cap P(c,T,\phi)=\emptyset.
  2. Synthesis from MSO Specifications: given an MSO formula \phi, synthesize a
semantically minimal (b,r)-bounded p/t-net N satisfying P(c,T,\phi)\subseteq
P(N, c).
  3. Semantically Safest Subsystem: given an MSO formula \phi defining a set of
safe partial orders, and a b-bounded p/t-net N, possibly containing unsafe
behaviors, synthesize the safest (b,r)-bounded p/t-net N' whose behavior lies
in between P(N,c)\cap P(c,T,\phi) and P(N,c).
  4. Behavioral Repair: given two MSO formulas \phi and \psi, and a b-bounded
p/t-net N, synthesize a semantically minimal (b,r)-bounded p/t net N' whose
behavior lies in between P(N,c) \cap P(c,T,\phi) and P(c,T,\psi).
  5. Synthesis from Contracts: given an MSO formula \phi^yes specifying a set
of good behaviors and an MSO formula \phi^no specifying a set of bad behaviors,
synthesize a semantically minimal (b,r)-bounded p/t-net N such that
P(c,T,\phi^yes) \subseteq P(N,c) but P(c,T,\phi^no ) \cap P(N,c)=\emptyset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2699</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2699</id><created>2014-02-11</created><updated>2014-06-07</updated><authors><author><keyname>Gong</keyname><forenames>Neil Zhenqiang</forenames></author><author><keyname>Wang</keyname><forenames>Di</forenames></author></authors><title>On the Security of Trustee-based Social Authentications</title><categories>cs.CR cs.SI</categories><comments>13 pages. Accepted by IEEE Transactions on Information Forensics and
  Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, authenticating users with the help of their friends (i.e.,
trustee-based social authentication) has been shown to be a promising backup
authentication mechanism. A user in this system is associated with a few
trustees that were selected from the user's friends. When the user wants to
regain access to the account, the service provider sends different verification
codes to the user's trustees. The user must obtain at least k (i.e., recovery
threshold) verification codes from the trustees before being directed to reset
his or her password.
  In this paper, we provide the first systematic study about the security of
trustee-based social authentications. Specifically, we first introduce a novel
framework of attacks, which we call forest fire attacks. In these attacks, an
attacker initially obtains a small number of compromised users, and then the
attacker iteratively attacks the rest of users by exploiting trustee-based
social authentications. Then, we construct a probabilistic model to formalize
the threats of forest fire attacks and their costs for attackers. Moreover, we
introduce various defense strategies. Finally, we apply our framework to
extensively evaluate various concrete attack and defense strategies using three
real-world social network datasets. Our results have strong implications for
the design of more secure trustee-based social authentications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2700</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2700</id><created>2014-02-11</created><updated>2014-07-25</updated><authors><author><keyname>Hubi&#x10d;ka</keyname><forenames>Jan</forenames></author><author><keyname>Ne&#x161;et&#x159;il</keyname><forenames>Jaroslav</forenames></author></authors><title>Bowtie-free graphs have a Ramsey lift</title><categories>math.CO cs.DM</categories><comments>22 pages, 4 figures. Version with minor corrections and new section
  on ordering and expansion property</comments><msc-class>05C55, 05C15, 05D10, 03C35</msc-class><acm-class>G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A bowtie is a graph consisting of two triangles with one vertex identified.
We show that the class of all (countable) graphs not containing a bowtie as a
subgraph have a Ramsey lift (expansion). This solves one of the old problems in
the area and it is the first non-trivial Ramsey class with a non-trivial
algebraic closure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2701</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2701</id><created>2014-02-11</created><authors><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author><author><keyname>Malkhi</keyname><forenames>Dahlia</forenames></author></authors><title>Optimal Gossip with Direct Addressing</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gossip algorithms spread information by having nodes repeatedly forward
information to a few random contacts. By their very nature, gossip algorithms
tend to be distributed and fault tolerant. If done right, they can also be fast
and message-efficient. A common model for gossip communication is the random
phone call model, in which in each synchronous round each node can PUSH or PULL
information to or from a random other node. For example, Karp et al. [FOCS
2000] gave algorithms in this model that spread a message to all nodes in
$\Theta(\log n)$ rounds while sending only $O(\log \log n)$ messages per node
on average.
  Recently, Avin and Els\&quot;asser [DISC 2013], studied the random phone call
model with the natural and commonly used assumption of direct addressing.
Direct addressing allows nodes to directly contact nodes whose ID (e.g., IP
address) was learned before. They show that in this setting, one can &quot;break the
$\log n$ barrier&quot; and achieve a gossip algorithm running in $O(\sqrt{\log n})$
rounds, albeit while using $O(\sqrt{\log n})$ messages per node.
  We study the same model and give a simple gossip algorithm which spreads a
message in only $O(\log \log n)$ rounds. We also prove a matching $\Omega(\log
\log n)$ lower bound which shows that this running time is best possible. In
particular we show that any gossip algorithm takes with high probability at
least $0.99 \log \log n$ rounds to terminate. Lastly, our algorithm can be
tweaked to send only $O(1)$ messages per node on average with only $O(\log n)$
bits per message. Our algorithm therefore simultaneously achieves the optimal
round-, message-, and bit-complexity for this setting. As all prior gossip
algorithms, our algorithm is also robust against failures. In particular, if in
the beginning an oblivious adversary fails any $F$ nodes our algorithm still,
with high probability, informs all but $o(F)$ surviving nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2703</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2703</id><created>2014-02-11</created><authors><author><keyname>Tsang</keyname><forenames>Jeffrey</forenames></author><author><keyname>Pereira</keyname><forenames>Rajesh</forenames></author></authors><title>Taking all positive eigenvectors is suboptimal in classical
  multidimensional scaling</title><categories>math.ST cs.NA math.OC stat.TH</categories><comments>13 pages, 1 figure, 1 table, 1 supplementary file</comments><msc-class>91C15, 90C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is hard to overstate the importance of multidimensional scaling as an
analysis technique in the broad sciences. Classical, or Torgerson
multidimensional scaling is one of the main variants, with the advantage that
it has a closed-form analytic solution. However, this solution is exact if and
only if the distances are Euclidean. Conversely, there has been comparatively
little discussion on what to do in the presence of negative eigenvalues: the
intuitive solution, prima facie justifiable in least-squares terms, is to take
every positive eigenvector as a dimension. We show that this, minimizing
least-squares to the centred distances instead of the true distances, is
suboptimal - throwing away positive eigenvectors can decrease the error even as
we project to fewer dimensions. We provide provably better methods for handling
this common case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2704</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2704</id><created>2014-02-11</created><authors><author><keyname>Watkins</keyname><forenames>Chris</forenames></author><author><keyname>Buttkewitz</keyname><forenames>Yvonne</forenames></author></authors><title>Sex as Gibbs Sampling: a probability model of evolution</title><categories>q-bio.PE cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that evolutionary computation can be implemented as standard
Markov-chain Monte-Carlo (MCMC) sampling. With some care, `genetic algorithms'
can be constructed that are reversible Markov chains that satisfy detailed
balance; it follows that the stationary distribution of populations is a Gibbs
distribution in a simple factorised form. For some standard and popular
nonparametric probability models, we exhibit Gibbs-sampling procedures that are
plausible genetic algorithms. At mutation-selection equilibrium, a population
of genomes is analogous to a sample from a Bayesian posterior, and the genomes
are analogous to latent variables. We suggest this is a general, tractable, and
insightful formulation of evolutionary computation in terms of standard machine
learning concepts and techniques.
  In addition, we show that evolutionary processes in which selection acts by
differences in fecundity are not reversible, and also that it is not possible
to construct reversible evolutionary models in which each child is produced by
only two parents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2707</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2707</id><created>2014-02-11</created><updated>2014-02-17</updated><authors><author><keyname>Tanbourgi</keyname><forenames>Ralph</forenames></author><author><keyname>Singh</keyname><forenames>Sarabjot</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author><author><keyname>Jondral</keyname><forenames>Friedrich K.</forenames></author></authors><title>Analysis of Non-Coherent Joint-Transmission Cooperation in Heterogeneous
  Cellular Networks</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE ICC 2014, 6 pages, 5 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Base station (BS) cooperation is set to play a key role in managing
interference in dense heterogeneous cellular networks (HCNs). Non-coherent
joint transmission (JT) is particularly appealing due to its low complexity,
smaller overhead, and ability for load balancing. However, a general analysis
of this technique is difficult mostly due to the lack of tractable models. This
paper addresses this gap and presents a tractable model for analyzing
non-coherent JT in HCNs, while incorporating key system parameters such as
user-centric BS clustering and channel-dependent cooperation activation.
Assuming all BSs of each tier follow a stationary Poisson point process, the
coverage probability for non-coherent JT is derived. Using the developed model,
it is shown that for small cooperative clusters of small-cell BSs, non-coherent
JT by small cells provides spectral efficiency gains without significantly
increasing cell load. Further, when cooperation is aggressively triggered
intra-cluster frequency reuse within small cells is favorable over
intra-cluster coordinated scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2708</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2708</id><created>2014-02-11</created><updated>2014-02-14</updated><authors><author><keyname>Zhu</keyname><forenames>Minghui</forenames></author><author><keyname>Otte</keyname><forenames>Michael</forenames></author><author><keyname>Chaudhari</keyname><forenames>Pratik</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author></authors><title>Game theoretic controller synthesis for multi-robot motion planning Part
  I : Trajectory based algorithms</title><categories>cs.MA cs.GT cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a class of multi-robot motion planning problems where each robot
is associated with multiple objectives and decoupled task specifications. The
problems are formulated as an open-loop non-cooperative differential game. A
distributed anytime algorithm is proposed to compute a Nash equilibrium of the
game. The following properties are proven: (i) the algorithm asymptotically
converges to the set of Nash equilibrium; (ii) for scalar cost functionals, the
price of stability equals one; (iii) for the worst case, the computational
complexity and communication cost are linear in the robot number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2709</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2709</id><created>2014-02-11</created><updated>2014-05-19</updated><authors><author><keyname>Gunn</keyname><forenames>Lachlan J.</forenames></author><author><keyname>Allison</keyname><forenames>Andrew</forenames></author><author><keyname>Abbott</keyname><forenames>Derek</forenames></author></authors><title>A directional coupler attack against the Kish key distribution system</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Kish key distribution system has been proposed as a class ical
alternative to quantum key distribution. The idealized Kish scheme elegantly
promise s secure key distribution by exploiting thermal noise in a transmission
line. However, we demonstrate that it is vulnerable to nonidealities in its
components, such as the finite resistance of the transmission line connecting
its endpoints. We introduce a novel attack against this nonideality using
directional wave measurements, and experimentally demonstrate its efficacy. Our
attack is based on causality: in a spatially distributed system, propagation is
needed for thermodynamic equilibration, and that leaks information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2710</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2710</id><created>2014-02-11</created><authors><author><keyname>Wang</keyname><forenames>L.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Haardt</keyname><forenames>M.</forenames></author></authors><title>Direction Finding Algorithms with Joint Iterative Subspace Optimization</title><categories>cs.DS</categories><comments>11 figures, 4 tables. IEEE Transactions on Aerospace and Electronic
  Systems, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a reduced-rank scheme with joint iterative optimization is
presented for direction of arrival estimation. A rank-reduction matrix and an
auxiliary reduced-rank parameter vector are jointly optimized to calculate the
output power with respect to each scanning angle. Subspace algorithms to
estimate the rank-reduction matrix and the auxiliary vector are proposed.
Simulations are performed to show that the proposed algorithms achieve an
enhanced performance over existing algorithms in the studied scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2712</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2712</id><created>2014-02-11</created><updated>2014-04-17</updated><authors><author><keyname>Liu</keyname><forenames>Jiamou</forenames></author><author><keyname>Ross</keyname><forenames>Kostya</forenames></author></authors><title>Dynamic Partial Sorting</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The dynamic partial sorting problem asks for an algorithm that maintains
lists of numbers under the link, cut and change value operations, and queries
the sorted sequence of the $k$ least numbers in one of the lists. We first
solve the problem in $O(k\log (n))$ time for queries and $O(\log (n))$ time for
updates using the tournament tree data structure, where $n$ is the number of
elements in the lists. We then introduce a layered tournament tree data
structure and solve the same problem in $O(\log_\varphi^* (n) k\log (k))$ time
for queries and $O\left(\log (n)\cdot\log^2\log (n)\right)$ for updates, where
$\varphi$ is the golden ratio and $\log_\varphi^*(n)$ is the iterated
logarithmic function with base $\varphi$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2720</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2720</id><created>2014-02-11</created><authors><author><keyname>Jiang</keyname><forenames>Hong</forenames></author><author><keyname>Huang</keyname><forenames>Gang</forenames></author><author><keyname>Wilford</keyname><forenames>Paul</forenames></author></authors><title>Noise Analysis for Lensless Compressive Imaging</title><categories>cs.CV</categories><comments>11 pages, 12 figures. arXiv admin note: text overlap with
  arXiv:1402.0785</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the signal to noise ratio (SNR) in a recently proposed lensless
compressive imaging architecture. The architecture consists of a sensor of a
single detector element and an aperture assembly of an array of aperture
elements, each of which has a programmable transmittance. This lensless
compressive imaging architecture can be used in conjunction with compressive
sensing to capture images in a compressed form of compressive measurements. In
this paper, we perform noise analysis of this lensless compressive imaging
architecture and compare it with pinhole aperture imaging and lens aperture
imaging. We will show that the SNR in the lensless compressive imaging is
independent of the image resolution, while that in either pinhole aperture
imaging or lens aperture imaging decreases as the image resolution increases.
Consequently, the SNR in the lensless compressive imaging can be much higher if
the image resolution is large enough.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2733</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2733</id><created>2014-02-11</created><updated>2014-02-18</updated><authors><author><keyname>Mulherkar</keyname><forenames>Jaideep</forenames></author></authors><title>An efficient algorithm for the entropy rate of a hidden Markov model
  with unambiguous symbols</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate an efficient formula to compute the entropy rate $H(\mu)$ of a
hidden Markov process with $q$ output symbols where at least one symbol is
unambiguously received. Using an approximation to $H(\mu)$ to the first $N$
terms we give a $O(Nq^3$) algorithm to compute the entropy rate of the hidden
Markov model. We use the algorithm to estimate the entropy rate when the
parameters of the hidden Markov model are unknown.In the case of $q =2$ the
process is the output of the Z-channel and we use this fact to give bounds on
the capacity of the Gilbert channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2735</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2735</id><created>2014-02-11</created><authors><author><keyname>Caldwell</keyname><forenames>Timothy M.</forenames></author><author><keyname>Coleman</keyname><forenames>Dave</forenames></author><author><keyname>Correll</keyname><forenames>Nikolaus</forenames></author></authors><title>Optimal Parameter Identification for Discrete Mechanical Systems with
  Application to Flexible Object Manipulation</title><categories>cs.RO</categories><comments>8 pages, 5 figures, submitted to IROS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for system identification of flexible objects by
measuring forces and displacement during interaction with a manipulating arm.
We model the object's structure and flexibility by a chain of rigid bodies
connected by torsional springs. Unlike previous work, the proposed optimal
control approach using variational integrators allows identification of closed
loops, which include the robot arm itself. This allows using the resulting
models for planning in configuration space of the robot. In order to solve the
resulting problem efficiently, we develop a novel method for fast discrete-time
adjoint-based gradient calculation. The feasibility of the approach is
demonstrated using full physics simulation in trep and using data recorded from
a 7-DOF series elastic robot arm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2741</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2741</id><created>2014-02-12</created><authors><author><keyname>Papamichail</keyname><forenames>Dimitris</forenames></author><author><keyname>Caputi</keyname><forenames>Thomas</forenames></author><author><keyname>Papamichail</keyname><forenames>Georgios</forenames></author></authors><title>The Level Ancestor Problem in Practice</title><categories>cs.DS</categories><comments>12 pages, 5 figures, 2 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Given a rooted tree T, the level ancestor problem is the problem of answering
queries of the form LA(v, d), which identify the level d ancestor of a node v
in the tree. Several algorithms of varied complexity have been proposed in the
literature, including optimal solutions that preprocess T in linear bounded
time and proceed to answer queries in constant bounded time. Despite its
significance and numerous applications, to date there have been no comparative
studies of the performance of these algorithms and no implementations are
widely available. In our experimental study we have implemented and compared
several solutions for the level ancestor problem, including two optimal
algorithms, and examine their space requirements and time performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2744</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2744</id><created>2014-02-12</created><authors><author><keyname>Wei</keyname><forenames>Bo</forenames></author><author><keyname>Varshney</keyname><forenames>Ambuj</forenames></author><author><keyname>Hu</keyname><forenames>Wen</forenames></author><author><keyname>Patwari</keyname><forenames>Neal</forenames></author><author><keyname>Voigt</keyname><forenames>Thiemo</forenames></author><author><keyname>Chou</keyname><forenames>Chun Tung</forenames></author></authors><title>dRTI: Directional Radio Tomographic Imaging</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio tomographic imaging (RTI) enables device free localisation of people
and objects in many challenging environments and situations. Its basic
principle is to detect the changes in the statistics of some radio quality
measurements in order to infer the presence of people and objects in the radio
path. However, the localisation accuracy of RTI suffers from complicated radio
propagation behaviours such as multipath fading and shadowing. In order to
improve RTI localisation accuracy, we propose to use inexpensive and energy
efficient electronically switched directional (ESD) antennas to improve the
quality of radio link behaviour observations, and therefore, the localisation
accuracy of RTI. We implement a directional RTI (dRTI) system to understand how
directional antennas can be used to improve RTI localisation accuracy. We also
study the impact of the choice of antenna directions on the localisation
accuracy of dRTI and propose methods to effectively choose informative antenna
directions to improve localisation accuracy while reducing overhead. We
evaluate the performance of dRTI in diverse indoor environments and show that
dRTI significantly outperforms the existing RTI localisation methods based on
omni-directional antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2745</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2745</id><created>2014-02-12</created><authors><author><keyname>Chavan</keyname><forenames>Pallavi Vijay</forenames></author><author><keyname>Atique</keyname><forenames>Dr. Mohammad</forenames></author><author><keyname>Malik</keyname><forenames>Dr. Latesh</forenames></author></authors><title>Design and Implementation of Hierarchical Visual cryptography with
  Expansion less Shares</title><categories>cs.CR</categories><comments>12 pages, 7 figures, International journal of Network security and
  its applications</comments><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol.6, No.1, January 2014</journal-ref><doi>10.5121/ijnsa.2014.6108</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Novel idea of hierarchical visual cryptography is stated in this paper. The
key concept of hierarchical visual cryptography is based upon visual
cryptography. Visual cryptography encrypts secret information into two pieces
called as shares. These two shares are stacked together by logical XOR
operation to reveal the original secret. Hierarchical visual cryptography
encrypts the secret in various levels. The encryption in turn is expansionless.
The original secret size is retained in the shares at all levels. In this paper
secret is encrypted at two different levels. Four shares are generated out of
hierarchical visual cryptography. Any three shares are collectively taken to
form the key share. All shares generated are meaningless giving no information
by visual inspection. Performance analysis is also obtained based upon various
categories of secrets. The greying effect is completely removed while revealing
the secret Removal of greying effect do not change the meaning of secret.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2760</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2760</id><created>2014-02-12</created><updated>2015-10-14</updated><authors><author><keyname>Chalopin</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author><author><keyname>Dieudonn&#xe9;</keyname><forenames>Yoann</forenames></author><author><keyname>Labourel</keyname><forenames>Arnaud</forenames></author><author><keyname>Pelc</keyname><forenames>Andrzej</forenames></author></authors><title>Rendezvous in Networks in Spite of Delay Faults</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two mobile agents, starting from different nodes of an unknown network, have
to meet at the same node. Agents move in synchronous rounds using a
deterministic algorithm. Each agent has a different label, which it can use in
the execution of the algorithm, but it does not know the label of the other
agent. Agents do not know any bound on the size of the network. In each round
an agent decides if it remains idle or if it wants to move to one of the
adjacent nodes. Agents are subject to delay faults: if an agent incurs a fault
in a given round, it remains in the current node, regardless of its decision.
If it planned to move and the fault happened, the agent is aware of it. We
consider three scenarios of fault distribution: random (independently in each
round and for each agent with constant probability 0 &lt; p &lt; 1), unbounded adver-
sarial (the adversary can delay an agent for an arbitrary finite number of
consecutive rounds) and bounded adversarial (the adversary can delay an agent
for at most c consecutive rounds, where c is unknown to the agents). The
quality measure of a rendezvous algorithm is its cost, which is the total
number of edge traversals. For random faults, we show an algorithm with cost
polynomial in the size n of the network and polylogarithmic in the larger label
L, which achieves rendezvous with very high probability in arbitrary networks.
By contrast, for unbounded adversarial faults we show that rendezvous is not
feasible, even in the class of rings. Under this scenario we give a rendezvous
algorithm with cost O(nl), where l is the smaller label, working in arbitrary
trees, and we show that \Omega(l) is the lower bound on rendezvous cost, even
for the two-node tree. For bounded adversarial faults, we give a rendezvous
algorithm working for arbitrary networks, with cost polynomial in n, and
logarithmic in the bound c and in the larger label L.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2773</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2773</id><created>2014-02-12</created><updated>2014-11-25</updated><authors><author><keyname>Sundararajan</keyname><forenames>Gopalakrishnan</forenames></author><author><keyname>Winstead</keyname><forenames>Chris</forenames></author><author><keyname>Boutillon</keyname><forenames>Emmanuel</forenames></author></authors><title>Noisy Gradient Descent Bit-Flip Decoding for LDPC Codes</title><categories>cs.IT math.IT</categories><comments>16 pages, 22 figures, 2 tables</comments><acm-class>E.4</acm-class><journal-ref>IEEE Trans. on Communications v62 n10 pp3385-3400 (2014)</journal-ref><doi>10.1109/TCOMM.2014.2356458</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A modified Gradient Descent Bit Flipping (GDBF) algorithm is proposed for
decoding Low Density Parity Check (LDPC) codes on the binary-input additive
white Gaussian noise channel. The new algorithm, called Noisy GDBF (NGDBF),
introduces a random perturbation into each symbol metric at each iteration. The
noise perturbation allows the algorithm to escape from undesirable local
maxima, resulting in improved performance. A combination of heuristic
improvements to the algorithm are proposed and evaluated. When the proposed
heuristics are applied, NGDBF performs better than any previously reported GDBF
variant, and comes within 0.5 dB of the belief propagation algorithm for
several tested codes. Unlike other previous GDBF algorithms that provide an
escape from local maxima, the proposed algorithm uses only local, fully
parallelizable operations and does not require computing a global objective
function or a sort over symbol metrics, making it highly efficient in
comparison. The proposed NGDBF algorithm requires channel state information
which must be obtained from a signal to noise ratio (SNR) estimator.
Architectural details are presented for implementing the NGDBF algorithm.
Complexity analysis and optimizations are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2782</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2782</id><created>2014-02-12</created><updated>2014-02-13</updated><authors><author><keyname>Glantz</keyname><forenames>Roland</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author></authors><title>Tree-based Coarsening and Partitioning of Complex Networks</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications produce massive complex networks whose analysis would
benefit from parallel processing. Parallel algorithms, in turn, often require a
suitable network partition. For solving optimization tasks such as graph
partitioning on large networks, multilevel methods are preferred in practice.
Yet, complex networks pose challenges to established multilevel algorithms, in
particular to their coarsening phase.
  One way to specify a (recursive) coarsening of a graph is to rate its edges
and then contract the edges as prioritized by the rating. In this paper we (i)
define weights for the edges of a network that express the edges' importance
for connectivity, (ii) compute a minimum weight spanning tree $T^m$ with
respect to these weights, and (iii) rate the network edges based on the
conductance values of $T^m$'s fundamental cuts. To this end, we also (iv)
develop the first optimal linear-time algorithm to compute the conductance
values of \emph{all} fundamental cuts of a given spanning tree. We integrate
the new edge rating into a leading multilevel graph partitioner and equip the
latter with a new greedy postprocessing for optimizing the maximum
communication volume (MCV). Experiments on bipartitioning frequently used
benchmark networks show that the postprocessing already reduces MCV by 11.3%.
Our new edge rating further reduces MCV by 10.3% compared to the previously
best rating with the postprocessing in place for both ratings. In total, with a
modest increase in running time, our new approach reduces the MCV of complex
network partitions by 20.4%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2793</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2793</id><created>2014-02-12</created><updated>2014-05-21</updated><authors><author><keyname>Krzywicki</keyname><forenames>D.</forenames></author><author><keyname>Faber</keyname><forenames>&#x141;.</forenames></author><author><keyname>Byrski</keyname><forenames>A.</forenames></author><author><keyname>Kisiel-Dorohinicki</keyname><forenames>M.</forenames></author></authors><title>Computing Agents for Decision Support Systems</title><categories>cs.MA</categories><journal-ref>Future Generation Computer Systems, Volume 37, July 2014, Pages
  390-400</journal-ref><doi>10.1016/j.future.2014.02.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In decision support systems, it is essential to get a candidate solution
fast, even if it means resorting to an approximation. This constraint
introduces a scalability requirement with regard to the kind of heuristics
which can be used in such systems. As execution time is bounded, these
algorithms need to give better results and scale up with additional computing
resources instead of additional time. In this paper, we show how multi-agent
systems can fulfil these requirements. We recall as an example the concept of
Evolutionary Multi-Agent Systems, which combine evolutionary and agent
computing paradigms. We describe several possible implementations and present
experimental results demonstrating how additional resources improve the
efficiency of such systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2796</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2796</id><created>2014-02-12</created><authors><author><keyname>Celli</keyname><forenames>Fabio</forenames></author><author><keyname>Poesio</keyname><forenames>Massimo</forenames></author></authors><title>PR2: A Language Independent Unsupervised Tool for Personality
  Recognition from Text</title><categories>cs.CL</categories><comments>4 pages, peer reviewed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present PR2, a personality recognition system available online, that
performs instance-based classification of Big5 personality types from
unstructured text, using language-independent features. It has been tested on
English and Italian, achieving performances up to f=.68.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2801</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2801</id><created>2014-02-12</created><updated>2014-10-08</updated><authors><author><keyname>Pai</keyname><forenames>Mallesh M.</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author></authors><title>An Anti-Folk Theorem for Large Repeated Games with Imperfect Monitoring</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study infinitely repeated games in settings of imperfect monitoring. We
first prove a family of theorems that show that when the signals observed by
the players satisfy a condition known as $(\epsilon, \gamma)$-differential
privacy, that the folk theorem has little bite: for values of $\epsilon$ and
$\gamma$ sufficiently small, for a fixed discount factor, any equilibrium of
the repeated game involve players playing approximate equilibria of the stage
game in every period. Next, we argue that in large games ($n$ player games in
which unilateral deviations by single players have only a small impact on the
utility of other players), many monitoring settings naturally lead to signals
that satisfy $(\epsilon,\gamma)$-differential privacy, for $\epsilon$ and
$\gamma$ tending to zero as the number of players $n$ grows large. We conclude
that in such settings, the set of equilibria of the repeated game collapse to
the set of equilibria of the stage game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2807</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2807</id><created>2014-02-12</created><authors><author><keyname>Zhou</keyname><forenames>Rui</forenames></author><author><keyname>Liu</keyname><forenames>Chengfei</forenames></author><author><keyname>Yu</keyname><forenames>Jeffrey Xu</forenames></author><author><keyname>Liang</keyname><forenames>Weifa</forenames></author><author><keyname>Zhang</keyname><forenames>Yanchun</forenames></author></authors><title>Efficient Truss Maintenance in Evolving Networks</title><categories>cs.DB cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Truss was proposed to study social network data represented by graphs. A
k-truss of a graph is a cohesive subgraph, in which each edge is contained in
at least k-2 triangles within the subgraph. While truss has been demonstrated
as superior to model the close relationship in social networks and efficient
algorithms for finding trusses have been extensively studied, very little
attention has been paid to truss maintenance. However, most social networks are
evolving networks. It may be infeasible to recompute trusses from scratch from
time to time in order to find the up-to-date $k$-trusses in the evolving
networks. In this paper, we discuss how to maintain trusses in a graph with
dynamic updates. We first discuss a set of properties on maintaining trusses,
then propose algorithms on maintaining trusses on edge deletions and
insertions, finally, we discuss truss index maintenance. We test the proposed
techniques on real datasets. The experiment results show the promise of our
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2810</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2810</id><created>2014-02-12</created><authors><author><keyname>Bampis</keyname><forenames>Evripidis</forenames></author><author><keyname>Chau</keyname><forenames>Vincent</forenames></author><author><keyname>Letsios</keyname><forenames>Dimitrios</forenames></author><author><keyname>Lucarelli</keyname><forenames>Giorgio</forenames></author><author><keyname>Milis</keyname><forenames>Ioannis</forenames></author><author><keyname>Zois</keyname><forenames>Georgios</forenames></author></authors><title>Energy Efficient Scheduling of MapReduce Jobs</title><categories>cs.DC</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MapReduce is emerged as a prominent programming model for data-intensive
computation. In this work, we study power-aware MapReduce scheduling in the
speed scaling setting first introduced by Yao et al. [FOCS 1995]. We focus on
the minimization of the total weighted completion time of a set of MapReduce
jobs under a given budget of energy. Using a linear programming relaxation of
our problem, we derive a polynomial time constant-factor approximation
algorithm. We also propose a convex programming formulation that we combine
with standard list scheduling policies, and we evaluate their performance using
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2812</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2812</id><created>2014-02-12</created><updated>2015-07-17</updated><authors><author><keyname>Parsa</keyname><forenames>Salman</forenames></author></authors><title>Algorithms for Dynamic Reeb Graphs</title><categories>cs.CG cs.DS</categories><comments>There was a problem with the argument used in the original
  submission. It seems that the truly dynamic nature of the problem puts it in
  the category of problems such as dynamic graph connectivity that do not have
  known poly-logarithmic algorihtms. To see the claims in the abstract and a
  reduction of the problem to a dynamic graph problem refer to my PhD thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for updating the Reeb graph under fully dynamic
changes of the function values. The basic event is the interchange of two
consecutive vertex values. The algorithm updates the Reeb graph in $O(l g{n})$
worst-case deterministic time for each such interchange, where $l$ is an upper
bound on the size of the star of the involved vertices, and g(n) is a
worst-case bound for the dynamic graph connectivity problem. Moreover, we argue
that $O(l)$ is a lower bound for this operation in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2826</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2826</id><created>2014-02-11</created><authors><author><keyname>Bera</keyname><forenames>Aniket</forenames></author><author><keyname>Manocha</keyname><forenames>Dinesh</forenames></author></authors><title>Realtime Multilevel Crowd Tracking using Reciprocal Velocity Obstacles</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel, realtime algorithm to compute the trajectory of each
pedestrian in moderately dense crowd scenes. Our formulation is based on an
adaptive particle filtering scheme that uses a multi-agent motion model based
on velocity-obstacles, and takes into account local interactions as well as
physical and personal constraints of each pedestrian. Our method dynamically
changes the number of particles allocated to each pedestrian based on different
confidence metrics. Additionally, we use a new high-definition crowd video
dataset, which is used to evaluate the performance of different pedestrian
tracking algorithms. This dataset consists of videos of indoor and outdoor
scenes, recorded at different locations with 30-80 pedestrians. We highlight
the performance benefits of our algorithm over prior techniques using this
dataset. In practice, our algorithm can compute trajectories of tens of
pedestrians on a multi-core desktop CPU at interactive rates (27-30 frames per
second). To the best of our knowledge, our approach is 4-5 times faster than
prior methods, which provide similar accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2827</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2827</id><created>2014-02-10</created><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Harnessing the Complexity of Education with Information Technology</title><categories>cs.CY</categories><comments>5 pages, 2 figures. Commentary</comments><acm-class>K.3; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Education at all levels is facing several challenges in most countries, such
as low quality, high costs, lack of educators, and unsatisfied student demand.
Traditional approaches are becoming unable to deliver the required education.
Several causes for this inefficiency can be identified. I argue that beyond
specific causes, the lack of effective education is related to complexity.
However, information technology is helping us overcome this complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2840</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2840</id><created>2014-02-12</created><updated>2014-06-30</updated><authors><author><keyname>Doyen</keyname><forenames>Laurent</forenames></author><author><keyname>Massart</keyname><forenames>Thierry</forenames></author><author><keyname>Shirmohammadi</keyname><forenames>Mahsa</forenames></author></authors><title>Robust Synchronization in Markov Decision Processes</title><categories>cs.LO</categories><comments>27 pages, 9 figures, 3 Tables. arXiv admin note: text overlap with
  arXiv:1310.2935</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider synchronizing properties of Markov decision processes (MDP),
viewed as generators of sequences of probability distributions over states. A
probability distribution is p-synchronizing if the probability mass is at least
p in some state, and a sequence of probability distributions is weakly
p-synchronizing, or strongly p-synchronizing if respectively infinitely many,
or all but finitely many distributions in the sequence are p-synchronizing.
  For each synchronizing mode, an MDP can be (i) sure winning if there is a
strategy that produces a 1-synchronizing sequence; (ii) almost-sure winning if
there is a strategy that produces a sequence that is, for all {\epsilon} &gt; 0, a
(1-{\epsilon})-synchronizing sequence; (iii) limit-sure winning if for all
{\epsilon} &gt; 0, there is a strategy that produces a
(1-{\epsilon})-synchronizing sequence.
  For each synchronizing and winning mode, we consider the problem of deciding
whether an MDP is winning, and we establish matching upper and lower complexity
bounds of the problems, as well as the optimal memory requirement for winning
strategies: (a) for all winning modes, we show that the problems are
PSPACE-complete for weakly synchronizing, and PTIME-complete for strongly
synchronizing; (b) we show that for weakly synchronizing, exponential memory is
sufficient and may be necessary for sure winning, and infinite memory is
necessary for almost-sure winning; for strongly synchronizing, linear-size
memory is sufficient and may be necessary in all modes; (c) we show a
robustness result that the almost-sure and limit-sure winning modes coincide
for both weakly and strongly synchronizing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2843</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2843</id><created>2014-02-12</created><updated>2014-02-14</updated><authors><author><keyname>Bonnet</keyname><forenames>Edouard</forenames></author><author><keyname>Paschos</keyname><forenames>Vangelis Th.</forenames></author></authors><title>Sparsification and subexponential approximation</title><categories>cs.CC cs.DS</categories><comments>16 pages</comments><msc-class>68Q17</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Instance sparsification is well-known in the world of exact computation since
it is very closely linked to the Exponential Time Hypothesis. In this paper, we
extend the concept of sparsification in order to capture subexponential time
approximation. We develop a new tool for inapproximability, called
approximation preserving sparsification and use it in order to get strong
inapproximability results in subexponential time for several fundamental
optimization problems as Max Independent Set, Min Dominating Set, Min Feedback
Vertex Set, and Min Set Cover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2845</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2845</id><created>2014-02-12</created><updated>2014-08-04</updated><authors><author><keyname>Gorodetsky</keyname><forenames>Alex A.</forenames></author><author><keyname>Marzouk</keyname><forenames>Youssef M.</forenames></author></authors><title>Efficient Localization of Discontinuities in Complex Computational
  Simulations</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Surrogate models for computational simulations are input-output
approximations that allow computationally intensive analyses, such as
uncertainty propagation and inference, to be performed efficiently. When a
simulation output does not depend smoothly on its inputs, the error and
convergence rate of many approximation methods deteriorate substantially. This
paper details a method for efficiently localizing discontinuities in the input
parameter domain, so that the model output can be approximated as a piecewise
smooth function. The approach comprises an initialization phase, which uses
polynomial annihilation to assign function values to different regions and thus
seed an automated labeling procedure, followed by a refinement phase that
adaptively updates a kernel support vector machine representation of the
separating surface via active learning. The overall approach avoids structured
grids and exploits any available simplicity in the geometry of the separating
surface, thus reducing the number of model evaluations required to localize the
discontinuity. The method is illustrated on examples of up to eleven
dimensions, including algebraic models and ODE/PDE systems, and demonstrates
improved scaling and efficiency over other discontinuity localization
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2852</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2852</id><created>2014-02-12</created><updated>2014-02-13</updated><authors><author><keyname>Onn</keyname><forenames>Shmuel</forenames></author></authors><title>Robust Integer Programming</title><categories>math.OC cs.DM cs.DS math.CO</categories><msc-class>05A, 15A, 51M, 52A, 52B, 52C, 62H, 68Q, 68R, 68U, 68W, 90B, 90C</msc-class><journal-ref>Operations Research Letters, 42:558-560, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a complexity classification of four variants of robust integer
programming when the underlying Graver basis is given. We discuss applications
to robust multicommodity flows and multidimensional transportation, and
describe an effective parametrization of robust integer programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2863</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2863</id><created>2014-02-12</created><authors><author><keyname>Dai</keyname><forenames>Liang</forenames></author><author><keyname>Soltanalian</keyname><forenames>Mojtaba</forenames></author><author><keyname>Pelckmans</keyname><forenames>Kristiaan</forenames></author></authors><title>On the Randomized Kaczmarz Algorithm</title><categories>cs.SY math.OC</categories><comments>This paper will appear in IEEE Signal processing letters, vol. 21,
  no. 3, March 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Randomized Kaczmarz Algorithm is a randomized method which aims at
solving a consistent system of over determined linear equations. This note
discusses how to find an optimized randomization scheme for this algorithm,
which is related to the question raised by \cite{c2}. Illustrative experiments
are conducted to support the findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2864</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2864</id><created>2014-02-12</created><updated>2014-05-25</updated><authors><author><keyname>Dai</keyname><forenames>Liang</forenames></author><author><keyname>Pelckmans</keyname><forenames>Kristiaan</forenames></author></authors><title>Sparse Estimation From Noisy Observations of an Overdetermined Linear
  System</title><categories>cs.SY stat.ML</categories><comments>This paper is provisionally accepted by Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note studies a method for the efficient estimation of a finite number of
unknown parameters from linear equations, which are perturbed by Gaussian
noise.
  In case the unknown parameters have only few nonzero entries, the proposed
estimator performs more efficiently than a traditional approach.
  The method consists of three steps:
  (1) a classical Least Squares Estimate (LSE),
  (2) the support is recovered through a Linear Programming (LP) optimization
problem which can be computed using a soft-thresholding step,
  (3) a de-biasing step using a LSE on the estimated support set.
  The main contribution of this note is a formal derivation of an associated
ORACLE property of the final estimate.
  That is, when the number of samples is large enough, the estimate is shown to
equal the LSE based on the support of the {\em true} parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2867</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2867</id><created>2014-02-10</created><authors><author><keyname>Kerracher</keyname><forenames>Natalie</forenames></author><author><keyname>Kennedy</keyname><forenames>Jessie</forenames></author><author><keyname>Chalmers</keyname><forenames>Kevin</forenames></author></authors><title>Tasks for Temporal Graph Visualisation</title><categories>cs.OH</categories><comments>42 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [1], we describe the design and development of a task taxonomy for
temporal graph visualisation. This paper details the full instantiation of that
task taxonomy. Our task taxonomy is based on the Andrienko framework [2], which
uses a systematic approach to develop a formal task framework for visual tasks
specifically associated with Exploratory Data Analysis. The Andrienko framework
is intended to be applicable to all types of data, however, it does not
consider relational (graph) data. We therefore extended both their data model
and task framework for temporal graph data, and instantiated the extended
version to produce a comprehensive list of tasks of interest during exploratory
analysis of temporal graph data. As expected, our instantiation of the
framework resulted in a very large task list; with more than 144 variations of
attribute based tasks alone, it is too large to fit in a standard journal
paper, hence we provide the detailed listing in this document.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2871</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2871</id><created>2014-02-12</created><authors><author><keyname>Amato</keyname><forenames>Christopher</forenames></author><author><keyname>Konidaris</keyname><forenames>George D.</forenames></author><author><keyname>Cruz</keyname><forenames>Gabriel</forenames></author><author><keyname>Maynor</keyname><forenames>Christopher A.</forenames></author><author><keyname>How</keyname><forenames>Jonathan P.</forenames></author><author><keyname>Kaelbling</keyname><forenames>Leslie P.</forenames></author></authors><title>Planning for Decentralized Control of Multiple Robots Under Uncertainty</title><categories>cs.RO cs.AI cs.MA</categories><acm-class>I.2.9; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a probabilistic framework for synthesizing control policies for
general multi-robot systems, given environment and sensor models and a cost
function. Decentralized, partially observable Markov decision processes
(Dec-POMDPs) are a general model of decision processes where a team of agents
must cooperate to optimize some objective (specified by a shared reward or cost
function) in the presence of uncertainty, but where communication limitations
mean that the agents cannot share their state, so execution must proceed in a
decentralized fashion. While Dec-POMDPs are typically intractable to solve for
real-world problems, recent research on the use of macro-actions in Dec-POMDPs
has significantly increased the size of problem that can be practically solved
as a Dec-POMDP. We describe this general model, and show how, in contrast to
most existing methods that are specialized to a particular problem class, it
can synthesize control policies that use whatever opportunities for
coordination are present in the problem, while balancing off uncertainty in
outcomes, sensor information, and information about other agents. We use three
variations on a warehouse task to show that a single planner of this type can
generate cooperative behavior using task allocation, direct communication, and
signaling, as appropriate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2880</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2880</id><created>2014-02-12</created><authors><author><keyname>Scemama</keyname><forenames>Anthony</forenames></author><author><keyname>Renon</keyname><forenames>Nicolas</forenames></author><author><keyname>Rapacioli</keyname><forenames>Mathias</forenames></author></authors><title>A Sparse SCF algorithm and its parallel implementation: Application to
  DFTB</title><categories>physics.chem-ph cs.DC physics.comp-ph</categories><comments>13 pages, 11 figures</comments><journal-ref>J. Chem. Theory Comput., 2014, 10 (6), 2344-2354</journal-ref><doi>10.1021/ct500115v</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm and its parallel implementation for solving a self
consistent problem as encountered in Hartree Fock or Density Functional Theory.
The algorithm takes advantage of the sparsity of matrices through the use of
local molecular orbitals. The implementation allows to exploit efficiently
modern symmetric multiprocessing (SMP) computer architectures. As a first
application, the algorithm is used within the density functional based tight
binding method, for which most of the computational time is spent in the linear
algebra routines (diagonalization of the Fock/Kohn-Sham matrix). We show that
with this algorithm (i) single point calculations on very large systems
(millions of atoms) can be performed on large SMP machines (ii) calculations
involving intermediate size systems (1~000--100~000 atoms) are also strongly
accelerated and can run efficiently on standard servers (iii) the error on the
total energy due to the use of a cut-off in the molecular orbital coefficients
can be controlled such that it remains smaller than the SCF convergence
criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2890</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2890</id><created>2014-02-12</created><authors><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Lin</keyname><forenames>Yen-Hung</forenames></author><author><keyname>Luk-Pat</keyname><forenames>Gerard</forenames></author><author><keyname>Ding</keyname><forenames>Duo</forenames></author><author><keyname>Lucas</keyname><forenames>Kevin</forenames></author><author><keyname>Pan</keyname><forenames>David Z.</forenames></author></authors><title>A High-Performance Triple Patterning Layout Decomposer with Balanced
  Density</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Triple patterning lithography (TPL) has received more and more attentions
from industry as one of the leading candidate for 14nm/11nm nodes. In this
paper, we propose a high performance layout decomposer for TPL. Density
balancing is seamlessly integrated into all key steps in our TPL layout
decomposition, including density-balanced semi-definite programming (SDP),
density-based mapping, and density-balanced graph simplification. Our new TPL
decomposer can obtain high performance even compared to previous
state-of-the-art layout decomposers which are not balanced-density aware, e.g.,
by Yu et al. (ICCAD'11), Fang et al. (DAC'12), and Kuang et al. (DAC'13).
Furthermore, the balanced-density version of our decomposer can provide more
balanced density which leads to less edge placement error (EPE), while the
conflict and stitch numbers are still very comparable to our
non-balanced-density baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2892</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2892</id><created>2014-02-12</created><authors><author><keyname>Slimani</keyname><forenames>Thabet</forenames></author><author><keyname>Lazzez</keyname><forenames>Amor</forenames></author></authors><title>Efficient Analysis of Pattern and Association Rule Mining Approaches</title><categories>cs.DB</categories><comments>14 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:1312.4800; and with arXiv:1109.2427 by other authors</comments><journal-ref>International Journal of Information Technology and Computer
  Science (IJITCS), vol.6, no.3, pp.70-81, 2014</journal-ref><doi>10.5815/ijitcs.2014.03.09</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process of data mining produces various patterns from a given data
source. The most recognized data mining tasks are the process of discovering
frequent itemsets, frequent sequential patterns, frequent sequential rules and
frequent association rules. Numerous efficient algorithms have been proposed to
do the above processes. Frequent pattern mining has been a focused topic in
data mining research with a good number of references in literature and for
that reason an important progress has been made, varying from performant
algorithms for frequent itemset mining in transaction databases to complex
algorithms, such as sequential pattern mining, structured pattern mining,
correlation mining. Association Rule mining (ARM) is one of the utmost current
data mining techniques designed to group objects together from large databases
aiming to extract the interesting correlation and relation among huge amount of
data. In this article, we provide a brief review and analysis of the current
status of frequent pattern mining and discuss some promising research
directions. Additionally, this paper includes a comparative study between the
performance of the described approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2894</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2894</id><created>2014-02-12</created><authors><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Dong</keyname><forenames>Sheqin</forenames></author><author><keyname>Goto</keyname><forenames>Statoshi</forenames></author></authors><title>Multi-Voltage and Level-Shifter Assignment Driven Floorplanning</title><categories>cs.AR</categories><doi>10.1109/ASICON.2009.5351219</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As technology scales, low power design has become a significant requirement
for SOC designers. Among the existing techniques, Multiple-Supply Voltage (MSV)
is a popular and effective method to reduce both dynamic and static power.
Besides, level shifters consume area and delay, and should be considered during
floorplanning. In this paper, we present a new floorplanning system, called
MVLSAF, to solve multi-voltage and level shifter assignment problem. We use a
convex cost network flow algorithm to assign arbitrary number of legal working
voltages and a minimum cost flow algorithm to handle level-shifter assignment.
The experimental results show MVLSAF is effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2899</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2899</id><created>2014-02-12</created><authors><author><keyname>Ding</keyname><forenames>Duo</forenames></author><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Pan</keyname><forenames>David Z.</forenames></author></authors><title>GLOW: A global router for low-power thermal-reliable interconnect
  synthesis using photonic wavelength multiplexing</title><categories>cs.AR</categories><doi>10.1109/ASPDAC.2012.6165031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we examine the integration potential and explore the design
space of low power thermal reliable on-chip interconnect synthesis featuring
nanophotonics Wavelength Division Multiplexing (WDM). With the recent
advancements, it is foreseen that nanophotonics holds the promise to be
employed for future on-chip data signalling due to its unique power efficiency,
signal delay and huge multiplexing potential. However, there are major
challenges to address before feasible on-chip integration could be reached. In
this paper, we present GLOW, a hybrid global router to provide low power
opto-electronic interconnect synthesis under the considerations of thermal
reliability and various physical design constraints such as optical power,
delay and signal quality. GLOW is evaluated with testing cases derived from
ISPD07-08 global routing benchmarks. Compared with a greedy approach, GLOW
demonstrates around 23%-50% of total optical power reduction, revealing great
potential of on-chip WDM interconnect synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2902</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2902</id><created>2014-02-10</created><authors><author><keyname>Fan</keyname><forenames>Deliang</forenames></author><author><keyname>Sharad</keyname><forenames>Mrigank</forenames></author><author><keyname>Sengupta</keyname><forenames>Abhronil</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Hierarchical Temporal Memory Based on Spin-Neurons and Resistive Memory
  for Energy-Efficient Brain-Inspired Computing</title><categories>cs.ET cond-mat.dis-nn</categories><comments>this work was submitted to IEEE Transactions on Neural Networks and
  Learning Systems. It is under review now</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical temporal memory (HTM) tries to mimic the computing in
cerebral-neocortex. It identifies spatial and temporal patterns in the input
for making inferences. This may require large number of computationally
expensive tasks like, dot-product evaluations. Nano-devices that can provide
direct mapping for such primitives are of great interest. In this work we show
that the computing blocks for HTM can be mapped using low-voltage,
fast-switching, magneto-metallic spin-neurons combined with emerging resistive
cross-bar network (RCN). Results show possibility of more than 200x lower
energy as compared to 45nm CMOS ASIC design
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2903</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2903</id><created>2014-02-12</created><authors><author><keyname>Levit</keyname><forenames>Vadim E.</forenames></author><author><keyname>Mandrescu</keyname><forenames>Eugen</forenames></author></authors><title>Computing Unique Maximum Matchings in O(m) time for Konig-Egervary
  Graphs and Unicyclic Graphs</title><categories>cs.DM math.CO</categories><comments>10 pages, 5 figures</comments><msc-class>05C70 (Primary), 05C85 (Secondary)</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let alpha(G) denote the maximum size of an independent set of vertices and
mu(G) be the cardinality of a maximum matching in a graph G. A matching
saturating all the vertices is perfect. If alpha(G) + mu(G) equals the number
of vertices of G, then it is called a Konig-Egervary graph. A graph is
unicyclic if it has a unique cycle.
  In 2010, Bartha conjectured that a unique perfect matching, if it exists, can
be found in O(m) time, where m is the number of edges.
  In this paper we validate this conjecture for Konig-Egervary graphs and
unicylic graphs. We propose a variation of Karp-Sipser leaf-removal algorithm
(Karp and Spiser, 1981), which ends with an empty graph if and only if the
original graph is a Konig-Egervary graph with a unique perfect matching
obtained as an output as well.
  We also show that a unicyclic non-bipartite graph G may have at most one
perfect matching, and this is the case where G is a Konig-Egervary graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2904</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2904</id><created>2014-02-12</created><authors><author><keyname>Ding</keyname><forenames>Duo</forenames></author><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Ghosh</keyname><forenames>Joydeep</forenames></author><author><keyname>Pan</keyname><forenames>David Z.</forenames></author></authors><title>EPIC: Efficient prediction of IC manufacturing hotspots with a unified
  meta-classification formulation</title><categories>cs.AR</categories><doi>10.1109/ASPDAC.2012.6164956</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present EPIC, an efficient and effective predictor for IC
manufacturing hotspots in deep sub-wavelength lithography. EPIC proposes a
unified framework to combine different hotspot detection methods together, such
as machine learning and pattern matching, using mathematical
programming/optimization. EPIC algorithm has been tested on a number of
industry benchmarks under advanced manufacturing conditions. It demonstrates so
far the best capability in selectively combining the desirable features of
various hotspot detection methods (3.5-8.2% accuracy improvement) as well as
significant suppression of the detection noise (e.g., 80% false-alarm
reduction). These characteristics make EPIC very suitable for conducting high
performance physical verification and guiding efficient manufacturability
friendly physical design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2906</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2906</id><created>2014-02-12</created><authors><author><keyname>Lin</keyname><forenames>Yen-Hung</forenames></author><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Pan</keyname><forenames>David Z.</forenames></author><author><keyname>Li</keyname><forenames>Yih-Lang</forenames></author></authors><title>TRIAD: a triple patterning lithography aware detailed router</title><categories>cs.AR</categories><doi>10.1145/2429384.2429408</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  TPL-friendly detailed routers require a systematic approach to detect TPL
conflicts. However, the complexity of conflict graph (CG) impedes directly
detecting TPL conflicts in CG. This work proposes a token graph-embedded
conflict graph (TECG) to facilitate the TPL conflict detection while
maintaining high coloring-flexibility. We then develop a TPL aware detailed
router (TRIAD) by applying TECG to a gridless router with the TPL stitch
generation. Compared to a greedy coloring approach, experimental results
indicate that TRIAD generates no conflicts and few stitches with shorter
wirelength at the cost of 2.41x of runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2908</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2908</id><created>2014-02-12</created><authors><author><keyname>Schmitz</keyname><forenames>Sylvain</forenames></author><author><keyname>Schnoebelen</keyname><forenames>Philippe</forenames></author></authors><title>The Power of Well-Structured Systems</title><categories>cs.LO</categories><comments>Invited talk</comments><journal-ref>Proceedings of Concur 2013, Lecture Notes in Computer Science vol.
  8052, pp. 5--24</journal-ref><doi>10.1007/978-3-642-40184-8_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Well-structured systems, aka WSTSs, are computational models where the set of
possible configurations is equipped with a well-quasi-ordering which is
compatible with the transition relation between configurations. This structure
supports generic decidability results that are important in verification and
several other fields.
  This paper recalls the basic theory underlying well-structured systems and
shows how two classic decision algorithms can be formulated as an exhaustive
search for some &quot;bad&quot; sequences. This lets us describe new powerful techniques
for the complexity analysis of WSTS algorithms. Recently, these techniques have
been successful in precisely characterising the power, in a
complexity-theoretical sense, of several important WSTS models like unreliable
channel systems, monotonic counter machines, or networks of timed systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2925</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2925</id><created>2014-02-12</created><authors><author><keyname>Triki</keyname><forenames>Slim</forenames></author><author><keyname>Mekki</keyname><forenames>Taher</forenames></author><author><keyname>Kamoun</keyname><forenames>Anas</forenames></author></authors><title>Modeling Switched Behavior with Hybrid Bond Graph: Application to a Tank
  system</title><categories>cs.SY</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 11,
  Issue 1, No 2,pp. 60-66, January 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different approaches have been used in the development of system models. In
addition, modeling and simulation approaches are essential for design,
analysis, control, and diagnosis of complex systems. This work presents a
Simulink model for systems with mixed continuous and discrete behaviors. The
model simulated was developed using the bond graph methodology and we model
hybrid systems using hybrid bond graphs (HBGs), that incorporates local
switching functions that enable the reconfiguration of energy flow paths. This
approach has been implemented as a software tool called the MOdeling and
Transformation of HBGs for Simulation (MOTHS) tool suite which incorporates a
model translator that create Simulink models. Simulation model of a three-tank
system that includes a switching component was developed using the bond graph
methodology, and MoTHS software were used to build a Simulink model of the
dynamic behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2936</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2936</id><created>2014-02-12</created><updated>2015-01-06</updated><authors><author><keyname>Steinwandt</keyname><forenames>Jens</forenames></author><author><keyname>Roemer</keyname><forenames>Florian</forenames></author><author><keyname>Haardt</keyname><forenames>Martin</forenames></author><author><keyname>Del Galdo</keyname><forenames>Giovanni</forenames></author></authors><title>R-dimensional ESPRIT-type algorithms for strictly second-order
  non-circular sources and their performance analysis</title><categories>cs.IT math.IT</categories><comments>accepted at IEEE Transactions on Signal Processing, 15 pages, 6
  figures</comments><doi>10.1109/TSP.2014.2342673</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-resolution parameter estimation algorithms designed to exploit the prior
knowledge about incident signals from strictly second-order (SO) non-circular
(NC) sources allow for a lower estimation error and can resolve twice as many
sources. In this paper, we derive the R-D NC Standard ESPRIT and the R-D NC
Unitary ESPRIT algorithms that provide a significantly better performance
compared to their original versions for arbitrary source signals. They are
applicable to shift-invariant R-D antenna arrays and do not require a
centrosymmetric array structure. Moreover, we present a first-order asymptotic
performance analysis of the proposed algorithms, which is based on the error in
the signal subspace estimate arising from the noise perturbation. The derived
expressions for the resulting parameter estimation error are explicit in the
noise realizations and asymptotic in the effective signal-to-noise ratio (SNR),
i.e., the results become exact for either high SNRs or a large sample size. We
also provide mean squared error (MSE) expressions, where only the assumptions
of a zero mean and finite SO moments of the noise are required, but no
assumptions about its statistics are necessary. As a main result, we
analytically prove that the asymptotic performance of both R-D NC ESPRIT-type
algorithms is identical in the high effective SNR regime. Finally, a case study
shows that no improvement from strictly non-circular sources can be achieved in
the special case of a single source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2941</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2941</id><created>2014-02-06</created><authors><author><keyname>Khan</keyname><forenames>Zohaib</forenames></author><author><keyname>Shafait</keyname><forenames>Faisal</forenames></author><author><keyname>Hu</keyname><forenames>Yiqun</forenames></author><author><keyname>Mian</keyname><forenames>Ajmal</forenames></author></authors><title>Multispectral Palmprint Encoding and Recognition</title><categories>cs.CV</categories><comments>Preliminary version of this manuscript was published in ICCV 2011. Z.
  Khan A. Mian and Y. Hu, &quot;Contour Code: Robust and Efficient Multispectral
  Palmprint Encoding for Human Recognition&quot;, International Conference on
  Computer Vision, 2011. MATLAB Code available:
  https://sites.google.com/site/zohaibnet/Home/codes</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Palmprints are emerging as a new entity in multi-modal biometrics for human
identification and verification. Multispectral palmprint images captured in the
visible and infrared spectrum not only contain the wrinkles and ridge structure
of a palm, but also the underlying pattern of veins; making them a highly
discriminating biometric identifier. In this paper, we propose a feature
encoding scheme for robust and highly accurate representation and matching of
multispectral palmprints. To facilitate compact storage of the feature, we
design a binary hash table structure that allows for efficient matching in
large databases. Comprehensive experiments for both identification and
verification scenarios are performed on two public datasets -- one captured
with a contact-based sensor (PolyU dataset), and the other with a contact-free
sensor (CASIA dataset). Recognition results in various experimental setups show
that the proposed method consistently outperforms existing state-of-the-art
methods. Error rates achieved by our method (0.003% on PolyU and 0.2% on CASIA)
are the lowest reported in literature on both dataset and clearly indicate the
viability of palmprint as a reliable and promising biometric. All source codes
are publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2948</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2948</id><created>2014-02-11</created><updated>2014-05-16</updated><authors><author><keyname>&#x10c;erm&#xe1;k</keyname><forenames>Petr</forenames></author><author><keyname>Lomuscio</keyname><forenames>Alessio</forenames></author><author><keyname>Mogavero</keyname><forenames>Fabio</forenames></author><author><keyname>Murano</keyname><forenames>Aniello</forenames></author></authors><title>MCMAS-SLK: A Model Checker for the Verification of Strategy Logic
  Specifications</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce MCMAS-SLK, a BDD-based model checker for the verification of
systems against specifications expressed in a novel, epistemic variant of
strategy logic. We give syntax and semantics of the specification language and
introduce a labelling algorithm for epistemic and strategy logic modalities. We
provide details of the checker which can also be used for synthesising agents'
strategies so that a specification is satisfied by the system. We evaluate the
efficiency of the implementation by discussing the results obtained for the
dining cryptographers protocol and a variant of the cake-cutting problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2949</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2949</id><created>2014-02-10</created><authors><author><keyname>Karper</keyname><forenames>Aaron</forenames></author></authors><title>A Programming Language Oriented Approach to Computability</title><categories>cs.PL cs.LO</categories><comments>Bachelor thesis at the University of Bern, supervised by Professor
  Dr. Thomas Strahm</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The field of computability and complexity was, where computer science sprung
from. Turing, Church, and Kleene all developed formalisms that demonstrated
what they held &quot;intuitively computable&quot;. The times change however and today's
(aspiring) computer scientists are less proficient in building automata or
composing functions and are much more native to the world of programming
languages. This article will try to introduce typical concepts of computability
theory and complexity in a form more fitted for a modern developer. It is
mostly based on \cite{jones}, but takes input from other sources to provide
examples, additional information, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2959</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2959</id><created>2014-02-12</created><authors><author><keyname>Ochoa</keyname><forenames>Gabriela</forenames><affiliation>LISIC</affiliation></author><author><keyname>Verel</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LISIC</affiliation></author><author><keyname>Daolio</keyname><forenames>Fabio</forenames><affiliation>ISI</affiliation></author><author><keyname>Tomassini</keyname><forenames>Marco</forenames><affiliation>ISI</affiliation></author></authors><title>Local Optima Networks: A New Model of Combinatorial Fitness Landscapes</title><categories>cs.NE cs.AI</categories><proxy>ccsd</proxy><journal-ref>Recent Advances in the Theory and Application of Fitness
  Landscapes, Hendrik Richter, Andries Engelbrecht (Ed.) (2014) 233-262</journal-ref><doi>10.1007/978-3-642-41888-4_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter overviews a recently introduced network-based model of
combinatorial landscapes: Local Optima Networks (LON). The model compresses the
information given by the whole search space into a smaller mathematical object
that is a graph having as vertices the local optima and as edges the possible
weighted transitions between them. Two definitions of edges have been proposed:
basin-transition and escape-edges, which capture relevant topological features
of the underlying search spaces. This network model brings a new set of metrics
to characterize the structure of combinatorial landscapes, those associated
with the science of complex networks. These metrics are described, and results
are presented of local optima network extraction and analysis for two selected
combinatorial landscapes: NK landscapes and the quadratic assignment problem.
Network features are found to correlate with and even predict the performance
of heuristic search algorithms operating on these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2963</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2963</id><created>2014-02-12</created><authors><author><keyname>Bradley</keyname><forenames>William F.</forenames></author></authors><title>Running in Circles: Packet Routing on Ring Networks</title><categories>math.PR cs.NI</categories><comments>Bradley, William F. (2002). Running in circles: packet routing on
  ring networks (Doctoral dissertation, Massachusetts Institute of Technology)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I analyze packet routing on unidirectional ring networks, with an eye towards
establishing bounds on the expected length of the queues. Suppose we route
packets by a greedy &quot;hot potato&quot; protocol. If packets are inserted by a
Bernoulli process and have uniform destinations around the ring, and if the
nominal load is kept fixed, then I can construct an upper bound on the expected
queue length per node that is independent of the size of the ring. If the
packets only travel one or two steps, I can calculate the exact expected queue
length for rings of any size.
  I also show some stability results under more general circumstances. If the
packets are inserted by any ergodic hidden Markov process with nominal loads
less than one, and routed by any greedy protocol, I prove that the ring is
ergodic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2967</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2967</id><created>2014-02-10</created><updated>2015-03-30</updated><authors><author><keyname>Br&#xe1;zdil</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Chmel&#xed;k</keyname><forenames>Martin</forenames></author><author><keyname>Forejt</keyname><forenames>Vojt&#x11b;ch</forenames></author><author><keyname>K&#x159;et&#xed;nsk&#xfd;</keyname><forenames>Jan</forenames></author><author><keyname>Kwiatkowska</keyname><forenames>Marta</forenames></author><author><keyname>Parker</keyname><forenames>David</forenames></author><author><keyname>Ujma</keyname><forenames>Mateusz</forenames></author></authors><title>Verification of Markov Decision Processes using Learning Algorithms</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general framework for applying machine-learning algorithms to
the verification of Markov decision processes (MDPs). The primary goal of these
techniques is to improve performance by avoiding an exhaustive exploration of
the state space. Our framework focuses on probabilistic reachability, which is
a core property for verification, and is illustrated through two distinct
instantiations. The first assumes that full knowledge of the MDP is available,
and performs a heuristic-driven partial exploration of the model, yielding
precise lower and upper bounds on the required probability. The second tackles
the case where we may only sample the MDP, and yields probabilistic guarantees,
again in terms of both the lower and upper bounds, which provides efficient
stopping criteria for the approximation. The latter is the first extension of
statistical model-checking for unbounded properties in MDPs. In contrast with
other related approaches, we do not restrict our attention to time-bounded
(finite-horizon) or discounted properties, nor assume any particular properties
of the MDP. We also show how our techniques extend to LTL objectives. We
present experimental results showing the performance of our framework on
several examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2970</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2970</id><created>2014-02-12</created><updated>2014-05-08</updated><authors><author><keyname>Skowron</keyname><forenames>Piotr</forenames></author><author><keyname>Rzadca</keyname><forenames>Krzysztof</forenames></author><author><keyname>Datta</keyname><forenames>Anwitaman</forenames></author></authors><title>People are Processors: Coalitional Auctions for Complex Projects</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To successfully complete a complex project, be it a construction of an
airport or of a backbone IT system or crowd-sourced projects, agents (companies
or individuals) must form a team (a coalition) having required competences and
resources. A team can be formed either by the project issuer based on
individual agents' offers (centralized formation); or by the agents themselves
(decentralized formation) bidding for a project as a consortium---in that case
many feasible teams compete for the employment contract. In these models, we
investigate rational strategies of the agents (what salary should they ask?
with whom should they team up?) under different organizations of the market. We
propose various concepts allowing to characterize the stability of the winning
teams. We show that there may be no (rigorously) strongly winning coalition,
but the weakly winning and the auction-winning coalitions are guaranteed to
exist. In a general setting, with an oracle that decides whether a coalition is
feasible, we show how to find winning coalitions with a polynomial number of
calls to the oracle. We also determine the complexity of the problem in a
special case in which a project is a set of independent tasks. Each task must
be processed by a single agent, but processing speeds differ between agents and
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2991</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2991</id><created>2014-02-11</created><authors><author><keyname>Graillat</keyname><forenames>Stef</forenames><affiliation>LIP6</affiliation></author><author><keyname>Lef&#xe8;vre</keyname><forenames>Vincent</forenames><affiliation>Inria Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme</affiliation></author><author><keyname>Muller</keyname><forenames>Jean-Michel</forenames><affiliation>Inria Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme</affiliation></author></authors><title>On the maximum relative error when computing x^n in floating-point
  arithmetic</title><categories>cs.NA math.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we improve the usual relative error bound for the computation
of x^n through iterated multiplications by x in binary floating-point
arithmetic. The obtained error bound is only slightly better than the usual
one, but it is simpler. We also discuss the more general problem of computing
the product of n terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.2996</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.2996</id><created>2014-02-12</created><authors><author><keyname>Vilisov</keyname><forenames>Valery</forenames></author></authors><title>Robot Training Under Conditions of Incomplete Information</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of the works of the author about adaptive algorithms of
teaching the robotic systems with the help of operator is described here. An
operator is assumed to be an experience decision-maker and sane carrier of a
target which the robotic system needs to achieve. The works characteristic is
that the behavior of the robotic system is not specified a priori (as standard)
but is formed adaptively based on the information about the situation and
decisions made by a decision-maker. In this scheme the robotic system and the
decision-maker can cooperate in the normal operation mode of the robotic system
or in the time sharing mode with the possibility to plan actively the
experiment on the robotic system. If the adaptive scheme is chosen, there are
teaching stages and operating stages of the robotic system. At that the
decision-maker can act slowly having the possibility to weigh the decision
made. This way allows the robotic system reacting flexibly by switching between
preset models and respond to the environment instability. The data integrity
about the environment condition and about target preferences of an operator
plays a very important role in robotic system work. The effective work of the
robotic system depends on the effective settings of a preference model of the
robotic system based on the decisions of the decision-maker and on the
effective control. The influence of settings and control factors on the index
of effectiveness of the robotic system is subject of this work. The uncertainty
may be caused by the data flow limitation received by the operator on the stage
of the model setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3010</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3010</id><created>2014-02-12</created><authors><author><keyname>&#xd6;zkural</keyname><forenames>Eray</forenames></author><author><keyname>Aykanat</keyname><forenames>Cevdet</forenames></author></authors><title>1-D and 2-D Parallel Algorithms for All-Pairs Similarity Problem</title><categories>cs.IR cs.DC</categories><acm-class>H.3.3; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All-pairs similarity problem asks to find all vector pairs in a set of
vectors the similarities of which surpass a given similarity threshold, and it
is a computational kernel in data mining and information retrieval for several
tasks. We investigate the parallelization of a recent fast sequential
algorithm. We propose effective 1-D and 2-D data distribution strategies that
preserve the essential optimizations in the fast algorithm. 1-D parallel
algorithms distribute either dimensions or vectors, whereas the 2-D parallel
algorithm distributes data both ways. Additional contributions to the 1-D
vertical distribution include a local pruning strategy to reduce the number of
candidates, a recursive pruning algorithm, and block processing to reduce
imbalance. The parallel algorithms were programmed in OCaml which affords much
convenience. Our experiments indicate that the performance depends on the
dataset, therefore a variety of parallelizations is useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3011</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3011</id><created>2014-02-12</created><updated>2014-02-14</updated><authors><author><keyname>Marques-Silva</keyname><forenames>Joao</forenames></author><author><keyname>Janota</keyname><forenames>Mikolas</forenames></author></authors><title>Computing Minimal Sets on Propositional Formulae I: Problems &amp;
  Reductions</title><categories>cs.LO</categories><comments>This version contains some fixes in formatting and bibliography</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boolean Satisfiability (SAT) is arguably the archetypical NP-complete
decision problem. Progress in SAT solving algorithms has motivated an ever
increasing number of practical applications in recent years. However, many
practical uses of SAT involve solving function as opposed to decision problems.
Concrete examples include computing minimal unsatisfiable subsets, minimal
correction subsets, prime implicates and implicants, minimal models, backbone
literals, and autarkies, among several others. In most cases, solving a
function problem requires a number of adaptive or non-adaptive calls to a SAT
solver. Given the computational complexity of SAT, it is therefore important to
develop algorithms that either require the smallest possible number of calls to
the SAT solver, or that involve simpler instances. This paper addresses a
number of representative function problems defined on Boolean formulas, and
shows that all these function problems can be reduced to a generic problem of
computing a minimal set subject to a monotone predicate. This problem is
referred to as the Minimal Set over Monotone Predicate (MSMP) problem. This
exercise provides new ways for solving well-known function problems, including
prime implicates, minimal correction subsets, backbone literals, independent
variables and autarkies, among several others. Moreover, this exercise
motivates the development of more efficient algorithms for the MSMP problem.
Finally the paper outlines a number of areas of future research related with
extensions of the MSMP problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3021</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3021</id><created>2014-02-12</created><authors><author><keyname>Alur</keyname><forenames>Rajeev</forenames></author><author><keyname>Freilich</keyname><forenames>Adam</forenames></author><author><keyname>Raghothaman</keyname><forenames>Mukund</forenames></author></authors><title>Regular Combinators for String Transformations</title><categories>cs.FL</categories><comments>This is the full version, with omitted proofs and constructions, of
  the conference paper currently in submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on (partial) functions that map input strings to a monoid such as
the set of integers with addition and the set of output strings with
concatenation. The notion of regularity for such functions has been defined
using two-way finite-state transducers, (one-way) cost register automata, and
MSO-definable graph transformations. In this paper, we give an algebraic and
machine-independent characterization of this class analogous to the definition
of regular languages by regular expressions. When the monoid is commutative, we
prove that every regular function can be constructed from constant functions
using the combinators of choice, split sum, and iterated sum, that are analogs
of union, concatenation, and Kleene-*, respectively, but enforce unique (or
unambiguous) parsing. Our main result is for the general case of
non-commutative monoids, which is of particular interest for capturing regular
string-to-string transformations for document processing. We prove that the
following additional combinators suffice for constructing all regular
functions: (1) the left-additive versions of split sum and iterated sum, which
allow transformations such as string reversal; (2) sum of functions, which
allows transformations such as copying of strings; and (3) function
composition, or alternatively, a new concept of chained sum, which allows
output values from adjacent blocks to mix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3022</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3022</id><created>2014-02-12</created><updated>2014-06-15</updated><authors><author><keyname>Zgonnikov</keyname><forenames>Arkady</forenames></author><author><keyname>Lubashevsky</keyname><forenames>Ihor</forenames></author><author><keyname>Kanemoto</keyname><forenames>Shigeru</forenames></author><author><keyname>Miyazawa</keyname><forenames>Toru</forenames></author><author><keyname>Suzuki</keyname><forenames>Takashi</forenames></author></authors><title>To react or not to react? Intrinsic stochasticity of human control in
  virtual stick balancing</title><categories>physics.bio-ph cs.SY nlin.AO q-bio.NC</categories><comments>18 pages, 9 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding how humans control unstable systems is central to many research
problems, with applications ranging from quiet standing to aircraft landing.
Increasingly much evidence appears in favor of event-driven control hypothesis:
human operators only start actively controlling the system when the discrepancy
between the current and desired system states becomes large enough. The
event-driven models based on the concept of threshold can explain many features
of the experimentally observed dynamics. However, much still remains unclear
about the dynamics of human-controlled systems, which likely indicates that
humans employ more intricate control mechanisms. The present paper argues that
control activation in humans may be not threshold-driven, but instead
intrinsically stochastic, noise-driven. Specifically, we suggest that control
activation stems from stochastic interplay between the operator's need to keep
the controlled system near the goal state on one hand and the tendency to
postpone interrupting the system dynamics on the other hand. We propose a model
capturing this interplay and show that it matches the experimental data on
human balancing of virtual overdamped stick. Our results illuminate that the
noise-driven activation mechanism plays a crucial role at least in the
considered task, and, hypothetically, in a broad range of human-controlled
processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3032</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3032</id><created>2014-02-13</created><authors><author><keyname>Zhang</keyname><forenames>Ziming</forenames></author></authors><title>Regularization for Multiple Kernel Learning via Sum-Product Networks</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we are interested in constructing general graph-based
regularizers for multiple kernel learning (MKL) given a structure which is used
to describe the way of combining basis kernels. Such structures are represented
by sum-product networks (SPNs) in our method. Accordingly we propose a new
convex regularization method for MLK based on a path-dependent kernel weighting
function which encodes the entire SPN structure in our method. Under certain
conditions and from the view of probability, this function can be considered to
follow multinomial distributions over the weights associated with product nodes
in SPNs. We also analyze the convexity of our regularizer and the complexity of
our induced classifiers, and further propose an efficient wrapper algorithm to
optimize our formulation. In our experiments, we apply our method to ......
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3034</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3034</id><created>2014-02-13</created><authors><author><keyname>Singh</keyname><forenames>Sukhpal</forenames></author><author><keyname>Chana</keyname><forenames>Inderveer</forenames></author></authors><title>Formal Specification Language Based IaaS Cloud Workload Regression
  Analysis</title><categories>cs.DC</categories><comments>6 Pages including 2 figures, Published by IEEE Joint Chapter of
  IE/PEL/CS under IEEE UP section</comments><report-no>978-1?4799?1375?6/13/$31.00 {copyright}2013 IEEE</report-no><msc-class>97P70</msc-class><acm-class>J.7</acm-class><journal-ref>Presented in the IEEE International Conference on Control,
  Computing, Communication and Materials (ICCCCM-2013) held at UIT, Allahabad,
  India on August 03-04, 2013</journal-ref><doi>10.1109/ICCCCM.2013.6648901</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Cloud Computing is an emerging area for accessing computing resources. In
general, Cloud service providers offer services that can be clustered into
three categories: SaaS, PaaS and IaaS. This paper discusses the Cloud workload
analysis. The efficient Cloud workload resource mapping technique is proposed.
This paper aims to provide a means of understanding and investigating IaaS
Cloud workloads and the resources. In this paper, regression analysis is used
to analyze the Cloud workloads and identifies the relationship between Cloud
workloads and available resources. The effective organization of dynamic nature
resources can be done with the help of Cloud workloads. Till Cloud workload is
considered a vital talent, the Cloud resources cannot be consumed in an
effective style. The proposed technique has been validated by Z Formal
specification language. This approach is effective in minimizing the cost and
submission burst time of Cloud workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3036</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3036</id><created>2014-02-13</created><authors><author><keyname>Morgenthaler</keyname><forenames>J. David</forenames></author><author><keyname>Hu</keyname><forenames>T. C.</forenames></author></authors><title>Optimal Alphabetic Ternary Trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new algorithm to construct optimal alphabetic ternary trees, where
every internal node has at most three children. This algorithm generalizes the
classic Hu-Tucker algorithm, though the overall computational complexity has
yet to be determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3040</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3040</id><created>2014-02-13</created><authors><author><keyname>Hong</keyname><forenames>Jia-Fei</forenames></author><author><keyname>Ahrens</keyname><forenames>Kathleen</forenames></author><author><keyname>Huang</keyname><forenames>Chu-Ren</forenames></author></authors><title>Event Structure of Transitive Verb: A MARVS perspective</title><categories>cs.CL</categories><comments>Chinese Lexical Semantics Workshop 2011(CLSW 2011) published in
  International Journal of Computer Processing of Languages (Vol. 24, No. 01,
  March 2012)</comments><journal-ref>International Journal of Computer Processing of Languages (Vol.
  24, No. 01, March 2012)</journal-ref><doi>10.1142/S179384061240003X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Module-Attribute Representation of Verbal Semantics (MARVS) is a theory of
the representation of verbal semantics that is based on Mandarin Chinese data
(Huang et al. 2000). In the MARVS theory, there are two different types of
modules: Event Structure Modules and Role Modules. There are also two sets of
attributes: Event-Internal Attributes and Role-Internal Attributes, which are
linked to the Event Structure Module and the Role Module, respectively. In this
study, we focus on four transitive verbs as chi1(eat), wan2(play),
huan4(change) and shao1(burn) and explore their event structures by the MARVS
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3044</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3044</id><created>2014-02-13</created><updated>2016-01-08</updated><authors><author><keyname>Skowron</keyname><forenames>Piotr</forenames></author><author><keyname>Faliszewski</keyname><forenames>Piotr</forenames></author><author><keyname>Lang</keyname><forenames>Jerome</forenames></author></authors><title>Finding a Collective Set of Items: From Proportional Multirepresentation
  to Group Recommendation</title><categories>cs.GT cs.AI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following problem: There is a set of items (e.g., movies) and
a group of agents (e.g., passengers on a plane); each agent has some intrinsic
utility for each of the items. Our goal is to pick a set of $K$ items that
maximize the total derived utility of all the agents (i.e., in our example we
are to pick $K$ movies that we put on the plane's entertainment system).
However, the actual utility that an agent derives from a given item is only a
fraction of its intrinsic one, and this fraction depends on how the agent ranks
the item among the chosen, available, ones. We provide a formal specification
of the model and provide concrete examples and settings where it is applicable.
We show that the problem is hard in general, but we show a number of
tractability results for its natural special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3066</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3066</id><created>2014-02-13</created><updated>2014-04-26</updated><authors><author><keyname>Achilleos</keyname><forenames>Antonis</forenames></author></authors><title>Complexity Jumps In Multiagent Justification Logic Under Interacting
  Justifications</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Logic of Proofs, LP, and its successor, Justification Logic, is a
refinement of the modal logic approach to epistemology in which
proofs/justifications are taken into account. In 2000 Kuznets showed that
satisfiability for LP is in the second level of the polynomial hierarchy, a
result which has been successfully repeated for all other one-agent
justification logics whose complexity is known.
  We introduce a family of multi-agent justification logics with interactions
between the agents' justifications, by extending and generalizing the two-agent
versions of the Logic of Proofs introduced by Yavorskaya in 2008. Known
concepts and tools from the single-agent justification setting are adjusted for
this multiple agent case. We present tableau rules and some preliminary
complexity results. In several cases the satisfiability problem for these
logics remains in the second level of the polynomial hierarchy, while for
others it is PSPACE or EXP-hard. Furthermore, this problem becomes PSPACE-hard
even for certain two-agent logics, while there are EXP-hard logics of three
agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3067</identifier>
 <datestamp>2014-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3067</id><created>2014-02-13</created><updated>2014-07-11</updated><authors><author><keyname>Baez</keyname><forenames>John C.</forenames></author><author><keyname>Fritz</keyname><forenames>Tobias</forenames></author></authors><title>A Bayesian Characterization of Relative Entropy</title><categories>cs.IT math-ph math.IT math.MP math.PR quant-ph</categories><comments>32 pages, minor revision</comments><msc-class>Primary 94A17, Secondary 62F15, 18B99</msc-class><journal-ref>Theory and Applications of Categories, Vol. 29, 2014, No. 16, pp
  421-456</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new characterization of relative entropy, also known as the
Kullback-Leibler divergence. We use a number of interesting categories related
to probability theory. In particular, we consider a category FinStat where an
object is a finite set equipped with a probability distribution, while a
morphism is a measure-preserving function $f: X \to Y$ together with a
stochastic right inverse $s: Y \to X$. The function $f$ can be thought of as a
measurement process, while s provides a hypothesis about the state of the
measured system given the result of a measurement. Given this data we can
define the entropy of the probability distribution on $X$ relative to the
&quot;prior&quot; given by pushing the probability distribution on $Y$ forwards along
$s$. We say that $s$ is &quot;optimal&quot; if these distributions agree. We show that
any convex linear, lower semicontinuous functor from FinStat to the additive
monoid $[0,\infty]$ which vanishes when $s$ is optimal must be a scalar
multiple of this relative entropy. Our proof is independent of all earlier
characterizations, but inspired by the work of Petz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3070</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3070</id><created>2014-02-13</created><authors><author><keyname>Gupta</keyname><forenames>Parth</forenames></author><author><keyname>Banchs</keyname><forenames>Rafael E.</forenames></author><author><keyname>Rosso</keyname><forenames>Paolo</forenames></author></authors><title>Squeezing bottlenecks: exploring the limits of autoencoder semantic
  representation capabilities</title><categories>cs.IR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a comprehensive study on the use of autoencoders for modelling
text data, in which (differently from previous studies) we focus our attention
on the following issues: i) we explore the suitability of two different models
bDA and rsDA for constructing deep autoencoders for text data at the sentence
level; ii) we propose and evaluate two novel metrics for better assessing the
text-reconstruction capabilities of autoencoders; and iii) we propose an
automatic method to find the critical bottleneck dimensionality for text
language representations (below which structural information is lost).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3072</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3072</id><created>2014-02-13</created><updated>2014-02-19</updated><authors><author><keyname>Yun</keyname><forenames>Se-Young</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author></authors><title>Community Detection via Random and Adaptive Sampling</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider networks consisting of a finite number of
non-overlapping communities. To extract these communities, the interaction
between pairs of nodes may be sampled from a large available data set, which
allows a given node pair to be sampled several times. When a node pair is
sampled, the observed outcome is a binary random variable, equal to 1 if nodes
interact and to 0 otherwise. The outcome is more likely to be positive if nodes
belong to the same communities. For a given budget of node pair samples or
observations, we wish to jointly design a sampling strategy (the sequence of
sampled node pairs) and a clustering algorithm that recover the hidden
communities with the highest possible accuracy. We consider both non-adaptive
and adaptive sampling strategies, and for both classes of strategies, we derive
fundamental performance limits satisfied by any sampling and clustering
algorithm. In particular, we provide necessary conditions for the existence of
algorithms recovering the communities accurately as the network size grows
large. We also devise simple algorithms that accurately reconstruct the
communities when this is at all possible, hence proving that the proposed
necessary conditions for accurate community detection are also sufficient. The
classical problem of community detection in the stochastic block model can be
seen as a particular instance of the problems consider here. But our framework
covers more general scenarios where the sequence of sampled node pairs can be
designed in an adaptive manner. The paper provides new results for the
stochastic block model, and extends the analysis to the case of adaptive
sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3074</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3074</id><created>2014-02-13</created><authors><author><keyname>Ferner</keyname><forenames>Ulric J.</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Aboutorab</keyname><forenames>Neda</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Scheduling Advantages of Network Coded Storage in Point-to-Multipoint
  Networks</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider scheduling strategies for point-to-multipoint (PMP) storage area
networks (SANs) that use network coded storage (NCS). In particular, we present
a simple SAN system model, two server scheduling algorithms for PMP networks,
and analytical expressions for internal and external blocking probability. We
point to select scheduling advantages in NCS systems under normal operating
conditions, where content requests can be temporarily denied owing to finite
system capacity from drive I/O access or storage redundancy limitations. NCS
can lead to improvements in throughput and blocking probability due to
increased immediate scheduling options, and complements other well documented
NCS advantages such as regeneration, and can be used as a guide for future
storage system design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3080</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3080</id><created>2014-02-13</created><authors><author><keyname>Viswam</keyname><forenames>Santhy</forenames></author><author><keyname>Karattil</keyname><forenames>Sajeer</forenames></author></authors><title>Software Requirement Specification Using Reverse Speech Technology</title><categories>cs.CL cs.SD</categories><comments>5 pages, International Journal of Computer Trends and Technology
  (IJCTT) vol.5 no.4</comments><msc-class>91F20(primary) 03B65, 68T50(secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech analysis had been taken to a new level with the discovery of Reverse
Speech (RS). RS is the discovery of hidden messages, referred as reversals, in
normal speech. Works are in progress for exploiting the relevance of RS in
different real world applications such as investigation, medical field etc. In
this paper we represent an innovative method for preparing a reliable Software
Requirement Specification (SRS) document with the help of reverse speech. As
SRS act as the backbone for the successful completion of any project, a
reliable method is needed to overcome the inconsistencies. Using RS such a
reliable method for SRS documentation was developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3091</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3091</id><created>2014-02-13</created><authors><author><keyname>Drexler-Lemire</keyname><forenames>Christopher</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>Notes and Note-Pairs in Noergaard's Infinity Series</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Danish composer Per Noergaard defined the &quot;infinity series&quot; s =
(s(n))_n&gt;=0 by the rules s(0) = 0, s(2n) = -s(n) for n &gt;= 1, and s(2n + 1) =
s(n) + 1 for n &gt;= 0; it figures prominently in many of his compositions. Here
we give several new results about this sequence: first, the set of binary
representations of the positions of each note forms a context-free language
that is not regular; second, a complete characterization of exactly which
note-pairs appear; third, that consecutive occurrences of identical phrases are
widely separated. We also consider to what extent the infinity series is
unique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3096</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3096</id><created>2014-02-13</created><authors><author><keyname>Deli</keyname><forenames>Irfan</forenames></author><author><keyname>&#xc7;a&#x11f;man</keyname><forenames>Naim</forenames></author></authors><title>Relations on FP-Soft Sets Applied to Decision Making Problems</title><categories>math.LO cs.AI</categories><comments>soft applications</comments><journal-ref>Journal of New Theory 3 (2015) 98-107</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we first define relations on the fuzzy parametrized soft sets
and study their properties. We also give a decision making method based on
these relations. In approximate reasoning, relations on the fuzzy parametrized
soft sets have shown to be of a primordial importance. Finally, the method is
successfully applied to a problems that contain uncertainties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3107</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3107</id><created>2014-02-13</created><authors><author><keyname>Abdelhafid</keyname><forenames>Zitouni</forenames></author></authors><title>Rigorous Description Of Design Components Functionality: An Approach
  Based Contract</title><categories>cs.SE</categories><comments>10pages, 7 figures</comments><journal-ref>IJCSI : International Journal of Computer Science Issues, ISSN:
  1694-0814, Vol. 9, Issue. 1, pp 187-196, January 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current models for software components have made component-based software
engineering practical. However, these models are limited in the sense that
their support for the characterization/specification of design components
primarily deals with syntactic issues. To avoid mismatch and misuse of
components, more comprehensive specification of software components is
required, In this paper, we present a contract-based approach to analyze and
model the both aspects (functional and non-functional) properties of design
components and their composition in order to detect and correct composition
errors. This approach permits to characterize the structural, interface and
behavioural aspects of design component. To enable this we present a pattern
contract language that captures the structural and behavioral requirements
associated with a range of patterns, as well as the system properties that are
guaranteed as a result. In addition, we propose the use of the LOTOS language
as an ADL for formalizing these aspects. We illustrate the approach by applying
it to a standard design
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3119</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3119</id><created>2014-02-13</created><authors><author><keyname>Ntranos</keyname><forenames>Vasilis</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Cellular Interference Alignment</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment promises that, in Gaussian interference channels, each
link can support half of a degree of freedom (DoF) per pair of transmit-receive
antennas. However, in general, this result requires to precode the data bearing
signals over a signal space of asymptotically large diversity, e.g., over an
infinite number of dimensions for time-frequency varying fading channels, or
over an infinite number of rationally independent signal levels, in the case of
time-frequency invariant channels. In this work we consider a wireless cellular
system scenario where the promised optimal DoFs are achieved with linear
precoding in one-shot (i.e., over a single time-frequency slot). We focus on
the uplink of a symmetric cellular system, where each cell is split into three
sectors with orthogonal intra-sector multiple access. In our model,
interference is &quot;local&quot;, i.e., it is due to transmitters in neighboring cells
only. We consider a message-passing backhaul network architecture, in which
nearby sectors can exchange already decoded messages and propose an alignment
solution that can achieve the optimal DoFs. To avoid signaling schemes relying
on the strength of interference, we further introduce the notion of
\emph{topologically robust} schemes, which are able to guarantee a minimum rate
(or DoFs) irrespectively of the strength of the interfering links. Towards this
end, we design an alignment scheme which is topologically robust and still
achieves the same optimum DoFs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3125</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3125</id><created>2014-02-13</created><authors><author><keyname>Jakobsen</keyname><forenames>Sune K</forenames></author></authors><title>Information Theoretical Cryptogenography</title><categories>cs.CR cs.IT math.IT</categories><comments>34 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider problems where $n$ people are communicating and a random subset
of them is trying to leak information, without making it clear who are leaking
the information. We introduce a measure of suspicion, and show that the amount
of leaked information will always be bounded by the expected increase in
suspicion, and that this bound is tight. We ask the question: Suppose a large
number of people have some information they want to leak, but they want to
ensure that after the communication, an observer will assign probability at
most $c$ to the events that each of them is trying to leak the information. How
much information can they reliably leak, per person who is leaking? We show
that the answer is $- \frac{\log(1-c)}{c} -\log(e)$ bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3138</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3138</id><created>2014-02-13</created><updated>2014-05-02</updated><authors><author><keyname>Chen</keyname><forenames>Jeremy</forenames></author></authors><title>Social Networks and the Choices People Make</title><categories>cs.SI</categories><acm-class>J.1; J.4; K.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social marketing is becoming increasingly important in contemporary business.
Central to social marketing is quantifying how consumers choose between
alternatives and how they influence each other. This work considers a new but
simple multinomial choice model for multiple agents connected in a
recommendation network based on the explicit modeling of choice adoption
behavior. Efficiently computable closed-form solutions, absent from analyses of
threshold/cascade models, are obtained together with insights on how the
network affects aggregate decision making. A stylized &quot;brand ambassador&quot;
selection problem is posed to model targeting in social marketing. Therein, it
is shown that a greedy selection strategy leads to solutions achieving at least
$1-1/e$ of the optimal value. In an extended example of imposing exogenous
controls, a pricing problem is considered wherein it is shown that the single
player profit optimization problem is concave, implying the existence of pure
strategy equilibria for the associated pricing game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3144</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3144</id><created>2014-02-13</created><updated>2014-10-21</updated><authors><author><keyname>Claesen</keyname><forenames>Marc</forenames></author><author><keyname>De Smet</keyname><forenames>Frank</forenames></author><author><keyname>Suykens</keyname><forenames>Johan A. K.</forenames></author><author><keyname>De Moor</keyname><forenames>Bart</forenames></author></authors><title>A Robust Ensemble Approach to Learn From Positive and Unlabeled Data
  Using SVM Base Models</title><categories>stat.ML cs.LG</categories><comments>34 pages, 6 figures, 6 tables. Accepted for publication in
  Neurocomputing: Special Issue on Advances in Learning with Label Noise</comments><acm-class>G.3; I.2.6; I.5.1</acm-class><doi>10.1016/j.neucom.2014.10.081</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach to learn binary classifiers when only positive
and unlabeled instances are available (PU learning). This problem is routinely
cast as a supervised task with label noise in the negative set. We use an
ensemble of SVM models trained on bootstrap resamples of the training data for
increased robustness against label noise. The approach can be considered in a
bagging framework which provides an intuitive explanation for its mechanics in
a semi-supervised setting. We compared our method to state-of-the-art
approaches in simulations using multiple public benchmark data sets. The
included benchmark comprises three settings with increasing label noise: (i)
fully supervised, (ii) PU learning and (iii) PU learning with false positives.
Our approach shows a marginal improvement over existing methods in the second
setting and a significant improvement in the third.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3149</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3149</id><created>2014-02-13</created><authors><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Dong</keyname><forenames>Sheqin</forenames></author><author><keyname>Chen</keyname><forenames>Song</forenames></author><author><keyname>Goto</keyname><forenames>Satoshi</forenames></author></authors><title>Voltage and Level-Shifter Assignment Driven Floorplanning</title><categories>cs.AR</categories><comments>IEICE Transactions on Fundamentals of Electronics, Communications and
  Computer Sciences, 2009</comments><journal-ref>IEICE transactions on fundamentals of electronics, communications
  and computer sciences 92.12 (2009): 2990-2997</journal-ref><doi>10.1587/transfun.E92.A.2990</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low Power Design has become a significant requirement when the CMOS
technology entered the nanometer era. Multiple-Supply Voltage (MSV) is a
popular and effective method for both dynamic and static power reduction while
maintaining performance. Level shifters may cause area and Interconnect Length
Overhead (ILO), and should be considered at both floorplanning and
post-floorplanning stages. In this paper, we propose a two phases algorithm
framework, called VLSAF, to solve voltage and level shifter assignment problem.
At floorplanning phase, we use a convex cost network flow algorithm to assign
voltage and a minimum cost flow algorithm to handle level-shifter assignment.
At post-floorplanning phase, a heuristic method is adopted to redistribute
white spaces and calculate the positions and shapes of level shifters. The
experimental results show VLSAF is effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3150</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3150</id><created>2014-02-13</created><authors><author><keyname>Gao</keyname><forenames>Jhih-Rong</forenames></author><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Pan</keyname><forenames>David Z.</forenames></author></authors><title>Lithography Hotspot Detection and Mitigation in Nanometer VLSI</title><categories>cs.AR</categories><comments>ASICON 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With continued feature size scaling, even state of the art semiconductor
manufacturing processes will often run into layouts with poor printability and
yield. Identifying lithography hotspots is important at both physical
verification and early physical design stages. While detailed lithography
simulations can be very accurate, they may be too computationally expensive for
full-chip scale and physical design inner loops. Meanwhile, pattern matching
and machine learning based hotspot detection methods can provide acceptable
quality and yet fast turn-around-time for full-chip scale physical verification
and design. In this paper, we discuss some key issues and recent results on
lithography hotspot detection and mitigation in nanometer VLSI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3163</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3163</id><created>2014-02-13</created><updated>2015-02-23</updated><authors><author><keyname>Yang</keyname><forenames>Xiaohao</forenames></author><author><keyname>Juhas</keyname><forenames>Pavol</forenames></author><author><keyname>Farrow</keyname><forenames>Christopher L.</forenames></author><author><keyname>Billinge</keyname><forenames>Simon J. L.</forenames></author></authors><title>xPDFsuite: an end-to-end software solution for high throughput pair
  distribution function transformation, visualization and analysis</title><categories>cond-mat.mtrl-sci cs.HC</categories><comments>3 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The xPDFsuite software program is described. It is for processing and
analyzing atomic pair distribution functions (PDF) from X-ray powder
diffraction data. It provides a convenient GUI for SrXplanr and PDFgetX3,
allowing the users to easily obtain 1D diffraction pattern from raw 2D
diffraction images and then transform them to PDFs. It also bundles PDFgui
which allows the users to create structure models and fit to the experiment
data. It is specially useful for working with large numbers of datasets such as
from high throughout measurements. Some of the key features are: real time PDF
transformation and plotting; 2D waterfall, false color heatmap, and 3D contour
plotting for multiple datasets; static and dynamic mask editing; geometric
calibration of powder diffraction image; configurations and project saving and
loading; Pearson correlation analysis on selected datasets; written in Python
and support multiple platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3173</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3173</id><created>2014-02-13</created><authors><author><keyname>S&#xfd;kora</keyname><forenames>Jan</forenames></author><author><keyname>&#x160;ejnoha</keyname><forenames>Michal</forenames></author><author><keyname>&#x160;ejnoha</keyname><forenames>Ji&#x159;&#xed;</forenames></author></authors><title>Homogenization of coupled heat and moisture transport in masonry
  structures including interfaces</title><categories>cs.CE</categories><doi>10.1016/j.amc.2011.02.050</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Homogenization of a simultaneous heat and moisture flow in a masonry wall is
presented in this paper. The principle objective is to examine an impact of the
assumed imperfect hydraulic contact on the resulting homogenized properties.
Such a contact is characterized by a certain mismatching resistance allowing us
to represent a discontinuous evolution of temperature and moisture fields
across the interface, which is in general attributed to discontinuous capillary
pressures caused by different pore size distributions of the adjacent porous
materials. In achieving this, two particular laboratory experiments were
performed to provide distributions of temperature and relative humidity in a
sample of the masonry wall, which in turn served to extract the corresponding
jumps and subsequently to obtain the required interface transition parameters
by matching numerical predictions and experimental results. The results suggest
a low importance of accounting for imperfect hydraulic contact for the
derivation of macroscopic homogenized properties. On the other hand, they
strongly support the need for a fully coupled multi-scale analysis due to
significant dependence of the homogenized properties on actual moisture
gradients and corresponding values of both macroscopic temperature and relative
humidity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3174</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3174</id><created>2014-02-13</created><authors><author><keyname>S&#xfd;kora</keyname><forenames>J.</forenames></author></authors><title>Modeling of Degradation Processes in Historical Mortars</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of presented paper is modeling of degradation processes in historical
mortars exposed to moisture impact during freezing. Internal damage caused by
ice crystallization in pores is one of the most important factors limiting the
service life of historical structures. Coupling the transport processes with
the mechanical part will allow us to address the impact of moisture on the
durability, strength and stiffness of mortars. This should be accomplished with
the help of a complex thermo-hygro-mechanical model representing one of the
prime objectives of this work. The proposed formulation is based on the
extension of the classical poroelasticity models with the damage mechanics. An
example of two-dimensional moisture transport in the environment with
temperature below freezing point is presented to support the theoretical
derivations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3175</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3175</id><created>2014-02-13</created><updated>2015-07-06</updated><authors><author><keyname>Kova&#x10d;evi&#x107;</keyname><forenames>Mladen</forenames></author><author><keyname>Stanojevi&#x107;</keyname><forenames>Ivan</forenames></author><author><keyname>&#x160;enk</keyname><forenames>Vojin</forenames></author></authors><title>Information-Geometric Equivalence of Transportation Polytopes</title><categories>cs.IT math.CO math.IT</categories><comments>6 pages, 1 figure. v2: A remark concerning Frechet-Hoeffding bounds
  is added</comments><msc-class>94A17, 52B11, 52B12, 62B10, 62H17, 54C99</msc-class><journal-ref>Probl. Inf. Transm., vol. 51, no. 2, pp. 103-109, Apr. 2015</journal-ref><doi>10.1134/S0032946015020027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with transportation polytopes in the probability simplex
(that is, sets of categorical bivariate probability distributions with
prescribed marginals). Information projections between such polytopes are
studied, and a sufficient condition is described under which these mappings are
homeomorphisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3193</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3193</id><created>2014-02-13</created><authors><author><keyname>Bauckhage</keyname><forenames>Christian</forenames></author></authors><title>Characterizations and Kullback-Leibler Divergence of Gompertz
  Distributions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we characterize the Gompertz distribution in terms of extreme
value distributions and point out that it implicitly models the interplay of
two antagonistic growth processes. In addition, we derive a closed form
expressions for the Kullback-Leibler divergence between two Gompertz
Distributions. Although the latter is rather easy to obtain, it seems not to
have been widely reported before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3198</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3198</id><created>2014-02-13</created><authors><author><keyname>Kroll</keyname><forenames>Martin</forenames></author></authors><title>A graph theoretic linkage attack on microdata in a metric space</title><categories>cs.CR</categories><comments>24 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Certain methods of analysis require the knowledge of the spatial distances
between entities whose data are stored in a microdata table. For instance, such
knowledge is necessary and sufficient to perform data mining tasks such as
nearest neighbour searches or clustering. However, when inter-record distances
are published in addition to the microdata for research purposes, the risk of
identity disclosure has to be taken into consideration again. In order to
tackle this problem, we introduce a flexible graph model for microdata in a
metric space and propose a linkage attack based on realistic assumptions of a
data snooper's background knowledge. This attack is based on the idea of
finding a maximum approximate common subgraph of two vertex-labelled and
edge-weighted graphs. By adapting a standard argument from algorithmic graph
theory to our setup, this task is transformed to the maximum clique detection
problem in a corresponding product graph. Using a toy example and experimental
results on simulated data show that publishing even approximate distances could
increase the risk of identity disclosure unreasonably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3199</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3199</id><created>2014-02-13</created><authors><author><keyname>Chaturvedi</keyname><forenames>Namit</forenames></author><author><keyname>Gelderie</keyname><forenames>Marcus</forenames></author></authors><title>Weak $\omega$-Regular Trace Languages</title><categories>cs.FL</categories><comments>12 pages in main body, 4 pages in appendix, 2 figures</comments><msc-class>20M35, 68Q45, 68Q85</msc-class><acm-class>F.1.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mazurkiewicz traces describe concurrent behaviors of distributed systems.
Trace-closed word languages, which are &quot;linearizations&quot; of trace languages,
constitute a weaker notion of concurrency but still give us tools to
investigate the latter. In this vein, our contribution is twofold. Firstly, we
develop definitions that allow classification of $\omega$-regular trace
languages in terms of the corresponding trace-closed $\omega$-regular word
languages, capturing E-recognizable (reachability) and (deterministically)
B\&quot;uchi recognizable languages. Secondly, we demonstrate the first
automata-theoretic result that shows the equivalence of $\omega$-regular
trace-closed word languages and Boolean combinations of deterministically
$I$-diamond B\&quot;uchi recognizable trace-closed languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3210</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3210</id><created>2014-02-13</created><authors><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author><author><keyname>Fletcher</keyname><forenames>Alyson K.</forenames></author></authors><title>On the Convergence of Approximate Message Passing with Arbitrary
  Matrices</title><categories>cs.IT math.IT</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate message passing (AMP) methods and their variants have attracted
considerable recent attention for the problem of estimating a random vector
$\mathbf{x}$ observed through a linear transform $\mathbf{A}$. In the case of
large i.i.d. $\mathbf{A}$, the methods exhibit fast convergence with precise
analytic characterizations on the algorithm behavior. However, the convergence
of AMP under general transforms is not fully understood. In this paper, we
provide sufficient conditions for the convergence of a damped version of the
generalized AMP (GAMP) algorithm in the case of Gaussian distributions. It is
shown that, with sufficient damping the algorithm can be guaranteed to
converge, but the amount of damping grows with peak-to-average ratio of the
squared singular values of $\mathbf{A}$. This condition explains the good
performance of AMP methods on i.i.d. matrices, but also their difficulties with
other classes of transforms. A related sufficient condition is then derived for
the local stability of the damped GAMP method under more general (possibly
non-Gaussian) distributions, assuming certain strict convexity conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3213</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3213</id><created>2014-02-13</created><authors><author><keyname>Aladren</keyname><forenames>Aitor</forenames></author><author><keyname>Bodiroza</keyname><forenames>Sasa</forenames></author><author><keyname>Chitsaz</keyname><forenames>Hamidreza</forenames></author><author><keyname>Guerrero</keyname><forenames>J. J.</forenames></author><author><keyname>Hafner</keyname><forenames>Verena</forenames></author><author><keyname>Hauser</keyname><forenames>Kris</forenames></author><author><keyname>Jevtic</keyname><forenames>Aleksandar</forenames></author><author><keyname>Kazemi</keyname><forenames>Moslem</forenames></author><author><keyname>Lara</keyname><forenames>Bruno</forenames></author><author><keyname>Lopez-Nicolas</keyname><forenames>Gonzalo</forenames></author><author><keyname>Neubert</keyname><forenames>Peer</forenames></author><author><keyname>Protzel</keyname><forenames>Peter</forenames></author><author><keyname>Riek</keyname><forenames>Laurel D.</forenames></author><author><keyname>Sunderhauf</keyname><forenames>Niko</forenames></author><author><keyname>Yap</keyname><forenames>Chee</forenames></author></authors><title>Proceedings of the 1st Workshop on Robotics Challenges and Vision
  (RCV2013)</title><categories>cs.RO</categories><comments>http://compbio.cs.wayne.edu/robotics/rcv2013/proceedings-emb.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proceedings of the 1st Workshop on Robotics Challenges and Vision (RCV2013)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3215</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3215</id><created>2014-02-13</created><authors><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author></authors><title>Analysis of Compressed Sensing with Spatially-Coupled Orthogonal
  Matrices</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent development in compressed sensing (CS) has revealed that the use of a
special design of measurement matrix, namely the spatially-coupled matrix, can
achieve the information-theoretic limit of CS. In this paper, we consider the
measurement matrix which consists of the spatially-coupled \emph{orthogonal}
matrices. One example of such matrices are the randomly selected discrete
Fourier transform (DFT) matrices. Such selection enjoys a less memory
complexity and a faster multiplication procedure. Our contributions are the
replica calculations to find the mean-square-error (MSE) of the Bayes-optimal
reconstruction for such setup. We illustrate that the reconstruction thresholds
under the spatially-coupled orthogonal and Gaussian ensembles are quite
different especially in the noisy cases. In particular, the spatially coupled
orthogonal matrices achieve the faster convergence rate, the lower measurement
rate, and the reduced MSE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3225</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3225</id><created>2014-02-13</created><updated>2014-09-30</updated><authors><author><keyname>Lotfi</keyname><forenames>Mohammad Hassan</forenames></author><author><keyname>Kesidis</keyname><forenames>George</forenames></author><author><keyname>Sarkar</keyname><forenames>Saswati</forenames></author></authors><title>Market-Based Power Allocation for a Differentially Priced FDMA System</title><categories>cs.IT cs.GT cs.NI math.IT</categories><comments>5 Pages, 2 Figures, Conference Publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of differential pricing and QoS
assignment by a broadband data provider. In our model, the broadband data
provider decides on the power allocated to an end-user not only based on
parameters of the transmission medium, but also based on the price the user is
willing to pay. In addition, end-users bid the price that they are willing to
pay to the BTS based on their channel condition, the throughput they require,
and their belief about other users' parameters. We will characterize the
optimum power allocation by the BTS which turns out to be a modification of the
solution to the well-known water-filling problem. We also characterize the
optimum bidding strategy of end-users using the belief of each user about the
cell condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3236</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3236</id><created>2014-02-13</created><authors><author><keyname>Ali</keyname><forenames>Abdulaziz</forenames></author><author><keyname>Bothe</keyname><forenames>Dieter</forenames></author></authors><title>A graph-theoretical approach for the computation of connected
  iso-surfaces based on volumetric data</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existing combinatorial methods for iso-surface computation are efficient
for pure visualization purposes, but it is known that the resulting
iso-surfaces can have holes, and topological problems like missing or wrong
connectivity can appear. To avoid such problems, we introduce a
graph-theoretical method for the computation of iso-surfaces on cuboid meshes
in $\mathbb{R}^3$. The method for the generation of iso-surfaces employs
labeled cuboid graphs $G(V,E,\mathcal{F})$ such that $V$ is the set of vertices
of a cuboid $C\subset\mathbb{R}^3$, $E$ is the set of edges of $C$ and
$\mathcal{F}\,:\,V\rightarrow [0,1]$. The nodes of $G$ are weighted by the
values of $\mathcal{F}$ which represents the volumetric information, e.g.\ from
a Volume of Fluid method. Using a given iso-level $c\in (0,1)$, we first obtain
all iso-points, i.e.\ points where the value $c$ is attained by the
edge-interpolated $\mathcal{F}$-field. The iso-surface is then built from
iso-elements which are composed of triangles and are such that their polygonal
boundary has only iso-points as vertices. All vertices lie on the faces of a
single mesh cell.
  We give a proof that the generated iso-surface is connected up to the
boundary of the domain and it can be decomposed into different oriented
components. Two different components may have discrete points or line segments
in common. The graph-theoretical method for the computation of iso-surfaces
developed in this paper enables to recover local information of the iso-surface
that can be used e.g.\ to compute discrete mean curvature and to solve surface
PDEs. Concerning the computational effort, the resulting algorithm is as
efficient as existing combinatorial methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3247</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3247</id><created>2014-02-13</created><updated>2014-02-21</updated><authors><author><keyname>Blasco</keyname><forenames>Pol</forenames></author><author><keyname>Gunduz</keyname><forenames>Deniz</forenames></author></authors><title>Learning-Based Optimization of Cache Content in a Small Cell Base
  Station</title><categories>cs.IT math.IT</categories><comments>Accepted to IEEE ICC 2014, Sydney, Australia. Minor typos corrected.
  Algorithm MCUCB corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal cache content placement in a wireless small cell base station (sBS)
with limited backhaul capacity is studied. The sBS has a large cache memory and
provides content-level selective offloading by delivering high data rate
contents to users in its coverage area. The goal of the sBS content controller
(CC) is to store the most popular contents in the sBS cache memory such that
the maximum amount of data can be fetched directly form the sBS, not relying on
the limited backhaul resources during peak traffic periods. If the popularity
profile is known in advance, the problem reduces to a knapsack problem.
However, it is assumed in this work that, the popularity profile of the files
is not known by the CC, and it can only observe the instantaneous demand for
the cached content. Hence, the cache content placement is optimised based on
the demand history. By refreshing the cache content at regular time intervals,
the CC tries to learn the popularity profile, while exploiting the limited
cache capacity in the best way possible. Three algorithms are studied for this
cache content placement problem, leading to different exploitation-exploration
trade-offs. We provide extensive numerical simulations in order to study the
time-evolution of these algorithms, and the impact of the system parameters,
such as the number of files, the number of users, the cache size, and the
skewness of the popularity profile, on the performance. It is shown that the
proposed algorithms quickly learn the popularity profile for a wide range of
system parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3261</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3261</id><created>2014-02-13</created><authors><author><keyname>Heller</keyname><forenames>Jan</forenames><affiliation>LAAS, CTU/FEE</affiliation></author><author><keyname>Henrion</keyname><forenames>Didier</forenames><affiliation>LAAS, CTU/FEE</affiliation></author><author><keyname>Pajdla</keyname><forenames>Tomas</forenames></author></authors><title>Hand-Eye and Robot-World Calibration by Global Polynomial Optimization</title><categories>cs.CV math.OC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need to relate measurements made by a camera to a different known
coordinate system arises in many engineering applications. Historically, it
appeared for the first time in the connection with cameras mounted on robotic
systems. This problem is commonly known as hand-eye calibration. In this paper,
we present several formulations of hand-eye calibration that lead to
multivariate polynomial optimization problems. We show that the method of
convex linear matrix inequality (LMI) relaxations can be used to effectively
solve these problems and to obtain globally optimal solutions. Further, we show
that the same approach can be used for the simultaneous hand-eye and
robot-world calibration. Finally, we validate the proposed solutions using both
synthetic and real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3264</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3264</id><created>2014-02-13</created><updated>2015-07-24</updated><authors><author><keyname>Couvreur</keyname><forenames>Alain</forenames></author><author><keyname>Otmani</keyname><forenames>Ayoub</forenames></author><author><keyname>Tillich</keyname><forenames>Jean-Pierre</forenames></author></authors><title>Polynomial Time Attack on Wild McEliece Over Quadratic Extensions</title><categories>cs.CR cs.IT math.IT math.NT</categories><comments>The material of this article was presented at the conference
  EUROCRYPT 2014 (Copenhagen, Denmark) and published in its proceedings. Due to
  space constraints, most of the proofs were omitted in the proceedings
  version. The present article is a long revisited version including all the
  missing proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a polynomial time structural attack against the McEliece system
based on Wild Goppa codes from a quadratic finite field extension. This attack
uses the fact that such codes can be distinguished from random codes to compute
some filtration, that is to say a family of nested subcodes which will reveal
their secret algebraic description.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3277</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3277</id><created>2014-02-13</created><updated>2016-03-08</updated><authors><author><keyname>Place</keyname><forenames>Thomas</forenames><affiliation>Bordeaux University, France</affiliation></author><author><keyname>Zeitoun</keyname><forenames>Marc</forenames><affiliation>Bordeaux University, France</affiliation></author></authors><title>Separating Regular Languages with First-Order Logic</title><categories>cs.FL cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 12 (1:5) 2016</journal-ref><doi>10.2168/LMCS-12(1:5)2016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two languages, a separator is a third language that contains the first
one and is disjoint from the second one. We investigate the following decision
problem: given two regular input languages of finite words, decide whether
there exists a first-order definable separator. We prove that in order to
answer this question, sufficient information can be extracted from semigroups
recognizing the input languages, using a fixpoint computation. This yields an
EXPTIME algorithm for checking first-order separability. Moreover, the
correctness proof of this algorithm yields a stronger result, namely a
description of a possible separator. Finally, we generalize this technique to
answer the same question for regular languages of infinite words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3281</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3281</id><created>2014-02-13</created><updated>2014-03-25</updated><authors><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author></authors><title>Partitioning Complex Networks via Size-constrained Clustering</title><categories>cs.DC cs.DS cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most commonly used method to tackle the graph partitioning problem in
practice is the multilevel approach. During a coarsening phase, a multilevel
graph partitioning algorithm reduces the graph size by iteratively contracting
nodes and edges until the graph is small enough to be partitioned by some other
algorithm. A partition of the input graph is then constructed by successively
transferring the solution to the next finer graph and applying a local search
algorithm to improve the current solution.
  In this paper, we describe a novel approach to partition graphs effectively
especially if the networks have a highly irregular structure. More precisely,
our algorithm provides graph coarsening by iteratively contracting
size-constrained clusterings that are computed using a label propagation
algorithm. The same algorithm that provides the size-constrained clusterings
can also be used during uncoarsening as a fast and simple local search
algorithm.
  Depending on the algorithm's configuration, we are able to compute partitions
of very high quality outperforming all competitors, or partitions that are
comparable to the best competitor in terms of quality, hMetis, while being
nearly an order of magnitude faster on average. The fastest configuration
partitions the largest graph available to us with 3.3 billion edges using a
single machine in about ten minutes while cutting less than half of the edges
than the fastest competitor, kMetis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3288</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3288</id><created>2014-02-12</created><authors><author><keyname>Friedkin</keyname><forenames>Noah E.</forenames></author><author><keyname>Johnsen</keyname><forenames>Eugene C.</forenames></author></authors><title>Two Steps to Obfuscation</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note addresses the historical antecedents of the 1998 PageRank measure
of centrality. An identity relation links it to 1990-1991 models of Friedkin
and Johnsen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3301</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3301</id><created>2014-02-13</created><authors><author><keyname>Abdulhamid</keyname><forenames>Shafii M.</forenames></author><author><keyname>Ahmad</keyname><forenames>Sulaiman</forenames></author><author><keyname>Waziri</keyname><forenames>Victor O.</forenames></author><author><keyname>Jibril</keyname><forenames>Fatima N.</forenames></author></authors><title>Privacy and National Security Issues in Social Networks: The Challenges</title><categories>cs.SI cs.CY</categories><comments>7 pages, 1 table</comments><journal-ref>International Journal of the Computer, the Internet and Management
  Vol. 19. No.3 (September-December, 2011) pp 14 -20</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social networks are becoming a major growth point of the internet, as
individuals, companies and governments constantly desire to interact with one
another, the ability of the internet to deliver this networking capabilities
grows stronger. In this paper, we looked at the structure and components of the
member profile and the challenges of privacy issues faced by individuals and
governments that participate in social networking. We also looked at how it can
be used to distort national security, how it became the new weapons of mass
mobilization and also how social networks have became the rallying forces for
revolutions and social justice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3305</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3305</id><created>2014-02-11</created><authors><author><keyname>Klein</keyname><forenames>Martin</forenames></author><author><keyname>Sanderson</keyname><forenames>Robert</forenames></author><author><keyname>Van de Sompel</keyname><forenames>Herbert</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>Real-Time Notification for Resource Synchronization</title><categories>cs.DC</categories><comments>12 pages, 5 figues, 1 table The experiments were conducted in 2012 as
  part of the authors' work in the ResourceSync project. For further
  information and the ResourceSync framework specification please see
  http://www.openarchives.org/rs/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web applications frequently leverage resources made available by remote web
servers. As resources are created, updated, deleted, or moved, these
applications face challenges to remain in lockstep with the server's change
dynamics. Several approaches exist to help meet this challenge for use cases
where &quot;good enough&quot; synchronization is acceptable. But when strict resource
coverage or low synchronization latency is required, commonly accepted
Web-based solutions remain elusive. This paper details characteristics of an
approach that aims at decreasing synchronization latency while maintaining
desired levels of accuracy. The approach builds on pushing change notifications
and pulling changed resources and it is explored with an experiment based on a
DBpedia Live instance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3314</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3314</id><created>2014-02-13</created><updated>2014-07-16</updated><authors><author><keyname>Muscholl</keyname><forenames>Anca</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Walukiewicz</keyname><forenames>Igor</forenames><affiliation>LaBRI</affiliation></author></authors><title>Distributed synthesis for acyclic architectures</title><categories>cs.LO cs.SY</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The distributed synthesis problem is about constructing cor- rect distributed
systems, i.e., systems that satisfy a given specification. We consider a
slightly more general problem of distributed control, where the goal is to
restrict the behavior of a given distributed system in order to satisfy the
specification. Our systems are finite state machines that communicate via
rendez-vous (Zielonka automata). We show decidability of the synthesis problem
for all omega-regular local specifications, under the restriction that the
communication graph of the system is acyclic. This result extends a previous
decidability result for a restricted form of local reachability specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3317</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3317</id><created>2014-02-13</created><authors><author><keyname>Al-Matouq</keyname><forenames>Ali</forenames></author><author><keyname>Vincent</keyname><forenames>Tyrone</forenames></author></authors><title>Multiple Window Moving Horizon Estimation</title><categories>cs.SY</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Long horizon lengths in Moving Horizon Estimation are desirable to reach the
performance limits of the full information estimator. However, the conventional
MHE technique suffers from a number of deficiencies in this respect. First, the
problem complexity scales at least linearly with the horizon length selected,
which restrains from selecting long horizons if computational limitations are
present. Second, there is no monitoring of constraint activity/inactivity which
results in conducting redundant constrained minimizations even when no
constraints are active. In this study we develop a Multiple-Window Moving
Horizon Estimation strategy (MW-MHE) that exploits constraint inactivity to
reduce the problem size in long horizon estimation problems. The arrival cost
is approximated using the unconstrained full information estimator arrival cost
to guarantee stability of the technique. A new horizon length selection
criteria is developed based on maximum sensitivity between remote states in
time. The development will be in terms of general causal descriptor systems,
which includes the standard state space representation as a special case. The
potential of the new estimation algorithm will be demonstrated with an example
showing a significant reduction in both computation time and numerical errors
compared to conventional MHE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3319</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3319</id><created>2014-02-13</created><updated>2015-02-24</updated><authors><author><keyname>Skoric</keyname><forenames>Boris</forenames></author><author><keyname>de Hoogh</keyname><forenames>Sebastiaan J. A.</forenames></author><author><keyname>Zannone</keyname><forenames>Nicola</forenames></author></authors><title>Flow-based reputation with uncertainty: Evidence-Based Subjective Logic</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of reputation is widely used as a measure of trustworthiness
based on ratings from members in a community. The adoption of reputation
systems, however, relies on their ability to capture the actual trustworthiness
of a target. Several reputation models for aggregating trust information have
been proposed in the literature. The choice of model has an impact on the
reliability of the aggregated trust information as well as on the procedure
used to compute reputations. Two prominent models are flow-based reputation
(e.g., EigenTrust, PageRank) and Subjective Logic based reputation. Flow-based
models provide an automated method to aggregate trust information, but they are
not able to express the level of uncertainty in the information. In contrast,
Subjective Logic extends probabilistic models with an explicit notion of
uncertainty, but the calculation of reputation depends on the structure of the
trust network and often requires information to be discarded. These are severe
drawbacks.
  In this work, we observe that the `opinion discounting' operation in
Subjective Logic has a number of basic problems. We resolve these problems by
providing a new discounting operator that describes the flow of evidence from
one party to another. The adoption of our discounting rule results in a
consistent Subjective Logic algebra that is entirely based on the handling of
evidence. We show that the new algebra enables the construction of an automated
reputation assessment procedure for arbitrary trust networks, where the
calculation no longer depends on the structure of the network, and does not
need to throw away any information. Thus, we obtain the best of both worlds:
flow-based reputation and consistent handling of uncertainties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3329</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3329</id><created>2014-02-13</created><authors><author><keyname>Hsu</keyname><forenames>Justin</forenames></author><author><keyname>Gaboardi</keyname><forenames>Marco</forenames></author><author><keyname>Haeberlen</keyname><forenames>Andreas</forenames></author><author><keyname>Khanna</keyname><forenames>Sanjeev</forenames></author><author><keyname>Narayan</keyname><forenames>Arjun</forenames></author><author><keyname>Pierce</keyname><forenames>Benjamin C.</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author></authors><title>Differential Privacy: An Economic Method for Choosing Epsilon</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential privacy is becoming a gold standard for privacy research; it
offers a guaranteed bound on loss of privacy due to release of query results,
even under worst-case assumptions. The theory of differential privacy is an
active research area, and there are now differentially private algorithms for a
wide range of interesting problems.
  However, the question of when differential privacy works in practice has
received relatively little attention. In particular, there is still no rigorous
method for choosing the key parameter $\epsilon$, which controls the crucial
tradeoff between the strength of the privacy guarantee and the accuracy of the
published results.
  In this paper, we examine the role that these parameters play in concrete
applications, identifying the key questions that must be addressed when
choosing specific values. This choice requires balancing the interests of two
different parties: the data analyst and the prospective participant, who must
decide whether to allow their data to be included in the analysis. We propose a
simple model that expresses this balance as formulas over a handful of
parameters, and we use our model to choose $\epsilon$ on a series of simple
statistical studies. We also explore a surprising insight: in some
circumstances, a differentially private study can be more accurate than a
non-private study for the same cost, under our model. Finally, we discuss the
simplifying assumptions in our model and outline a research agenda for possible
refinements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3331</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3331</id><created>2014-02-13</created><authors><author><keyname>Nongpiur</keyname><forenames>R. C.</forenames></author><author><keyname>Shpak</keyname><forenames>D. J.</forenames></author></authors><title>L-infinity Norm Design of Linear-phase Robust Broadband Beamformers
  using Constrained Optimization</title><categories>cs.SY cs.IT math.IT</categories><journal-ref>IEEE Transactions on Signal Processing, vol. 61, no. 23, pp.
  6034-6046, Dec. 2013</journal-ref><doi>10.1109/TSP.2013.2283463</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method for the design of linear-phase robust far-field broadband
beamformers using constrained optimization is proposed. In the method, the
maximum passband ripple and minimum stopband attenuation are ensured to be
within prescribed levels, while at the same time maintaining a good
linear-phase characteristic at a prescribed group delay in the passband. Since
the beamformer is intended primarily for small-sized microphone arrays where
the microphone spacing is small relative to the wavelength at low frequencies,
the beamformer can become highly sensitive to spatial white noise and array
imperfections if a direct minimization of the error is performed. Therefore, to
limit the sensitivity of the beamformer the optimization is carried out by
constraining a sensitivity parameter, namely, the white noise gain (WNG) to be
above prescribed levels across the frequency band. Two novel design variants
have been developed. The first variant is formulated as a convex optimization
problem where the maximum error in the passband is minimized, while the second
variant is formulated as an iterative optimization problem and has the
advantage of significantly improving the linear-phase characteristics of the
beamformer under any prescribed group delay or linear-array configuration. In
the second variant, the passband group-delay deviation is minimized while
ensuring that the maximum passband ripple and stopband attenuation are within
prescribed levels. To reduce the computational effort in carrying out the
optimization, a nonuniform variable sampling approach over the frequency and
angular dimensions is used to compute the required parameters. Experiment
results show that beamformers designed using the proposed methods have much
smaller passband group-delay deviation for similar passband ripple and stopband
attenuation than a modified version of an existing method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3332</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3332</id><created>2014-02-13</created><updated>2014-10-30</updated><authors><author><keyname>Ghali</keyname><forenames>Cesar</forenames></author><author><keyname>Tsudik</keyname><forenames>Gene</forenames></author><author><keyname>Uzun</keyname><forenames>Ersin</forenames></author></authors><title>Elements of Trust in Named-Data Networking</title><categories>cs.NI cs.CR</categories><comments>9 pages, 2 figures</comments><journal-ref>ACM SIGCOMM Computer Communication Review, Volume 44 Issue 5,
  October 2014</journal-ref><doi>10.1145/2677046.2677049</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In contrast to today's IP-based host-oriented Internet architecture,
Information-Centric Networking (ICN) emphasizes content by making it directly
addressable and routable. Named Data Networking (NDN) architecture is an
instance of ICN that is being developed as a candidate next-generation Internet
architecture. By opportunistically caching content within the network (in
routers), NDN appears to be well-suited for large-scale content distribution
and for meeting the needs of increasingly mobile and bandwidth-hungry
applications that dominate today's Internet.
  One key feature of NDN is the requirement for each content object to be
digitally signed by its producer. Thus, NDN should be, in principle, immune to
distributing fake (aka &quot;poisoned&quot;) content. However, in practice, this poses
two challenges for detecting fake content in NDN routers: (1) overhead due to
signature verification and certificate chain traversal, and (2) lack of trust
context, i.e., determining which public keys are trusted to verify which
content. Because of these issues, NDN does not force routers to verify content
signatures, which makes the architecture susceptible to content poisoning
attacks.
  This paper explores root causes of, and some cures for, content poisoning
attacks in NDN. In the process, it becomes apparent that meaningful mitigation
of content poisoning is contingent upon a network-layer trust management
architecture, elements of which we construct while carefully justifying
specific design choices. This work represents the initial effort towards
comprehensive trust management for NDN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3337</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3337</id><created>2014-02-13</created><updated>2015-04-08</updated><authors><author><keyname>Konda</keyname><forenames>Kishore</forenames></author><author><keyname>Memisevic</keyname><forenames>Roland</forenames></author><author><keyname>Krueger</keyname><forenames>David</forenames></author></authors><title>Zero-bias autoencoders and the benefits of co-adapting features</title><categories>stat.ML cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regularized training of an autoencoder typically results in hidden unit
biases that take on large negative values. We show that negative biases are a
natural result of using a hidden layer whose responsibility is to both
represent the input data and act as a selection mechanism that ensures sparsity
of the representation. We then show that negative biases impede the learning of
data distributions whose intrinsic dimensionality is high. We also propose a
new activation function that decouples the two roles of the hidden layer and
that allows us to learn representations on data with very high intrinsic
dimensionality, where standard autoencoders typically fail. Since the decoupled
activation function acts like an implicit regularizer, the model can be trained
by minimizing the reconstruction error of training data, without requiring any
additional regularization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3341</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3341</id><created>2014-02-13</created><authors><author><keyname>Cioab&#x103;</keyname><forenames>Sebastian M.</forenames></author><author><keyname>Lazebnik</keyname><forenames>Felix</forenames></author><author><keyname>Li</keyname><forenames>Weiqiang</forenames></author></authors><title>On the Spectrum of Wenger Graphs</title><categories>math.CO cs.DM</categories><comments>9 pages; accepted for publication to J. Combin. Theory, Series B</comments><msc-class>05C50, 15A18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $q=p^e$, where $p$ is a prime and $e\geq 1$ is an integer. For $m\geq 1$,
let $P$ and $L$ be two copies of the $(m+1)$-dimensional vector spaces over the
finite field $\mathbb{F}_q$. Consider the bipartite graph $W_m(q)$ with partite
sets $P$ and $L$ defined as follows: a point $(p)=(p_1,p_2,\ldots,p_{m+1})\in
P$ is adjacent to a line $[l]=[l_1,l_2,\ldots,l_{m+1}]\in L$ if and only if the
following $m$ equalities hold: $l_{i+1} + p_{i+1}=l_{i}p_1$ for $i=1,\ldots,
m$. We call the graphs $W_m(q)$ Wenger graphs. In this paper, we determine all
distinct eigenvalues of the adjacency matrix of $W_m(q)$ and their
multiplicities. We also survey results on Wenger graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3344</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3344</id><created>2014-02-13</created><updated>2014-02-24</updated><authors><author><keyname>Zhang</keyname><forenames>Chong</forenames></author><author><keyname>Zhao</keyname><forenames>Yu</forenames></author><author><keyname>Triesch</keyname><forenames>Jochen</forenames></author><author><keyname>Shi</keyname><forenames>Bertram E.</forenames></author></authors><title>Intrinsically Motivated Learning of Visual Motion Perception and Smooth
  Pursuit</title><categories>cs.CV q-bio.NC</categories><comments>6 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We extend the framework of efficient coding, which has been used to model the
development of sensory processing in isolation, to model the development of the
perception/action cycle. Our extension combines sparse coding and reinforcement
learning so that sensory processing and behavior co-develop to optimize a
shared intrinsic motivational signal: the fidelity of the neural encoding of
the sensory input under resource constraints. Applying this framework to a
model system consisting of an active eye behaving in a time varying
environment, we find that this generic principle leads to the simultaneous
development of both smooth pursuit behavior and model neurons whose properties
are similar to those of primary visual cortical neurons selective for different
directions of visual motion. We suggest that this general principle may form
the basis for a unified and integrated explanation of many perception/action
loops.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3346</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3346</id><created>2014-02-13</created><updated>2015-03-12</updated><authors><author><keyname>Montufar</keyname><forenames>Guido</forenames></author><author><keyname>Ay</keyname><forenames>Nihat</forenames></author><author><keyname>Ghazi-Zahedi</keyname><forenames>Keyan</forenames></author></authors><title>Geometry and Expressive Power of Conditional Restricted Boltzmann
  Machines</title><categories>cs.NE cs.LG stat.ML</categories><comments>30 pages, 5 figures, 1 algorithm</comments><msc-class>60K99, 68T05, 68R05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditional restricted Boltzmann machines are undirected stochastic neural
networks with a layer of input and output units connected bipartitely to a
layer of hidden units. These networks define models of conditional probability
distributions on the states of the output units given the states of the input
units, parametrized by interaction weights and biases. We address the
representational power of these models, proving results their ability to
represent conditional Markov random fields and conditional distributions with
restricted supports, the minimal size of universal approximators, the maximal
model approximation errors, and on the dimension of the set of representable
conditional distributions. We contribute new tools for investigating
conditional probability models, which allow us to improve the results that can
be derived from existing work on restricted Boltzmann machine probability
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3350</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3350</id><created>2014-02-13</created><updated>2014-03-22</updated><authors><author><keyname>Mehta</keyname><forenames>Ruta</forenames></author></authors><title>Constant Rank Bimatrix Games are PPAD-hard</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rank of a bimatrix game (A,B) is defined as rank(A+B). Computing a Nash
equilibrium (NE) of a rank-$0$, i.e., zero-sum game is equivalent to linear
programming (von Neumann'28, Dantzig'51). In 2005, Kannan and Theobald gave an
FPTAS for constant rank games, and asked if there exists a polynomial time
algorithm to compute an exact NE. Adsul et al. (2011) answered this question
affirmatively for rank-$1$ games, leaving rank-2 and beyond unresolved.
  In this paper we show that NE computation in games with rank $\ge 3$, is
PPAD-hard, settling a decade long open problem. Interestingly, this is the
first instance that a problem with an FPTAS turns out to be PPAD-hard. Our
reduction bypasses graphical games and game gadgets, and provides a simpler
proof of PPAD-hardness for NE computation in bimatrix games. In addition, we
get:
  * An equivalence between 2D-Linear-FIXP and PPAD, improving a result by
Etessami and Yannakakis (2007) on equivalence between Linear-FIXP and PPAD.
  * NE computation in a bimatrix game with convex set of Nash equilibria is as
hard as solving a simple stochastic game.
  * Computing a symmetric NE of a symmetric bimatrix game with rank $\ge 6$ is
PPAD-hard.
  * Computing a (1/poly(n))-approximate fixed-point of a (Linear-FIXP)
piecewise-linear function is PPAD-hard.
  The status of rank-$2$ games remains unresolved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3352</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3352</id><created>2014-02-13</created><authors><author><keyname>Nongpiur</keyname><forenames>R. C.</forenames></author><author><keyname>Shpak</keyname><forenames>D. J.</forenames></author><author><keyname>Antoniou</keyname><forenames>A.</forenames></author></authors><title>Improved Design Method for Nearly Linear-Phase IIR Filters Using
  Constrained Optimization</title><categories>cs.SY</categories><journal-ref>IEEE Transactions on Signal Processing, vol. 61, no. 4, Feb. 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new optimization method for the design of nearly linear-phase IIR digital
filters that satisfy prescribed specifications is proposed. The group-delay
deviation is minimized under the constraint that the passband ripple and
stopband attenuation are within the prescribed specifications and either a
prescribed or an optimized group delay can be achieved. By representing the
filter in terms of a cascade of second-order sections, a non-restrictive
stability constraint characterized by a set of linear inequality constraints
can be incorporated in the optimization algorithm. An additional feature of the
method, which is very useful in certain applications, is that it provides the
capability of constraining the maximum gain in transition bands to be below a
prescribed level. Experimental results show that filters designed using the
proposed method have much lower group-delay deviation for the same passband
ripple and stopband attenuation when compared with corresponding filters
designed with several state-of-the-art competing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3364</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3364</id><created>2014-02-14</created><updated>2014-02-21</updated><authors><author><keyname>Abu-Ata</keyname><forenames>Muad</forenames></author><author><keyname>Dragan</keyname><forenames>Feodor F.</forenames></author></authors><title>Metric tree-like structures in real-life networks: an empirical study</title><categories>cs.SI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on solid theoretical foundations, we present strong evidences that a
number of real-life networks, taken from different domains like Internet
measurements, biological data, web graphs, social and collaboration networks,
exhibit tree-like structures from a metric point of view. We investigate few
graph parameters, namely, the tree-distortion and the tree-stretch, the
tree-length and the tree-breadth, the Gromov's hyperbolicity, the
cluster-diameter and the cluster-radius in a layering partition of a graph,
which capture and quantify this phenomenon of being metrically close to a tree.
By bringing all those parameters together, we not only provide efficient means
for detecting such metric tree-like structures in large-scale networks but also
show how such structures can be used, for example, to efficiently and compactly
encode approximate distance and almost shortest path information and to fast
and accurately estimate diameters and radii of those networks. Estimating the
diameter and the radius of a graph or distances between its arbitrary vertices
are fundamental primitives in many data and graph mining algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3371</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3371</id><created>2014-02-14</created><authors><author><keyname>Ballatore</keyname><forenames>Andrea</forenames></author><author><keyname>Bertolotto</keyname><forenames>Michela</forenames></author><author><keyname>Wilson</keyname><forenames>David C.</forenames></author></authors><title>An evaluative baseline for geo-semantic relatedness and similarity</title><categories>cs.CL</categories><comments>GeoInformatica 2014</comments><doi>10.1007/s10707-013-0197-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In geographic information science and semantics, the computation of semantic
similarity is widely recognised as key to supporting a vast number of tasks in
information integration and retrieval. By contrast, the role of geo-semantic
relatedness has been largely ignored. In natural language processing, semantic
relatedness is often confused with the more specific semantic similarity. In
this article, we discuss a notion of geo-semantic relatedness based on Lehrer's
semantic fields, and we compare it with geo-semantic similarity. We then
describe and validate the Geo Relatedness and Similarity Dataset (GeReSiD), a
new open dataset designed to evaluate computational measures of geo-semantic
relatedness and similarity. This dataset is larger than existing datasets of
this kind, and includes 97 geographic terms combined into 50 term pairs rated
by 203 human subjects. GeReSiD is available online and can be used as an
evaluation baseline to determine empirically to what degree a given
computational model approximates geo-semantic relatedness and similarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3374</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3374</id><created>2014-02-14</created><authors><author><keyname>Thippeswamy</keyname><forenames>B M</forenames></author><author><keyname>Reshma</keyname><forenames>S</forenames></author><author><keyname>Shaila</keyname><forenames>K</forenames></author><author><keyname>Venugopal</keyname><forenames>K R</forenames></author><author><keyname>Iyengar</keyname><forenames>S S</forenames></author><author><keyname>Patnaik</keyname><forenames>L M</forenames></author></authors><title>EDOCR: Energy Density On-demand Cluster Routing in Wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>18 pages,7 Figures</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.6, No.1, January 2014</journal-ref><doi>10.5121/ijcnc.2014.6115</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy management is one of the critical parameters in Wireless Sensor
Networks. In this paper we attempt for a solution to balance the energy usage
for maximizing the network lifetime, increase the packet delivery ratio and
throughput. Our proposed algorithm is based on Energy Density of the clusters
in Wireless Sensor Networks. The cluster head is selected using two step method
and on-demand routing approach to calculate the balanced energy shortest path
from source to sink. This unique approach maintains the balanced energy
utilization among all nodes by selecting the different cluster heads
dynamically. Our simulation results have compared with one of the plain routing
scheme (EBRP) and cluster based routing (TSCHS), which shows the significant
improvements in minimizing the delay and energy utilization and maximizing the
network lifetime and throughput with respect to these works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3382</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3382</id><created>2014-02-14</created><authors><author><keyname>Rajan</keyname><forenames>K.</forenames></author><author><keyname>Ramalingam</keyname><forenames>Dr. V.</forenames></author><author><keyname>Ganesan</keyname><forenames>Dr. M.</forenames></author></authors><title>Machine Learning of Phonologically Conditioned Noun Declensions For
  Tamil Morphological Generators</title><categories>cs.CL</categories><comments>13 pages. International Journal of Computer Engineering and
  Applications, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents machine learning solutions to a practical problem of
Natural Language Generation (NLG), particularly the word formation in
agglutinative languages like Tamil, in a supervised manner. The morphological
generator is an important component of Natural Language Processing in
Artificial Intelligence. It generates word forms given a root and affixes. The
morphophonemic changes like addition, deletion, alternation etc., occur when
two or more morphemes or words joined together. The Sandhi rules should be
explicitly specified in the rule based morphological analyzers and generators.
In machine learning framework, these rules can be learned automatically by the
system from the training samples and subsequently be applied for new inputs. In
this paper we proposed the machine learning models which learn the
morphophonemic rules for noun declensions from the given training data. These
models are trained to learn sandhi rules using various learning algorithms and
the performance of those algorithms are presented. From this we conclude that
machine learning of morphological processing such as word form generation can
be successfully learned in a supervised manner, without explicit description of
rules. The performance of Decision trees and Bayesian machine learning
algorithms on noun declensions are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3384</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3384</id><created>2014-02-14</created><updated>2014-12-01</updated><authors><author><keyname>Wang</keyname><forenames>Weina</forenames></author><author><keyname>Ying</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Junshan</forenames></author></authors><title>A Minimax Distortion View of Differentially Private Query Release</title><categories>cs.CR cs.DB cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of differentially private query release through a
synthetic database approach. Departing from the existing approaches that
require the query set to be specified in advance, we advocate to devise
query-set independent mechanisms, with an ambitious goal of providing accurate
answers, while meeting the privacy constraints, for all queries in a general
query class. Specifically, a differentially private mechanism is constructed to
&quot;encode&quot; rich stochastic structure into the synthetic database, and
&quot;customized&quot; companion estimators are then derived to provide accurate answers
by making use of all available information, including the mechanism (which is
public information) and the query functions. Accordingly, the distortion under
the best of this kind of mechanisms at the worst-case query in a general query
class, so called the minimax distortion, provides a fundamental
characterization of differentially private query release.
  For the general class of statistical queries, we prove that with the
squared-error distortion measure, the minimax distortion is $O(1/n)$ by
deriving asymptotically tight upper and lower bounds in the regime that the
database size $n$ goes to infinity. The upper bound is achievable by a
mechanism $\mathcal{E}$ and its corresponding companion estimators, which
points directly to the feasibility of the proposed approach in large databases.
We further evaluate the mechanism $\mathcal{E}$ and the companion estimators
through experiments on real datasets from Netflix and Facebook. Experimental
results show improvement over the state-of-art MWEM algorithm and verify the
scaling behavior $O(1/n)$ of the minimax distortion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3388</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3388</id><created>2014-02-14</created><updated>2014-09-24</updated><authors><author><keyname>Esparza</keyname><forenames>Javier</forenames></author><author><keyname>K&#x159;et&#xed;nsk&#xfd;</keyname><forenames>Jan</forenames></author></authors><title>From LTL to Deterministic Automata: A Safraless Compositional Approach</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm to construct a deterministic Rabin automaton for
an LTL formula $\varphi$. The automaton is the product of a master automaton
and an array of slave automata, one for each $G$-subformula of $\varphi$. The
slave automaton for $G\psi$ is in charge of recognizing whether $FG\psi$ holds.
As opposed to standard determinization procedures, the states of all our
automata have a clear logical structure, which allows to apply various
optimizations. Our construction subsumes former algorithms for fragments of
LTL. Experimental results show improvement in the sizes of the resulting
automata compared to existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3392</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3392</id><created>2014-02-14</created><authors><author><keyname>Giesen</keyname><forenames>Fabian</forenames></author></authors><title>Interleaved entropy coders</title><categories>cs.IT math.IT</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ANS family of arithmetic coders developed by Jarek Duda has the unique
property that encoder and decoder are completely symmetric in the sense that a
decoder reading bits will be in the exact same state that the encoder was in
when writing those bits---all &quot;buffering&quot; of information is explicitly part of
the coder state and identical between encoder and decoder. As a consequence,
the output from multiple ABS/ANS coders can be interleaved into the same
bitstream without any additional metadata. This allows for very efficient
encoding and decoding on CPUs supporting superscalar execution or SIMD
instructions, as well as GPU implementations. We also show how interleaving
without additional metadata can be implemented for any entropy coder, at some
increase in encoder complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3401</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3401</id><created>2014-02-14</created><updated>2014-11-05</updated><authors><author><keyname>Chaabane</keyname><forenames>Abdelberi</forenames></author><author><keyname>Chen</keyname><forenames>Terence</forenames></author><author><keyname>Cunche</keyname><forenames>Mathieu</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author><author><keyname>Friedman</keyname><forenames>Arik</forenames></author><author><keyname>Kaafar</keyname><forenames>Mohamed Ali</forenames></author></authors><title>Censorship in the Wild: Analyzing Internet Filtering in Syria</title><categories>cs.CY cs.CR cs.NI</categories><comments>A preliminary version of this paper appears in IMC 2014. This is the
  full version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet censorship is enforced by numerous governments worldwide, however,
due to the lack of publicly available information, as well as the inherent
risks of performing active measurements, it is often hard for the research
community to investigate censorship practices in the wild. Thus, the leak of
600GB worth of logs from 7 Blue Coat SG-9000 proxies, deployed in Syria to
filter Internet traffic at a country scale, represents a unique opportunity to
provide a detailed snapshot of a real-world censorship ecosystem. This paper
presents the methodology and the results of a measurement analysis of the
leaked Blue Coat logs, revealing a relatively stealthy, yet quite targeted,
censorship. We find that traffic is filtered in several ways: using IP
addresses and domain names to block subnets or websites, and keywords or
categories to target specific content. We show that keyword-based censorship
produces some collateral damage as many requests are blocked even if they do
not relate to sensitive content. We also discover that Instant Messaging is
heavily censored, while filtering of social media is limited to specific pages.
Finally, we show that Syrian users try to evade censorship by using web/socks
proxies, Tor, VPNs, and BitTorrent. To the best of our knowledge, our work
provides the first analytical look into Internet filtering in Syria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3405</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3405</id><created>2014-02-14</created><authors><author><keyname>Cerra</keyname><forenames>Daniele</forenames></author><author><keyname>Datcu</keyname><forenames>Mihai</forenames></author><author><keyname>Reinartz</keyname><forenames>Peter</forenames></author></authors><title>Authorship Analysis based on Data Compression</title><categories>cs.CL cs.DL cs.IR stat.ML</categories><doi>10.1016/j.patrec.2014.01.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes to perform authorship analysis using the Fast Compression
Distance (FCD), a similarity measure based on compression with dictionaries
directly extracted from the written texts. The FCD computes a similarity
between two documents through an effective binary search on the intersection
set between the two related dictionaries. In the reported experiments the
proposed method is applied to documents which are heterogeneous in style,
written in five different languages and coming from different historical
periods. Results are comparable to the state of the art and outperform
traditional compression-based methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3418</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3418</id><created>2014-02-14</created><authors><author><keyname>Kholodov</keyname><forenames>Alexander S.</forenames></author></authors><title>Scientific works citation analysis</title><categories>cs.DL</categories><comments>in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several questions of scientometrics parameters organization are considered.
Two new indices for scientific works citation analysis are proposed. They
provide more detailed and reliable scientific significance assessment of
individual authors and scientific groups basing on the publication activity
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3426</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3426</id><created>2014-02-14</created><updated>2015-05-27</updated><authors><author><keyname>Shokri</keyname><forenames>Reza</forenames></author></authors><title>Privacy Games: Optimal User-Centric Data Obfuscation</title><categories>cs.CR cs.GT</categories><doi>10.1515/popets-2015-0024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design user-centric obfuscation mechanisms that impose the
minimum utility loss for guaranteeing user's privacy. We optimize utility
subject to a joint guarantee of differential privacy (indistinguishability) and
distortion privacy (inference error). This double shield of protection limits
the information leakage through obfuscation mechanism as well as the posterior
inference. We show that the privacy achieved through joint
differential-distortion mechanisms against optimal attacks is as large as the
maximum privacy that can be achieved by either of these mechanisms separately.
Their utility cost is also not larger than what either of the differential or
distortion mechanisms imposes. We model the optimization problem as a
leader-follower game between the designer of obfuscation mechanism and the
potential adversary, and design adaptive mechanisms that anticipate and protect
against optimal inference algorithms. Thus, the obfuscation mechanism is
optimal against any inference algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3427</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3427</id><created>2014-02-14</created><updated>2015-08-30</updated><authors><author><keyname>Chatzis</keyname><forenames>Sotirios P.</forenames></author></authors><title>Maximum Entropy Discrimination Denoising Autoencoders</title><categories>cs.LG</categories><comments>This paper has been withdrawn by the author due to errors in the
  experiments (software bugs)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Denoising autoencoders (DAs) are typically applied to relatively large
datasets for unsupervised learning of representative data encodings, they rely
on the idea of making the learned representations robust to partial corruption
of the input pattern, and perform learning using stochastic gradient descent
with relatively large datasets. In this paper, we present a fully Bayesian DA
architecture that allows for the application of DAs even when data is scarce.
Our novel approach formulates the signal encoding problem under a nonparametric
Bayesian regard, considering a Gaussian process prior over the latent input
encodings generated given the (corrupt) input observations. Subsequently, the
decoder modules of our model are formulated as large-margin regression models,
treated under the Bayesian inference paradigm, by exploiting the maximum
entropy discrimination (MED) framework. We exhibit the effectiveness of our
approach using several datasets, dealing with both classification and transfer
learning applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3435</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3435</id><created>2014-02-14</created><updated>2014-09-12</updated><authors><author><keyname>Ma&#xdf;berg</keyname><forenames>Jens</forenames></author></authors><title>Generalized Huffman Coding for Binary Trees with Choosable Edge Lengths</title><categories>cs.IT cs.DS math.CO math.IT</categories><comments>9 pages</comments><journal-ref>Information Processing Letters 115 (4), pp. 502-506 (2015)</journal-ref><doi>10.1016/j.ipl.2014.11.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study binary trees with choosable edge lengths, in
particular rooted binary trees with the property that the two edges leading
from every non-leaf to its two children are assigned integral lengths $l_1$ and
$l_2$ with $l_1+l_2 =k$ for a constant $k\in\mathbb{N}$. The depth of a leaf is
the total length of the edges of the unique root-leaf-path.
  We present a generalization of the Huffman Coding that can decide in
polynomial time if for given values $d_1,\ldots,d_n\in\mathbb{N}_{\geq 0}$
there exists a rooted binary tree with choosable edge lengths with $n$ leaves
having depths at most $d_1,\ldots ,d_n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3444</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3444</id><created>2014-02-14</created><updated>2015-11-12</updated><authors><author><keyname>Silvestri</keyname><forenames>Francesco</forenames></author></authors><title>Subgraph Enumeration in Massive Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of enumerating all instances of a given pattern graph
in a large data graph. Our focus is on determining the input/output (I/O)
complexity of this problem. Let $E$ be the number of edges in the data graph,
$k=O(1)$ be the number of vertices in the pattern graph, $B$ be the block
length, and $M$ be the main memory size. The main results of the paper are two
algorithms that enumerate all instances of the pattern graph. The first one is
a deterministic algorithm that exploits a suitable independent set of the
pattern graph of size $1\leq s \leq k/2$ and requires
$O\left(E^{k-s}/\left(BM^{k-s-1}\right)\right)$ I/Os. The second algorithm is a
randomized algorithm that enumerates all instances in
$O\left(E^{k/2}/\left(BM^{k/2-1}\right)\right)$ expected I/Os; the same bound
also applies with high probability under some assumptions. A lower bound shows
that the deterministic algorithm is optimal for some pattern graphs with
$s=k/2$ (e.g., paths and cycles of even length, meshes of even side), while the
randomized algorithm is optimal for a wide class of pattern graphs, called Alon
class (e.g., cliques, cycles and every graph with a perfect matching).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3447</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3447</id><created>2014-02-14</created><authors><author><keyname>Caragiannis</keyname><forenames>Ioannis</forenames></author><author><keyname>Voudouris</keyname><forenames>Alexandros A.</forenames></author></authors><title>Welfare guarantees for proportional allocations</title><categories>cs.GT</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to the proportional allocation mechanism from the network
optimization literature, users compete for a divisible resource -- such as
bandwidth -- by submitting bids. The mechanism allocates to each user a
fraction of the resource that is proportional to her bid and collects an amount
equal to her bid as payment. Since users act as utility-maximizers, this
naturally defines a proportional allocation game. Recently, Syrgkanis and
Tardos (STOC 2013) quantified the inefficiency of equilibria in this game with
respect to the social welfare and presented a lower bound of 26.8% on the price
of anarchy over coarse-correlated and Bayes-Nash equilibria in the full and
incomplete information settings, respectively. In this paper, we improve this
bound to 50% over both equilibrium concepts. Our analysis is simpler and,
furthermore, we argue that it cannot be improved by arguments that do not take
the equilibrium structure into account. We also extend it to settings with
budget constraints where we show the first constant bound (between 36% and 50%)
on the price of anarchy of the corresponding game with respect to an effective
welfare benchmark that takes budgets into account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3448</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3448</id><created>2014-02-14</created><authors><author><keyname>Fiorenzi</keyname><forenames>Francesca</forenames></author></authors><title>Periodic configurations of subshifts on groups</title><categories>cs.FL math.DS math.GR</categories><journal-ref>Internat. J. Algebra Comput. 19 (2009) no. 3, 315-335</journal-ref><doi>10.1142/S0218196709005123</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the density of periodic configurations for shift spaces defined on
(the Cayley graph of) a finitely generated group. We prove that in the case of
a full shift on a residually finite group and in that of a group shift space on
an abelian group, the periodic configurations are dense. In the one-dimensional
case we prove the density for irreducible sofic shifts. In connection with this
we study the surjunctivity of cellular automata and local selfmappings. Some
related decision problems for shift spaces of finite type are also
investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3449</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3449</id><created>2014-02-14</created><updated>2014-10-02</updated><authors><author><keyname>Nakanishi</keyname><forenames>Masaki</forenames></author></authors><title>Quantum Pushdown Automata with a Garbage Tape</title><categories>quant-ph cs.CC cs.FL</categories><comments>v3 Proofs in Section 4 were revised. Introduction was revised.
  Theorem 1 was removed since Theorem 2, which is Theorem 1 in the new version,
  is the generalization of it. Several other minor revisions were made. v4 The
  well-formedness conditions were added. Several other minor revisions were
  made</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several kinds of quantum pushdown automaton models have been proposed, and
their computational power is investigated intensively. However, for some
quantum pushdown automaton models, it is not known whether quantum models are
at least as powerful as classical counterparts or not. This is due to the
reversibility restriction. In this paper, we introduce a new quantum pushdown
automaton model that has a garbage tape. This model can overcome the
reversibility restriction by exploiting the garbage tape to store popped
symbols. We show that the proposed model can simulate any quantum pushdown
automaton with a classical stack as well as any probabilistic pushdown
automaton. We also show that our model can solve a certain promise problem
exactly while deterministic pushdown automata cannot. These results imply that
our model is strictly more powerful than classical counterparts in the setting
of exact, one-sided error and non-deterministic computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3450</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3450</id><created>2014-02-14</created><authors><author><keyname>Caragiannis</keyname><forenames>Ioannis</forenames></author><author><keyname>Fanelli</keyname><forenames>Angelo</forenames></author><author><keyname>Gravin</keyname><forenames>Nick</forenames></author></authors><title>Short sequences of improvement moves lead to approximate equilibria in
  constraint satisfaction games</title><categories>cs.GT cs.CC</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm that computes approximate pure Nash equilibria in a
broad class of constraint satisfaction games that generalize the well-known cut
and party affiliation games. Our results improve previous ones by Bhalgat et
al.~(EC 10) in terms of the obtained approximation guarantee. More importantly,
our algorithm identifies a polynomially-long sequence of improvement moves from
any initial state to an approximate equilibrium in these games. The existence
of such short sequences is an interesting structural property which, to the
best of our knowledge, was not known before. Our techniques adapt and extend
our previous work for congestion games (FOCS 11) but the current analysis is
considerably simpler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3452</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3452</id><created>2014-02-14</created><authors><author><keyname>Lohrey</keyname><forenames>Markus</forenames></author><author><keyname>Schmidt-Schauss</keyname><forenames>Manfred</forenames></author></authors><title>Processing Succinct Matrices and Vectors</title><categories>cs.DS cs.CC</categories><comments>An extended abstract of this paper will appear in the Proceedings of
  CSR 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of algorithmic problems for matrices that are
represented by multi-terminal decision diagrams (MTDD). These are a variant of
ordered decision diagrams, where the terminal nodes are labeled with arbitrary
elements of a semiring (instead of 0 and 1). A simple example shows that the
product of two MTDD-represented matrices cannot be represented by an MTDD of
polynomial size. To overcome this deficiency, we extended MTDDs to MTDD_+ by
allowing componentwise symbolic addition of variables (of the same dimension)
in rules. It is shown that accessing an entry, equality checking, matrix
multiplication, and other basic matrix operations can be solved in polynomial
time for MTDD_+-represented matrices. On the other hand, testing whether the
determinant of a MTDD-represented matrix vanishes PSPACE$-complete, and the
same problem is NP-complete for MTDD_+-represented diagonal matrices. Computing
a specific entry in a product of MTDD-represented matrices is #P-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3470</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3470</id><created>2014-02-14</created><authors><author><keyname>Bosch</keyname><forenames>Thomas</forenames></author><author><keyname>Wira-Alam</keyname><forenames>Andias</forenames></author><author><keyname>Mathiak</keyname><forenames>Brigitte</forenames></author></authors><title>Designing an Ontology for the Data Documentation Initiative</title><categories>cs.IR cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An ontology of the DDI 3 data model will be designed by following the
ontology engineering methodology to be evolved based on state-of-the-art
methodologies. Hence DDI 3 data and metadata can be represented in form of a
standard web interchange format RDF and processed by highly available RDF
tools. As a consequence the DDI community has the possibility to publish and
link LOD data sets to become part of the LOD cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3472</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3472</id><created>2014-02-13</created><authors><author><keyname>Bliznets</keyname><forenames>Ivan</forenames></author><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author></authors><title>A subexponential parameterized algorithm for Proper Interval Completion</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Proper Interval Completion problem we are given a graph G and an
integer k, and the task is to turn G using at most k edge additions into a
proper interval graph, i.e., a graph admitting an intersection model of
equal-length intervals on a line. The study of Proper Interval Completion from
the viewpoint of parameterized complexity has been initiated by Kaplan, Shamir
and Tarjan [FOCS 1994; SIAM J. Comput. 1999], who showed an algorithm for the
problem working in $O(16^k (n + m))$ time. In this paper we present an
algorithm with running time $k^{O(k^{2/3})} + O(nm(kn + m))$, which is the
first subexponential parameterized algorithm for Proper Interval Completion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3473</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3473</id><created>2014-02-13</created><updated>2014-11-10</updated><authors><author><keyname>Bliznets</keyname><forenames>Ivan</forenames></author><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author></authors><title>A subexponential parameterized algorithm for Interval Completion</title><categories>cs.DS</categories><comments>v2: An overview of the proof has been added; v3: updated introduction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Interval Completion problem we are given a graph G and an integer k,
and the task is to turn G using at most k edge additions into an interval
graph, i.e., a graph admitting an intersection model of intervals on a line.
Motivated by applications in sparse matrix multiplication and molecular
biology, Kaplan, Shamir and Tarjan [FOCS 1994; SIAM J. Comput. 1999] asked for
a fixed-parameter algorithm solving this problem. This question was answer
affirmatively more than a decade later by Villanger at el. [STOC 2007; SIAM J.
Comput. 2009], who presented an algorithm with running time $O(k^{2k}n^3m)$. We
give the first subexponential parameterized algorithm solving Interval
Completion in time $k^{O(\sqrt{k})} n^{O(1)}$. This adds Interval Completion to
a very small list of parameterized graph modification problems solvable in
subexponential time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3483</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3483</id><created>2014-02-14</created><authors><author><keyname>Pi&#x161;korec</keyname><forenames>Matija</forenames></author><author><keyname>Antulov-Fantulin</keyname><forenames>Nino</forenames></author><author><keyname>Novak</keyname><forenames>Petra Kralj</forenames></author><author><keyname>Mozeti&#x10d;</keyname><forenames>Igor</forenames></author><author><keyname>Gr&#x10d;ar</keyname><forenames>Miha</forenames></author><author><keyname>Vodenska</keyname><forenames>Irena</forenames></author><author><keyname>&#x160;muc</keyname><forenames>Tomislav</forenames></author></authors><title>News Cohesiveness: an Indicator of Systemic Risk in Financial Markets</title><categories>cs.SI physics.soc-ph q-fin.ST</categories><journal-ref>Scientific Reports 4: 5038 (2014)</journal-ref><doi>10.1038/srep05038</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by recent financial crises significant research efforts have been
put into studying contagion effects and herding behaviour in financial markets.
Much less has been said about influence of financial news on financial markets.
We propose a novel measure of collective behaviour in financial news on the
Web, News Cohesiveness Index (NCI), and show that it can be used as a systemic
risk indicator. We evaluate the NCI on financial documents from large Web news
sources on a daily basis from October 2011 to July 2013 and analyse the
interplay between financial markets and financially related news. We
hypothesized that strong cohesion in financial news reflects movements in the
financial markets. Cohesiveness is more general and robust measure of systemic
risk expressed in news, than measures based on simple occurrences of specific
terms. Our results indicate that cohesiveness in the financial news is highly
correlated with and driven by volatility on the financial markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3484</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3484</id><created>2014-02-14</created><authors><author><keyname>Schmuck</keyname><forenames>Anne-Kathrin</forenames></author><author><keyname>Raisch</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Simulation and Bisimulation over Multiple Time Scales in a Behavioral
  Setting</title><categories>cs.SY</categories><comments>Submitted to 22nd Mediterranean Conference on Control and Automation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new behavioral system model with distinct external
and internal signals possibly evolving on different time scales. This allows to
capture abstraction processes or signal aggregation in the context of control
and verification of large scale systems. For this new system model different
notions of simulation and bisimulation are derived, ensuring that they are,
respectively, preorders and equivalence relations for the system class under
consideration. These relations can capture a wide selection of similarity
notions available in the literature. This paper therefore provides a suitable
framework for their comparison
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3488</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3488</id><created>2014-02-14</created><updated>2015-09-17</updated><authors><author><keyname>Wehmuth</keyname><forenames>Klaus</forenames><affiliation>LNCC / MCTI</affiliation></author><author><keyname>Ziviani</keyname><forenames>Artur</forenames><affiliation>LNCC / MCTI</affiliation></author><author><keyname>Fleury</keyname><forenames>Eric</forenames><affiliation>ENS de Lyon / INRIA - Universit&#xe9; de Lyon</affiliation></author></authors><title>A Unifying Model for Representing Time-Varying Graphs</title><categories>cs.DS cs.DM cs.SI</categories><comments>Also appears in the Proc. of the IEEE International Conference on
  Data Science and Advanced Analytics (IEEE DSAA'2015)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph-based models form a fundamental aspect of data representation in Data
Sciences and play a key role in modeling complex networked systems. In
particular, recently there is an ever-increasing interest in modeling dynamic
complex networks, i.e. networks in which the topological structure (nodes and
edges) may vary over time. In this context, we propose a novel model for
representing finite discrete Time-Varying Graphs (TVGs), which are typically
used to model dynamic complex networked systems. We analyze the data structures
built from our proposed model and demonstrate that, for most practical cases,
the asymptotic memory complexity of our model is in the order of the
cardinality of the set of edges. Further, we show that our proposal is an
unifying model that can represent several previous (classes of) models for
dynamic networks found in the recent literature, which in general are unable to
represent each other. In contrast to previous models, our proposal is also able
to intrinsically model cyclic (i.e. periodic) behavior in dynamic networks.
These representation capabilities attest the expressive power of our proposed
unifying model for TVGs. We thus believe our unifying model for TVGs is a step
forward in the theoretical foundations for data analysis of complex networked
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3490</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3490</id><created>2014-02-14</created><updated>2014-05-12</updated><authors><author><keyname>Deng</keyname><forenames>Xinyang</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>D numbers theory: a generalization of Dempster-Shafer theory</title><categories>cs.AI</categories><comments>This paper has been withdrawn by the authors due to a crucial error
  of the combination rule</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dempster-Shafer theory is widely applied to uncertainty modelling and
knowledge reasoning due to its ability of expressing uncertain information.
However, some conditions, such as exclusiveness hypothesis and completeness
constraint, limit its development and application to a large extend. To
overcome these shortcomings in Dempster-Shafer theory and enhance its
capability of representing uncertain information, a novel theory called D
numbers theory is systematically proposed in this paper. Within the proposed
theory, uncertain information is expressed by D numbers, reasoning and
synthesization of information are implemented by D numbers combination rule.
The proposed D numbers theory is an generalization of Dempster-Shafer theory,
which inherits the advantage of Dempster-Shafer theory and strengthens its
capability of uncertainty modelling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3501</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3501</id><created>2014-02-14</created><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author><author><keyname>Gautier</keyname><forenames>Thierry</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Pernet</keyname><forenames>Cl&#xe9;ment</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Sultan</keyname><forenames>Ziad</forenames><affiliation>LJK, INRIA Grenoble Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author></authors><title>Parallel computation of echelon forms</title><categories>cs.SC cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose efficient parallel algorithms and implementations on shared memory
architectures of LU factorization over a finite field. Compared to the
corresponding numerical routines, we have identified three main difficulties
specific to linear algebra over finite fields. First, the arithmetic complexity
could be dominated by modular reductions. Therefore, it is mandatory to delay
as much as possible these reductions while mixing fine-grain parallelizations
of tiled iterative and recursive algorithms. Second, fast linear algebra
variants, e.g., using Strassen-Winograd algorithm, never suffer from
instability and can thus be widely used in cascade with the classical
algorithms. There, trade-offs are to be made between size of blocks well suited
to those fast variants or to load and communication balancing. Third, many
applications over finite fields require the rank profile of the matrix (quite
often rank deficient) rather than the solution to a linear system. It is thus
important to design parallel algorithms that preserve and compute this rank
profile. Moreover, as the rank profile is only discovered during the algorithm,
block size has then to be dynamic. We propose and compare several block
decomposition: tile iterative with left-looking, right-looking and Crout
variants, slab and tile recursive. Experiments demonstrate that the tile
recursive variant performs better and matches the performance of reference
numerical software when no rank deficiency occur. Furthermore, even in the most
heterogeneous case, namely when all pivot blocks are rank deficient, we show
that it is possbile to maintain a high efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3506</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3506</id><created>2014-02-14</created><updated>2014-03-13</updated><authors><author><keyname>Schmuck</keyname><forenames>Anne-Kathrin</forenames></author><author><keyname>Raisch</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Constructing (Bi)Similar Finite State Abstractions using Asynchronous
  $l$-Complete Approximations</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper constructs a finite state abstraction of a possibly
continuous-time and infinite state model in two steps. First, a finite external
signal space is added, generating a so called $\Phi$-dynamical system.
Secondly, the strongest asynchronous $l$-complete approximation of the external
dynamics is constructed. As our main results, we show that (i) the abstraction
simulates the original system, and (ii) bisimilarity between the original
system and its abstraction holds, if and only if the original system is
$l$-complete and its state space satisfies an additional property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3511</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3511</id><created>2014-02-14</created><authors><author><keyname>Koutn&#xed;k</keyname><forenames>Jan</forenames></author><author><keyname>Greff</keyname><forenames>Klaus</forenames></author><author><keyname>Gomez</keyname><forenames>Faustino</forenames></author><author><keyname>Schmidhuber</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>A Clockwork RNN</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequence prediction and classification are ubiquitous and challenging
problems in machine learning that can require identifying complex dependencies
between temporally distant inputs. Recurrent Neural Networks (RNNs) have the
ability, in theory, to cope with these temporal dependencies by virtue of the
short-term memory implemented by their recurrent (feedback) connections.
However, in practice they are difficult to train successfully when the
long-term memory is required. This paper introduces a simple, yet powerful
modification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in
which the hidden layer is partitioned into separate modules, each processing
inputs at its own temporal granularity, making computations only at its
prescribed clock rate. Rather than making the standard RNN models more complex,
CW-RNN reduces the number of RNN parameters, improves the performance
significantly in the tasks tested, and speeds up the network evaluation. The
network is demonstrated in preliminary experiments involving two tasks: audio
signal generation and TIMIT spoken word classification, where it outperforms
both RNN and LSTM networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3520</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3520</id><created>2014-02-14</created><authors><author><keyname>Schwandter</keyname><forenames>Stefan</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Matz</keyname><forenames>Gerald</forenames></author></authors><title>Spatially-Coupled LDPC Codes for Decode-and-Forward Relaying of Two
  Correlated Sources over the BEC</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Communications, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a decode-and-forward transmission scheme based on
spatially-coupled low-density parity-check (SC-LDPC) codes for a network
consisting of two (possibly correlated) sources, one relay, and one
destination. The links between the nodes are modeled as binary erasure
channels. Joint source-channel coding with joint channel decoding is used to
exploit the correlation. The relay performs network coding. We derive
analytical bounds on the achievable rates for the binary erasure time-division
multiple-access relay channel with correlated sources. We then design bilayer
SC-LDPC codes and analyze their asymptotic performance for this scenario. We
prove analytically that the proposed coding scheme achieves the theoretical
limit for symmetric channel conditions and uncorrelated sources. Using density
evolution, we furthermore demonstrate that our scheme approaches the
theoretical limit also for non-symmetric channel conditions and when the
sources are correlated, and we observe the threshold saturation effect that is
typical for spatially-coupled systems. Finally, we give simulation results for
large block lengths, which validate the DE analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3542</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3542</id><created>2014-02-14</created><authors><author><keyname>Pan</keyname><forenames>Liming</forenames></author><author><keyname>Hao</keyname><forenames>Dong</forenames></author><author><keyname>Rong</keyname><forenames>Zhihai</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Zero-Determinant Strategies in the Iterated Public Goods Game</title><categories>cs.GT physics.soc-ph q-bio.PE</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Press and Dyson have proposed a new class of probabilistic and
conditional strategies for the two-player iterated Prisoner's Dilemma,
so-called zero-determinant strategies. A player adopting zero-determinant
strategies is able to pin the expected payoff of the opponents or to enforce a
linear relationship between his own payoff and the opponents' payoff, in a
unilateral way. This paper considers zero-determinant strategies in the
iterated public goods game, a representative multi-player evolutionary game
where in each round each player will choose whether or not put his tokens into
a public pot, and the tokens in this pot are multiplied by a factor larger than
one and then evenly divided among all players. The analytical and numerical
results exhibit a similar yet different scenario to the case of two-player
games: (i) with small number of players or a small multiplication factor, a
player is able to unilaterally pin the expected total payoff of all other
players; (ii) a player is able to set the ratio between his payoff and the
total payoff of all other players, but this ratio is limited by an upper bound
if the multiplication factor exceeds a threshold that depends on the number of
players.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3543</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3543</id><created>2014-02-14</created><updated>2015-08-10</updated><authors><author><keyname>Gopalan</keyname><forenames>Parikshit</forenames></author><author><keyname>Yehudayoff</keyname><forenames>Amir</forenames></author></authors><title>Inequalities and tail bounds for elementary symmetric polynomial with
  applications</title><categories>cs.CC</categories><msc-class>68Q87, 68W20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the extent of independence needed to approximate the product of
bounded random variables in expectation, a natural question that has
applications in pseudorandomness and min-wise independent hashing.
  For random variables whose absolute value is bounded by $1$, we give an error
bound of the form $\sigma^{\Omega(k)}$ where $k$ is the amount of independence
and $\sigma^2$ is the total variance of the sum. Previously known bounds only
applied in more restricted settings, and were quanitively weaker. We use this
to give a simpler and more modular analysis of a construction of min-wise
independent hash functions and pseudorandom generators for combinatorial
rectangles due to Gopalan et al., which also slightly improves their
seed-length.
  Our proof relies on a new analytic inequality for the elementary symmetric
polynomials $S_k(x)$ for $x \in \mathbb{R}^n$ which we believe to be of
independent interest. We show that if $|S_k(x)|,|S_{k+1}(x)|$ are small
relative to $|S_{k-1}(x)|$ for some $k&gt;0$ then $|S_\ell(x)|$ is also small for
all $\ell &gt; k$. From these, we derive tail bounds for the elementary symmetric
polynomials when the inputs are only $k$-wise independent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3545</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3545</id><created>2014-02-14</created><updated>2015-05-29</updated><authors><author><keyname>M&#xfc;ller</keyname><forenames>Eike Hermann</forenames></author><author><keyname>Scheichl</keyname><forenames>Robert</forenames></author><author><keyname>Vainikko</keyname><forenames>Eero</forenames></author></authors><title>Petascale elliptic solvers for anisotropic PDEs on GPU clusters</title><categories>cs.DC cs.NA math.NA</categories><comments>20 pages, 6 figures. Additional explanations and clarifications of
  the characteristics of the PDE; discussion and estimate of the condition
  number. Added section and figure on the robustness of both the single-level
  and the multigrid method under variations of the Courant number. Clarified
  the terminology in the performance analysis. Added section on preliminary
  strong scaling results</comments><msc-class>65Y05 (Primary), 65N55 (Secondary)</msc-class><acm-class>G.1.0; G.1.8; C.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memory bound applications such as solvers for large sparse systems of
equations remain a challenge for GPUs. Fast solvers should be based on
numerically efficient algorithms and implemented such that global memory access
is minimised. To solve systems with up to one trillion ($10^{12}$) unknowns the
code has to make efficient use of several million individual processor cores on
large GPU clusters. We describe the multi-GPU implementation of two
algorithmically optimal iterative solvers for anisotropic elliptic PDEs which
are encountered in atmospheric modelling. In this application the condition
number is large but independent of the grid resolution and both methods are
asymptotically optimal, albeit with different absolute performance. We
parallelise the solvers and adapt them to the specific features of GPU
architectures, paying particular attention to efficient global memory access.
We achieve a performance of up to 0.78 PFLOPs when solving an equation with
$0.55\cdot 10^{12}$ unknowns on 16384 GPUs; this corresponds to about $3\%$ of
the theoretical peak performance of the machine and we use more than $40\%$ of
the peak memory bandwidth with a Conjugate Gradient (CG) solver. Although the
other solver, a geometric multigrid algorithm, has a slightly worse performance
in terms of FLOPs per second, overall it is faster as it needs less iterations
to converge; the multigrid algorithm can solve a linear PDE with half a
trillion unknowns in about one second.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3547</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3547</id><created>2014-02-14</created><updated>2014-04-19</updated><authors><author><keyname>Shachnai</keyname><forenames>Hadas</forenames></author><author><keyname>Zehavi</keyname><forenames>Meirav</forenames></author></authors><title>Representative Families: A Unified Tradeoff-Based Approach</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $M=(E,{\cal I})$ be a matroid, and let $\cal S$ be a family of subsets of
size $p$ of $E$. A subfamily $\widehat{\cal S}\subseteq{\cal S}$ represents
${\cal S}$ if for every pair of sets $X\in{\cal S}$ and $Y\subseteq E\setminus
X$ such that $X\cup Y\in{\cal I}$, there is a set $\widehat{X}\in\widehat{\cal
S}$ disjoint from $Y$ such that $\widehat{X}\cup Y\in{\cal I}$. Fomin et al.
(Proc. ACM-SIAM Symposium on Discrete Algorithms, 2014) introduced a powerful
technique for fast computation of representative families for uniform matroids.
In this paper, we show that this technique leads to a unified approach for
substantially improving the running times of parameterized algorithms for some
classic problems. This includes, among others, $k$-Partial Cover, $k$-Internal
Out-Branching, and Long Directed Cycle. Our approach exploits an interesting
tradeoff between running time and the size of the representative families.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3549</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3549</id><created>2014-02-14</created><authors><author><keyname>Wang</keyname><forenames>Xiajun</forenames></author><author><keyname>Huang</keyname><forenames>Song</forenames></author><author><keyname>Fu</keyname><forenames>Song</forenames></author><author><keyname>Kavi</keyname><forenames>Krishna</forenames></author></authors><title>Characterizing Workload of Web Applications on Virtualized Servers</title><categories>cs.PF cs.DC</categories><comments>8 pages, 8 figures, The Fourth Workshop on Big Data Benchmarks,
  Performance Optimization, and Emerging Hardware in conjunction with the 19th
  ACM International Conference on Architectural Support for Programming
  Languages and Operating Systems (ASPLOS-2014), Salt Lake City, Utah, USA,
  March 1-5, 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  With the ever increasing demands of cloud computing services, planning and
management of cloud resources has become a more and more important issue which
directed affects the resource utilization and SLA and customer satisfaction.
But before any management strategy is made, a good understanding of
applications' workload in virtualized environment is the basic fact and
principle to the resource management methods. Unfortunately, little work has
been focused on this area. Lack of raw data could be one reason; another reason
is that people still use the traditional models or methods shared under
non-virtualized environment. The study of applications' workload in virtualized
environment should take on some of its peculiar features comparing to the
non-virtualized environment. In this paper, we are open to analyze the workload
demands that reflect applications' behavior and the impact of virtualization.
The results are obtained from an experimental cloud testbed running web
applications, specifically the RUBiS benchmark application. We profile the
workload dynamics on both virtualized and non-virtualized environments and
compare the findings. The experimental results are valuable for us to estimate
the performance of applications on computer architectures, to predict SLA
compliance or violation based on the projected application workload and to
guide the decision making to support applications with the right hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3557</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3557</id><created>2014-02-14</created><authors><author><keyname>Tripathi</keyname><forenames>Subarna</forenames></author><author><keyname>Hwang</keyname><forenames>Youngbae</forenames></author><author><keyname>Belongie</keyname><forenames>Serge</forenames></author><author><keyname>Nguyen</keyname><forenames>Truong</forenames></author></authors><title>Improving Streaming Video Segmentation with Early and Mid-Level Visual
  Processing</title><categories>cs.CV</categories><comments>WACV accepted paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite recent advances in video segmentation, many opportunities remain to
improve it using a variety of low and mid-level visual cues. We propose
improvements to the leading streaming graph-based hierarchical video
segmentation (streamGBH) method based on early and mid level visual processing.
The extensive experimental analysis of our approach validates the improvement
of hierarchical supervoxel representation by incorporating motion and color
with effective filtering. We also pose and illuminate some open questions
towards intermediate level video analysis as further extension to streamGBH. We
exploit the supervoxels as an initialization towards estimation of dominant
affine motion regions, followed by merging of such motion regions in order to
hierarchically segment a video in a novel motion-segmentation framework which
aims at subsequent applications such as foreground recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3573</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3573</id><created>2014-02-14</created><updated>2015-05-19</updated><authors><author><keyname>Papoutsakis</keyname><forenames>Ioannis</forenames></author></authors><title>Tree 3-spanners of diameter at most 5</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tree spanners approximate distances within graphs; a subtree of a graph is a
tree $t$-spanner of the graph if and only if for every pair of vertices their
distance in the subtree is at most $t$ times their distance in the graph. When
a graph contains a subtree of diameter at most $t$, then trivially admits a
tree $t$-spanner. Now, determining whether a graph admits a tree $t$-spanner of
diameter at most $t+1$ is an NP complete problem, when $t\geq 4$, and it is
tractable, when $t\leq 3$. Although it is not known whether it is tractable to
decide graphs that admit a tree 3-spanner of any diameter, an efficient
algorithm to determine graphs that admit a tree 3-spanner of diameter at most 5
is presented. Moreover, it is proved that if a graph of diameter at most 3
admits a tee 3-spanner, then it admits a tree 3-spanner of diameter at most 5.
Hence, this algorithm decides tree 3-spanner admissibility of diameter at most
3 graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3578</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3578</id><created>2014-02-10</created><authors><author><keyname>Kaliszyk</keyname><forenames>Cezary</forenames></author><author><keyname>Urban</keyname><forenames>Josef</forenames></author></authors><title>Learning-assisted Theorem Proving with Millions of Lemmas</title><categories>cs.AI cs.DL cs.LG cs.LO</categories><comments>journal version of arXiv:1310.2797 (which was submitted to LPAR
  conference)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large formal mathematical libraries consist of millions of atomic inference
steps that give rise to a corresponding number of proved statements (lemmas).
Analogously to the informal mathematical practice, only a tiny fraction of such
statements is named and re-used in later proofs by formal mathematicians. In
this work, we suggest and implement criteria defining the estimated usefulness
of the HOL Light lemmas for proving further theorems. We use these criteria to
mine the large inference graph of the lemmas in the HOL Light and Flyspeck
libraries, adding up to millions of the best lemmas to the pool of statements
that can be re-used in later proofs. We show that in combination with
learning-based relevance filtering, such methods significantly strengthen
automated theorem proving of new conjectures over large formal mathematical
libraries such as Flyspeck.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3588</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3588</id><created>2014-02-14</created><updated>2014-07-04</updated><authors><author><keyname>V&#xe1;s&#xe1;rhelyi</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Vir&#xe1;gh</keyname><forenames>Csaba</forenames></author><author><keyname>Somorjai</keyname><forenames>Gerg&#x151;</forenames></author><author><keyname>Tarcai</keyname><forenames>Norbert</forenames></author><author><keyname>Sz&#xf6;r&#xe9;nyi</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Nepusz</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Vicsek</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>Outdoor flocking and formation flight with autonomous aerial robots</title><categories>cs.RO cs.MA</categories><comments>Accepted @ IROS 2014 conference</comments><msc-class>68T40, 68T42, 93C85, 70Q05, 93C95</msc-class><acm-class>I.2.8; I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first decentralized multi-copter flock that performs stable
autonomous outdoor flight with up to 10 flying agents. By decentralized and
autonomous we mean that all members navigate themselves based on the dynamic
information received from other robots in the vicinity. We do not use central
data processing or control; instead, all the necessary computations are carried
out by miniature on-board computers. The only global information the system
exploits is from GPS receivers, while the units use wireless modules to share
this positional information with other flock members locally. Collective
behavior is based on a decentralized control framework with bio-inspiration
from statistical physical modelling of animal swarms. In addition, the model is
optimized for stable group flight even in a noisy, windy, delayed and
error-prone environment. Using this framework we successfully implemented
several fundamental collective flight tasks with up to 10 units: i) we achieved
self-propelled flocking in a bounded area with self-organized object avoidance
capabilities and ii) performed collective target tracking with stable formation
flights (grid, rotating ring, straight line). With realistic numerical
simulations we demonstrated that the local broadcast-type communication and the
decentralized autonomous control method allows for the scalability of the model
for much larger flocks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3606</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3606</id><created>2014-02-14</created><updated>2015-08-10</updated><authors><author><keyname>Gopalakrishnan</keyname><forenames>Ragavendran</forenames></author><author><keyname>Doroudi</keyname><forenames>Sherwin</forenames></author><author><keyname>Ward</keyname><forenames>Amy R.</forenames></author><author><keyname>Wierman</keyname><forenames>Adam</forenames></author></authors><title>Routing and Staffing when Servers are Strategic</title><categories>cs.GT cs.SY math.OC</categories><comments>First submitted for journal publication in 2014; latest revision
  submitted in 2015. Presented in select conferences throughout 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, research focusing on the design of routing and staffing
policies for service systems has modeled servers as having fixed (possibly
heterogeneous) service rates. However, service systems are generally staffed by
people. Furthermore, people respond to workload incentives; that is, how hard a
person works can depend both on how much work there is, and how the work is
divided between the people responsible for it. In a service system, the routing
and staffing policies control such workload incentives; and so the rate servers
work will be impacted by the system's routing and staffing policies. This
observation has consequences when modeling service system performance, and our
objective is to investigate those consequences.
  We do this in the context of the M/M/N queue, which is the canonical model
for large service systems. First, we present a model for &quot;strategic&quot; servers
that choose their service rate in order to maximize a trade-off between an
&quot;effort cost&quot;, which captures the idea that servers exert more effort when
working at a faster rate, and a &quot;value of idleness&quot;, which assumes that servers
value having idle time. Next, we characterize the symmetric Nash equilibrium
service rate under any routing policy that routes based on the server idle
time. We find that the system must operate in a quality-driven regime, in which
servers have idle time, in order for an equilibrium to exist, which implies
that the staffing must have a first-order term that strictly exceeds that of
the common square-root staffing policy. Then, within the class of policies that
admit an equilibrium, we (asymptotically) solve the problem of minimizing the
total cost, when there are linear staffing costs and linear waiting costs.
Finally, we end by exploring the question of whether routing policies that are
based on the service rate, instead of the server idle time, can improve system
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3609</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3609</id><created>2014-02-14</created><authors><author><keyname>Levi</keyname><forenames>Reut</forenames></author><author><keyname>Ron</keyname><forenames>Dana</forenames></author><author><keyname>Rubinfeld</keyname><forenames>Ronitt</forenames></author></authors><title>Local Algorithms for Sparse Spanning Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the study of the problem of designing sublinear-time ({\em
local\/}) algorithms that, given an edge $(u,v)$ in a connected graph
$G=(V,E)$, decide whether $(u,v)$ belongs to a sparse spanning graph $G' =
(V,E')$ of $G$. Namely, $G'$ should be connected and $|E'|$ should be upper
bounded by $(1+\epsilon)|V|$ for a given parameter $\epsilon &gt; 0$. To this end
the algorithms may query the incidence relation of the graph $G$, and we seek
algorithms whose query complexity and running time (per given edge $(u,v)$) is
as small as possible. Such an algorithm may be randomized but (for a fixed
choice of its random coins) its decision on different edges in the graph should
be consistent with the same spanning graph $G'$ and independent of the order of
queries.
  We first show that for general (bounded-degree) graphs, the query complexity
of any such algorithm must be $\Omega(\sqrt{|V|})$. This lower bound holds for
graphs that have high expansion. We then turn to design and analyze algorithms
both for graphs with high expansion (obtaining a result that roughly matches
the lower bound) and for graphs that are (strongly) non-expanding (obtaining
results in which the complexity does not depend on $|V|$). The complexity of
the problem for graphs that do not fall into these two categories is left as an
open question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3610</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3610</id><created>2014-02-14</created><authors><author><keyname>Gopalakrishnan</keyname><forenames>Ragavendran</forenames></author><author><keyname>Marden</keyname><forenames>Jason R.</forenames></author><author><keyname>Wierman</keyname><forenames>Adam</forenames></author></authors><title>Potential Games are Necessary to Ensure Pure Nash Equilibria in Cost
  Sharing Games</title><categories>cs.GT cs.MA cs.SY math.CO</categories><comments>Presented at various conferences in 2013. Accepted for journal
  publication in December 2013</comments><msc-class>Primary: 91A10, secondary: 91A40, 91A80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of designing distribution rules to share &quot;welfare&quot;
(cost or revenue) among individually strategic agents. There are many known
distribution rules that guarantee the existence of a (pure) Nash equilibrium in
this setting, e.g., the Shapley value and its weighted variants; however, a
characterization of the space of distribution rules that guarantee the
existence of a Nash equilibrium is unknown. Our work provides an exact
characterization of this space for a specific class of scalable and separable
games, which includes a variety of applications such as facility location,
routing, network formation, and coverage games. Given arbitrary local welfare
functions W, we prove that a distribution rule guarantees equilibrium existence
for all games (i.e., all possible sets of resources, agent action sets, etc.)
if and only if it is equivalent to a generalized weighted Shapley value on some
&quot;ground&quot; welfare functions W', which can be distinct from W. However, if
budget-balance is required in addition to the existence of a Nash equilibrium,
then W' must be the same as W. We also provide an alternate characterization of
this space in terms of &quot;generalized&quot; marginal contributions, which is more
appealing from the point of view of computational tractability. A possibly
surprising consequence of our result is that, in order to guarantee equilibrium
existence in all games with any fixed local welfare functions, it is necessary
to work within the class of potential games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3611</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3611</id><created>2014-02-14</created><authors><author><keyname>Kim</keyname><forenames>Kangjin</forenames></author><author><keyname>Fainekos</keyname><forenames>Georgios</forenames></author></authors><title>Revision of Specification Automata under Quantitative Preferences</title><categories>cs.FL</categories><comments>9 pages, 3 figures, 3 tables, in Proceedings of the IEEE Conference
  on Robotics and Automation, May 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of revising specifications with preferences for automata
based control synthesis problems. In this class of revision problems, the user
provides a numerical ranking of the desirability of the subgoals in their
specifications. When the specification cannot be satisfied on the system, then
our algorithms automatically revise the specification so that the least
desirable user goals are removed from the specification. We propose two
different versions of the revision problem with preferences. In the first
version, the algorithm returns an exact solution while in the second version
the algorithm is an approximation algorithm with non-constant approximation
ratio. Finally, we demonstrate the scalability of our algorithms and we
experimentally study the approximation ratio of the approximation algorithm on
random problem instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3613</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3613</id><created>2014-02-14</created><authors><author><keyname>Janovsk&#xfd;</keyname><forenames>Pavel</forenames></author><author><keyname>&#x10c;&#xe1;p</keyname><forenames>Michal</forenames></author><author><keyname>Vok&#x159;&#xed;nek</keyname><forenames>Ji&#x159;&#xed;</forenames></author></authors><title>Finding Coordinated Paths for Multiple Holonomic Agents in 2-d Polygonal
  Environment</title><categories>cs.AI cs.RO</categories><comments>Proceedings of the 13th International Conference on Autonomous Agents
  and Multiagent Systems (AAMAS 2014)</comments><msc-class>68T42 (Primary), 68T40 (Secondary)</msc-class><acm-class>I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Avoiding collisions is one of the vital tasks for systems of autonomous
mobile agents. We focus on the problem of finding continuous coordinated paths
for multiple mobile disc agents in a 2-d environment with polygonal obstacles.
The problem is PSPACE-hard, with the state space growing exponentially in the
number of agents. Therefore, the state of the art methods include mainly
reactive techniques and sampling-based iterative algorithms.
  We compare the performance of a widely-used reactive method ORCA with three
variants of a popular planning algorithm RRT* applied to multi-agent path
planning and find that an algorithm combining reactive collision avoidance and
RRT* planning, which we call ORCA-RRT* can be used to solve instances that are
out of the reach of either of the techniques. We experimentally show that: 1)
the reactive part of the algorithm can efficiently solve many multi-agent path
finding problems involving large number of agents, for which RRT* algorithm is
often unable to find a solution in limited time and 2) the planning component
of the algorithm is able to solve many instances containing local minima, where
reactive techniques typically fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3626</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3626</id><created>2014-02-14</created><authors><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Strong converse for the quantum capacity of the erasure channel for
  almost all codes</title><categories>quant-ph cs.IT math.IT</categories><comments>15 pages, submission to the 9th Conference on the Theory of Quantum
  Computation, Communication, and Cryptography (TQC 2014)</comments><journal-ref>Proceedings of the 9th Conference on the Theory of Quantum
  Computation, Communication and Cryptography, LIPIcs vol. 27, pages 52-66, May
  2014</journal-ref><doi>10.4230/LIPIcs.TQC.2014.52</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A strong converse theorem for channel capacity establishes that the error
probability in any communication scheme for a given channel necessarily tends
to one if the rate of communication exceeds the channel's capacity.
Establishing such a theorem for the quantum capacity of degradable channels has
been an elusive task, with the strongest progress so far being a so-called
&quot;pretty strong converse&quot;. In this work, Morgan and Winter proved that the
quantum error of any quantum communication scheme for a given degradable
channel converges to a value larger than $1/\sqrt{2}$ in the limit of many
channel uses if the quantum rate of communication exceeds the channel's quantum
capacity. The present paper establishes a theorem that is a counterpart to this
&quot;pretty strong converse&quot;. We prove that the large fraction of codes having a
rate exceeding the erasure channel's quantum capacity have a quantum error
tending to one in the limit of many channel uses. Thus, our work adds to the
body of evidence that a fully strong converse theorem should hold for the
quantum capacity of the erasure channel. As a side result, we prove that the
classical capacity of the quantum erasure channel obeys the strong converse
property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3631</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3631</id><created>2014-02-14</created><updated>2014-05-08</updated><authors><author><keyname>Hsu</keyname><forenames>Justin</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Roughgarden</keyname><forenames>Tim</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author></authors><title>Privately Solving Linear Programs</title><categories>cs.DS cs.CR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we initiate the systematic study of solving linear programs
under differential privacy. The first step is simply to define the problem: to
this end, we introduce several natural classes of private linear programs that
capture different ways sensitive data can be incorporated into a linear
program. For each class of linear programs we give an efficient, differentially
private solver based on the multiplicative weights framework, or we give an
impossibility result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3634</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3634</id><created>2014-02-14</created><authors><author><keyname>Srivastava</keyname><forenames>Vaibhav</forenames></author><author><keyname>Leonard</keyname><forenames>Naomi Ehrich</forenames></author></authors><title>Collective Decision-Making in Ideal Networks: The Speed-Accuracy
  Tradeoff</title><categories>math.OC cs.MA cs.SY</categories><comments>to appear in IEEE TCNS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study collective decision-making in a model of human groups, with network
interactions, performing two alternative choice tasks. We focus on the
speed-accuracy tradeoff, i.e., the tradeoff between a quick decision and a
reliable decision, for individuals in the network. We model the evidence
aggregation process across the network using a coupled drift diffusion model
(DDM) and consider the free response paradigm in which individuals take their
time to make the decision. We develop reduced DDMs as decoupled approximations
to the coupled DDM and characterize their efficiency. We determine high
probability bounds on the error rate and the expected decision time for the
reduced DDM. We show the effect of the decision-maker's location in the network
on their decision-making performance under several threshold selection
criteria. Finally, we extend the coupled DDM to the coupled Ornstein-Uhlenbeck
model for decision-making in two alternative choice tasks with recency effects,
and to the coupled race model for decision-making in multiple alternative
choice tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3643</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3643</id><created>2014-02-14</created><authors><author><keyname>Akbarpour</keyname><forenames>Mohammad</forenames></author><author><keyname>Li</keyname><forenames>Shengwu</forenames></author><author><keyname>Gharan</keyname><forenames>Shayan Oveis</forenames></author></authors><title>Dynamic Matching Market Design</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a simple benchmark model of dynamic matching in networked
markets, where agents arrive and depart stochastically and the network of
acceptable transactions among agents forms a random graph. We analyze our model
from three perspectives: waiting, optimization, and information. The main
insight of our analysis is that waiting to thicken the market can be
substantially more important than increasing the speed of transactions, and
this is quite robust to the presence of waiting costs. From an optimization
perspective, naive local algorithms, that choose the right time to match agents
but do not exploit global network structure, can perform very close to optimal
algorithms. From an information perspective, algorithms that employ even
partial information on agents' departure times perform substantially better
than those that lack such information. To elicit agents' departure times, we
design an incentive-compatible continuous-time dynamic mechanism without
transfers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3648</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3648</id><created>2014-02-15</created><authors><author><keyname>Kabra</keyname><forenames>Shikha</forenames></author><author><keyname>Agarwal</keyname><forenames>Ritika</forenames></author></authors><title>Auto Spell Suggestion for High Quality Speech Synthesis in Hindi</title><categories>cs.CL cs.SD</categories><comments>4 pages, 5 figures. International Journal of Computer Applications,
  2014</comments><doi>10.5120/15302-4039</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of Text-to-Speech (TTS) synthesis in a particular language is to
convert arbitrary input text to intelligible and natural sounding speech.
However, for a particular language like Hindi, which is a highly confusing
language (due to very close spellings), it is not an easy task to identify
errors/mistakes in input text and an incorrect text degrade the quality of
output speech hence this paper is a contribution to the development of high
quality speech synthesis with the involvement of Spellchecker which generates
spell suggestions for misspelled words automatically. Involvement of
spellchecker would increase the efficiency of speech synthesis by providing
spell suggestions for incorrect input text. Furthermore, we have provided the
comparative study for evaluating the resultant effect on to phonetic text by
adding spellchecker on to input text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3651</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3651</id><created>2014-02-15</created><authors><author><keyname>Patel</keyname><forenames>Bhavikkumar</forenames></author><author><keyname>Shah</keyname><forenames>Dhrumil</forenames></author></authors><title>Evaluating ECG Capturing Using Sound-Card of PC/Laptop</title><categories>cs.OH</categories><comments>22 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of the Evaluating ECG capturing using sound-card of PC/Laptop is
provided portable and low cost ECG monitoring system using laptop and mobile
phones. There is no need to interface micro controller or any other device to
transmit ECG data. This research is based on hardware design, implementation,
signal capturing and Evaluation of an ECG processing and analyzing system which
attend the physicians in heart disease diagnosis. Some important modification
is given in design part to avoid all definitive ECG instrument problems faced
in previous designs. Moreover, attenuate power frequency noise and noise that
produces from patient's body have required additional developments. The
hardware design has basically three units: transduction and conditioning Unit,
interfacing unit and data processing unit.The most focusing factor is the ECG
signal/data transmits in laptop/PC via microphone pin. The live simulation is
possible using SOUNDSCOPE software in PC/Laptop. The software program that is
written in MATLAB and LAB-View performs data acquisition (record, stored,
filtration) and several tasks such as QRS detection, calculate heart rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3653</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3653</id><created>2014-02-15</created><authors><author><keyname>Becker</keyname><forenames>Aaron</forenames></author><author><keyname>Ertel</keyname><forenames>Chris</forenames></author><author><keyname>McLurkin</keyname><forenames>James</forenames></author></authors><title>Crowdsourcing Swarm Manipulation Experiments: A Massive Online User
  Study with Large Swarms of Simple Robots</title><categories>cs.RO</categories><comments>8 pages, 13 figures, to appear at 2014 IEEE International Conference
  on Robotics and Automation (ICRA 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Micro- and nanorobotics have the potential to revolutionize many applications
including targeted material delivery, assembly, and surgery. The same
properties that promise breakthrough solutions---small size and large
populations---present unique challenges to generating controlled motion. We
want to use large swarms of robots to perform manipulation tasks;
unfortunately, human-swarm interaction studies as conducted today are limited
in sample size, are difficult to reproduce, and are prone to hardware failures.
We present an alternative.
  This paper examines the perils, pitfalls, and possibilities we discovered by
launching SwarmControl.net, an online game where players steer swarms of up to
500 robots to complete manipulation challenges. We record statistics from
thousands of players, and use the game to explore aspects of large-population
robot control. We present the game framework as a new, open-source tool for
large-scale user experiments. Our results have potential applications in human
control of micro- and nanorobots, supply insight for automatic controllers, and
provide a template for large online robotic research experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3654</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3654</id><created>2014-02-15</created><authors><author><keyname>Singhala</keyname><forenames>Piyush</forenames></author><author><keyname>Shah</keyname><forenames>Dhrumil</forenames></author><author><keyname>Patel</keyname><forenames>Bhavikkumar</forenames></author></authors><title>Temperature Control using Fuzzy Logic</title><categories>cs.SY</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of the temperature control is to heat the system up todelimitated
temperature, afterwardhold it at that temperature in insured manner. Fuzzy
Logic Controller (FLC) is best way in which this type of precision control can
be accomplished by controller. During past twenty yearssignificant amount of
research using fuzzy logichas done in this field of control of non-linear
dynamical system. Here we have developed temperature control system using fuzzy
logic. Control theory techniques are the root from which convention controllers
are deducted. The desired response of the output can be guaranteed by the
feedback controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3655</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3655</id><created>2014-02-15</created><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author><author><keyname>Vinod</keyname><forenames>A.</forenames></author><author><keyname>Jeyakumar</keyname><forenames>P.</forenames></author></authors><title>An Energy Efficient Neighbour Node Discovery Method for Wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>4 figures and 5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The discovery of neighbouring nodes in multihop wireless networks has become
a key challenge. Due to tribulations in communication, synchronization loss
between nodes, disparity in transmission power etc, the connectivity of nodes
will always experience disruptions. On the other hand, the energy utilization
by the nodes also became critical . In this paper, we propose a new method for
neighbour discovery in wireless sensor networks (WSNs) which pays an eminent
consideration for energy utilization and QoS parameters like latency,
throughput, error rate etc. In the proposed method, the network routing is
enhanced using AOMDV protocol which can accurately discover the neighbour nodes
and power management with HMAC protocol which reduces the energy utilization
significantly. A complete analysis is being performed to estimate how the Q o S
metrics varies in various scenarios of power consumption in wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3656</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3656</id><created>2014-02-15</created><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author><author><keyname>Vijayalakshmi</keyname><forenames>V. J.</forenames></author></authors><title>Analysis of Carrier Frequency Selective Offset Estimation - Using
  Zero-IF and ZCZIn MC-DS-CDMA</title><categories>cs.NI cs.IT math.IT</categories><comments>5 pages and 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method for frequency synchronization based upon Zero-Intermediate
Frequency Zero-IF receiver and characteristics of the received signal s power
spectrum for MC-DSCDMA Up link system is proposed in this paper. In addition to
this, employing Zero Correlation Zone (ZCZ) sequences, designed specifically
for quasi synchronous up link transmissions, is proposed to exploit frequency
and temporal diversity in frequency-selective block fading channels. The
variance for Carrier Frequency Offset (CFO) estimators of MC DS CDMA Uplink is
compared with that of an OFDM system to estimate the CFO. Our study and results
show that the MC DS CDMA system is outperforming the OFDM method
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3657</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3657</id><created>2014-02-15</created><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author><author><keyname>Kumar</keyname><forenames>B. Praveen</forenames></author><author><keyname>Babu</keyname><forenames>S. Suresh</forenames></author><author><keyname>Purusothaman</keyname><forenames>R.</forenames></author><author><keyname>Thomas</keyname><forenames>shijin</forenames></author></authors><title>A Narrative Vehicle Protection Representation for Vehicle Speed
  Regulator Under Driver Exhaustion -- A Study</title><categories>cs.CV cs.HC</categories><comments>4 pages 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Driver fatigue is one of the important factors that cause traffic accidents,
and the ever-increasing number due to diminished drivers vigilance level has
become a problem of serious concern to society. Drivers with a diminished
vigilance level suffer from a marked decline in their abilities of perception,
recognition, and vehicle control, and therefore pose serious danger to their
own life and the lives of other people. Exhaustion resulting from sleep
deprivation or sleep disorders is an important factor in the creasing number of
accidents. In this projected work, we discuss the various methods of the
existing and the proposed method based on a real time online safety prototype
that controls the vehicle speed under driver fatigue. The purpose of such a
model is to advance a system to detect fatigue symptoms in drivers and control
the speed of vehicle to avoid accidents. This system was tested adequately with
subjects of different technology of various researchers finally the validity of
the proposed model for vehicle speed controller based on driver fatigue
detection is shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3661</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3661</id><created>2014-02-15</created><updated>2014-12-04</updated><authors><author><keyname>Jeljeli</keyname><forenames>Hamza</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Resolution of Linear Algebra for the Discrete Logarithm Problem Using
  GPU and Multi-core Architectures</title><categories>cs.CR</categories><comments>Euro-Par 2014 Parallel Processing, Aug 2014, Porto, Portugal.
  \&amp;lt;http://europar2014.dcc.fc.up.pt/\&amp;gt;</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cryptanalysis, solving the discrete logarithm problem (DLP) is key to
assessing the security of many public-key cryptosystems. The index-calculus
methods, that attack the DLP in multiplicative subgroups of finite fields,
require solving large sparse systems of linear equations modulo large primes.
This article deals with how we can run this computation on GPU- and
multi-core-based clusters, featuring InfiniBand networking. More specifically,
we present the sparse linear algebra algorithms that are proposed in the
literature, in particular the block Wiedemann algorithm. We discuss the
parallelization of the central matrix--vector product operation from both
algorithmic and practical points of view, and illustrate how our approach has
contributed to the recent record-sized DLP computation in GF($2^{809}$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3664</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3664</id><created>2014-02-15</created><authors><author><keyname>Deng</keyname><forenames>Xinyang</forenames></author><author><keyname>Hu</keyname><forenames>Yong</forenames></author><author><keyname>Chan</keyname><forenames>Felix</forenames></author><author><keyname>Mahadevan</keyname><forenames>Sankaran</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>Parameter estimation based on interval-valued belief structures</title><categories>cs.AI</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parameter estimation based on uncertain data represented as belief structures
is one of the latest problems in the Dempster-Shafer theory. In this paper, a
novel method is proposed for the parameter estimation in the case where belief
structures are uncertain and represented as interval-valued belief structures.
Within our proposed method, the maximization of likelihood criterion and
minimization of estimated parameter's uncertainty are taken into consideration
simultaneously. As an illustration, the proposed method is employed to estimate
parameters for deterministic and uncertain belief structures, which
demonstrates its effectiveness and versatility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3668</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3668</id><created>2014-02-15</created><updated>2014-06-12</updated><authors><author><keyname>Granger</keyname><forenames>Robert</forenames></author><author><keyname>Kleinjung</keyname><forenames>Thorsten</forenames></author><author><keyname>Zumbr&#xe4;gel</keyname><forenames>Jens</forenames></author></authors><title>Breaking `128-bit Secure' Supersingular Binary Curves (or how to solve
  discrete logarithms in ${\mathbb F}_{2^{4 \cdot 1223}}$ and ${\mathbb
  F}_{2^{12 \cdot 367}}$)</title><categories>math.NT cs.CR</categories><comments>18 pages, 1 figure</comments><msc-class>11T71, 12Y05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In late 2012 and early 2013 the discrete logarithm problem (DLP) in finite
fields of small characteristic underwent a dramatic series of breakthroughs,
culminating in a heuristic quasi-polynomial time algorithm, due to Barbulescu,
Gaudry, Joux and Thom\'e. Using these developments, Adj, Menezes, Oliveira and
Rodr\'iguez-Henr\'iquez analysed the concrete security of the DLP, as it arises
from pairings on (the Jacobians of) various genus one and two supersingular
curves in the literature, which were originally thought to be $128$-bit secure.
In particular, they suggested that the new algorithms have no impact on the
security of a genus one curve over ${\mathbb F}_{2^{1223}}$, and reduce the
security of a genus two curve over ${\mathbb F}_{2^{367}}$ to $94.6$ bits. In
this paper we propose a new field representation and efficient general descent
principles which together make the new techniques far more practical. Indeed,
at the `128-bit security level' our analysis shows that the aforementioned
genus one curve has approximately $59$ bits of security, and we report a total
break of the genus two curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3689</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3689</id><created>2014-02-15</created><authors><author><keyname>Janvier</keyname><forenames>Maxime</forenames></author><author><keyname>Alameda-Pineda</keyname><forenames>Xavier</forenames></author><author><keyname>Girin</keyname><forenames>Laurent</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author></authors><title>Sound Representation and Classification Benchmark for Domestic Robots</title><categories>cs.SD cs.RO</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of sound representation and classification and present
results of a comparative study in the context of a domestic robotic scenario. A
dataset of sounds was recorded in realistic conditions (background noise,
presence of several sound sources, reverberations, etc.) using the humanoid
robot NAO. An extended benchmark is carried out to test a variety of
representations combined with several classifiers. We provide results obtained
with the annotated dataset and we assess the methods quantitatively on the
basis of their classification scores, computation times and memory
requirements. The annotated dataset is publicly available at
https://team.inria.fr/perception/nard/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3690</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3690</id><created>2014-02-15</created><updated>2014-05-20</updated><authors><author><keyname>Heras</keyname><forenames>J&#xf3;nathan</forenames></author><author><keyname>Komendantskaya</keyname><forenames>Ekaterina</forenames></author><author><keyname>Schmidt</keyname><forenames>Martin</forenames></author></authors><title>(Co)recursion in Logic Programming: Lazy vs Eager</title><categories>cs.PL cs.LO</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CoAlgebraic Logic Programming (CoALP) is a dialect of Logic Programming
designed to bring a more precise compile-time and run-time analysis of
termination and productivity for recursive and corecursive functions in Logic
Programming. Its second goal is to introduce guarded lazy (co)recursion akin to
functional theorem provers into logic programming. In this paper, we explain
lazy features of CoALP, and compare them with the loop-analysis and eager
execution in Coinductive Logic Programming (CoLP). We conclude by outlining the
future directions in developing the guarded (co)recursion in logic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3696</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3696</id><created>2014-02-15</created><authors><author><keyname>Broutin</keyname><forenames>Nicolas</forenames></author><author><keyname>Devroye</keyname><forenames>Luc</forenames></author><author><keyname>Lugosi</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>Connectivity of sparse Bluetooth networks</title><categories>math.PR cs.DM cs.NI math.CO</categories><msc-class>05C80, 60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a random geometric graph defined on $n$ vertices uniformly
distributed in the $d$-dimensional unit torus. Two vertices are connected if
their distance is less than a &quot;visibility radius&quot; $r_n$. We consider {\sl
Bluetooth networks} that are locally sparsified random geometric graphs. Each
vertex selects $c$ of its neighbors in the random geometric graph at random and
connects only to the selected points. We show that if the visibility radius is
at least of the order of $n^{-(1-\delta)/d}$ for some $\delta &gt; 0$, then a
constant value of $c$ is sufficient for the graph to be connected, with high
probability. It suffices to take $c \ge \sqrt{(1+\epsilon)/\delta} + K$ for any
positive $\epsilon$ where $K$ is a constant depending on $d$ only. On the other
hand, with $c\le \sqrt{(1-\epsilon)/\delta}$, the graph is disconnected, with
high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3698</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3698</id><created>2014-02-15</created><authors><author><keyname>Back</keyname><forenames>Adam</forenames></author><author><keyname>Bentov</keyname><forenames>Iddo</forenames></author></authors><title>Note on fair coin toss via Bitcoin</title><categories>cs.CR</categories><comments>See also https://bitcointalk.org/index.php?topic=277048.0</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short note we show that the Bitcoin network can allow remote parties
to gamble with their bitcoins by tossing a fair or biased coin, with no need
for a trusted party, and without the possibility of extortion by dishonest
parties who try to abort. The superfluousness of having a trusted party implies
that there is no house edge, as is the case with centralized services that are
supposed to generate a profit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3718</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3718</id><created>2014-02-15</created><authors><author><keyname>Long</keyname><forenames>Ying</forenames></author><author><keyname>Wu</keyname><forenames>Kang</forenames></author><author><keyname>Mao</keyname><forenames>Qizhi</forenames></author></authors><title>Simulating urban expansion in the parcel level for all Chinese cities</title><categories>cs.MA</categories><comments>23 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale models are generally associated with big modelling units in
space, like counties or super grids (several to dozens km2). Few applied urban
models can pursue large-scale extent with fine-level units simultaneously due
to data availability and computation load. The framework of automatic
identification and characterization parcels developed by Long and Liu (2013)
makes such an ideal model possible by establishing existing urban parcels using
road networks and points of interest for a super large area (like a country or
a continent). In this study, a mega-vector-parcels cellular automata model
(MVP-CA) is developed for simulating urban expansion in the parcel level for
all 654 Chinese cities. Existing urban parcels in 2012, for initiating MVP-CA,
are generated using multi-levelled road networks and ubiquitous points of
interest, followed by simulating parcel-based urban expansion of all cities
during 2012-2017. Reflecting national spatial development strategies discussed
extensively by academics and decision makers, the baseline scenario and other
two simulated urban expansion scenarios have been tested and compared
horizontally. As the first fine-scale urban expansion model from the national
scope, its academic contributions, practical applications, and potential biases
are discussed in this paper as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3722</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3722</id><created>2014-02-15</created><authors><author><keyname>Goldberg</keyname><forenames>Yoav</forenames></author><author><keyname>Levy</keyname><forenames>Omer</forenames></author></authors><title>word2vec Explained: deriving Mikolov et al.'s negative-sampling
  word-embedding method</title><categories>cs.CL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The word2vec software of Tomas Mikolov and colleagues
(https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and
provides state-of-the-art word embeddings. The learning models behind the
software are described in two research papers. We found the description of the
models in these papers to be somewhat cryptic and hard to follow. While the
motivations and presentation may be obvious to the neural-networks
language-modeling crowd, we had to struggle quite a bit to figure out the
rationale behind the equations.
  This note is an attempt to explain equation (4) (negative sampling) in
&quot;Distributed Representations of Words and Phrases and their Compositionality&quot;
by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3727</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3727</id><created>2014-02-15</created><updated>2015-01-12</updated><authors><author><keyname>Park</keyname><forenames>Jaehyun</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Multi-user Linear Precoding for Multi-polarized Massive MIMO System
  under Imperfect CSIT</title><categories>cs.IT math.IT</categories><comments>accepted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The space limitation and the channel acquisition prevent Massive MIMO from
being easily deployed in a practical setup. Motivated by current deployments of
LTE-Advanced, the use of multi-polarized antennas can be an efficient solution
to address the space constraint. Furthermore, the dual-structured precoding, in
which a preprocessing based on the spatial correlation and a subsequent linear
precoding based on the short-term channel state information at the transmitter
(CSIT) are concatenated, can reduce the feedback overhead efficiently. By
grouping and preprocessing spatially correlated mobile stations (MSs), the
dimension of the precoding signal space is reduced and the corresponding
short-term CSIT dimension is reduced. In this paper, to reduce the feedback
overhead further, we propose a dual-structured multi-user linear precoding, in
which the subgrouping method based on co-polarization is additionally applied
to the spatially grouped MSs in the preprocessing stage. Furthermore, under
imperfect CSIT, the proposed scheme is asymptotically analyzed based on random
matrix theory. By investigating the behavior of the asymptotic performance, we
also propose a new dual-structured precoding in which the precoding mode is
switched between two dual-structured precoding strategies with 1) the
preprocessing based only on the spatial correlation and 2) the preprocessing
based on both the spatial correlation and polarization. Finally, we extend it
to 3D dual-structured precoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3735</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3735</id><created>2014-02-15</created><updated>2014-02-18</updated><authors><author><keyname>Panagou</keyname><forenames>Dimitra</forenames></author><author><keyname>Turpin</keyname><forenames>Matthew</forenames></author><author><keyname>Kumar</keyname><forenames>Vijay</forenames></author></authors><title>Decentralized Goal Assignment and Trajectory Generation in Multi-Robot
  Networks</title><categories>cs.MA cs.RO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of decentralized goal assignment and
trajectory generation for multi-robot networks when only local communication is
available, and proposes an approach based on methods related to switched
systems and set invariance. A family of Lyapunov-like functions is employed to
encode the (local) decision making among candidate goal assignments, under
which the agents pick the assignment which results in the shortest total
distance to the goals. An additional family of Lyapunov-like barrier functions
is activated in the case when the optimal assignment may lead to colliding
trajectories, maintaining thus system safety while preserving the convergence
guarantees. The proposed switching strategies give rise to feedback control
policies which are scalable and computationally efficient as the number of
agents increases, and therefore are suitable for applications including
first-response deployment of robotic networks under limited information
sharing. Simulations demonstrate the efficacy of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3736</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3736</id><created>2014-02-15</created><updated>2014-04-18</updated><authors><author><keyname>Fiala</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>Hubi&#x10d;ka</keyname><forenames>Jan</forenames></author><author><keyname>Long</keyname><forenames>Yangjing</forenames></author></authors><title>Universality of intervals of line graph order</title><categories>math.CO cs.DM</categories><comments>13 pages, 8 figures, accepted to European Journal of Combinatorics</comments><msc-class>05C60, 05C76, 06A06</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for every $d\geq 3$ the homomorphism order of the class of line
graphs of finite graphs with maximal degree $d$ is universal. This means that
every finite or countably infinite partially ordered set may be represented by
line graphs of graphs with maximal degree $d$ ordered by the existence of a
homomorphism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3749</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3749</id><created>2014-02-15</created><authors><author><keyname>Becker</keyname><forenames>Aaron</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Fekete</keyname><forenames>S&#xe1;ndor P.</forenames></author><author><keyname>McLurkin</keyname><forenames>James</forenames></author></authors><title>Particle Computation: Designing Worlds to Control Robot Swarms with only
  Global Signals</title><categories>cs.RO</categories><comments>8 pages, 7 figures, to appear IEEE International Conference on
  Robotics and Automation (ICRA 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Micro- and nanorobots are often controlled by global input signals, such as
an electromagnetic or gravitational field. These fields move each robot
maximally until it hits a stationary obstacle or another stationary robot. This
paper investigates 2D motion-planning complexity for large swarms of simple
mobile robots (such as bacteria, sensors, or smart building material).
  In previous work we proved it is NP-hard to decide whether a given initial
configuration can be transformed into a desired target configuration; in this
paper we prove a stronger result: the problem of finding an optimal control
sequence is PSPACE-complete. On the positive side, we show we can build useful
systems by designing obstacles. We present a reconfigurable hardware platform
and demonstrate how to form arbitrary permutations and build a compact absolute
encoder. We then take the same platform and use dual-rail logic to build a
universal logic gate that concurrently evaluates AND, NAND, NOR and OR
operations. Using many of these gates and appropriate interconnects we can
evaluate any logical expression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3757</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3757</id><created>2014-02-16</created><updated>2015-08-22</updated><authors><author><keyname>Wang</keyname><forenames>Weina</forenames></author><author><keyname>Ying</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Junshan</forenames></author></authors><title>On the Relation Between Identifiability, Differential Privacy and
  Mutual-Information Privacy</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the relation between three different notions of
privacy: identifiability, differential privacy and mutual-information privacy.
Under a unified privacy-distortion framework, where the distortion is defined
to be the Hamming distance of the input and output databases, we establish some
fundamental connections between these three privacy notions. Given a distortion
level $D$, define $\epsilon_{\mathrm{i}}^*(D)$ to be the smallest (best)
identifiability level, and $\epsilon_{\mathrm{d}}^*(D)$ to be the smallest
differential privacy level. We characterize $\epsilon_{\mathrm{i}}^*(D)$ and
$\epsilon_{\mathrm{d}}^*(D)$, and prove that
$\epsilon_{\mathrm{i}}^*(D)-\epsilon_X\le\epsilon_{\mathrm{d}}^*(D)\le\epsilon_{\mathrm{i}}^*(D)$
for $D$ in some range, where $\epsilon_X$ is a constant depending on the
distribution of the original database $X$, and diminishes to zero when the
distribution of $X$ is uniform. Furthermore, we show that identifiability and
mutual-information privacy are consistent in the sense that given distortion
level $D$, the mechanism that optimizes the mutual-information privacy also
minimizes the identifiability level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3766</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3766</id><created>2014-02-16</created><authors><author><keyname>Borello</keyname><forenames>Alex</forenames><affiliation>LACL</affiliation></author><author><keyname>Cervelle</keyname><forenames>Julien</forenames><affiliation>LACL</affiliation></author><author><keyname>Vanier</keyname><forenames>Pascal</forenames></author></authors><title>Turing degrees of limit sets of cellular automata</title><categories>cs.FL nlin.CG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular automata are discrete dynamical systems and a model of computation.
The limit set of a cellular automaton consists of the configurations having an
infinite sequence of preimages. It is well known that these always contain a
computable point and that any non-trivial property on them is undecidable. We
go one step further in this article by giving a full characterization of the
sets of Turing degrees of cellular automata: they are the same as the sets of
Turing degrees of effectively closed sets containing a computable point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3774</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3774</id><created>2014-02-16</created><updated>2014-05-27</updated><authors><author><keyname>Fiala</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>Klav&#xed;k</keyname><forenames>Pavel</forenames></author><author><keyname>Kratochv&#xed;l</keyname><forenames>Jan</forenames></author><author><keyname>Nedela</keyname><forenames>Roman</forenames></author></authors><title>Algorithmic Aspects of Regular Graph Covers with Applications to Planar
  Graphs</title><categories>math.CO cs.DM cs.DS</categories><comments>The conference version accepted to ICALP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ covers a graph $H$ if there exists a locally bijective
homomorphism from $G$ to $H$. We deal with regular covers in which this locally
bijective homomorphism is prescribed by an action of a subgroup of ${\rm
Aut}(G)$. Regular covers have many applications in constructions and studies of
big objects all over mathematics and computer science.
  We study computational aspects of regular covers that have not been addressed
before. The decision problem RegularCover asks for two given graphs $G$ and $H$
whether $G$ regularly covers $H$. When $|H|=1$, this problem becomes Cayley
graph recognition for which the complexity is still unresolved. Another special
case arises for $|G| = |H|$ when it becomes the graph isomorphism problem.
Therefore, we restrict ourselves to graph classes with polynomially solvable
graph isomorphism.
  Inspired by Negami, we apply the structural results used by Babai in the
1970's to study automorphism groups of graphs. Our main result is the following
FPT meta-algorithm: Let $\cal C$ be a class of graphs such that the structure
of automorphism groups of 3-connected graphs in $\cal C$ is simple. Then we can
solve RegularCover for $\cal C$-inputs $G$ in time $O^*(2^{e(H)/2})$ where
$e(H)$ denotes the number of the edges of $H$. As one example of $\cal C$, this
meta-algorithm applies to planar graphs. In comparison, testing general graph
covers is known to be NP-complete for planar inputs $G$ even for small fixed
graphs $H$ such as $K_4$ or $K_5$. Most of our results also apply to general
graphs, in particular the complete structural understanding of regular covers
for 2-cuts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3779</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3779</id><created>2014-02-16</created><authors><author><keyname>Mavridis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Bellotto</keyname><forenames>Nicola</forenames></author><author><keyname>Iliopoulos</keyname><forenames>Konstantinos</forenames></author><author><keyname>Van de Weghe</keyname><forenames>Nico</forenames></author></authors><title>QTC3D: Extending the Qualitative Trajectory Calculus to Three Dimensions</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial interactions between agents (humans, animals, or machines) carry
information of high value to human or electronic observers. However, not all
the information contained in a pair of continuous trajectories is important and
thus the need for qualitative descriptions of interaction trajectories arises.
The Qualitative Trajectory Calculus (QTC) (Van de Weghe, 2004) is a promising
development towards this goal. Numerous variants of QTC have been proposed in
the past and QTC has been applied towards analyzing various interaction
domains. However, an inherent limitation of those QTC variations that deal with
lateral movements is that they are limited to two-dimensional motion;
therefore, complex three-dimensional interactions, such as those occurring
between flying planes or birds, cannot be captured. Towards that purpose, in
this paper QTC3Dis presented: a novel qualitative trajectory calculus that can
deal with full three-dimensional interactions. QTC3D is based on
transformations of the Frenet-Serret frames accompanying the trajectories of
the moving objects. Apart from the theoretical exposition, including definition
and properties, as well as computational aspects, we also present an
application of QTC3D towards modeling bird flight. Thus, the power of QTC is
now extended to the full dimensionality of physical space, enabling succinct
yet rich representations of spatial interactions between agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3781</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3781</id><created>2014-02-16</created><authors><author><keyname>Alzeini</keyname><forenames>H I</forenames></author><author><keyname>Hameed</keyname><forenames>Sh A</forenames></author><author><keyname>Habaebi</keyname><forenames>M H</forenames></author></authors><title>A Framework for Developing Real-Time OLAP algorithm using Multi-core
  processing and GPU: Heterogeneous Computing</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The overwhelmingly increasing amount of stored data has spurred researchers
seeking different methods in order to optimally take advantage of it which
mostly have faced a response time problem as a result of this enormous size of
data. Most of solutions have suggested materialization as a favourite solution.
However, such a solution cannot attain Real- Time answers anyhow. In this paper
we propose a framework illustrating the barriers and suggested solutions in the
way of achieving Real-Time OLAP answers that are significantly used in decision
support systems and data warehouses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3782</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3782</id><created>2014-02-16</created><authors><author><keyname>Angel</keyname><forenames>Eric</forenames></author><author><keyname>Bampis</keyname><forenames>Evripidis</forenames></author><author><keyname>Chau</keyname><forenames>Vincent</forenames></author><author><keyname>Thang</keyname><forenames>Nguyen Kim</forenames></author></authors><title>Throughput Maximization in Multiprocessor Speed-Scaling</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are given a set of $n$ jobs that have to be executed on a set of $m$
speed-scalable machines that can vary their speeds dynamically using the energy
model introduced in [Yao et al., FOCS'95]. Every job $j$ is characterized by
its release date $r_j$, its deadline $d_j$, its processing volume $p_{i,j}$ if
$j$ is executed on machine $i$ and its weight $w_j$. We are also given a budget
of energy $E$ and our objective is to maximize the weighted throughput, i.e.
the total weight of jobs that are completed between their respective release
dates and deadlines. We propose a polynomial-time approximation algorithm where
the preemption of the jobs is allowed but not their migration. Our algorithm
uses a primal-dual approach on a linearized version of a convex program with
linear constraints. Furthermore, we present two optimal algorithms for the
non-preemptive case where the number of machines is bounded by a fixed
constant. More specifically, we consider: {\em (a)} the case of identical
processing volumes, i.e. $p_{i,j}=p$ for every $i$ and $j$, for which we
present a polynomial-time algorithm for the unweighted version, which becomes a
pseudopolynomial-time algorithm for the weighted throughput version, and {\em
(b)} the case of agreeable instances, i.e. for which $r_i \le r_j$ if and only
if $d_i \le d_j$, for which we present a pseudopolynomial-time algorithm. Both
algorithms are based on a discretization of the problem and the use of dynamic
programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3783</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3783</id><created>2014-02-16</created><authors><author><keyname>Montorsi</keyname><forenames>Francesco</forenames></author><author><keyname>Pancaldi</keyname><forenames>Fabrizio</forenames></author><author><keyname>Vitetta</keyname><forenames>Giorgio M.</forenames></author></authors><title>Map-Aware Models for Indoor Wireless Localization Systems: An
  Experimental Study</title><categories>cs.IT math.IT stat.AP</categories><comments>13 pages, 11 figures, 1 table. IEEE Transactions on Wireless
  Communications, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The accuracy of indoor wireless localization systems can be substantially
enhanced by map-awareness, i.e., by the knowledge of the map of the environment
in which localization signals are acquired. In fact, this knowledge can be
exploited to cancel out, at least to some extent, the signal degradation due to
propagation through physical obstructions, i.e., to the so called
non-line-of-sight bias. This result can be achieved by developing novel
localization techniques that rely on proper map-aware statistical modelling of
the measurements they process. In this manuscript a unified statistical model
for the measurements acquired in map-aware localization systems based on
time-of-arrival and received signal strength techniques is developed and its
experimental validation is illustrated. Finally, the accuracy of the proposed
map-aware model is assessed and compared with that offered by its map-unaware
counterparts. Our numerical results show that, when the quality of acquired
measurements is poor, map-aware modelling can enhance localization accuracy by
up to 110% in certain scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3788</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3788</id><created>2014-02-16</created><authors><author><keyname>Litvinenko</keyname><forenames>Natalya</forenames></author></authors><title>Using of GPUs for cluster analysis of large data by K-means method</title><categories>cs.DC</categories><msc-class>68W10</msc-class><acm-class>B.2.4; C.1.2; C.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This problem was solved within the framework of the grant project &quot;Solving of
problems of cluster analysis with application of parallel algorithms and cloud
technologies&quot; in the Institute of Mathematics and Mathematical Modelling in
Almaty. The problem of cluster analysis for the large amount of data is very
important in different areas of science - genetics, biology, sociology etc. At
the same time, such statistical known packages as STATISTICA, STADIA, SYSTAT
and others do not allow to solve large problems. The new algorithm that uses
the high processing power of GPUs for solving clustering problems by the
K-means method was developed. This algorithm is implemented as a C++
application in Microsoft Visual Studio 2010 with using the GPU Nvidia GeForce
660. The developed software package for solving clustering problems by the
method of K - means with using GPUs allows us to handle up to 2 million records
with number of features up to 25. The gain in the computing time is in factor
5. We plan to increase this factor up to 20-30 after improving the algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3789</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3789</id><created>2014-02-16</created><authors><author><keyname>Litvinenko</keyname><forenames>Natalya</forenames></author></authors><title>Parallel algorithms for problems of cluster analysis with very large
  amount of data</title><categories>cs.DC</categories><msc-class>91C20, 68W10, 62-07</msc-class><acm-class>D.1.3; G.1.0; G.4; H.3.3; I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we solve on GPUs massive problems with large amount of data,
which are not appropriate for solution with the SIMD technology. For the given
problem we consider a three-level parallelization. The multithreading of CPU is
used at the top level and graphic processors for massive computing. For solving
problems of cluster analysis on GPUs the nearest neighbor method (NNM) is
developed. This algorithm allows us to handle up to 2 millions records with
number of features up to 25. Since sequential and parallel algorithms are
fundamentally different, it is difficult to compare the computation times.
However, some comparisons are made. The gain in the computing time is about 10
times. We plan to increase this factor up to 50-100 after fine tuning of
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3796</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3796</id><created>2014-02-16</created><updated>2014-11-11</updated><authors><author><keyname>Even</keyname><forenames>Guy</forenames></author><author><keyname>Medina</keyname><forenames>Moti</forenames></author><author><keyname>Ron</keyname><forenames>Dana</forenames></author></authors><title>Best of Two Local Models: Local Centralized and Local Distributed
  Algorithms</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two models of computation: centralized local algorithms and local
distributed algorithms. Algorithms in one model are adapted to the other model
to obtain improved algorithms.
  Distributed vertex coloring is employed to design improved centralized local
algorithms for: maximal independent set, maximal matching, and an approximation
scheme for maximum (weighted) matching over bounded degree graphs. The
improvement is threefold: the algorithms are deterministic, stateless, and the
number of probes grows polynomially in $\log^* n$, where $n$ is the number of
vertices of the input graph.
  The recursive centralized local improvement technique by Nguyen and
Onak~\cite{onak2008} is employed to obtain an improved distributed
approximation scheme for maximum (weighted) matching. The improvement is
twofold: we reduce the number of rounds from $O(\log n)$ to $O(\log^*n)$ for a
wide range of instances and, our algorithms are deterministic rather than
randomized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3797</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3797</id><created>2014-02-16</created><updated>2015-01-20</updated><authors><author><keyname>Gupte</keyname><forenames>Pratik Vinay</forenames></author><author><keyname>Ravindran</keyname><forenames>Balaraman</forenames></author></authors><title>Scalable Positional Analysis for Studying Evolution of Nodes in Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Presented at the workshop on Mining Networks and Graphs: A Big Data
  Analytic Challenge, held in conjunction with the SIAM Data Mining (SDM)
  Conference in April 2014. 13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In social network analysis, the fundamental idea behind the notion of
position is to discover actors who have similar structural signatures.
Positional analysis of social networks involves partitioning the actors into
disjoint sets using a notion of equivalence which captures the structure of
relationships among actors. Classical approaches to Positional Analysis, such
as Regular equivalence and Equitable Partitions, are too strict in grouping
actors and often lead to trivial partitioning of actors in real world networks.
An Epsilon Equitable Partition (EEP) of a graph, which is similar in spirit to
Stochastic Blockmodels, is a useful relaxation to the notion of structural
equivalence which results in meaningful partitioning of actors. In this paper
we propose and implement a new scalable distributed algorithm based on
MapReduce methodology to find EEP of a graph. Empirical studies on random
power-law graphs show that our algorithm is highly scalable for sparse graphs,
thereby giving us the ability to study positional analysis on very large scale
networks. We also present the results of our algorithm on time evolving
snapshots of the facebook and flickr social graphs. Results show the importance
of positional analysis on large dynamic networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3801</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3801</id><created>2014-02-16</created><authors><author><keyname>Benerjee</keyname><forenames>Krishna Gopal</forenames></author><author><keyname>Gupta</keyname><forenames>Manish Kumar</forenames></author></authors><title>On Heterogeneous Regenerating Codes and Capacity of Distributed Storage
  Systems</title><categories>cs.IT math.IT</categories><comments>submitted to conference Netcod 2014, 3 figures, 5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous Distributed Storage Systems (DSS) are close to real world
applications for data storage. Internet caching system and peer-to-peer storage
clouds are the examples of such DSS. In this work, we calculate the capacity
formula for such systems where each node store different number of packets and
each having a different repair bandwidth (node can be repaired by contacting a
specific set of nodes). The tradeoff curve between storage and repair bandwidth
is studied for such heterogeneous DSS. By analyzing the capacity formula new
minimum bandwidth regenerating (MBR) and minimum storage regenerating (MBR)
points are obtained on the curve. It is shown that in some cases these are
better than the homogeneous DSS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3809</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3809</id><created>2014-02-16</created><updated>2014-03-13</updated><authors><author><keyname>Heroux</keyname><forenames>Michael A.</forenames></author></authors><title>Toward Resilient Algorithms and Applications</title><categories>cs.MS cs.DC</categories><acm-class>C.4; D.1.3; D.4.5; G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade, the high performance computing community has become
increasingly concerned that preserving the reliable, digital machine model will
become too costly or infeasible. In this paper we discuss four approaches for
developing new algorithms that are resilient to hard and soft failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3811</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3811</id><created>2014-02-16</created><updated>2014-07-02</updated><authors><author><keyname>Gao</keyname><forenames>Wei</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Dropout Rademacher Complexity of Deep Neural Networks</title><categories>cs.NE stat.ML</categories><comments>20 pagea</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Great successes of deep neural networks have been witnessed in various real
applications. Many algorithmic and implementation techniques have been
developed, however, theoretical understanding of many aspects of deep neural
networks is far from clear. A particular interesting issue is the usefulness of
dropout, which was motivated from the intuition of preventing complex
co-adaptation of feature detectors. In this paper, we study the Rademacher
complexity of different types of dropout, and our theoretical results disclose
that for shallow neural networks (with one or none hidden layer) dropout is
able to reduce the Rademacher complexity in polynomial, whereas for deep neural
networks it can amazingly lead to an exponential reduction of the Rademacher
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3821</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3821</id><created>2014-02-16</created><authors><author><keyname>Cornebize</keyname><forenames>Tom</forenames></author><author><keyname>Falcone</keyname><forenames>Yli&#xe8;s</forenames></author></authors><title>Efficient and Generalized Decentralized Monitoring of Regular Languages</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main contribution of this paper is an efficient and generalized
decentralized monitoring algorithm allowing to detect satisfaction or violation
of any regular specification by local monitors alone in a system without
central observation point. Our algorithm does not assume any form of
synchronization between system events and communication of monitors, uses state
machines as underlying mechanism for efficiency, and tries to keep the number
and size of messages exchanged between monitors to a minimum. We provide a full
implementation of the algorithm with an open-source benchmark to evaluate its
efficiency in terms of number, size of exchanged messages, and delay induced by
communication between monitors. Experimental results demonstrate the
effectiveness of our algorithm which outperforms the previous most general one
along several (new) monitoring metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3835</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3835</id><created>2014-02-16</created><authors><author><keyname>Canonne</keyname><forenames>Cl&#xe9;ment</forenames></author><author><keyname>Rubinfeld</keyname><forenames>Ronitt</forenames></author></authors><title>Testing probability distributions underlying aggregated data</title><categories>cs.DS cs.DM math.PR math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze and study a hybrid model for testing and learning
probability distributions. Here, in addition to samples, the testing algorithm
is provided with one of two different types of oracles to the unknown
distribution $D$ over $[n]$. More precisely, we define both the dual and
cumulative dual access models, in which the algorithm $A$ can both sample from
$D$ and respectively, for any $i\in[n]$,
  - query the probability mass $D(i)$ (query access); or
  - get the total mass of $\{1,\dots,i\}$, i.e. $\sum_{j=1}^i D(j)$ (cumulative
access)
  These two models, by generalizing the previously studied sampling and query
oracle models, allow us to bypass the strong lower bounds established for a
number of problems in these settings, while capturing several interesting
aspects of these problems -- and providing new insight on the limitations of
the models. Finally, we show that while the testing algorithms can be in most
cases strictly more efficient, some tasks remain hard even with this additional
power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3847</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3847</id><created>2014-02-16</created><authors><author><keyname>Bosco</keyname><forenames>Claudio</forenames></author><author><keyname>de Rigo</keyname><forenames>Daniele</forenames></author><author><keyname>Dewitte</keyname><forenames>Olivier</forenames></author><author><keyname>Montanarella</keyname><forenames>Luca</forenames></author></authors><title>Towards the reproducibility in soil erosion modeling: a new Pan-European
  soil erosion map</title><categories>cs.SY cs.CE physics.geo-ph</categories><comments>9 pages, from a poster presented at the Wageningen Conference on
  Applied Soil Science &quot;Soil Science in a Changing World&quot;, 18 - 22 September
  2011, Wageningen, The Netherlands</comments><doi>10.6084/m9.figshare.936872</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Soil erosion by water is a widespread phenomenon throughout Europe and has
the potentiality, with his on-site and off-site effects, to affect water
quality, food security and floods. Despite the implementation of numerous and
different models for estimating soil erosion by water in Europe, there is still
a lack of harmonization of assessment methodologies.
  Often, different approaches result in soil erosion rates significantly
different. Even when the same model is applied to the same region the results
may differ. This can be due to the way the model is implemented (i.e. with the
selection of different algorithms when available) and/or to the use of datasets
having different resolution or accuracy. Scientific computation is emerging as
one of the central topic of the scientific method, for overcoming these
problems there is thus the necessity to develop reproducible computational
method where codes and data are available.
  The present study illustrates this approach. Using only public available
datasets, we applied the Revised Universal Soil loss Equation (RUSLE) to locate
the most sensitive areas to soil erosion by water in Europe.
  A significant effort was made for selecting the better simplified equations
to be used when a strict application of the RUSLE model is not possible. In
particular for the computation of the Rainfall Erosivity factor (R) the
reproducible research paradigm was applied. The calculation of the R factor was
implemented using public datasets and the GNU R language. An easily
reproducible validation procedure based on measured precipitation time series
was applied using MATLAB language. Designing the computational modelling
architecture with the aim to ease as much as possible the future reuse of the
model in analysing climate change scenarios is also a challenging goal of the
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3849</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3849</id><created>2014-02-16</created><authors><author><keyname>Chitta</keyname><forenames>Radha</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Havens</keyname><forenames>Timothy C.</forenames></author><author><keyname>Jain</keyname><forenames>Anil K.</forenames></author></authors><title>Scalable Kernel Clustering: Approximate Kernel k-means</title><categories>cs.CV cs.DS cs.LG</categories><comments>15 pages, 6 figures,extension of the work &quot;Approximate Kernel
  k-means: Solution to large scale kernel clustering&quot; published in KDD 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel-based clustering algorithms have the ability to capture the non-linear
structure in real world data. Among various kernel-based clustering algorithms,
kernel k-means has gained popularity due to its simple iterative nature and
ease of implementation. However, its run-time complexity and memory footprint
increase quadratically in terms of the size of the data set, and hence, large
data sets cannot be clustered efficiently. In this paper, we propose an
approximation scheme based on randomization, called the Approximate Kernel
k-means. We approximate the cluster centers using the kernel similarity between
a few sampled points and all the points in the data set. We show that the
proposed method achieves better clustering performance than the traditional low
rank kernel approximation based clustering schemes. We also demonstrate that
its running time and memory requirements are significantly lower than those of
kernel k-means, with only a small reduction in the clustering quality on
several public domain large data sets. We then employ ensemble clustering
techniques to further enhance the performance of our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3851</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3851</id><created>2014-02-16</created><updated>2014-04-17</updated><authors><author><keyname>Koutis</keyname><forenames>Ioannis</forenames></author></authors><title>Simple parallel and distributed algorithms for spectral graph
  sparsification</title><categories>cs.DS</categories><comments>replaces &quot;A simple parallel and distributed algorithm for spectral
  sparsification&quot;. Minor changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a simple algorithm for spectral graph sparsification, based on
iterative computations of weighted spanners and uniform sampling. Leveraging
the algorithms of Baswana and Sen for computing spanners, we obtain the first
distributed spectral sparsification algorithm. We also obtain a parallel
algorithm with improved work and time guarantees. Combining this algorithm with
the parallel framework of Peng and Spielman for solving symmetric diagonally
dominant linear systems, we get a parallel solver which is much closer to being
practical and significantly more efficient in terms of the total work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3856</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3856</id><created>2014-02-16</created><authors><author><keyname>Hyde</keyname><forenames>Kayleigh</forenames></author><author><keyname>Kjos-Hanssen</keyname><forenames>Bj&#xf8;rn</forenames></author></authors><title>Nondeterministic automatic complexity of almost square-free and strongly
  cube-free words</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shallit and Wang studied deterministic automatic complexity of words. They
showed that the automatic Hausdorff dimension $I(\mathbf t)$ of the infinite
Thue word satisfies $1/3\le I(\mathbf t)\le 2/3$. We improve that result by
showing that $I(\mathbf t)\ge 1/2$. For nondeterministic automatic complexity
we show $I(\mathbf t)=1/2$. We prove that such complexity $A_N$ of a word $x$
of length $n$ satisfies $A_N(x)\le b(n):=\lfloor n/2\rfloor + 1$. This enables
us to define the complexity deficiency $D(x)=b(n)-A_N(x)$. If $x$ is
square-free then $D(x)=0$. If $x$ almost square-free in the sense of Fraenkel
and Simpson, or if $x$ is a strongly cube-free binary word such as the infinite
Thue word, then $D(x)\le 1$. On the other hand, there is no constant upper
bound on $D$ for strongly cube-free words in a ternary alphabet, nor for
cube-free words in a binary alphabet. The decision problem whether $D(x)\ge d$
for given $x$, $d$ belongs to $NP\cap E$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3869</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3869</id><created>2014-02-16</created><updated>2014-05-15</updated><authors><author><keyname>Wang</keyname><forenames>Yilun</forenames></author></authors><title>FTVd is beyond Fast Total Variation regularized Deconvolution</title><categories>cs.CV</categories><acm-class>G.1.6; G.4; I.4.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revisit the &quot;FTVd&quot; algorithm for Fast Total Variation
Regularized Deconvolution, which has been widely used in the past few years.
Both its original version implemented in the MATLAB software FTVd 3.0 and its
related variant implemented in the latter version FTVd 4.0 are considered
\cite{Wang08FTVdsoftware}. We propose that the intermediate results during the
iterations are the solutions of a series of combined Tikhonov and total
variation regularized image deconvolution models and therefore some of them
often have even better image quality than the final solution, which is
corresponding to the pure total variation regularized model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3873</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3873</id><created>2014-02-16</created><updated>2014-12-24</updated><authors><author><keyname>He</keyname><forenames>Peng</forenames></author><author><keyname>Li</keyname><forenames>Bing</forenames></author><author><keyname>Liu</keyname><forenames>Xiao</forenames></author><author><keyname>Chen</keyname><forenames>Jun</forenames></author><author><keyname>Ma</keyname><forenames>Yutao</forenames></author></authors><title>An Empirical Study on Software Defect Prediction with a Simplified
  Metric Set</title><categories>cs.SE</categories><comments>24 pages, 9 figures, 13 tables</comments><msc-class>68N30</msc-class><acm-class>D.2.8; D.2.9</acm-class><journal-ref>Information and Software Technology, 59(3): 170-190, 2015</journal-ref><doi>10.1016/j.infsof.2014.11.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software defect prediction plays a crucial role in estimating the most
defect-prone components of software, and a large number of studies have pursued
improving prediction accuracy within a project or across projects. However, the
rules for making an appropriate decision between within- and cross-project
defect prediction when available historical data are insufficient remain
unclear. The objective of this work is to validate the feasibility of the
predictor built with a simplified metric set for software defect prediction in
different scenarios, and to investigate practical guidelines for the choice of
training data, classifier and metric subset of a given project. First, based on
six typical classifiers, we constructed three types of predictors using the
size of software metric set in three scenarios. Then, we validated the
acceptable performance of the predictor based on Top-k metrics in terms of
statistical methods. Finally, we attempted to minimize the Top-k metric subset
by removing redundant metrics, and we tested the stability of such a minimum
metric subset with one-way ANOVA tests. The experimental results indicate that
(1) the choice of training data should depend on the specific requirement of
prediction accuracy; (2) the predictor built with a simplified metric set works
well and is very useful in case limited resources are supplied; (3) simple
classifiers (e.g., Naive Bayes) also tend to perform well when using a
simplified metric set for defect prediction; and (4) in several cases, the
minimum metric subset can be identified to facilitate the procedure of general
defect prediction with acceptable loss of prediction precision in practice. The
guideline for choosing a suitable simplified metric set in different scenarios
is presented in Table 12.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3876</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3876</id><created>2014-02-16</created><updated>2014-05-04</updated><authors><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Pettersson</keyname><forenames>William</forenames></author></authors><title>Fixed parameter tractable algorithms in combinatorial topology</title><categories>math.GT cs.CG</categories><comments>16 pages, 9 figures</comments><msc-class>57N10 (Primary) 57Q15, 68W05 (Secondary)</msc-class><journal-ref>Lecture Notes in Computer Science, vol. 8591, 2014, pp. 300-311</journal-ref><doi>10.1007/978-3-319-08783-2_26</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To enumerate 3-manifold triangulations with a given property, one typically
begins with a set of potential face pairing graphs (also known as dual
1-skeletons), and then attempts to flesh each graph out into full
triangulations using an exponential-time enumeration. However, asymptotically
most graphs do not result in any 3-manifold triangulation, which leads to
significant &quot;wasted time&quot; in topological enumeration algorithms. Here we give a
new algorithm to determine whether a given face pairing graph supports any
3-manifold triangulation, and show this to be fixed parameter tractable in the
treewidth of the graph.
  We extend this result to a &quot;meta-theorem&quot; by defining a broad class of
properties of triangulations, each with a corresponding fixed parameter
tractable existence algorithm. We explicitly implement this algorithm in the
most generic setting, and we identify heuristics that in practice are seen to
mitigate the large constants that so often occur in parameterised complexity,
highlighting the practicality of our techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3890</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3890</id><created>2014-02-17</created><authors><author><keyname>Brzezinski</keyname><forenames>Michal</forenames></author></authors><title>Power laws in citation distributions: Evidence from Scopus</title><categories>cs.DL physics.soc-ph stat.AP</categories><comments>18 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling distributions of citations to scientific papers is crucial for
understanding how science develops. However, there is a considerable empirical
controversy on which statistical model fits the citation distributions best.
This paper is concerned with rigorous empirical detection of power-law
behaviour in the distribution of citations received by the most highly cited
scientific papers. We have used a large, novel data set on citations to
scientific papers published between 1998 and 2002 drawn from Scopus. The
power-law model is compared with a number of alternative models using a
likelihood ratio test. We have found that the power-law hypothesis is rejected
for around half of the Scopus fields of science. For these fields of science,
the Yule, power-law with exponential cut-off and log-normal distributions seem
to fit the data better than the pure power-law model. On the other hand, when
the power-law hypothesis is not rejected, it is usually empirically
indistinguishable from most of the alternative models. The pure power-law model
seems to be the best model only for the most highly cited papers in &quot;Physics
and Astronomy&quot;. Overall, our results seem to support theories implying that the
most highly cited scientific papers follow the Yule, power-law with exponential
cut-off or log-normal distribution. Our findings suggest also that power laws
in citation distributions, when present, account only for a very small fraction
of the published papers (less than 1% for most of science fields) and that the
power-law scaling parameter (exponent) is substantially higher (from around 3.2
to around 4.7) than found in the older literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3891</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3891</id><created>2014-02-17</created><authors><author><keyname>RM</keyname><forenames>Vinodhini G Chandrasekaran</forenames></author></authors><title>Performance Evaluation of Machine Learning Classifiers in Sentiment
  Mining</title><categories>cs.LG cs.CL cs.IR</categories><comments>4 pages 2 tables, International Journal of Computer Trends and
  Technology, volume 4, Issue 6, june 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, the use of machine learning classifiers is of great value in
solving a variety of problems in text classification. Sentiment mining is a
kind of text classification in which, messages are classified according to
sentiment orientation such as positive or negative. This paper extends the idea
of evaluating the performance of various classifiers to show their
effectiveness in sentiment mining of online product reviews. The product
reviews are collected from Amazon reviews. To evaluate the performance of
classifiers various evaluation methods like random sampling, linear sampling
and bootstrap sampling are used. Our results shows that support vector machine
with bootstrap sampling method outperforms others classifiers and sampling
methods in terms of misclassification rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3892</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3892</id><created>2014-02-17</created><authors><author><keyname>Othman</keyname><forenames>Nasri Bin</forenames></author><author><keyname>Legara</keyname><forenames>Erika Fille</forenames></author><author><keyname>Selvam</keyname><forenames>Vicknesh</forenames></author><author><keyname>Monterola</keyname><forenames>Christopher</forenames></author></authors><title>Simulating Congestion Dynamics of Train Rapid Transit using Smart Card
  Data</title><categories>cs.MA physics.soc-ph</categories><comments>10 pages, 5 figures, submitted to International Conference on
  Computational Science 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Investigating congestion in train rapid transit systems (RTS) in today's
urban cities is a challenge compounded by limited data availability and
difficulties in model validation. Here, we integrate information from travel
smart card data, a mathematical model of route choice, and a full-scale
agent-based model of the Singapore RTS to provide a more comprehensive
understanding of the congestion dynamics than can be obtained through
analytical modelling alone. Our model is empirically validated, and allows for
close inspection of the dynamics including station crowdedness, average travel
duration, and frequency of missed trains---all highly pertinent factors in
service quality. Using current data, the crowdedness in all 121 stations
appears to be distributed log-normally. In our preliminary scenarios, we
investigate the effect of population growth on service quality. We find that
the current population (2 million) lies below a critical point; and increasing
it beyond a factor of $\sim10\%$ leads to an exponential deterioration in
service quality. We also predict that incentivizing commuters to avoid the most
congested hours can bring modest improvements to the service quality provided
the population remains under the critical point. Finally, our model can be used
to generate simulated data for analytical modelling when such data are not
empirically available, as is often the case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3895</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3895</id><created>2014-02-17</created><authors><author><keyname>Shanmugam</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author></authors><title>Bounding Multiple Unicasts through Index Coding and Locally Repairable
  Codes</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures.Submitted to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a duality result between linear index coding and Locally
Repairable Codes (LRCs). Specifically, we show that a natural extension of LRCs
we call Generalized Locally Repairable Codes (GLCRs) are exactly dual to linear
index codes. In a GLRC, every node is decodable from a specific set of other
nodes and these sets induce a recoverability directed graph. We show that the
dual linear subspace of a GLRC is a solution to an index coding instance where
the side information graph is this GLRC recoverability graph. We show that the
GLRC rate is equivalent to the complementary index coding rate, i.e. the number
of transmissions saved by coding. Our second result uses this duality to
establish a new upper bound for the multiple unicast network coding problem. In
multiple unicast network coding, we are given a directed acyclic graph and r
sources that want to send independent messages to r corresponding destinations.
Our new upper bound is efficiently computable and relies on a strong
approximation result for complementary index coding. We believe that our bound
could lead to a logarithmic approximation factor for multiple unicast network
coding if a plausible connection we state is verified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3898</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3898</id><created>2014-02-17</created><authors><author><keyname>Shanmugam</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author></authors><title>Graph Theory versus Minimum Rank for Index Coding</title><categories>cs.IT math.IT</categories><comments>8 pages, 2 figures. Submitted to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We obtain novel index coding schemes and show that they provably outperform
all previously known graph theoretic bounds proposed so far. Further, we
establish a rather strong negative result: all known graph theoretic bounds are
within a logarithmic factor from the chromatic number. This is in striking
contrast to minrank since prior work has shown that it can outperform the
chromatic number by a polynomial factor in some cases. The conclusion is that
all known graph theoretic bounds are not much stronger than the chromatic
number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3902</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3902</id><created>2014-02-17</created><updated>2014-11-06</updated><authors><author><keyname>Kocaoglu</keyname><forenames>Murat</forenames></author><author><keyname>Shanmugam</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Klivans</keyname><forenames>Adam</forenames></author></authors><title>Sparse Polynomial Learning and Graph Sketching</title><categories>cs.LG</categories><comments>14 pages; to appear in NIPS 2014l Updated proof of Theorem 5 and some
  other minor changes during revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $f:\{-1,1\}^n$ be a polynomial with at most $s$ non-zero real
coefficients. We give an algorithm for exactly reconstructing f given random
examples from the uniform distribution on $\{-1,1\}^n$ that runs in time
polynomial in $n$ and $2s$ and succeeds if the function satisfies the unique
sign property: there is one output value which corresponds to a unique set of
values of the participating parities. This sufficient condition is satisfied
when every coefficient of f is perturbed by a small random noise, or satisfied
with high probability when s parity functions are chosen randomly or when all
the coefficients are positive. Learning sparse polynomials over the Boolean
domain in time polynomial in $n$ and $2s$ is considered notoriously hard in the
worst-case. Our result shows that the problem is tractable for almost all
sparse polynomials. Then, we show an application of this result to hypergraph
sketching which is the problem of learning a sparse (both in the number of
hyperedges and the size of the hyperedges) hypergraph from uniformly drawn
random cuts. We also provide experimental results on a real world dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3909</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3909</id><created>2014-02-17</created><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Panolan</keyname><forenames>Fahad</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Representative Sets of Product Families</title><categories>cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1304.4626</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A subfamily ${\cal F}'$ of a set family ${\cal F}$ is said to $q$-{\em
represent} ${\cal F}$ if for every $A \in {\cal F}$ and $B$ of size $q$ such
that $A \cap B = \emptyset$ there exists a set $A' \in {\cal F}'$ such that $A'
\cap B = \emptyset$. In this paper, we consider the efficient computation of
$q$-representative sets for {\em product} families ${\cal F}$. A family ${\cal
F}$ is a product family if there exist families ${\cal A}$ and ${\cal B}$ such
that ${\cal F} = \{A \cup B~:~A \in {\cal A}, B \in {\cal B}, A \cap B =
\emptyset\}$. Our main technical contribution is an algorithm which given
${\cal A}$, ${\cal B}$ and $q$ computes a $q$-representative family ${\cal F}'$
of ${\cal F}$. The running time of our algorithm is sublinear in $|{\cal F}|$
for many choices of ${\cal A}$, ${\cal B}$ and $q$ which occur naturally in
several dynamic programming algorithms. We also give an algorithm for the
computation of $q$-representative sets for product families ${\cal F}$ in the
more general setting where $q$-representation also involves independence in a
matroid in addition to disjointness. This algorithm considerably outperforms
the naive approach where one first computes ${\cal F}$ from ${\cal A}$ and
${\cal B}$, and then computes the $q$-representative family ${\cal F}'$ from
${\cal F}$.
  We give two applications of our new algorithms for computing
$q$-representative sets for product families. The first is a
$3.8408^{k}n^{O(1)}$ deterministic algorithm for the Multilinear Monomial
Detection ($k$-MlD) problem. The second is a significant improvement of
deterministic dynamic programming algorithms for &quot;connectivity problems&quot; on
graphs of bounded treewidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3913</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3913</id><created>2014-02-17</created><authors><author><keyname>Qian</keyname><forenames>Li</forenames></author><author><keyname>Liu</keyname><forenames>Lizhen</forenames></author><author><keyname>Hu</keyname><forenames>Yue</forenames></author></authors><title>Analysis of Personalized Information Service System for Digital
  Libraries</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Along with the rapid development of digital library, digital library of the
third generation, taking the main characteristics of the personalized service,
has already become the mainstream today. This article first analyzes the
concept and characteristics of digital library, then elaborates the
inevitability of personalized service as well as the necessity of its
development and based on the Propose of establishing interested knowledge-base,
enumerates several ways of personalized service, at last according to
individualized service, this paper proposes the model of &quot;learningoriented
individual digital library&quot; and prospects the trends of digital library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3920</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3920</id><created>2014-02-17</created><updated>2014-06-23</updated><authors><author><keyname>Thramboulidis</keyname><forenames>Kleanthis</forenames></author></authors><title>A Framework for the Implementation of Industrial Automation Systems
  Based on PLCs</title><categories>cs.SE</categories><comments>13 pages, 9 figures Corrected typos. Corrections to Fig. 5a and 9.
  results unchanged</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Industrial automation systems (IASs) are traditionally developed using a
sequential approach where the automation software, which is commonly based on
the IEC 61131 languages, is developed when the design and in many cases the
implementation of mechanical parts have been completed. However, it is claimed
that this approach does not lead to the optimal system design and that the IEC
61131 does not meet new challenges in this domain. In this paper, a system
engineering process based on the new version of IEC 61131, which supports
Object Orientation, is presented. SysML and UML are utilized to introduce a
higher layer of abstraction in the design space of IAS and Internet of Things
(IoT) is considered as an enabling technology for the integration of Cyber and
Cyber-physical components of the system, bringing into the industrial
automation domain the benefits of these technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3926</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3926</id><created>2014-02-17</created><authors><author><keyname>Kato</keyname><forenames>Toshiyuki</forenames></author><author><keyname>Hino</keyname><forenames>Hideitsu</forenames></author><author><keyname>Murata</keyname><forenames>Noboru</forenames></author></authors><title>Sparse Coding Approach for Multi-Frame Image Super Resolution</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An image super-resolution method from multiple observation of low-resolution
images is proposed. The method is based on sub-pixel accuracy block matching
for estimating relative displacements of observed images, and sparse signal
representation for estimating the corresponding high-resolution image. Relative
displacements of small patches of observed low-resolution images are accurately
estimated by a computationally efficient block matching method. Since the
estimated displacements are also regarded as a warping component of image
degradation process, the matching results are directly utilized to generate
low-resolution dictionary for sparse image representation. The matching scores
of the block matching are used to select a subset of low-resolution patches for
reconstructing a high-resolution patch, that is, an adaptive selection of
informative low-resolution images is realized. When there is only one
low-resolution image, the proposed method works as a single-frame
super-resolution method. The proposed method is shown to perform comparable or
superior to conventional single- and multi-frame super-resolution methods
through experiments using various real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3928</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3928</id><created>2014-02-17</created><authors><author><keyname>Adimoolam</keyname><forenames>Santosh Arvind</forenames></author></authors><title>Parametrization of completeness in symbolic abstraction of bounded input
  linear systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A good state-time quantized symbolic abstraction of an already input
quantized control system would satisfy three conditions: proximity, soundness
and completeness. Extant approaches for symbolic abstraction of unstable
systems limit to satisfying proximity and soundness but not completeness.
Instability of systems is an impediment to constructing fully complete
state-time quantized symbolic models for bounded and quantized input unstable
systems, even using supervisory feedback. Therefore, in this paper we come up
with a way of parametrization of completeness of the symbolic model through the
quintessential notion of Trimmed-Input Approximate Bisimulation which is
introduced in the paper. The amount of completeness is specified by a parameter
called trimming of the set of input trajectories. We subsequently discuss a
procedure of constructing state-time quantized symbolic models which are
near-complete in addition to being sound and proximate with respect to the time
quantized models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3937</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3937</id><created>2014-02-17</created><authors><author><keyname>Haboush</keyname><forenames>Ahmad Khader</forenames></author></authors><title>A Hybrid Modified Semantic Matching Algorithm Based on Instances
  Detection With Case Study on Renewable Energy</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This Matching input keywords with historical or information domain is an
important point in modern computations in order to find the best match
information domain for specific input queries. Matching algorithms represents
hot area of researches in computer science and artificial intelligence. In the
area of text matching, it is more reliable to study semantics of the pattern
and query in terms of semantic matching. This paper improves the semantic
matching results between input queries and information ontology domain. The
contributed algorithm is a hybrid technique that is based on matching extracted
instances from booth, the queries and in information domain. The instances
extraction algorithm that is presented in this paper are contributed which is
based on mathematical and statistical analysis of objects with respect to each
other and also with respect to marked objects. The instances that are instances
from the queries and information domain are subjected to semantic matching to
find the best match, match percentage, and to improve the decision making
process. An application case was studied in this paper which is related to
renewable energy, where the input queries represents the customer requirements
input and the knowledge domain is renewable energy vendors profiles. The
comparison was made with most known recent matching researches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3939</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3939</id><created>2014-02-17</created><authors><author><keyname>Cheng</keyname><forenames>Suqi</forenames></author><author><keyname>Shen</keyname><forenames>Hua-Wei</forenames></author><author><keyname>Huang</keyname><forenames>Junming</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Cheng</keyname><forenames>Xue-Qi</forenames></author></authors><title>IMRank: Influence Maximization via Finding Self-Consistent Ranking</title><categories>cs.SI cs.DS</categories><comments>10 pages, 8 figures, this paper has been submitted to SIGIR2014</comments><acm-class>F.2.2; D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence maximization, fundamental for word-of-mouth marketing and viral
marketing, aims to find a set of seed nodes maximizing influence spread on
social network. Early methods mainly fall into two paradigms with certain
benefits and drawbacks: (1)Greedy algorithms, selecting seed nodes one by one,
give a guaranteed accuracy relying on the accurate approximation of influence
spread with high computational cost; (2)Heuristic algorithms, estimating
influence spread using efficient heuristics, have low computational cost but
unstable accuracy.
  We first point out that greedy algorithms are essentially finding a
self-consistent ranking, where nodes' ranks are consistent with their
ranking-based marginal influence spread. This insight motivates us to develop
an iterative ranking framework, i.e., IMRank, to efficiently solve influence
maximization problem under independent cascade model. Starting from an initial
ranking, e.g., one obtained from efficient heuristic algorithm, IMRank finds a
self-consistent ranking by reordering nodes iteratively in terms of their
ranking-based marginal influence spread computed according to current ranking.
We also prove that IMRank definitely converges to a self-consistent ranking
starting from any initial ranking. Furthermore, within this framework, a
last-to-first allocating strategy and a generalization of this strategy are
proposed to improve the efficiency of estimating ranking-based marginal
influence spread for a given ranking. In this way, IMRank achieves both
remarkable efficiency and high accuracy by leveraging simultaneously the
benefits of greedy algorithms and heuristic algorithms. As demonstrated by
extensive experiments on large scale real-world social networks, IMRank always
achieves high accuracy comparable to greedy algorithms, with computational cost
reduced dramatically, even about $10-100$ times faster than other scalable
heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3941</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3941</id><created>2014-02-17</created><updated>2014-04-25</updated><authors><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author><author><keyname>Martinez</keyname><forenames>Alfonso</forenames></author><author><keyname>F&#xe0;bregas</keyname><forenames>Albert Guill&#xe9;n i</forenames></author></authors><title>The Saddlepoint Approximation: Unified Random Coding Asymptotics for
  Fixed and Varying Rates</title><categories>cs.IT math.IT</categories><comments>Accepted to ISIT 2014, presented without publication at ITA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a saddlepoint approximation of the random-coding union
bound of Polyanskiy et al. for i.i.d. random coding over discrete memoryless
channels. The approximation is single-letter, and can thus be computed
efficiently. Moreover, it is shown to be asymptotically tight for both fixed
and varying rates, unifying existing achievability results in the regimes of
error exponents, second-order coding rates, and moderate deviations. For fixed
rates, novel exact-asymptotics expressions are specified to within a
multiplicative 1+o(1) term. A numerical example is provided for which the
approximation is remarkably accurate even at short block lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3942</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3942</id><created>2014-02-17</created><authors><author><keyname>Pakprod</keyname><forenames>Nuttakan</forenames></author><author><keyname>Wannapiroon</keyname><forenames>Panita</forenames></author></authors><title>Development of Interactive Instructional Model Using Augmented Reality
  based on Edutainment to Enhance Emotional Quotient</title><categories>cs.CY</categories><comments>8 pages</comments><journal-ref>IJITE: Results Vol.2, No.4, December 2013 (43-50)</journal-ref><doi>10.5121/ijite.2013.2405</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The research aims to develop an interactive instructional model using
augmented reality based on edutainment to enhance emotional quotient and
evaluate the model. Two phases of the research will be carried out: a
development and an evaluation of the model. Samples are experts in the field of
IT, child psychology, and 7th grade curriculum management. Ten experts are
selected by purposive sampling method. The obtained data are analyzed using
mean and standard deviation. The research result demonstrates the following
findings: 1) The results of this research show that Model consists of 3
elements: IIAR, EduLA, and EQ. EQ is a means to assess EQ based on Time Series
Experimental Design using 2 kinds of tools; i.e. EQ Assessment by programs in
tablet computers, and EQ Assessment by behavioral observation. 2) The ten
experts have evaluated the model and commented that the developed model showed
high suitability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3944</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3944</id><created>2014-02-17</created><authors><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Heidrich</keyname><forenames>Jens</forenames></author></authors><title>Context-driven Software Project Estimation</title><categories>cs.SE</categories><comments>2 pages</comments><journal-ref>Proceedings of the 2nd ACM-IEEE International Symposium on
  Empirical Software Engineering (ISESE 2003), volume 2, pages 15-16, Rome,
  Italy, September 20 - October 1 2003</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using quantitative data from past projects for software project estimation
requires context knowledge that characterizes its origin and indicates its
applicability for future use. This article sketches the SPRINT I technique for
project planning and controlling. The underlying prediction mechanism is based
on the identification of similar past projects and the building of so-called
clusters with typical data curves. The article focuses on how to characterize
these clusters with context knowledge and how to use context information from
actual projects for prediction. The SPRINT approach is tool-supported and first
evaluations have been conducted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3951</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3951</id><created>2014-02-17</created><updated>2014-03-05</updated><authors><author><keyname>van der Ham</keyname><forenames>Jeroen</forenames></author><author><keyname>Ghijsen</keyname><forenames>Mattijs</forenames></author><author><keyname>Grosso</keyname><forenames>Paola</forenames></author><author><keyname>de Laat</keyname><forenames>Cees</forenames></author></authors><title>Trends in Computer Network Modeling Towards the Future Internet</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This article provides a taxonomy of current and past network modeling
efforts. In all these efforts over the last few years we see a trend towards
not only describing the network, but connected devices as well. This is
especially current given the many Future Internet projects, which are combining
different models, and resources in order to provide complete virtual
infrastructures to users.
  An important mechanism for managing complexity is the creation of an abstract
model, a step which has been undertaken in computer networks too. The fact that
more and more devices are network capable, coupled with increasing popularity
of the Internet, has made computer networks an important focus area for
modeling. The large number of connected devices creates an increasing
complexity which must be harnessed to keep the networks functioning.
  Over the years many different models for computer networks have been
proposed, and used for different purposes. While for some time the community
has moved away from the need of full topology exchange, this requirement
resurfaced for optical networks. Subsequently, research on topology
descriptions has seen a rise in the last few years. Many different models have
been created and published, yet there is no publication that shows an overview
of the different approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3962</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3962</id><created>2014-02-17</created><updated>2014-05-12</updated><authors><author><keyname>Bruy&#xe8;re</keyname><forenames>V&#xe9;ronique</forenames></author><author><keyname>Meunier</keyname><forenames>No&#xe9;mie</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Secure Equilibria in Weighted Games</title><categories>cs.GT cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two-player non zero-sum infinite duration games played on
weighted graphs. We extend the notion of secure equilibrium introduced by
Chatterjee et al., from the Boolean setting to this quantitative setting. As
for the Boolean setting, our notion of secure equilibrium refines the classical
notion of Nash equilibrium. We prove that secure equilibria always exist in a
large class of weighted games which includes common measures like sup, inf, lim
sup, lim inf, mean-payoff, and discounted sum. Moreover we show that one can
synthesize finite-memory strategy profiles with few memory. We also prove that
the constrained existence problem for secure equilibria is decidable for sup,
inf, lim sup, lim inf and mean-payoff measures. Our solutions rely on new
results for zero-sum quantitative games with lexicographic objectives that are
interesting on their own right.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3973</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3973</id><created>2014-02-17</created><authors><author><keyname>Dirksen</keyname><forenames>Sjoerd</forenames></author></authors><title>Dimensionality reduction with subgaussian matrices: a unified theory</title><categories>cs.IT cs.DS math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a theory for Euclidean dimensionality reduction with subgaussian
matrices which unifies several restricted isometry property and
Johnson-Lindenstrauss type results obtained earlier for specific data sets. In
particular, we recover and, in several cases, improve results for sets of
sparse and structured sparse vectors, low-rank matrices and tensors, and smooth
manifolds. In addition, we establish a new Johnson-Lindenstrauss embedding for
data sets taking the form of an infinite union of subspaces of a Hilbert space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3982</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3982</id><created>2014-02-17</created><authors><author><keyname>Kim</keyname><forenames>Juhoon</forenames></author><author><keyname>Sarrar</keyname><forenames>Nadi</forenames></author><author><keyname>Feldmann</keyname><forenames>Anja</forenames></author></authors><title>Watching the IPv6 Takeoff from an IXP's Viewpoint</title><categories>cs.NI</categories><comments>Also appears as TU-Berlin technical report 2013-01, ISSN: 1436-9915</comments><report-no>2014-01 (ISSN: 1436-9915)</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The different level of interest in deploying the new Internet address space
across network operators has kept IPv6 tardy in its deployment. However, since
the last block of IPv4 addresses has been assigned, Internet communities took
the concern of the address space scarcity seriously and started to move forward
actively. After the successful IPv6 test on 8 June, 2011 (World IPv6 Day [1]),
network operators and service/content providers were brought together for
preparing the next step of the IPv6 global deployment (World IPv6 Launch on 6
June, 2012 [2]). The main purpose of the event was to permanently enable their
IPv6 connectivity. In this paper, based on the Internet traffic collected from
a large European Internet Exchange Point (IXP), we present the status of IPv6
traffic mainly focusing on the periods of the two global IPv6 events. Our
results show that IPv6 traffic is responsible for a small fraction such as 0.5
% of the total traffic in the peak period. Nevertheless, we are positively
impressed by the facts that the increase of IPv6 traffic/prefixes shows a steep
increase and that the application mix of IPv6 traffic starts to imitate the one
of IPv4-dominated Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3985</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3985</id><created>2014-02-17</created><authors><author><keyname>Payaswini</keyname><forenames>P</forenames></author><author><keyname>Manjaiah</keyname><forenames>D. H</forenames></author></authors><title>Challenges and issues in 4G Networks Mobility Management</title><categories>cs.NI</categories><comments>4 Pages, &quot;Published with International Journal of Computer Trends and
  Technology (IJCTT)&quot;. arXiv admin note: text overlap with arXiv:1105.0043 by
  other authors</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  volume 4 Issue 5 May 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless broadband technology is now in motion to provide higher data rate,
wider coverage and improved mobility. Towards this the 4G - network is an
integration of various wireless technologies and expected to provide seamless
mobility. Moreover 4G-networks will be entirely packet switched systems based
on IP protocol. One of the research challenges for 4G-Network is the design of
intelligent mobility management techniques that take advantage of IP-based
technologies to achieve global roaming among various access technologies. Hence
Mobile IPv6 is considered to be one of the key technologies for integration of
heterogeneous networks. However the original Mobile IPv6 does not support fast
handover, which is essential function for mobile networks. Number of research
groups working towards this to develop a common protocol to enable seamless
mobility. In this paper we identify and explore the different issues and
challenges related to mobility management in 4G - networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.3986</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.3986</id><created>2014-02-17</created><authors><author><keyname>Aknine</keyname><forenames>Samir</forenames></author></authors><title>New Mechanism for Multiagent Extensible Negotiations</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiagent negotiation mechanisms advise original solutions to several
problems for which usual problem solving methods are inappropriate. Mainly
negotiation models are based on agents' interactions through messages. Agents
interact in order to reach an agreement for solving a specific problem. In this
work, we study a new variant of negotiations, which has not yet been addressed
in existing works. This negotiation form is denoted extensible negotiation. In
contrast with current negotiation models, this form of negotiation allows the
agents to dynamically extend the set of items under negotiation. This facility
gives more acceptable solutions for the agents in their negotiation. The
advantage of enlarging the negotiation space is to certainly offer more
facilities for the agents for reaching new agreements which would not have been
obtained using usual negotiation methods. This paper presents the protocol and
the strategies used by the agents to deal with such negotiations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4004</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4004</id><created>2014-02-17</created><authors><author><keyname>Gale</keyname><forenames>Ella</forenames></author><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Design of a Hybrid Robot Control System using Memristor-Model and
  Ant-Inspired Based Information Transfer Protocols</title><categories>cs.RO cs.ET cs.SY</categories><comments>Conference</comments><msc-class>93Cxx, 94Cxx,</msc-class><acm-class>B.3.1; I.1.2</acm-class><journal-ref>Workshop on Unconventional Approaches to Robotics, Automation and
  Control (UARACIN), at International Conference on Robotics and Automation
  (ICRA) 2013, Karlsruhe, Germany, Fr-Ws-09, pgs. 34-36</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is not always possible for a robot to process all the information from its
sensors in a timely manner and thus quick and yet valid approximations of the
robot's situation are needed. Here we design hybrid control for a robot within
this limit using algorithms inspired by ant worker placement behaviour and
based on memristor-based non-linearity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4007</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4007</id><created>2014-02-17</created><authors><author><keyname>Gale</keyname><forenames>Ella</forenames></author><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Does the D.C. Response of Memristors Allow Robotic Short-Term Memory and
  a Possible Route to Artificial Time Perception?</title><categories>cs.RO cs.ET cs.NE</categories><comments>3 page position paper</comments><msc-class>94Cxx</msc-class><journal-ref>Workshop on Unconventional Approaches to Robotics, Automation and
  Control (UARACIN), at International Conference on Robotics and Automation
  (ICRA) 2013, Karlsruhe, Germany, Fr-Ws-09, pgs. 22-24</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time perception is essential for task switching, and in the mammalian brain
appears alongside other processes. Memristors are electronic components used as
synapses and as models for neurons. The d.c. response of memristors can be
considered as a type of short-term memory. Interactions of the memristor d.c.
response within networks of memristors leads to the emergence of oscillatory
dynamics and intermittent spike trains, which are similar to neural dynamics.
Based on this data, the structure of a memristor network control for a robot as
it undergoes task switching is discussed and it is suggested that these
emergent network dynamics could improve the performance of role switching and
learning in an artificial intelligence and perhaps create artificial time
perception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4009</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4009</id><created>2014-02-17</created><authors><author><keyname>V</keyname><forenames>Kashyap.</forenames></author><author><keyname>P</keyname><forenames>Boominathan.</forenames></author></authors><title>Privacy Shielding against Mass Surveillance</title><categories>cs.CY cs.CR</categories><comments>8 pages,8 figures,Published with International Journal of Engineering
  Trends and Technology (IJETT)</comments><doi>10.14445/22315381/IJETT-V8P213</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Privacy Shielding against Mass Surveillance provides a step by step tactical
approach to protecting the privacy of all the users of the internet from mass
surveillance programs by the governments and other state agencies. Protection
of privacy is of prime importance and Privacy Shielding provides the right
means against mass surveillance programs and from malicious users trying to
gain access to your systems. Although protection is difficult when massive
government agencies like the National Security Agency and The Government
Communications Headquarters target internet users for surveillance, it is
possible because the target is not you as an individual but the entire mass as
a whole. With the right approach and a broad perspective of the term Privacy,
it is possible for one to freely access and share information over the internet
without being victims of surveillance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4010</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4010</id><created>2014-02-17</created><authors><author><keyname>Benedi&#x10d;i&#x10d;</keyname><forenames>Lucas</forenames></author><author><keyname>Cruz</keyname><forenames>Felipe A.</forenames></author><author><keyname>Hamada</keyname><forenames>Tsuyoshi</forenames></author><author><keyname>Koro&#x161;ec</keyname><forenames>Peter</forenames></author></authors><title>A GRASS GIS parallel module for radio-propagation predictions</title><categories>cs.DC</categories><comments>13 pages, 12 figures and 2 tables. International Journal of
  Geographical Information Science, 2014</comments><doi>10.1080/13658816.2013.879151</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geographical information systems are ideal candidates for the application of
parallel programming techniques, mainly because they usually handle large data
sets. To help us deal with complex calculations over such data sets, we
investigated the performance constraints of a classic master-worker parallel
paradigm over a message-passing communication model. To this end, we present a
new approach that employs an external database in order to improve the
calculation/communication overlap, thus reducing the idle times for the worker
processes. The presented approach is implemented as part of a parallel
radio-coverage prediction tool for the GRASS environment. The prediction
calculation employs digital elevation models and land-usage data in order to
analyze the radio coverage of a geographical area. We provide an extended
analysis of the experimental results, which are based on real data from an LTE
network currently deployed in Slovenia. Based on the results of the
experiments, which were performed on a computer cluster, the new approach
exhibits better scalability than the traditional master-worker approach. We
successfully tackled real-world data sets, while greatly reducing the
processing time and saturating the hardware utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4013</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4013</id><created>2014-02-17</created><authors><author><keyname>Gale</keyname><forenames>Ella</forenames></author><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author><author><keyname>Erokhin</keyname><forenames>Victor</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>The Short-term Memory (D.C. Response) of the Memristor Demonstrates the
  Causes of the Memristor Frequency Effect</title><categories>cs.ET cond-mat.mtrl-sci cs.AR physics.chem-ph</categories><comments>Conference paper, to appear in CASFEST 2014 June, Melbourne</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A memristor is often identified by showing its distinctive pinched hysteresis
curve and testing for the effect of frequency. The hysteresis size should
relate to frequency and shrink to zero as the frequency approaches infinity.
Although mathematically understood, the material causes for this are not well
known. The d.c. response of the memristor is a decaying curve with its own
timescale. We show via mathematical reasoning that this decaying curve when
transformed to a.c. leads to the frequency effect by considering a descretized
curve. We then demonstrate the validity of this approach with experimental data
from two different types of memristors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4028</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4028</id><created>2014-02-17</created><authors><author><keyname>Fancsali</keyname><forenames>Szabolcs L.</forenames><affiliation>MTA-ELTE Geometric and Algebraic Combinatorics Research Group</affiliation></author><author><keyname>Sziklai</keyname><forenames>P&#xe9;ter</forenames><affiliation>ELTE Institute of Mathematics Department of Computer Science and MTA-ELTE Geometric and Algebraic Combinatorics Research Group</affiliation></author></authors><title>Lines in higgledy-piggledy position</title><categories>math.CO cs.DM</categories><comments>17 pages</comments><msc-class>05B25, 51E20</msc-class><journal-ref>Electronic Journal of Combinatorics Volume 21, Issue 2 (2014)
  Paper #P2.56</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine sets of lines in PG(d,F) meeting each hyperplane in a generator
set of points. We prove that such a set has to contain at least 1.5d lines if
the field F has more than 1.5d elements, and at least 2d-1 lines if the field F
is algebraically closed. We show that suitable 2d-1 lines constitute such a set
(if |F| &gt; or = 2d-1), proving that the lower bound is tight over algebraically
closed fields. At last, we will see that the strong (s,A) subspace designs
constructed by Guruswami and Kopparty have better (smaller) parameter A than
one would think at first sight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4029</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4029</id><created>2014-02-17</created><authors><author><keyname>Gater</keyname><forenames>Deborah</forenames></author><author><keyname>Iqbal</keyname><forenames>Attya</forenames></author><author><keyname>Davey</keyname><forenames>Jeffrey</forenames></author><author><keyname>Gale</keyname><forenames>Ella</forenames></author></authors><title>Connecting Spiking Neurons to a Spiking Memristor Network Changes the
  Memristor Dynamics</title><categories>cs.ET cs.NE physics.bio-ph</categories><comments>Conference paper, 4 pages</comments><msc-class>92C-06, 94C-06</msc-class><journal-ref>Proceedings of the International Conference on Electronics,
  Circuits and Systems (ICECS) 2013, Abu Dhabi, UAE, December 8th-11th,
  534--537</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memristors have been suggested as neuromorphic computing elements. Spike-time
dependent plasticity and the Hodgkin-Huxley model of the neuron have both been
modelled effectively by memristor theory. The d.c. response of the memristor is
a current spike. Based on these three facts we suggest that memristors are
well-placed to interface directly with neurons. In this paper we show that
connecting a spiking memristor network to spiking neuronal cells causes a
change in the memristor network dynamics by: removing the memristor spikes,
which we show is due to the effects of connection to aqueous medium; causing a
change in current decay rate consistent with a change in memristor state;
presenting more-linear $I-t$ dynamics; and increasing the memristor spiking
rate, as a consequence of interaction with the spiking neurons. This
demonstrates that neurons are capable of communicating directly with
memristors, without the need for computer translation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4031</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4031</id><created>2014-02-17</created><updated>2015-06-24</updated><authors><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author><author><keyname>Teixeira</keyname><forenames>Andre M. H.</forenames></author><author><keyname>Langbort</keyname><forenames>Cedric</forenames></author></authors><title>Estimation with Strategic Sensors</title><categories>cs.GT cs.SY math.OC</categories><comments>Results are generalized, illustrative examples are added, and the
  literature review is improved</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model of estimation in the presence of strategic,
self-interested sensors. We employ a game-theoretic setup to model the
interaction between the sensors and the receiver. The cost function of the
receiver is equal to the estimation error variance while the cost function of
the sensor contains an extra term which is determined by its private
information. We start by the single sensor case in which the receiver has
access to a noisy but honest side information in addition to the message
transmitted by a strategic sensor. We study both static and dynamic estimation
problems. For both these problems, we characterize a family of equilibria in
which the sensor and the receiver employ simple strategies. Interestingly, for
the dynamic estimation problem, we find an equilibrium for which the strategic
sensor uses a memory-less policy. We generalize the static estimation setup to
multiple sensors with synchronous communication structure (i.e., all the
sensors transmit their messages simultaneously). We prove the maybe surprising
fact that, for the constructed equilibrium in affine strategies, the estimation
quality degrades as the number of sensors increases. However, if the sensors
are herding (i.e., copying each other policies), the quality of the receiver's
estimation improves as the number of sensors increases. Finally, we consider
the asynchronous communication structure (i.e., the sensors transmit their
messages sequentially).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4033</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4033</id><created>2014-02-17</created><authors><author><keyname>Zhong</keyname><forenames>Erheng</forenames></author><author><keyname>Xiang</keyname><forenames>Evan Wei</forenames></author><author><keyname>Fan</keyname><forenames>Wei</forenames></author><author><keyname>Liu</keyname><forenames>Nathan Nan</forenames></author><author><keyname>Yang</keyname><forenames>Qiang</forenames></author></authors><title>Friendship Prediction in Composite Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Friendship prediction is an important task in social network analysis (SNA).
It can help users identify friends and improve their level of activity. Most
previous approaches predict users' friendship based on their historical
records, such as their existing friendship, social interactions, etc. However,
in reality, most users have limited friends in a single network, and the data
can be very sparse. The sparsity problem causes existing methods to overfit the
rare observations and suffer from serious performance degradation. This is
particularly true when a new social network just starts to form. We observe
that many of today's social networks are composite in nature, where people are
often engaged in multiple networks. In addition, users' friendships are always
correlated, for example, they are both friends on Facebook and Google+. Thus,
by considering those overlapping users as the bridge, the friendship knowledge
in other networks can help predict their friendships in the current network.
This can be achieved by exploiting the knowledge in different networks in a
collective manner. However, as each individual network has its own properties
that can be incompatible and inconsistent with other networks, the naive
merging of all networks into a single one may not work well. The proposed
solution is to extract the common behaviors between different networks via a
hierarchical Bayesian model. It captures the common knowledge across networks,
while avoiding negative impacts due to network differences. Empirical studies
demonstrate that the proposed approach improves the mean average precision of
friendship prediction over state-of-the-art baselines on nine real-world social
networking datasets significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4036</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4036</id><created>2014-02-17</created><authors><author><keyname>Gale</keyname><forenames>Ella</forenames></author><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Is Spiking Logic the Route to Memristor-Based Computers?</title><categories>cs.ET cond-mat.mtrl-sci cs.AR cs.NE</categories><comments>Conference paper. Work also reported in US patent: `Logic device and
  method of performing a logical operation', patent application no. 14/089,191
  (November 25, 2013)</comments><msc-class>94C-06</msc-class><acm-class>C.1.3; B.3.1</acm-class><journal-ref>Proceedings of the International Conference on Electronics,
  Circuits and Systems (ICECS) 2013, Abu Dhabi, UAE, December 8th-11th,
  297--300</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memristors have been suggested as a novel route to neuromorphic computing
based on the similarity between neurons (synapses and ion pumps) and
memristors. The D.C. action of the memristor is a current spike, which we think
will be fruitful for building memristor computers. In this paper, we introduce
4 different logical assignations to implement sequential logic in the memristor
and introduce the physical rules, summation, `bounce-back', directionality and
`diminishing returns', elucidated from our investigations. We then demonstrate
how memristor sequential logic works by instantiating a NOT gate, an AND gate
and a Full Adder with a single memristor. The Full Adder makes use of the
memristor's memory to add three binary values together and outputs the value,
the carry digit and even the order they were input in.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4037</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4037</id><created>2014-02-17</created><updated>2015-02-18</updated><authors><author><keyname>Kannan</keyname><forenames>Sampath</forenames></author><author><keyname>Mathieu</keyname><forenames>Claire</forenames></author><author><keyname>Zhou</keyname><forenames>Hang</forenames></author></authors><title>Near-Linear Query Complexity for Graph Inference</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How efficiently can we find an unknown graph using distance or shortest path
queries between its vertices? Let $G = (V,E)$ be an unweighted, connected graph
of bounded degree. The edge set $E$ is initially unknown, and the graph can be
accessed using a \emph{distance oracle}, which receives a pair of vertices
$(u,v)$ and returns the distance between $u$ and $v$. In the
\emph{verification} problem, we are given a hypothetical graph $\hat G =
(V,\hat E)$ and want to check whether $G$ is equal to $\hat G$. We analyze a
natural greedy algorithm and prove that it uses $n^{1+o(1)}$ distance queries.
In the more difficult \emph{reconstruction} problem, $\hat G$ is not given, and
the goal is to find the graph $G$. If the graph can be accessed using a
\emph{shortest path oracle}, which returns not just the distance but an actual
shortest path between $u$ and $v$, we show that extending the idea of greedy
gives a reconstruction algorithm that uses $n^{1+o(1)}$ shortest path queries.
When the graph has bounded treewidth, we further bound the query complexity of
the greedy algorithms for both problems by $\tilde O(n)$. When the graph is
chordal, we provide a randomized algorithm for reconstruction using $\tilde
O(n)$ distance queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4043</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4043</id><created>2014-02-17</created><updated>2014-04-26</updated><authors><author><keyname>Jagadeesan</keyname><forenames>Radha</forenames></author><author><keyname>Riely</keyname><forenames>James</forenames></author></authors><title>Between Linearizability and Quiescent Consistency: Quantitative
  Quiescent Consistency</title><categories>cs.PL</categories><comments>Short version in ICALP 2014. http://icalp2014.itu.dk/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linearizability is the de facto correctness criterion for concurrent data
structures. Unfortunately, linearizability imposes a performance penalty which
scales linearly in the number of contending threads. Quiescent consistency is
an alternative criterion which guarantees that a concurrent data structure
behaves correctly when accessed sequentially. Yet quiescent consistency says
very little about executions that have any contention.
  We define quantitative quiescent consistency (QQC), a relaxation of
linearizability where the degree of relaxation is proportional to the degree of
contention. When quiescent, no relaxation is allowed, and therefore QQC refines
quiescent consistency, unlike other proposed relaxations of linearizability. We
show that high performance counters and stacks designed to satisfy quiescent
consistency continue to satisfy QQC. The precise assumptions under which QQC
holds provides fresh insight on these structures. To demonstrate the robustness
of QQC, we provide three natural characterizations and prove compositionality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4046</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4046</id><created>2014-02-17</created><authors><author><keyname>Gale</keyname><forenames>Ella</forenames></author><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Boolean Logic Gates From A Single Memristor Via Low-Level Sequential
  Logic</title><categories>cs.ET cs.AR</categories><comments>Conference paper, published in Springer Lecture Notes in Computer
  Science http://link.springer.com/chapter/10.1007%2F978-3-642-39074-6_9</comments><msc-class>94C-06</msc-class><journal-ref>Lecture Notes in Computer Science Volume 7956 2013 Unconventional
  Computation and Natural Computation 12th International Conference, UCNC 2013,
  Milan, Italy, July 1-5, 2013. Proceedings, 78-89</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By using the memristor's memory to both store a bit and perform an operation
with a second input bit, simple Boolean logic gates have been built with a
single memristor. The operation makes use of the interaction of current spikes
(occasionally called current transients) found in both memristors and other
devices. The sequential time-based logic methodology allows two logical input
bits to be used on a one-port by sending the bits separated in time. The
resulting logic gate is faster than one relying on memristor's state switching,
low power and requires only one memristor. We experimentally demonstrate
working OR and XOR gates made with a single flexible Titanium dioxide sol-gel
memristor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4050</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4050</id><created>2014-02-17</created><updated>2015-10-01</updated><authors><author><keyname>Auletta</keyname><forenames>Vincenzo</forenames></author><author><keyname>Caragiannis</keyname><forenames>Ioannis</forenames></author><author><keyname>Ferraioli</keyname><forenames>Diodato</forenames></author><author><keyname>Galdi</keyname><forenames>Clemente</forenames></author><author><keyname>Persiano</keyname><forenames>Giuseppe</forenames></author></authors><title>Minority Becomes Majority in Social Networks</title><categories>cs.GT cs.DS cs.MA cs.SI</categories><comments>To appear in WINE 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is often observed that agents tend to imitate the behavior of their
neighbors in a social network. This imitating behavior might lead to the
strategic decision of adopting a public behavior that differs from what the
agent believes is the right one and this can subvert the behavior of the
population as a whole.
  In this paper, we consider the case in which agents express preferences over
two alternatives and model social pressure with the majority dynamics: at each
step an agent is selected and its preference is replaced by the majority of the
preferences of her neighbors. In case of a tie, the agent does not change her
current preference. A profile of the agents' preferences is stable if the
preference of each agent coincides with the preference of at least half of the
neighbors (thus, the system is in equilibrium).
  We ask whether there are network topologies that are robust to social
pressure. That is, we ask if there are graphs in which the majority of
preferences in an initial profile always coincides with the majority of the
preference in all stable profiles reachable from that profile. We completely
characterize the graphs with this robustness property by showing that this is
possible only if the graph has no edge or is a clique or very close to a
clique. In other words, except for this handful of graphs, every graph admits
at least one initial profile of preferences in which the majority dynamics can
subvert the initial majority. We also show that deciding whether a graph admits
a minority that becomes majority is NP-hard when the minority size is at most
1/4-th of the social network size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4053</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4053</id><created>2014-02-17</created><authors><author><keyname>Kir&#xe1;ly</keyname><forenames>Franz J</forenames></author><author><keyname>Ehler</keyname><forenames>Martin</forenames></author></authors><title>The Algebraic Approach to Phase Retrieval and Explicit Inversion at the
  Identifiability Threshold</title><categories>math.FA cs.CV cs.IT math.AG math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study phase retrieval from magnitude measurements of an unknown signal as
an algebraic estimation problem. Indeed, phase retrieval from rank-one and more
general linear measurements can be treated in an algebraic way. It is verified
that a certain number of generic rank-one or generic linear measurements are
sufficient to enable signal reconstruction for generic signals, and slightly
more generic measurements yield reconstructability for all signals. Our results
solve a few open problems stated in the recent literature. Furthermore, we show
how the algebraic estimation problem can be solved by a closed-form algebraic
estimation technique, termed ideal regression, providing non-asymptotic success
guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4062</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4062</id><created>2014-02-17</created><updated>2014-03-01</updated><authors><author><keyname>Bonchi</keyname><forenames>Filippo</forenames></author><author><keyname>Milius</keyname><forenames>Stefan</forenames></author><author><keyname>Silva</keyname><forenames>Alexandra</forenames></author><author><keyname>Zanasi</keyname><forenames>Fabio</forenames></author></authors><title>How to Kill Epsilons with a Dagger -- A Coalgebraic Take on Systems with
  Algebraic Label Structure</title><categories>cs.LO</categories><msc-class>68Q55, 68Q70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an abstract framework for modeling state-based systems with
internal behavior as e.g. given by silent or $\epsilon$-transitions. Our
approach employs monads with a parametrized fixpoint operator $\dagger$ to give
a semantics to those systems and implement a sound procedure of abstraction of
the internal transitions, whose labels are seen as the unit of a free monoid.
More broadly, our approach extends the standard coalgebraic framework for
state-based systems by taking into account the algebraic structure of the
labels of their transitions. This allows to consider a wide range of other
examples, including Mazurkiewicz traces for concurrent systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4064</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4064</id><created>2014-02-17</created><updated>2014-04-03</updated><authors><author><keyname>Ku&#x142;akowski</keyname><forenames>Konrad</forenames></author></authors><title>Notes on the existence of solutions in the pairwise comparisons method
  using the Heuristic Rating Estimation approach</title><categories>cs.DM</categories><comments>8 pages</comments><doi>10.1007/s10472-015-9474-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pairwise comparisons are a well-known method for modelling of the subjective
preferences of a decision maker. A popular implementation of the method is
based on solving an eigenvalue problem for M - the matrix of pairwise
comparisons. This does not take into account the actual values of preference.
The Heuristic Rating Estimation (HRE) approach is a modification of this method
in which allows modelling of the reference values. To determine the relative
order of preferences is to solve a certain linear equation system defined by
the matrix A and the constant term vector b (both derived from M). The article
explores the properties of these equation systems. In particular, it is proven
that for some small data inconsistency the A matrix is an M-matrix, hence the
equation proposed by the HRE approach has a unique strictly positive solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4067</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4067</id><created>2014-02-17</created><authors><author><keyname>Aja-Fernandez</keyname><forenames>Santiago</forenames></author><author><keyname>Vegas-Sanchez-Ferrero</keyname><forenames>Gonzalo</forenames></author><author><keyname>Trsitan-Vega</keyname><forenames>Antonio</forenames></author></authors><title>Statistical Noise Analysis in SENSE Parallel MRI</title><categories>cs.CV</categories><report-no>TECH-LPI2012-01. V2.0</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A complete first and second order statistical characterization of noise in
SENSE reconstructed data is proposed. SENSE acquisitions have usually been
modeled as Rician distributed, since the data reconstruction takes place into
the spatial domain, where Gaussian noise is assumed. However, this model just
holds for the first order statistics and obviates other effects induced by
coils correlations and the reconstruction interpolation. Those effects are
properly taken into account in this study, in order to fully justify a final
SENSE noise model. As a result, some interesting features of the reconstructed
image arise: (1) There is a strong correlation between adjacent lines. (2) The
resulting distribution is non-stationary and therefore the variance of noise
will vary from point to point across the image. Closed equations for the
calculation of the variance of noise and the correlation coefficient between
lines are proposed. The proposed model is totally compatible with g-factor
formulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4069</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4069</id><created>2014-02-17</created><updated>2014-11-24</updated><authors><author><keyname>Garc&#xe9;s</keyname><forenames>Yasel</forenames></author><author><keyname>Torres</keyname><forenames>Esley</forenames></author><author><keyname>Pereira</keyname><forenames>Osvaldo</forenames></author><author><keyname>Rodr&#xed;guez</keyname><forenames>Roberto</forenames></author></authors><title>Application of the Ring Theory in the Segmentation of Digital Images</title><categories>cs.CV</categories><comments>Very interesting new index to compute the similarity among images.
  arXiv admin note: substantial text overlap with arXiv:1306.2624</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ring theory is one of the branches of the abstract algebra that has been
broadly used in images. However, ring theory has not been very related with
image segmentation. In this paper, we propose a new index of similarity among
images using Zn rings and the entropy function. This new index was applied as a
new stopping criterion to the Mean Shift Iterative Algorithm with the goal to
reach a better segmentation. An analysis on the performance of the algorithm
with this new stopping criterion is carried out. The obtained results proved
that the new index is a suitable tool to compare images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4073</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4073</id><created>2014-02-17</created><authors><author><keyname>Kaser</keyname><forenames>Owen</forenames></author><author><keyname>Lemire</keyname><forenames>Daniel</forenames></author></authors><title>Threshold and Symmetric Functions over Bitmaps</title><categories>cs.DB cs.DS</categories><comments>This paper uses small fonts and colours and is only intended for
  electronic viewing</comments><report-no>TR-14-001</report-no><acm-class>H.2.2; H.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bitmap indexes are routinely used to speed up simple aggregate queries in
databases. Set operations such as intersections, unions and complements can be
represented as logical operations (AND, OR, NOT). However, less is known about
the application of bitmap indexes to more advanced queries. We want to extend
the applicability of bitmap indexes. As a starting point, we consider symmetric
Boolean queries (e.g., threshold functions). For example, we might consider
stores as sets of products, and ask for products that are on sale in 2 to 10
stores. Such symmetric Boolean queries generalize intersection, union, and
T-occurrence queries.
  It may not be immediately obvious to an engineer how to use bitmap indexes
for symmetric Boolean queries. Yet, maybe surprisingly, we find that the best
of our bitmap-based algorithms are competitive with the state-of-the-art
algorithms for important special cases (e.g., MergeOpt, MergeSkip, DivideSkip,
ScanCount). Moreover, unlike the competing algorithms, the result of our
computation is again a bitmap which can be further processed within a bitmap
index.
  We review algorithmic design issues such as the aggregation of many
compressed bitmaps. We conclude with a discussion on other advanced queries
that bitmap indexes might be able to support efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4084</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4084</id><created>2014-02-17</created><authors><author><keyname>Moroshko</keyname><forenames>Edward</forenames></author><author><keyname>Crammer</keyname><forenames>Koby</forenames></author></authors><title>Selective Sampling with Drift</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently there has been much work on selective sampling, an online active
learning setting, in which algorithms work in rounds. On each round an
algorithm receives an input and makes a prediction. Then, it can decide whether
to query a label, and if so to update its model, otherwise the input is
discarded. Most of this work is focused on the stationary case, where it is
assumed that there is a fixed target model, and the performance of the
algorithm is compared to a fixed model. However, in many real-world
applications, such as spam prediction, the best target function may drift over
time, or have shifts from time to time. We develop a novel selective sampling
algorithm for the drifting setting, analyze it under no assumptions on the
mechanism generating the sequence of instances, and derive new mistake bounds
that depend on the amount of drift in the problem. Simulations on synthetic and
real-world datasets demonstrate the superiority of our algorithms as a
selective sampling algorithm in the drifting setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4100</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4100</id><created>2014-02-13</created><authors><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Yang</keyname><forenames>Hong-Chuan</forenames></author><author><keyname>Hasna</keyname><forenames>Mazen O.</forenames></author></authors><title>Generalized Area Spectral Efficiency: An Effective Performance Metric
  for Green Wireless Communications</title><categories>cs.NI cs.IT math.IT</categories><comments>11 pages, 8 figures, accepted by TCom</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Area spectral efficiency (ASE) was introduced as a metric to quantify the
spectral utilization efficiency of cellular systems. Unlike other performance
metrics, ASE takes into account the spatial property of cellular systems. In
this paper, we generalize the concept of ASE to study arbitrary wireless
transmissions. Specifically, we introduce the notion of affected area to
characterize the spatial property of arbitrary wireless transmissions. Based on
the definition of affected area, we define the performance metric, generalized
area spectral efficiency (GASE), to quantify the spatial spectral utilization
efficiency as well as the greenness of wireless transmissions. After
illustrating its evaluation for point-to-point transmission, we analyze the
GASE performance of several different transmission scenarios, including
dual-hop relay transmission, three-node cooperative relay transmission and
underlay cognitive radio transmission. We derive closed-form expressions for
the GASE metric of each transmission scenario under Rayleigh fading environment
whenever possible. Through mathematical analysis and numerical examples, we
show that the GASE metric provides a new perspective on the design and
optimization of wireless transmissions, especially on the transmitting power
selection. We also show that introducing relay nodes can greatly improve the
spatial utilization efficiency of wireless systems. We illustrate that the GASE
metric can help optimize the deployment of underlay cognitive radio systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4101</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4101</id><created>2014-02-17</created><authors><author><keyname>Nascimento</keyname><forenames>Marcelo Zanchetta do</forenames></author><author><keyname>Batista</keyname><forenames>Val&#xe9;rio Ramos</forenames></author></authors><title>First steps to Virtual Mammography: Simulating external compressions of
  the breast with the Surface Evolver</title><categories>cs.CE physics.med-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a computational modelling that reproduces the
breast compression processes used to obtain the mammogram. The main result is a
programme in which one can track the first steps of virtual mammography. On the
one hand, our modelling enables addition of structures that represent different
tissues, muscles and glands in the breast. On the other hand, we shall validate
and implement it by means of laboratory tests with phantoms. To the best of our
knowledge, these two characteristics do confer originality to our research.
This is because their interrelation seems not to be properly established
elsewhere yet. We conclude that our model reproduces the same shapes and
measurements really taken from the volunteer's breasts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4102</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4102</id><created>2014-02-17</created><updated>2014-05-12</updated><authors><author><keyname>Chen</keyname><forenames>Tianqi</forenames></author><author><keyname>Fox</keyname><forenames>Emily B.</forenames></author><author><keyname>Guestrin</keyname><forenames>Carlos</forenames></author></authors><title>Stochastic Gradient Hamiltonian Monte Carlo</title><categories>stat.ME cs.LG stat.ML</categories><comments>ICML 2014 version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for
defining distant proposals with high acceptance probabilities in a
Metropolis-Hastings framework, enabling more efficient exploration of the state
space than standard random-walk proposals. The popularity of such methods has
grown significantly in recent years. However, a limitation of HMC methods is
the required gradient computation for simulation of the Hamiltonian dynamical
system-such computation is infeasible in problems involving a large sample size
or streaming data. Instead, we must rely on a noisy gradient estimate computed
from a subset of the data. In this paper, we explore the properties of such a
stochastic gradient HMC approach. Surprisingly, the natural implementation of
the stochastic approximation can be arbitrarily bad. To address this problem we
introduce a variant that uses second-order Langevin dynamics with a friction
term that counteracts the effects of the noisy gradient, maintaining the
desired target distribution as the invariant distribution. Results on simulated
data validate our theory. We also provide an application of our methods to a
classification task using neural networks and to online Bayesian matrix
factorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4111</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4111</id><created>2014-02-17</created><updated>2014-02-18</updated><authors><author><keyname>Cohen-Addad</keyname><forenames>Vincent</forenames></author><author><keyname>Li</keyname><forenames>Zhentao</forenames></author><author><keyname>Mathieu</keyname><forenames>Claire</forenames></author><author><keyname>Millis</keyname><forenames>Ioannis</forenames></author></authors><title>Energy-efficient algorithms for non-preemptive speed-scaling</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We improve complexity bounds for energy-efficient speed scheduling problems
for both the single processor and multi-processor cases. Energy conservation
has become a major concern, so revisiting traditional scheduling problems to
take into account the energy consumption has been part of the agenda of the
scheduling community for the past few years.
  We consider the energy minimizing speed scaling problem introduced by Yao et
al. where we wish to schedule a set of jobs, each with a release date, deadline
and work volume, on a set of identical processors. The processors may change
speed as a function of time and the energy they consume is the $\alpha$th power
of its speed. The objective is then to find a feasible schedule which minimizes
the total energy used.
  We show that in the setting with an arbitrary number of processors where all
work volumes are equal, there is a $2(1+\varepsilon)(5(1+\varepsilon))^{\alpha
-1}\tilde{B}_{\alpha}=O_{\alpha}(1)$ approximation algorithm, where
$\tilde{B}_{\alpha}$ is the generalized Bell number. This is the first constant
factor algorithm for this problem. This algorithm extends to general unequal
processor-dependent work volumes, up to losing a factor of
$(\frac{(1+r)r}{2})^{\alpha}$ in the approximation, where $r$ is the maximum
ratio between two work volumes. We then show this latter problem is APX-hard,
even in the special case when all release dates and deadlines are equal and $r$
is 4.
  In the single processor case, we introduce a new linear programming
formulation of speed scaling and prove that its integrality gap is at most
$12^{\alpha -1}$. As a corollary, we obtain a $(12(1+\varepsilon))^{\alpha -1}$
approximation algorithm where there is a single processor, improving on the
previous best bound of $2^{\alpha-1}(1+\varepsilon)^{\alpha}\tilde{B}_{\alpha}$
when $\alpha \ge 25$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4157</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4157</id><created>2014-02-17</created><updated>2014-05-12</updated><authors><author><keyname>Calliess</keyname><forenames>Jan-Peter</forenames></author><author><keyname>Osborne</keyname><forenames>Michael</forenames></author><author><keyname>Roberts</keyname><forenames>Stephen</forenames></author></authors><title>Conservative collision prediction and avoidance for stochastic
  trajectories in continuous time and space</title><categories>cs.AI cs.MA cs.RO</categories><comments>This preprint is an extended version of a conference paper that is to
  appear in \textit{Proceedings of the 13th International Conference on
  Autonomous Agents and Multiagent Systems (AAMAS 2014)}</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing work in multi-agent collision prediction and avoidance typically
assumes discrete-time trajectories with Gaussian uncertainty or that are
completely deterministic. We propose an approach that allows detection of
collisions even between continuous, stochastic trajectories with the only
restriction that means and variances can be computed. To this end, we employ
probabilistic bounds to derive criterion functions whose negative sign provably
is indicative of probable collisions. For criterion functions that are
Lipschitz, an algorithm is provided to rapidly find negative values or prove
their absence. We propose an iterative policy-search approach that avoids prior
discretisations and yields collision-free trajectories with adjustably high
certainty. We test our method with both fixed-priority and auction-based
protocols for coordinating the iterative planning process. Results are provided
in collision-avoidance simulations of feedback controlled plants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4159</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4159</id><created>2014-02-17</created><authors><author><keyname>Wang</keyname><forenames>Xiaozhe</forenames></author><author><keyname>Chiang</keyname><forenames>Hsiao-Dong</forenames></author></authors><title>Application of Pseudo-Transient Continuation Method in Dynamic Stability
  Analysis</title><categories>cs.SY</categories><comments>This paper has been accepted by IEEE PES general meeting, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, pseudo-transient continuation method has been modified and
implemented in power system long-term stability analysis. This method is a
middle ground between integration and steady state calculation, thus is a good
compromise between accuracy and efficiency. Pseudo-transient continuation
method can be applied in the long-term stability model directly to accelerate
simulation speed and can also be implemented in the QSS model to overcome
numerical difficulties. Numerical examples show that pseudo-transient
continuation method can provide correct approximations for the long-term
stability model in terms of trajectories and stability assessment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4160</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4160</id><created>2014-02-13</created><authors><author><keyname>Nongpiur</keyname><forenames>R. C.</forenames></author><author><keyname>Shpak</keyname><forenames>D. J.</forenames></author></authors><title>Maximizing the Signal-to-Alias Ratio in Non-Uniform Filter Banks for
  Acoustic Echo Cancellation</title><categories>cs.SD</categories><comments>IEEE Transactions on Circuits and Systems I: Regular Paper, vol. 59,
  no. 10, Oct. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method for designing non-uniform filter-banks for acoustic echo
cancellation is proposed. In the method, the analysis prototype filter design
is framed as a convex optimization problem that maximizes the signal-to-alias
ratio (SAR) in the analysis banks. Since each sub-band has a different
bandwidth, the contribution to the overall SAR from each analysis bank is taken
into account during optimization. To increase the degrees of freedom during
optimization, no constraints are imposed on the phase or group delay of the
filters; at the same time, low delay is achieved by ensuring that the resulting
filters are minimum phase. Experimental results show that the filter bank
designed using the proposed method results in a sub-band adaptive filter with a
much better echo return loss enhancement (ERLE) when compared with existing
design methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4164</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4164</id><created>2014-02-17</created><authors><author><keyname>Marques</keyname><forenames>Eduardo R. B.</forenames></author></authors><title>Fine-grained Patches for Java Software Upgrades</title><categories>cs.SE</categories><comments>In Proc. HotSWUp'13, 6 pages, 4 figures, 1 table</comments><journal-ref>Proc. 5th Workshop on Hot Topics in Software Upgrades, USENIX,
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel methodology for deriving fine-grained patches of Java
software. We consider an abstract-syntax tree (AST) representation of Java
classes compiled to the Java Virtual Machine (JVM) format, and a difference
analysis over the AST representation to derive patches. The AST representation
defines an appropriate abstraction level for analyzing differences, yielding
compact patches that correlate modularly to actual source code changes. The
approach contrasts to other common, coarse-grained approaches, like plain
binary differences, which may easily lead to disproportionately large patches.
We present the main traits of the methodology, a prototype tool called aspa
that implements it, and a case-study analysis on the use of aspa to derive
patches for the Java 2 SE API. The case-study results illustrate that aspa
patches have a significantly smaller size than patches derived by binary
differencing tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4172</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4172</id><created>2014-02-17</created><updated>2014-05-23</updated><authors><author><keyname>Xu</keyname><forenames>Wenyan</forenames></author></authors><title>A propositional system induced by Japaridze's approach to IF logic</title><categories>cs.LO math.LO</categories><msc-class>03B47</msc-class><acm-class>F.1.1; F.1.2; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cirquent calculus is a new proof-theoretic and semantic approach introduced
for the needs of computability logic by G.Japaridze, who also showed that,
through cirquent calculus, one can capture, refine and generalize
independence-friendly (IF) logic. Specifically, the approach allows us to
account for independence from propositional connectives in the same spirit as
the traditional IF logic accounts for independence from quantifiers.
Japaridze's treatment of IF logic, however, was purely semantical, and no
deductive system was proposed. The present paper constructs a formal system
sound and complete w.r.t. the propositional fragment of Japaridze's
cirquent-based semantics for IF logic. Such a system can thus be considered an
axiomatization of purely propositional IF logic in its full generality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4178</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4178</id><created>2014-02-17</created><updated>2015-01-01</updated><authors><author><keyname>Angelelli</keyname><forenames>Enrico</forenames></author><author><keyname>Kalinowski</keyname><forenames>Thomas</forenames></author><author><keyname>Kapoor</keyname><forenames>Reena</forenames></author><author><keyname>Savelsbergh</keyname><forenames>Martin W. P.</forenames></author></authors><title>A reclaimer scheduling problem arising in coal stockyard management</title><categories>cs.DS</categories><comments>26 pages</comments><doi>10.1007/s10951-015-0436-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a number of variants of an abstract scheduling problem inspired by
the scheduling of reclaimers in the stockyard of a coal export terminal. We
analyze the complexity of each of the variants, providing complexity proofs for
some and polynomial algorithms for others. For one, especially interesting
variant, we also develop a constant factor approximation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4179</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4179</id><created>2014-02-17</created><updated>2014-03-13</updated><authors><author><keyname>Tejedor</keyname><forenames>Alejandro</forenames></author><author><keyname>Longjas</keyname><forenames>Anthony</forenames></author><author><keyname>Zaliapin</keyname><forenames>Ilya</forenames></author><author><keyname>Ambroj</keyname><forenames>Samuel</forenames></author><author><keyname>Foufoula-Georgiou</keyname><forenames>Efi</forenames></author></authors><title>Network robustness assessed within a dual connectivity perspective</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network robustness against attacks has been widely studied in fields as
diverse as the Internet, power grids and human societies. Typically, in these
studies, robustness is assessed only in terms of the connectivity of the nodes
unaffected by the attack. Here we put forward the idea that the connectivity of
the affected nodes can play a crucial role in properly evaluating the overall
network robustness and its future recovery from the attack. Specifically, we
propose a dual perspective approach wherein at any instant in the network
evolution under attack, two distinct networks are defined: (i) the Active
Network (AN) composed of the unaffected nodes and (ii) the Idle Network (IN)
composed of the affected nodes. The proposed robustness metric considers both
the efficiency of destroying the AN and the efficiency of building-up the IN.
We show, via analysis of both prototype networks and real world data, that
trade-offs between the efficiency of Active and Idle network dynamics give rise
to surprising crossovers and re-ranking of different attack strategies,
pointing to significant implications for decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4182</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4182</id><created>2014-02-17</created><updated>2014-04-07</updated><authors><author><keyname>Allamanis</keyname><forenames>Miltiadis</forenames></author><author><keyname>Barr</keyname><forenames>Earl T.</forenames></author><author><keyname>Bird</keyname><forenames>Christian</forenames></author><author><keyname>Sutton</keyname><forenames>Charles</forenames></author></authors><title>Learning Natural Coding Conventions</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every programmer has a characteristic style, ranging from preferences about
identifier naming to preferences about object relationships and design
patterns. Coding conventions define a consistent syntactic style, fostering
readability and hence maintainability. When collaborating, programmers strive
to obey a project's coding conventions. However, one third of reviews of
changes contain feedback about coding conventions, indicating that programmers
do not always follow them and that project members care deeply about adherence.
Unfortunately, programmers are often unaware of coding conventions because
inferring them requires a global view, one that aggregates the many local
decisions programmers make and identifies emergent consensus on style. We
present NATURALIZE, a framework that learns the style of a codebase, and
suggests revisions to improve stylistic consistency. NATURALIZE builds on
recent work in applying statistical natural language processing to source code.
We apply NATURALIZE to suggest natural identifier names and formatting
conventions. We present four tools focused on ensuring natural code during
development and release management, including code review. NATURALIZE achieves
94% accuracy in its top suggestions for identifier names and can even transfer
knowledge about conventions across projects, leveraging a corpus of 10,968 open
source projects. We used NATURALIZE to generate 18 patches for 5 open source
projects: 14 were accepted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4183</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4183</id><created>2014-02-17</created><authors><author><keyname>Ge</keyname><forenames>Dongdong</forenames></author><author><keyname>Wang</keyname><forenames>Zizhuo</forenames></author><author><keyname>Wei</keyname><forenames>Lai</forenames></author><author><keyname>Zhang</keyname><forenames>Jiawei</forenames></author></authors><title>An Improved Algorithm for Fixed-Hub Single Allocation Problem</title><categories>cs.DS cs.CC math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the fixed-hub single allocation problem (FHSAP). In this
problem, a network consists of hub nodes and terminal nodes. Hubs are fixed and
fully connected; each terminal node is connected to a single hub which routes
all its traffic. The goal is to minimize the cost of routing the traffic in the
network. In this paper, we propose a linear programming (LP)-based rounding
algorithm. The algorithm is based on two ideas. First, we modify the LP
relaxation formulation introduced in Ernst and Krishnamoorthy (1996, 1999) by
incorporating a set of validity constraints. Then, after obtaining a fractional
solution to the LP relaxation, we make use of a geometric rounding algorithm to
obtain an integral solution. We show that by incorporating the validity
constraints, the strengthened LP often provides much tighter upper bounds than
the previous methods with a little more computational effort, and the solution
obtained often has a much smaller gap with the optimal solution. We also
formulate a robust version of the FHSAP and show that it can guard against data
uncertainty with little cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4194</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4194</id><created>2014-02-17</created><updated>2014-07-20</updated><authors><author><keyname>Dughmi</keyname><forenames>Shaddin</forenames></author></authors><title>On the Hardness of Signaling</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a recent surge of interest in the role of information in
strategic interactions. Much of this work seeks to understand how the realized
equilibrium of a game is influenced by uncertainty in the environment and the
information available to players in the game. Lurking beneath this literature
is a fundamental, yet largely unexplored, algorithmic question: how should a
&quot;market maker&quot; who is privy to additional information, and equipped with a
specified objective, inform the players in the game? This is an informational
analogue of the mechanism design question, and views the information structure
of a game as a mathematical object to be designed, rather than an exogenous
variable.
  We initiate a complexity-theoretic examination of the design of optimal
information structures in general Bayesian games, a task often referred to as
signaling. We focus on one of the simplest instantiations of the signaling
question: Bayesian zero-sum games, and a principal who must choose an
information structure maximizing the equilibrium payoff of one of the players.
In this setting, we show that optimal signaling is computationally intractable,
and in some cases hard to approximate, assuming that it is hard to recover a
planted clique from an Erdos-Renyi random graph. This is despite the fact that
equilibria in these games are computable in polynomial time, and therefore
suggests that the hardness of optimal signaling is a distinct phenomenon from
the hardness of equilibrium computation. Necessitated by the non-local nature
of information structures, en-route to our results we prove an &quot;amplification
lemma&quot; for the planted clique problem which may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4211</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4211</id><created>2014-02-17</created><authors><author><keyname>Bao</keyname><forenames>Yukun</forenames></author><author><keyname>Xiong</keyname><forenames>Tao</forenames></author><author><keyname>Hu</keyname><forenames>Zhongyi</forenames></author><author><keyname>Kibelloh</keyname><forenames>Mboni</forenames></author></authors><title>Exploring gender differences on general and specific computer
  self-efficacy in mobile learning adoption</title><categories>cs.CY</categories><comments>30 pages</comments><journal-ref>Journal of Educational Computing Reasearch.2013, Vol.
  49(1).111-132</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reasons for contradictory findings regarding the gender moderate effect on
computer self-efficacy in the adoption of e-learning/mobile learning are
limited. Recognizing the multilevel nature of the computer self-efficacy (CSE),
this study attempts to explore gender differences in the adoption of mobile
learning, by extending the Technology Acceptance Model (TAM) with general and
specific CSE. Data collected from 137 university students were tested against
the research model using the structural equation modeling approach. The results
suggest that there are significant gender differences in perceptions of general
CSE, perceived ease of use and behavioral intention to use but no significant
differences in specific CSE, perceived usefulness. Additionally, the findings
reveal that specific CSE is more salient than general CSE in influencing
perceived ease of use while general CSE seems to be the salient factor on
perceived usefulness for both female and male combined. Moreover, general CSE
was salient to determine the behavioral intention to use indirectly for female
despite lower perception of general CSE than male's, and specific CSE exhibited
stronger indirect effect on behavioral intention to use than general CSE for
female despite similar perception of specific CSE as males'. These findings
provide important implications for mobile learning adoption and usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4223</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4223</id><created>2014-02-17</created><updated>2014-04-29</updated><authors><author><keyname>Barth</keyname><forenames>Andreas</forenames></author><author><keyname>Marx</keyname><forenames>Werner</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Mutz</keyname><forenames>Ruediger</forenames></author></authors><title>On the origins and the historical roots of the Higgs boson research from
  a bibliometric perspective</title><categories>physics.hist-ph cs.DL hep-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subject of our present paper is the analysis of the origins or historical
roots of the Higgs boson research from a bibliometric perspective, using a
segmented regression analysis in a reference publication year spectroscopy
(RPYS). Our analysis is based on the references cited in the Higgs boson
publications published since 1974. The objective of our analysis consists of
identifying concrete individual publications in the Higgs boson research
context to which the scientific community frequently had referred to. As a
consequence, we are interested in seminal works which contributed to a high
extent to the discovery of the Higgs boson. Our results show that researchers
in the Higgs boson field preferably refer to more recently published papers -
particular papers published since the beginning of the sixties. For example,
our analysis reveals seven major contributions which appeared within the
sixties: Englert and Brout (1964), Higgs (1964, 2 papers), and Guralnik et al.
(1964) on the Higgs mechanism as well as Glashow (1961), Weinberg (1967), and
Salam (1968) on the unification of weak and electromagnetic interaction. Even
if the Nobel Prize award highlights the outstanding importance of the work of
Peter Higgs and Francois Englert, bibliometrics offer the additional
possibility of getting hints to other publications in this research field
(especially to historical publications), which are of vital importance from the
expert point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4225</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4225</id><created>2014-02-17</created><authors><author><keyname>Suh</keyname><forenames>Changho</forenames></author></authors><title>Information Theory of Matrix Completion</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to the International Symposium on Information
  Theory 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix completion is a fundamental problem that comes up in a variety of
applications like the Netflix problem, collaborative filtering, computer
vision, and crowdsourcing. The goal of the problem is to recover a k-by-n
unknown matrix from a subset of its noiseless (or noisy) entries. We define an
information-theoretic notion of completion capacity C that quantifies the
maximum number of entries that one observation of an entry can resolve. This
number provides the minimum number m of entries required for reliable
reconstruction: m=kn/C. Translating the problem into a distributed joint
source-channel coding problem with encoder restriction, we characterize the
completion capacity for a wide class of stochastic models of the unknown matrix
and the observation process. Our achievability proof is inspired by that of the
Slepian-Wolf theorem. For an arbitrary stochastic matrix, we derive an upper
bound on the completion capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4233</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4233</id><created>2014-02-18</created><authors><author><keyname>Shomer</keyname><forenames>Assaf</forenames></author></authors><title>On the Phase Space of Block-Hiding Strategies in Bitcoin-like networks</title><categories>cs.CR</categories><comments>28 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We calculate the probability of success of block-hiding mining strategies in
Bitcoin-like networks. These strategies involve building a secret branch of the
block-tree and publishing it opportunistically, aiming to replace the top of
the main branch and rip the reward associated with the secretly mined blocks.
We identify two types of block-hiding strategies and chart the parameter space
where those are more beneficial than the standard mining strategy described in
Nakamoto's paper. Our analysis suggests a generalization of the notion of the
relative hashing power as a measure for a miner's influence on the network.
Block-hiding strategies are beneficial only when this measure of influence
exceeds a certain threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4238</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4238</id><created>2014-02-18</created><updated>2014-06-22</updated><authors><author><keyname>Luo</keyname><forenames>Shixin</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Lim</keyname><forenames>Teng Joon</forenames></author></authors><title>Downlink and Uplink Energy Minimization Through User Association and
  Beamforming in Cloud RAN</title><categories>cs.IT math.IT</categories><comments>29 pages, 4 figures, submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cloud radio access network (C-RAN) concept, in which densely deployed
access points (APs) are empowered by cloud computing to cooperatively support
mobile users (MUs), to improve mobile data rates, has been recently proposed.
However, the high density of active (&quot;on&quot;) APs results in severe interference
and also inefficient energy consumption. Moreover, the growing popularity of
highly interactive applications with stringent uplink (UL) requirements, e.g.
network gaming and real-time broadcasting by wireless users, means that the UL
transmission is becoming more crucial and requires special attention. Therefore
in this paper, we propose a joint downlink (DL) and UL MU-AP association and
beamforming design to coordinate interference in the C-RAN for energy
minimization, a problem which is shown to be NP hard. Due to the new
consideration of UL transmission, it is shown that the two state-of-the-art
approaches for finding computationally efficient solutions of joint MU-AP
association and beamforming considering only the DL, i.e., group-sparse
optimization and relaxed-integer programming, cannot be modified in a
straightforward way to solve our problem. Leveraging on the celebrated UL-DL
duality result, we show that by establishing a virtual DL transmission for the
original UL transmission, the joint DL and UL optimization problem can be
converted to an equivalent DL problem in C-RAN with two inter-related
subproblems for the original and virtual DL transmissions, respectively. Based
on this transformation, two efficient algorithms for joint DL and UL MU-AP
association and beamforming design are proposed, whose performances are
evaluated and compared with other benchmarking schemes through extensive
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4246</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4246</id><created>2014-02-18</created><authors><author><keyname>Elliadka</keyname><forenames>Keshava M</forenames></author><author><keyname>Morelos-Zaragoza</keyname><forenames>Robert</forenames></author></authors><title>Precoding by Priority: A UEP Scheme for RaptorQ Codes</title><categories>cs.IT math.IT</categories><comments>5 Pages, 8 Figures, submitted to ISIT-2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Raptor codes are the first class of fountain codes with linear time encoding
and decoding. These codes are recommended in standards such as Third Generation
Partnership Project (3GPP) and digital video broadcasting. RaptorQ codes are an
extension to Raptor codes, having better coding efficiency and flexibility.
Standard Raptor and RaptorQ codes are systematic with equal error protection of
the data. However, in many applications such as MPEG transmission, there is a
need for Unequal Error Protection (UEP): namely, some data symbols require
higher error correction capabilities compared to others. We propose an approach
that we call Priority Based Precode Ratio (PBPR) to achieve UEP for systematic
RaptorQ and Raptor codes. Our UEP assumes that all symbols in a source block
belong to the same importance class. The UEP is achieved by changing the number
of precode symbols depending on the priority of the information symbols in the
source block. PBPR provides UEP with the same number of decoding overhead
symbols for source blocks with different importance classes. We demonstrate
consistent improvements in the error correction capability of higher importance
class compared to the lower importance class across the entire range of channel
erasure probabilities. We also show that PBPR does not result in a significant
increase in decoding and encoding times compared to the standard
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4247</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4247</id><created>2014-02-18</created><authors><author><keyname>Parq</keyname><forenames>Jae-Hyeon</forenames></author><author><keyname>Sevre</keyname><forenames>Erik</forenames></author><author><keyname>Lee</keyname><forenames>Sang-Mook</forenames></author></authors><title>Effects of Easy Hybrid Parallelization with CUDA for
  Numerical-Atomic-Orbital Density Functional Theory Calculation</title><categories>cs.DC</categories><comments>20 pages, 3 figures</comments><journal-ref>International Journal of Computer Applications, Volume 98, No.13,
  pp. 20-27 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We modified a MPI-friendly density functional theory (DFT) source code within
hybrid parallelization including CUDA. Our objective is to find out how simple
conversions within the hybrid parallelization with mid-range GPUs affect DFT
code not originally suitable to CUDA. We settled several rules of hybrid
parallelization for numerical-atomic-orbital (NAO) DFT codes. The test was
performed on a magnetite material system with OpenMX code by utilizing a
hardware system containing 2 Xeon E5606 CPUs and 2 Quadro 4000 GPUs. 3-way
hybrid routines obtained a speedup of 7.55 while 2-way hybrid speedup by 10.94.
GPUs with CUDA complement the efficiency of OpenMP and compensate CPUs'
excessive competition within MPI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4258</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4258</id><created>2014-02-18</created><authors><author><keyname>Sebastian</keyname><forenames>V. Bino</forenames></author><author><keyname>Unnikrishnan</keyname><forenames>A</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Kannan</forenames></author><author><keyname>Ramkumar</keyname><forenames>P. B</forenames></author></authors><title>Morphological filtering on hypergraphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this article is to develop computationally efficient
mathematical morphology operators on hypergraphs. To this aim we consider
lattice structures on hypergraphs on which we build morphological operators. We
develop a pair of dual adjunctions between the vertex set and the hyper edge
set of a hypergraph H, by defining a vertex-hyperedge correspondence. This
allows us to recover the classical notion of a dilation/erosion of a subset of
vertices and to extend it to subhypergraphs of H. Afterward, we propose several
new openings, closings, granulometries and alternate sequential filters acting
(i) on the subsets of the vertex and hyperedge set of H and (ii) on the
subhypergraphs of a hypergraph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4259</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4259</id><created>2014-02-18</created><authors><author><keyname>Marazzato</keyname><forenames>Roberto</forenames></author><author><keyname>Sparavigna</keyname><forenames>Amelia Carolina</forenames></author></authors><title>Extracting Networks of Characters and Places from Written Works with
  CHAPLIN</title><categories>cs.CY cs.CL</categories><comments>Keywords: Literary experiments, Networks, Graph Visualization
  Software, Text Data Analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are proposing a tool able to gather information on social networks from
narrative texts. Its name is CHAPLIN, CHAracters and PLaces Interaction
Network, implemented in VB.NET. Characters and places of the narrative works
are extracted in a list of raw words. Aided by the interface, the user selects
names out of them. After this choice, the tool allows the user to enter some
parameters, and, according to them, creates a network where the nodes are the
characters and places, and the edges their interactions. Edges are labelled by
performances. The output is a GV file, written in the DOT graph scripting
language, which is rendered by means of the free open source software Graphviz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4279</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4279</id><created>2014-02-18</created><updated>2015-03-06</updated><authors><author><keyname>Schuster</keyname><forenames>Ingmar</forenames></author></authors><title>A Bayesian Model of node interaction in networks</title><categories>cs.LG stat.ME stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We are concerned with modeling the strength of links in networks by taking
into account how often those links are used. Link usage is a strong indicator
of how closely two nodes are related, but existing network models in Bayesian
Statistics and Machine Learning are able to predict only wether a link exists
at all. As priors for latent attributes of network nodes we explore the Chinese
Restaurant Process (CRP) and a multivariate Gaussian with fixed dimensionality.
The model is applied to a social network dataset and a word coocurrence
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4280</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4280</id><created>2014-02-18</created><authors><author><keyname>Fern&#xe1;ndez</keyname><forenames>Alejandro</forenames></author><author><keyname>Garzaldeen</keyname><forenames>Badie</forenames></author><author><keyname>Gr&#xfc;tzner</keyname><forenames>Ines</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Guided Support for Collaborative Modeling, Enactment and Simulation of
  Software Development Processes</title><categories>cs.SE</categories><comments>8 pages. The final publication is available at
  http://onlinelibrary.wiley.com/doi/10.1002/spip.199/abstract</comments><journal-ref>International Journal on Software Process: Improvement and
  Practice, 9(2):95-106, 2004</journal-ref><doi>10.1002/spip.199</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the awareness of the importance of distributed software development
has been growing in the software engineering community. Economic constraints,
more and more outsourcing of development activities, and the increasing spatial
distribution of companies come along with challenges of how to organize
distributed development.
  In this article, we reason that a common process understanding is mandatory
for successful distributed development. Integrated process planning, guidance
and enactment are seen as enabling technologies to reach a unique process view.
  We sketch a synthesis of the software process modeling environment SPEARMINT
and the XCHIPS system for web-based process support. Hereby, planners and
developers are provided with collaborative planning and enactment support and
advanced process guidance via electronic process guides (EPGs). We describe the
usage of this integrated environment by using a case study for the development
of a learning system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4283</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4283</id><created>2014-02-18</created><authors><author><keyname>Chaudhari</keyname><forenames>P.</forenames></author><author><keyname>Rana</keyname><forenames>D. P.</forenames></author><author><keyname>Mehta</keyname><forenames>R. G.</forenames></author><author><keyname>Mistry</keyname><forenames>N. J.</forenames></author><author><keyname>Raghuwanshi</keyname><forenames>M. M.</forenames></author></authors><title>Discretization of Temporal Data: A Survey</title><categories>cs.DB cs.LG</categories><comments>4 pages, 1 Table</comments><journal-ref>International Journal of Computer Science and Information Security
  (IJCSIS), ISSN:1947-5500, Vol. 11, No. 2</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In real world, the huge amount of temporal data is to be processed in many
application areas such as scientific, financial, network monitoring, sensor
data analysis. Data mining techniques are primarily oriented to handle discrete
features. In the case of temporal data the time plays an important role on the
characteristics of data. To consider this effect, the data discretization
techniques have to consider the time while processing to resolve the issue by
finding the intervals of data which are more concise and precise with respect
to time. Here, this research is reviewing different data discretization
techniques used in temporal data applications according to the inclusion or
exclusion of: class label, temporal order of the data and handling of stream
data to open the research direction for temporal data discretization to improve
the performance of data mining technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4293</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4293</id><created>2014-02-18</created><authors><author><keyname>Davies</keyname><forenames>Alex</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>The Random Forest Kernel and other kernels for big data from random
  partitions</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Random Partition Kernels, a new class of kernels derived by
demonstrating a natural connection between random partitions of objects and
kernels between those objects. We show how the construction can be used to
create kernels from methods that would not normally be viewed as random
partitions, such as Random Forest. To demonstrate the potential of this method,
we propose two new kernels, the Random Forest Kernel and the Fast Cluster
Kernel, and show that these kernels consistently outperform standard kernels on
problems involving real-world datasets. Finally, we show how the form of these
kernels lend themselves to a natural approximation that is appropriate for
certain big data problems, allowing $O(N)$ inference in methods such as
Gaussian Processes, Support Vector Machines and Kernel PCA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4303</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4303</id><created>2014-02-18</created><updated>2016-03-02</updated><authors><author><keyname>Geist</keyname><forenames>Christian</forenames></author></authors><title>Finding Preference Profiles of Condorcet Dimension $k$ via SAT</title><categories>cs.MA cs.AI cs.LO</categories><comments>Corrected typos, updated references, and added conclusion</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Condorcet winning sets are a set-valued generalization of the well-known
concept of a Condorcet winner. As supersets of Condorcet winning sets are
always Condorcet winning sets themselves, an interesting property of preference
profiles is the size of the smallest Condorcet winning set they admit. This
smallest size is called the Condorcet dimension of a preference profile. Since
little is known about profiles that have a certain Condorcet dimension, we show
in this paper how the problem of finding a preference profile that has a given
Condorcet dimension can be encoded as a satisfiability problem and solved by a
SAT solver. Initial results include a minimal example of a preference profile
of Condorcet dimension 3, improving previously known examples both in terms of
the number of agents as well as alternatives. Due to the high complexity of
such problems it remains open whether a preference profile of Condorcet
dimension 4 exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4304</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4304</id><created>2014-02-18</created><updated>2014-04-24</updated><authors><author><keyname>Lloyd</keyname><forenames>James Robert</forenames></author><author><keyname>Duvenaud</keyname><forenames>David</forenames></author><author><keyname>Grosse</keyname><forenames>Roger</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joshua B.</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>Automatic Construction and Natural-Language Description of Nonparametric
  Regression Models</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the beginnings of an automatic statistician, focusing on
regression problems. Our system explores an open-ended space of statistical
models to discover a good explanation of a data set, and then produces a
detailed report with figures and natural-language text. Our approach treats
unknown regression functions nonparametrically using Gaussian processes, which
has two important consequences. First, Gaussian processes can model functions
in terms of high-level properties (e.g. smoothness, trends, periodicity,
changepoints). Taken together with the compositional structure of our language
of models this allows us to automatically describe functions in simple terms.
Second, the use of flexible nonparametric models and a rich language for
composing them in an open-ended manner also results in state-of-the-art
extrapolation performance evaluated over 13 real time series data sets from
various domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4306</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4306</id><created>2014-02-18</created><updated>2014-02-19</updated><authors><author><keyname>Shah</keyname><forenames>Amar</forenames></author><author><keyname>Wilson</keyname><forenames>Andrew Gordon</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>Student-t Processes as Alternatives to Gaussian Processes</title><categories>stat.ML cs.AI cs.LG stat.ME</categories><comments>13 pages, 6 figures, 1 table. To appear in &quot;The Seventeenth
  International Conference on Artificial Intelligence and Statistics (AISTATS),
  2014.&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the Student-t process as an alternative to the Gaussian
process as a nonparametric prior over functions. We derive closed form
expressions for the marginal likelihood and predictive distribution of a
Student-t process, by integrating away an inverse Wishart process prior over
the covariance kernel of a Gaussian process model. We show surprising
equivalences between different hierarchical Gaussian process models leading to
Student-t processes, and derive a new sampling scheme for the inverse Wishart
process, which helps elucidate these equivalences. Overall, we show that a
Student-t process can retain the attractive properties of a Gaussian process --
a nonparametric representation, analytic marginal and predictive distributions,
and easy model selection through covariance kernels -- but has enhanced
flexibility, and predictive covariances that, unlike a Gaussian process,
explicitly depend on the values of training observations. We verify empirically
that a Student-t process is especially useful in situations where there are
changes in covariance structure, or in applications like Bayesian optimization,
where accurate predictive covariances are critical for good performance. These
advantages come at no additional computational cost over Gaussian processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4308</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4308</id><created>2014-02-18</created><updated>2014-06-24</updated><authors><author><keyname>Kittichokechai</keyname><forenames>Kittipong</forenames></author><author><keyname>Oechtering</keyname><forenames>Tobias J.</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Lossy Source Coding with Reconstruction Privacy</title><categories>cs.IT math.IT</categories><comments>22 pages, added proofs, to be presented at ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of lossy source coding with side information under a
privacy constraint that the reconstruction sequence at a decoder should be kept
secret to a certain extent from another terminal such as an eavesdropper, a
sender, or a helper. We are interested in how the reconstruction privacy
constraint at a particular terminal affects the rate-distortion tradeoff. In
this work, we allow the decoder to use a random mapping, and give inner and
outer bounds to the rate-distortion-equivocation region for different cases
where the side information is available non-causally and causally at the
decoder. In the special case where each reconstruction symbol depends only on
the source description and current side information symbol, the complete
rate-distortion-equivocation region is provided. A binary example illustrating
a new tradeoff due to the new privacy constraint, and a gain from the use of a
stochastic decoder is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4310</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4310</id><created>2014-02-18</created><updated>2014-04-14</updated><authors><author><keyname>Lu</keyname><forenames>Jiyong</forenames></author><author><keyname>Guang</keyname><forenames>Xuan</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author></authors><title>Distributed Storage over Unidirectional Ring Networks</title><categories>cs.IT math.IT</categories><comments>28 pages, one column, 12 figures, submitted for possible publication.
  arXiv admin note: substantial text overlap with arXiv:1401.5168</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study distributed storage problems over unidirectional ring
networks, whose storage nodes form a directed ring and data is transmitted
along the same direction. The original data is distributed to store on these
nodes. Each user can connect one and only one storage node to download the
total data. A lower bound on the reconstructing bandwidth to recover the
original data for each user is proposed, and it is achievable for arbitrary
parameters. If a distributed storage scheme can achieve this lower bound with
equality for every user, we say it an optimal reconstructing distributed
storage scheme (ORDSS). Furthermore, the repair problem for a failed storage
node in ORDSSes is under consideration and a tight lower bound on the repair
bandwidth is obtained. In particular, we indicate the fact that for any ORDSS,
every storage node can be repaired with repair bandwidth achieving the lower
bound with equality. In addition, we present two constructions for ORDSSes of
arbitrary parameters, called MDS construction and ED construction,
respectively. Particularly, ED construction, by using the concept of Euclidean
division, is more efficient by our analysis in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4312</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4312</id><created>2014-02-18</created><updated>2014-04-16</updated><authors><author><keyname>Klauck</keyname><forenames>Hartmut</forenames></author><author><keyname>Podder</keyname><forenames>Supartha</forenames></author></authors><title>Two Results about Quantum Messages</title><categories>quant-ph cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show two results about the relationship between quantum and classical
messages. Our first contribution is to show how to replace a quantum message in
a one-way communication protocol by a deterministic message, establishing that
for all partial Boolean functions $f:\{0,1\}^n\times\{0,1\}^m\to\{0,1\}$ we
have $D^{A\to B}(f)\leq O(Q^{A\to B,*}(f)\cdot m)$. This bound was previously
known for total functions, while for partial functions this improves on results
by Aaronson, in which either a log-factor on the right hand is present, or the
left hand side is $R^{A\to B}(f)$, and in which also no entanglement is
allowed.
  In our second contribution we investigate the power of quantum proofs over
classical proofs. We give the first example of a scenario, where quantum proofs
lead to exponential savings in computing a Boolean function. The previously
only known separation between the power of quantum and classical proofs is in a
setting where the input is also quantum.
  We exhibit a partial Boolean function $f$, such that there is a one-way
quantum communication protocol receiving a quantum proof (i.e., a protocol of
type QMA) that has cost $O(\log n)$ for $f$, whereas every one-way quantum
protocol for $f$ receiving a classical proof (protocol of type QCMA) requires
communication $\Omega(\sqrt n/\log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4314</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4314</id><created>2014-02-18</created><authors><author><keyname>Dombek</keyname><forenames>Daniel</forenames></author><author><keyname>Mas&#xe1;kov&#xe1;</keyname><forenames>Zuzana</forenames></author><author><keyname>V&#xe1;vra</keyname><forenames>Tom&#xe1;&#x161;</forenames></author></authors><title>Confluent Parry numbers, their spectra, and integers in positive- and
  negative-base number systems</title><categories>math.CO cs.DM</categories><comments>22pp</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the expansions of real numbers in positive and
negative real base as introduced by R\'enyi, and Ito &amp; Sadahiro, respectively.
In particular, we compare the sets $\mathbb{Z}_\beta^+$ and
$\mathbb{Z}_{-\beta}$ of nonnegative $\beta$-integers and $(-\beta)$-integers.
We describe all bases $(\pm\beta)$ for which $\mathbb{Z}_\beta^+$ and
$\mathbb{Z}_{-\beta}$ can be coded by infinite words which are fixed points of
conjugated morphisms, and consequently have the same language. Moreover, we
prove that this happens precisely for $\beta$ with another interesting
property, namely that any integer linear combination of non-negative powers of
the base $-\beta$ with coefficients in $\{0,1,\dots,\lfloor\beta\rfloor\}$ is a
$(-\beta)$-integer, although the corresponding sequence of digits is forbidden
as a $(-\beta)$-integer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4320</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4320</id><created>2014-02-18</created><authors><author><keyname>Wang</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Gobbo</keyname><forenames>Federico</forenames></author><author><keyname>Lane</keyname><forenames>Michael</forenames></author></authors><title>Turning Time from Enemy into an Ally Using the Pomodoro Technique</title><categories>cs.SE</categories><comments>18 pages, 4 figures, 2 tables, book chapter. Agility Across Time and
  Space (Smite et al. ed.), 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time is one of the most important factors dominating agile software
development processes in distributed settings. Effective time management helps
agile teams to plan and monitor the work to be performed, and create and
maintain a fast yet sustainable pace. The Pomodoro Technique is one promising
time management technique. Its application and adaptation in Sourcesense Milan
Team surfaced various benefits, challenges and implications for distributed
agile software development. Lessons learnt from the experiences of Sourcesense
Milan Team can be useful for other distributed agile teams to turn time from
enemy into an ally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4322</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4322</id><created>2014-02-18</created><authors><author><keyname>Mart&#xed;nez-P&#xe9;rez</keyname><forenames>A.</forenames></author></authors><title>On the properties of $\alpha$-unchaining single linkage hierarchical
  clustering</title><categories>cs.LG</categories><comments>14 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:1210.6292</comments><msc-class>62H30, 68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the election of a hierarchical clustering method, theoretic properties may
give some insight to determine which method is the most suitable to treat a
clustering problem. Herein, we study some basic properties of two hierarchical
clustering methods: $\alpha$-unchaining single linkage or $SL(\alpha)$ and a
modified version of this one, $SL^*(\alpha)$. We compare the results with the
properties satisfied by the classical linkage-based hierarchical clustering
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4325</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4325</id><created>2014-02-18</created><authors><author><keyname>Ma</keyname><forenames>Athen</forenames></author><author><keyname>Mondragon</keyname><forenames>Raul J</forenames></author></authors><title>Rich-cores in networks</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 6 figures</comments><doi>10.1371/journal.pone.0119678</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A core is said to be a group of central and densely connected nodes which
governs the overall behavior of a network. Profiling this meso--scale structure
currently relies on a limited number of methods which are often complex, and
have scalability issues when dealing with very large networks. As a result, we
are yet to fully understand its impact on network properties and dynamics. Here
we introduce a simple method to profile this structure by combining the
concepts of core/periphery and rich-club. The key challenge in addressing such
association of the two concepts is to establish a way to define the membership
of the core. The notion of a &quot;rich-club&quot; describes nodes which are essentially
the hub of a network, as they play a dominating role in structural and
functional properties. Interestingly, the definition of a rich-club naturally
emphasizes high degree nodes and divides a network into two subgroups. Our
approach theoretically couples the underlying principle of a rich-club with the
escape time of a random walker, and a rich-core is defined by examining changes
in the associated persistence probability. The method is fast and scalable to
large networks. In particular, we successfully show that the evolution of the
core in \emph{C. elegans} and World Trade networks correspond to key
development stages and responses to historical events respectively.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="56000" completeListSize="102538">1122234|57001</resumptionToken>
</ListRecords>
</OAI-PMH>
