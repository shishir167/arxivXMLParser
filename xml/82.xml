<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:51:23Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|81001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04596</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04596</id><created>2015-07-16</created><authors><author><keyname>Aledavood</keyname><forenames>Talayeh</forenames></author><author><keyname>L&#xf3;pez</keyname><forenames>Eduardo</forenames></author><author><keyname>Roberts</keyname><forenames>Sam G. B.</forenames></author><author><keyname>Reed-Tsochas</keyname><forenames>Felix</forenames></author><author><keyname>Moro</keyname><forenames>Esteban</forenames></author><author><keyname>Dunbar</keyname><forenames>Robin I. M.</forenames></author><author><keyname>Saram&#xe4;ki</keyname><forenames>Jari</forenames></author></authors><title>Channel-Specific Daily Patterns in Mobile Phone Communication</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans follow circadian rhythms, visible in their activity levels as well as
physiological and psychological factors. Such rhythms are also visible in
electronic communication records, where the aggregated activity levels of e.g.
mobile telephone calls or Wikipedia edits are known to follow their own daily
patterns. Here, we study the daily communication patterns of 24 individuals
over 18 months, and show that each individual has a different, persistent
communication pattern. These patterns may differ for calls and text messages,
which points towards calls and texts serving a different role in communication.
For both calls and texts, evenings play a special role. There are also
differences in the daily patterns of males and females both for calls and
texts, both in how they communicate with individuals of the same gender vs.
opposite gender, and also in how communication is allocated at social ties of
different nature (kin ties vs. non-kin ties). Taken together, our results show
that there is an unexpected richness to the daily communication patterns, from
different types of ties being activated at different times of day to different
roles of communication channels and gender differences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04603</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04603</id><created>2015-07-16</created><authors><author><keyname>Gao</keyname><forenames>Xinyu</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Turbo-Like Beamforming Based on Tabu Search Algorithm for
  Millimeter-Wave Massive MIMO Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For millimeter-wave (mmWave) massive MIMO systems, the codebook-based analog
beamforming (including transmit precoding and receive combining) is usually
used to compensate the severe attenuation of mmWave signals. However,
conventional beamforming schemes involve complicated search among pre-defined
codebooks to find out the optimal pair of analog precoder and analog combiner.
To solve this problem, by exploring the idea of turbo equalizer together with
tabu search (TS) algorithm, we propose a Turbo-like beamforming scheme based on
TS, which is called Turbo-TS beamforming in this paper, to achieve the
near-optimal performance with low complexity. Specifically, the proposed
Turbo-TS beamforming scheme is composed of the following two key components: 1)
Based on the iterative information exchange between the base station and the
user, we design a Turbo-like joint search scheme to find out the near-optimal
pair of analog precoder and analog combiner; 2) Inspired by the idea of TS
algorithm developed in artificial intelligence, we propose a TS-based
precoding/combining scheme to intelligently search the best precoder/combiner
in each iteration of Turbo-like joint search with low complexity. Analysis
shows that the proposed Turbo-TS beamforming can considerably reduce the
searching complexity, and simulation results verify that it can achieve the
near-optimal performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04606</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04606</id><created>2015-07-16</created><authors><author><keyname>Chen</keyname><forenames>Mingming</forenames></author><author><keyname>Kuzmin</keyname><forenames>Konstantin</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author></authors><title>Extension of Modularity Density for Overlapping Community Structure</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages in Advances in Social Networks Analysis and Mining (ASONAM),
  2014 IEEE/ACM International Conference on</comments><doi>10.1109/ASONAM.2014.6921686</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modularity is widely used to effectively measure the strength of the disjoint
community structure found by community detection algorithms. Although several
overlapping extensions of modularity were proposed to measure the quality of
overlapping community structure, there is lack of systematic comparison of
different extensions. To fill this gap, we overview overlapping extensions of
modularity to select the best. In addition, we extend the Modularity Density
metric to enable its usage for overlapping communities. The experimental
results on four real networks using overlapping extensions of modularity,
overlapping modularity density, and six other community quality metrics show
that the best results are obtained when the product of the belonging
coefficients of two nodes is used as the belonging function. Moreover, our
experiments indicate that overlapping modularity density is a better measure of
the quality of overlapping community structure than other metrics considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04608</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04608</id><created>2015-07-16</created><authors><author><keyname>Gaspar</keyname><forenames>Ivan</forenames></author><author><keyname>Mendes</keyname><forenames>Luciano</forenames></author><author><keyname>Matth&#xe9;</keyname><forenames>Maximilian</forenames></author><author><keyname>Michailow</keyname><forenames>Nicola</forenames></author><author><keyname>Zhang</keyname><forenames>Dan</forenames></author><author><keyname>Albertiy</keyname><forenames>Antonio</forenames></author><author><keyname>Fettweis</keyname><forenames>Gerhard</forenames></author></authors><title>GFDM - A Framework for Virtual PHY Services in 5G Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The next generation of wireless networks will face different challenges from
new scenarios. The main contribution of this paper is to show that Generalized
Frequency Division Multiplexing (GFDM), as a baseline of flexible circular
filtered multicarrier systems, can be used as a framework to virtualize the PHY
service for the upper layers of 5G networks. This framework opens the
possibility to apply software-defined network principles to produce
software-defined waveforms capable of addressing the requirements of future
mobile networks. Hence, a block oriented concept will be used to provide the
modulation service, emulating different flavors of waveforms designed to go
beyond the well-established Orthogonal Frequency Division Multiplexing (OFDM)
principles, in scenarios where they perform best. The virtual physical layer
(PHY) service opens the opportunity to have a fast and dynamic evolution of the
infrastructure, as applications change over time. The presented unified
modulation concept contributes with future research directions to address burst
and continuous transmissions, referencing basic approaches for synchronization
and advanced receiver design that can be exploited in future for the whole
frame structure design and channel estimation strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04614</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04614</id><created>2015-07-16</created><updated>2015-07-19</updated><authors><author><keyname>Hartig</keyname><forenames>Olaf</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Jorge</forenames></author></authors><title>LDQL: A Query Language for the Web of Linked Data (Extended Version)</title><categories>cs.DB</categories><comments>39 pages, Extended version of a paper published in ISWC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Web of Linked Data is composed of tons of RDF documents interlinked to
each other forming a huge repository of distributed semantic data. Effectively
querying this distributed data source is an important open problem in the
Semantic Web area. In this paper, we propose LDQL, a declarative language to
query Linked Data on the Web. One of the novelties of LDQL is that it expresses
separately (i) patterns that describe the expected query result, and (ii) Web
navigation paths that select the data sources to be used for computing the
result. We present a formal syntax and semantics, prove equivalence rules, and
study the expressiveness of the language. In particular, we show that LDQL is
strictly more expressive than the query formalisms that have been proposed
previously for Linked Data on the Web. The high expressiveness allows LDQL to
define queries for which a complete execution is not computationally feasible
over the Web. We formally study this issue and provide a syntactic sufficient
condition to avoid this problem; queries satisfying this condition are ensured
to have a procedure to be effectively evaluated over the Web of Linked Data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04618</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04618</id><created>2015-07-16</created><authors><author><keyname>Shen</keyname><forenames>Wenqian</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Shi</keyname><forenames>Yi</forenames></author><author><keyname>Zhu</keyname><forenames>Xudong</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Compressive sensing based differential channel feedback for massive MIMO</title><categories>cs.IT math.IT</categories><journal-ref>Electronics Letters, vol. 51, no. 22, pp. 1824-1826, Oct. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multiple-input multiple-output (MIMO) is becoming a key technology
for future 5G wireless communications. Channel feedback for massive MIMO is
challenging due to the substantially increased dimension of MIMO channel
matrix. In this letter, we propose a compressive sensing (CS) based
differential channel feedback scheme to reduce the feedback overhead.
Specifically, the temporal correlation of time-varying channels is exploited to
generate the differential channel impulse response (CIR) between two CIRs in
neighboring time slots, which enjoys a much stronger sparsity than the original
sparse CIRs. Thus, the base station can recover the differential CIR from the
highly compressed differential CIR under the framework of CS theory.
Simulations show that the proposed scheme reduces the feedback overhead by
about 20\% compared with the direct CS-based scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04624</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04624</id><created>2015-07-16</created><authors><author><keyname>Grabow</keyname><forenames>Carsten</forenames></author><author><keyname>Grosskinsky</keyname><forenames>Stefan</forenames></author><author><keyname>Kurths</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Timme</keyname><forenames>Marc</forenames></author></authors><title>Collective Relaxation Dynamics of Small-World Networks</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>12 pages, 10 figures, published in PRE</comments><journal-ref>Physical Review E 91, 052815 (2015)</journal-ref><doi>10.1103/PhysRevE.91.052815</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks exhibit a wide range of collective dynamic phenomena,
including synchronization, diffusion, relaxation, and coordination processes.
Their asymptotic dynamics is generically characterized by the local Jacobian,
graph Laplacian or a similar linear operator. The structure of networks with
regular, small-world and random connectivities are reasonably well understood,
but their collective dynamical properties remain largely unknown. Here we
present a two-stage mean-field theory to derive analytic expressions for
network spectra. A single formula covers the spectrum from regular via
small-world to strongly randomized topologies in Watts-Strogatz networks,
explaining the simultaneous dependencies on network size N, average degree k
and topological randomness q. We present simplified analytic predictions for
the second largest and smallest eigenvalue, and numerical checks confirm our
theoretical predictions for zero, small and moderate topological randomness q,
including the entire small-world regime. For large q of the order of one, we
apply standard random matrix theory thereby overarching the full range from
regular to randomized network topologies. These results may contribute to our
analytic and mechanistic understanding of collective relaxation phenomena of
network dynamical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04630</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04630</id><created>2015-07-16</created><authors><author><keyname>Bonatti</keyname><forenames>Piero Andrea</forenames></author><author><keyname>Petrova</keyname><forenames>Iliana Mineva</forenames></author><author><keyname>Sauro</keyname><forenames>Luigi</forenames></author></authors><title>Optimizing the computation of overriding</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce optimization techniques for reasoning in DLN---a recently
introduced family of nonmonotonic description logics whose characterizing
features appear well-suited to model the applicative examples naturally arising
in biomedical domains and semantic web access control policies. Such
optimizations are validated experimentally on large KBs with more than 30K
axioms. Speedups exceed 1 order of magnitude. For the first time, response
times compatible with real-time reasoning are obtained with nonmonotonic KBs of
this size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04631</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04631</id><created>2015-07-16</created><authors><author><keyname>Shekaramiz</keyname><forenames>Alireza</forenames></author><author><keyname>Liebeherr</keyname><forenames>Jorg</forenames></author><author><keyname>Burchard</keyname><forenames>Almut</forenames></author></authors><title>Window Flow Control Systems with Random Service</title><categories>cs.PF cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an extension of the window flow control analysis by R. Agrawal
et.al. (Reference [1]), C.-S. Chang (Reference [6]), and C.-S. Chang et. al.
(Reference [8]) to a system with random service time and fixed feedback delay.
We consider two network service models. In the first model, the network service
process itself has no time correlations. The second model addresses a two-state
Markov-modulated service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04635</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04635</id><created>2015-07-16</created><updated>2015-12-21</updated><authors><author><keyname>van de Meent</keyname><forenames>Jan-Willem</forenames></author><author><keyname>Tolpin</keyname><forenames>David</forenames></author><author><keyname>Paige</keyname><forenames>Brooks</forenames></author><author><keyname>Wood</keyname><forenames>Frank</forenames></author></authors><title>Black-Box Policy Search with Probabilistic Programs</title><categories>stat.ML cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we explore how probabilistic programs can be used to represent
policies in sequential decision problems. In this formulation, a probabilistic
program is a black-box stochastic simulator for both the problem domain and the
agent. We relate classic policy gradient techniques to recently introduced
black-box variational methods which generalize to probabilistic program
inference. We present case studies in the Canadian traveler problem, Rock
Sample, and a benchmark for optimal diagnosis inspired by Guess Who. Each study
illustrates how programs can efficiently represent policies using moderate
numbers of parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04642</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04642</id><created>2015-07-16</created><updated>2016-02-22</updated><authors><author><keyname>Such</keyname><forenames>Jose M.</forenames></author><author><keyname>Criado</keyname><forenames>Natalia</forenames></author></authors><title>Resolving Multi-party Privacy Conflicts in Social Media</title><categories>cs.SI</categories><comments>Authors' version of the paper accepted for publication at IEEE
  Transactions on Knowledge and Data Engineering, IEEE Transactions on
  Knowledge and Data Engineering, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Items shared through Social Media may affect more than one user's privacy ---
e.g., photos that depict multiple users, comments that mention multiple users,
events in which multiple users are invited, etc. The lack of multi-party
privacy management support in current mainstream Social Media infrastructures
makes users unable to appropriately control to whom these items are actually
shared or not. Computational mechanisms that are able to merge the privacy
preferences of multiple users into a single policy for an item can help solve
this problem. However, merging multiple users' privacy preferences is not an
easy task, because privacy preferences may conflict, so methods to resolve
conflicts are needed. Moreover, these methods need to consider how users' would
actually reach an agreement about a solution to the conflict in order to
propose solutions that can be acceptable by all of the users affected by the
item to be shared. Current approaches are either too demanding or only consider
fixed ways of aggregating privacy preferences. In this paper, we propose the
first computational mechanism to resolve conflicts for multi-party privacy
management in Social Media that is able to adapt to different situations by
modelling the concessions that users make to reach a solution to the conflicts.
We also present results of a user study in which our proposed mechanism
outperformed other existing approaches in terms of how many times each approach
matched users' behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04645</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04645</id><created>2015-07-16</created><authors><author><keyname>Chakrabarti</keyname><forenames>Amit</forenames></author><author><keyname>Wirth</keyname><forenames>Anthony</forenames></author></authors><title>Incidence Geometries and the Pass Complexity of Semi-Streaming Set Cover</title><categories>cs.CC cs.DS</categories><comments>20 pages</comments><msc-class>68Q17, 05B25, 51E30, 68W25</msc-class><acm-class>F.2.2; F.2.3; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Set cover, over a universe of size $n$, may be modelled as a data-streaming
problem, where the $m$ sets that comprise the instance are to be read one by
one. A semi-streaming algorithm is allowed only $O(n\, \mathrm{poly}\{\log n,
\log m\})$ space to process this stream. For each $p \ge 1$, we give a very
simple deterministic algorithm that makes $p$ passes over the input stream and
returns an appropriately certified $(p+1)n^{1/(p+1)}$-approximation to the
optimum set cover. More importantly, we proceed to show that this approximation
factor is essentially tight, by showing that a factor better than
$0.99\,n^{1/(p+1)}/(p+1)^2$ is unachievable for a $p$-pass semi-streaming
algorithm, even allowing randomisation. In particular, this implies that
achieving a $\Theta(\log n)$-approximation requires $\Omega(\log n/\log\log n)$
passes, which is tight up to the $\log\log n$ factor. These results extend to a
relaxation of the set cover problem where we are allowed to leave an
$\varepsilon$ fraction of the universe uncovered: the tight bounds on the best
approximation factor achievable in $p$ passes turn out to be
$\Theta_p(\min\{n^{1/(p+1)}, \varepsilon^{-1/p}\})$. Our lower bounds are based
on a construction of a family of high-rank incidence geometries, which may be
thought of as vast generalisations of affine planes. This construction, based
on algebraic techniques, appears flexible enough to find other applications and
is therefore interesting in its own right.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04646</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04646</id><created>2015-07-16</created><authors><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Wei</keyname><forenames>Furu</forenames></author><author><keyname>Li</keyname><forenames>Sujian</forenames></author><author><keyname>Ji</keyname><forenames>Heng</forenames></author><author><keyname>Zhou</keyname><forenames>Ming</forenames></author><author><keyname>Wang</keyname><forenames>Houfeng</forenames></author></authors><title>A Dependency-Based Neural Network for Relation Classification</title><categories>cs.CL cs.LG cs.NE</categories><comments>This preprint is the full version of a short paper accepted in the
  annual meeting of the Association for Computational Linguistics (ACL) 2015
  (Beijing, China)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous research on relation classification has verified the effectiveness
of using dependency shortest paths or subtrees. In this paper, we further
explore how to make full use of the combination of these dependency
information. We first propose a new structure, termed augmented dependency path
(ADP), which is composed of the shortest dependency path between two entities
and the subtrees attached to the shortest path. To exploit the semantic
representation behind the ADP structure, we develop dependency-based neural
networks (DepNN): a recursive neural network designed to model the subtrees,
and a convolutional neural network to capture the most important features on
the shortest path. Experiments on the SemEval-2010 dataset show that our
proposed method achieves state-of-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04657</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04657</id><created>2015-07-16</created><authors><author><keyname>Singh</keyname><forenames>Rahul</forenames></author><author><keyname>Ma</keyname><forenames>Ke</forenames></author><author><keyname>Thatte</keyname><forenames>Anupam</forenames></author><author><keyname>Kumar</keyname><forenames>P. R.</forenames></author><author><keyname>Xie</keyname><forenames>Le</forenames></author></authors><title>A Theory for the Operation of the Independent System Operator in a Smart
  Grid with Stochastic Renewables, Demand Response and Storage</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address a key issue of designing architectures and
algorithms which generate optimal demand response in a decentralized manner for
a smart-grid consisting of several stochastic renewables and dynamic loads. By
optimal demand response, we refer to the demand response which maximizes the
utility of the agents connected to the smart-grid. By decentralized we refer to
the desirable case where neither the independent system operator (ISO) needs to
know the dynamics/utilities of the agents, nor do the agents need to have a
knowledge of the dynamics/utilities of other agents connected to the grid. The
communication between the ISO and agents is restricted to the ISO announcing a
pricing policy and the agents responding with their energy
generation/consumption bids in response to the pricing policy.
  We provide a complete solution for both the deterministic and stochastic
cases. It features a price iteration scheme that results in optimality of
social welfare. We also provide an optimal solution for the case where there is
a common randomness affecting and observed by all agents. This solution can be
computationally complex, and we pose approximations. For the more general
partially observed randomness case, we exhibit a relaxation that significantly
reduces complexity. We also provide an approximation strategy that leads to a
model predictive control (MPC) approach. Simulation results comparing the
resulting optimal demand response with the existing architectures employed by
the ISO illustrate the benefit in social welfare utility realized by our
scheme. To the best of the authors' knowledge, this is the first work of its
kind to explicitly mark out the optimal response of dynamic demand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04658</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04658</id><created>2015-07-16</created><authors><author><keyname>Park</keyname><forenames>Jihong</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author><author><keyname>Zander</keyname><forenames>Jens</forenames></author></authors><title>Tractable Resource Management in Millimeter-Wave Overlaid Ultra-Dense
  Cellular Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>submitted to IEEE Globecom 2015 Workshop on 5G &amp; Beyond (6 pages, 5
  figures, and 1 table)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What does millimeter-wave (mmW) seek assistance for from micro-wave ({\mu}W)
in a mmW overlaid 5G cellular network? This paper raises the question of
whether to complement downlink (DL) or uplink (UL) transmissions, and concludes
that {\mu}W should aid UL more. Such dedication to UL results from the low mmW
UL rate due to high peak-to-average power ratio (PAPR) at mobile users. The
DL/UL allocations are tractably provided based on a novel closed-form mm-{\mu}W
spectral efficiency (SE) derivation via stochastic geometry. The findings
explicitly indicate: (i) both DL/UL mmW (or {\mu}W) SEs coincidentally converge
on the same value in an ultra-dense cellular network (UDN) and (ii) such a mmW
(or {\mu}W) UDN SE is a logarithmic function of BS-to-user density ratio. The
corresponding mm-{\mu}W resource management is evaluated by utilizing a three
dimensional (3D) blockage model with real geography in Seoul, Korea.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04674</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04674</id><created>2015-07-16</created><authors><author><keyname>Chekuri</keyname><forenames>Chandra</forenames></author><author><keyname>Madan</keyname><forenames>Vivek</forenames></author></authors><title>Simple and Fast Rounding Algorithms for Directed and Node-weighted
  Multiway Cut</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Directed Multiway Cut(Dir-MC) the input is an edge-weighted directed graph
$G=(V,E)$ and a set of $k$ terminal nodes $\{s_1,s_2,\ldots,s_k\} \subseteq V$;
the goal is to find a min-weight subset of edges whose removal ensures that
there is no path from $s_i$ to $s_j$ for any $i \neq j$. In Node-weighted
Multiway Cut(Node-MC) the input is a node-weighted undirected graph $G$ and a
set of $k$ terminal nodes $\{s_1,s_2,\ldots,s_k\} \subseteq V$; the goal is to
remove a min-weight subset of nodes to disconnect each pair of terminals.
Dir-MC admits a $2$-approximation [Naor, Zosin '97] and Node-MC admits a
$2(1-\frac{1}{k})$-approximation [Garg, Vazirani, Yannakakis '94], both via
rounding of LP relaxations. Previous rounding algorithms for these problems,
from nearly twenty years ago, are based on careful rounding of an &quot;optimum&quot;
solution to an LP relaxation. This is particularly true for Dir-MC for which
the rounding relies on a custom LP formulation instead of the natural distance
based LP relaxation [Naor, Zosin '97].
  In this paper we describe extremely simple and near linear-time rounding
algorithms for Dir-MC and Node-MC via a natural distance based LP relaxation.
The dual of this relaxation is a special case of the maximum multicommodity
flow problem. Our algorithms achieve the same bounds as before but have the
significant advantage in that they can work with &quot;any feasible&quot; solution to the
relaxation. Consequently, in addition to obtaining &quot;book&quot; proofs of LP rounding
for these two basic problems, we also obtain significantly faster approximation
algorithms by taking advantage of known algorithms for computing near-optimal
solutions for maximum multicommodity flow problems. We also investigate lower
bounds for Dir-MC when $k=2$ and in particular prove that the integrality gap
of the LP relaxation is $2$ even in directed planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04678</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04678</id><created>2015-07-13</created><authors><author><keyname>Kahle</keyname><forenames>Reinhard</forenames></author><author><keyname>Keller</keyname><forenames>Wilfried</forenames></author></authors><title>Syntax versus Semantics</title><categories>math.HO cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>F.4.1; F.4.m</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We report on the idea to use colours to distinguish syntax and semantics as
an educational tool in logic classes. This distinction gives also reason to
reflect on some philosophical issues concerning semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04680</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04680</id><created>2015-07-16</created><authors><author><keyname>J</keyname><forenames>Jeya Pradha</forenames></author><author><keyname>Kalamkar</keyname><forenames>Sanket S.</forenames></author><author><keyname>Banerjee</keyname><forenames>Adrish</forenames></author></authors><title>On Information and Energy Cooperation in Energy Harvesting Cognitive
  Radio</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 4 figures, to be presented in IEEE PIMRC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the cooperation between primary and secondary users at
information and energy levels when both users are energy harvesting nodes. In
particular, a secondary transmitter helps relaying the primary message, and in
turn, gains the spectrum access as a reward. Also, the primary transmitter
supplies energy to the secondary transmitter if the latter is
energy-constrained, which facilitates an uninterrupted cooperation. We address
this two-level cooperation over a finite horizon with the finite battery
constraint at the secondary transmitter. While promising the rate-guaranteed
service to both primary and secondary users, we aim to maximize the primary
rate. We develop an iterative algorithm that obtains the optimal offline power
policies for primary and secondary users. To acquire insights about the
structure of the optimal solution, we examine specific scenarios. Furthermore,
we investigate the effects of the secondary rate constraint and finite battery
on the primary rate and the probability of cooperation. We show that the joint
information and energy cooperation increases the chances of cooperation and
achieves significant rate gains over only information cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04701</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04701</id><created>2015-07-16</created><authors><author><keyname>Urtubey</keyname><forenames>Luis</forenames></author></authors><title>To Teach Modal Logic: An Opinionated Survey</title><categories>math.HO cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  I aim to promote an alternative agenda for teaching modal logic chiefly
inspired by the relationships between modal logic and philosophy. The guiding
idea for this proposal is a reappraisal of the interest of modal logic in
philosophy, which do not stem mainly from mathematical issues, but which is
motivated by central problems of philosophy and language. I will point out some
themes to start elaborating a guide for a more comprehensive approach to teach
modal logic, and consider the contributions of dual-process theories in
cognitive science, in order to explore a pedagogical framework for the proposed
point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04717</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04717</id><created>2015-07-16</created><updated>2016-03-07</updated><authors><author><keyname>Rudi</keyname><forenames>Alessandro</forenames></author><author><keyname>Camoriano</keyname><forenames>Raffaello</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author></authors><title>Less is More: Nystr\&quot;om Computational Regularization</title><categories>stat.ML cs.LG</categories><comments>NIPS 2015 (oral)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study Nystr\&quot;om type subsampling approaches to large scale kernel methods,
and prove learning bounds in the statistical learning setting, where random
sampling and high probability estimates are considered. In particular, we prove
that these approaches can achieve optimal learning bounds, provided the
subsampling level is suitably chosen. These results suggest a simple
incremental variant of Nystr\&quot;om Kernel Regularized Least Squares, where the
subsampling level implements a form of computational regularization, in the
sense that it controls at the same time regularization and computations.
Extensive experimental analysis shows that the considered approach achieves
state of the art performances on benchmark large scale datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04720</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04720</id><created>2015-07-16</created><authors><author><keyname>Marzolla</keyname><forenames>Moreno</forenames></author></authors><title>Quantitative Analysis of the Italian National Scientific Qualification
  Part II: applicant profiles and final reports</title><categories>cs.DL</categories><msc-class>62P99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The National Scientific Qualification (ASN) has been introduced as a
prerequisite for applying for a tenured associate or full professor position at
state-recognized Italian universities. The ASN is meant to attest that an
individual has reached a suitable level of scientific maturity to apply for
professorship positions. A controversial aspect of the ASN is its reliance on
quantitative indicators as one of the parameters that must be used to assess
applicants. In a previous works we have undertaken a comprehensive analysis of
the publicly available results, focusing on the quantitative indicators of
applicants. Here we complete the analysis by focusing on the content of the
application forms and of the final reports that contain a written assessment of
each applicant by a five-member panel. The application forms provide useful
information about the age distribution of applicants, and the distribution of
publication types and other qualifications for each scientific discipline.
Furthermore, analysis of co-qualifications (i.e., the same individual getting
multiple qualification) unveils the relationships and partial overlaps that
exist between disciplines. The content of final reports are evaluated through
simple metrics to understand whether they provide useful feedback to the
applicants. In the final part of this work we contrast the methodology used in
the National Scientific Qualification (ASN) with the best practices that are
recommended for assessment of individual researchers. We argue that the ASN
shows serious deficiencies that should be addressed in future rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04727</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04727</id><created>2015-07-16</created><authors><author><keyname>Sheikhattar</keyname><forenames>Alireza</forenames></author><author><keyname>Fritz</keyname><forenames>Jonathan B.</forenames></author><author><keyname>Shamma</keyname><forenames>Shihab A.</forenames></author><author><keyname>Babadi</keyname><forenames>Behtash</forenames></author></authors><title>Recursive Sparse Point Process Regression with Application to
  Spectrotemporal Receptive Field Plasticity Analysis</title><categories>cs.NE cs.SY math.OC stat.AP stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the sparse time-varying parameter
vectors of a point process model in an online fashion, where the observations
and inputs respectively consist of binary and continuous time series. We
construct a novel objective function by incorporating a forgetting factor
mechanism into the point process log-likelihood to enforce adaptivity and
employ $\ell_1$-regularization to capture the sparsity. We provide a rigorous
analysis of the maximizers of the objective function, which extends the
guarantees of compressed sensing to our setting. We construct two recursive
filters for online estimation of the parameter vectors based on proximal
optimization techniques, as well as a novel filter for recursive computation of
statistical confidence regions. Simulation studies reveal that our algorithms
outperform several existing point process filters in terms of trackability,
goodness-of-fit and mean square error. We finally apply our filtering
algorithms to experimentally recorded spiking data from the ferret primary
auditory cortex during attentive behavior in a click rate discrimination task.
Our analysis provides new insights into the time-course of the spectrotemporal
receptive field plasticity of the auditory neurons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04734</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04734</id><created>2015-07-16</created><authors><author><keyname>Jalali</keyname><forenames>Amin</forenames></author><author><keyname>Xiao</keyname><forenames>Lin</forenames></author><author><keyname>Fazel</keyname><forenames>Maryam</forenames></author></authors><title>Variational Gram Functions: Convex Analysis and Optimization</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new class of convex penalty functions, called variational Gram
functions (VGFs), that can promote pairwise relations, such as orthogonality,
among a set of vectors in a vector space. When used as regularizers in convex
optimization problems, these functions find application in hierarchical
classification, multitask learning, and estimation of vectors with disjoint
supports, among other applications. We describe a general condition for
convexity, which is then used to prove the convexity of a few known functions
as well as some new ones. We give a characterization of the associated
subdifferential and the proximal operator, and discuss efficient optimization
algorithms for some structured regularized loss-minimization problems using
VGFs. Numerical experiments on a hierarchical classification problem are also
presented that demonstrate the effectiveness of VGFs and the associated
optimization algorithms in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04739</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04739</id><created>2015-07-16</created><updated>2015-11-05</updated><authors><author><keyname>Lelarge</keyname><forenames>Marc</forenames></author></authors><title>Counting matchings in irregular bipartite graphs and random lifts</title><categories>math.CO cs.DM math-ph math.MP</categories><comments>26 pages, extended version (results for random lifts and more related
  work)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a sharp lower bound on the number of matchings of a given size in a
bipartite graph. When specialized to regular bipartite graphs, our results
imply Friedland's Lower Matching Conjecture and Schrijver's theorem proven by
Gurvits and Csikvari. Indeed, our work extends the recent work of Csikvari done
for regular and bi-regular bipartite graphs. Moreover, our lower bounds are
order optimal as they are attained for a sequence of $2$-lifts of the original
graph as well as for random $n$-lifts of the original graph when $n$ tends to
infinity.
  We then extend our results to permanents and subpermanents sums. For
permanents, we are able to recover the lower bound of Schrijver recently proved
by Gurvits using stable polynomials. Our proof is algorithmic and borrows ideas
from the theory of local weak convergence of graphs, statistical physics and
covers of graphs. We provide new lower bounds for subpermanents sums and obtain
new results on the number of matching in random $n$-lifts with some
implications for the matching measure and the spectral measure of random
$n$-lifts as well as for the spectral measure of infinite trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04760</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04760</id><created>2015-07-16</created><updated>2016-03-01</updated><authors><author><keyname>Fridman</keyname><forenames>Lex</forenames></author><author><keyname>Langhans</keyname><forenames>Philipp</forenames></author><author><keyname>Lee</keyname><forenames>Joonbum</forenames></author><author><keyname>Reimer</keyname><forenames>Bryan</forenames></author></authors><title>Driver Gaze Region Estimation Without Using Eye Movement</title><categories>cs.CV</categories><comments>Accepted for Publication in IEEE Intelligent Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated estimation of the allocation of a driver's visual attention may be
a critical component of future Advanced Driver Assistance Systems. In theory,
vision-based tracking of the eye can provide a good estimate of gaze location.
In practice, eye tracking from video is challenging because of sunglasses,
eyeglass reflections, lighting conditions, occlusions, motion blur, and other
factors. Estimation of head pose, on the other hand, is robust to many of these
effects, but cannot provide as fine-grained of a resolution in localizing the
gaze. However, for the purpose of keeping the driver safe, it is sufficient to
partition gaze into regions. In this effort, we propose a system that extracts
facial features and classifies their spatial configuration into six regions in
real-time. Our proposed method achieves an average accuracy of 91.4% at an
average decision rate of 11 Hz on a dataset of 50 drivers from an on-road
study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04761</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04761</id><created>2015-07-16</created><authors><author><keyname>Kereliuk</keyname><forenames>Corey</forenames></author><author><keyname>Sturm</keyname><forenames>Bob L.</forenames></author><author><keyname>Larsen</keyname><forenames>Jan</forenames></author></authors><title>Deep Learning and Music Adversaries</title><categories>cs.LG cs.NE cs.SD</categories><comments>13 pages, 6 figures, 3 tables, 6 sections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An adversary is essentially an algorithm intent on making a classification
system perform in some particular way given an input, e.g., increase the
probability of a false negative. Recent work builds adversaries for deep
learning systems applied to image object recognition, which exploits the
parameters of the system to find the minimal perturbation of the input image
such that the network misclassifies it with high confidence. We adapt this
approach to construct and deploy an adversary of deep learning systems applied
to music content analysis. In our case, however, the input to the systems is
magnitude spectral frames, which requires special care in order to produce
valid input audio signals from network-derived perturbations. For two different
train-test partitionings of two benchmark datasets, and two different deep
architectures, we find that this adversary is very effective in defeating the
resulting systems. We find the convolutional networks are more robust, however,
compared with systems based on a majority vote over individually classified
audio frames. Furthermore, we integrate the adversary into the training of new
deep systems, but do not find that this improves their resilience against the
same adversary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04774</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04774</id><created>2015-07-16</created><authors><author><keyname>Boothby</keyname><forenames>Tomas</forenames></author><author><keyname>King</keyname><forenames>Andrew D.</forenames></author><author><keyname>Roy</keyname><forenames>Aidan</forenames></author></authors><title>Fast clique minor generation in Chimera qubit connectivity graphs</title><categories>cs.DM cs.DS quant-ph</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current generation of D-Wave quantum annealing processor is designed to
minimize the energy of an Ising spin configuration whose pairwise interactions
lie on the edges of a {\em Chimera} graph $\mathcal C_{M,N,L}$. In order to
solve an Ising spin problem with arbitrary pairwise interaction structure, the
corresponding graph must be minor-embedded into a Chimera graph. We define a
combinatorial class of {\em native clique minors} in Chimera graphs with vertex
images of uniform, near minimal size, and provide a polynomial-time algorithm
that finds a maximum native clique minor in a given induced subgraph of a
Chimera graph. These minors allow improvement over recent work and have
immediate practical applications in the field of quantum annealing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04775</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04775</id><created>2015-07-16</created><updated>2015-08-04</updated><authors><author><keyname>Smaldino</keyname><forenames>Paul E.</forenames></author><author><keyname>Janssen</keyname><forenames>Marco A.</forenames></author><author><keyname>Hillis</keyname><forenames>Vicken</forenames></author><author><keyname>Bednar</keyname><forenames>Jenna</forenames></author></authors><title>Adoption as a Social Marker: The Diffusion of Products in a Multigroup
  Environment</title><categories>physics.soc-ph cs.SI</categories><comments>24 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social identities are among the key factors driving social behavior in
complex societies. Recent attention to social identity in consumer behavior
indicates that individuals avoid products that might signal membership in an
outgroup. Yet the population-level consequences of adoption as identity
signaling are largely unknown. Whereas previous work has focused on asymmetric
attraction and repulsion among groups with different social identities, here we
consider the spread of innovations in a structured population in which there
are multiple groups who don't want to be mistaken for one another, using both
analytical and agent-based modeling. We investigate how identity signaling
influences overall patterns of adoption in structured populations, and consider
the role of structural factors such as demographic skew and communication scale
on diffusion dynamics. We find that as products become emergent social markers,
aversion to outgroup-associated products can decrease overall patterns of
adoption and promote the extent to which uptake of a product is locally
polarized. When communications are long-range, polarization can become global.
This research has particular relevance to widely beneficial but
identity-relevant products, such as green technologies, where overall levels of
adoption determine the positive benefits that accrue to society at large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04777</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04777</id><created>2015-07-16</created><updated>2016-02-24</updated><authors><author><keyname>Mandt</keyname><forenames>Stephan</forenames></author><author><keyname>Wenzel</keyname><forenames>Florian</forenames></author><author><keyname>Nakajima</keyname><forenames>Shinichi</forenames></author><author><keyname>Cunningham</keyname><forenames>John P.</forenames></author><author><keyname>Lippert</keyname><forenames>Christoph</forenames></author><author><keyname>Kloft</keyname><forenames>Marius</forenames></author></authors><title>Sparse Estimation in a Correlated Probit Model</title><categories>stat.ML cs.LG</categories><comments>18 pages + appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the goals of statistical genetics is to find associations between
genetic data and binary phenotypes, such as heritable diseases. Often, the data
are obfuscated by confounders such as age, ethnicity, or population structure.
Linear mixed models are linear regression models that correct for confounding
by means of correlated label noise; they are widely appreciated in the field of
statistical genetics. We generalize this modeling paradigm to binary
classification, where we face the problem that marginalizing over the noise
leads to an intractable, high-dimensional integral. We present a scalable,
approximate inference algorithm that lets us fit the model to high-dimensional
data sets. The algorithm selects features based on an $\ell_1$ norm regularizer
which are up to 40% less confounded compared to the outcomes of uncorrected
feature selection, as we show. The proposed method also outperforms Gaussian
process classification and uncorrelated probit regression in terms of
prediction performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04793</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04793</id><created>2015-07-16</created><updated>2016-01-05</updated><authors><author><keyname>Oymak</keyname><forenames>Samet</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author><author><keyname>Soltanolkotabi</keyname><forenames>Mahdi</forenames></author></authors><title>Sharp Time--Data Tradeoffs for Linear Inverse Problems</title><categories>cs.IT cs.LG math.IT math.OC math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we characterize sharp time-data tradeoffs for optimization
problems used for solving linear inverse problems. We focus on the minimization
of a least-squares objective subject to a constraint defined as the sub-level
set of a penalty function. We present a unified convergence analysis of the
gradient projection algorithm applied to such problems. We sharply characterize
the convergence rate associated with a wide variety of random measurement
ensembles in terms of the number of measurements and structural complexity of
the signal with respect to the chosen penalty function. The results apply to
both convex and nonconvex constraints, demonstrating that a linear convergence
rate is attainable even though the least squares objective is not strongly
convex in these settings. When specialized to Gaussian measurements our results
show that such linear convergence occurs when the number of measurements is
merely 4 times the minimal number required to recover the desired signal at all
(a.k.a. the phase transition). We also achieve a slower but geometric rate of
convergence precisely above the phase transition point. Extensive numerical
results suggest that the derived rates exactly match the empirical performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04798</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04798</id><created>2015-07-16</created><authors><author><keyname>R&#xf6;nnqvist</keyname><forenames>Samuel</forenames></author></authors><title>Exploratory topic modeling with distributional semantics</title><categories>cs.IR cs.CL cs.LG</categories><comments>Conference: The Fourteenth International Symposium on Intelligent
  Data Analysis (IDA 2015)</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  As we continue to collect and store textual data in a multitude of domains,
we are regularly confronted with material whose largely unknown thematic
structure we want to uncover. With unsupervised, exploratory analysis, no prior
knowledge about the content is required and highly open-ended tasks can be
supported. In the past few years, probabilistic topic modeling has emerged as a
popular approach to this problem. Nevertheless, the representation of the
latent topics as aggregations of semi-coherent terms limits their
interpretability and level of detail.
  This paper presents an alternative approach to topic modeling that maps
topics as a network for exploration, based on distributional semantics using
learned word vectors. From the granular level of terms and their semantic
similarity relations global topic structures emerge as clustered regions and
gradients of concepts. Moreover, the paper discusses the visual interactive
representation of the topic map, which plays an important role in supporting
its exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04808</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04808</id><created>2015-07-16</created><updated>2015-11-25</updated><authors><author><keyname>Serban</keyname><forenames>Iulian V.</forenames></author><author><keyname>Sordoni</keyname><forenames>Alessandro</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Pineau</keyname><forenames>Joelle</forenames></author></authors><title>Building End-To-End Dialogue Systems Using Generative Hierarchical
  Neural Network Models</title><categories>cs.CL cs.AI cs.LG cs.NE</categories><comments>8 pages with references; will appear in AAAI 2016 (Special Track on
  Cognitive Systems)</comments><acm-class>I.5.1; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the task of building open domain, conversational dialogue
systems based on large dialogue corpora using generative models. Generative
models produce system responses that are autonomously generated word-by-word,
opening up the possibility for realistic, flexible interactions. In support of
this goal, we extend the recently proposed hierarchical recurrent
encoder-decoder neural network to the dialogue domain, and demonstrate that
this model is competitive with state-of-the-art neural language models and
back-off n-gram models. We investigate the limitations of this and similar
approaches, and show how its performance can be improved by bootstrapping the
learning from a larger question-answer pair corpus and from pretrained word
embeddings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04811</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04811</id><created>2015-07-16</created><updated>2016-02-12</updated><authors><author><keyname>Xu</keyname><forenames>Jian</forenames></author><author><keyname>Shao</keyname><forenames>Xuhui</forenames></author><author><keyname>Ma</keyname><forenames>Jianjie</forenames></author><author><keyname>Lee</keyname><forenames>Kuang-chih</forenames></author><author><keyname>Qi</keyname><forenames>Hang</forenames></author><author><keyname>Lu</keyname><forenames>Quan</forenames></author></authors><title>Lift-Based Bidding in Ad Selection</title><categories>cs.GT cs.AI</categories><comments>AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-time bidding (RTB) has become one of the largest online advertising
markets in the world. Today the bid price per ad impression is typically
decided by the expected value of how it can lead to a desired action event
(e.g., registering an account or placing a purchase order) to the advertiser.
However, this industry standard approach to decide the bid price does not
consider the actual effect of the ad shown to the user, which should be
measured based on the performance lift among users who have been or have not
been exposed to a certain treatment of ads. In this paper, we propose a new
bidding strategy and prove that if the bid price is decided based on the
performance lift rather than absolute performance value, advertisers can
actually gain more action events. We describe the modeling methodology to
predict the performance lift and demonstrate the actual performance gain
through blind A/B test with real ad campaigns in an industry-leading
Demand-Side Platform (DSP). We also discuss the relationship between
attribution models and bidding strategies. We prove that, to move the DSPs to
bid based on performance lift, they should be rewarded according to the
relative performance lift they contribute.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04816</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04816</id><created>2015-07-16</created><authors><author><keyname>Van</keyname><forenames>Thanh The</forenames></author><author><keyname>Le</keyname><forenames>Thanh Manh</forenames></author></authors><title>RBIR Based on Signature Graph</title><categories>cs.CV</categories><comments>4 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper approaches the image retrieval system on the base of visual
features local region RBIR (region-based image retrieval). First of all, the
paper presents a method for extracting the interest points based on
Harris-Laplace to create the feature region of the image. Next, in order to
reduce the storage space and speed up query image, the paper builds the binary
signature structure to describe the visual content of image. Based on the
image's binary signature, the paper builds the SG (signature graph) to classify
and store image's binary signatures. Since then, the paper builds the image
retrieval algorithm on SG through the similar measure EMD (earth mover's
distance) between the image's binary signatures. Last but not least, the paper
gives an image retrieval model RBIR, experiments and assesses the image
retrieval method on Corel image database over 10,000 images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04817</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04817</id><created>2015-07-16</created><updated>2016-02-13</updated><authors><author><keyname>Nguyen</keyname><forenames>Phuc C.</forenames></author><author><keyname>Tobin-Hochstadt</keyname><forenames>Sam</forenames></author><author><keyname>Van Horn</keyname><forenames>David</forenames></author></authors><title>Higher-order symbolic execution for contract verification and refutation</title><categories>cs.PL</categories><comments>This paper unifies and expands upon the work presented in the papers
  &quot;Soft contract verification&quot; [arXiv:1307.6239], and &quot;Relatively complete
  counterexamples for higher-order programs&quot; [arXiv:1411.3967]. It also
  subsumes the work in the paper &quot;Higher-order symbolic execution via
  contracts&quot; [arXiv:1103.1362]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach to automated reasoning about higher-order programs
by endowing symbolic execution with a notion of higher-order, symbolic values.
Our approach is sound and relatively complete with respect to a first-order
solver for base type values. Therefore, it can form the basis of automated
verification and bug-finding tools for higher-order programs.
  To validate our approach, we use it to develop and evaluate a system for
verifying and refuting behavioral software contracts of components in a
functional language, which we call soft contract verification. In doing so, we
discover a mutually beneficial relation between behavioral contracts and
higher-order symbolic execution.
  Our system uses higher-order symbolic execution, leveraging contracts as a
source of symbolic values including unknown behavioral values, and employs an
updatable heap of contract invariants to reason about flow-sensitive facts.
Whenever a contract is refuted, it reports a concrete counterexample
reproducing the error, which may involve solving for an unknown function. The
approach is able to analyze first-class contracts, recursive data structures,
unknown functions, and control-flow-sensitive refinements of values, which are
all idiomatic in dynamic languages. It makes effective use of an off-the-shelf
solver to decide problems without heavy encodings. The approach is competitive
with a wide range of existing tools---including type systems, flow analyzers,
and model checkers---on their own benchmarks. We have built a tool which
analyzes programs written in Racket, and report on its effectiveness in
verifying and refuting contracts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04820</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04820</id><created>2015-07-16</created><authors><author><keyname>Lehmann</keyname><forenames>Karsten</forenames></author><author><keyname>Grastien</keyname><forenames>Alban</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>The Complexity of Switching and FACTS Maximum-Potential-Flow Problems</title><categories>cs.CC</categories><comments>arXiv admin note: text overlap with arXiv:1411.4369</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This papers considers the problem of maximizing the load that can be served
by a power network. We use the commonly accepted Linear DC power network model
and consider wo configuration options: switching lines and using FACTS devices.
We present the first comprehensive complexity study of this optimization
problem. Our results show hat the problem is NP-complete and that there is no
fully polynomial-time approximation scheme. For switching, these results extend
to planar networks with a aximum-node degree of 3. Additionally, we demonstrate
that the optimization problems are still NP-hard if we restrict the network
structure to cacti with a maximum degree of 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04822</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04822</id><created>2015-07-16</created><authors><author><keyname>Zhang</keyname><forenames>Zhenliang</forenames></author><author><keyname>Wang</keyname><forenames>Yuan</forenames></author><author><keyname>Chong</keyname><forenames>Edwin K. P.</forenames></author><author><keyname>Pezeshki</keyname><forenames>Ali</forenames></author><author><keyname>Scharf</keyname><forenames>Louis</forenames></author></authors><title>Subspace selection for projection maximization with matroid constraints</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that there is a ground set which consists of a large number of
vectors in a Hilbert space. Consider the problem of selecting a subset of the
ground set such that the projection of a vector of interest onto the subspace
spanned by the vectors in the chosen subset reaches the maximum norm. This
problem is generally NP-hard, and alternative approximation algorithms such as
forward regression and orthogonal matching pursuit have been proposed as
heuristic approaches. In this paper, we investigate bounds on the performance
of these algorithms by introducing the notions of elemental curvatures. More
specifically, we derive lower bounds, as functions of these elemental
curvatures, for performance of the aforementioned algorithms with respect to
that of the optimal solution under uniform and non-uniform matroid constraints,
respectively. We show that if the elements in the ground set are mutually
orthogonal, then these algorithms are optimal when the matroid is uniform and
they achieve at least $1/2$-approximations of the optimal solution when the
matroid is non-uniform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04831</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04831</id><created>2015-07-17</created><authors><author><keyname>Hu</keyname><forenames>Yongtao</forenames></author><author><keyname>Ren</keyname><forenames>Jimmy</forenames></author><author><keyname>Dai</keyname><forenames>Jingwen</forenames></author><author><keyname>Yuan</keyname><forenames>Chang</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author><author><keyname>Wang</keyname><forenames>Wenping</forenames></author></authors><title>Deep Multimodal Speaker Naming</title><categories>cs.CV cs.LG cs.MM cs.SD</categories><acm-class>H.3</acm-class><doi>10.1145/2733373.2806293</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic speaker naming is the problem of localizing as well as identifying
each speaking character in a TV/movie/live show video. This is a challenging
problem mainly attributes to its multimodal nature, namely face cue alone is
insufficient to achieve good performance. Previous multimodal approaches to
this problem usually process the data of different modalities individually and
merge them using handcrafted heuristics. Such approaches work well for simple
scenes, but fail to achieve high performance for speakers with large appearance
variations. In this paper, we propose a novel convolutional neural networks
(CNN) based learning framework to automatically learn the fusion function of
both face and audio cues. We show that without using face tracking, facial
landmark localization or subtitle/transcript, our system with robust multimodal
feature extraction is able to achieve state-of-the-art speaker naming
performance evaluated on two diverse TV series. The dataset and implementation
of our algorithm are publicly available online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04835</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04835</id><created>2015-07-17</created><authors><author><keyname>Tai</keyname><forenames>Cheng</forenames></author><author><keyname>E</keyname><forenames>Weinan</forenames></author></authors><title>Multiscale Adaptive Representation of Signals: I. The Basic Framework</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a framework for designing multi-scale, adaptive, shift-invariant
frames and bi-frames for representing signals. The new framework, called
AdaFrame, improves over dictionary learning-based techniques in terms of
computational efficiency at inference time. It improves classical multi-scale
basis such as wavelet frames in terms of coding efficiency. It provides an
attractive alternative to dictionary learning-based techniques for low level
signal processing tasks, such as compression and denoising, as well as high
level tasks, such as feature extraction for object recognition. Connections
with deep convolutional networks are also discussed. In particular, the
proposed framework reveals a drawback in the commonly used approach for
visualizing the activations of the intermediate layers in convolutional
networks, and suggests a natural alternative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04840</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04840</id><created>2015-07-17</created><authors><author><keyname>Chen</keyname><forenames>Shaoshi</forenames></author><author><keyname>Koutschan</keyname><forenames>Christoph</forenames></author></authors><title>Proof of the Wilf-Zeilberger Conjecture</title><categories>math.CO cs.SC</categories><comments>18 pages, 1 figure</comments><msc-class>33C70, 33F10</msc-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In 1992, Wilf and Zeilberger conjectured that a hypergeometric term in
several discrete and continuous variables is holonomic if and only if it is
proper. Strictly speaking the conjecture does not hold, but it is true when
reformulated properly: Payne proved a piecewise interpretation in 1997, and
independently, Abramov and Petkovsek in 2002 proved a conjugate interpretation.
Both results address the pure discrete case of the conjecture. In this paper we
extend their work to hypergeometric terms in several discrete and continuous
variables and prove the conjugate interpretation of the Wilf-Zeilberger
conjecture in this mixed setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04844</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04844</id><created>2015-07-17</created><authors><author><keyname>Wu</keyname><forenames>Xiang</forenames></author></authors><title>Learning Robust Deep Face Representation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development of convolution neural network, more and more researchers
focus their attention on the advantage of CNN for face recognition task. In
this paper, we propose a deep convolution network for learning a robust face
representation. The deep convolution net is constructed by 4 convolution
layers, 4 max pooling layers and 2 fully connected layers, which totally
contains about 4M parameters. The Max-Feature-Map activation function is used
instead of ReLU because the ReLU might lead to the loss of information due to
the sparsity while the Max-Feature-Map can get the compact and discriminative
feature vectors. The model is trained on CASIA-WebFace dataset and evaluated on
LFW dataset. The result on LFW achieves 97.77% on unsupervised setting for
single net.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04860</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04860</id><created>2015-07-17</created><authors><author><keyname>Antonioli</keyname><forenames>Daniele</forenames></author><author><keyname>Tippenhauer</keyname><forenames>Nils Ole</forenames></author></authors><title>MiniCPS: A toolkit for security research on CPS Networks</title><categories>cs.NI cs.CR</categories><comments>8 pages, 6 figures, 1 code listing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, tremendous effort has been spent to modernizing
communication infrastructure in Cyber-Physical Systems (CPS) such as Industrial
Control Systems (ICS) and related Supervisory Control and Data Acquisition
(SCADA) systems. While a great amount of research has been conducted on network
security of office and home networks, recently the security of CPS and related
systems has gained a lot of attention. Unfortunately, real-world CPS are often
not open to security researchers, and as a result very few reference systems
and topologies are available. In this work, we present MiniCPS, a CPS
simulation toolbox intended to alleviate this problem. The goal of MiniCPS is
to create an extensible, reproducible research environment targeted to
communications and physical-layer interactions in CPS. MiniCPS builds on
Mininet to provide lightweight real-time network emulation, and extends Mininet
with tools to simulate typical CPS components such as programmable logic
controllers, which use industrial protocols (Ethernet/IP, Modbus/TCP). In
addition, MiniCPS defines a simple API to enable physical-layer interaction
simulation. In this work, we demonstrate applications of MiniCPS in two example
scenarios, and show how MiniCPS can be used to develop attacks and defenses
that are directly applicable to real systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04869</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04869</id><created>2015-07-17</created><updated>2015-11-02</updated><authors><author><keyname>Mochaourab</keyname><forenames>Rami</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author></authors><title>Adaptive Pilot Clustering in Heterogeneous Massive MIMO Networks</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Wireless Communications, 30 pages,
  13 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the uplink of a cellular massive MIMO network. Acquiring channel
state information at the base stations (BSs) requires uplink pilot signaling.
Since the number of orthogonal pilot sequences is limited by the channel
coherence, pilot reuse across cells is necessary to achieve high spectral
efficiency. However, finding efficient pilot reuse patterns is non-trivial
especially in practical asymmetric BS deployments. We approach this problem
using coalitional game theory. Each BS has a few unique pilots and can form
coalitions with other BSs to gain access to more pilots. The BSs in a coalition
thus benefit from serving more users in their cells, at the expense of higher
pilot contamination and interference. Given that a cell's average spectral
efficiency depends on the overall pilot reuse pattern, the suitable coalitional
game model is in partition form. We develop a low-complexity distributed
coalition formation based on individual stability. By incorporating a searching
budget constraint for each BS, we are able to control the algorithm's
complexity and ensure its convergence to a solution of the game called
individually stable coalition structure. Simulation results reveal fast
algorithmic convergence and substantial performance gains over the baseline
schemes with no pilot reuse, full pilot reuse, or random pilot reuse pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04885</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04885</id><created>2015-07-17</created><authors><author><keyname>Kinne</keyname><forenames>Jeff</forenames></author><author><keyname>Manuch</keyname><forenames>J&#xe1;n</forenames></author><author><keyname>Rafiey</keyname><forenames>Akbar</forenames></author><author><keyname>Rafiey</keyname><forenames>Arash</forenames></author></authors><title>Ordering with precedence constraints and budget minimization</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a variation of the scheduling with precedence constraints
problem that has applications to molecular folding and production management.
We are given a bipartite graph $H=(B,S)$. Vertices in $B$ are thought of as
goods or services that must be \emph{bought} to produce items in $S$ that are
to be \emph{sold}. An edge from $j\in S$ to $i\in B$ indicates that the
production of $j$ requires the purchase of $i$. Each vertex in $B$ has a cost,
and each vertex in $S$ results in some gain. The goal is to obtain an ordering
of $B\cup S$ that respects the precedence constraints and maximizes the minimal
net profit encountered as the vertices are processed. We call this optimal
value the \emph{budget} or \emph{capital} investment required for the bipartite
graph, and refer to our problem as \emph{the bipartite graph ordering problem}.
  The problem is equivalent to a version of an NP-complete molecular folding
problem that has been studied recently [14]. Work on the molecular folding
problem has focused on heuristic algorithms and exponential-time exact
algorithms for the un-weighted problem where costs are $\pm 1$ and when
restricted to graphs arising from RNA folding.
  The bipartite graph present work seeks exact algorithms for solving the
bipartite ordering problem. We demonstrate an algorithm that computes the
optimal ordering in time $O^*(2^n)$ when $n$ is the number of vertices in the
input bipartite graph. Our main result is a general strategy that can be used
to find an optimal ordering in polynomial time for bipartite graphs that
satisfy certain properties. We apply the technique to a variety of graph
classes, obtaining polynomial-time solutions to the bipartite graph ordering
problem for bipartite graphs that are convex, trivially perfect, co-bipartite
graphs, and trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04888</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04888</id><created>2015-07-17</created><updated>2015-11-15</updated><authors><author><keyname>Wulfmeier</keyname><forenames>Markus</forenames></author><author><keyname>Ondruska</keyname><forenames>Peter</forenames></author><author><keyname>Posner</keyname><forenames>Ingmar</forenames></author></authors><title>Maximum Entropy Deep Inverse Reinforcement Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a general framework for employing deep architectures - in
particular neural networks - to solve the inverse reinforcement learning (IRL)
problem. Specifically, we propose to exploit the representational capacity and
favourable computational complexity of deep networks to approximate complex,
nonlinear reward functions. We show that the Maximum Entropy paradigm for IRL
lends itself naturally to the efficient training of deep architectures. At test
time, the approach leads to a computational complexity independent of the
number of demonstrations. This makes it especially well-suited for applications
in life-long learning scenarios commonly encountered in robotics. We
demonstrate that our approach achieves performance commensurate to the
state-of-the-art on existing benchmarks already with simple, comparatively
shallow network architectures while significantly outperforming the
state-of-the-art on an alternative benchmark based on more complex, highly
varying reward structures representing strong interactions between features.
Furthermore, we extend the approach to include convolutional layers in order to
eliminate the dependency on precomputed features of current algorithms and to
underline the substantial gain in flexibility in framing IRL in the context of
deep learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04900</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04900</id><created>2015-07-17</created><authors><author><keyname>Di Tommaso</keyname><forenames>Giorgia</forenames></author><author><keyname>Stilo</keyname><forenames>Giovanni</forenames></author><author><keyname>Velardi</keyname><forenames>Paola</forenames></author></authors><title>Analysis of women leadership in enterprise social networks</title><categories>cs.SI cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a Social Network Analysis toolkit to monitor an
Enterprise Social Network and help analyzing informal leadership as a function
of social ties and topic discussions. The toolkit has been developed in the
context of a regional project, Fiordaliso, funded by Regione Lazio (a region of
central Italy) and leaded by Reply, an international network of specialized
companies in the field of digital services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04907</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04907</id><created>2015-07-17</created><updated>2015-11-27</updated><authors><author><keyname>Kolman</keyname><forenames>Petr</forenames></author><author><keyname>Kouteck&#xfd;</keyname><forenames>Martin</forenames></author><author><keyname>Tiwary</keyname><forenames>Hans Raj</forenames></author></authors><title>Extension Complexity, MSO Logic, and Treewidth</title><categories>cs.DS cs.CC</categories><comments>This is a major revision of the earlier version. New title,
  introduction, and organization. 11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the convex hull $P_{\varphi}(G)$ of all satisfying assignments of
a given MSO formula $\varphi$ on a given graph $G$. We show that there exists
an extended formulation of the polytope $P_{\varphi}(G)$ that can be described
by $f(|\varphi|,\tau)\cdot n$ inequalities, where $n$ is the number of vertices
in $G$, $\tau$ is the treewidth of $G$ and $f$ is a computable function
depending only on $\varphi$ and $\tau.$
  In other words, we prove that the extension complexity of $P_{\varphi}(G)$ is
linear in the size of the graph $G$, with a constant depending on the treewidth
of $G$ and the formula $\varphi$. This provides a first meta-theorem about the
extension complexity of polytopes related to a wide class of problems and
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04908</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04908</id><created>2015-07-17</created><authors><author><keyname>Brodic</keyname><forenames>Darko</forenames></author><author><keyname>Milivojevic</keyname><forenames>Zoran N.</forenames></author><author><keyname>Amelio</keyname><forenames>Alessia</forenames></author></authors><title>Analysis of the South Slavic Scripts by Run-Length Features of the Image
  Texture</title><categories>cs.CV cs.CL</categories><comments>9 pages, 9 figures, In Electronics 2015, Elektronika IR
  Elektrotechnika, ISSN 1392-1215 (in press)</comments><journal-ref>Elektronika Ir Elektrotechnika, ISSN 1392-1215, VOL. 21, NO. 4,
  2015</journal-ref><doi>10.5755/j01.eee.21.4.12785</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proposes an algorithm for the script recognition based on the
texture characteristics. The image texture is achieved by coding each letter
with the equivalent script type (number code) according to its position in the
text line. Each code is transformed into equivalent gray level pixel creating
an 1-D image. Then, the image texture is subjected to the run-length analysis.
This analysis extracts the run-length features, which are classified to make a
distinction between the scripts under consideration. In the experiment, a
custom oriented database is subject to the proposed algorithm. The database
consists of some text documents written in Cyrillic, Latin and Glagolitic
scripts. Furthermore, it is divided into training and test parts. The results
of the experiment show that 3 out of 5 run-length features can be used for
effective differentiation between the analyzed South Slavic scripts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04909</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04909</id><created>2015-07-17</created><authors><author><keyname>Marckert</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Saheb-Djahromi</keyname><forenames>Nasser</forenames></author><author><keyname>Zemmari</keyname><forenames>Akka</forenames></author></authors><title>Election algorithms with random delays in trees</title><categories>cs.DC</categories><journal-ref>Discrete Mathematics and Theoretical Computer Science (DMTCS),
  611-622, 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The election is a classical problem in distributed algorithmic. It aims to
design and to analyze a distributed algorithm choosing a node in a graph, here,
in a tree. In this paper, a class of randomized algorithms for the election is
studied. The election amounts to removing leaves one by one until the tree is
reduced to a unique node which is then elected. The algorithm assigns to each
leaf a probability distribution (that may depends on the information
transmitted by the eliminated nodes) used by the leaf to generate its remaining
random lifetime. In the general case, the probability of each node to be
elected is given. For two categories of algorithms, close formulas are
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04910</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04910</id><created>2015-07-17</created><authors><author><keyname>Vorobev</keyname><forenames>Aleksandr</forenames></author><author><keyname>Gusev</keyname><forenames>Gleb</forenames></author></authors><title>Lower Bounds for Multi-armed Bandit with Non-equivalent Multiple Plays</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the stochastic multi-armed bandit problem with non-equivalent
multiple plays where, at each step, an agent chooses not only a set of arms,
but also their order, which influences reward distribution. In several problem
formulations with different assumptions, we provide lower bounds for regret
with standard asymptotics $O(\log{t})$ but novel coefficients and provide
optimal algorithms, thus proving that these bounds cannot be improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04913</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04913</id><created>2015-07-17</created><authors><author><keyname>Han</keyname><forenames>Xintong</forenames></author><author><keyname>Zhang</keyname><forenames>Chongyang</forenames></author><author><keyname>Lin</keyname><forenames>Weiyao</forenames></author><author><keyname>Xu</keyname><forenames>Mingliang</forenames></author><author><keyname>Sheng</keyname><forenames>Bin</forenames></author><author><keyname>Mei</keyname><forenames>Tao</forenames></author></authors><title>Tree-based Visualization and Optimization for Image Collection</title><categories>cs.MM cs.AI cs.CV</categories><comments>This manuscript is the accepted version for T-CYB (IEEE Transactions
  on Cybernetics) IEEE Trans. Cybernetics, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The visualization of an image collection is the process of displaying a
collection of images on a screen under some specific layout requirements. This
paper focuses on an important problem that is not well addressed by the
previous methods: visualizing image collections into arbitrary layout shapes
while arranging images according to user-defined semantic or visual
correlations (e.g., color or object category). To this end, we first propose a
property-based tree construction scheme to organize images of a collection into
a tree structure according to user-defined properties. In this way, images can
be adaptively placed with the desired semantic or visual correlations in the
final visualization layout. Then, we design a two-step visualization
optimization scheme to further optimize image layouts. As a result, multiple
layout effects including layout shape and image overlap ratio can be
effectively controlled to guarantee a satisfactory visualization. Finally, we
also propose a tree-transfer scheme such that visualization layouts can be
adaptively changed when users select different &quot;images of interest&quot;. We
demonstrate the effectiveness of our proposed approach through the comparisons
with state-of-the-art visualization techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04914</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04914</id><created>2015-07-17</created><updated>2015-07-22</updated><authors><author><keyname>Zimmermann</keyname><forenames>Christian</forenames></author></authors><title>A Categorization of Transparency-Enhancing Technologies</title><categories>cs.CY</categories><comments>This paper will be presented at the Amsterdam Privacy Conference
  2015, a conference without proceeding</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variety of Transparency-Enhancing Technologies has been presented during
the past years. However, investigation of frameworks for classification and
assessment of Transparency-Enhancing Technologies has lacked behind. The lack
of precise classification and categorization approaches poses an obstacle not
only to systematic requirements analysis for Transparency-Enhancing
Technologies but also to investigation and analysis of their capabilities and
their suitability to contribute to privacy protection. This paper addresses
this research gap. In particular, it presents a set of categorization
parameters for describing the properties and functionality of a
Transparency-Enhancing Technology on the one hand, and a categorization of
Transparency-Enhancing Technologies on the other hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04921</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04921</id><created>2015-07-05</created><authors><author><keyname>Yeung</keyname><forenames>Chi Ho</forenames></author></authors><title>Do recommender systems benefit users?</title><categories>cs.CY cs.IR cs.SI physics.soc-ph</categories><comments>15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems are present in many web applications to guide our
choices. They increase sales and benefit sellers, but whether they benefit
customers by providing relevant products is questionable. Here we introduce a
model to examine the benefit of recommender systems for users, and found that
recommendations from the system can be equivalent to random draws if one relies
too strongly on the system. Nevertheless, with sufficient information about
user preferences, recommendations become accurate and an abrupt transition to
this accurate regime is observed for some algorithms. On the other hand, we
found that a high accuracy evaluated by common accuracy metrics does not
necessarily correspond to a high real accuracy nor a benefit for users, which
serves as an alarm for operators and researchers of recommender systems. We
tested our model with a real dataset and observed similar behaviors. Finally, a
recommendation approach with improved accuracy is suggested. These results
imply that recommender systems can benefit users, but relying too strongly on
the system may render the system ineffective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04924</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04924</id><created>2015-07-17</created><authors><author><keyname>Anzabi-Nezhad</keyname><forenames>Nima S.</forenames></author><author><keyname>Hodtani</keyname><forenames>Ghosheh Abed</forenames></author><author><keyname>Kakhki</keyname><forenames>Mohammad Molavi</forenames></author></authors><title>A New and More General Capacity Theorem for the Gaussian Channel with
  Two-sided Input-Noise Dependent State Information</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new and general version of Gaussian channel in presence of
two-sided state information correlated to the channel input and noise is
considered. Determining a general achievable rate for the channel and obtaining
the capacity in a non-limiting case, we try to analyze and solve the Gaussian
version of the Cover-Chiang theorem -as an open problem- mathematically and
information-theoretically. Our capacity theorem, while including all previous
theorems as its special cases, explains situations that can not be analyzed by
them; for example, the effect of the correlation between the side information
and the channel input on the capacity of the channel that can not be analyzed
with Costa's &quot;writing on dirty paper&quot; theorem. Meanwhile, we try to introduce
our new idea, i.e., describing the concept of &quot;cognition&quot; of a communicating
object (transmitter, receiver, relay and so on) on some variable (channel
noise, interference and so on) with the information-theoretic concept of &quot;side
information&quot; correlated to that variable and known by the object. According to
our theorem, the channel capacity is an increasing function of the mutual
information of the side information and the channel noise. Therefore our
channel and its capacity theorem exemplify the &quot;cognition&quot; of the transmitter
and receiver on the channel noise based on the new description. Our capacity
theorem has interesting interpretations originated from this new idea.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04925</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04925</id><created>2015-07-17</created><authors><author><keyname>Bei</keyname><forenames>Xiaohui</forenames></author><author><keyname>Garg</keyname><forenames>Jugal</forenames></author><author><keyname>Hoefer</keyname><forenames>Martin</forenames></author></authors><title>Tatonnement for Linear and Gross Substitutes Markets</title><categories>cs.GT cs.DS</categories><comments>27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central question in economics and computer science is when and how markets
can arrive at an equilibrium. Many existing algorithms for computing equilibria
in classes of Arrow-Debreu markets rely on full knowledge of the utility
functions of all agents, while in reality we often face unknown markets without
this information. A classic approach to unknown markets from economics is
tatonnement -- dynamic price update processes that only require oracle access
to query the aggregate demand of each good. In this paper, we design a new
class of tatonnement algorithms. For the first time, we show how tatonnement
can converge in polynomial time to market equilibria in linear markets and
spending constraint markets, where the main obstacle is a non-continuous demand
function. This also gives the first polynomial-time algorithm for spending
constraint markets and settles an open question raised in [Duan and Mehlhorn,
ICALP'13]. Moreover, our algorithms can be applied to unknown markets with weak
gross substitutes (WGS) property, in which they converge to
$(1+\epsilon)$-approximate market equilibria in time polynomial in market
parameters and $\log(1/\epsilon)$. This exponentially improves the previous
convergence rate of polynomial in market parameters and $1/\epsilon$. Our
approach uses ideas developed for full information linear markets and applies
them in unknown linear and non-linear markets. Since our oracles return demands
based on real utility functions, precision and representation of prices and
demands become a major technical challenge. Here we develop new tools and
insights, which may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04928</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04928</id><created>2015-07-17</created><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>A Brain-like Cognitive Process with Shared Methods</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a detailed cognitive structure and related processes
that it might use. It also suggests some specific methods for carrying out
those processes, where the methods interact well with each other. The main
purpose of this paper is to reaffirm earlier research on knowledge-based
'concept trees' and experience-based 'symbolic neural networks' by showing how
they can be used as part of the described system in more detail. While an
earlier paper defined 3 levels of functional requirement, this paper defines
physical and conceptual levels, based mostly on reinforcement and basic
counting mechanisms. The counting mechanism is used slightly differently, to
measure a level of 'cohesion' instead of a 'correct' classification, over whole
pattern instances. The structure is therefore modelled closely on the human
brain, where the pattern creation and activation can also be automatic and the
information can flow seamlessly between the two main constructs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04930</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04930</id><created>2015-06-30</created><authors><author><keyname>Tib&#xe9;ly</keyname><forenames>Gergely</forenames></author><author><keyname>Pollner</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>Palla</keyname><forenames>Gergely</forenames></author></authors><title>Comparing the hierarchy of author given tags and repository given tags
  in a large document archive</title><categories>physics.soc-ph cs.DL physics.data-an</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Folksonomies - large databases arising from collaborative tagging of items by
independent users - are becoming an increasingly important way of categorizing
information. In these systems users can tag items with free words, resulting in
a tripartite item-tag-user network. Although there are no prescribed relations
between tags, the way users think about the different categories presumably has
some built in hierarchy, in which more special concepts are descendants of some
more general categories. Several applications would benefit from the knowledge
of this hierarchy. Here we apply a recent method to check the differences and
similarities of hierarchies resulting from tags given by independent
individuals and from tags given by a centrally managed repository system. The
results from out method showed substantial differences between the lower part
of the hierarchies, and in contrast, a relatively high similarity at the top of
the hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04935</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04935</id><created>2015-07-17</created><authors><author><keyname>Kretz</keyname><forenames>Tobias</forenames></author><author><keyname>Lohmiller</keyname><forenames>Jochen</forenames></author><author><keyname>Schlaich</keyname><forenames>Johannes</forenames></author></authors><title>The Inflection Point of the Speed-Density Relation and the Social Force
  Model</title><categories>physics.soc-ph cs.MA</categories><comments>accepted for presentation at conference Traffic and Granular Flow
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been argued that the speed-density digram of pedestrian movement has
an inflection point. This inflection point was found empirically in
investigations of closed-loop single-file pedestrian movement. The reduced
complexity of single-file movement does not only allow a higher precision for
the evaluation of empirical data, but it occasionally also allows analytical
considerations for micosimulation models. In this way it will be shown that
certain (common) variants of the Social Force Model (SFM) do not produce an
inflection point in the speed-density diagram if infinitely many pedestrians
contribute to the force computed for one pedestrian. We propose a modified
Social Force Model that produces the inflection point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04938</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04938</id><created>2015-07-17</created><authors><author><keyname>Pattanayak</keyname><forenames>Sukhamoy</forenames></author><author><keyname>Singh</keyname><forenames>Abhay Kumar</forenames></author></authors><title>A class of cyclic Codes Over the Ring $\Z_4[u]/&lt;u^2&gt;$ and its Gray image</title><categories>cs.IT math.IT</categories><comments>10 pages</comments><msc-class>9415, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes over R have been introduced recently. In this paper, we study
the cyclic codes over R and their $\Z_2$ image. Making use of algebraic
structure, we find the some good $\Z_2$ codes of length 28.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04943</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04943</id><created>2015-07-17</created><authors><author><keyname>Platzer</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Differential Hybrid Games</title><categories>cs.LO cs.GT cs.PL math.DS math.LO</categories><report-no>CMU-CS-14-102</report-no><msc-class>03B70, 91A23, 34A38, 91A25, 03F03</msc-class><acm-class>F.3.1; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces differential hybrid games, which combine differential
games with hybrid games. In both kinds of games, two players interact with
continuous dynamics. The difference is that hybrid games also provide all the
features of hybrid systems and discrete games, but only deterministic
differential equations. Differential games, instead, provide differential
equations with input by both players, but not the luxury of hybrid games, such
as mode switches and discrete or alternating adversarial interaction. This
paper augments differential game logic with modalities for the combined
dynamics of differential hybrid games. It shows how hybrid games subsume
differential games and introduces differential game invariants and differential
game variants for proving properties of differential games inductively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04955</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04955</id><created>2015-07-17</created><authors><author><keyname>Amarilli</keyname><forenames>Antoine</forenames></author></authors><title>Structurally Tractable Uncertain Data</title><categories>cs.DB</categories><comments>11 pages, 1 figure, 1 table. To appear in SIGMOD/PODS PhD Symposium
  2015</comments><acm-class>H.2.1</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Many data management applications must deal with data which is uncertain,
incomplete, or noisy. However, on existing uncertain data representations, we
cannot tractably perform the important query evaluation tasks of determining
query possibility, certainty, or probability: these problems are hard on
arbitrary uncertain input instances. We thus ask whether we could restrict the
structure of uncertain data so as to guarantee the tractability of exact query
evaluation. We present our tractability results for tree and tree-like
uncertain data, and a vision for probabilistic rule reasoning. We also study
uncertainty about order, proposing a suitable representation, and study
uncertain data conditioned by additional observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04961</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04961</id><created>2015-07-17</created><updated>2016-01-20</updated><authors><author><keyname>Gunaratne</keyname><forenames>Junius</forenames></author><author><keyname>Nov</keyname><forenames>Oded</forenames></author></authors><title>Using Interactive Information Labels to Assist Decision Making Under
  Uncertainty: The Case for Long-term Saving</title><categories>cs.CY cs.HC</categories><comments>This paper is withdrawn as it is still work in progress</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Product information labels can help users understand complex information
leading them to make better decisions. One area where consumers are
particularly prone to make costly decision-making errors is long-term saving,
which requires understanding of complex concepts such as uncertainty and
trade-offs. While most people are poorly equipped to deal with such concepts,
interactive design can potentially help users make better decisions. We
developed an interactive information label to assist consumers with retirement
saving decision-making. To evaluate it, we exposed 382 users to one of three
user interface conditions in a retirement saving simulator where they made 35
yearly decisions under changing circumstances. We found significantly better
ability of users to reach their goals with the information label. Furthermore,
users who interacted with the label made better decisions than those who were
presented with a static information label. Lastly, we found the label
particularly effective in helping novice savers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04978</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04978</id><created>2015-07-17</created><authors><author><keyname>Manolakos</keyname><forenames>Alexandros</forenames></author><author><keyname>Chowdhury</keyname><forenames>Mainak</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea J.</forenames></author></authors><title>Energy-based Modulation for Noncoherent Massive SIMO Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An uplink system with a single antenna transmitter and a single receiver with
a large number of antennas is considered. We propose an energy-detection-based
single-shot noncoherent communication scheme which does not use the
instantaneous channel state information (CSI), but rather only the knowledge of
the channel statistics. The suggested system uses a transmitter that modulates
information on the power of the symbols, and a receiver which measures only the
average energy across the antennas. We propose constellation designs which are
asymptotically optimal with respect to symbol error rate (SER) with an
increasing number of antennas, for any finite signal to noise ratio (SNR) at
the receiver, under different assumptions on the availability of CSI statistics
(exact channel fading distribution or the first few moments of the channel
fading distribution). We also consider the case of imperfect knowledge of the
channel statistics and describe in detail the case when there is a bounded
uncertainty on the moments of the fading distribution. We present numerical
results on the SER performance achieved by these designs in typical scenarios
and find that they may outperform existing noncoherent constellations, e.g.,
conventional Amplitude Shift Keying (ASK), and pilot-based schemes, e.g., Pulse
Amplitude Modulation (PAM). We also observe that an optimized constellation for
a specific channel distribution makes it very sensitive to uncertainties in the
channel statistics. In particular, constellation designs based on optimistic
channel conditions could lead to significant performance degradation in terms
of the achieved symbol error rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04988</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04988</id><created>2015-07-17</created><authors><author><keyname>Bharadwaj</keyname><forenames>S. N.</forenames></author><author><keyname>Jagadeesha</keyname><forenames>S. N.</forenames></author><author><keyname>Ravindra</keyname><forenames>S.</forenames></author></authors><title>Target localization in wireless sensor networks based on received signal
  strength</title><categories>cs.NI</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We consider the problem of localizing a target taking the help of a set of
anchor beacon nodes.A small number of beacon nodes are deployed at known
locations in the area.The target can detect a beacon provided it happens to lie
within the beacons's transmission range.Thus, the target contains a measurement
vector containing the readings of the beacons: '1' corresponding to a beacon if
it is able to detect the target and '0' if the beacon is not able to detect the
target.The goal is two fold: to determine the location of the target based on
the binary measurement vector at the target and to study the behavior of the
localization uncertainty as a function of the beacon transmission range and the
number of beacons deployed.Beacon transmission range means signal strength of
the beacon to transmit and receive the signals which is called as Received
Signal Strength.To localize the target, we propose a grid mapping based
approach, where the readings corresponding to locations on a grid overlaid on a
region of interest are used to localize a target.To study the behavior of the
localization uncertainty as a function of the sensing radius and number of
beacons,extensive simulations and numerical experiments are carried out.The
results provide insights into an importance of optimally setting the sensing
radius and the improvement obtainable with increasing number of beacons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04997</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04997</id><created>2015-07-17</created><authors><author><keyname>Rodr&#xed;guez-Fdez</keyname><forenames>I.</forenames></author><author><keyname>Mucientes</keyname><forenames>M.</forenames></author><author><keyname>Bugar&#xed;n</keyname><forenames>A.</forenames></author></authors><title>FRULER: Fuzzy Rule Learning through Evolution for Regression</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In regression problems, the use of TSK fuzzy systems is widely extended due
to the precision of the obtained models. Moreover, the use of simple linear TSK
models is a good choice in many real problems due to the easy understanding of
the relationship between the output and input variables. In this paper we
present FRULER, a new genetic fuzzy system for automatically learning accurate
and simple linguistic TSK fuzzy rule bases for regression problems. In order to
reduce the complexity of the learned models while keeping a high accuracy, the
algorithm consists of three stages: instance selection, multi-granularity fuzzy
discretization of the input variables, and the evolutionary learning of the
rule base that uses the Elastic Net regularization to obtain the consequents of
the rules. Each stage was validated using 28 real-world datasets and FRULER was
compared with three state of the art enetic fuzzy systems. Experimental results
show that FRULER achieves the most accurate and simple models compared even
with approximative approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05014</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05014</id><created>2015-07-17</created><updated>2016-03-07</updated><authors><author><keyname>Rungger</keyname><forenames>Matthias</forenames></author><author><keyname>Zamani</keyname><forenames>Majid</forenames></author></authors><title>Compositional Construction of Approximate Abstractions of Interconnected
  Control Systems</title><categories>math.OC cs.SY</categories><msc-class>93C10</msc-class><acm-class>I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a compositional construction of approximate abstractions of
interconnected control systems. In our framework, an abstraction acts as a
substitute in the controller design process and is itself a continuous control
system. The abstraction is related to the concrete control system via a
so-called simulation function: a Lyapunov-like function, which is used to
establish a quantitative bound between the behavior of the approximate
abstraction and the concrete system. In the first part of the paper, we provide
a small gain type condition that facilitates the compositional construction of
an abstraction of an interconnected control system together with a simulation
function from the abstractions and simulation functions of the individual
subsystems. In the second part of the paper, we restrict our attention to
linear control system and characterize simulation functions in terms of
controlled invariant, externally stabilizable subspaces. Based on those
characterizations, we propose a particular scheme to construct abstractions for
linear control systems. We illustrate the compositional construction of an
abstraction on an interconnected system consisting of four linear subsystems.
We use the abstraction as a substitute to synthesize a controller to enforce a
certain linear temporal logic specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05019</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05019</id><created>2015-07-17</created><authors><author><keyname>Paudel</keyname><forenames>Subodh</forenames><affiliation>Mines Nantes</affiliation></author><author><keyname>Nguyen</keyname><forenames>Phuong H.</forenames><affiliation>Mines Nantes</affiliation></author><author><keyname>Kling</keyname><forenames>Wil L.</forenames><affiliation>Mines Nantes</affiliation></author><author><keyname>Elmitri</keyname><forenames>Mohamed</forenames><affiliation>Mines Nantes</affiliation></author><author><keyname>Lacarri&#xe8;re</keyname><forenames>Bruno</forenames><affiliation>Mines Nantes</affiliation></author><author><keyname>Corre</keyname><forenames>Olivier Le</forenames><affiliation>Mines Nantes</affiliation></author></authors><title>Support Vector Machine in Prediction of Building Energy Demand Using
  Pseudo Dynamic Approach</title><categories>cs.AI stat.AP</categories><comments>Proceedings of ECOS 2015-The 28th International Conference on
  Efficiency, Cost, Optimization, Simulation and Environmental Impact of Energy
  Systems , Jun 2015, Pau, France</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building's energy consumption prediction is a major concern in the recent
years and many efforts have been achieved in order to improve the energy
management of buildings. In particular, the prediction of energy consumption in
building is essential for the energy operator to build an optimal operating
strategy, which could be integrated to building's energy management system
(BEMS). This paper proposes a prediction model for building energy consumption
using support vector machine (SVM). Data-driven model, for instance, SVM is
very sensitive to the selection of training data. Thus the relevant days data
selection method based on Dynamic Time Warping is used to train SVM model. In
addition, to encompass thermal inertia of building, pseudo dynamic model is
applied since it takes into account information of transition of energy
consumption effects and occupancy profile. Relevant days data selection and
whole training data model is applied to the case studies of Ecole des Mines de
Nantes, France Office building. The results showed that support vector machine
based on relevant data selection method is able to predict the energy
consumption of building with a high accuracy in compare to whole data training.
In addition, relevant data selection method is computationally cheaper (around
8 minute training time) in contrast to whole data training (around 31 hour for
weekend and 116 hour for working days) and reveals realistic control
implementation for online system as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05033</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05033</id><created>2015-07-17</created><authors><author><keyname>Gomez</keyname><forenames>Luis</forenames></author><author><keyname>Alvarez</keyname><forenames>Luis</forenames></author><author><keyname>Mazorra</keyname><forenames>Luis</forenames></author><author><keyname>Frery</keyname><forenames>Alejandro C.</forenames></author></authors><title>Classification of Complex Wishart Matrices with a Diffusion-Reaction
  System guided by Stochastic Distances</title><categories>cs.CV</categories><comments>Accepted for publication in Philosophical Transactions A</comments><doi>10.1098/rsta.2015.0118</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method for PolSAR (Polarimetric Synthetic Aperture Radar)
imagery classification based on stochastic distances in the space of random
matrices obeying complex Wishart distributions. Given a collection of
prototypes $\{Z_m\}_{m=1}^M$ and a stochastic distance $d(.,.)$, we classify
any random matrix $X$ using two criteria in an iterative setup. Firstly, we
associate $X$ to the class which minimizes the weighted stochastic distance
$w_md(X,Z_m)$, where the positive weights $w_m$ are computed to maximize the
class discrimination power. Secondly, we improve the result by embedding the
classification problem into a diffusion-reaction partial differential system
where the diffusion term smooths the patches within the image, and the reaction
term tends to move the pixel values towards the closest class prototype. In
particular, the method inherits the benefits of speckle reduction by
diffusion-like methods. Results on synthetic and real PolSAR data show the
performance of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05045</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05045</id><created>2015-07-08</created><updated>2016-02-11</updated><authors><author><keyname>Csat&#xf3;</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>Ranking in Swiss system chess team tournaments</title><categories>stat.AP cs.DM cs.GT math.CO</categories><comments>39 pages, 8 figures</comments><msc-class>15A06, 91B14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper suggests a family of paired comparison-based scoring procedures
using linear algebra for ranking the participants of a Swiss system chess team
tournament. We present the fundamental challenges of ranking in Swiss system,
the features of individual and team competitions as well as the failures of
official lexicographical orders. The tournament is represented as a ranking
problem, our model is discussed with respect to the properties of the score,
generalized row sum and least squares methods, all of them obtained by solving
a certain linear system of equations.
  A case study given by the two recent chess team European championships is
analysed in detail. Final rankings are compared by their distances and
visualized with multidimensional scaling (MDS). Differences to official ranking
are revealed by the decomposition of least squares method. Rankings are
evaluated by prediction accuracy, retrodictive performance, and stability. The
paper argues for the use of least squares method with a results matrix
favouring match points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05049</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05049</id><created>2015-07-14</created><authors><author><keyname>Descal&#xe7;o</keyname><forenames>L.</forenames></author><author><keyname>Carvalho</keyname><forenames>Paula</forenames></author><author><keyname>Cruz</keyname><forenames>J. P.</forenames></author><author><keyname>Oliveira</keyname><forenames>Paula</forenames></author><author><keyname>Seabra</keyname><forenames>Dina</forenames></author></authors><title>Computer-assisted independent study in mutivariate calculus</title><categories>math.HO cs.CY</categories><comments>8 pages, EDULEARN15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning mathematics requires students to work in an independent way which is
particularly challenging for such an abstract subject. Advancements in
technology and, taking the student as the focus of his own learning, led to a
change of paradigm in the design and development of educational contents. In
this paper we describe the first experience with an interactive feedback and
assessment tool (Siacua), based on parameterized math exercises, and explain
how we use it to motivate student independent study in a multivariate calculus
environment. We have defined an index about the subject, trying to make it
consensual enough for being used in other courses about multivariate calculus.
Then we have created a concept map, selected some existing parameterized
true/false questions from PmatE project and classified them using our concept
map, for being reused in our system. For complementing the course we have
created about one hundred parameterized multiple choice question templates in
system Megua and generated about one thousand instances for using in Siacua.
Results based on data collected by this tool and also based on an informal
survey are presented. This first experience allows us to conclude our approach
has an important impact on student motivation and contributes to the success on
learning multivariate calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05053</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05053</id><created>2015-07-17</created><authors><author><keyname>O'Shea</keyname><forenames>Keiron</forenames></author></authors><title>Massively Deep Artificial Neural Networks for Handwritten Digit
  Recognition</title><categories>cs.CV cs.LG cs.NE</categories><comments>2 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Greedy Restrictive Boltzmann Machines yield an fairly low 0.72% error rate on
the famous MNIST database of handwritten digits. All that was required to
achieve this result was a high number of hidden layers consisting of many
neurons, and a graphics card to greatly speed up the rate of learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05061</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05061</id><created>2015-07-16</created><updated>2015-07-26</updated><authors><author><keyname>Younes</keyname><forenames>Ahmed</forenames></author><author><keyname>Rowe</keyname><forenames>Jonathan E.</forenames></author></authors><title>A Polynomial Time Bounded-error Quantum Algorithm for Boolean
  Satisfiability</title><categories>cs.CC cs.DS</categories><comments>15 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:1505.06284</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of the paper is to answer a long-standing open problem on the
relationship between NP and BQP. The paper shows that BQP contains NP by
proposing a BQP quantum algorithm for the MAX-E3-SAT problem which is a
fundamental NP-hard problem. Given an E3-CNF Boolean formula, the aim of the
MAX-E3-SAT problem is to find the variable assignment that maximizes the number
of satisfied clauses. The proposed algorithm runs in $O(m^2)$ for an E3-CNF
Boolean formula with $m$ clauses and in the worst case runs in $O(n^6)$ for an
E3-CNF Boolean formula with $n$ inputs. The proposed algorithm maximizes the
set of satisfied clauses using a novel iterative partial negation and partial
measurement technique. The algorithm is shown to achieve an arbitrary high
probability of success of $1-\epsilon$ for small $\epsilon&gt;0$ using a
polynomial resources. In addition to solving the MAX-E3-SAT problem, the
proposed algorithm can also be used to decide if an E3-CNF Boolean formula is
satisfiable or not, which is an NP-complete problem, based on the maximum
number of satisfied clauses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05081</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05081</id><created>2015-07-01</created><authors><author><keyname>Roach</keyname><forenames>Matthew</forenames></author></authors><title>Toward a new language of legal drafting</title><categories>cs.CY cs.DL</categories><comments>Stanford University LLM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lawyers should write in document markup language just like web developers,
digital publishers, scientists, and almost everyone else
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05085</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05085</id><created>2015-07-17</created><authors><author><keyname>Gupta</keyname><forenames>Udit</forenames></author></authors><title>Secure management of logs in internet of things</title><categories>cs.CR</categories><comments>6 pages, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ever since the advent of computing, managing data has been of extreme
importance. With innumerable devices getting added to network infrastructure,
there has been a proportionate increase in the data which needs to be stored.
With the advent of Internet of Things (IOT) it is anticipated that billions of
devices will be a part of the internet in another decade. Since those devices
will be communicating with each other on a regular basis with little or no
human intervention, plethora of real time data will be generated in quick time
which will result in large number of log files. Apart from complexity
pertaining to storage, it will be mandatory to maintain confidentiality and
integrity of these logs in IOT enabled devices. This paper will provide a brief
overview about how logs can be efficiently and securely stored in IOT devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05086</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05086</id><created>2015-07-17</created><updated>2015-07-20</updated><authors><author><keyname>Pan</keyname><forenames>Xinghao</forenames></author><author><keyname>Papailiopoulos</keyname><forenames>Dimitris</forenames></author><author><keyname>Oymak</keyname><forenames>Samet</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Parallel Correlation Clustering on Big Graphs</title><categories>cs.DC cs.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a similarity graph between items, correlation clustering (CC) groups
similar items together and dissimilar ones apart. One of the most popular CC
algorithms is KwikCluster: an algorithm that serially clusters neighborhoods of
vertices, and obtains a 3-approximation ratio. Unfortunately, KwikCluster in
practice requires a large number of clustering rounds, a potential bottleneck
for large graphs.
  We present C4 and ClusterWild!, two algorithms for parallel correlation
clustering that run in a polylogarithmic number of rounds and achieve nearly
linear speedups, provably. C4 uses concurrency control to enforce
serializability of a parallel clustering process, and guarantees a
3-approximation ratio. ClusterWild! is a coordination free algorithm that
abandons consistency for the benefit of better scaling; this leads to a
provably small loss in the 3-approximation ratio.
  We provide extensive experimental results for both algorithms, where we
outperform the state of the art, both in terms of clustering accuracy and
running time. We show that our algorithms can cluster billion-edge graphs in
under 5 seconds on 32 cores, while achieving a 15x speedup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05087</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05087</id><created>2015-07-17</created><authors><author><keyname>Giri</keyname><forenames>Ritwik</forenames></author><author><keyname>Rao</keyname><forenames>Bhaskar D.</forenames></author></authors><title>Type I and Type II Bayesian Methods for Sparse Signal Recovery using
  Scale Mixtures</title><categories>cs.LG stat.ML</categories><comments>Under Review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a generalized scale mixture family of
distributions, namely the Power Exponential Scale Mixture (PESM) family, to
model the sparsity inducing priors currently in use for sparse signal recovery
(SSR). We show that the successful and popular methods such as LASSO,
Reweighted $\ell_1$ and Reweighted $\ell_2$ methods can be formulated in an
unified manner in a maximum a posteriori (MAP) or Type I Bayesian framework
using an appropriate member of the PESM family as the sparsity inducing prior.
In addition, exploiting the natural hierarchical framework induced by the PESM
family, we utilize these priors in a Type II framework and develop the
corresponding EM based estimation algorithms. Some insight into the differences
between Type I and Type II methods is provided and of particular interest in
the algorithmic development is the Type II variant of the popular and
successful reweighted $\ell_1$ method. Extensive empirical results are provided
and they show that the Type II methods exhibit better support recovery than the
corresponding Type I methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05098</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05098</id><created>2015-07-17</created><authors><author><keyname>Alajajian</keyname><forenames>S. E.</forenames></author><author><keyname>Williams</keyname><forenames>J. R.</forenames></author><author><keyname>Reagan</keyname><forenames>A. J.</forenames></author><author><keyname>Alajajian</keyname><forenames>S. C.</forenames></author><author><keyname>Frank</keyname><forenames>M. R.</forenames></author><author><keyname>Mitchell</keyname><forenames>L.</forenames></author><author><keyname>Lahne</keyname><forenames>J.</forenames></author><author><keyname>Danforth</keyname><forenames>C. M.</forenames></author><author><keyname>Dodds</keyname><forenames>P. S.</forenames></author></authors><title>The Lexicocalorimeter: Gauging public health through caloric input and
  output on social media</title><categories>physics.soc-ph cs.CY cs.SI</categories><comments>Manuscript: 16 pages, 6 figures, 1 table, Supplementary Information:
  7 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and develop a Lexicocalorimeter: an online, interactive instrument
for measuring the &quot;caloric content&quot; of social media and other large-scale
texts. We do so by constructing extensive yet improvable tables of food and
activity related phrases, and respectively assigning them with sourced
estimates of caloric intake and expenditure. We show that for Twitter, our
naive measures of &quot;caloric input&quot;, &quot;caloric output&quot;, and the ratio of these
measures---&quot;caloric balance&quot;---are all strong correlates with health and
well-being demographics for the contiguous United States. Our caloric balance
measure outperforms both its constituent quantities, is tunable to specific
demographic measures such as diabetes rates, provides a real-time signal
reflecting a population's health, and has the potential to be used alongside
traditional survey data in the development of public policy and collective
self-awareness. Because our Lexicocalorimeter is a linear superposition of
principled phrase scores, we also show we can move beyond correlations to
explore what people talk about in collective detail, and assist in the
understanding and explanation of how population-scale conditions vary, a
capacity unavailable to black-box type methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05103</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05103</id><created>2015-07-17</created><authors><author><keyname>Barri&#xe8;re</keyname><forenames>L.</forenames></author><author><keyname>Comellas</keyname><forenames>F.</forenames></author><author><keyname>Dalf&#xf3;</keyname><forenames>C.</forenames></author><author><keyname>Fiol</keyname><forenames>M. A.</forenames></author></authors><title>Deterministic hierarchical networks</title><categories>cs.SI cs.DM math.CO</categories><msc-class>05C82, 90B10, 90B18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been shown that many networks associated with complex systems are
small-world (they have both a large local clustering coefficient and a small
diameter) and they are also scale-free (the degrees are distributed according
to a power law). Moreover, these networks are very often hierarchical, as they
describe the modularity of the systems that are modeled. Most of the studies
for complex networks are based on stochastic methods. However, a deterministic
method, with an exact determination of the main relevant parameters of the
networks, has proven useful. Indeed, this approach complements and enhances the
probabilistic and simulation techniques and, therefore, it provides a better
understanding of the systems modeled. In this paper we find the radius,
diameter, clustering coefficient and degree distribution of a generic family of
deterministic hierarchical small-world scale-free networks that has been
considered for modeling real-life complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05106</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05106</id><created>2015-07-17</created><authors><author><keyname>Alman</keyname><forenames>Josh</forenames></author><author><keyname>Williams</keyname><forenames>Ryan</forenames></author></authors><title>Probabilistic Polynomials and Hamming Nearest Neighbors</title><categories>cs.DS cs.CC math.CO</categories><comments>16 pages. To appear in 56th Annual IEEE Symposium on Foundations of
  Computer Science (FOCS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to compute any symmetric Boolean function on $n$ variables over
any field (as well as the integers) with a probabilistic polynomial of degree
$O(\sqrt{n \log(1/\epsilon)})$ and error at most $\epsilon$. The degree
dependence on $n$ and $\epsilon$ is optimal, matching a lower bound of Razborov
(1987) and Smolensky (1987) for the MAJORITY function. The proof is
constructive: a low-degree polynomial can be efficiently sampled from the
distribution.
  This polynomial construction is combined with other algebraic ideas to give
the first subquadratic time algorithm for computing a (worst-case) batch of
Hamming distances in superlogarithmic dimensions, exactly. To illustrate, let
$c(n) : \mathbb{N} \rightarrow \mathbb{N}$. Suppose we are given a database $D$
of $n$ vectors in $\{0,1\}^{c(n) \log n}$ and a collection of $n$ query vectors
$Q$ in the same dimension. For all $u \in Q$, we wish to compute a $v \in D$
with minimum Hamming distance from $u$. We solve this problem in $n^{2-1/O(c(n)
\log^2 c(n))}$ randomized time. Hence, the problem is in &quot;truly subquadratic&quot;
time for $O(\log n)$ dimensions, and in subquadratic time for $d = o((\log^2
n)/(\log \log n)^2)$. We apply the algorithm to computing pairs with maximum
inner product, closest pair in $\ell_1$ for vectors with bounded integer
entries, and pairs with maximum Jaccard coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05108</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05108</id><created>2015-06-22</created><authors><author><keyname>Zhang</keyname><forenames>Xiao</forenames></author><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author></authors><title>Multiway spectral community detection in networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>10 pages, 5 figures</comments><journal-ref>Phys. Rev. E 92, 052808 (2015)</journal-ref><doi>10.1103/PhysRevE.92.052808</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most widely used methods for community detection in networks is
the maximization of the quality function known as modularity. Of the many
maximization techniques that have been used in this context, some of the most
conceptually attractive are the spectral methods, which are based on the
eigenvectors of the modularity matrix. Spectral algorithms have, however, been
limited by and large to the division of networks into only two or three
communities, with divisions into more than three being achieved by repeated
two-way division. Here we present a spectral algorithm that can directly divide
a network into any number of communities. The algorithm makes use of a mapping
from modularity maximization to a vector partitioning problem, combined with a
fast heuristic for vector partitioning. We compare the performance of this
spectral algorithm with previous approaches and find it to give superior
results, particularly in cases where community sizes are unbalanced. We also
give demonstrative applications of the algorithm to two real-world networks and
find that it produces results in good agreement with expectations for the
networks studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05120</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05120</id><created>2015-07-17</created><authors><author><keyname>Benosman</keyname><forenames>Mouhacine</forenames></author><author><keyname>Xia</keyname><forenames>Meng</forenames></author></authors><title>Extremum Seeking-based Indirect Adaptive Control for Nonlinear Systems
  with State and Time-Dependent Uncertainties</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study in this paper the problem of adaptive trajectory tracking for
nonlinear systems affine in the control with bounded state-dependent and
time-dependent uncertainties. We propose to use a modular approach, in the
sense that we first design a robust nonlinear state feedback which renders the
closed loop input to state stable(ISS) between an estimation error of the
uncertain parameters and an output tracking error. Next, we complement this
robust ISS controller with a model-free multiparametric extremum seeking (MES)
algorithm to estimate the model uncertainties. The combination of the ISS
feedback and the MES algorithm gives an indirect adaptive controller. We show
the efficiency of this approach on a two-link robot manipulator example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05122</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05122</id><created>2015-07-17</created><authors><author><keyname>Wu</keyname><forenames>Yingxiao</forenames></author><author><keyname>Zhuang</keyname><forenames>Yan</forenames></author><author><keyname>Long</keyname><forenames>Xi</forenames></author><author><keyname>Lin</keyname><forenames>Feng</forenames></author><author><keyname>Xu</keyname><forenames>Wenyao</forenames></author></authors><title>Human Gender Classification: A Review</title><categories>cs.AI</categories><comments>12 pages, 2 figures, Journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gender contains a wide range of information regarding to the characteristics
difference between male and female. Successful gender recognition is essential
and critical for many applications in the commercial domains such as
applications of human-computer interaction and computer-aided physiological or
psychological analysis. Some have proposed various approaches for automatic
gender classification using the features derived from human bodies and/or
behaviors. First, this paper introduces the challenge and application for
gender classification research. Then, the development and framework of gender
classification are described. Besides, we compare these state-of-the-art
approaches, including vision-based methods, biological information-based
method, and social network information-based method, to provide a comprehensive
review in the area of gender classification. In mean time, we highlight the
strength and discuss the limitation of each method. Finally, this review also
discusses several promising applications for the future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05129</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05129</id><created>2015-07-17</created><authors><author><keyname>Catal&#xe1;n</keyname><forenames>Sandra</forenames></author><author><keyname>Igual</keyname><forenames>Francisco D.</forenames></author><author><keyname>Mayo</keyname><forenames>Rafael</forenames></author><author><keyname>Pi&#xf1;uel</keyname><forenames>Luis</forenames></author><author><keyname>Quintana-Ort&#xed;</keyname><forenames>Enrique S.</forenames></author><author><keyname>Rodr&#xed;guez-S&#xe1;nchez</keyname><forenames>Rafael</forenames></author></authors><title>Performance and Energy Optimization of Matrix Multiplication on
  Asymmetric big.LITTLE Processors</title><categories>cs.DC</categories><comments>Presented at HiPEAC 2015, Amsterdam. Foundation of the Asymmetric
  BLIS implementation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asymmetric processors have emerged as an appealing technology for severely
energy-constrained environments, especially in the mobile market where
heterogeneity in applications is mainstream. In addition, given the growing
interest on ultra low-power architectures for high performance computing, this
type of platforms are also being investigated in the road towards the
implementation of energy- efficient high-performance scientific applications.
In this paper, we propose a first step towards a complete implementation of the
BLAS interface adapted to asymmetric ARM big.LITTLE processors, analyzing the
trade-offs between performance and energy efficiency when compared to existing
homogeneous (symmetric) multi-threaded BLAS implementations. Our experimental
results reveal important gains in performance while maintaining the energy
efficiency of homogeneous solutions by efficiently exploiting all the resources
of the asymmetric processor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05133</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05133</id><created>2015-07-17</created><updated>2015-08-10</updated><authors><author><keyname>Arechiga</keyname><forenames>Nikos</forenames></author><author><keyname>Kapinski</keyname><forenames>James</forenames></author><author><keyname>Deshmukh</keyname><forenames>Jyotirmoy</forenames></author><author><keyname>Platzer</keyname><forenames>Andre</forenames></author><author><keyname>Krogh</keyname><forenames>Bruce</forenames></author></authors><title>Forward Invariant Cuts to Simplify Proofs of Safety</title><categories>cs.LO</categories><comments>Extended version of EMSOFT paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of deductive techniques, such as theorem provers, has several
advantages in safety verification of hybrid sys- tems; however,
state-of-the-art theorem provers require ex- tensive manual intervention.
Furthermore, there is often a gap between the type of assistance that a theorem
prover requires to make progress on a proof task and the assis- tance that a
system designer is able to provide. This paper presents an extension to
KeYmaera, a deductive verification tool for differential dynamic logic; the new
technique allows local reasoning using system designer intuition about per-
formance within particular modes as part of a proof task. Our approach allows
the theorem prover to leverage for- ward invariants, discovered using numerical
techniques, as part of a proof of safety. We introduce a new inference rule
into the proof calculus of KeYmaera, the forward invariant cut rule, and we
present a methodology to discover useful forward invariants, which are then
used with the new cut rule to complete verification tasks. We demonstrate how
our new approach can be used to complete verification tasks that lie out of the
reach of existing deductive approaches us- ing several examples, including one
involving an automotive powertrain control system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05134</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05134</id><created>2015-07-17</created><authors><author><keyname>Port</keyname><forenames>Alexander</forenames></author><author><keyname>Gheorghita</keyname><forenames>Iulia</forenames></author><author><keyname>Guth</keyname><forenames>Daniel</forenames></author><author><keyname>Clark</keyname><forenames>John M.</forenames></author><author><keyname>Liang</keyname><forenames>Crystal</forenames></author><author><keyname>Dasu</keyname><forenames>Shival</forenames></author><author><keyname>Marcolli</keyname><forenames>Matilde</forenames></author></authors><title>Persistent Topology of Syntax</title><categories>cs.CL math.AT</categories><comments>15 pages, 25 jpg figures</comments><msc-class>91F20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the persistent homology of the data set of syntactic parameters of
the world languages. We show that, while homology generators behave erratically
over the whole data set, non-trivial persistent homology appears when one
restricts to specific language families. Different families exhibit different
persistent homology. We focus on the cases of the Indo-European and the
Niger-Congo families, for which we compare persistent homology over different
cluster filtering values. We investigate the possible significance, in
historical linguistic terms, of the presence of persistent generators of the
first homology. In particular, we show that the persistent first homology
generator we find in the Indo-European family is not due (as one might guess)
to the Anglo-Norman bridge in the Indo-European phylogenetic network, but is
related to the position of Ancient Greek and the Hellenic branch within the
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05136</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05136</id><created>2015-07-17</created><updated>2015-08-20</updated><authors><author><keyname>Raghavendra</keyname><forenames>Prasad</forenames></author><author><keyname>Schramm</keyname><forenames>Tselil</forenames></author></authors><title>Tight Lower Bounds for Planted Clique in the Degree-4 SOS Program</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a lower bound of $\tilde{\Omega}(\sqrt{n})$ for the degree-4
Sum-of-Squares SDP relaxation for the planted clique problem. Specifically, we
show that on an Erd\&quot;os-R\'enyi graph $G(n,\tfrac{1}{2})$, with high
probability there is a feasible point for the degree-4 SOS relaxation of the
clique problem with an objective value of $\tilde{\Omega}(\sqrt{n})$, so that
the program cannot distinguish between a random graph and a random graph with a
planted clique of size $\tilde{O}(\sqrt{n})$. This bound is tight.
  We build on the works of Deshpande and Montanari and Meka et al., who give
lower bounds of $\tilde{\Omega}(n^{1/3})$ and $\tilde{\Omega}(n^{1/4})$
respectively. We improve on their results by making a perturbation to the SDP
solution proposed in their work, then showing that this perturbation remains
PSD as the objective value approaches $\tilde{\Omega}(n^{1/2})$.
  In an independent work, Hopkins, Kothari and Potechin [HKP15] have obtained a
similar lower bound for the degree-$4$ SOS relaxation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05138</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05138</id><created>2015-07-17</created><authors><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Fifty Years of MIMO Detection: The Road to Large-Scale MIMOs</title><categories>cs.IT math.IT</categories><comments>51 pages, 36 figures, 10 tables, 659 references, accepted to appear
  on IEEE Communications Surveys &amp; Tutorials, June 2015</comments><journal-ref>IEEE Communications Surveys &amp; Tutorials, vol. 17, no. 4, pp.
  1941-1988, Fourth Quarter 2015</journal-ref><doi>10.1109/COMST.2015.2475242</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emerging massive/large-scale MIMO (LS-MIMO) systems relying on very large
antenna arrays have become a hot topic of wireless communications. Compared to
the LTE based 4G mobile communication system that allows for up to 8 antenna
elements at the base station (BS), the LS-MIMO system entails an unprecedented
number of antennas, say 100 or more, at the BS. The huge leap in the number of
BS antennas opens the door to a new research field in communication theory,
propagation and electronics, where random matrix theory begins to play a
dominant role.
  In this paper, we provide a recital on the historic heritages and novel
challenges facing LS-MIMOs from a detection perspective. Firstly, we highlight
the fundamentals of MIMO detection, including the nature of co-channel
interference, the generality of the MIMO detection problem, the received signal
models of both linear memoryless MIMO channels and dispersive MIMO channels
exhibiting memory, as well as the complex-valued versus real-valued MIMO system
models. Then, an extensive review of the representative MIMO detection methods
conceived during the past 50 years (1965-2015) is presented, and relevant
insights as well as lessons are inferred for designing complexity-scalable MIMO
detection algorithms that are potentially applicable to LS-MIMO systems.
Furthermore, we divide the LS-MIMO systems into two types, and elaborate on the
distinct detection strategies suitable for each of them. The type-I LS-MIMO
corresponds to the case where the number of active users is much smaller than
the number of BS antennas, which is currently the mainstream definition of
LS-MIMO. The type-II LS-MIMO corresponds to the case where the number of active
users is comparable to the number of BS antennas. Finally, we discuss the
applicability of existing MIMO detection algorithms in LS-MIMO systems, and
review some of the recent advances in LS-MIMO detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05143</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05143</id><created>2015-07-17</created><authors><author><keyname>Tralie</keyname><forenames>Christopher J.</forenames></author><author><keyname>Bendich</keyname><forenames>Paul</forenames></author></authors><title>Cover Song Identification with Timbral Shape Sequences</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel low level feature for identifying cover songs which
quantifies the relative changes in the smoothed frequency spectrum of a song.
Our key insight is that a sliding window representation of a chunk of audio can
be viewed as a time-ordered point cloud in high dimensions. For corresponding
chunks of audio between different versions of the same song, these point clouds
are approximately rotated, translated, and scaled copies of each other. If we
treat MFCC embeddings as point clouds and cast the problem as a relative shape
sequence, we are able to correctly identify 42/80 cover songs in the &quot;Covers
80&quot; dataset. By contrast, all other work to date on cover songs exclusively
relies on matching note sequences from Chroma derived features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05144</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05144</id><created>2015-07-18</created><authors><author><keyname>Abdolee</keyname><forenames>Reza</forenames></author><author><keyname>Champagne</keyname><forenames>Benoit</forenames></author></authors><title>Centralized Adaptation for Parameter Estimation over Wireless Sensor
  Networks</title><categories>cs.SY math.OC stat.AP</categories><comments>IEEE Communication Letter, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the performance of centralized least mean-squares (CLMS) algorithms
in wireless sensor networks where nodes transmit their data over fading
channels to a central processing unit (e.g., fusion center or cluster head),
for parameter estimation. Wireless channel impairments, including fading and
path loss, distort the transmitted data, cause link failure and degrade the
performance of the adaptive solutions. To address this problem, we propose a
novel CLMS algorithm that uses a refined version of the transmitted data and
benefits from a link failure alarm strategy to discard severely distorted data.
Furthermore, to remove the bias due to communication noise from the estimate,
we introduce a bias-elimination scheme that also leads to a lower steady-state
mean-square error. Our theoretical findings are supported by numerical
simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05145</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05145</id><created>2015-07-18</created><authors><author><keyname>K&#xf6;nig</keyname><forenames>Daniel</forenames></author><author><keyname>Lohrey</keyname><forenames>Markus</forenames></author><author><keyname>Zetzsche</keyname><forenames>Georg</forenames></author></authors><title>Knapsack and subset sum problems in nilpotent, polycyclic, and
  co-context-free groups</title><categories>math.GR cs.FL</categories><msc-class>20F10, 68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that the knapsack problem (introduced by Myasnikov, Nikolaev, and
Ushakov) is undecidable in a direct product of sufficiently many copies of the
discrete Heisenberg group (which is nilpotent of class 2). Moreover, for the
discrete Heisenberg group itself, the knapsack problem is decidable. Hence,
decidability of the knapsack problem is not preserved under direct products. It
is also shown that for every co-context-free group, the knapsack problem is
decidable. For the subset sum problem (also introduced by Myasnikov, Nikolaev,
and Ushakov) we show that it belongs to the class NL (nondeterministic
logspace) for every finitely generated virtually nilpotent group and that there
exists a polycyclic group with an NP-complete subset sum problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05150</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05150</id><created>2015-07-18</created><updated>2015-11-20</updated><authors><author><keyname>Nwana</keyname><forenames>Amandianeze O.</forenames></author><author><keyname>Chen</keyname><forenames>Tshuan</forenames></author></authors><title>Towards Understanding User Preferences from User Tagging Behavior for
  Personalization</title><categories>cs.MM cs.HC cs.IR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personalizing image tags is a relatively new and growing area of research,
and in order to advance this research community, we must review and challenge
the de-facto standard of defining tag importance. We believe that for greater
progress to be made, we must go beyond tags that merely describe objects that
are visually represented in the image, towards more user-centric and subjective
notions such as emotion, sentiment, and preferences.
  We focus on the notion of user preferences and show that the order that users
list tags on images is correlated to the order of preference over the tags that
they provided for the image. While this observation is not completely
surprising, to our knowledge, we are the first to explore this aspect of user
tagging behavior systematically and report empirical results to support this
observation. We argue that this observation can be exploited to help advance
the image tagging (and related) communities.
  Our contributions include: 1.) conducting a user study demonstrating this
observation, 2.) collecting a dataset with user tag preferences explicitly
collected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05154</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05154</id><created>2015-07-18</created><authors><author><keyname>Abdolee</keyname><forenames>Reza</forenames></author><author><keyname>Champagne</keyname><forenames>Benoit</forenames></author></authors><title>Diffusion LMS Strategies in Sensor Networks with Noisy Input Data</title><categories>cs.SY math.OC</categories><comments>IEEE/ACM Transaction on Networking, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the performance of distributed least-mean square (LMS)
algorithms for parameter estimation over sensor networks where the regression
data of each node are corrupted by white measurement noise. Under this
condition, we show that the estimates produced by distributed LMS algorithms
will be biased if the regression noise is excluded from consideration. We
propose a bias-elimination technique and develop a novel class of diffusion LMS
algorithms that can mitigate the effect of regression noise and obtain an
unbiased estimate of the unknown parameter vector over the network. In our
development, we first assume that the variances of the regression noises are
known a-priori. Later, we relax this assumption by estimating these variances
in real-time. We analyze the stability and convergence of the proposed
algorithms and derive closed-form expressions to characterize their mean-square
error performance in transient and steady-state regimes. We further provide
computer experiment results that illustrate the efficiency of the proposed
algorithms and support the analytical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05164</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05164</id><created>2015-07-18</created><authors><author><keyname>Mironov</keyname><forenames>Andrew M.</forenames></author></authors><title>A theory of probabilistic automata, part 1</title><categories>cs.FL</categories><comments>123 pages, in Russian</comments><msc-class>68Q10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the book we present main concepts of probabilistic automata theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05169</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05169</id><created>2015-07-18</created><authors><author><keyname>Spiegelman</keyname><forenames>Alexander</forenames></author><author><keyname>Cassuto</keyname><forenames>Yuval</forenames></author><author><keyname>Chockler</keyname><forenames>Gregory</forenames></author><author><keyname>Keidar</keyname><forenames>Idit</forenames></author></authors><title>Space Bounds for Reliable Storage: Fundamental Limits of Coding</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the inherent space requirements of shared storage algorithms in
asynchronous fault-prone systems. Previous works use codes to achieve a better
storage cost than the well-known replication approach. However, a closer look
reveals that they incur extra costs somewhere else: Some use unbounded storage
in communication links, while others assume bounded concurrency or synchronous
periods. We prove here that this is inherent, and indeed, if there is no bound
on the concurrency level, then the storage cost of any reliable storage
algorithm is at least f+1 times the data size, where f is the number of
tolerated failures. We further present a technique for combining erasure-codes
with full replication so as to obtain the best of both. We present a storage
algorithm whose storage cost is close to the lower bound in the worst case, and
adapts to the concurrency level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05174</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05174</id><created>2015-07-18</created><authors><author><keyname>Fantel</keyname><forenames>Jasmin</forenames></author><author><keyname>Gao</keyname><forenames>Yan</forenames></author></authors><title>Joint Data Scheduling and FEC Coding for Multihomed Wireless Video
  Delivery</title><categories>cs.MM cs.NI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of mobile video delivery in heterogenous
wireless networks from a server to multihomed device. Most existing works only
consider delivering video streaming on single path which bandwidth is limited
causing ultimate video transmission rate. To solve this live video streaming
transmission bottleneck problem, we propose a novel solution named Joint Data
Allocation and Fountain Coding (JDAFC) method that contain below characters:
(1) path selection, (2) dynamic data allocation, and (3) fountain coding. We
evaluate the performance of JDAFC by simulation experiments using Exata and
JVSM and compare it with some reference solutions. Experimental results
represent that JDAFC outperforms the competing solutions in improving the video
peak signal-to-noise ratio as well as reducing the end-to-end delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05175</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05175</id><created>2015-07-18</created><updated>2015-07-29</updated><authors><author><keyname>Paperman</keyname><forenames>Charles</forenames></author></authors><title>Finite-Degree Predicates and Two-Variable First-Order Logic</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two-variable first-order logic on finite words with a fixed
number of quantifier alternations. We show that all languages with a neutral
letter definable using the order and finite-degree predicates are also
definable with the order predicate only. From this result we derive the
separation of the alternation hierarchy of two-variable logic on this
signature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05181</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05181</id><created>2015-07-18</created><authors><author><keyname>Balog</keyname><forenames>Matej</forenames></author><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author></authors><title>The Mondrian Process for Machine Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report is concerned with the Mondrian process and its applications in
machine learning. The Mondrian process is a guillotine-partition-valued
stochastic process that possesses an elegant self-consistency property. The
first part of the report uses simple concepts from applied probability to
define the Mondrian process and explore its properties.
  The Mondrian process has been used as the main building block of a clever
online random forest classification algorithm that turns out to be equivalent
to its batch counterpart. We outline a slight adaptation of this algorithm to
regression, as the remainder of the report uses regression as a case study of
how Mondrian processes can be utilized in machine learning. In particular, the
Mondrian process will be used to construct a fast approximation to the
computationally expensive kernel ridge regression problem with a Laplace
kernel.
  The complexity of random guillotine partitions generated by a Mondrian
process and hence the complexity of the resulting regression models is
controlled by a lifetime hyperparameter. It turns out that these models can be
efficiently trained and evaluated for all lifetimes in a given range at once,
without needing to retrain them from scratch for each lifetime value. This
leads to an efficient procedure for determining the right model complexity for
a dataset at hand.
  The limitation of having a single lifetime hyperparameter will motivate the
final Mondrian grid model, in which each input dimension is endowed with its
own lifetime parameter. In this model we preserve the property that its
hyperparameters can be tweaked without needing to retrain the modified model
from scratch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05185</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05185</id><created>2015-07-18</created><authors><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Lin</keyname><forenames>Qihang</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author></authors><title>Fast Sparse Least-Squares Regression with Non-Asymptotic Guarantees</title><categories>math.ST cs.CC stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a fast approximation method for {\it large-scale
high-dimensional} sparse least-squares regression problem by exploiting the
Johnson-Lindenstrauss (JL) transforms, which embed a set of high-dimensional
vectors into a low-dimensional space. In particular, we propose to apply the JL
transforms to the data matrix and the target vector and then to solve a sparse
least-squares problem on the compressed data with a {\it slightly larger
regularization parameter}. Theoretically, we establish the optimization error
bound of the learned model for two different sparsity-inducing regularizers,
i.e., the elastic net and the $\ell_1$ norm. Compared with previous relevant
work, our analysis is {\it non-asymptotic and exhibits more insights} on the
bound, the sample complexity and the regularization. As an illustration, we
also provide an error bound of the {\it Dantzig selector} under JL transforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05206</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05206</id><created>2015-07-18</created><updated>2015-07-21</updated><authors><author><keyname>Burstein</keyname><forenames>David</forenames></author><author><keyname>Kenter</keyname><forenames>Franklin</forenames></author><author><keyname>Kun</keyname><forenames>Jeremy</forenames></author><author><keyname>Shi</keyname><forenames>Feng</forenames></author></authors><title>Information Monitoring in Routing Networks</title><categories>cs.CR cs.DM cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the large effort devoted to cybersecurity research over the last
decades, cyber intrusions and attacks are still increasing, constantly
challenging the design of the current system. The increasingly frequent events
of BGP (Border Gateway Protocol) route hijacking have brought the topic of
traffic interception into attention. While numerous edits and improvements are
proposed for BGP, a theoretical understanding of the fundamental design of this
system is missing. Here we define and analyze an abstract model of information
monitoring in routing networks. Specifically, we study algorithms that measure
the potential of groups of dishonest agents to divert traffic through their
infrastructure under the constraint that messages must reach their intended
destinations. We relate two variants of our model based on the allowed kinds of
lies, define strategies for colluding agents, and prove optimality in special
cases. In our main theorem we derive a provably optimal monitoring strategy for
subsets of agents in which no two are adjacent, and we extend this strategy to
the general case. Finally, we use our results to analyze the susceptibility of
real and synthetic networks to endogenous information monitoring. In the
Autonomous Systems (AS) graph of the United States, we show that compromising
only 18 random nodes in the AS graph surprisingly captures 10\% of all traffic
of the network in expectation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05212</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05212</id><created>2015-07-18</created><authors><author><keyname>Dyshko</keyname><forenames>Serhii</forenames></author></authors><title>Geometric approach to the MacWilliams Extension Theorem for codes over
  modules</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The MacWilliams Extension Theorem states that each linear Hamming isometry of
a linear code extends to a monomial map. In this paper an analogue of the
extension theorem for linear codes over a module alphabet is observed. A
geometric approach to the extendability of isometries is described. For a
matrix module alphabet we found the minimum length of a code for which an
unextendable Hamming isometry exists. We also proved an extension theorem for
MDS codes over a module alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05214</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05214</id><created>2015-07-18</created><authors><author><keyname>Korba</keyname><forenames>Antonia</forenames></author></authors><title>On the Application of Link Analysis Algorithms for Ranking Bipartite
  Graphs</title><categories>cs.IR</categories><comments>Thesis in Greek for the University of Patras Engineering Diploma
  (Master in Engineering) from the Department of Computer Engineering and
  Informatics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently bipartite graphs have been widely used to represent the relationship
two sets of items for information retrieval applications. The Web offers a wide
range of data which can be represented by bipartite graphs, such us movies and
reviewers in recomender systems, queries and URLs in search engines, users and
posts in social networks. The size and the dynamic nature of such graphs
generate the need for more efficient ranking methods.
  In this thesis, at first we present the fundamental mathematical backround
that we use subsequently and we describe the basic principles of the
Perron-Frobebius theory for non negative matrices as well as the the basic
principles of the Markov chain theory. Then, we propose a novel algorithm named
BipartiteRank, which is suitable to rank scenarios, that can be represented as
a bipartite graph. This algorithm is based on the random surfer model and
inherits the basic mathematical characteristics of PageRank. What makes it
different, is the fact that it introduces an alternative type of teleportation,
based on the block structure of the bipartite graph in order to achieve more
efficient ranking. Finally, we support this opinion with mathematical arguments
and then we confirm it experimentally through a series of tests on real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05215</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05215</id><created>2015-07-18</created><authors><author><keyname>Du</keyname><forenames>Fan</forenames></author><author><keyname>Brul&#xe9;</keyname><forenames>Joshua</forenames></author><author><keyname>Enns</keyname><forenames>Peter</forenames></author><author><keyname>Manjunatha</keyname><forenames>Varun</forenames></author><author><keyname>Segev</keyname><forenames>Yoav</forenames></author></authors><title>MetroViz: Visual Analysis of Public Transportation Data</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the quality and usage of public transportation resources is
important for schedule optimization and resource allocation. Ridership and
adherence are the two main dimensions for evaluating the quality of service.
Using Automatic Vehicle Location (AVL), Automatic Passenger Count (APC), and
Global Positioning System (GPS) data, ridership data and adherence data of
public transportation can be collected. In this paper, we discuss the
development of a visualization tool for exploring public transportation data.
We introduce &quot;map view&quot; and &quot;route view&quot; to help users locate stops in the
context of geography and route information. To visualize ridership and
adherence information over several years, we introduce &quot;calendar view&quot; - a
miniaturized calendar that provides an overview of data where users can
interactively select specific days to explore individual trips and stops (&quot;trip
subview&quot; and &quot;stop subview&quot;). MetroViz was evaluated via a series of usability
tests that included researchers from the Center for Advanced Transportation
Technology (CATT) and students from the University of Maryland - College Park
in which test participants used the tool to explore three years of bus transit
data from Blacksburg, Virginia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05223</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05223</id><created>2015-07-18</created><authors><author><keyname>Klouda</keyname><forenames>Karel</forenames></author><author><keyname>Medkov&#xe1;</keyname><forenames>Kate&#x159;ina</forenames></author></authors><title>Synchronizing delay for binary uniform morphisms</title><categories>math.CO cs.FL</categories><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Circular D0L-systems are those with finite synchronizing delay. We introduce
a tool called graph of overhangs which can be used to find the minimal value of
synchronizing delay of a given D0L-system. By studying the graphs of overhangs,
a general upper bound on the minimal value of a synchronizing delay of a
circular D0L-system with a binary uniform morphism is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05224</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05224</id><created>2015-07-18</created><updated>2015-12-14</updated><authors><author><keyname>Garimella</keyname><forenames>Kiran</forenames></author><author><keyname>Morales</keyname><forenames>Gianmarco De Francisci</forenames></author><author><keyname>Gionis</keyname><forenames>Aristides</forenames></author><author><keyname>Mathioudakis</keyname><forenames>Michael</forenames></author></authors><title>Quantifying Controversy in Social Media</title><categories>cs.SI</categories><comments>To appear in WSDM 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Which topics spark the most heated debates in social media? Identifying these
topics is a first step towards creating systems which pierce echo chambers. In
this paper, we perform the first systematic methodological study of controversy
detection using social-media network structure and content.
  Unlike previous work, rather than identifying controversy in a single
hand-picked topic and use domain-specific knowledge, we focus on comparing
topics in any domain. Our approach to quantifying controversy is a graph-based
three-stage pipeline, which involves (i) building a conversation graph about a
topic, which represents alignment of opinion among users; (ii) partitioning the
conversation graph to identify potential sides of controversy; and (iii)
measuring the amount of controversy from characteristics of the graph.
  We perform an extensive comparison of controversy measures, as well as graph
building approaches and data sources. We use both controversial and
non-controversial topics on Twitter, as well as other external datasets. We
find that our new random-walk-based measure outperforms existing ones in
capturing the intuitive notion of controversy, and show that content features
are vastly less helpful in this task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05228</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05228</id><created>2015-07-18</created><authors><author><keyname>Abdolee</keyname><forenames>Reza</forenames></author><author><keyname>Champagne</keyname><forenames>Benoit</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Diffusion Adaptation over Multi-Agent Networks with Wireless Link
  Impairments</title><categories>cs.SY cs.DC cs.IT math.IT math.PR</categories><comments>IEEE Transaction on Mobile Computing, July 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the performance of diffusion least-mean-square algorithms for
distributed parameter estimation in multi-agent networks when nodes exchange
information over wireless communication links. Wireless channel impairments,
such as fading and path-loss, adversely affect the exchanged data and cause
instability and performance degradation if left unattended. To mitigate these
effects, we incorporate equalization coefficients into the diffusion
combination step and update the combination weights dynamically in the face of
randomly changing neighborhoods due to fading conditions. When channel state
information (CSI) is unavailable, we determine the equalization factors from
pilot-aided channel coefficient estimates. The analysis reveals that by
properly monitoring the CSI over the network and choosing sufficiently small
adaptation step-sizes, the diffusion strategies are able to deliver
satisfactory performance in the presence of fading and path loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05230</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05230</id><created>2015-07-18</created><authors><author><keyname>Hopkins</keyname><forenames>Samuel B.</forenames></author><author><keyname>Kothari</keyname><forenames>Pravesh K.</forenames></author><author><keyname>Potechin</keyname><forenames>Aaron</forenames></author></authors><title>SoS and Planted Clique: Tight Analysis of MPW Moments at all Degrees and
  an Optimal Lower Bound at Degree Four</title><categories>cs.CC</categories><comments>67 pages, 2 figures</comments><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding large cliques in random graphs and its &quot;planted&quot;
variant, where one wants to recover a clique of size $\omega \gg \log{(n)}$
added to an \Erdos-\Renyi graph $G \sim G(n,\frac{1}{2})$, have been intensely
studied. Nevertheless, existing polynomial time algorithms can only recover
planted cliques of size $\omega = \Omega(\sqrt{n})$. By contrast, information
theoretically, one can recover planted cliques so long as $\omega \gg
\log{(n)}$. In this work, we continue the investigation of algorithms from the
sum of squares hierarchy for solving the planted clique problem begun by Meka,
Potechin, and Wigderson (MPW, 2015) and Deshpande and Montanari (DM,2015). Our
main results improve upon both these previous works by showing:
  1. Degree four SoS does not recover the planted clique unless $\omega \gg
\sqrt n poly \log n$, improving upon the bound $\omega \gg n^{1/3}$ due to DM.
A similar result was obtained independently by Raghavendra and Schramm (2015).
  2. For $2 &lt; d = o(\sqrt{\log{(n)}})$, degree $2d$ SoS does not recover the
planted clique unless $\omega \gg n^{1/(d + 1)} /(2^d poly \log n)$, improving
upon the bound due to MPW.
  Our proof for the second result is based on a fine spectral analysis of the
certificate used in the prior works MPW,DM and Feige and Krauthgamer (2003) by
decomposing it along an appropriately chosen basis. Along the way, we develop
combinatorial tools to analyze the spectrum of random matrices with dependent
entries and to understand the symmetries in the eigenspaces of the set
symmetric matrices inspired by work of Grigoriev (2001).
  An argument of Kelner shows that the first result cannot be proved using the
same certificate. Rather, our proof involves constructing and analyzing a new
certificate that yields the nearly tight lower bound by &quot;correcting&quot; the
certificate of previous works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05233</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05233</id><created>2015-07-18</created><authors><author><keyname>Abdolee</keyname><forenames>Reza</forenames></author><author><keyname>Champagne</keyname><forenames>Benoit</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Estimation of Space-Time Varying Parameters Using a Diffusion LMS
  Algorithm</title><categories>cs.SY math.PR</categories><comments>IEEE Transaction on Signal Processing, Oct. 2013</comments><journal-ref>IEEE Transaction on Signal Processing, 62(2014) 403--418</journal-ref><doi>10.1109/TSP.2013.2289888</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of distributed adaptive estimation over networks where
nodes cooperate to estimate physical parameters that can vary over both space
and time domains. We use a set of basis functions to characterize the
space-varying nature of the parameters and propose a diffusion least
mean-squares (LMS) strategy to recover these parameters from successive time
measurements. We analyze the stability and convergence of the proposed
algorithm, and derive closed-form expressions to predict its learning behavior
and steady-state performance in terms of mean-square error. We find that in the
estimation of the space-varying parameters using distributed approaches, the
covariance matrix of the regression data at each node becomes rank-deficient.
Our analysis reveals that the proposed algorithm can overcome this difficulty
to a large extent by benefiting from the network stochastic matrices that are
used to combine exchanged information between nodes. We provide computer
experiments to illustrate and support the theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05240</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05240</id><created>2015-07-18</created><authors><author><keyname>Sinha</keyname><forenames>Abhishek</forenames></author><author><keyname>Paschos</keyname><forenames>Georgios</forenames></author><author><keyname>Li</keyname><forenames>Chih-ping</forenames></author><author><keyname>Modiano</keyname><forenames>Eytan</forenames></author></authors><title>Throughput-Optimal Multihop Broadcast on Directed Acyclic Wireless
  Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of efficiently broadcasting packets in multi-hop
wireless networks. At each time slot the network controller activates a set of
non-interfering links and forwards selected copies of packets on each activated
link. A packet is considered jointly received only when all nodes in the
network have obtained a copy of it. The maximum rate of jointly received
packets is referred to as the broadcast capacity of the network. Existing
policies achieve the broadcast capacity by balancing traffic over a set of
spanning trees, which are difficult to maintain in a large and time-varying
wireless network. We propose a new dynamic algorithm that achieves the
broadcast capacity when the underlying network topology is a directed acyclic
graph (DAG). This algorithm is decentralized, utilizes local queue-length
information only and does not require the use of global topological structures
such as spanning trees. The principal technical challenge inherent in the
problem is the absence of work-conservation principle due to the duplication of
packets, which renders traditional queuing modelling inapplicable. We overcome
this difficulty by studying relative packet deficits and imposing in-order
delivery constraints to every node in the network. Although in-order packet
delivery, in general, leads to degraded throughput in graphs with cycles, we
show that it is throughput optimal in DAGs and can be exploited to simplify the
design and analysis of optimal algorithms. Our characterization leads to a
polynomial time algorithm for computing the broadcast capacity of any wireless
DAG under the primary interference constraints. Additionally, we propose an
extension of our algorithm which can be effectively used for broadcasting in
any network with arbitrary topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05242</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05242</id><created>2015-07-18</created><authors><author><keyname>Acharya</keyname><forenames>Subhashri</forenames></author><author><keyname>Srimany</keyname><forenames>Pramita</forenames></author><author><keyname>Kundu</keyname><forenames>Sanchari</forenames></author><author><keyname>Dastidar</keyname><forenames>JayatiGhosh</forenames></author></authors><title>Data Hiding in Video using Triangularization LSB Technique</title><categories>cs.MM</categories><journal-ref>International Journal of Advanced Trends in Computer Science and
  Engineering, Volume 4, No.3, May - June 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The importance of data hiding in the field of Information Technology is a
widely accepted. The challenge is to be able to pass information in a manner
that the very existence of the message is unknown in order to repel attention
of the potential attacker. Steganography is a technique that has been widely
used to achieve this objective. However Steganography is often found to be
lacking when it comes to hiding bulk data. Attempting to hide data in a video
overcomes this problem because of the large sized cover object (video) as
compared to an image in the case of steganography. This paper attempts to
propose a scheme using which data can be hidden in a video. We focus on the
Triangularization method and make use of the Least Significant Bit (LSB)
technique in hiding messages in a video.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05243</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05243</id><created>2015-07-18</created><authors><author><keyname>Paul</keyname><forenames>Jonathan Fidelis</forenames></author><author><keyname>Seth</keyname><forenames>Dibyabiva</forenames></author><author><keyname>Paul</keyname><forenames>Cijo</forenames></author><author><keyname>Dastidar</keyname><forenames>Jayati Ghosh</forenames></author></authors><title>Hand Gesture Recognition Library</title><categories>cs.CV</categories><journal-ref>International Journal of Science and Applied Information
  Technology, Volume 3, No.2, March - April 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we have presented a hand gesture recognition library. Various
functions include detecting cluster count, cluster orientation, finger pointing
direction, etc. To use these functions first the input image needs to be
processed into a logical array for which a function has been developed. The
library has been developed keeping flexibility in mind and thus provides
application developers a wide range of options to develop custom gestures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05244</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05244</id><created>2015-07-18</created><authors><author><keyname>Dastidar</keyname><forenames>Jayati Ghosh</forenames></author><author><keyname>Sarkar</keyname><forenames>Surabhi</forenames></author><author><keyname>Sinha</keyname><forenames>Rick Punyadyuti</forenames></author><author><keyname>Basu</keyname><forenames>Kasturi</forenames></author></authors><title>Handwriting Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the method to recognize offline handwritten characters.
A robust algorithm for handwriting segmentation is described here with the help
of which individual characters can be segmented from a selected word from a
paragraph of handwritten text image which is given as input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05245</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05245</id><created>2015-07-18</created><authors><author><keyname>Thakur</keyname><forenames>Gautam S.</forenames></author><author><keyname>Bhaduri</keyname><forenames>Budhendra L.</forenames></author><author><keyname>Piburn</keyname><forenames>Jesse O.</forenames></author><author><keyname>Sims</keyname><forenames>Kelly M.</forenames></author><author><keyname>Stewart</keyname><forenames>Robert N.</forenames></author><author><keyname>Urban</keyname><forenames>Marie L.</forenames></author></authors><title>PlanetSense: A Real-time Streaming and Spatio-temporal Analytics
  Platform for Gathering Geo-spatial Intelligence from Open Source Data</title><categories>cs.CY cs.SI</categories><acm-class>C.3; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geospatial intelligence has traditionally relied on the use of archived and
unvarying data for planning and exploration purposes. In consequence, the tools
and methods that are architected to provide insight and generate projections
only rely on such datasets. Albeit, if this approach has proven effective in
several cases, such as land use identification and route mapping, it has
severely restricted the ability of researchers to inculcate current information
in their work. This approach is inadequate in scenarios requiring real-time
information to act and to adjust in ever changing dynamic environments, such as
evacuation and rescue missions. In this work, we propose PlanetSense, a
platform for geospatial intelligence that is built to harness the existing
power of archived data and add to that, the dynamics of real-time streams,
seamlessly integrated with sophisticated data mining algorithms and analytics
tools for generating operational intelligence on the fly. The platform has four
main components - i. GeoData Cloud - a data architecture for storing and
managing disparate datasets; ii. Mechanism to harvest real-time streaming data;
iii. Data analytics framework; iv. Presentation and visualization through web
interface and RESTful services. Using two case studies, we underpin the
necessity of our platform in modeling ambient population and building occupancy
at scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05248</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05248</id><created>2015-07-19</created><authors><author><keyname>Zha</keyname><forenames>Yilong</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Zhou</keyname><forenames>Changsong</forenames></author></authors><title>Unfolding large-scale online collaborative human dynamics</title><categories>physics.soc-ph cs.SI</categories><comments>9 pages and 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale interacting human activities underlie all social and economic
phenomena, but quantitative understanding of regular patterns and mechanism is
very challenging and still rare. Self-organized online collaborative activities
with precise record of event timing provide unprecedented opportunity. Our
empirical analysis of the history of millions of updates in Wikipedia shows a
universal double power-law distribution of time intervals between consecutive
updates of an article. We then propose a generic model to unfold collaborative
human activities into three modules: (i) individual behavior characterized by
Poissonian initiation of an action, (ii) human interaction captured by a
cascading response to others with a power-law waiting time, and (iii)
population growth due to increasing number of interacting individuals. This
unfolding allows us to obtain analytical formula that is fully supported by the
universal patterns in empirical data. Our modeling approaches reveal
&quot;simplicity&quot; beyond complex interacting human activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05251</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05251</id><created>2015-07-19</created><updated>2016-01-05</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Petersen</keyname><forenames>Alexander</forenames></author><author><keyname>Ivanova</keyname><forenames>Inga</forenames></author></authors><title>The Self-Organization of Meaning and the Reflexive Communication of
  Information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following a suggestion of Warren Weaver, we extend the Shannon model of
communication piecemeal into a complex systems model in which communication is
differentiated both vertically and horizontally. This model enables us to
bridge the divide between Niklas Luhmann's theory of the self-organization of
meaning in communications and empirical research using information theory.
First, we distinguish between communication relations and correlations among
patterns of relations. The correlations span a vector space in which relations
are positioned and can be provided with meaning. Second, positions provide
reflexive perspectives. Whereas the different meanings are integrated locally,
each instantiation opens global perspectives--&quot;horizons of meaning&quot;--along
eigenvectors of the communication matrix. These next-order codifications of
meaning can be expected to generate redundancies when interacting in
instantiations. Increases in redundancy indicate new options and can be
measured as local reduction of prevailing uncertainty (in bits). The systemic
generation of new options can be considered as a hallmark of the
knowledge-based economy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05259</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05259</id><created>2015-07-19</created><updated>2015-10-29</updated><authors><author><keyname>Zafar</keyname><forenames>Muhammad Bilal</forenames></author><author><keyname>Valera</keyname><forenames>Isabel</forenames></author><author><keyname>Rodriguez</keyname><forenames>Manuel Gomez</forenames></author><author><keyname>Gummadi</keyname><forenames>Krishna P.</forenames></author></authors><title>Learning Fair Classifiers</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated data-driven decision systems are ubiquitous across a wide variety
of online services, from online social networking and e-commerce to
e-government. These systems rely on complex learning methods and vast amounts
of data to optimize the service functionality, satisfaction of the end user and
profitability. However, there is a growing concern that these automated
decisions can lead to user discrimination, even in the absence of intent,
leading to a lack of fairness, i.e., their outcomes have a disproportionally
large adverse impact on particular groups of people sharing one or more
sensitive attributes (e.g., race, sex). In this paper, we introduce a flexible
mechanism to design fair classifiers in a principled manner. Then, we
instantiate this mechanism on three well-known classifiers -- logistic
regression, hinge loss and linear and nonlinear support vector machines.
Experiments on both synthetic and real-world data show that our mechanism
allows for a fine-grained control of the level of fairness, often at a minimal
cost in terms of accuracy, and it provides more flexibility than alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05263</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05263</id><created>2015-07-19</created><authors><author><keyname>Ciuonzo</keyname><forenames>Domenico</forenames></author><author><keyname>De Maio</keyname><forenames>Antonio</forenames></author><author><keyname>Orlando</keyname><forenames>Danilo</forenames></author></authors><title>A Unifying Framework for Adaptive Radar Detection in Homogeneous plus
  Structured Interference-Part I: On the Maximal Invariant Statistic</title><categories>cs.IT math.IT stat.ME</categories><comments>Submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of adaptive multidimensional/multichannel
signal detection in homogeneous Gaussian disturbance with unknown covariance
matrix and structured deterministic interference. The aforementioned problem
corresponds to a generalization of the well-known Generalized Multivariate
Analysis of Variance (GMANOVA). In this first part of the work, we formulate
the considered problem in canonical form and, after identifying a desirable
group of transformations for the considered hypothesis testing, we derive a
Maximal Invariant Statistic (MIS) for the problem at hand. Furthermore, we
provide the MIS distribution in the form of a stochastic representation.
Finally, strong connections to the MIS obtained in the open literature in
simpler scenarios are underlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05266</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05266</id><created>2015-07-19</created><authors><author><keyname>Ciuonzo</keyname><forenames>Domenico</forenames></author><author><keyname>De Maio</keyname><forenames>Antonio</forenames></author><author><keyname>Orlando</keyname><forenames>Danilo</forenames></author></authors><title>A Unifying Framework for Adaptive Radar Detection in Homogeneous plus
  Structured Interference-Part II: Detectors Design</title><categories>cs.IT math.IT stat.ME</categories><comments>Submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of adaptive multidimensional/multichannel
signal detection in homogeneous Gaussian disturbance with unknown covariance
matrix and structured (unknown) deterministic interference. The aforementioned
problem extends the well-known Generalized Multivariate Analysis of Variance
(GMANOVA) tackled in the open literature. In a companion paper, we have
obtained the Maximal Invariant Statistic (MIS) for the problem under
consideration, as an enabling tool for the design of suitable detectors which
possess the Constant False-Alarm Rate (CFAR) property. Herein, we focus on the
development of several theoretically-founded detectors for the problem under
consideration. First, all the considered detectors are shown to be function of
the MIS, thus proving their CFARness property. Secondly, coincidence or
statistical equivalence among some of them in such a general signal model is
proved. Thirdly, strong connections to well-known simpler scenarios found in
adaptive detection literature are established. Finally, simulation results are
provided for a comparison of the proposed receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05268</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05268</id><created>2015-07-19</created><authors><author><keyname>Dalal</keyname><forenames>Gal</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Reinforcement Learning for the Unit Commitment Problem</title><categories>cs.AI</categories><comments>Accepted and presented in IEEE PES PowerTech, Eindhoven 2015, paper
  ID 462731</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we solve the day-ahead unit commitment (UC) problem, by
formulating it as a Markov decision process (MDP) and finding a low-cost policy
for generation scheduling. We present two reinforcement learning algorithms,
and devise a third one. We compare our results to previous work that uses
simulated annealing (SA), and show a 27% improvement in operation costs, with
running time of 2.5 minutes (compared to 2.5 hours of existing
state-of-the-art).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05272</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05272</id><created>2015-07-19</created><authors><author><keyname>Koponen</keyname><forenames>Laura</forenames></author><author><keyname>Oikarinen</keyname><forenames>Emilia</forenames></author><author><keyname>Janhunen</keyname><forenames>Tomi</forenames></author><author><keyname>S&#xe4;il&#xe4;</keyname><forenames>Laura</forenames></author></authors><title>Optimizing Phylogenetic Supertrees Using Answer Set Programming</title><categories>cs.CE cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP),
  Proceedings of ICLP 2015</comments><msc-class>68T30</msc-class><journal-ref>Theory and Practice of Logic Programming 15 (2015) 604-619</journal-ref><doi>10.1017/S1471068415000265</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The supertree construction problem is about combining several phylogenetic
trees with possibly conflicting information into a single tree that has all the
leaves of the source trees as its leaves and the relationships between the
leaves are as consistent with the source trees as possible. This leads to an
optimization problem that is computationally challenging and typically
heuristic methods, such as matrix representation with parsimony (MRP), are
used. In this paper we consider the use of answer set programming to solve the
supertree construction problem in terms of two alternative encodings. The first
is based on an existing encoding of trees using substructures known as
quartets, while the other novel encoding captures the relationships present in
trees through direct projections. We use these encodings to compute a
genus-level supertree for the family of cats (Felidae). Furthermore, we compare
our results to recent supertrees obtained by the MRP method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05274</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05274</id><created>2015-07-19</created><authors><author><keyname>Ibrahim</keyname><forenames>Abdelrahman Abbas</forenames></author><author><keyname>Salman</keyname><forenames>Nael</forenames></author></authors><title>Multi-Lingual Ontology Server (MOS) for discovering Web services</title><categories>cs.SE</categories><comments>6 pages, 3 figures</comments><journal-ref>International Journal of Computer Science Trends and Technology
  (IJCST) V3(4): Page(18-22) Jul-Aug 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Searching for appropriate web services on the internet is becoming more and
more laborious, because it depends on human processing and evaluating of the
available web services in UDDI repositories. Furthermore, if the requester
language is different form available WSDL files then it would be more
complicated. If this process could be done automatically, this will save effort
and time. In order to make this factual, ontologies and semantic web
technologies were used, ontology is needed to facilitate interoperability
between agents and web services, to make them interoperate semantically, and to
make processing of the data could be achieved automatically. In paper we
propose an ontology server expected to help searching and selecting appropriate
web service even if it is available in UDDI in different language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05275</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05275</id><created>2015-07-19</created><authors><author><keyname>Khatun</keyname><forenames>Shanjida</forenames></author><author><keyname>Alam</keyname><forenames>Hasib Ul</forenames></author><author><keyname>Shatabda</keyname><forenames>Swakkhar</forenames></author></authors><title>An Efficient Genetic Algorithm for Discovering Diverse-Frequent Patterns</title><categories>cs.AI</categories><comments>2015 International Conference on Electrical Engineering and
  Information Communication Technology (ICEEICT)</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Working with exhaustive search on large dataset is infeasible for several
reasons. Recently, developed techniques that made pattern set mining feasible
by a general solver with long execution time that supports heuristic search and
are limited to small datasets only. In this paper, we investigate an approach
which aims to find diverse set of patterns using genetic algorithm to mine
diverse frequent patterns. We propose a fast heuristic search algorithm that
outperforms state-of-the-art methods on a standard set of benchmarks and
capable to produce satisfactory results within a short period of time. Our
proposed algorithm uses a relative encoding scheme for the patterns and an
effective twin removal technique to ensure diversity throughout the search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05278</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05278</id><created>2015-07-19</created><authors><author><keyname>Feng</keyname><forenames>Yuan</forenames></author><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author></authors><title>Toward automatic verification of quantum cryptographic protocols</title><categories>cs.CR quant-ph</categories><comments>Accepted by Concur'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several quantum process algebras have been proposed and successfully applied
in verification of quantum cryptographic protocols. All of the bisimulations
proposed so far for quantum processes in these process algebras are
state-based, implying that they only compare individual quantum states, but not
a combination of them. This paper remedies this problem by introducing a novel
notion of distribution-based bisimulation for quantum processes. We further
propose an approximate version of this bisimulation that enables us to prove
more sophisticated security properties of quantum protocols which cannot be
verified using the previous bisimulations. In particular, we prove that the
quantum key distribution protocol BB84 is sound and (asymptotically) secure
against the intercept-resend attacks by showing that the BB84 protocol, when
executed with such an attacker concurrently, is approximately bisimilar to an
ideal protocol, whose soundness and security are obviously guaranteed, with at
most an exponentially decreasing gap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05281</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05281</id><created>2015-07-19</created><authors><author><keyname>Yoshioka</keyname><forenames>Hidekazu</forenames></author></authors><title>On Dual-Finite Volume Methods for Extended Porous Medium Equations</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article shows that the unconditional stability of the Dual-Finite Volume
Method, which is at least valid for linear problems, is not true for generic
nonlinear differential equations including the PMEs unless the coefficient
appearing in the numerical fluxes are appropriately evaluated. This article
provides a theoretically truly isotone numerical fluxes specialized for solving
the PMEs presented, which is still as simple as the conventional fully-upwind
counterpart.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05282</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05282</id><created>2015-07-19</created><updated>2015-12-09</updated><authors><author><keyname>Chatterjee</keyname><forenames>Kingshuk</forenames></author><author><keyname>Ray</keyname><forenames>Kumar Sankar</forenames></author></authors><title>Watson-Crick Quantum Finite Automata</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  1-way quantum finite automata are deterministic and reversible in nature,
which greatly reduces its accepting property. In fact the set of languages
accepted by 1-way quantum finite automata is a proper subset of regular
languages. In this paper we replace the tape head of 1-way quantum finite
automata with DNA double strand and name the model Watson-Crick quantum finite
automata. The non-injective complementarity relation of Watson-Crick automata
introduces non-determinism in the quantum model. We show that this introduction
of non-determinism increases the computational power of 1-way Quantum finite
automata significantly. We establish that Watson-Crick quantum finite automata
can accept all regular languages and that it also accepts some languages not
accepted by any multihead deterministic finite automata. Exploiting the
superposition property of quantum finite automata we show that Watson-Crick
quantum finite automata accept the language L=ww where w belongs to {a,b}*.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05283</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05283</id><created>2015-07-19</created><updated>2015-12-09</updated><authors><author><keyname>Chatterjee</keyname><forenames>Kingshuk</forenames></author><author><keyname>Ray</keyname><forenames>Kumar Sankar</forenames></author></authors><title>Reversible Watson-Crick Automata</title><categories>cs.FL</categories><comments>arXiv admin note: text overlap with arXiv:1507.05282</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Watson-Crick automata are finite automata working on double strands.
Extensive research work has already been done on non-deterministic Watson-Crick
automata and on deterministic Watson-Crick automata. In this paper, we
introduce a new model of Watson-Crick automata which is reversible in nature
named reversible Watson-Crick automata and explore its computational power. We
show even though the model is reversible and one way it accepts all regular
languages and also analyze the state complexity of the above stated model with
respect to non-deterministic block automata and non-deterministic finite
automata and establish its superiority. We further explore the relation of the
reversible model with twin-shuffle language and recursively enumerable
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05284</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05284</id><created>2015-07-19</created><authors><author><keyname>Chatterjee</keyname><forenames>Kingshuk</forenames></author><author><keyname>Ray</keyname><forenames>Kumar Sankar</forenames></author></authors><title>Deterministic parallel communicating Watson-Crick automata systems</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have introduced the deterministic variant of parallel
communicating Watson-Crick automata systems. We show that similar to the
non-deterministic version, the deterministic version can also recognise some
non-regular uniletter languages. We further establish that strongly
deterministic Watson-Crick automata systems and deterministic Watson-Crick
automata system are incomparable in terms of their computational ability. We
have also compared the computational ability of our system with multihead
finite automata and parallel communicating finite automata systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05290</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05290</id><created>2015-07-19</created><authors><author><keyname>Kaji</keyname><forenames>Shizuo</forenames></author><author><keyname>Ochiai</keyname><forenames>Hiroyuki</forenames></author></authors><title>A concise parametrisation of affine transformation</title><categories>cs.GR</categories><acm-class>I.3.5, I.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Good parametrisation of affine transformations is essential to interpolation,
deformation and analysis of shapes and animation. It has been one of the
central research topics in computer graphics. However, there is no single
perfect method and each one has both advantages and disadvantages. In this
paper, we propose a novel parametrisation of affine transformation, which is a
generalisation to or an improvement of some existing methods. Our method adds
yet another choice to existing toolbox and shows better performance in some
applications. A C++ implementation is available to make our framework ready to
use in various applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05305</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05305</id><created>2015-07-19</created><authors><author><keyname>Yanofsky</keyname><forenames>Noson S.</forenames></author></authors><title>Computability and Complexity of Categorical Structures</title><categories>cs.CC cs.LO math.CT</categories><comments>33 pages</comments><msc-class>18-XX, 03-XX, 03D15, 68Q30, 03D10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine various categorical structures that can and cannot be constructed.
We show that total computable functions can be mimicked by constructible
functors. More generally, whatever can be done by a Turing machine can be
constructed by categories. Since there are infinitary constructions in category
theory, it is shown that category theory is strictly more powerful than Turing
machines. In particular, categories can solve the Halting Problem for Turing
machines. We also show that categories can solve any problem in the arithmetic
hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05307</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05307</id><created>2015-07-19</created><authors><author><keyname>Ben-David</keyname><forenames>Shai</forenames></author></authors><title>2 Notes on Classes with Vapnik-Chervonenkis Dimension 1</title><categories>cs.LG</categories><acm-class>G.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Vapnik-Chervonenkis dimension is a combinatorial parameter that reflects
the &quot;complexity&quot; of a set of sets (a.k.a. concept classes). It has been
introduced by Vapnik and Chervonenkis in their seminal 1971 paper and has since
found many applications, most notably in machine learning theory and in
computational geometry. Arguably the most influential consequence of the VC
analysis is the fundamental theorem of statistical machine learning, stating
that a concept class is learnable (in some precise sense) if and only if its
VC-dimension is finite. Furthermore, for such classes a most simple learning
rule - empirical risk minimization (ERM) - is guaranteed to succeed.
  The simplest non-trivial structures, in terms of the VC-dimension, are the
classes (i.e., sets of subsets) for which that dimension is 1.
  In this note we show a couple of curious results concerning such classes. The
first result shows that such classes share a very simple structure, and, as a
corollary, the labeling information contained in any sample labeled by such a
class can be compressed into a single instance.
  The second result shows that due to some subtle measurability issues, in
spite of the above mentioned fundamental theorem, there are classes of
dimension 1 for which an ERM learning rule fails miserably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05313</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05313</id><created>2015-07-19</created><updated>2015-11-16</updated><authors><author><keyname>Zhang</keyname><forenames>Anderson Y.</forenames></author><author><keyname>Zhou</keyname><forenames>Harrison H.</forenames></author></authors><title>Minimax Rates of Community Detection in Stochastic Block Models</title><categories>math.ST cs.SI stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently network analysis has gained more and more attentions in statistics,
as well as in computer science, probability, and applied mathematics. Community
detection for the stochastic block model (SBM) is probably the most studied
topic in network analysis. Many methodologies have been proposed. Some
beautiful and significant phase transition results are obtained in various
settings. In this paper, we provide a general minimax theory for community
detection. It gives minimax rates of the mis-match ratio for a wide rage of
settings including homogeneous and inhomogeneous SBMs, dense and sparse
networks, finite and growing number of communities. The minimax rates are
exponential, different from polynomial rates we often see in statistical
literature. An immediate consequence of the result is to establish threshold
phenomenon for strong consistency (exact recovery) as well as weak consistency
(partial recovery). We obtain the upper bound by a range of penalized
likelihood-type approaches. The lower bound is achieved by a novel reduction
from a global mis-match ratio to a local clustering problem for one node
through an exchangeability property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05316</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05316</id><created>2015-07-19</created><authors><author><keyname>Barbier</keyname><forenames>Morgan</forenames><affiliation>GREYC</affiliation></author><author><keyname>Cheballah</keyname><forenames>Hayat</forenames><affiliation>GREYC</affiliation></author><author><keyname>Bars</keyname><forenames>Jean-Marie Le</forenames></author></authors><title>Properties and constructions of coincident functions</title><categories>cs.CR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extensive studies of Boolean functions are carried in many fields. The Mobius
transform is often involved for these studies. In particular, it plays a
central role in coincident functions, the class of Boolean functions invariant
by this transformation. This class -- which has been recently introduced -- has
interesting properties, in particular if we want to control both the Hamming
weight and the degree. We propose an innovative way to handle the Mobius
transform which allows the composition between several Boolean functions and
the use of Shannon or Reed-Muller decompositions. Thus we benefit from a better
knowledge of coin-cident functions and introduce new properties. We show
experimentally that for many features, coincident functions look like any
Boolean functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05317</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05317</id><created>2015-07-19</created><authors><author><keyname>Heged&#xfc;s</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Li</keyname><forenames>Zijia</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author><author><keyname>Schr&#xf6;cker</keyname><forenames>Hans-Peter</forenames></author></authors><title>From the Fundamental Theorem of Algebra to Kempe's Universality Theorem</title><categories>math.RA cs.RO</categories><journal-ref>Internat. Math. Nachrichten, Nr. 229 (2015), 13-26</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article provides a gentle introduction for a general mathematical
audience to the factorization theory of motion polynomials and its application
in mechanism science. This theory connects in a rather unexpected way a
seemingly abstract mathematical topic, the non-unique factorization of certain
polynomials over the ring of dual quaternions, with engineering applications.
Four years after its introduction, it is already clear how beneficial it has
been to both fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05331</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05331</id><created>2015-07-19</created><authors><author><keyname>Bayer</keyname><forenames>Justin</forenames></author><author><keyname>Karl</keyname><forenames>Maximilian</forenames></author><author><keyname>Korhammer</keyname><forenames>Daniela</forenames></author><author><keyname>van der Smagt</keyname><forenames>Patrick</forenames></author></authors><title>Fast Adaptive Weight Noise</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Marginalising out uncertain quantities within the internal representations or
parameters of neural networks is of central importance for a wide range of
learning techniques, such as empirical, variational or full Bayesian methods.
We set out to generalise fast dropout (Wang &amp; Manning, 2013) to cover a wider
variety of noise processes in neural networks. This leads to an efficient
calculation of the marginal likelihood and predictive distribution which evades
sampling and the consequential increase in training time due to highly variant
gradient estimates. This allows us to approximate variational Bayes for the
parameters of feed-forward neural networks. Inspired by the minimum description
length principle, we also propose and experimentally verify the direct
optimisation of the regularised predictive distribution. The methods yield
results competitive with previous neural network based approaches and Gaussian
processes on a wide range of regression tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05348</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05348</id><created>2015-07-19</created><authors><author><keyname>Cai</keyname><forenames>Zhaowei</forenames></author><author><keyname>Saberian</keyname><forenames>Mohammad</forenames></author><author><keyname>Vasconcelos</keyname><forenames>Nuno</forenames></author></authors><title>Learning Complexity-Aware Cascades for Deep Pedestrian Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of complexity-aware cascaded detectors, combining features of very
different complexities, is considered. A new cascade design procedure is
introduced, by formulating cascade learning as the Lagrangian optimization of a
risk that accounts for both accuracy and complexity. A boosting algorithm,
denoted as complexity aware cascade training (CompACT), is then derived to
solve this optimization. CompACT cascades are shown to seek an optimal
trade-off between accuracy and complexity by pushing features of higher
complexity to the later cascade stages, where only a few difficult candidate
patches remain to be classified. This enables the use of features of vastly
different complexities in a single detector. In result, the feature pool can be
expanded to features previously impractical for cascade design, such as the
responses of a deep convolutional neural network (CNN). This is demonstrated
through the design of a pedestrian detector with a pool of features whose
complexities span orders of magnitude. The resulting cascade generalizes the
combination of a CNN with an object proposal mechanism: rather than a
pre-processing stage, CompACT cascades seamlessly integrate CNNs in their
stages. This enables state of the art performance on the Caltech and KITTI
datasets, at fairly fast speeds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05352</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05352</id><created>2015-07-19</created><updated>2016-02-25</updated><authors><author><keyname>Boulogeorgos</keyname><forenames>Alexandros-Apostolos A.</forenames></author><author><keyname>Kapinas</keyname><forenames>Vasileios M.</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>I/Q-Imbalance Self-Interference Coordination</title><categories>cs.IT math.IT</categories><comments>Published in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel low-complexity scheme, which improves the
performance of single-antenna multi-carrier communication systems, suffering
from in-phase and quadrature (I/Q)-imbalance (IQI) at the receiver. We refer to
the proposed scheme as I/Q-imbalance self-interference coordination (IQSC).
IQSC does not only mitigate the detrimental effects of IQI, but, through
appropriate signal processing, also coordinates the self-interference terms
produced by IQI in order to achieve second-order frequency diversity. However,
these benefits come at the expense of a reduction in transmission rate. More
specifically, IQSC is a simple transmit diversity scheme that improves the
signal quality at the receiver by elementary signal processing operations
across symmetric (mirror) pairs of subcarriers. Thereby, the proposed
transmission protocol has a similar complexity as Alamouti's space-time block
coding scheme and does not require extra transmit power nor any feedback. To
evaluate the performance of IQSC, we derive closed-form expressions for the
resulting outage probability and symbol error rate. Interestingly, IQSC
outperforms not only existing IQI compensation schemes but also the ideal
system without IQI for the same spectral efficiency and practical target error
rates, while it achieves almost the same performance as ideal (i.e., IQI-free)
equal-rate repetition coding. Our findings reveal that IQSC is a promising
low-complexity technique for significantly increasing the reliability of
low-cost devices that suffer from high levels of IQI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05363</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05363</id><created>2015-07-19</created><authors><author><keyname>Zhu</keyname><forenames>Xudong</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Dai</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author><author><keyname>Moonen</keyname><forenames>Marc</forenames></author></authors><title>Tracking A Dynamic Sparse Channel Via Differential Orthogonal Matching
  Pursuit</title><categories>cs.IT math.IT</categories><comments>Conference: Milcom 2015 Track 1 - Waveforms and Signal Processing -
  IEEE Military Communications Conference 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of tracking a dynamic sparse channel in a
broadband wireless communication system. A probabilistic signal model is
firstly proposed to describe the special features of temporal correlations of
dynamic sparse channels: path delays change slowly over time, while path gains
evolve faster. Based on such temporal correlations, we then propose the
differential orthogonal matching pursuit (D-OMP) algorithm to track a dynamic
sparse channel in a sequential way by updating the small channel variation over
time. Compared with other channel tracking algorithms, simulation results
demonstrate that the proposed D-OMP algorithm can track dynamic sparse channels
faster with improved accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05365</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05365</id><created>2015-07-19</created><authors><author><keyname>Zhu</keyname><forenames>Xudong</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Dai</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>Structured Matching Pursuit for Reconstruction of Dynamic Sparse
  Channels</title><categories>cs.IT math.IT</categories><comments>Conference: 2015 IEEE Global Communications Conference: Wireless
  Communications - Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, by exploiting the special features of temporal correlations of
dynamic sparse channels that path delays change slowly over time but path gains
evolve faster, we propose the structured matching pursuit (SMP) algorithm to
realize the reconstruction of dynamic sparse channels. Specifically, the SMP
algorithm divides the path delays of dynamic sparse channels into two different
parts to be considered separately, i.e., the common channel taps and the
dynamic channel taps. Based on this separation, the proposed SMP algorithm
simultaneously detects the common channel taps of dynamic sparse channels in
all time slots at first, and then tracks the dynamic channel taps in each
single time slot individually. Theoretical analysis of the proposed SMP
algorithm provides a guarantee that the common channel taps can be successfully
detected with a high probability, and the reconstruction distortion of dynamic
sparse channels is linearly upper bounded by the noise power. Simulation
results demonstrate that the proposed SMP algorithm has excellent
reconstruction performance with competitive computational complexity compared
with conventional reconstruction algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05366</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05366</id><created>2015-07-19</created><authors><author><keyname>Daubechies</keyname><forenames>Ingrid</forenames></author><author><keyname>Wang</keyname><forenames>Yi</forenames></author><author><keyname>Wu</keyname><forenames>Hau-tieng</forenames></author></authors><title>ConceFT: Concentration of Frequency and Time via a multitapered
  synchrosqueezed transform</title><categories>math.ST cs.NA stat.AP stat.ME stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method is proposed to determine the time-frequency content of
time-dependent signals consisting of multiple oscillatory components, with
time-varying amplitudes and instantaneous frequencies. Numerical experiments as
well as a theoretical analysis are presented to assess its effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05367</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05367</id><created>2015-07-19</created><authors><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Baldassarre</keyname><forenames>Luca</forenames></author><author><keyname>El-Halabi</keyname><forenames>Marwa</forenames></author><author><keyname>Tran-Dinh</keyname><forenames>Quoc</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>Structured Sparsity: Discrete and Convex approaches</title><categories>cs.IT math.IT math.OC stat.ML</categories><comments>30 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing (CS) exploits sparsity to recover sparse or compressible
signals from dimensionality reducing, non-adaptive sensing mechanisms. Sparsity
is also used to enhance interpretability in machine learning and statistics
applications: While the ambient dimension is vast in modern data analysis
problems, the relevant information therein typically resides in a much lower
dimensional space. However, many solutions proposed nowadays do not leverage
the true underlying structure. Recent results in CS extend the simple sparsity
idea to more sophisticated {\em structured} sparsity models, which describe the
interdependency between the nonzero components of a signal, allowing to
increase the interpretability of the results and lead to better recovery
performance. In order to better understand the impact of structured sparsity,
in this chapter we analyze the connections between the discrete models and
their convex relaxations, highlighting their relative advantages. We start with
the general group sparse model and then elaborate on two important special
cases: the dispersive and the hierarchical models. For each, we present the
models in their discrete nature, discuss how to solve the ensuing discrete
problems and then describe convex relaxations. We also consider more general
structures as defined by set functions and present their convex proxies.
Further, we discuss efficient optimization solutions for structured sparsity
problems and illustrate structured sparsity in action via three applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05370</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05370</id><created>2015-07-19</created><authors><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author><author><keyname>Jafarpour</keyname><forenames>Sina</forenames></author><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author></authors><title>Linear Inverse Problems with Norm and Sparsity Constraints</title><categories>cs.IT math.IT math.OC stat.ML</categories><comments>21 pages, authors in alphabetical order</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe two nonconventional algorithms for linear regression, called GAME
and CLASH. The salient characteristics of these approaches is that they exploit
the convex $\ell_1$-ball and non-convex $\ell_0$-sparsity constraints jointly
in sparse recovery. To establish the theoretical approximation guarantees of
GAME and CLASH, we cover an interesting range of topics from game theory,
convex and combinatorial optimization. We illustrate that these approaches lead
to improved theoretical guarantees and empirical performance beyond convex and
non-convex solvers alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05371</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05371</id><created>2015-07-19</created><updated>2016-01-08</updated><authors><author><keyname>Bresler</keyname><forenames>Guy</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author><author><keyname>Voloch</keyname><forenames>Luis F.</forenames></author></authors><title>Regret Guarantees for Item-Item Collaborative Filtering</title><categories>cs.LG cs.IR cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is much empirical evidence that item-item collaborative filtering works
well in practice. Motivated to understand this, we provide a framework to
design and analyze various recommendation algorithms. The setup amounts to
online binary matrix completion, where at each time a random user requests a
recommendation and the algorithm chooses an entry to reveal in the user's row.
The goal is to minimize regret, or equivalently to maximize the number of +1
entries revealed at any time. We analyze an item-item collaborative filtering
algorithm that can achieve fundamentally better performance compared to
user-user collaborative filtering. The algorithm achieves good &quot;cold-start&quot;
performance (appropriately defined) by quickly making good recommendations to
new users about whom there is little information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05372</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05372</id><created>2015-07-19</created><updated>2015-12-14</updated><authors><author><keyname>Lin</keyname><forenames>Yu-Ting</forenames></author><author><keyname>Flandrin</keyname><forenames>Patrick</forenames></author><author><keyname>Wu</keyname><forenames>Hau-tieng</forenames></author></authors><title>When interpolation-induced reflection artifact meets time-frequency
  analysis</title><categories>stat.AP cs.NA stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While extracting the temporal dynamical features based on the time-frequency
analyses, like the reassignment and synchrosqueezing transform, attracts more
and more interest in bio-medical data analysis, we should be careful about
artifacts generated by interpolation schemes, in particular when the sampling
rate is not significantly higher than the frequency of the oscillatory
component we are interested in. In this study, we formulate the problem called
the reflection effect and provide a theoretical justification of the statement.
We also show examples in the anesthetic depth analysis with clear but
undesirable artifacts. The results show that the artifact associated with the
reflection effect exists not only theoretically but practically. Its influence
is pronounced when we apply the time-frequency analyses to extract the
time-varying dynamics hidden inside the signal. In conclusion, we have to
carefully deal with the artifact associated with the reflection effect by
choosing a proper interpolation scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05379</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05379</id><created>2015-07-19</created><updated>2015-11-15</updated><authors><author><keyname>Lim</keyname><forenames>Lek-Heng</forenames></author></authors><title>Hodge Laplacians on graphs</title><categories>cs.IT math.IT</categories><comments>38 pages, 10 figures, AMS Short Course on &quot;Geometry and Topology in
  Statistical Inference&quot; at the 2014 Joint Mathematics Meeting. To appear in S.
  Mukherjee (Ed.), Geometry and Topology in Statistical Inference, Proceedings
  of Symposia in Applied Mathematics, 73, AMS, Providence, RI, 2015</comments><msc-class>05C21, 05C50, 58A14, 20G10, 35J05, 62F07, 91A70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is an elementary introduction to the Hodge Laplacian on a graph, a
higher-order generalization of the graph Laplacian. We will discuss basic
properties including cohomology and Hodge theory. At the end we will also
discuss the nonlinear Laplacian on a graph, a nonlinear generalization of the
graph Laplacian as its name implies. These generalized Laplacians will be
constructed out of coboundary operators, i.e., discrete analogues of exterior
derivatives. The main feature of our approach is simplicity --- this article
requires only knowledge of linear algebra and graph theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05387</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05387</id><created>2015-07-20</created><authors><author><keyname>Cariow</keyname><forenames>Aleksandr</forenames></author><author><keyname>Majorkowska-Mech</keyname><forenames>Dorota</forenames></author></authors><title>An algorithm for discrete fractional Hadamard transform</title><categories>cs.DS</categories><comments>22 pages, 4 figures</comments><msc-class>15A04, 15A23, 65Y20</msc-class><acm-class>F.2.1; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel algorithm for calculating the discrete fractional Hadamard
transform for data vectors whose size N is a power of two. A direct method for
calculation of the discrete fractional Hadamard transform requires $N^2$
multiplications, while in proposed algorithm the number of real multiplications
is reduced to $N$log$_2N$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05388</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05388</id><created>2015-07-20</created><authors><author><keyname>Fichte</keyname><forenames>Johannes K.</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author><author><keyname>Woltran</keyname><forenames>Stefan</forenames></author></authors><title>Dual-normal Logic Programs - the Forgotten Class</title><categories>cs.CC cs.AI cs.LO</categories><comments>This is the author's self-archived copy including detailed proofs. To
  appear in Theory and Practice of Logic Programming (TPLP), Proceedings of the
  31st International Conference on Logic Programming (ICLP 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disjunctive Answer Set Programming is a powerful declarative programming
paradigm with complexity beyond NP. Identifying classes of programs for which
the consistency problem is in NP is of interest from the theoretical standpoint
and can potentially lead to improvements in the design of answer set
programming solvers. One of such classes consists of dual-normal programs,
where the number of positive body atoms in proper rules is at most one. Unlike
other classes of programs, dual-normal programs have received little attention
so far. In this paper we study this class. We relate dual-normal programs to
propositional theories and to normal programs by presenting several
inter-translations. With the translation from dual-normal to normal programs at
hand, we introduce the novel class of body-cycle free programs, which are in
many respects dual to head-cycle free programs. We establish the expressive
power of dual-normal programs in terms of SE- and UE-models, and compare them
to normal programs. We also discuss the complexity of deciding whether
dual-normal programs are strongly and uniformly equivalent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05389</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05389</id><created>2015-07-20</created><authors><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author></authors><title>Opportunistic Beamforming with Wireless Powered 1-bit Feedback Through
  Rectenna Array</title><categories>cs.IT cs.NI math.IT</categories><comments>accepted for publication in IEEE Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter deals with the opportunistic beamforming (OBF) scheme for
multi-antenna downlink with spatial randomness. In contrast to conventional
OBF, the terminals return only 1-bit feedback, which is powered by wireless
power transfer through a rectenna array. We study two fundamental topologies
for the combination of the rectenna elements; the direct-current combiner and
the radio-frequency combiner. The beam outage probability is derived in closed
form for both combination schemes, by using high order statistics and
stochastic geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05391</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05391</id><created>2015-07-20</created><authors><author><keyname>Afanasieva</keyname><forenames>I. V.</forenames></author></authors><title>Data Acquisition and Control System for High-Performance Large-Area CCD
  Systems</title><categories>cs.CE astro-ph.IM</categories><comments>6 pages, 5 figures</comments><journal-ref>Astrophysical Bulletin, April 2015, Volume 70, Issue 2, pp 232-237</journal-ref><doi>10.1134/S1990341315020017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Astronomical CCD systems based on second-generation DINACON controllers were
developed at the SAO RAS Advanced Design Laboratory more than seven years ago
and since then have been in constant operation at the 6-meter and Zeiss-1000
telescopes. Such systems use monolithic large-area CCDs. We describe the
software developed for the control of a family of large-area CCD systems
equipped with a DINACON-II controller. The software suite serves for
acquisition, primary reduction, visualization, and storage of video data, and
also for the control, setup, and diagnostics of the CCD system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05398</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05398</id><created>2015-07-20</created><authors><author><keyname>Paliwal</keyname><forenames>Srajan</forenames></author><author><keyname>Tiwary</keyname><forenames>Saurabh</forenames></author><author><keyname>Chaudhury</keyname><forenames>Bhaskar</forenames></author><author><keyname>Gupta</keyname><forenames>Manish K.</forenames></author></authors><title>Generating Binary Optimal Codes Using Heterogeneous Parallel Computing</title><categories>cs.IT cs.DC math.IT</categories><comments>8 pages, draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generation of optimal codes is a well known problem in coding theory. Many
computational approaches exist in the literature for finding record breaking
codes. However generating codes with long lengths $n$ using serial algorithms
is computationally very expensive, for example the worst case time complexity
of a Greedy algorithm is $\mathcal{O}(n\; 4^n)$. In order to improve the
efficiency of generating codes with long lengths, we propose and investigate
some parallel algorithms using General Purpose Graphic Processing Units
(GPGPU). This paper considers the implementation of parallel Greedy algorithm
using GPGPU-CUDA (Computed Unified Device Architecture) framework and discusses
various optimization techniques to accelerate the GPU code. The performance
achieved for optimized parallel implementations is more than two to three
orders of magnitude faster than that of serial implementation and shows a great
potential of GPGPU in the field of coding theory applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05408</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05408</id><created>2015-07-20</created><authors><author><keyname>Banda</keyname><forenames>Juan M.</forenames></author><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author><author><keyname>Shah</keyname><forenames>Nigam H.</forenames></author><author><keyname>Dumontier</keyname><forenames>Michel</forenames></author></authors><title>Provenance-Centered Dataset of Drug-Drug Interactions</title><categories>cs.CY</categories><comments>In Proceedings of the 14th International Semantic Web Conference
  (ISWC) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the years several studies have demonstrated the ability to identify
potential drug-drug interactions via data mining from the literature (MEDLINE),
electronic health records, public databases (Drugbank), etc. While each one of
these approaches is properly statistically validated, they do not take into
consideration the overlap between them as one of their decision making
variables. In this paper we present LInked Drug-Drug Interactions (LIDDI), a
public nanopublication-based RDF dataset with trusty URIs that encompasses some
of the most cited prediction methods and sources to provide researchers a
resource for leveraging the work of others into their prediction methods. As
one of the main issues to overcome the usage of external resources is their
mappings between drug names and identifiers used, we also provide the set of
mappings we curated to be able to compare the multiple sources we aggregate in
our dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05409</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05409</id><created>2015-07-20</created><updated>2016-01-11</updated><authors><author><keyname>Mukhoty</keyname><forenames>Bhaskar</forenames></author><author><keyname>Gupta</keyname><forenames>Ruchir</forenames></author><author><keyname>Singh</keyname><forenames>Y. N.</forenames></author></authors><title>A Parameter-free Affinity Based Clustering</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several methods have been proposed to estimate the number of clusters in a
dataset; the basic ideal behind all of them has been to study an index that
measures inter-cluster separation and intra-cluster cohesion over a range of
cluster numbers and report the number which gives an optimum value of the
index. In this paper we propose a simple, parameter free approach that is like
human cognition to form clusters, where closely lying points are easily
identified to form a cluster and total number of clusters are revealed. To
identify closely lying points, affinity of two points is defined as a function
of distance and a threshold affinity is identified, above which two points in a
dataset are likely to be in the same cluster. Well separated clusters are
identified even in the presence of outliers, whereas for not so well separated
dataset, final number of clusters are estimated and the detected clusters are
merged to produce the final clusters. Experiments performed with several large
dimensional synthetic and real datasets show good results with robustness to
noise and density variation within dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05441</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05441</id><created>2015-07-20</created><authors><author><keyname>Gao</keyname><forenames>Zhen</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Lu</keyname><forenames>Zhaohua</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Super-Resolution Sparse MIMO-OFDM Channel Estimation Based on Spatial
  and Temporal Correlations</title><categories>cs.IT math.IT</categories><comments>This paper has been accepted by IEEE Communications Letters</comments><journal-ref>IEEE Commun. Lett., vol. 18, no. 7, pp. 1266-1269, Jul. 2014</journal-ref><doi>10.1109/LCOMM.2014.2325027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter proposes a parametric sparse multiple input multiple output
(MIMO)-OFDM channel estimation scheme based on the finite rate of innovation
(FRI) theory, whereby super-resolution estimates of path delays with arbitrary
values can be achieved. Meanwhile, both the spatial and temporal correlations
of wireless MIMO channels are exploited to improve the accuracy of the channel
estimation. For outdoor communication scenarios, where wireless channels are
sparse in nature, path delays of different transmit-receive antenna pairs share
a common sparse pattern due to the spatial correlation of MIMO channels.
Meanwhile, the channel sparse pattern is nearly unchanged during several
adjacent OFDM symbols due to the temporal correlation of MIMO channels. By
simultaneously exploiting those MIMO channel characteristics, the proposed
scheme performs better than existing state-of-the-art schemes. Furthermore, by
joint processing of signals associated with different antennas, the pilot
overhead can be reduced under the framework of the FRI theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05444</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05444</id><created>2015-07-20</created><updated>2015-12-05</updated><authors><author><keyname>Rainforth</keyname><forenames>Tom</forenames></author><author><keyname>Wood</keyname><forenames>Frank</forenames></author></authors><title>Canonical Correlation Forests</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce canonical correlation forests (CCFs), a new decision tree
ensemble method for classification. Individual canonical correlation trees are
binary decision trees with hyperplane splits based on canonical correlation
components. Unlike axis-aligned alternatives, the decision surfaces of CCFs are
not restricted to the coordinate system of the input features and therefore
more naturally represent data with correlation between the features.
Additionally we introduce a novel alternative to bagging, the projection
bootstrap, which maintains use of the full dataset in selecting split points.
CCFs do not require parameter tuning and our experiments show that they
out-perform axis-aligned random forests, other state-of-the-art tree ensemble
methods and all of the 179 popular classifiers considered in a recent extensive
survey.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05454</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05454</id><created>2015-07-20</created><authors><author><keyname>Mesnard</keyname><forenames>Fred</forenames></author><author><keyname>Payet</keyname><forenames>&#xc9;tienne</forenames></author><author><keyname>Vidal</keyname><forenames>Germ&#xe1;n</forenames></author></authors><title>Concolic Testing in Logic Programming</title><categories>cs.PL</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP),
  Proceedings of ICLP 2015</comments><journal-ref>Theory and Practice of Logic Programming 15 (2015) 711-725</journal-ref><doi>10.1017/S1471068415000332</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software testing is one of the most popular validation techniques in the
software industry. Surprisingly, we can only find a few approaches to testing
in the context of logic programming. In this paper, we introduce a systematic
approach for dynamic testing that combines both concrete and symbolic
execution. Our approach is fully automatic and guarantees full path coverage
when it terminates. We prove some basic properties of our technique and
illustrate its practical usefulness through a prototype implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05455</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05455</id><created>2015-07-20</created><updated>2015-07-27</updated><authors><author><keyname>Barrack</keyname><forenames>Duncan</forenames></author><author><keyname>Goulding</keyname><forenames>James</forenames></author><author><keyname>Hopcraft</keyname><forenames>Keith</forenames></author><author><keyname>Preston</keyname><forenames>Simon</forenames></author><author><keyname>Smith</keyname><forenames>Gavin</forenames></author></authors><title>AMP: a new time-frequency feature extraction method for intermittent
  time-series data</title><categories>cs.LG</categories><comments>Paper accepted in workshop on mining and learning from time series
  (MiLeTS) to be held in conjunction with KDD 2015 August 10- 13 2015, Sydney,
  Australia</comments><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The characterisation of time-series data via their most salient features is
extremely important in a range of machine learning task, not least of all with
regards to classification and clustering. While there exist many feature
extraction techniques suitable for non-intermittent time-series data, these
approaches are not always appropriate for intermittent time-series data, where
intermittency is characterized by constant values for large periods of time
punctuated by sharp and transient increases or decreases in value.
  Motivated by this, we present aggregation, mode decomposition and projection
(AMP) a feature extraction technique particularly suited to intermittent
time-series data which contain time-frequency patterns. For our method all
individual time-series within a set are combined to form a non-intermittent
aggregate. This is decomposed into a set of components which represent the
intrinsic time-frequency signals within the data set. Individual time-series
can then be fit to these components to obtain a set of numerical features that
represent their intrinsic time-frequency patterns. To demonstrate the
effectiveness of AMP, we evaluate against the real word task of clustering
intermittent time-series data. Using synthetically generated data we show that
a clustering approach which uses the features derived from AMP significantly
outperforms traditional clustering methods. Our technique is further
exemplified on a real world data set where AMP can be used to discover
groupings of individuals which correspond to real world sub-populations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05458</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05458</id><created>2015-07-20</created><authors><author><keyname>Sim</keyname><forenames>Aaron</forenames></author><author><keyname>Yaliraki</keyname><forenames>Sophia N</forenames></author><author><keyname>Barahona</keyname><forenames>Mauricio</forenames></author><author><keyname>Stumpf</keyname><forenames>Michael P H</forenames></author></authors><title>Great cities look small</title><categories>physics.soc-ph cs.SI</categories><comments>19 pages, 8 figures</comments><journal-ref>J. R. Soc. Interface 2015 12 20150315. Published 15 July 2015</journal-ref><doi>10.1098/rsif.2015.0315</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Great cities connect people; failed cities isolate people. Despite the
fundamental importance of physical, face-to-face social-ties in the functioning
of cities, these connectivity networks are not explicitly observed in their
entirety. Attempts at estimating them often rely on unrealistic
over-simplifications such as the assumption of spatial homogeneity. Here we
propose a mathematical model of human interactions in terms of a local strategy
of maximising the number of beneficial connections attainable under the
constraint of limited individual travelling-time budgets. By incorporating
census and openly-available online multi-modal transport data, we are able to
characterise the connectivity of geometrically and topologically complex
cities. Beyond providing a candidate measure of greatness, this model allows
one to quantify and assess the impact of transport developments, population
growth, and other infrastructure and demographic changes on a city. Supported
by validations of GDP and HIV infection rates across United States metropolitan
areas, we illustrate the effect of changes in local and city-wide
connectivities by considering the economic impact of two contemporary inter-
and intra-city transport developments in the United Kingdom: High Speed Rail 2
and London Crossrail. This derivation of the model suggests that the scaling of
different urban indicators with population size has an explicitly mechanistic
origin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05461</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05461</id><created>2015-07-20</created><authors><author><keyname>Despr&#xe9;</keyname><forenames>Vincent</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Daniel</forenames></author><author><keyname>L&#xe9;v&#xea;que</keyname><forenames>Benjamin</forenames></author></authors><title>Encoding toroidal triangulations</title><categories>cs.DM cs.CG math.CO</categories><comments>41 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Poulalhon and Schaeffer introduced an elegant method to linearly encode a
planar triangulation optimally. The method is based on performing a special
depth-first search algorithm on a particular orientation of the triangulation:
the minimal Schnyder wood. Recent progress toward generalizing Schnyder woods
to higher genus enables us to generalize this method to the toroidal case. In
the plane, the method leads to a bijection between planar triangulations and
some particular trees. For the torus we obtain a similar bijection but with
particular unicellular maps (maps with only one face).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05463</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05463</id><created>2015-07-20</created><authors><author><keyname>Eiben</keyname><forenames>Eduard</forenames></author><author><keyname>Ganian</keyname><forenames>Robert</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Solving Problems on Graphs of High Rank-Width</title><categories>cs.DS</categories><comments>Accepted at WADS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A modulator of a graph G to a specified graph class H is a set of vertices
whose deletion puts G into H. The cardinality of a modulator to various
tractable graph classes has long been used as a structural parameter which can
be exploited to obtain FPT algorithms for a range of hard problems. Here we
investigate what happens when a graph contains a modulator which is large but
&quot;well-structured&quot; (in the sense of having bounded rank-width). Can such
modulators still be exploited to obtain efficient algorithms? And is it even
possible to find such modulators efficiently?
  We first show that the parameters derived from such well-structured
modulators are strictly more general than the cardinality of modulators and
rank-width itself. Then, we develop an FPT algorithm for finding such
well-structured modulators to any graph class which can be characterized by a
finite set of forbidden induced subgraphs. We proceed by showing how
well-structured modulators can be used to obtain efficient parameterized
algorithms for Minimum Vertex Cover and Maximum Clique. Finally, we use
well-structured modulators to develop an algorithmic meta-theorem for deciding
problems expressible in Monadic Second Order (MSO) logic, and prove that this
result is tight in the sense that it cannot be generalized to LinEMSO problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05467</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05467</id><created>2015-07-20</created><authors><author><keyname>Thai</keyname><forenames>Long</forenames></author><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author></authors><title>Budget Constrained Execution of Multiple Bag-of-Tasks Applications on
  the Cloud</title><categories>cs.DC</categories><comments>8th IEEE International Conference on Cloud Computing (CLOUD 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimising the execution of Bag-of-Tasks (BoT) applications on the cloud is a
hard problem due to the trade- offs between performance and monetary cost. The
problem can be further complicated when multiple BoT applications need to be
executed. In this paper, we propose and implement a heuristic algorithm that
schedules tasks of multiple applications onto different cloud virtual machines
in order to maximise performance while satisfying a given budget constraint.
Current approaches are limited in task scheduling since they place a limit on
the number of cloud resources that can be employed by the applications.
However, in the proposed algorithm there are no such limits, and in comparison
with other approaches, the algorithm on average achieves an improved
performance of 10%. The experimental results also highlight that the algorithm
yields consistent performance even with low budget constraints which cannot be
achieved by competing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05470</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05470</id><created>2015-07-20</created><authors><author><keyname>Thai</keyname><forenames>Long</forenames></author><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author></authors><title>Task Scheduling on the Cloud with Hard Constraints</title><categories>cs.DC</categories><comments>Visionary Track of the IEEE 11th World Congress on Services (IEEE
  SERVICES 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scheduling Bag-of-Tasks (BoT) applications on the cloud can be more
challenging than grid and cluster environ- ments. This is because a user may
have a budgetary constraint or a deadline for executing the BoT application in
order to keep the overall execution costs low. The research in this paper is
motivated to investigate task scheduling on the cloud, given two hard
constraints based on a user-defined budget and a deadline. A heuristic
algorithm is proposed and implemented to satisfy the hard constraints for
executing the BoT application in a cost effective manner. The proposed
algorithm is evaluated using four scenarios that are based on the trade-off
between performance and the cost of using different cloud resource types. The
experimental evaluation confirms the feasibility of the algorithm in satisfying
the constraints. The key observation is that multiple resource types can be a
better alternative to using a single type of resource.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05472</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05472</id><created>2015-07-20</created><authors><author><keyname>Mantripragada</keyname><forenames>Kiran</forenames></author><author><keyname>Tizzei</keyname><forenames>Leonardo P.</forenames></author><author><keyname>Binotto</keyname><forenames>Alecio P. D.</forenames></author><author><keyname>Netto</keyname><forenames>Marco A. S.</forenames></author></authors><title>An SLA-based Advisor for Placement of HPC Jobs on Hybrid Clouds</title><categories>cs.DC</categories><comments>16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several scientific and industry applications require High Performance
Computing (HPC) resources to process and/or simulate complex models. Not long
ago, companies, research institutes, and universities used to acquire and
maintain on-premise computer clusters; but, recently, cloud computing has
emerged as an alternative for a subset of HPC applications. This poses a
challenge to end-users, who have to decide where to run their jobs: on local
clusters or burst to a remote cloud service provider. While current research on
HPC cloud has focused on comparing performance of on-premise clusters against
cloud resources, we build on top of existing efforts and introduce an advisory
service to help users make this decision considering the trade-offs of resource
costs, performance, and availability on hybrid clouds. We evaluated our service
using a real test-bed with a seismic processing application based on Full
Waveform Inversion; a technique used by geophysicists in the oil &amp; gas industry
and earthquake prediction. We also discuss how the advisor can be used for
other applications and highlight the main lessons learned constructing this
service to reduce costs and turnaround times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05485</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05485</id><created>2015-07-20</created><updated>2015-10-07</updated><authors><author><keyname>Lairez</keyname><forenames>Pierre</forenames></author></authors><title>A deterministic algorithm to compute approximate roots of polynomial
  systems in polynomial average time</title><categories>math.NA cs.CC</categories><msc-class>68Q25, 65H10, 65H20, 65Y20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a deterministic algorithm that computes an approximate root of n
complex polynomial equations in n unknowns in average polynomial time with
respect to the size of the input, in the Blum-Shub-Smale model with square
root. It rests upon a derandomization of an algorithm of Beltr\'an and Pardo
and gives a deterministic affirmative answer to Smale's 17th problem. The main
idea is to make use of the randomness contained in the input itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05487</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05487</id><created>2015-07-20</created><authors><author><keyname>Evangelatos</keyname><forenames>Spyridon</forenames></author><author><keyname>Moustakas</keyname><forenames>Aris L.</forenames></author><author><keyname>Polydoros</keyname><forenames>Andreas</forenames></author></authors><title>Outage and Capacity Comparisons For Ground Relaying Systems Using
  Stochastic Geometry</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures, accepted by IEEE ISWCS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrent cooperative transmission for relaying purposes in mobile
communication networks is relevant in current institutional systems with
limited infrastructure, and and may be viewed as a potential range-extension
mechanism for future commercial networks, including vehicular autonomous
networking. The complexity of the overall system has encouraged certain
abstractions at the physical layer which are critically analyzed in the present
paper. We show via analytic stochastic geometry tools that the receiver
structure plays a crucial role in the outage behavior of the relays,
particularly for realistic flooding protocols. This approach aims to help
understand the cross-layer aspects of such networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05489</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05489</id><created>2015-07-20</created><authors><author><keyname>Romanoni</keyname><forenames>Andrea</forenames></author><author><keyname>Matteucci</keyname><forenames>Matteo</forenames></author></authors><title>Efficient moving point handling for incremental 3D manifold
  reconstruction</title><categories>cs.CV</categories><comments>Accepted in International Conference on Image Analysis and Processing
  (ICIAP 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As incremental Structure from Motion algorithms become effective, a good
sparse point cloud representing the map of the scene becomes available
frame-by-frame. From the 3D Delaunay triangulation of these points,
state-of-the-art algorithms build a manifold rough model of the scene. These
algorithms integrate incrementally new points to the 3D reconstruction only if
their position estimate does not change. Indeed, whenever a point moves in a 3D
Delaunay triangulation, for instance because its estimation gets refined, a set
of tetrahedra have to be removed and replaced with new ones to maintain the
Delaunay property; the management of the manifold reconstruction becomes thus
complex and it entails a potentially big overhead. In this paper we investigate
different approaches and we propose an efficient policy to deal with moving
points in the manifold estimation process. We tested our approach with four
sequences of the KITTI dataset and we show the effectiveness of our proposal in
comparison with state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05492</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05492</id><created>2015-07-20</created><authors><author><keyname>Chen</keyname><forenames>Mingming</forenames></author><author><keyname>Liu</keyname><forenames>Sisi</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author></authors><title>Parallel Toolkit for Measuring the Quality of Network Community
  Structure</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages; in Network Intelligence Conference (ENIC), 2014 European</comments><doi>10.1109/ENIC.2014.26</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many networks display community structure which identifies groups of nodes
within which connections are denser than between them. Detecting and
characterizing such community structure, which is known as community detection,
is one of the fundamental issues in the study of network systems. It has
received a considerable attention in the last years. Numerous techniques have
been developed for both efficient and effective community detection. Among
them, the most efficient algorithm is the label propagation algorithm whose
computational complexity is O(|E|). Although it is linear in the number of
edges, the running time is still too long for very large networks, creating the
need for parallel community detection. Also, computing community quality
metrics for community structure is computationally expensive both with and
without ground truth. However, to date we are not aware of any effort to
introduce parallelism for this problem. In this paper, we provide a parallel
toolkit to calculate the values of such metrics. We evaluate the parallel
algorithms on both distributed memory machine and shared memory machine. The
experimental results show that they yield a significant performance gain over
sequential execution in terms of total running time, speedup, and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05497</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05497</id><created>2015-07-20</created><authors><author><keyname>Ignatov</keyname><forenames>Dmitry I.</forenames></author><author><keyname>Kornilov</keyname><forenames>Denis</forenames></author></authors><title>RAPS: A Recommender Algorithm Based on Pattern Structures</title><categories>cs.IR cs.AI cs.DM</categories><comments>The paper presented at FCA4AI 2015 in conjunction with IJCAI 2015</comments><msc-class>06F99</msc-class><acm-class>H.3.3; H.2.8; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new algorithm for recommender systems with numeric ratings which
is based on Pattern Structures (RAPS). As the input the algorithm takes rating
matrix, e.g., such that it contains movies rated by users. For a target user,
the algorithm returns a rated list of items (movies) based on its previous
ratings and ratings of other users. We compare the results of the proposed
algorithm in terms of precision and recall measures with Slope One, one of the
state-of-the-art item-based algorithms, on Movie Lens dataset and RAPS
demonstrates the best or comparable quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05498</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05498</id><created>2015-07-20</created><authors><author><keyname>Jung</keyname><forenames>Alexander</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>G&#xf6;rtz</keyname><forenames>Norbert</forenames></author></authors><title>On the Minimax Risk of Dictionary Learning</title><categories>stat.ML cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning a dictionary matrix from a number of
observed signals, which are assumed to be generated via a linear model with a
common underlying dictionary. In particular, we derive lower bounds on the
minimum achievable worst case mean squared error (MSE), regardless of
computational complexity of the dictionary learning (DL) schemes. By casting DL
as a classical (or frequentist) estimation problem, the lower bounds on the
worst case MSE are derived by following an established information-theoretic
approach to minimax estimation. The main conceptual contribution of this paper
is the adaption of the information-theoretic approach to minimax estimation for
the DL problem in order to derive lower bounds on the worst case MSE of any DL
scheme. We derive three different lower bounds applying to different generative
models for the observed signals. The first bound applies to a wide range of
models, it only requires the existence of a covariance matrix of the (unknown)
underlying coefficient vector. By specializing this bound to the case of sparse
coefficient distributions, and assuming the true dictionary satisfies the
restricted isometry property, we obtain a lower bound on the worst case MSE of
DL schemes in terms of a signal to noise ratio (SNR). The third bound applies
to a more restrictive subclass of coefficient distributions by requiring the
non-zero coefficients to be Gaussian. While, compared with the previous two
bounds, the applicability of this final bound is the most limited it is the
tightest of the three bounds in the low SNR regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05500</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05500</id><created>2015-07-20</created><updated>2015-07-21</updated><authors><author><keyname>Kokkinis</keyname><forenames>Ioannis</forenames></author></authors><title>On the complexity of probabilistic justification logic</title><categories>cs.LO</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The logic PJ is a probabilistic logic over the basic justification logic J.
In this paper we establish upper and lower bounds for the complexity of PJ. The
main result of the paper is that the complexity of the logic PJ remains the
same as the complexity of the logic J.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05501</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05501</id><created>2015-07-20</created><authors><author><keyname>Bongers</keyname><forenames>Ewout</forenames></author><author><keyname>Pouwelse</keyname><forenames>Johan</forenames></author></authors><title>A survey of P2P multidimensional indexing structures</title><categories>cs.DC</categories><comments>Course IN4306 - Literature Survey</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Traditional databases have long since reaped the benefits of multidimensional
indexes. Numerous proposals in the literature describe multidimensional index
designs for P2P systems. However, none of these designs have had real world
implementations. Several proposals for P2P multidimensional indexes are
reviewed and analyzed. Znet and VBI-tree are the most promising from a
technical standpoint. All of the proposed designs assume honest nodes and are
thus open to abuse. This is a critical flaw that must be solved before any of
the proposed systems can be used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05506</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05506</id><created>2015-07-20</created><updated>2015-07-24</updated><authors><author><keyname>Kewat</keyname><forenames>Pramod Kumar</forenames></author><author><keyname>Kumari</keyname><forenames>Priti</forenames></author></authors><title>Cyclic codes from the second class two-prime Whiteman's generalized
  cyclotomic sequence with order 6</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $n_1=ef+1$ and $n_2=ef'+1$ be two distinct odd primes with positive
integers $e,\ f,\ f'.$ In this paper, the two-prime Whiteman's generalized
cyclotomic sequence of order $e=6$ is employed to construct several classes of
cyclic codes over $\mathrm{GF}(q)$ with length $n_1n_2$. The lower bounds on
the minimum distance of these cyclic codes are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05513</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05513</id><created>2015-07-20</created><authors><author><keyname>Liu</keyname><forenames>Yao</forenames></author><author><keyname>Duan</keyname><forenames>Zhenhua</forenames></author><author><keyname>Tian</keyname><forenames>Cong</forenames></author></authors><title>An Improved Decision Procedure for Linear Time Mu-Calculus</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An improved Present Future form (PF form) for linear time $\mu$-calculus
($\nu$TL) is presented in this paper. In particular, the future part of the new
version turns into the conjunction of elements in the closure of a formula. We
show that every closed $\nu$TL formula can be transformed into the new PF form.
Additionally, based on the PF form, an algorithm for constructing Present
Future form Graph (PFG), which can be utilized to describe models of a formula,
is given. Further, an intuitive and efficient decision procedure for checking
satisfiability of the guarded fragment of $\nu$TL formulas based on PFG is
proposed and implemented in C++. The new decision procedure has the best time
complexity over the existing ones despite the cost of exponential space.
Finally, a PFG-based model checking approach for $\nu$TL is discussed where a
counterexample can be obtained visually when a model violates a property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05516</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05516</id><created>2015-07-20</created><authors><author><keyname>Sauco-Gallardo</keyname><forenames>Adri&#xe1;n</forenames></author><author><keyname>Fern&#xe1;ndez-Plazaola</keyname><forenames>Unai</forenames></author><author><keyname>D&#xed;ez</keyname><forenames>Luis</forenames></author><author><keyname>Martos-Naya</keyname><forenames>Eduardo</forenames></author></authors><title>Higher Order Statistics in Switched Diversity Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the level crossing rate (LCR) and the average fade duration of the
output signal-to-noise-ratio (SNR) in generalized switched diversity systems.
By using a common approach, we study these higher order statistics for two
different kinds of configurations: (1) Colocated diversity, i.e. receiver
equipped with multiple antennas, and (2) Distributed diversity, i.e. relaying
link with multiple single-antenna threshold-based decode-and-forward (DF)
relays. In both cases, we consider the switched diversity combining strategies
Selection Combining and Switch \&amp; Stay Combining (SSC). Whenever using
threshold-based techniques such as DF or SSC, the output SNR is a discontinuous
random process and hence classic Rice approach to calculate the LCR is not
applicable. Thus, we use an alternative formulation in terms of the one and
two-dimensional cumulative distribution functions of the output SNR. Our
results are general, and hold for any arbitrary distribution of fading at the
different diversity branches. Moreover, we develop a general asymptotic
framework to calculate these higher order statistics in high mean SNR
environments which only needs of the univariate probability density function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05523</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05523</id><created>2015-07-20</created><authors><author><keyname>Lai</keyname><forenames>Siwei</forenames></author><author><keyname>Liu</keyname><forenames>Kang</forenames></author><author><keyname>Xu</keyname><forenames>Liheng</forenames></author><author><keyname>Zhao</keyname><forenames>Jun</forenames></author></authors><title>How to Generate a Good Word Embedding?</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze three critical components of word embedding training: the model,
the corpus, and the training parameters. We systematize existing
neural-network-based word embedding algorithms and compare them using the same
corpus. We evaluate each word embedding in three ways: analyzing its semantic
properties, using it as a feature for supervised tasks and using it to
initialize neural networks. We also provide several simple guidelines for
training word embeddings. First, we discover that corpus domain is more
important than corpus size. We recommend choosing a corpus in a suitable domain
for the desired task, after that, using a larger corpus yields better results.
Second, we find that faster models provide sufficient performance in most
cases, and more complex models can be used if the training corpus is
sufficiently large. Third, the early stopping metric for iterating should rely
on the development set of the desired task rather than the validation loss of
training embedding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05527</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05527</id><created>2015-07-20</created><authors><author><keyname>Inala</keyname><forenames>Jeevana Priya</forenames></author><author><keyname>Qiu</keyname><forenames>Xiaokang</forenames></author><author><keyname>Lerner</keyname><forenames>Ben</forenames></author><author><keyname>Solar-Lezama</keyname><forenames>Armando</forenames></author></authors><title>Type Assisted Synthesis of Recursive Transformers on Algebraic Data
  Types</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show how synthesis can help implement interesting functions
involving pattern matching and algebraic data types. One of the novel aspects
of this work is the combination of type inference and counterexample-guided
inductive synthesis (CEGIS) in order to support very high-level notations for
describing the space of possible implementations that the synthesizer should
consider. The paper also describes a set of optimizations that significantly
improve the performance and scalability of the system.
  The approach is evaluated on a set of case studies which most notably include
synthesizing desugaring functions for lambda calculus that force the
synthesizer to discover Church encodings for pairs and boolean operations, as
well as a procedure to generate constraints for type inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05532</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05532</id><created>2015-07-20</created><updated>2015-09-09</updated><authors><author><keyname>Lu</keyname><forenames>Na</forenames></author><author><keyname>Miao</keyname><forenames>Hongyu</forenames></author></authors><title>Clustering Tree-structured Data on Manifold</title><categories>cs.CV cs.LG</categories><comments>14 pages, 7 figures, 7 tables</comments><msc-class>68T10, 62H30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tree-structured data usually contain both topological and geometrical
information, and are necessarily considered on manifold instead of Euclidean
space for appropriate data parameterization and analysis. In this study, we
propose a novel tree-structured data parameterization, called
Topology-Attribute matrix (T-A matrix), so the data clustering task can be
conducted on matrix manifold. We incorporate the structure constraints embedded
in data into the negative matrix factorization method to determine meta-trees
from the T-A matrix, and the signature vector of each single tree can then be
extracted by meta-tree decomposition. The meta-tree space turns out to be a
cone space, in which we explore the distance metric and implement the
clustering algorithm based on the concepts like Fr\'echet mean. Finally, the
T-A matrix based clustering (TAMBAC) framework is evaluated and compared using
both simulated data and real retinal images to illustrate its efficiency and
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05533</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05533</id><created>2015-07-20</created><authors><author><keyname>Gerami</keyname><forenames>Majid</forenames></author><author><keyname>Xiao</keyname><forenames>Ming</forenames></author><author><keyname>Salimi</keyname><forenames>Somayeh</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Secure Partial Repair in Wireless Caching Networks with Broadcast
  Channels</title><categories>cs.IT math.IT</categories><comments>To Appear in IEEE Conference on Communication and Network Security
  (CNS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study security in partial repair in wireless caching networks where parts
of the stored packets in the caching nodes are susceptible to be erased. Let us
denote a caching node that has lost parts of its stored packets as a sick
caching node and a caching node that has not lost any packet as a healthy
caching node. In partial repair, a set of caching nodes (among sick and healthy
caching nodes) broadcast information to other sick caching nodes to recover the
erased packets. The broadcast information from a caching node is assumed to be
received without any error by all other caching nodes. All the sick caching
nodes then are able to recover their erased packets, while using the broadcast
information and the nonerased packets in their storage as side information. In
this setting, if an eavesdropper overhears the broadcast channels, it might
obtain some information about the stored file. We thus study secure partial
repair in the senses of information-theoretically strong and weak security. In
both senses, we investigate the secrecy caching capacity, namely, the maximum
amount of information which can be stored in the caching network such that
there is no leakage of information during a partial repair process. We then
deduce the strong and weak secrecy caching capacities, and also derive the
sufficient finite field sizes for achieving the capacities. Finally, we propose
optimal secure codes for exact partial repair, in which the recovered packets
are exactly the same as erased packets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05539</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05539</id><created>2015-07-20</created><updated>2015-07-25</updated><authors><author><keyname>Ameloot</keyname><forenames>Tom J.</forenames></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author><author><keyname>Marczak</keyname><forenames>William R.</forenames></author><author><keyname>Alvaro</keyname><forenames>Peter</forenames></author><author><keyname>Hellerstein</keyname><forenames>Joseph M.</forenames></author></authors><title>Putting Logic-Based Distributed Systems on Stable Grounds</title><categories>cs.LO</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><doi>10.1017/S1471068415000381</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Declarative Networking paradigm, Datalog-like languages are used to
express distributed computations. Whereas recently formal operational semantics
for these languages have been developed, a corresponding declarative semantics
has been lacking so far. The challenge is to capture precisely the amount of
nondeterminism that is inherent to distributed computations due to concurrency,
networking delays, and asynchronous communication. This paper shows how a
declarative, model-based semantics can be obtained by simply using the
well-known stable model semantics for Datalog with negation. We show that the
model-based semantics matches previously proposed formal operational semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05541</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05541</id><created>2015-07-16</created><authors><author><keyname>Lehmann</keyname><forenames>Karsten</forenames></author><author><keyname>Bent</keyname><forenames>Russell</forenames></author><author><keyname>Pan</keyname><forenames>Feng</forenames></author></authors><title>Maximizing electrical power supply using FACTS devices</title><categories>cs.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Modern society critically depends on the services electric power provides.
Power systems rely on a network of power lines and transformers to deliver
power from sources of power (generators) to the consumers (loads). However,
when power lines fail (for example, through lightning or natural disasters) or
when the system is heavily used, the network is often unable to fulfill all of
the demand for power. While systems are vulnerable to these failures,
increasingly, sophisticated control devices are being deployed to improve the
efficiency of power systems. Such devices can also be used to improve the
resiliency of power systems to failures. In this paper, we focus on using FACTS
devices in this context. A FACTS device allows power grid operators to adjust
the impedance parameters of power lines, thereby redistributing flow in the
network and potentially increasing the amount of power that is supplied. Here
we develop new approaches for determining the optimal parameter settings for
FACTS devices in order to supply the maximal amount of power when networks are
stressed, e.g. power line failures and heavy utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05544</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05544</id><created>2015-07-20</created><authors><author><keyname>Eiben</keyname><forenames>Eduard</forenames></author><author><keyname>Ganian</keyname><forenames>Robert</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Meta-Kernelization using Well-Structured Modulators</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernelization investigates exact preprocessing algorithms with performance
guarantees. The most prevalent type of parameters used in kernelization is the
solution size for optimization problems; however, also structural parameters
have been successfully used to obtain polynomial kernels for a wide range of
problems. Many of these parameters can be defined as the size of a smallest
modulator of the given graph into a fixed graph class (i.e., a set of vertices
whose deletion puts the graph into the graph class). Such parameters admit the
construction of polynomial kernels even when the solution size is large or not
applicable. This work follows up on the research on meta-kernelization
frameworks in terms of structural parameters.
  We develop a class of parameters which are based on a more general view on
modulators: instead of size, the parameters employ a combination of rank-width
and split decompositions to measure structure inside the modulator. This allows
us to lift kernelization results from modulator-size to more general
parameters, hence providing smaller kernels. We show (i) how such large but
well-structured modulators can be efficiently approximated, (ii) how they can
be used to obtain polynomial kernels for any graph problem expressible in
Monadic Second Order logic, and (iii) how they allow the extension of previous
results in the area of structural meta-kernelization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05546</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05546</id><created>2015-07-20</created><authors><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author><author><keyname>Gonzales</keyname><forenames>Anne Muriel V.</forenames></author><author><keyname>Villanueva</keyname><forenames>Mariann Jocel S.</forenames></author><author><keyname>Mendoza</keyname><forenames>Arlene A.</forenames></author></authors><title>Automatic Identification of Animal Breeds and Species Using Bioacoustics
  and Artificial Neural Networks</title><categories>cs.SD q-bio.QM</categories><comments>17 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this research endeavor, it was hypothesized that the sound produced by
animals during their vocalizations can be used as identifiers of the animal
breed or species even if they sound the same to unaided human ear. To test this
hypothesis, three artificial neural networks (ANNs) were developed using
bioacoustics properties as inputs for the respective automatic identification
of 13 bird species, eight dog breeds, and 11 frog species. Recorded
vocalizations of these animals were collected and processed using several known
signal processing techniques to convert the respective sounds into computable
bioacoustics values. The converted values of the vocalizations, together with
the breed or species identifications, were used to train the ANNs following a
ten-fold cross validation technique. Tests show that the respective ANNs can
correctly identify 71.43\% of the birds, 94.44\% of the dogs, and 90.91\% of
the frogs. This result show that bioacoustics and ANN can be used to
automatically determine animal breeds and species, which together could be a
promising automated tool for animal identification, biodiversity determination,
animal conservation, and other animal welfare efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05576</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05576</id><created>2015-07-20</created><authors><author><keyname>Hamdi</keyname><forenames>Rami</forenames></author><author><keyname>Ajib</keyname><forenames>Wessam</forenames></author></authors><title>Joint Optimal Number of RF chains and Power Allocation for Downlink
  Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>IEEE Vehicular Technology Conference (IEEE VTC-Fall 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the downlink of massive multiple-input
multiple-output (MIMO) systems that include a single cell Base Station (BS)
equipped with large number of antennas serving multiple users. As the number of
RF chains is getting large, the system model considered in this paper assumes a
non negligible circuit power consumption. Hence, the aim of this work is to
find the optimal balance between the power consumed by the RF chains and the
transmitted power. First, assuming an equal power allocation among users, the
optimal number of RF chains to be activated is analytically found. Then, for a
given number of RF chains we derive analytically the optimal power allocation
among users. Based on these analysis, we propose an iterative algorithm that
computes jointly the optimal number of RF chains and the optimal power
allocation vector. Simulations validate the analytical results and show the
high performance provided by the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05578</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05578</id><created>2015-07-20</created><authors><author><keyname>Raj</keyname><forenames>Anant</forenames></author><author><keyname>Namboodiri</keyname><forenames>Vinay P.</forenames></author><author><keyname>Tuytelaars</keyname><forenames>Tinne</forenames></author></authors><title>Subspace Alignment Based Domain Adaptation for RCNN Detector</title><categories>cs.CV</categories><comments>26th British Machine Vision Conference, Swansea, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose subspace alignment based domain adaptation of the
state of the art RCNN based object detector. The aim is to be able to achieve
high quality object detection in novel, real world target scenarios without
requiring labels from the target domain. While, unsupervised domain adaptation
has been studied in the case of object classification, for object detection it
has been relatively unexplored. In subspace based domain adaptation for
objects, we need access to source and target subspaces for the bounding box
features. The absence of supervision (labels and bounding boxes are absent)
makes the task challenging. In this paper, we show that we can still adapt sub-
spaces that are localized to the object by obtaining detections from the RCNN
detector trained on source and applied on target. Then we form localized
subspaces from the detections and show that subspace alignment based adaptation
between these subspaces yields improved object detection. This evaluation is
done by considering challenging real world datasets of PASCAL VOC as source and
validation set of Microsoft COCO dataset as target for various categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05581</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05581</id><created>2015-07-20</created><authors><author><keyname>Enea</keyname><forenames>Constantin</forenames><affiliation>LIAFA</affiliation></author><author><keyname>Sighireanu</keyname><forenames>Mihaela</forenames><affiliation>LIAFA</affiliation></author><author><keyname>Wu</keyname><forenames>Zhilin</forenames></author></authors><title>On Automated Lemma Generation for Separation Logic with Inductive
  Definitions</title><categories>cs.LO cs.SE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separation Logic with inductive definitions is a well-known approach for
deductive verification of programs that manipulate dynamic data structures.
Deciding verification conditions in this context is usually based on
user-provided lemmas relating the inductive definitions. We propose a novel
approach for generating these lemmas automatically which is based on simple
syntactic criteria and deterministic strategies for applying them. Our approach
focuses on iterative programs, although it can be applied to recursive programs
as well, and specifications that describe not only the shape of the data
structures, but also their content or their size. Empirically, we find that our
approach is powerful enough to deal with sophisticated benchmarks, e.g.,
iterative procedures for searching, inserting, or deleting elements in sorted
lists, binary search tress, red-black trees, and AVL trees, in a very efficient
way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05591</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05591</id><created>2015-07-20</created><updated>2015-12-26</updated><authors><author><keyname>Chung</keyname><forenames>Yeounoh</forenames></author><author><keyname>Mortensen</keyname><forenames>Michael Lind</forenames></author><author><keyname>Binnig</keyname><forenames>Carsten</forenames></author><author><keyname>Kraska</keyname><forenames>Tim</forenames></author></authors><title>Estimating the Impact of Unknown Unknowns on Aggregate Query Results</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is common practice for data scientists to acquire and integrate disparate
data sources to achieve higher quality results. But even with a perfectly
cleaned and merged data set, two fundamental questions remain: (1) is the
integrated data set complete and (2) what is the impact of any unknown (i.e.,
unobserved) data on query results?
  In this work, we develop and analyze techniques to estimate the impact of the
unknown data (a.k.a., unknown unknowns) on simple aggregate queries. The key
idea is that the overlap between different data sources enables us to estimate
the number and values of the missing data items. Our main techniques are
parameter-free and do not assume prior knowledge about the distribution.
Through a series of experiments, we show that estimating the impact of unknown
unknowns is invaluable to better assess the results of aggregate queries over
integrated data sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05592</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05592</id><created>2015-07-20</created><authors><author><keyname>Fefferman</keyname><forenames>Bill</forenames></author><author><keyname>Umans</keyname><forenames>Chris</forenames></author></authors><title>The Power of Quantum Fourier Sampling</title><categories>cs.CC quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A line of work initiated by Terhal and DiVincenzo and Bremner, Jozsa, and
Shepherd, shows that quantum computers can efficiently sample from probability
distributions that cannot be exactly sampled efficiently on a classical
computer, unless the PH collapses. Aaronson and Arkhipov take this further by
considering a distribution that can be sampled efficiently by linear optical
quantum computation, that under two feasible conjectures, cannot even be
approximately sampled classically within bounded total variation distance,
unless the PH collapses.
  In this work we use Quantum Fourier Sampling to construct a class of
distributions that can be sampled by a quantum computer. We then argue that
these distributions cannot be approximately sampled classically, unless the PH
collapses, under variants of the Aaronson and Arkhipov conjectures.
  In particular, we show a general class of quantumly sampleable distributions
each of which is based on an &quot;Efficiently Specifiable&quot; polynomial, for which a
classical approximate sampler implies an average-case approximation. This class
of polynomials contains the Permanent but also includes, for example, the
Hamiltonian Cycle polynomial, and many other familiar #P-hard polynomials.
  Although our construction, unlike that proposed by Aaronson and Arkhipov,
likely requires a universal quantum computer, we are able to use this
additional power to weaken the conjectures needed to prove approximate sampling
hardness results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05593</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05593</id><created>2015-07-20</created><authors><author><keyname>Chadwick</keyname><forenames>Jeffrey N.</forenames></author><author><keyname>Bindel</keyname><forenames>David S.</forenames></author></authors><title>An Efficient Solver for Sparse Linear Systems Based on Rank-Structured
  Cholesky Factorization</title><categories>cs.NA math.NA</categories><msc-class>65F05, 65F08, 65F50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct factorization methods for the solution of large, sparse linear systems
that arise from PDE discretizations are robust, but typically show poor time
and memory scalability for large systems. In this paper, we describe an
efficient sparse, rank-structured Cholesky algorithm for solution of the
positive definite linear system $A x = b$ when $A$ comes from a discretized
partial-differential equation. Our approach combines the efficient memory
access patterns of conventional supernodal Cholesky algorithms with the memory
efficiency of rank-structured direct solvers. For several test problems arising
from PDE discretizations, our method takes less memory than standard sparse
Cholesky solvers and less wall-clock time than standard preconditioned
iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05597</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05597</id><created>2015-07-20</created><updated>2015-10-28</updated><authors><author><keyname>Hernandez</keyname><forenames>Noe</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author><author><keyname>Magid</keyname><forenames>Evgeni</forenames></author><author><keyname>Savage</keyname><forenames>Jesus</forenames></author><author><keyname>Rosenblueth</keyname><forenames>David A.</forenames></author></authors><title>Marimba: A Tool for Verifying Properties of Hidden Markov Models</title><categories>cs.LO cs.RO</categories><comments>Tool paper accepted in the 13th International Symposium on Automated
  Technology for Verification and Analysis (ATVA 2015)</comments><doi>10.1007/978-3-319-24953-7_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The formal verification of properties of Hidden Markov Models (HMMs) is
highly desirable for gaining confidence in the correctness of the model and the
corresponding system. A significant step towards HMM verification was the
development by Zhang et al. of a family of logics for verifying HMMs, called
POCTL*, and its model checking algorithm. As far as we know, the verification
tool we present here is the first one based on Zhang et al.'s approach. As an
example of its effective application, we verify properties of a handover task
in the context of human-robot interaction. Our tool was implemented in Haskell,
and the experimental evaluation was performed using the humanoid robot Bert2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05602</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05602</id><created>2015-07-20</created><updated>2016-01-06</updated><authors><author><keyname>Sahoo</keyname><forenames>Shaon</forenames></author></authors><title>Analyzing research performance: proposition of a new complementary index</title><categories>cs.DL physics.soc-ph</categories><comments>18+ pages; 1 table; title changed; two more references; substantially
  expanded version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A researcher collaborating with many groups will normally have more papers
(and thus higher citations and $h$-index) than a researcher spending all
his/her time working alone or in a small group. While analyzing an author's
research merit, it is therefore not enough to consider only the collective
impact of the published papers, it is also necessary to quantify his/her share
in the impact. For this quantification, here I propose the $I$-index which is
defined as an author's percentage share in the total citations that his/her
papers have attracted. It is argued that this $I$-index does not directly
depend on the most of the subjective issues like an author's influence,
affiliation, seniority or career break. A simple application of the Central
Limit Theorem shows that, the scheme of equidistribution of credit among the
coauthors of a paper will give us the most probable value of the $I$-index
(with an associated small standard deviation which decreases with increasing
$h$-index). I show that the total citations ($N_c$), the $h$-index and the
$I$-index are three independent parameters (within their bounds), and together
they give a comprehensive idea of an author's overall research performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05605</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05605</id><created>2015-07-20</created><authors><author><keyname>Perry</keyname><forenames>William</forenames></author><author><keyname>Wein</keyname><forenames>Alexander S.</forenames></author></authors><title>A semidefinite program for unbalanced multisection in the stochastic
  block model</title><categories>cs.DS math.PR stat.ML</categories><comments>26 pages</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze semidefinite programming (SDP) algorithms that exactly recover
community structure in graphs generated from the stochastic block model. In
this model, a graph is randomly generated on a vertex set that is partitioned
into multiple communities of potentially different sizes, where edges are more
probable within communities than between communities. We achieve exact recovery
of the community structure, up to the information-theoretic limits determined
by Abbe and Sandon. By virtue of a semidefinite approach, our algorithms
succeed against a semirandom form of the stochastic block model, guaranteeing
generalization to scenarios with radically different noise structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05612</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05612</id><created>2015-07-20</created><authors><author><keyname>L&#xf6;ding</keyname><forenames>Christof</forenames></author><author><keyname>Madhusudan</keyname><forenames>P.</forenames></author><author><keyname>Neider</keyname><forenames>Daniel</forenames></author></authors><title>Abstract Learning Frameworks for Synthesis</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop abstract learning frameworks (ALFs) for synthesis that embody the
principles of CEGIS (counter-example based inductive synthesis) strategies that
have become widely applicable in recent years. Our framework defines a general
abstract framework of iterative learning, based on a hypothesis space that
captures the synthesized objects, a sample space that forms the space on which
induction is performed, and a concept space that abstractly defines the
semantics of the learning process. We show that a variety of synthesis
algorithms in current literature can be embedded in this general framework.
While studying these embeddings, we also generalize some of the synthesis
problems these instances are of, resulting in new ways of looking at synthesis
problems using learning. We also investigate convergence issues for the general
framework, and exhibit three recipes for convergence in finite time. The first
two recipes generalize current techniques for convergence used by existing
synthesis engines. The third technique is a more involved technique of which we
know of no existing instantiation, and we instantiate it to concrete synthesis
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05630</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05630</id><created>2015-07-20</created><authors><author><keyname>Grella</keyname><forenames>Matteo</forenames></author></authors><title>Notes About a More Aware Dependency Parser</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper I explain the reasons that led me to research and conceive a
novel technology for dependency parsing, mixing together the strengths of
data-driven transition-based and constraint-based approaches. In particular I
highlight the problem to infer the reliability of the results of a data-driven
transition-based parser, which is extremely important for high-level processes
that expect to use correct parsing results. I then briefly introduce a number
of notes about a new parser model I'm working on, capable to proceed with the
analysis in a &quot;more aware&quot; way, with a more &quot;robust&quot; concept of robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05644</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05644</id><created>2015-07-20</created><authors><author><keyname>Aleem</keyname><forenames>Saiqa</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author><author><keyname>Ahmed</keyname><forenames>Faheem</forenames></author></authors><title>Security Issues in Data Warehouse</title><categories>cs.CR</categories><comments>5th European Conference of Computer Science (ECCS 14)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data Warehouse provides storage for huge amounts of historical data from
heterogeneous operational sources in the form of multidimensional views, thus
supplying sensitive and useful information which help decision-makers to
improve the organizations business processes. A data warehouse environment must
ensure that data collected and stored in one big repository are not vulnerable.
A review of security approaches specifically for data warehouse environment and
issues concerning each type of security approach have been provided in this
paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05645</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05645</id><created>2015-07-20</created><authors><author><keyname>Alrasheedi</keyname><forenames>Muasaad</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author><author><keyname>Raza</keyname><forenames>Arif</forenames></author></authors><title>Instructor Perspectives of Mobile Learning Platform: An Empirical Study</title><categories>cs.CY</categories><journal-ref>International Journal of Computer Science &amp; Information Technology
  7(3)27-40, 2015</journal-ref><doi>10.5121/ijcsit.2015.7303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile learning (mLearning) is the cutting-edge learning platform to really
gain traction, driven mostly by the huge uptake in smartphones and their ever
increasing uses within the educational society. Education has long benefitted
from the proliferation of technology; however, mLearning adoption has not
proceeded at the pace one might expect. There is a disconnect between the rate
of adoption of the underlying platform (smartphones) and the use of that
technology within learning. The reasons behind this have been the subject of
several research studies. However, previous studies have mostly focused on
investigating the critical success factors (CSFs) from the student
perspectives. In this research, we have carried out an extensive study of the
six factors that impact the success of mLearning from instructors perspectives.
The results of the research showed that three factors: technical competence of
instructors, Instructors autonomy, and blended learning are the most important
elements that contribute to mLearning adoption from instructors perspectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05648</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05648</id><created>2015-07-20</created><authors><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Teel</keyname><forenames>Andrew R.</forenames></author></authors><title>Lyapunov-based sufficient conditions for stability of hybrid systems
  with memory</title><categories>math.DS cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid systems with memory are dynamical systems exhibiting both hybrid and
delay phenomena. In this note, we study the asymptotic stability of hybrid
systems with memory using generalized concepts of solutions. These generalized
solutions, motivated by studying robustness and well-posedness of such systems,
are defined on hybrid time domains and parameterized by both continuous and
discrete time. We establish Lyapunov-based sufficient conditions for asymptotic
stability using both Lyapunov-Razumikhin functions and Lyapunov-Krasovskii
functionals. Examples are provided to illustrate these conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05654</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05654</id><created>2015-07-20</created><authors><author><keyname>Aleem</keyname><forenames>Saiqa</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author><author><keyname>Ahmed</keyname><forenames>Faheem</forenames></author></authors><title>Business Process Mining Approaches: A Relative Comparison</title><categories>cs.SE</categories><journal-ref>International Journal of Science, Technology &amp; Management,
  4(1):1557-1564, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, information systems like ERP, CRM and WFM record different business
events or activities in a log named as event log. Process mining aims at
extracting information from event logs to capture business process as it is
being executed. Process mining is an important learning task based on captured
processes. In order to be competent organizations in the business world; they
have to adjust their business process along with the changing environment.
Sometimes a change in the business process implies a change into the whole
system. Process mining allows for the automated discovery of process models
from event logs. Process mining techniques has the ability to support
automatically business process (re)design. Typically, these techniques discover
a concrete workflow model and all possible processes registered in a given
events log. In this paper, detailed comparison among process mining methods
used in the business process mining and differences in their approaches have
been provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05664</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05664</id><created>2015-07-20</created><authors><author><keyname>Cohen</keyname><forenames>Kobi</forenames></author><author><keyname>Nedich</keyname><forenames>Angelia</forenames></author><author><keyname>Srikant</keyname><forenames>R.</forenames></author></authors><title>Distributed Learning Algorithms for Spectrum Sharing in Spatial Random
  Access Wireless Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>37 pages, 6 figures, part of this work was presented at the 13th
  International Symposium on Modeling and Optimization in Mobile, Ad Hoc and
  Wireless Networks (WiOpt), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider distributed optimization over orthogonal collision channels in
spatial random access networks. Users are spatially distributed and each user
is in the interference range of a few other users. Each user is allowed to
transmit over a subset of the shared channels with a certain attempt
probability. We study both the non-cooperative and cooperative settings. In the
former, the goal of each user is to maximize its own rate irrespective of the
utilities of other users. In the latter, the goal is to achieve proportionally
fair rates among users. Simple distributed learning algorithms are developed to
solve these problems. The efficiencies of the proposed algorithms are
demonstrated via both theoretical analysis and simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05670</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05670</id><created>2015-07-20</created><updated>2015-11-09</updated><authors><author><keyname>Zhu</keyname><forenames>Yuke</forenames></author><author><keyname>Zhang</keyname><forenames>Ce</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author></authors><title>Building a Large-scale Multimodal Knowledge Base System for Answering
  Visual Queries</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complexity of the visual world creates significant challenges for
comprehensive visual understanding. In spite of recent successes in visual
recognition, today's vision systems would still struggle to deal with visual
queries that require a deeper reasoning. We propose a knowledge base (KB)
framework to handle an assortment of visual queries, without the need to train
new classifiers for new tasks. Building such a large-scale multimodal KB
presents a major challenge of scalability. We cast a large-scale MRF into a KB
representation, incorporating visual, textual and structured data, as well as
their diverse relations. We introduce a scalable knowledge base construction
system that is capable of building a KB with half billion variables and
millions of parameters in a few hours. Our system achieves competitive results
compared to purpose-built models on standard recognition and retrieval tasks,
while exhibiting greater flexibility in answering richer visual queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05679</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05679</id><created>2015-07-20</created><authors><author><keyname>Hills</keyname><forenames>Gage</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author><author><keyname>Shulaker</keyname><forenames>Max Marcel</forenames></author><author><keyname>Wei</keyname><forenames>Hai</forenames></author><author><keyname>Lee</keyname><forenames>Chi-Shuen</forenames></author><author><keyname>Balasingam</keyname><forenames>Arjun</forenames></author><author><keyname>Wong</keyname><forenames>H. -S. Philip</forenames></author><author><keyname>Mitra</keyname><forenames>Subhasish</forenames></author></authors><title>Rapid Co-optimization of Processing and Circuit Design to Overcome
  Carbon Nanotube Variations</title><categories>cs.ET</categories><journal-ref>IEEE Transactions on Computer-Aided Design of Integrated Circuits
  and Systems, vol. 34, no. 7, pp. 1082-1095, July 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Carbon nanotube field-effect transistors (CNFETs) are promising candidates
for building energy-efficient digital systems at highly-scaled technology
nodes. However, carbon nanotubes (CNTs) are inherently subject to variations
that reduce circuit yield, increase susceptibility to noise, and severely
degrade their anticipated energy and speed benefits. Joint exploration and
optimization of CNT processing options and CNFET circuit design are required to
overcome this outstanding challenge. Unfortunately, existing approaches for
such exploration and optimization are computationally expensive, and mostly
rely on trial-and-error-based ad hoc techniques. In this paper, we present a
framework that quickly evaluates the impact of CNT variations on circuit delay
and noise margin, and systematically explores the large space of CNT processing
options to derive optimized CNT processing and CNFET circuit design guidelines.
We demonstrate that our framework: 1) runs over 100x faster than existing
approaches, and 2) accurately identifies the most important CNT processing
parameters, together with CNFET circuit design parameters (e.g., for CNFET
sizing and standard cell layouts), to minimize the impact of CNT variations on
CNFET circuit speed with less than 5% energy cost, while simultaneously meeting
circuit-level noise margin and yield constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05681</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05681</id><created>2015-07-20</created><authors><author><keyname>Schloemann</keyname><forenames>Javier</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Buehrer</keyname><forenames>R. Michael</forenames></author></authors><title>A Tractable Analysis of the Improvement in Unique Localizability Through
  Collaboration</title><categories>cs.IT math.IT</categories><comments>33 double-spaced pages, 12 figures. Submitted to IEEE Transactions on
  Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we mathematically characterize the improvement in device
localizability achieved by allowing collaboration among devices. Depending on
the detection sensitivity of the receivers in the devices, it is not unusual
for a device to be localized to lack a sufficient number of detectable
positioning signals from localized devices to determine its location without
ambiguity (i.e., to be uniquely localizable). This occurrence is well-known to
be a limiting factor in localization performance, especially in communications
systems. In cellular positioning, for example, cellular network designers call
this the hearability problem. We study the conditions required for unique
localizability and use tools from stochastic geometry to derive accurate
analytic expressions for the probabilities of meeting these conditions in the
noncollaborative and collaborative cases. We consider the scenario without
shadowing, the scenario with shadowing and universal frequency reuse, and,
finally, the shadowing scenario with random frequency reuse. The results from
the latter scenario, which apply particularly to cellular networks, reveal that
collaboration between two devices separated by only a short distance yields
drastic improvements in both devices' abilities to uniquely determine their
positions. The results from this analysis are very promising and motivate
delving further into techniques which enhance cellular positioning with
small-scale collaborative ranging observations among nearby devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05684</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05684</id><created>2015-07-20</created><updated>2015-10-02</updated><authors><author><keyname>Obermeier</keyname><forenames>Richard</forenames></author><author><keyname>Martinez-Lorenzo</keyname><forenames>Jose Angel</forenames></author></authors><title>Model-based Optimization of Compressive Antennas for
  High-Sensing-Capacity Applications</title><categories>math.OC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel, model-based compressive antenna design method
for high sensing capacity imaging applications. Given a set of design
constraints, the method maximizes the sensing capacity of the compressive
antenna by varying the constitutive properties of scatterers distributed along
the antenna. Preliminary 2D design results demonstrate the new method's ability
to produce antenna configurations with enhanced imaging capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05687</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05687</id><created>2015-07-20</created><authors><author><keyname>Liu</keyname><forenames>Xiao Fan</forenames></author><author><keyname>Tse</keyname><forenames>Chi Kong</forenames></author></authors><title>A General Framework for Complex Network Applications</title><categories>physics.soc-ph cs.SI q-fin.ST</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex network theory has been applied to solving practical problems from
different domains. In this paper, we present a general framework for complex
network applications. The keys of a successful application are a thorough
understanding of the real system and a correct mapping of complex network
theory to practical problems in the system. Despite of certain limitations
discussed in this paper, complex network theory provides a foundation on which
to develop powerful tools in analyzing and optimizing large interconnected
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05695</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05695</id><created>2015-07-20</created><authors><author><keyname>Wang</keyname><forenames>Runchun</forenames></author><author><keyname>Thakur</keyname><forenames>Chetan Singh</forenames></author><author><keyname>Hamilton</keyname><forenames>Tara Julia</forenames></author><author><keyname>Tapson</keyname><forenames>Jonathan</forenames></author><author><keyname>van Schaik</keyname><forenames>Andre</forenames></author></authors><title>A neuromorphic hardware architecture using the Neural Engineering
  Framework for pattern recognition</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a hardware architecture that uses the Neural Engineering Framework
(NEF) to implement large-scale neural networks on Field Programmable Gate
Arrays (FPGAs) for performing pattern recognition in real time. NEF is a
framework that is capable of synthesising large-scale cognitive systems from
subnetworks. We will first present the architecture of the proposed neural
network implemented using fixed-point numbers and demonstrate a routine that
computes the decoding weights by using the online pseudoinverse update method
(OPIUM) in a parallel and distributed manner. The proposed system is
efficiently implemented on a compact digital neural core. This neural core
consists of 64 neurons that are instantiated by a single physical neuron using
a time-multiplexing approach. As a proof of concept, we combined 128 identical
neural cores together to build a handwritten digit recognition system using the
MNIST database and achieved a recognition rate of 96.55%. The system is
implemented on a state-of-the-art FPGA and can process 5.12 million digits per
second. The architecture is not limited to handwriting recognition, but is
generally applicable as an extremely fast pattern recognition processor for
various kinds of patterns such as speech and images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05698</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05698</id><created>2015-07-20</created><updated>2015-10-19</updated><authors><author><keyname>Geraci</keyname><forenames>Giovanni</forenames></author><author><keyname>Wildemeersch</keyname><forenames>Matthias</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author></authors><title>Energy Efficiency of Distributed Signal Processing in Wireless Networks:
  A Cross-Layer Analysis</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to meet the growing mobile data demand, future wireless networks
will be equipped with a multitude of access points (APs). Besides the important
implications for the energy consumption, the trend towards densification
requires the development of decentralized and sustainable radio resource
management techniques. It is critically important to understand how the
distribution of signal processing operations affects the energy efficiency of
wireless networks. In this paper, we provide a cross-layer framework to
evaluate and compare the energy efficiency of wireless networks under different
levels of distribution of the signal processing load: (i) hybrid, where the
signal processing operations are shared between nodes and APs, (ii)
centralized, where signal processing is entirely implemented at the APs, and
(iii) fully distributed, where all operations are performed by the nodes. We
find that in practical wireless networks, hybrid signal processing exhibits a
significant energy efficiency gain over both centralized and fully distributed
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05699</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05699</id><created>2015-07-21</created><updated>2015-08-02</updated><authors><author><keyname>Hu</keyname><forenames>Peiyun</forenames></author><author><keyname>Ramanan</keyname><forenames>Deva</forenames></author></authors><title>Bottom-up and top-down reasoning with convolutional latent-variable
  models</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural nets (CNNs) have demonstrated remarkable performance in
recent history. Such approaches tend to work in a &quot;unidirectional&quot; bottom-up
feed-forward fashion. However, biological evidence suggests that feedback plays
a crucial role, particularly for detailed spatial understanding tasks. This
work introduces &quot;bidirectional&quot; architectures that also reason with top-down
feedback: neural units are influenced by both lower and higher-level units. We
do so by treating units as latent variables in a global energy function. We
call our models convolutional latent-variable models (CLVMs). From a
theoretical perspective, CLVMs unify several approaches for recognition,
including CNNs, generative deep models (e.g., Boltzmann machines), and
discriminative latent-variable models (e.g., DPMs).
  From a practical perspective, CLVMs are particularly well-suited for
multi-task learning. We describe a single architecture that simultaneously
achieves state-of-the-art accuracy for tasks spanning both high-level
recognition (part detection/localization) and low-level grouping (pixel
segmentation). Bidirectional reasoning is particularly helpful for detailed
low-level tasks, since they can take advantage of top-down feedback. Our
architectures are quite efficient, capable of processing an image in
milliseconds. We present results on benchmark datasets with both part/keypoint
labels and segmentation masks (such as PASCAL and LFW) that demonstrate a
significant improvement over prior art, in both speed and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05717</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05717</id><created>2015-07-21</created><authors><author><keyname>Shi</keyname><forenames>Baoguang</forenames></author><author><keyname>Bai</keyname><forenames>Xiang</forenames></author><author><keyname>Yao</keyname><forenames>Cong</forenames></author></authors><title>An End-to-End Trainable Neural Network for Image-based Sequence
  Recognition and Its Application to Scene Text Recognition</title><categories>cs.CV</categories><comments>5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image-based sequence recognition has been a long-standing research topic in
computer vision. In this paper, we investigate the problem of scene text
recognition, which is among the most important and challenging tasks in
image-based sequence recognition. A novel neural network architecture, which
integrates feature extraction, sequence modeling and transcription into a
unified framework, is proposed. Compared with previous systems for scene text
recognition, the proposed architecture possesses four distinctive properties:
(1) It is end-to-end trainable, in contrast to most of the existing algorithms
whose components are separately trained and tuned. (2) It naturally handles
sequences in arbitrary lengths, involving no character segmentation or
horizontal scale normalization. (3) It is not confined to any predefined
lexicon and achieves remarkable performances in both lexicon-free and
lexicon-based scene text recognition tasks. (4) It generates an effective yet
much smaller model, which is more practical for real-world application
scenarios. The experiments on standard benchmarks, including the IIIT-5K,
Street View Text and ICDAR datasets, demonstrate the superiority of the
proposed algorithm over the prior arts. Moreover, the proposed algorithm
performs well in the task of image-based music score recognition, which
evidently verifies the generality of it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05720</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05720</id><created>2015-07-21</created><authors><author><keyname>Budden</keyname><forenames>David M.</forenames></author><author><keyname>Crampin</keyname><forenames>Edmund J.</forenames></author></authors><title>Gene expression modelling across multiple cell-lines with MapReduce</title><categories>q-bio.QM cs.DC q-bio.GN stat.ML</categories><comments>10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the wealth of high-throughput sequencing data generated by recent
large-scale consortia, predictive gene expression modelling has become an
important tool for integrative analysis of transcriptomic and epigenetic data.
However, sequencing data-sets are characteristically large, and previously
modelling frameworks are typically inefficient and unable to leverage
multi-core or distributed processing architectures. In this study, we detail an
efficient and parallelised MapReduce implementation of gene expression
modelling. We leverage the computational efficiency of this framework to
provide an integrative analysis of over fifty histone modification data-sets
across a variety of cancerous and non-cancerous cell-lines. Our results
demonstrate that the genome-wide relationships between histone modifications
and mRNA transcription are lineage, tissue and karyotype-invariant, and that
models trained on matched epigenetic/transcriptomic data from non-cancerous
cell-lines are able to predict cancerous expression with equivalent genome-wide
fidelity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05722</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05722</id><created>2015-07-21</created><updated>2015-07-27</updated><authors><author><keyname>Joshi</keyname><forenames>Manish</forenames></author><author><keyname>Hadi</keyname><forenames>Theyazn Hassn</forenames></author></authors><title>A Review of Network Traffic Analysis and Prediction Techniques</title><categories>cs.NI cs.AI</categories><comments>23 pages, 3 figure, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis and prediction of network traffic has applications in wide
comprehensive set of areas and has newly attracted significant number of
studies. Different kinds of experiments are conducted and summarized to
identify various problems in existing computer network applications. Network
traffic analysis and prediction is a proactive approach to ensure secure,
reliable and qualitative network communication. Various techniques are proposed
and experimented for analyzing network traffic including neural network based
techniques to data mining techniques. Similarly, various Linear and non-linear
models are proposed for network traffic prediction. Several interesting
combinations of network analysis and prediction techniques are implemented to
attain efficient and effective results.
  This paper presents a survey on various such network analysis and traffic
prediction techniques. The uniqueness and rules of previous studies are
investigated. Moreover, various accomplished areas of analysis and prediction
of network traffic have been summed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05724</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05724</id><created>2015-07-21</created><updated>2016-02-08</updated><authors><author><keyname>Chen</keyname><forenames>Chen</forenames></author><author><keyname>Asoni</keyname><forenames>Daniele Enrico</forenames></author><author><keyname>Barrera</keyname><forenames>David</forenames></author><author><keyname>Danezis</keyname><forenames>George</forenames></author><author><keyname>Perrig</keyname><forenames>Adrian</forenames></author></authors><title>HORNET: High-speed Onion Routing at the Network Layer</title><categories>cs.CR</categories><comments>14 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present HORNET, a system that enables high-speed end-to-end anonymous
channels by leveraging next generation network architectures. HORNET is
designed as a low-latency onion routing system that operates at the network
layer thus enabling a wide range of applications. Our system uses only
symmetric cryptography for data forwarding yet requires no per-flow state on
intermediate nodes. This design enables HORNET nodes to process anonymous
traffic at over 93 Gb/s. HORNET can also scale as required, adding minimal
processing overhead per additional anonymous channel. We discuss design and
implementation details, as well as a performance and security evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05726</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05726</id><created>2015-07-21</created><authors><author><keyname>Wetzler</keyname><forenames>Aaron</forenames></author><author><keyname>Slossberg</keyname><forenames>Ron</forenames></author><author><keyname>Kimmel</keyname><forenames>Ron</forenames></author></authors><title>Rule Of Thumb: Deep derotation for improved fingertip detection</title><categories>cs.CV</categories><comments>To be published in proceedings of BMVC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a novel global orientation regression approach for articulated
objects using a deep convolutional neural network. This is integrated with an
in-plane image derotation scheme, DeROT, to tackle the problem of per-frame
fingertip detection in depth images. The method reduces the complexity of
learning in the space of articulated poses which is demonstrated by using two
distinct state-of-the-art learning based hand pose estimation methods applied
to fingertip detection. Significant classification improvements are shown over
the baseline implementation. Our framework involves no tracking, kinematic
constraints or explicit prior model of the articulated object in hand. To
support our approach we also describe a new pipeline for high accuracy magnetic
annotation and labeling of objects imaged by a depth camera.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05728</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05728</id><created>2015-07-21</created><authors><author><keyname>Li</keyname><forenames>Congduan</forenames></author><author><keyname>Weber</keyname><forenames>Steven</forenames></author><author><keyname>Walsh</keyname><forenames>John MacLaren</forenames></author></authors><title>On Multi-source Networks: Enumeration, Rate Region Computation, and
  Hierarchy</title><categories>cs.IT math.IT</categories><comments>63 pages, submitted to TransIT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the enumeration, rate region computation, and
hierarchy of general multi-source multi-sink hyperedge networks under network
coding, which includes multiple network models, such as independent distributed
storage systems and index coding problems, as special cases. A notion of
minimal networks and a notion of network equivalence under group action are
defined. An efficient algorithm capable of directly listing single minimal
canonical representatives from each network equivalence class is presented and
utilized to list all minimal canonical networks with up to 5 sources and
hyperedges. Computational tools are then applied to obtain the rate regions of
all of these canonical networks, providing exact expressions for 744,119 newly
solved network coding rate regions corresponding to more than 2 trillion
isomorphic network coding problems. In order to better understand and analyze
the huge repository of rate regions through hierarchy, several embedding and
combination operations are defined so that the rate region of the network after
operation can be derived from the rate regions of networks involved in the
operation. The embedding operations enable the definition and determination of
a list of forbidden network minors for the sufficiency of classes of linear
codes. The combination operations enable the rate regions of some larger
networks to be obtained as the combination of the rate regions of smaller
networks. The integration of both the combinations and embedding operators is
then shown to enable the calculation of rate regions for many networks not
reachable via combination operations alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05732</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05732</id><created>2015-07-21</created><authors><author><keyname>Yang</keyname><forenames>Shudi</forenames></author><author><keyname>Yao</keyname><forenames>Zhang-An</forenames></author><author><keyname>Zhao</keyname><forenames>Chang-An</forenames></author></authors><title>Complete Weight Enumerator of a Family of Linear Codes from Cyclotomy</title><categories>cs.IT math.IT</categories><comments>13 pages</comments><msc-class>94B15, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear codes have been an interesting topic in both theory and practice for
many years. In this paper, for a prime $p$, we determine the explicit complete
weight enumerators of a family of linear codes over $\mathbb{F}_p$ with
defining set related to cyclotomy. These codes may have applications in
cryptography and secret sharing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05737</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05737</id><created>2015-07-21</created><authors><author><keyname>Li</keyname><forenames>Xi</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Dick</keyname><forenames>Anthony</forenames></author><author><keyname>Zhang</keyname><forenames>Zhongfei</forenames></author><author><keyname>Zhuang</keyname><forenames>Yueting</forenames></author></authors><title>Online Metric-Weighted Linear Representations for Robust Visual Tracking</title><categories>cs.CV</categories><comments>51 pages. Appearing in IEEE Transactions on Pattern Analysis and
  Machine Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a visual tracker based on a metric-weighted linear
representation of appearance. In order to capture the interdependence of
different feature dimensions, we develop two online distance metric learning
methods using proximity comparison information and structured output learning.
The learned metric is then incorporated into a linear representation of
appearance.
  We show that online distance metric learning significantly improves the
robustness of the tracker, especially on those sequences exhibiting drastic
appearance changes. In order to bound growth in the number of training samples,
we design a time-weighted reservoir sampling method.
  Moreover, we enable our tracker to automatically perform object
identification during the process of object tracking, by introducing a
collection of static template samples belonging to several object classes of
interest. Object identification results for an entire video sequence are
achieved by systematically combining the tracking information and visual
recognition at each frame. Experimental results on challenging video sequences
demonstrate the effectiveness of the method for both inter-frame tracking and
object identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05738</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05738</id><created>2015-07-21</created><updated>2015-07-31</updated><authors><author><keyname>Yeung</keyname><forenames>Serena</forenames></author><author><keyname>Russakovsky</keyname><forenames>Olga</forenames></author><author><keyname>Jin</keyname><forenames>Ning</forenames></author><author><keyname>Andriluka</keyname><forenames>Mykhaylo</forenames></author><author><keyname>Mori</keyname><forenames>Greg</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author></authors><title>Every Moment Counts: Dense Detailed Labeling of Actions in Complex
  Videos</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every moment counts in action recognition. A comprehensive understanding of
human activity in video requires labeling every frame according to the actions
occurring, placing multiple labels densely over a video sequence. To study this
problem we extend the existing THUMOS dataset and introduce MultiTHUMOS, a new
dataset of dense labels over unconstrained internet videos. Modeling multiple,
dense labels benefits from temporal relations within and across classes. We
define a novel variant of long short-term memory (LSTM) deep networks for
modeling these temporal relations via multiple input and output connections. We
show that this model improves action labeling accuracy and further enables
deeper understanding tasks ranging from structured retrieval to action
prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05739</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05739</id><created>2015-07-21</created><updated>2015-09-21</updated><authors><author><keyname>Budur</keyname><forenames>Emrah</forenames></author><author><keyname>Lee</keyname><forenames>Seungmin</forenames></author><author><keyname>Kong</keyname><forenames>Vein S.</forenames></author></authors><title>Structural Analysis of Criminal Network and Predicting Hidden Links
  using Machine Learning</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of criminal networks is inherently difficult because of the nature
of the topic. Criminal networks are covert and most of the information is not
publicly available. This leads to small datasets available for analysis. The
available criminal network datasets consists of entities, i.e. individual or
organizations, which are linked to each other. The links between entities
indicates that there is a connection between these entities such as involvement
in the same criminal event, having commercial ties, and/or memberships in the
same criminal organization. Because of incognito criminal activities, there
could be many hidden links from entities to entities, which makes the publicly
available criminal networks incomplete. Revealing hidden links introduces new
information, e.g. affiliation of a suspected individual with a criminal
organization, which may not be known with public information. What will we be
able to find if we can run analysis on a larger dataset and use link prediction
to reveal the implicit connections? We plan to answer this question by using a
dataset that is an order of magnitude more than what is used in most criminal
networks analysis. And by using machine learning techniques, we will convert a
link prediction problem to a binary classification problem. We plan to reveal
hidden links and potentially hidden key attributes of the criminal network.
With a more complete picture of the network, we can potentially use this data
to thwart criminal organizations and/or take a Pareto approach in targeting key
nodes. We conclude our analysis with an effective destruction strategy to
weaken criminal networks and prove the effectiveness of revealing hidden links
when attacking to criminal networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05742</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05742</id><created>2015-07-21</created><authors><author><keyname>Bissyand&#xe9;</keyname><forenames>Tegawend&#xe9; F.</forenames></author></authors><title>Harvesting Fix Hints in the History of Bugs</title><categories>cs.SE</categories><comments>6 pages</comments><report-no>ISBN978-2-87971-143-0 / TR-SNT-2015-6</report-no><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In software development, fixing bugs is an important task that is time
consuming and cost-sensitive. While many approaches have been proposed to
automatically detect and patch software code, the strategies are limited to a
set of identified bugs that were thoroughly studied to define their properties.
They thus manage to cover a niche of faults such as infinite loops. We build on
the assumption that bugs, and the associated user bug reports, are repetitive
and propose a new approach of fix recommendations based on the history of bugs
and their associated fixes. In our approach, once a bug is reported, it is
automatically compared to all previously fixed bugs using information retrieval
techniques and machine learning classification. Based on this comparison, we
recommend top-{\em k} fix actions, identified from past fix examples, that may
be suitable as hints for software developers to address the new bug.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05746</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05746</id><created>2015-07-21</created><authors><author><keyname>Fricker</keyname><forenames>Christine</forenames></author><author><keyname>Guillemin</keyname><forenames>Fabrice</forenames></author><author><keyname>Robert</keyname><forenames>Philippe</forenames></author><author><keyname>Thompson</keyname><forenames>Guilherme</forenames></author></authors><title>Analysis of an offloading scheme for data centers in the framework of
  fog computing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of fog computing, we consider a simple case when data centers
installed at the edge of the network are backed up by a central (bigger) data
center. The system considered precisely comprises two data centers in parallel.
We assume that if a request arrives at an overloaded data center, it is
forwarded to the other data center with a given probability. Both data centers
are assumed to have a large number of servers (rescaling of the system) and
that traffic to one of them is causing saturation so that the other data center
may help to cope with this saturation regime by reducing the rejection of
requests. Our aim here is to qualitatively estimate the gain achieved by the
collaboration of the two data centers. After proving some convergence results,
related to the scaling limits of loss systems, for the process describing the
number of free servers at both data centers, we show that the performance of
the system can be expressed in terms of the invariant distribution of a random
walk in the quarter plane. By using and developing existing results in the
technical literature, explicit formulas for the blocking rates of such a system
are derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05751</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05751</id><created>2015-07-21</created><authors><author><keyname>Liu</keyname><forenames>Haiying</forenames></author><author><keyname>Feng</keyname><forenames>Keqin</forenames></author><author><keyname>Feng</keyname><forenames>Rongquan</forenames></author></authors><title>Nonexistence of Generalized Bent Functions From $Z_{2}^{n}$ to $Z_{m}$</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several nonexistence results on generalized bent functions $f:Z_{2}^{n}
\rightarrow Z_{m}$ presented by using some knowledge on cyclotomic number
fields and their imaginary quadratic subfields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05753</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05753</id><created>2015-07-21</created><updated>2015-08-10</updated><authors><author><keyname>Chermakani</keyname><forenames>Deepak Ponvel</forenames></author></authors><title>Optimal Aggregation of Blocks into Subproblems in Linear-Programs with
  Block-Diagonal-Structure</title><categories>cs.DM cs.CC math.OC</categories><comments>Added Theorem 8 to cover the case of the P function being linear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wall-clock-time is minimized for a solution to a linear-program with
block-diagonal-structure, by decomposing the linear-program into as many
small-sized subproblems as possible, each block resulting in a separate
subproblem, when the number of available parallel-processing-units is at least
equal to the number of blocks. This is not necessarily the case when the
parallel processing capability is limited, causing multiple subproblems to be
serially solved on the same processing-unit. In such a situation, it might be
better to aggregate blocks into larger sized subproblems. The optimal
aggregation strategy depends on the computing-platform used, and minimizes the
average-case running time for the set of subproblems. We show that optimal
aggregation is NP-hard when blocks are of unequal size, and that optimal
aggregation can be achieved within polynomial-time when blocks are of equal
size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05762</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05762</id><created>2015-07-21</created><authors><author><keyname>Gange</keyname><forenames>Graeme</forenames></author><author><keyname>Navas</keyname><forenames>Jorge A.</forenames></author><author><keyname>Schachte</keyname><forenames>Peter</forenames></author><author><keyname>Sondergaard</keyname><forenames>Harald</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter J.</forenames></author></authors><title>Horn Clauses as an Intermediate Representation for Program Analysis and
  Transformation</title><categories>cs.PL</categories><comments>To Appear in Theory and Practice of Logic Programming (TPLP),
  Proceedings of ICLP 2015</comments><journal-ref>Theory and Practice of Logic Programming 15 (2015) 526-542</journal-ref><doi>10.1017/S1471068415000204</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Many recent analyses for conventional imperative programs begin by
transforming programs into logic programs, capitalising on existing LP analyses
and simple LP semantics. We propose using logic programs as an intermediate
program representation throughout the compilation process. With restrictions
ensuring determinism and single-modedness, a logic program can easily be
transformed to machine language or other low-level language, while maintaining
the simple semantics that makes it suitable as a language for program analysis
and transformation. We present a simple LP language that enforces determinism
and single-modedness, and show that it makes a convenient program
representation for analysis and transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05765</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05765</id><created>2015-07-21</created><authors><author><keyname>Sun</keyname><forenames>Hong</forenames></author><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Gui</keyname><forenames>Ning</forenames></author><author><keyname>Blondia</keyname><forenames>Chris</forenames></author></authors><title>Promises and Challenges of Ambient Assisted Living Systems</title><categories>cs.CY</categories><comments>Published in the Proc. of the 6th Int.l Conference on Information
  Technology: New Generations (ITNG 2009), April 27-29, 2009, Las Vegas,
  Nevada, USA. IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The population of elderly people keeps increasing rapidly, which becomes a
predominant aspect of our societies. As such, solutions both efficacious and
cost-effective need to be sought. Ambient Assisted Living (AAL) is a new
approach which promises to address the needs from elderly people. Ambient
Intelligence technologies are widely developed in this domain aiming to
construct safe environments around assisted peoples and help them maintain
independent living. However, there are still many fundamental issues in AAL
that remain open. Most of the current efforts still do not fully express the
power of human being, and the importance of social connections and social
activities is less noticed. Our conjecture is that such features are
fundamental prerequisites towards truly effective AAL services. This paper
reviews the current status of researches on AAL, discusses the promises and
possible advantages of AAL, and also indicates the challenges we must meet in
order to develop practical and efficient AAL systems for elderly people. In
this paper, we also propose an approach to construct effective home-care system
for the elderly people.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05766</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05766</id><created>2015-07-21</created><updated>2015-11-09</updated><authors><author><keyname>Boreale</keyname><forenames>M.</forenames><affiliation>University of Florence</affiliation></author><author><keyname>Pampaloni</keyname><forenames>Francesca</forenames><affiliation>IMT - Lucca</affiliation></author></authors><title>Quantitative information flow under generic leakage functions and
  adaptive adversaries</title><categories>cs.LO cs.CR</categories><comments>Revised and extended version of conference paper with the same title
  appeared in Proc. of FORTE 2014, LNCS</comments><proxy>LMCS</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We put forward a model of action-based randomization mechanisms to analyse
quantitative information flow (QIF) under generic leakage functions, and under
possibly adaptive adversaries. This model subsumes many of the QIF models
proposed so far. Our main contributions include the following: (1) we identify
mild general conditions on the leakage function under which it is possible to
derive general and significant results on adaptive QIF; (2) we contrast the
efficiency of adaptive and non-adaptive strategies, showing that the latter are
as efficient as the former in terms of length up to an expansion factor bounded
by the number of available actions; (3) we show that the maximum information
leakage over strategies, given a finite time horizon, can be expressed in terms
of a Bellman equation. This can be used to compute an optimal finite strategy
recursively, by resorting to standard methods like backward induction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05775</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05775</id><created>2015-07-21</created><updated>2015-07-22</updated><authors><author><keyname>Zhou</keyname><forenames>Shuchang</forenames></author><author><keyname>Wu</keyname><forenames>Jia-Nan</forenames></author></authors><title>Compression of Fully-Connected Layer in Neural Network by Kronecker
  Product</title><categories>cs.NE cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose and study a technique to reduce the number of
parameters and computation time in fully-connected layers of neural networks
using Kronecker product, at a mild cost of the prediction quality. The
technique proceeds by replacing Fully-Connected layers with so-called Kronecker
Fully-Connected layers, where the weight matrices of the FC layers are
approximated by linear combinations of multiple Kronecker products of smaller
matrices. In particular, given a model trained on SVHN dataset, we are able to
construct a new KFC model with 73\% reduction in total number of parameters,
while the error only rises mildly. In contrast, using low-rank method can only
achieve 35\% reduction in total number of parameters given similar quality
degradation allowance. If we only compare the KFC layer with its counterpart
fully-connected layer, the reduction in the number of parameters exceeds 99\%.
The amount of computation is also reduced as we replace matrix product of the
large matrices in FC layers with matrix products of a few smaller matrices in
KFC layers. Further experiments on MNIST, SVHN and some Chinese Character
recognition models also demonstrate effectiveness of our technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05787</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05787</id><created>2015-07-21</created><authors><author><keyname>Guha</keyname><forenames>Shibashis</forenames></author><author><keyname>Krishna</keyname><forenames>Shankara Narayanan</forenames></author><author><keyname>Manasa</keyname><forenames>Lakshmi</forenames></author><author><keyname>Trivedi</keyname><forenames>Ashutosh</forenames></author></authors><title>Revisiting Robustness in Priced Timed Games</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Priced timed games are optimal-cost reachability games played between two
players---the controller and the environment---by moving a token along the
edges of infinite graphs of configurations of priced timed automata. The goal
of the controller is to reach a given set of target locations as cheaply as
possible, while the goal of the environment is the opposite. Priced timed games
are known to be undecidable for timed automata with $3$ or more clocks, while
they are known to be decidable for automata with $1$ clock.
  In an attempt to recover decidability for priced timed games Bouyer, Markey,
and Sankur studied robust priced timed games where the environment has the
power to slightly perturb delays proposed by the controller. Unfortunately,
however, they showed that the natural problem of deciding the existence of
optimal limit-strategy---optimal strategy of the controller where the
perturbations tend to vanish in the limit---is undecidable with $10$ or more
clocks. In this paper we revisit this problem and improve our understanding of
the decidability of these games. We show that the limit-strategy problem is
already undecidable for a subclass of robust priced timed games with $5$ or
more clocks. On a positive side, we show the decidability of the existence of
almost optimal strategies for the same subclass of one-clock robust priced
timed games by adapting a classical construction by Bouyer at al. for one-clock
priced timed games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05790</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05790</id><created>2015-07-21</created><authors><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>A System for Sensing Human Sentiments to Augment a Model for Predicting
  Rare Lake Events</title><categories>cs.SI</categories><comments>20 pages, 7 figures, appeared in Proceedings of the Joint 12th
  International Agricultural Engineering Conference and Exhibition, 65th PSAE
  National Convention, and 26th Philippine Agricultural Engineering Week (PSAE
  2015), KCC Convention and Events Center, General Santos City, Philippines,
  19-25 April 2015</comments><journal-ref>Philippine Computing Journal 10(2):30-32, December 2015</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Fish kill events (FKE) in the caldera lake of Taal occur rarely (only 0.5\%
in the last 10 years) but each event has a long-term effect on the
environmental health of the lake ecosystem, as well as a devastating effect on
the financial and emotional aspects of the residents whose livelihood rely on
aquaculture farming. Predicting with high accuracy when within seven days and
where on the vast expanse of the lake will FKEs strike will be a very important
early warning tool for the lake's aquaculture industry. Mathematical models to
predict the occurrences of FKEs developed by several studies done in the past
use as predictors the physico-chemical characteristics of the lake water, as
well as the meteorological parameters above it. Some of the models, however,
did not provide acceptable predictive accuracy and enough early warning because
they were developed with unbalanced binary data set, i.e., characterized by
dense negative examples (no FKE) and highly sparse positive examples (with
FKE). Other models require setting up an expensive sensor network to measure
the water parameters not only at the surface but also at several depths.
Presented in this paper is a system for capturing, measuring, and visualizing
the contextual sentiment polarity (CSP) of dated and geolocated social media
microposts of residents within 10km radius of the Taal Volcano crater
($14^\circ$N, $121^\circ$E). High frequency negative CSP co-occur with FKE for
two occasions making human expressions a viable non-physical sensors for
impending FKE to augment existing mathematical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05795</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05795</id><created>2015-07-21</created><authors><author><keyname>Funke</keyname><forenames>Simon W.</forenames></author><author><keyname>Kramer</keyname><forenames>Stephan C.</forenames></author><author><keyname>Piggott</keyname><forenames>Matthew D.</forenames></author></authors><title>Design optimisation and resource assessment for tidal-stream renewable
  energy farms using a new continuous turbine approach</title><categories>cs.CE math.OC</categories><acm-class>G.1.6; G.1.8; G.4; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach for optimising the design of tidal stream
turbine farms. In this approach, the turbine farm is represented by a turbine
density function that specifies the number of turbines per unit area and an
associated continuous locally-enhanced bottom friction field. The farm design
question is formulated as a mathematical optimisation problem constrained by
the shallow water equations and solved with efficient, gradient-based
optimisation methods. The resulting method is accurate, computationally
efficient, allows complex installation constraints, and supports different goal
quantities such as to maximise power or profit. The outputs of the optimisation
are the optimal number of turbines, their location within the farm, the overall
farm profit, the farm's power extraction, and the installation cost. We
demonstrate the capabilities of the method on a validated numerical model of
the Pentland Firth, Scotland. We optimise the design of four tidal farms
simultaneously, as well as individually, and study how farms in close proximity
may impact upon one another.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05796</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05796</id><created>2015-07-21</created><updated>2016-02-13</updated><authors><author><keyname>Fraigniaud</keyname><forenames>Pierre</forenames></author><author><keyname>Natale</keyname><forenames>Emanuele</forenames></author></authors><title>Noisy Rumor Spreading and Plurality Consensus</title><categories>cs.DC</categories><comments>Minor revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error-correcting codes are efficient methods for handling \emph{noisy}
communication channels in the context of technological networks. However, such
elaborate methods differ a lot from the unsophisticated way biological entities
are supposed to communicate. Yet, it has been recently shown by Feinerman,
Haeupler, and Korman {[}PODC 2014{]} that complex coordination tasks such as
\emph{rumor spreading} and \emph{majority consensus} can plausibly be achieved
in biological systems subject to noisy communication channels, where every
message transferred through a channel remains intact with small probability
$\frac{1}{2}+\epsilon$, without using coding techniques. This result is a
considerable step towards a better understanding of the way biological entities
may cooperate. It has been nevertheless be established only in the case of
2-valued \emph{opinions}: rumor spreading aims at broadcasting a single-bit
opinion to all nodes, and majority consensus aims at leading all nodes to adopt
the single-bit opinion that was initially present in the system with (relative)
majority. In this paper, we extend this previous work to $k$-valued opinions,
for any $k\geq2$.
  Our extension requires to address a series of important issues, some
conceptual, others technical. We had to entirely revisit the notion of noise,
for handling channels carrying $k$-\emph{valued} messages. In fact, we
precisely characterize the type of noise patterns for which plurality consensus
is solvable. Also, a key result employed in the bivalued case by Feinerman et
al. is an estimate of the probability of observing the most frequent opinion
from observing the mode of a small sample. We generalize this result to the
multivalued case by providing a new analytical proof for the bivalued case that
is amenable to be extended, by induction, and that is of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05800</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05800</id><created>2015-07-21</created><authors><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Ma</keyname><forenames>Yao</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Bandit-Based Task Assignment for Heterogeneous Crowdsourcing</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a task assignment problem in crowdsourcing, which is aimed at
collecting as many reliable labels as possible within a limited budget. A
challenge in this scenario is how to cope with the diversity of tasks and the
task-dependent reliability of workers, e.g., a worker may be good at
recognizing the name of sports teams, but not be familiar with cosmetics
brands. We refer to this practical setting as heterogeneous crowdsourcing. In
this paper, we propose a contextual bandit formulation for task assignment in
heterogeneous crowdsourcing, which is able to deal with the
exploration-exploitation trade-off in worker selection. We also theoretically
investigate the regret bounds for the proposed method, and demonstrate its
practical usefulness experimentally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05810</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05810</id><created>2015-07-21</created><authors><author><keyname>Vucinic</keyname><forenames>Malisa</forenames><affiliation>ST MICROELECTRONICS</affiliation></author><author><keyname>Tourancheau</keyname><forenames>Bernard</forenames><affiliation>EVA</affiliation></author><author><keyname>Watteyne</keyname><forenames>Thomas</forenames><affiliation>EVA</affiliation></author><author><keyname>Rousseau</keyname><forenames>Franck</forenames><affiliation>ST MICROELECTRONICS</affiliation></author><author><keyname>Duda</keyname><forenames>Andrzej</forenames><affiliation>ST MICROELECTRONICS</affiliation></author><author><keyname>Guizzetti</keyname><forenames>Roberto</forenames><affiliation>ST MICROELECTRONICS</affiliation></author><author><keyname>Damon</keyname><forenames>Laurent</forenames><affiliation>ST MICROELECTRONICS</affiliation></author></authors><title>DTLS Performance in Duty-Cycled Networks</title><categories>cs.NI</categories><comments>International Symposium on Personal, Indoor and Mobile Radio
  Communications (PIMRC - 2015), IEEE, IEEE, 2015,
  http://pimrc2015.eee.hku.hk/index.html</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Datagram Transport Layer Security (DTLS) protocol is the IETF standard
for securing the Internet of Things. The Constrained Application Protocol,
ZigBee IP, and Lightweight Machine-to-Machine (LWM2M) mandate its use for
securing application traffic. There has been much debate in both the
standardization and research communities on the applicability of DTLS to
constrained environments. The main concerns are the communication overhead and
latency of the DTLS handshake, and the memory footprint of a DTLS
implementation. This paper provides a thorough performance evaluation of DTLS
in different duty-cycled networks through real-world experimentation, emulation
and analysis. In particular, we measure the duration of the DTLS handshake when
using three duty cycling link-layer protocols: preamble-sampling, the IEEE
802.15.4 beacon-enabled mode and the IEEE 802.15.4e Time Slotted Channel
Hopping mode. The reported results demonstrate surprisingly poor performance of
DTLS in radio duty-cycled networks. Because a DTLS client and a server exchange
more than 10 signaling packets, the DTLS handshake takes between a handful of
seconds and several tens of seconds, with similar results for different duty
cycling protocols. Moreover, because of their limited memory, typical
constrained nodes can only maintain 3-5 simultaneous DTLS sessions, which
highlights the need for using DTLS parsimoniously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05812</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05812</id><created>2015-07-21</created><authors><author><keyname>Coladon</keyname><forenames>Titouan</forenames><affiliation>ST MICROELECTRONICS</affiliation></author><author><keyname>Vucinic</keyname><forenames>Malisa</forenames><affiliation>ST MICROELECTRONICS</affiliation></author><author><keyname>Tourancheau</keyname><forenames>Bernard</forenames></author></authors><title>Multiple Redundancy Constants with Trickle</title><categories>cs.NI</categories><comments>International Symposium on Personal, Indoor and Mobile Radio
  Communications (PIMRC - 2015), IEEE, IEEE, 2015,
  http://pimrc2015.eee.hku.hk/index.html</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor network protocols very often use the Trickle algorithm to
govern information dissemination. For example, the widely used IPv6 Routing
Protocol for Low-Power and Lossy Networks (RPL) uses Trickle to emit control
packets. We derive an analytical model of Trickle to take into account multiple
redundancy constants and the common lack of synchronization among nodes.
Moreover, we demonstrate message count unfairness when Trickle uses a unique
global redundancy constant because nodes with less neighbors transmit more
often. Consequently, we propose a heuristic algorithm that calculates a
redundancy constant for each node as a function of its number of neighbors. Our
calculated redundancy constants reduce unfairness among nodes by distributing
more equally the number of transmitted messages in the network. Our analytical
model is validated by emulations of constrained devices running the Contiki
Operating System and its IPv6 networking stack. Furthermore, results very well
corroborate the heuristic algorithm improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05819</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05819</id><created>2015-07-21</created><authors><author><keyname>Wright</keyname><forenames>Joss</forenames></author><author><keyname>Darer</keyname><forenames>Alexander</forenames></author><author><keyname>Farnan</keyname><forenames>Oliver</forenames></author></authors><title>Detecting Internet Filtering from Geographic Time Series</title><categories>cs.CY cs.LG cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an approach based on principle component analysis to identify
per-country anomalous periods in traffic usage as a means to detect internet
filtering, and demonstrate the applicability of this approach with global usage
statistics from the Tor Project. In contrast to previous country-specific
investigations, our techniques use deviation from global patterns of usage to
identify countries straying from predicted behaviour, allowing the
identification of periods of filtering and related events in any country for
which usage statistics exist. To our knowledge the work presented here is the
first automated approach to detecting internet filtering at a global scale.
  We demonstrate the applicability of our approach by identifying known
historical filtering events as well as events injected synthetically into a
dataset, and evaluate the sensitivity of this technique against different
classes of censorship events. Importantly, our results show that usage of
circumvention tools, such as those provided by the Tor Project, act not only as
direct indicators of network censorship but also as a meaningful proxy variable
for related events such as protests in which internet use is restricted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05841</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05841</id><created>2015-07-21</created><updated>2016-01-19</updated><authors><author><keyname>Marinov</keyname><forenames>Martin</forenames></author><author><keyname>Gregg</keyname><forenames>David</forenames></author></authors><title>On the GI-Completeness of a Sorting Networks Isomorphism</title><categories>cs.CC</categories><acm-class>F.1.3; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The subitemset isomorphism problem is really important and there are
excellent practical solutions described in the literature. However, the
computational complexity analysis and classification of the BZ (Bundala and
Zavodny) subitemset isomorphism problem is currently an open problem. In this
paper we prove that checking whether two sorting networks are BZ isomorphic to
each other is GI-Complete; the general GI (Graph Isomorphism) problem is known
to be in NP and LWPP, but widely believed to be neither P nor NP-Complete;
recent research suggests that the problem is in QP. Moreover, we state the BZ
sorting network isomorphism problem as a general isomorphism problem on
itemsets --- because every sorting network is represented by Bundala and
Zavodny as an itemset. The complexity classification presented in this paper
applies sorting networks, as well as the general itemset isomorphism problem.
The main consequence of our work is that currently no polynomial-time algorithm
exists for solving the BZ sorting network subitemset isomorphism problem;
however the CM (Choi and Moon) sorting network isomorphism problem can be
efficiently solved in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05854</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05854</id><created>2015-07-21</created><authors><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Jin</keyname><forenames>Chi</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author><author><keyname>Netrapalli</keyname><forenames>Praneeth</forenames></author></authors><title>Computing Matrix Squareroot via Non Convex Local Search</title><categories>math.NA cs.DS cs.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing the squareroot of a positive
semidefinite (PSD) matrix. Several fast algorithms (some based on eigenvalue
decomposition and some based on Taylor expansion) are known to solve this
problem.
  In this paper, we propose another way to solve this problem: a natural
algorithm performing gradient descent on a non-convex formulation of the matrix
squareroot problem. We show that on an $n\times n$ input PSD matrix ${M}$, if
the initial point is well conditioned, then the algorithm finds an
$\epsilon$-accurate solution in $O\left(\kappa^{3/2} \log
\frac{\left\|{M}\right\|_F}{\epsilon}\right)$ iterations, where $\kappa$ is the
condition number of $M$. Each iteration involves three matrix multiplications
(and does not use either matrix inversions or solutions of linear system),
giving a total run time of
$O\left(n^{\omega}\kappa^{3/2}\log\frac{\left\|{M}\right\|_F}{\epsilon}\right)$,
where $\omega$ is the matrix multiplication exponent. Furthermore we show that
our algorithm is robust to errors in each iteration. We also show a lower bound
of $\Omega(\kappa)$ iterations for our algorithm demonstrating that the
dependence of our result on $\kappa$ is necessary.
  Existing analyses of similar algorithms (e.g., Newton's method) require
commutativity of the input matrix with each iterate of the algorithm which is
ensured by choosing the starting iterate carefully. Our analysis, on the other
hand, is much more general and does not require each iterate to commute with
the input matrix. Consequently, our result guarantees convergence from a wide
range of starting points.
  More generally, our result demonstrates that non-convex optimization can be a
viable approach to obtaining fast and robust algorithms. Our argument is quite
general and we believe it will find application in designing such algorithms
for other problems in numerical linear algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05875</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05875</id><created>2015-07-21</created><authors><author><keyname>Recknagel</keyname><forenames>Arne</forenames></author><author><keyname>Besold</keyname><forenames>Tarek R.</forenames></author></authors><title>Efficient Dodgson-Score Calculation Using Heuristics and Parallel
  Computing</title><categories>cs.AI cs.DC cs.GT cs.MA</categories><acm-class>I.2.8; I.2.11; I.2.3; H.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conflict of interest is the permanent companion of any population of agents
(computational or biological). For that reason, the ability to compromise is of
paramount importance, making voting a key element of societal mechanisms. One
of the voting procedures most often discussed in the literature and, due to its
intuitiveness, also conceptually quite appealing is Charles Dodgson's scoring
rule, basically using the respective closeness to being a Condorcet winner for
evaluating competing alternatives. In this paper, we offer insights on the
practical limits of algorithms computing the exact Dodgson scores from a number
of votes. While the problem itself is theoretically intractable, this work
proposes and analyses five different solutions which try distinct approaches to
practically solve the issue in an effective manner. Additionally, three of the
discussed procedures can be run in parallel which has the potential of
drastically reducing the problem size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05877</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05877</id><created>2015-07-21</created><authors><author><keyname>De Angelis</keyname><forenames>Emanuele</forenames><affiliation>DEC, University G. d'Annunzio, Pescara, Italy</affiliation></author><author><keyname>Fioravanti</keyname><forenames>Fabio</forenames><affiliation>DEC, University G. d'Annunzio, Pescara, Italy</affiliation></author><author><keyname>Pettorossi</keyname><forenames>Alberto</forenames><affiliation>DICII, Universita' di Roma Tor Vergata, Roma, Italy</affiliation></author><author><keyname>Proietti</keyname><forenames>Maurizio</forenames><affiliation>CNR-IASI, Roma, Italy</affiliation></author></authors><title>Proving Correctness of Imperative Programs by Linearizing Constrained
  Horn Clauses</title><categories>cs.LO cs.PL</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP),
  Proceedings of ICLP 2015</comments><journal-ref>Theory and Practice of Logic Programming 15 (2015) 635-650</journal-ref><doi>10.1017/S1471068415000289</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for verifying the correctness of imperative programs
which is based on the automated transformation of their specifications. Given a
program prog, we consider a partial correctness specification of the form
$\{\varphi\}$ prog $\{\psi\}$, where the assertions $\varphi$ and $\psi$ are
predicates defined by a set Spec of possibly recursive Horn clauses with linear
arithmetic (LA) constraints in their premise (also called constrained Horn
clauses). The verification method consists in constructing a set PC of
constrained Horn clauses whose satisfiability implies that $\{\varphi\}$ prog
$\{\psi\}$ is valid. We highlight some limitations of state-of-the-art
constrained Horn clause solving methods, here called LA-solving methods, which
prove the satisfiability of the clauses by looking for linear arithmetic
interpretations of the predicates. In particular, we prove that there exist
some specifications that cannot be proved valid by any of those LA-solving
methods. These specifications require the proof of satisfiability of a set PC
of constrained Horn clauses that contain nonlinear clauses (that is, clauses
with more than one atom in their premise). Then, we present a transformation,
called linearization, that converts PC into a set of linear clauses (that is,
clauses with at most one atom in their premise). We show that several
specifications that could not be proved valid by LA-solving methods, can be
proved valid after linearization. We also present a strategy for performing
linearization in an automatic way and we report on some experimental results
obtained by using a preliminary implementation of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05880</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05880</id><created>2015-07-21</created><authors><author><keyname>Vural</keyname><forenames>Elif</forenames></author><author><keyname>Guillemot</keyname><forenames>Christine</forenames></author></authors><title>A study of the classification of low-dimensional data with supervised
  manifold learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervised manifold learning methods learn data representations by preserving
the geometric structure of data while enhancing the separation between data
samples from different classes. In this paper, we propose a theoretical study
of supervised manifold learning for classification. We first focus on the
supervised Laplacian eigenmaps algorithm and study the conditions under which
this method computes low-dimensional embeddings where different classes become
linearly separable. We then consider arbitrary supervised manifold learning
algorithms that compute a linearly separable embedding and study the accuracy
of the classifiers given by the out-of-sample extensions of these embeddings.
We characterize the classification accuracy in terms of several parameters of
the classifier such as the separation between different classes in the
embedding, the regularity of the interpolation function and the number of
training samples. The proposed analysis is supported by experiments on
synthetic and real data and has potential for guiding the design of classifiers
for intrinsically low-dimensional data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05881</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05881</id><created>2015-07-21</created><authors><author><keyname>Estrada</keyname><forenames>Ernesto</forenames></author><author><keyname>Vargas-Estrada</keyname><forenames>Eusebio</forenames></author><author><keyname>Ando</keyname><forenames>Hiroyasu</forenames></author></authors><title>Communicability Angles Reveal Critical Edges for Network Consensus
  Dynamics</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages, 2 figures</comments><doi>10.1103/PhysRevE.92.052809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the question of determining how the topological structure
influences a consensus dynamical process taking place on a network. By
considering a large dataset of real-world networks we first determine that the
removal of edges according to their communicability angle -an angle between
position vectors of the nodes in an Euclidean communicability space- increases
the average time of consensus by a factor of 5.68 in real-world networks. The
edge betweenness centrality also identifies -in a smaller proportion- those
critical edges for the consensus dynamics, i.e., its removal increases the time
of consensus by a factor of 3.70. We justify theoretically these findings on
the basis of the role played by the algebraic connectivity and the
isoperimetric number of networks on the dynamical process studied, and their
connections with the properties mentioned before. Finally, we study the role
played by global topological parameters of networks on the consensus dynamics.
We determine that the network density and the average distance-sum -an
analogous of the node degree for shortest-path distances, account for more than
80% of the variance of the average time of consensus in the real-world networks
studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05888</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05888</id><created>2015-07-21</created><authors><author><keyname>Gahlawat</keyname><forenames>Aditya</forenames></author><author><keyname>Peet</keyname><forenames>Matthew M.</forenames></author></authors><title>A Convex Approach to Analysis, State and Output Feedback Control
  Parabolic PDEs Using Sum-of-Squares</title><categories>cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1408.5206</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we use optimization-based methods to analyze the stability and
design state and output-feedback controllers for a class of one-dimensional
parabolic partial differential equations. The output may be the complete state
measurement or the boundary measurement of the state. The input considered is
Neumann boundary actuation. We use Lyapunov operators, duality, and the
Luenberger observer framework to reformulate the synthesis problem as a convex
optimization problem expressed as a set of Linear-Operator-Inequalities (LOIs).
We then show how feasibility of these LOIs may be tested using Semidefinite
Programming (SDP) and the Sum-of-Squares methodology. Moreover, we provide
numerical results which prove that the method can be generalized for
application to systems with other types of boundary conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05890</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05890</id><created>2015-07-21</created><updated>2015-07-24</updated><authors><author><keyname>Jansen</keyname><forenames>Bart M. P.</forenames></author></authors><title>On Structural Parameterizations of Hitting Set: Hitting Paths in Graphs
  Using 2-SAT</title><categories>cs.DS</categories><comments>Presented at the 41st International Workshop on Graph-Theoretic
  Concepts in Computer Science, WG 2015. (The statement of Lemma 4 was
  corrected in this update.)</comments><msc-class>05C85, 68Q25</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hitting Set is a classic problem in combinatorial optimization. Its input
consists of a set system F over a finite universe U and an integer t; the
question is whether there is a set of t elements that intersects every set in
F. The Hitting Set problem parameterized by the size of the solution is a
well-known W[2]-complete problem in parameterized complexity theory. In this
paper we investigate the complexity of Hitting Set under various structural
parameterizations of the input. Our starting point is the folklore result that
Hitting Set is polynomial-time solvable if there is a tree T on vertex set U
such that the sets in F induce connected subtrees of T. We consider the case
that there is a treelike graph with vertex set U such that the sets in F induce
connected subgraphs; the parameter of the problem is a measure of how treelike
the graph is. Our main positive result is an algorithm that, given a graph G
with cyclomatic number k, a collection P of simple paths in G, and an integer
t, determines in time 2^{5k} (|G| +|P|)^O(1) whether there is a vertex set of
size t that hits all paths in P. It is based on a connection to the 2-SAT
problem in multiple valued logic. For other parameterizations we derive
W[1]-hardness and para-NP-completeness results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05895</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05895</id><created>2015-07-21</created><authors><author><keyname>Kim</keyname><forenames>Song-Ju</forenames></author><author><keyname>Tsuruoka</keyname><forenames>Tohru</forenames></author><author><keyname>Hasegawa</keyname><forenames>Tsuyoshi</forenames></author><author><keyname>Aono</keyname><forenames>Masakazu</forenames></author></authors><title>Decision Maker based on Atomic Switches</title><categories>cs.AI cond-mat.mtrl-sci</categories><comments>10 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:1412.6141</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple model for an atomic switch-based decision maker (ASDM),
and show that, as long as its total volume of precipitated Ag atoms is
conserved when coupled with suitable operations, an atomic switch system
provides a sophisticated &quot;decision-making&quot; capability that is known to be one
of the most important intellectual abilities in human beings. We considered the
multi-armed bandit problem (MAB); the problem of finding, as accurately and
quickly as possible, the most profitable option from a set of options that
gives stochastic rewards. These decisions are made as dictated by each volume
of precipitated Ag atoms, which is moved in a manner similar to the
fluctuations of a rigid body in a tug-of-war game. The &quot;tug-of-war (TOW)
dynamics&quot; of the ASDM exhibits higher efficiency than conventional MAB solvers.
We show analytical calculations that validate the statistical reasons for the
ASDM dynamics to produce such high performance, despite its simplicity. These
results imply that various physical systems, in which some conservation law
holds, can be used to implement efficient &quot;decision-making objects.&quot; Efficient
MAB solvers are useful for many practical applications, because MAB abstracts a
variety of decision-making problems in real- world situations where an
efficient trial-and-error is required. The proposed scheme will introduce a new
physics-based analog computing paradigm, which will include such things as
&quot;intelligent nano devices&quot; and &quot;intelligent information networks&quot; based on
self-detection and self-judgment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05910</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05910</id><created>2015-07-21</created><updated>2015-11-29</updated><authors><author><keyname>Auvolat</keyname><forenames>Alex</forenames></author><author><keyname>Chandar</keyname><forenames>Sarath</forenames></author><author><keyname>Vincent</keyname><forenames>Pascal</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Clustering is Efficient for Approximate Maximum Inner Product Search</title><categories>cs.LG cs.CL stat.ML</categories><comments>10 pages, Under review at ICLR 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient Maximum Inner Product Search (MIPS) is an important task that has a
wide applicability in recommendation systems and classification with a large
number of classes. Solutions based on locality-sensitive hashing (LSH) as well
as tree-based solutions have been investigated in the recent literature, to
perform approximate MIPS in sublinear time. In this paper, we compare these to
another extremely simple approach for solving approximate MIPS, based on
variants of the k-means clustering algorithm. Specifically, we propose to train
a spherical k-means, after having reduced the MIPS problem to a Maximum Cosine
Similarity Search (MCSS). Experiments on two standard recommendation system
benchmarks as well as on large vocabulary word embeddings, show that this
simple approach yields much higher speedups, for the same retrieval precision,
than current state-of-the-art hashing-based and tree-based methods. This simple
method also yields more robust retrievals when the query is corrupted by noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05920</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05920</id><created>2015-07-21</created><authors><author><keyname>Joshi</keyname><forenames>Saurabh</forenames></author><author><keyname>Martins</keyname><forenames>Ruben</forenames></author><author><keyname>Manquinho</keyname><forenames>Vasco</forenames></author></authors><title>Generalized Totalizer Encoding for Pseudo-Boolean Constraints</title><categories>cs.LO cs.AI</categories><comments>10 pages, 2 figures, 2 tables. To be published in 21st International
  Conference on Principles and Practice of Constraint Programming 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pseudo-Boolean constraints, also known as 0-1 Integer Linear Constraints, are
used to model many real-world problems. A common approach to solve these
constraints is to encode them into a SAT formula. The runtime of the SAT solver
on such formula is sensitive to the manner in which the given pseudo-Boolean
constraints are encoded. In this paper, we propose generalized Totalizer
encoding (GTE), which is an arc-consistency preserving extension of the
Totalizer encoding to pseudo-Boolean constraints. Unlike some other encodings,
the number of auxiliary variables required for GTE does not depend on the
magnitudes of the coefficients. Instead, it depends on the number of distinct
combinations of these coefficients. We show the superiority of GTE with respect
to other encodings when large pseudo-Boolean constraints have low number of
distinct coefficients. Our experimental results also show that GTE remains
competitive even when the pseudo-Boolean constraints do not have this
characteristic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05924</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05924</id><created>2015-07-21</created><authors><author><keyname>Angjelichinoski</keyname><forenames>Marko</forenames></author><author><keyname>Stefanovic</keyname><forenames>Cedomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Liu</keyname><forenames>Hongpeng</forenames></author><author><keyname>Loh</keyname><forenames>Poh Chiang</forenames></author><author><keyname>Blaabjerg</keyname><forenames>Frede</forenames></author></authors><title>Multiuser Communication through Power Talk in DC MicroGrids</title><categories>cs.IT math.IT</categories><comments>Multiuser extension of the power talk concept. Submitted to IEEE JSAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power talk is a novel concept for communication among control units in
MicroGrids (MGs), carried out without a dedicated modem, but by using power
electronics that interface the common bus. The information is transmitted by
modulating the parameters of the primary control, incurring subtle power
deviations that can be detected by other units. In this paper, we develop power
talk communication strategies for DC MG systems with arbitrary number of
control units that carry out all-to-all communication. We investigate two
multiple access strategies: 1) TDMA, where only one unit transmits at a time,
and 2) full duplex, where all units transmit and receive simultaneously. We
introduce the notions of signaling space, where the power talk symbol
constellations are constructed, and detection space, where the demodulation of
the symbols is performed. The proposed communication technique is challenged by
the random changes of the bus parameters due to load variations in the system.
To this end, we employ a solution based on training sequences, which
re-establishes the signaling and detection spaces and thus enables reliable
information exchange. The presented results show that power talk is an
effective solution for reliable communication among units in DC MG systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05929</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05929</id><created>2015-07-21</created><authors><author><keyname>Donaldson</keyname><forenames>Roger</forenames></author><author><keyname>Gupta</keyname><forenames>Arijit</forenames></author><author><keyname>Plan</keyname><forenames>Yaniv</forenames></author><author><keyname>Reimer</keyname><forenames>Thomas</forenames></author></authors><title>Random mappings designed for commercial search engines</title><categories>cs.IR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a practical random mapping that takes any set of documents
represented as vectors in Euclidean space and then maps them to a sparse subset
of the Hamming cube while retaining ordering of inter-vector inner products.
Once represented in the sparse space, it is natural to index documents using
commercial text-based search engines which are specialized to take advantage of
this sparse and discrete structure for large-scale document retrieval. We give
a theoretical analysis of the mapping scheme, characterizing exact asymptotic
behavior and also giving non-asymptotic bounds which we verify through
numerical simulations. We balance the theoretical treatment with several
practical considerations; these allow substantial speed up of the method. We
further illustrate the use of this method on search over two real data sets: a
corpus of images represented by their color histograms, and a corpus of daily
stock market index values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05936</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05936</id><created>2015-07-21</created><authors><author><keyname>Park</keyname><forenames>Se Rim</forenames></author><author><keyname>Kolouri</keyname><forenames>Soheil</forenames></author><author><keyname>Kundu</keyname><forenames>Shinjini</forenames></author><author><keyname>Rohde</keyname><forenames>Gustavo</forenames></author></authors><title>The Cumulative Distribution Transform and Linear Pattern Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classifying (determining the label) of data emanating from sensors is an
important problem with many applications in science and technology. We describe
a new transform for patterns that can be interpreted as a probability density
function, that has special properties with regards to classification. The
transform, which we denote as the Cumulative Distribution Transform (CDT) is
invertible, with well defined forward and inverse operations. We show that it
can be useful in 'parsing out' variations (confounds) that are 'Lagrangian'
(displacement or transport) by converting these to 'Eulerian' variations in
transform domain. This conversion is the basis for our main result that
describes when the CDT can allow for linear classification to be possible in
signal domain. We also describe several properties of the transform and show,
with computational experiments that used both real and simulated data, that the
CDT can help render a variety of real world problems simpler to solve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05941</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05941</id><created>2015-07-21</created><authors><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros</forenames></author><author><keyname>Dolecek</keyname><forenames>Lara</forenames></author><author><keyname>Effros</keyname><forenames>Michelle</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author><author><keyname>Yeh</keyname><forenames>Edmund</forenames></author><author><keyname>Berry</keyname><forenames>Randall</forenames></author><author><keyname>Duffy</keyname><forenames>Ken</forenames></author><author><keyname>Feizi</keyname><forenames>Soheil</forenames></author><author><keyname>Kato</keyname><forenames>Saul</forenames></author><author><keyname>Kellis</keyname><forenames>Manolis</forenames></author><author><keyname>Licht</keyname><forenames>Stuart</forenames></author><author><keyname>Sorenson</keyname><forenames>Jon</forenames></author><author><keyname>Varshney</keyname><forenames>Lav</forenames></author><author><keyname>Vikalo</keyname><forenames>Haris</forenames></author></authors><title>A Perspective on Future Research Directions in Information Theory</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information theory is rapidly approaching its 70th birthday. What are
promising future directions for research in information theory? Where will
information theory be having the most impact in 10-20 years? What new and
emerging areas are ripe for the most impact, of the sort that information
theory has had on the telecommunications industry over the last 60 years? How
should the IEEE Information Theory Society promote high-risk new research
directions and broaden the reach of information theory, while continuing to be
true to its ideals and insisting on the intellectual rigor that makes its
breakthroughs so powerful? These are some of the questions that an ad hoc
committee (composed of the present authors) explored over the past two years.
We have discussed and debated these questions, and solicited detailed inputs
from experts in fields including genomics, biology, economics, and
neuroscience. This report is the result of these discussions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05944</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05944</id><created>2015-07-21</created><updated>2015-11-03</updated><authors><author><keyname>Kejlberg-Rasmussen</keyname><forenames>Casper</forenames></author><author><keyname>Kopelowitz</keyname><forenames>Tsvi</forenames></author><author><keyname>Pettie</keyname><forenames>Seth</forenames></author><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>Faster Worst Case Deterministic Dynamic Connectivity</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deterministic dynamic connectivity data structure for undirected
graphs with worst case update time $O\left(\sqrt{\frac{n(\log\log n)^2}{\log
n}}\right)$ and constant query time. This improves on the previous best
deterministic worst case algorithm of Frederickson (STOC 1983) and Eppstein
Galil, Italiano, and Nissenzweig (J. ACM 1997), which had update time
$O(\sqrt{n})$. All other algorithms for dynamic connectivity are either
randomized (Monte Carlo) or have only amortized performance guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05946</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05946</id><created>2015-07-21</created><updated>2015-08-01</updated><authors><author><keyname>Pinciroli</keyname><forenames>Carlo</forenames></author><author><keyname>Lee-Brown</keyname><forenames>Adam</forenames></author><author><keyname>Beltrame</keyname><forenames>Giovanni</forenames></author></authors><title>Buzz: An Extensible Programming Language for Self-Organizing
  Heterogeneous Robot Swarms</title><categories>cs.RO cs.MA cs.PL cs.SE</categories><comments>12 pages, 4 figures, submitted to IEEE Transactions on Robotics</comments><acm-class>I.2.9; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Buzz, a novel programming language for heterogeneous robot swarms.
Buzz advocates a compositional approach, offering primitives to define swarm
behaviors both from the perspective of the single robot and of the overall
swarm. Single-robot primitives include robot-specific instructions and
manipulation of neighborhood data. Swarm-based primitives allow for the dynamic
management of robot teams, and for sharing information globally across the
swarm. Self-organization stems from the completely decentralized mechanisms
upon which the Buzz run-time platform is based. The language can be extended to
add new primitives (thus supporting heterogeneous robot swarms), and its
run-time platform is designed to be laid on top of other frameworks, such as
Robot Operating System. We showcase the capabilities of Buzz by providing code
examples, and analyze scalability and robustness of the run-time platform
through realistic simulated experiments with representative swarm algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05950</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05950</id><created>2015-07-21</created><authors><author><keyname>Chan</keyname><forenames>Siu On</forenames></author><author><keyname>Papailiopoulos</keyname><forenames>Dimitris</forenames></author><author><keyname>Rubinstein</keyname><forenames>Aviad</forenames></author></authors><title>On the Worst-Case Approximability of Sparse PCA</title><categories>stat.ML cs.CC cs.DS cs.LG</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that Sparse PCA (Sparse Principal Component Analysis) is
NP-hard to solve exactly on worst-case instances. What is the complexity of
solving Sparse PCA approximately? Our contributions include: 1) a simple and
efficient algorithm that achieves an $n^{-1/3}$-approximation; 2) NP-hardness
of approximation to within $(1-\varepsilon)$, for some small constant
$\varepsilon &gt; 0$; 3) SSE-hardness of approximation to within any constant
factor; and 4) an $\exp\exp\left(\Omega\left(\sqrt{\log \log n}\right)\right)$
(&quot;quasi-quasi-polynomial&quot;) gap for the standard semidefinite program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05952</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05952</id><created>2015-07-21</created><updated>2015-12-08</updated><authors><author><keyname>Acharya</keyname><forenames>Jayadev</forenames></author><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Kamath</keyname><forenames>Gautam</forenames></author></authors><title>Optimal Testing for Properties of Distributions</title><categories>cs.DS cs.IT cs.LG math.IT math.ST stat.TH</categories><comments>31 pages, extended abstract appeared as a spotlight in NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given samples from an unknown distribution $p$, is it possible to distinguish
whether $p$ belongs to some class of distributions $\mathcal{C}$ versus $p$
being far from every distribution in $\mathcal{C}$? This fundamental question
has received tremendous attention in statistics, focusing primarily on
asymptotic analysis, and more recently in information theory and theoretical
computer science, where the emphasis has been on small sample size and
computational complexity. Nevertheless, even for basic properties of
distributions such as monotonicity, log-concavity, unimodality, independence,
and monotone-hazard rate, the optimal sample complexity is unknown.
  We provide a general approach via which we obtain sample-optimal and
computationally efficient testers for all these distribution families. At the
core of our approach is an algorithm which solves the following problem: Given
samples from an unknown distribution $p$, and a known distribution $q$, are $p$
and $q$ close in $\chi^2$-distance, or far in total variation distance?
  The optimality of our testers is established by providing matching lower
bounds with respect to both $n$ and $\varepsilon$. Finally, a necessary
building block for our testers and an important byproduct of our work are the
first known computationally efficient proper learners for discrete log-concave
and monotone hazard rate distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05955</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05955</id><created>2015-07-21</created><authors><author><keyname>Johnson</keyname><forenames>Richard A. B.</forenames></author><author><keyname>Meszaros</keyname><forenames>Gabor</forenames></author></authors><title>Sorting using non-binary comparisons</title><categories>math.CO cs.DS</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the problem of sorting a set of $n$ coins, each
with distinct but unknown weights, using an unusual scale. The classical
version of this problem, which has been well-studied, gives the user a binary
scale, enabling them to determine which is the lighter/heavier of any two
objects. We generalise this, considering a scale that accepts $k$ coins as
input and returns the $t^{\text{th}}$ lightest, for a fixed $k$ and $t$. We
consider this in both an on-line and off-line setting, and exhibit algorithms
in both settings that are best-possible in terms of the order of the number of
queries required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05956</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05956</id><created>2015-07-21</created><updated>2015-12-07</updated><authors><author><keyname>Lynch</keyname><forenames>Thomas W.</forenames></author></authors><title>Towards a Better Understanding of CAR, CDR, CADR and the Others</title><categories>cs.AI cs.PL</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article explains in detail the origin the function names CAR and CDR.
The article suggests that CAR and CDR are the basis of an access language for
retrieving elements from tree containers. It provides some simple rules for
reading the condensed forms of repeated CAR and CDR, e.g. CADR. This technique
might be especially useful for students who are learning LISP. The article also
suggests some extensions for this LISP tree access language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05964</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05964</id><created>2015-07-21</created><authors><author><keyname>Naumov</keyname><forenames>Pavel G.</forenames></author><author><keyname>Tao</keyname><forenames>Jia</forenames></author></authors><title>The Budget-Constrained Functional Dependency</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Armstrong's axioms of functional dependency form a well-known logical system
that captures properties of functional dependencies between sets of database
attributes. This article assumes that there are costs associated with
attributes and proposes an extension of Armstrong's system for reasoning about
budget-constrained functional dependencies in such a setting.
  The main technical result of this article is the completeness theorem for the
proposed logical system. Although the proposed axioms are obtained by just
adding cost subscript to the original Armstrong's axioms, the proof of the
completeness for the proposed system is significantly more complicated than
that for the Armstrong's system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05980</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05980</id><created>2015-07-21</created><authors><author><keyname>Borradaile</keyname><forenames>Glencora</forenames></author><author><keyname>Nayyeri</keyname><forenames>Amir</forenames></author><author><keyname>Zafarani</keyname><forenames>Farzad</forenames></author></authors><title>Towards single face shortest vertex-disjoint paths in undirected planar
  graphs</title><categories>cs.DS</categories><comments>This is a preliminary version of a paper that will appear in
  Proceedings of the 23rd European Symposium on Algorithms (ESA 2015), Patras,
  Greece</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $k$ pairs of terminals $\{(s_{1}, t_{1}), \ldots, (s_{k}, t_{k})\}$ in
a graph $G$, the min-sum $k$ vertex-disjoint paths problem is to find a
collection $\{Q_{1}, Q_{2}, \ldots, Q_{k}\}$ of vertex-disjoint paths with
minimum total length, where $Q_{i}$ is an $s_i$-to-$t_i$ path between $s_i$ and
$t_i$. We consider the problem in planar graphs, where little is known about
computational tractability, even in restricted cases. Kobayashi and Sommer
propose a polynomial-time algorithm for $k \le 3$ in undirected planar graphs
assuming all terminals are adjacent to at most two faces. Colin de Verdiere and
Schrijver give a polynomial-time algorithm when all the sources are on the
boundary of one face and all the sinks are on the boundary of another face and
ask about the existence of a polynomial-time algorithm provided all terminals
are on a common face. We make progress toward Colin de Verdiere and Schrijver's
open question by giving an $O(kn^5)$ time algorithm for undirected planar
graphs when $\{(s_{1}, t_{1}), \ldots, (s_{k}, t_{k})\}$ are in
counter-clockwise order on a common face.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05986</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05986</id><created>2015-07-21</created><updated>2015-07-23</updated><authors><author><keyname>Stulova</keyname><forenames>Nataliia</forenames></author><author><keyname>Morales</keyname><forenames>Jos&#xe9; F.</forenames></author><author><keyname>Hermenegildo</keyname><forenames>Manuel V.</forenames></author></authors><title>Practical Run-time Checking via Unobtrusive Property Caching</title><categories>cs.PL</categories><comments>30 pages, 1 table, 170 figures; added appendix with plots; To appear
  in Theory and Practice of Logic Programming (TPLP), Proceedings of ICLP 2015</comments><acm-class>D.1.6; D.2.4; F.3.1; F.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming 15 (2015) 726-741</journal-ref><doi>10.1017/S1471068415000344</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of annotations, referred to as assertions or contracts, to describe
program properties for which run-time tests are to be generated, has become
frequent in dynamic programing languages. However, the frameworks proposed to
support such run-time testing generally incur high time and/or space overheads
over standard program execution. We present an approach for reducing this
overhead that is based on the use of memoization to cache intermediate results
of check evaluation, avoiding repeated checking of previously verified
properties. Compared to approaches that reduce checking frequency, our proposal
has the advantage of being exhaustive (i.e., all tests are checked at all
points) while still being much more efficient than standard run-time checking.
Compared to the limited previous work on memoization, it performs the task
without requiring modifications to data structure representation or checking
code. While the approach is general and system-independent, we present it for
concreteness in the context of the Ciao run-time checking framework, which
allows us to provide an operational semantics with checks and caching. We also
report on a prototype implementation and provide some experimental results that
support that using a relatively small cache leads to significant decreases in
run-time checking overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05989</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05989</id><created>2015-07-21</created><authors><author><keyname>Martinez</keyname><forenames>Victor R.</forenames></author><author><keyname>Mancilla</keyname><forenames>Antonio</forenames></author><author><keyname>Gonzalez</keyname><forenames>Victor M.</forenames></author></authors><title>Tweeting Over The Border: An Empirical Study of Transnational Migration
  in San Diego and Tijuana</title><categories>cs.SI</categories><comments>11 pages, 6 figures, submitted</comments><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sociological studies on transnational migration are often based on surveys or
interviews, an expensive and time consuming approach. On the other hand, the
pervasiveness of mobile phones and location aware social networks has
introduced new ways to understand human mobility patterns at a national or
global scale. In this work, we leverage geo located information obtained from
Twitter as to understand transnational migration patterns between two border
cities (San Diego, USA and Tijuana, Mexico). We obtained 10.9 million geo
located tweets from December 2013 to January 2015. Our method infers human
mobility by inspecting tweet submissions and user's home locations. Our results
depict a trans national community structure that exhibits the formation of a
functional metropolitan area that physically transcends international borders.
These results show the potential for re analysing sociology phenomena from a
technology based empirical perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05991</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05991</id><created>2015-07-10</created><authors><author><keyname>Sreram</keyname><forenames>B.</forenames></author><author><keyname>Srinivasan</keyname><forenames>Seshadhri</forenames></author><author><keyname>Subathra</keyname><forenames>B.</forenames></author><author><keyname>Ramaswamy</keyname><forenames>Srini</forenames></author></authors><title>Design Contracts For Networked Automation Systems Co-design</title><categories>cs.SY</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networked automation systems (NAS) are characterized by confluence of
control, computation, communication and Information (C3I) technologies. Design
decisions of one domain are affected by the constraints posed by others.
Reliable NAS design should address the requirements of the system, and
simultaneously meet the constraints posed by other domains and this is called
co-design in literature. Co-design requires clear definition of interfaces
among these domains. Control design in NAS is affected by the timing
imperfections posed by other domains. In this investigation, we first study the
different sources of timing imperfections in NAS, and classify them based on
their occurrence. The concept of jitter is used to define the timing
imperfections induced by various system components. Using this analysis, we
classify the jitter based on their behavior and domain of occurrence. Our
analysis shows that the jitter induced in NAS can be classified based on domain
as- hardware, software and communication. Next, we use this analysis to model
the jitter from the components of NAS. Modeling timing imperfections helps in
capturing the interfaces among the domains, and we use the concept of design
contracts to capture the interfaces. Design contracts describe the semantic
mapping among the domains and are specified using the jitter margins.
Implementing design contracts requires knowledge of the jitter margin and, the
results from control theory are used to this extent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05994</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05994</id><created>2015-07-21</created><authors><author><keyname>Gao</keyname><forenames>Xiang</forenames></author><author><keyname>Edfors</keyname><forenames>Ove</forenames></author><author><keyname>Tufvesson</keyname><forenames>Fredrik</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Massive MIMO in Real Propagation Environments: Do All Antennas
  Contribute Equally?</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO can greatly increase both spectral and transmit-energy
efficiency. This is achieved by allowing the number of antennas and RF chains
to grow very large. However, the challenges include high system complexity and
hardware energy consumption. Here we investigate the possibilities to reduce
the required number of RF chains, by performing antenna selection. While this
approach is not a very effective strategy for theoretical independent Rayleigh
fading channels, a substantial reduction in the number of RF chains can be
achieved for real massive MIMO channels, without significant performance loss.
We evaluate antenna selection performance on measured channels at 2.6 GHz,
using a linear and a cylindrical array, both having 128 elements. Sum-rate
maximization is used as the criterion for antenna selection. A selection scheme
based on convex optimization is nearly optimal and used as a benchmark. The
achieved sum-rate is compared with that of a very simple scheme that selects
the antennas with the highest received power. The power-based scheme gives
performance close to the convex optimization scheme, for the measured channels.
This observation indicates a potential for significant reductions of massive
MIMO implementation complexity, by reducing the number of RF chains and
performing antenna selection using simple algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05995</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05995</id><created>2015-07-17</created><authors><author><keyname>Tang</keyname><forenames>Yuchao</forenames></author><author><keyname>Wu</keyname><forenames>Zhenggang</forenames></author><author><keyname>Zhu</keyname><forenames>Chuanxi</forenames></author></authors><title>An improved strategy for solving Sudoku by sparse optimization methods</title><categories>math.OC cs.DC</categories><comments>11 pages,5 figures</comments><msc-class>90C05, 90C25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We proposed several strategies to improve the sparse optimization methods for
solving Sudoku puzzles. Further, we defined a new difficult level for Sudoku.
We tested our proposed methods on Sudoku puzzles data-set. Numerical results
showed that we can improve the accurate recovery rate from 84%+ to 99%+ by the
L1 sparse optimization method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05998</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05998</id><created>2015-07-21</created><updated>2015-11-03</updated><authors><author><keyname>Banerjee</keyname><forenames>Siddhartha</forenames></author><author><keyname>Lofgren</keyname><forenames>Peter</forenames></author></authors><title>Fast Bidirectional Probability Estimation in Markov Models</title><categories>cs.DS</categories><comments>NIPS 2015</comments><acm-class>G.2.2; F.2.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new bidirectional algorithm for estimating Markov chain
multi-step transition probabilities: given a Markov chain, we want to estimate
the probability of hitting a given target state in $\ell$ steps after starting
from a given source distribution. Given the target state $t$, we use a
(reverse) local power iteration to construct an `expanded target distribution',
which has the same mean as the quantity we want to estimate, but a smaller
variance -- this can then be sampled efficiently by a Monte Carlo algorithm.
Our method extends to any Markov chain on a discrete (finite or countable)
state-space, and can be extended to compute functions of multi-step transition
probabilities such as PageRank, graph diffusions, hitting/return times, etc.
Our main result is that in `sparse' Markov Chains -- wherein the number of
transitions between states is comparable to the number of states -- the running
time of our algorithm for a uniform-random target node is order-wise smaller
than Monte Carlo and power iteration based algorithms; in particular, our
method can estimate a probability $p$ using only $O(1/\sqrt{p})$ running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.05999</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.05999</id><created>2015-07-21</created><updated>2015-12-14</updated><authors><author><keyname>Lofgren</keyname><forenames>Peter</forenames></author><author><keyname>Banerjee</keyname><forenames>Siddhartha</forenames></author><author><keyname>Goel</keyname><forenames>Ashish</forenames></author></authors><title>Personalized PageRank Estimation and Search: A Bidirectional Approach</title><categories>cs.DS cs.IR cs.SI</categories><comments>WSDM 2016</comments><acm-class>H.3.3; G.2.2</acm-class><doi>10.1145/2835776.2835823</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new algorithms for Personalized PageRank estimation and
Personalized PageRank search. First, for the problem of estimating Personalized
PageRank (PPR) from a source distribution to a target node, we present a new
bidirectional estimator with simple yet strong guarantees on correctness and
performance, and 3x to 8x speedup over existing estimators in experiments on a
diverse set of networks. Moreover, it has a clean algebraic structure which
enables it to be used as a primitive for the Personalized PageRank Search
problem: Given a network like Facebook, a query like &quot;people named John&quot;, and a
searching user, return the top nodes in the network ranked by PPR from the
perspective of the searching user. Previous solutions either score all nodes or
score candidate nodes one at a time, which is prohibitively slow for large
candidate sets. We develop a new algorithm based on our bidirectional PPR
estimator which identifies the most relevant results by sampling candidates
based on their PPR; this is the first solution to PPR search that can find the
best results without iterating through the set of all candidate results.
Finally, by combining PPR sampling with sequential PPR estimation and Monte
Carlo, we develop practical algorithms for PPR search, and we show via
experiments that our algorithms are efficient on networks with billions of
edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06000</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06000</id><created>2015-07-10</created><authors><author><keyname>Shamsi</keyname><forenames>Pourya</forenames></author></authors><title>On-line Survival Analysis of Power Electronic Converters Using Step
  Noise-Cox Processes</title><categories>cs.SY</categories><comments>This was written as a Power Electronic article. Hence, mathematical
  proofs are minimal and the introduction part covers a variety of basic
  information</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  This paper is focused on survival analysis of electrical components. The main
goal of this paper is to develop a method for on-line estimation of the Mean
Time To Failure (MTTF) of electrical components under dynamic stress levels.
The proposed method models the variations of the stress levels as a stochastic
process. Hence, a stochastic failure rate function can be developed for each
electrical component. Later, this function is used as the underlying rate of a
doubly stochastic Poisson process (known as Cox processes). Furthermore, this
Cox process is used for on-line estimation of the Mean Residual Life (MRL)
using the observed stress levels. The proposed method provides a good estimate
of the age and life expectancy of each component. An experimental case study is
provided to demonstrate the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06010</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06010</id><created>2015-07-21</created><authors><author><keyname>Johann</keyname><forenames>P.</forenames></author><author><keyname>Komendantskaya</keyname><forenames>E.</forenames></author><author><keyname>Komendantskiy</keyname><forenames>V.</forenames></author></authors><title>Structural Resolution for Logic Programming</title><categories>cs.LO</categories><comments>in ICLP 2015 Technical Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a Three Tier Tree Calculus (3TC) that defines in a systematic
way three tiers of tree structures underlying proof search in logic
programming. We use 3TC to define a new -- structural -- version of resolution
for logic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06011</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06011</id><created>2015-07-21</created><authors><author><keyname>Donovan</keyname><forenames>Brian</forenames></author><author><keyname>Work</keyname><forenames>Daniel B.</forenames></author></authors><title>Using coarse GPS data to quantify city-scale transportation system
  resilience to extreme events</title><categories>physics.soc-ph cs.SI</categories><comments>presented at the 2015 Transportation Research Board Annual Meeting,
  paper number 15-5465</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes a method to quantitatively measure the resilience of
transportation systems using GPS data from taxis. The granularity of the GPS
data necessary for this analysis is relatively coarse; it only requires
coordinates for the beginning and end of trips, the metered distance, and the
total travel time. The method works by computing the historical distribution of
pace (normalized travel times) between various regions of a city and measuring
the pace deviations during an unusual event. This method is applied to a
dataset of nearly 700 million taxi trips in New York City, which is used to
analyze the transportation infrastructure resilience to Hurricane Sandy. The
analysis indicates that Hurricane Sandy impacted traffic conditions for more
than five days, and caused a peak delay of two minutes per mile. Practically,
it identifies that the evacuation caused only minor disruptions, but
significant delays were encountered during the post-disaster reentry process.
Since the implementation of this method is very efficient, it could potentially
be used as an online monitoring tool, representing a first step toward
quantifying city scale resilience with coarse GPS data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06020</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06020</id><created>2015-07-21</created><authors><author><keyname>Amami</keyname><forenames>Rimah</forenames></author><author><keyname>Ayed</keyname><forenames>Dorra Ben</forenames></author><author><keyname>Ellouze</keyname><forenames>Noureddine</forenames></author></authors><title>Practical Selection of SVM Supervised Parameters with Different Feature
  Representations for Vowel Recognition</title><categories>cs.CL cs.LG</categories><comments>07 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that the classification performance of Support Vector Machine
(SVM) can be conveniently affected by the different parameters of the kernel
tricks and the regularization parameter, C. Thus, in this article, we propose a
study in order to find the suitable kernel with which SVM may achieve good
generalization performance as well as the parameters to use. We need to analyze
the behavior of the SVM classifier when these parameters take very small or
very large values. The study is conducted for a multi-class vowel recognition
using the TIMIT corpus. Furthermore, for the experiments, we used different
feature representations such as MFCC and PLP. Finally, a comparative study was
done to point out the impact of the choice of the parameters, kernel trick and
feature representations on the performance of the SVM classifier
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06021</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06021</id><created>2015-07-21</created><authors><author><keyname>Amami</keyname><forenames>Rimah</forenames></author><author><keyname>Ayed</keyname><forenames>Dorra Ben</forenames></author><author><keyname>Ellouze</keyname><forenames>Noureddine</forenames></author></authors><title>An Empirical Comparison of SVM and Some Supervised Learning Algorithms
  for Vowel recognition</title><categories>cs.CL cs.LG</categories><comments>08 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we conduct a study on the performance of some supervised
learning algorithms for vowel recognition. This study aims to compare the
accuracy of each algorithm. Thus, we present an empirical comparison between
five supervised learning classifiers and two combined classifiers: SVM, KNN,
Naive Bayes, Quadratic Bayes Normal (QDC) and Nearst Mean. Those algorithms
were tested for vowel recognition using TIMIT Corpus and Mel-frequency cepstral
coefficients (MFCCs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06023</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06023</id><created>2015-07-21</created><authors><author><keyname>Amami</keyname><forenames>Rimah</forenames></author><author><keyname>Manita</keyname><forenames>Ghaith</forenames></author><author><keyname>Smiti</keyname><forenames>Abir</forenames></author></authors><title>Robust speech recognition using consensus function based on multi-layer
  networks</title><categories>cs.CL cs.LG</categories><comments>06 pages</comments><journal-ref>9th Iberian Conference on Information Systems and Technologies
  (CISTI), Barcelona 18-21 June, 2014, pgs 1-6</journal-ref><doi>10.1109/CISTI.2014.6877093</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The clustering ensembles mingle numerous partitions of a specified data into
a single clustering solution. Clustering ensemble has emerged as a potent
approach for ameliorating both the forcefulness and the stability of
unsupervised classification results. One of the major problems in clustering
ensembles is to find the best consensus function. Finding final partition from
different clustering results requires skillfulness and robustness of the
classification algorithm. In addition, the major problem with the consensus
function is its sensitivity to the used data sets quality. This limitation is
due to the existence of noisy, silence or redundant data. This paper proposes a
novel consensus function of cluster ensembles based on Multilayer networks
technique and a maintenance database method. This maintenance database approach
is used in order to handle any given noisy speech and, thus, to guarantee the
quality of databases. This can generates good results and efficient data
partitions. To show its effectiveness, we support our strategy with empirical
evaluation using distorted speech from Aurora speech databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06025</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06025</id><created>2015-07-21</created><authors><author><keyname>Amami</keyname><forenames>Rimah</forenames></author><author><keyname>Ayed</keyname><forenames>Dorra Ben</forenames></author><author><keyname>Ellouze</keyname><forenames>Nouerddine</forenames></author></authors><title>Incorporating Belief Function in SVM for Phoneme Recognition</title><categories>cs.CL cs.LG</categories><comments>9th International Conference, Hybrid Artificial Intelligence Systems,
  Salamanca, Spain, June 11-13, 2014</comments><journal-ref>Lecture Notes in Computer Science Volume 8480, 2014, pp 191-199</journal-ref><doi>10.1007/978-3-319-07617-1_17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Support Vector Machine (SVM) method has been widely used in numerous
classification tasks. The main idea of this algorithm is based on the principle
of the margin maximization to find an hyperplane which separates the data into
two different classes.In this paper, SVM is applied to phoneme recognition
task. However, in many real-world problems, each phoneme in the data set for
recognition problems may differ in the degree of significance due to noise,
inaccuracies, or abnormal characteristics; All those problems can lead to the
inaccuracies in the prediction phase. Unfortunately, the standard formulation
of SVM does not take into account all those problems and, in particular, the
variation in the speech input. This paper presents a new formulation of SVM
(B-SVM) that attributes to each phoneme a confidence degree computed based on
its geometric position in the space. Then, this degree is used in order to
strengthen the class membership of the tested phoneme. Hence, we introduce a
reformulation of the standard SVM that incorporates the degree of belief.
Experimental performance on TIMIT database shows the effectiveness of the
proposed method B-SVM on a phoneme recognition problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06028</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06028</id><created>2015-07-21</created><authors><author><keyname>Amami</keyname><forenames>Rimah</forenames></author><author><keyname>Ayed</keyname><forenames>Dorra Ben</forenames></author><author><keyname>Ellouze</keyname><forenames>Noureddine</forenames></author></authors><title>The challenges of SVM optimization using Adaboost on a phoneme
  recognition problem</title><categories>cs.CL cs.LG</categories><journal-ref>IEEE 4th International Conference on Cognitive Infocommunications
  (CogInfoCom), Budapest 2-5 Dec. 2013, pgs 463-468</journal-ref><doi>10.1109/CogInfoCom.2013.6719292</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of digital technology is growing at a very fast pace which led to the
emergence of systems based on the cognitive infocommunications. The expansion
of this sector impose the use of combining methods in order to ensure the
robustness in cognitive systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06029</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06029</id><created>2015-07-21</created><authors><author><keyname>Magno</keyname><forenames>Katrina Joy H.</forenames></author><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>Towards Input Device Satisfaction Through Hand Anthropometry</title><categories>cs.CY cs.HC</categories><comments>20 pages, 12 figures, appeared in A.L. Sioson (ed.) Proceedings
  (CDROM) of the 10th National Conference on Information Technology Education
  (NCITE 2012), Laoag City, Ilocos Norte, Philippines, 18-20 October 2012</comments><journal-ref>Philippine Information Technology Journal 6(1):17-28 (2013)</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We collected the hand anthropometric data of 91 respondents to come up with a
Filipino-based measurement to determine the suitability of an input device for
a digital equipment, the standard PC keyboard. For correlation purposes, we
also collected other relevant information like age, height, province of origin,
and gender, among others. We computed the percentiles for each finger to
classify various finger dimensions and identify length-specific anthropometric
cut-points. We compared the percentiles of each finger dimension against the
actual length of the longest key combinations when correct finger placement is
used for typing, to determine whether the standard PC keyboard is fit for use
by our sampled population. Our analysis shows that the members of the
population with hand dimensions at extended position below 75th percentile and
at 99th percentile are the ones who would most likely not reach the longest key
combination for the left and the right hands, respectively. Using machine
vision and image processing techniques, we automated the anthropometric process
and compared the accuracy of its measurements to that of manual process'. We
compared the measurement generated by our automated anthropometric process with
the measurements using the manual one and we found out that they have a very
minimal absolute difference. The data collected from this study could be used
in other studies such as determining a good design for mobile and other
handheld devices, or input devices other than keyboard. The automated method
that we developed could be used to easily measure hand dimensions given a
digital image of the hand and could be extended for measuring the entire human
body for various other applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06038</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06038</id><created>2015-07-21</created><authors><author><keyname>Lupo</keyname><forenames>Cosmo</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author><author><keyname>Lloyd</keyname><forenames>Seth</forenames></author></authors><title>Quantum data hiding in the presence of noise</title><categories>quant-ph cs.IT math.IT</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When classical or quantum information is broadcast to separate receivers,
there exist codes that encrypt the encoded data such that the receivers cannot
recover it when performing local operations and classical communication, but
they can decode reliably if they bring their systems together and perform a
collective measurement. This phenomenon is known as quantum data hiding and
hitherto has been studied under the assumption that noise does not affect the
encoded systems. With the aim of applying the quantum data hiding effect in
practical scenarios, here we define the data-hiding capacity for hiding
classical information using a quantum channel. With this notion, we establish a
regularized upper bound on the data hiding capacity of any quantum broadcast
channel and we prove that coherent-state encodings have a strong limitation on
their data hiding rates. We then prove a lower bound on the data hiding
capacity of channels that map the maximally mixed state to the maximally mixed
state (we call these channels &quot;mictodiactic&quot;---they can be seen as a
generalization of unital channels when the input and output spaces are not
necessarily isomorphic) and argue how to extend this bound to generic channels
and to more than two receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06045</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06045</id><created>2015-07-21</created><authors><author><keyname>Hasseler</keyname><forenames>Gregory</forenames></author></authors><title>Adapting Stochastic Search For Real-time Dynamic Weighted Constraint
  Satisfaction</title><categories>cs.AI</categories><comments>187 pages, Master's Thesis submitted to State University of New York
  Institute of Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents two new algorithms for performing constraint satisfaction.
The first algorithm presented, DMaxWalkSat, is a constraint solver specialized
for solving dynamic, weighted constraint satisfaction problems. The second
algorithm, RDMaxWalkSat, is a derivative of DMaxWalkSat that has been modified
into an anytime algorithm, and hence support realtime constraint satisfaction.
DMaxWalkSat is shown to offer performance advantages in terms of solution
quality and runtime over its parent constraint solver, MaxWalkSat. RDMaxWalkSat
is shown to support anytime operation. The introduction of these algorithms
brings another tool to the areas of computer science that naturally represent
problems as constraint satisfaction problems, an example of which is the robust
coherence algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06053</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06053</id><created>2015-07-22</created><updated>2015-10-07</updated><authors><author><keyname>Xiao</keyname><forenames>Han</forenames></author></authors><title>On Kernel Mengerian Orientations of Line Multigraphs</title><categories>math.CO cs.DM</categories><comments>12 pages, corrected and slightly expanded version</comments><msc-class>90C10, 90C27, 90C57</msc-class><acm-class>G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a polyhedral description of kernels in orientations of line
multigraphs. Given a digraph $D$, let $FK(D)$ denote the fractional kernel
polytope defined on $D$, and let ${\sigma}(D)$ denote the linear system
defining $FK(D)$. A digraph $D$ is called kernel perfect if every induced
subdigraph $D^\prime$ has a kernel, called kernel ideal if $FK(D^\prime)$ is
integral for each induced subdigraph $D^\prime$, and called kernel Mengerian if
${\sigma} (D^\prime)$ is TDI for each induced subdigraph $D^\prime$. We show
that an orientation of a line multigraph is kernel perfect iff it is kernel
ideal iff it is kernel Mengerian. Our result strengthens the theorem of Borodin
et al. [3] on kernel perfect digraphs and generalizes the theorem of Kiraly and
Pap [7] on stable matching problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06056</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06056</id><created>2015-07-22</created><updated>2016-03-06</updated><authors><author><keyname>Ghosh</keyname><forenames>Subir Kumar</forenames></author><author><keyname>Pal</keyname><forenames>Sudebkumar Prasant</forenames></author></authors><title>A National Effort for Motivating Indian Students and Teachers towards
  Algorithmic Research</title><categories>cs.CY cs.DM cs.DS</categories><acm-class>F.2.0; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During 2008-2015, twenty-two introductory workshops on graph and geometric
algorithms were organized for teachers and students (undergraduate,
post-graduate and doctoral) of engineering colleges and universities at
different states and union territories of India. The lectures were meant to
provide exposure to the field of graph and geometric algorithms and to motivate
the participants towards research. Fifty-eight professors from TIFR, IITs,
IISc, IMSc, CMI, ISI Kolkata, and other institutes and universities delivered
invited lectures on different topics in the design and analysis of algorithms,
discrete applied mathematics, computer graphics, computer vision, and robotics.
The first four workshops were funded by TIFR, BRNS and IIT Kharagpur, and the
remaining workshops were funded by the NBHM. In this paper, we present the
salient features of these workshops, and state our observations on the national
impact of these workshops.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06065</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06065</id><created>2015-07-22</created><authors><author><keyname>Hosseini</keyname><forenames>Reshad</forenames></author><author><keyname>Mash'al</keyname><forenames>Mohamadreza</forenames></author></authors><title>MixEst: An Estimation Toolbox for Mixture Models</title><categories>stat.ML cs.LG</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixture models are powerful statistical models used in many applications
ranging from density estimation to clustering and classification. When dealing
with mixture models, there are many issues that the experimenter should be
aware of and needs to solve. The MixEst toolbox is a powerful and user-friendly
package for MATLAB that implements several state-of-the-art approaches to
address these problems. Additionally, MixEst gives the possibility of using
manifold optimization for fitting the density model, a feature specific to this
toolbox. MixEst simplifies using and integration of mixture models in
statistical models and applications. For developing mixture models of new
densities, the user just needs to provide a few functions for that statistical
distribution and the toolbox takes care of all the issues regarding mixture
models. MixEst is available at visionlab.ut.ac.ir/mixest and is fully
documented and is licensed under GPL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06070</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06070</id><created>2015-07-22</created><updated>2015-10-05</updated><authors><author><keyname>Don</keyname><forenames>Henk</forenames></author></authors><title>The Cerny conjecture and 1-contracting automata</title><categories>math.CO cs.FL</categories><comments>15 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A deterministic finite automaton is synchronizing if there exists a word that
sends all states of the automaton to the same state. \v{C}ern\'y conjectured in
1964 that a synchronizing automaton with $n$ states has a synchronizing word of
length at most $(n-1)^2$. We introduce the notion of aperiodically
$1-$contracting automata and prove that in these automata all subsets of the
state set are reachable, so that in particular they are synchronizing.
Furthermore, we give a sufficient condition under which the \v{C}ern\'y
conjecture holds for aperiodically $1-$contracting automata. As a special case,
we prove some results for circular automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06073</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06073</id><created>2015-07-22</created><authors><author><keyname>Tang</keyname><forenames>Hao</forenames></author><author><keyname>Wang</keyname><forenames>Weiran</forenames></author><author><keyname>Gimpel</keyname><forenames>Kevin</forenames></author><author><keyname>Livescu</keyname><forenames>Karen</forenames></author></authors><title>Discriminative Segmental Cascades for Feature-Rich Phone Recognition</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discriminative segmental models, such as segmental conditional random fields
(SCRFs) and segmental structured support vector machines (SSVMs), have had
success in speech recognition via both lattice rescoring and first-pass
decoding. However, such models suffer from slow decoding, hampering the use of
computationally expensive features, such as segment neural networks or other
high-order features. A typical solution is to use approximate decoding, either
by beam pruning in a single pass or by beam pruning to generate a lattice
followed by a second pass. In this work, we study discriminative segmental
models trained with a hinge loss (i.e., segmental structured SVMs). We show
that beam search is not suitable for learning rescoring models in this
approach, though it gives good approximate decoding performance when the model
is already well-trained. Instead, we consider an approach inspired by
structured prediction cascades, which use max-marginal pruning to generate
lattices. We obtain a high-accuracy phonetic recognition system with several
expensive feature types: a segment neural network, a second-order language
model, and second-order phone boundary features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06080</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06080</id><created>2015-07-22</created><authors><author><keyname>Patra</keyname><forenames>Chiranjib</forenames></author></authors><title>On the Conservation of Number of Nodes and the Consumed Energy in
  Wireless Sensor Network: A Statistical Mechanics Approach</title><categories>cs.NI</categories><comments>7 pages</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dominant dynamics of sensor networks consist of using the energy of the
sensor nodes to create the topology of hierarchical clustering using topology
control protocols. The topology thus created will always have optimum number of
nodes after using a certain amount of energy, vice versa optimum amount of
energy expense to use a certain number of nodes. This paper attempts to find a
relation between the number of nodes and the energy consumed using statistical
mechanics. This relationship thus obtained validates considerably well with the
simulation experiments with topology control protocols
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06103</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06103</id><created>2015-07-22</created><authors><author><keyname>Manna</keyname><forenames>Marco</forenames></author><author><keyname>Ricca</keyname><forenames>Francesco</forenames></author><author><keyname>Terracina</keyname><forenames>Giorgio</forenames></author></authors><title>Taming Primary Key Violations to Query Large Inconsistent Data</title><categories>cs.AI cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consistent query answering over a database that violates primary key
constraints is a classical hard problem in database research that has been
traditionally dealt with logic programming. However, the applicability of
existing logic-based solutions is restricted to data sets of moderate size.
This paper presents a novel decomposition and pruning strategy that reduces, in
polynomial time, the problem of computing the consistent answer to a
conjunctive query over a database subject to primary key constraints to a
collection of smaller problems of the same sort that can be solved
independently. The new strategy is naturally modeled and implemented using
Answer Set Programming (ASP). An experiment run on benchmarks from the database
world prove the effectiveness and efficiency of our ASP-based approach also on
large data sets. To appear in Theory and Practice of Logic Programming (TPLP),
Proceedings of ICLP 2015.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06105</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06105</id><created>2015-07-22</created><authors><author><keyname>Sun</keyname><forenames>Jianyuan</forenames></author><author><keyname>Zhong</keyname><forenames>Guoqiang</forenames></author><author><keyname>Dong</keyname><forenames>Junyu</forenames></author><author><keyname>Cai</keyname><forenames>Yajuan</forenames></author></authors><title>Banzhaf Random Forests</title><categories>cs.LG cs.CV stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:1302.4853 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random forests are a type of ensemble method which makes predictions by
combining the results of several independent trees. However, the theory of
random forests has long been outpaced by their application. In this paper, we
propose a novel random forests algorithm based on cooperative game theory.
Banzhaf power index is employed to evaluate the power of each feature by
traversing possible feature coalitions. Unlike the previously used information
gain rate of information theory, which simply chooses the most informative
feature, the Banzhaf power index can be considered as a metric of the
importance of each feature on the dependency among a group of features. More
importantly, we have proved the consistency of the proposed algorithm, named
Banzhaf random forests (BRF). This theoretical analysis takes a step towards
narrowing the gap between the theory and practice of random forests for
classification problems. Experiments on several UCI benchmark data sets show
that BRF is competitive with state-of-the-art classifiers and dramatically
outperforms previous consistent random forests. Particularly, it is much more
efficient than previous consistent random forests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06106</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06106</id><created>2015-07-22</created><authors><author><keyname>Borge-Holthoefer</keyname><forenames>Javier</forenames></author><author><keyname>Perra</keyname><forenames>Nicola</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Bruno</forenames></author><author><keyname>Gonz&#xe1;lez-Bail&#xf3;n</keyname><forenames>Sandra</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author><author><keyname>Vespignani</keyname><forenames>Alessandro</forenames></author></authors><title>The dynamic of information-driven coordination phenomena: a transfer
  entropy analysis</title><categories>physics.soc-ph cs.SI nlin.AO physics.data-an</categories><comments>46 pages (main text: 16; SI: 30)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data from social media are providing unprecedented opportunities to
investigate the processes that rule the dynamics of collective social
phenomena. Here, we consider an information theoretical approach to define and
measure the temporal and structural signatures typical of collective social
events as they arise and gain prominence. We use the symbolic transfer entropy
analysis of micro-blogging time series to extract directed networks of
influence among geolocalized sub-units in social systems. This methodology
captures the emergence of system-level dynamics close to the onset of socially
relevant collective phenomena. The framework is validated against a detailed
empirical analysis of five case studies. In particular, we identify a change in
the characteristic time-scale of the information transfer that flags the onset
of information-driven collective phenomena. Furthermore, our approach
identifies an order-disorder transition in the directed network of influence
between social sub-units. In the absence of a clear exogenous driving, social
collective phenomena can be represented as endogenously-driven structural
transitions of the information transfer network. This study provides results
that can help define models and predictive algorithms for the analysis of
societal events based on open source data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06111</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06111</id><created>2015-07-22</created><authors><author><keyname>Bandelt</keyname><forenames>Hans-Juergen</forenames></author><author><keyname>Chepoi</keyname><forenames>Victor</forenames></author><author><keyname>Knauer</keyname><forenames>Kolja</forenames></author></authors><title>COMs: Complexes of Oriented Matroids</title><categories>math.CO cs.DM</categories><comments>39 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In his seminal 1983 paper, Jim Lawrence introduced lopsided sets and featured
them as asymmetric counterparts of oriented matroids, both sharing the key
property of strong elimination. Moreover, symmetry of faces holds in both
structures as well as in the so-called affine oriented matroids. These two
fundamental properties (formulated for covectors) together lead to the natural
notion of &quot;conditional oriented matroid&quot; (abbreviated COM). These novel
structures can be characterized in terms of three cocircuits axioms,
generalizing the familiar characterization for oriented matroids. We describe a
binary composition scheme by which every COM can successively be erected as a
certain complex of oriented matroids, in essentially the same way as a lopsided
set can be glued together from its maximal hypercube faces. A realizable COM is
represented by a hyperplane arrangement restricted to an open convex set. Among
these are the examples formed by linear extensions of ordered sets,
generalizing the oriented matroids corresponding to the permutohedra. Relaxing
realizability to local realizability, we capture a wider class of combinatorial
objects: we show that non-positively curved Coxeter zonotopal complexes give
rise to locally realizable COMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06120</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06120</id><created>2015-07-22</created><updated>2015-12-16</updated><authors><author><keyname>Bola&#xf1;os</keyname><forenames>Marc</forenames></author><author><keyname>Dimiccoli</keyname><forenames>Mariella</forenames></author><author><keyname>Radeva</keyname><forenames>Petia</forenames></author></authors><title>Towards Storytelling from Visual Lifelogging: An Overview</title><categories>cs.CV</categories><comments>17 pages, 11 figures, Submitted to IEEE Transactions on Human-Machine
  Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual lifelogging consists in acquiring images that capture the daily
experiences of the user by wearing a camera over a long period of time. The
pictures taken offer considerable potential for knowledge mining concerning how
people live their lives, hence they open up new opportunities for many
potential applications in fields including healthcare, security, leisure and
the quantified self. However, automatically building a story from a huge
collection of unstructured egocentric data presents major challenges. This
paper provides a thorough review of advances made so far in egocentric data
analysis and, in view of the current state of the art, indicates new lines of
research to move us towards storytelling from visual lifelogging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06136</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06136</id><created>2015-07-22</created><authors><author><keyname>Moniz</keyname><forenames>Ant&#xf3;nio Brand&#xe3;o</forenames><affiliation>IET</affiliation></author></authors><title>Robots and humans as co-workers?</title><categories>cs.HC cs.RO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of work organisation systems with automated equipment is facing
new challenges and the emergence of new concepts. The social aspects that are
related with new concepts on the complex work environments (CWE) are becoming
more relevant for that design. The work with autonomous systems implies options
in the design of workplaces. Especially that happens in such complex
environments. The concepts of &quot;agents&quot;, &quot;co-working&quot; or &quot;human-centred
technical systems&quot; reveal new dimensions related to human-computer interaction
(HCI). With an increase in the number and complexity of those human-technology
interfaces, the capacities of human intervention can become limited,
originating further problems. The case of robotics is used to exemplify the
issues related with automation in working environments and the emergence of new
HCI approaches that would include social implications. We conclude that studies
on technology assessment of industrial robotics and autonomous agents on
manufacturing environment should also focus on the human involvement strategies
in organisations. A needed participatory strategy implies a new approach to
workplaces design. This means that the research focus must be on the relation
between technology and social dimensions not as separate entities, but
integrated in the design of an interaction system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06141</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06141</id><created>2015-07-22</created><authors><author><keyname>Aracena</keyname><forenames>Julio</forenames></author><author><keyname>Richard</keyname><forenames>Adrien</forenames></author><author><keyname>Salinas</keyname><forenames>Lilian</forenames></author></authors><title>Fixed points in conjunctive networks and maximal independent sets in
  graph contractions</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a graph $G$, let $\mathcal{C}$ be the set of conjunctive networks with
interaction graph $G$, and let $\mathcal{H}$ be the set of graphs obtained from
$G$ by contracting some edges. Let $\mathrm{fix}(f)$ be the number of fixed
points in a network $f\in \mathcal{C}$, and let $\mathrm{mis}(H)$ be the number
of maximal independent sets in $H\in\mathcal{H}$. Our main result is
\[\mathrm{mis}(G)~\leq~\max_{H\in\mathcal{H}}\mathrm{mis}(H)~\leq~
\max_{f\in\mathcal{C}}\mathrm{fix}(f)~\leq~
\left(\frac{3}{2}\right)^{m(G)}\mathrm{mis}(G) \] where $m(G)$ is the maximum
size of a matching $M$ of $G$ such that every edge of $M$ is contained in an
induced copy of $C_4$ that contains no other edge of $M$. Thus if $G$ has no
induced $C_4$ then
$\max_{H\in\mathcal{H}}\mathrm{mis}(H)=\max_{f\in\mathcal{C}}\mathrm{fix}(f)=\mathrm{mis}(G)$,
and this contrasts with following complexity result: It is coNP-hard to decide
if $\max_{f\in\mathcal{C}}\mathrm{fix}(f)=\mathrm{mis}(G)$ or if
$\max_{H\in\mathcal{H}}\mathrm{mis}(H)=\mathrm{mis}(G)$, even if $G$ has a
unique induced copy of $C_4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06148</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06148</id><created>2015-07-22</created><updated>2015-08-13</updated><authors><author><keyname>Tang</keyname><forenames>Chunming</forenames></author><author><keyname>Li</keyname><forenames>Nian</forenames></author><author><keyname>Qi</keyname><forenames>Yanfeng</forenames></author><author><keyname>Zhou</keyname><forenames>Zhengchun</forenames></author><author><keyname>Helleseth</keyname><forenames>Tor</forenames></author></authors><title>Linear codes with two or three weights from weakly regular bent
  functions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear codes with few weights have applications in consumer electronics,
communication, data storage system, secret sharing, authentication codes,
association schemes, and strongly regular graphs. This paper first generalizes
the method of constructing two-weight and three-weight linear codes of Ding et
al. \cite{DD2015} and Zhou et al. \cite{ZLFH2015} to general weakly regular
bent functions and determines the weight distributions of these linear codes.
It solves the open problem of Ding et al. \cite{DD2015}. Further, this paper
constructs new linear codes with two or three weights and presents the weight
distributions of these codes. They contains some optimal codes meeting certain
bound on linear codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06149</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06149</id><created>2015-07-22</created><authors><author><keyname>Srinivas</keyname><forenames>Suraj</forenames></author><author><keyname>Babu</keyname><forenames>R. Venkatesh</forenames></author></authors><title>Data-free parameter pruning for Deep Neural Networks</title><categories>cs.CV</categories><comments>BMVC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Neural nets (NNs) with millions of parameters are at the heart of many
state-of-the-art computer vision systems today. However, recent works have
shown that much smaller models can achieve similar levels of performance. In
this work, we address the problem of pruning parameters in a trained NN model.
Instead of removing individual weights one at a time as done in previous works,
we remove one neuron at a time. We show how similar neurons are redundant, and
propose a systematic way to remove them. Our experiments in pruning the densely
connected layers show that we can remove upto 85\% of the total parameters in
an MNIST-trained network, and about 35\% for AlexNet without significantly
affecting performance. Our method can be applied on top of most networks with a
fully connected layer to give a smaller network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06165</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06165</id><created>2015-07-22</created><updated>2015-11-25</updated><authors><author><keyname>Wang</keyname><forenames>Cheng</forenames></author></authors><title>Asynchronous Byzantine Agreement with Optimal Resilience and Linear
  Complexity</title><categories>cs.DC</categories><comments>Previous title: Fast Almost-Surely Terminating Byzantine Agreement</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given a system with $n &gt; 3t + 1$ processes, where $t$ is the tolerated number
of faulty ones, we present a fast asynchronous Byzantine agreement protocol
that can reach agreement in $O(t)$ expected running time. This improves the
$O(n^2)$ expected running time of Abraham, Dolev, and Halpern [PODC 2008].
Furthermore, if $n = (3 + \varepsilon) t$ for any $\varepsilon &gt; 0$, our
protocol can reach agreement in $O (1 / \varepsilon)$ expected running time.
This improves the result of Feldman and Micali [STOC 1988] (with constant
expected running time when $n &gt; 4 t$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06173</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06173</id><created>2015-07-22</created><authors><author><keyname>Adam</keyname><forenames>Amit</forenames></author><author><keyname>Dann</keyname><forenames>Christoph</forenames></author><author><keyname>Yair</keyname><forenames>Omer</forenames></author><author><keyname>Mazor</keyname><forenames>Shai</forenames></author><author><keyname>Nowozin</keyname><forenames>Sebastian</forenames></author></authors><title>Bayesian Time-of-Flight for Realtime Shape, Illumination and Albedo</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a computational model for shape, illumination and albedo inference
in a pulsed time-of-flight (TOF) camera. In contrast to TOF cameras based on
phase modulation, our camera enables general exposure profiles. This results in
added flexibility and requires novel computational approaches.
  To address this challenge we propose a generative probabilistic model that
accurately relates latent imaging conditions to observed camera responses.
While principled, realtime inference in the model turns out to be infeasible,
and we propose to employ efficient non-parametric regression trees to
approximate the model outputs. As a result we are able to provide, for each
pixel, at video frame rate, estimates and uncertainty for depth, effective
albedo, and ambient light intensity. These results we present are
state-of-the-art in depth imaging.
  The flexibility of our approach allows us to easily enrich our generative
model. We demonstrate that by extending the original single-path model to a
two-path model, capable of describing some multipath effects. The new model is
seamlessly integrated in the system at no additional computational cost.
  Our work also addresses the important question of optimal exposure design in
pulsed TOF systems. Finally, for benchmark purposes and to obtain realistic
empirical priors of multipath and insights into this phenomena, we propose a
physically accurate simulation of multipath phenomena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06174</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06174</id><created>2015-07-22</created><updated>2016-02-06</updated><authors><author><keyname>Averbuch</keyname><forenames>Amir</forenames></author><author><keyname>Shabat</keyname><forenames>Gil</forenames></author><author><keyname>Shkolnisky</keyname><forenames>Yoel</forenames></author></authors><title>Direct Inversion of the 3D Pseudo-polar Fourier Transform</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pseudo-polar Fourier transform is a specialized non-equally spaced
Fourier transform, which evaluates the Fourier transform on a near-polar grid,
known as the pseudo-polar grid. The advantage of the pseudo-polar grid over
other non-uniform sampling geometries is that the transformation, which samples
the Fourier transform on the pseudo-polar grid, can be inverted using a fast
and stable algorithm. For other sampling geometries, even if the non-equally
spaced Fourier transform can be inverted, the only known algorithms are
iterative. The convergence speed of these algorithms as well as their accuracy
are difficult to control, as they depend both on the sampling geometry as well
as on the unknown reconstructed object. In this paper, we present a direct
inversion algorithm for the three-dimensional pseudo-polar Fourier transform.
The algorithm is based only on one-dimensional resampling operations, and is
shown to be significantly faster than existing iterative inversion algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06175</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06175</id><created>2015-07-22</created><authors><author><keyname>Brakensiek</keyname><forenames>Joshua</forenames></author><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Zbarsky</keyname><forenames>Samuel</forenames></author></authors><title>Efficient Low-Redundancy Codes for Correcting Multiple Deletions</title><categories>cs.IT cs.DM cs.DS math.IT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of constructing binary codes to recover from $k$-bit
deletions with efficient encoding/decoding, for a fixed $k$. The single
deletion case is well understood, with the Varshamov-Tenengolts-Levenshtein
code from 1965 giving an asymptotically optimal construction with $\approx
2^n/n$ codewords of length $n$, i.e., at most $\log n$ bits of redundancy.
However, even for the case of two deletions, there was no known explicit
construction with redundancy less than $n^{\Omega(1)}$.
  For any fixed $k$, we construct a binary code with $c_k \log n$ redundancy
that can be decoded from $k$ deletions in $O_k(n \log^4 n)$ time. The
coefficient $c_k$ can be taken to be $O(k^2 \log k)$, which is only
quadratically worse than the optimal, non-constructive bound of $O(k)$. We also
indicate how to modify this code to allow for a combination of up to $k$
insertions and deletions.
  We also note that among *linear* codes capable of correcting $k$ deletions,
the $(k+1)$-fold repetition code is essentially the best possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06183</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06183</id><created>2015-07-22</created><updated>2015-07-23</updated><authors><author><keyname>Sapirshtein</keyname><forenames>Ayelet</forenames></author><author><keyname>Sompolinsky</keyname><forenames>Yonatan</forenames></author><author><keyname>Zohar</keyname><forenames>Aviv</forenames></author></authors><title>Optimal Selfish Mining Strategies in Bitcoin</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bitcoin is a decentralized crypto-currency, and an accompanying protocol,
created in 2008. Bitcoin nodes continuously generate and propagate
blocks---collections of newly approved transactions that are added to Bitcoin's
ledger. Block creation requires nodes to invest computational resources, but
also carries a reward in the form of bitcoins that are paid to the creator.
While the protocol requires nodes to quickly distribute newly created blocks,
strong nodes can in fact gain higher payoffs by withholding blocks they create
and selectively postponing their publication. The existence of such selfish
mining attacks was first reported by Eyal and Sirer, who have demonstrated a
specific deviation from the standard protocol (a strategy that we name SM1).
  In this paper we extend the underlying model for selfish mining attacks, and
provide an algorithm to find $\epsilon$-optimal policies for attackers within
the model, as well as tight upper bounds on the revenue of optimal policies. As
a consequence, we are able to provide lower bounds on the computational power
an attacker needs in order to benefit from selfish mining. We find that the
profit threshold -- the minimal fraction of resources required for a profitable
attack -- is strictly lower than the one induced by the SM1 scheme. Indeed, the
policies given by our algorithm dominate SM1, by better regulating
attack-withdrawals.
  Using our algorithm, we show that Eyal and Sirer's suggested countermeasure
to selfish mining is slightly less effective than previously conjectured. Next,
we gain insight into selfish mining in the presence of communication delays,
and show that, under a model that accounts for delays, the profit threshold
vanishes, and even small attackers have incentive to occasionally deviate from
the protocol. We conclude with observations regarding the combined power of
selfish mining and double spending attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06188</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06188</id><created>2015-07-22</created><authors><author><keyname>Ren</keyname><forenames>Ju</forenames><affiliation>Sherman</affiliation></author><author><keyname>Zhang</keyname><forenames>Yaoxue</forenames><affiliation>Sherman</affiliation></author><author><keyname>Zhang</keyname><forenames>Ning</forenames><affiliation>Sherman</affiliation></author><author><keyname>Zhang</keyname><forenames>Deyu</forenames><affiliation>Sherman</affiliation></author><author><keyname>Xuemin</keyname><affiliation>Sherman</affiliation></author><author><keyname>Shen</keyname></author></authors><title>Dynamic Channel Access for Energy Efficient Data Gathering in Cognitive
  Radio Sensor Networks</title><categories>cs.NI</categories><comments>13 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks (WSNs) operating in the license-free spectrum suffer
from uncontrolled interference as those spectrum bands become increasingly
crowded. The emerging cognitive radio sensor networks (CRSNs) provide a
promising solution to address this challenge by enabling sensor nodes to
opportunistically access licensed channels. However, since sensor nodes have to
consume considerable energy to support CR functionalities, such as channel
sensing and switching, the opportunistic channel accessing should be carefully
devised for improving the energy efficiency in CRSN. To this end, we
investigate the dynamic channel accessing problem to improve the energy
efficiency for a clustered CRSN. Under the primary users' protection
requirement, we study the resource allocation issues to maximize the energy
efficiency of utilizing a licensed channel for intra-cluster and inter-cluster
data transmission, respectively. With the consideration of the energy
consumption in channel sensing and switching, we further determine the
condition when sensor nodes should sense and switch to a licensed channel for
improving the energy efficiency, according to the packet loss rate of the
license-free channel. In addition, two dynamic channel accessing schemes are
proposed to identify the channel sensing and switching sequences for
intra-cluster and inter-cluster data transmission, respectively. Extensive
simulation results demonstrate that the proposed channel accessing schemes can
significantly reduce the energy consumption in CRSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06189</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06189</id><created>2015-07-22</created><authors><author><keyname>Buiras</keyname><forenames>Pablo</forenames></author><author><keyname>Stefan</keyname><forenames>Deian</forenames></author><author><keyname>Russo</keyname><forenames>Alejandro</forenames></author></authors><title>On Dynamic Flow-Sensitive Floating-Label Systems</title><categories>cs.CR cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flow-sensitive analysis for information-flow control (IFC) allows data
structures to have mutable security labels, i.e., labels that can change over
the course of the computation. This feature is often used to boost the
permissiveness of the IFC monitor, by rejecting fewer runs of programs, and to
reduce the burden of explicit label annotations. However, adding flow-sensitive
constructs (e.g., references or files) to a dynamic IFC system is subtle and
may also introduce high-bandwidth covert channels. In this work, we extend
LIO---a language-based floating-label system---with flow-sensitive references.
The key insight to safely manipulating the label of a reference is to not only
consider the label on the data stored in the reference, i.e., the reference
label, but also the label on the reference label itself. Taking this into
consideration, we provide an upgrade primitive that can be used to change the
label of a reference in a safe manner. We additionally provide a mechanism for
automatic upgrades to eliminate the burden of determining when a reference
should be upgraded. This approach naturally extends to a concurrent setting,
which has not been previously considered by dynamic flow-sensitive systems. For
both our sequential and concurrent calculi we prove non-interference by
embedding the flow-sensitive system into the original, flow-insensitive LIO
calculus---a surprising result on its own.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06199</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06199</id><created>2015-07-22</created><authors><author><keyname>Feldman</keyname><forenames>Moran</forenames></author><author><keyname>Izsak</keyname><forenames>Rani</forenames></author></authors><title>Building a Good Team: Secretary Problems and the Supermodular Degree</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Secretary Problem, one has to hire the best among n candidates. The
candidates are interviewed, one at a time, at a random order, and one has to
decide on the spot, whether to hire a candidate or continue interviewing. It is
well known that the best candidate can be hired with a probability of 1/e
(Dynkin, 1963). Recent works extend this problem to settings in which multiple
candidates can be hired, subject to some constraint. Here, one wishes to hire a
set of candidates maximizing a given set function.
  Almost all extensions considered in the literature assume the objective set
function is either linear or submodular. Unfortunately, real world functions
might not have either of these properties. Consider, for example, a scenario
where one hires researchers for a project. Indeed, it can be that some
researchers can substitute others for that matter. However, it can also be that
some combinations of researchers result in synergy (see, e.g, Woolley et al.,
Science 2010, for a research about collective intelligence). The first
phenomenon can be modeled by a submoudlar set function, while the latter
cannot.
  In this work, we study the secretary problem with an arbitrary non-negative
monotone function, subject to a general matroid constraint. It is not difficult
to prove that, generally, only very poor results can be obtained for this class
of objective functions. We tackle this hardness by combining the following:
1.Parametrizing our algorithms by the supermodular degree of the objective
function (defined by Feige and Izsak, ITCS 2013), which, roughly speaking,
measures the distance of a function from being submodular. 2.Suggesting an
(arguably) natural model that permits approximation guarantees that are
polynomial in the supermodular degree (as opposed to the standard model which
allows only exponential guarantees).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06203</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06203</id><created>2015-07-22</created><authors><author><keyname>Mart&#xed;nez</keyname><forenames>&#xc0;lex Oliveras</forenames></author><author><keyname>De Carvalho</keyname><forenames>Elisabeth</forenames></author><author><keyname>Nielsen</keyname><forenames>Jesper &#xd8;dum</forenames></author></authors><title>Towards Very Large Aperture Massive MIMO: a measurement based study</title><categories>cs.IT math.IT</categories><journal-ref>Globecom Workshops (GC Wkshps), 2014 , vol., no., pp.281,286, 8-12
  Dec. 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO is a new technique for wireless communications that claims to
offer very high system throughput and energy efficiency in multi-user
scenarios. The cost is to add a very large number of antennas at the base
station. Theoretical research has probed these benefits, but very few
measurements have showed the potential of Massive MIMO in practice. We
investigate the properties of measured Massive MIMO channels in a large indoor
venue. We describe a measurement campaign using 3 arrays having different shape
and aperture, with 64 antennas and 8 users with 2 antennas each. We focus on
the impact of the array aperture which is the main limiting factor in the
degrees of freedom available in the multiple antenna channel. We find that
performance is improved as the aperture increases, with an impact mostly
visible in crowded scenarios where the users are closely spaced. We also test
MIMO capability within a same user device with user proximity effect. We see a
good channel resolvability with confirmation of the strong effect of the user
hand grip. At last, we highlight that propagation conditions where
line-of-sight is dominant can be favorable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06210</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06210</id><created>2015-07-20</created><updated>2016-01-17</updated><authors><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Teel</keyname><forenames>Andrew R.</forenames></author></authors><title>Hybrid systems with memory: Existence of generalized solutions and
  well-posedness</title><categories>math.OC cs.SY math.CA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid systems with memory refer to dynamical systems exhibiting both hybrid
and delay phenomena. While systems of this type are frequently encountered in
many physical and engineering systems, particularly in control applications,
various issues centered around the robustness of hybrid delay systems have not
been adequately dealt with. In this paper, we establish some basic results on a
framework that allows to study hybrid systems with memory through generalized
concepts of solutions. In particular, we develop the basic existence of
generalized solutions using regularity conditions on the hybrid data, which are
formulated in a phase space of hybrid trajectories equipped with the graphical
convergence topology. In contrast with the uniform convergence topology that
has been often used, adopting the graphical convergence topology allows us to
establish well-posedness of hybrid systems with memory. We then show that, as a
consequence of well-posedness, pre-asymptotic stability of well-posed hybrid
systems with memory is robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06217</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06217</id><created>2015-07-22</created><updated>2016-01-23</updated><authors><author><keyname>Adams</keyname><forenames>Henry</forenames></author><author><keyname>Chepushtanova</keyname><forenames>Sofya</forenames></author><author><keyname>Emerson</keyname><forenames>Tegan</forenames></author><author><keyname>Hanson</keyname><forenames>Eric</forenames></author><author><keyname>Kirby</keyname><forenames>Michael</forenames></author><author><keyname>Motta</keyname><forenames>Francis</forenames></author><author><keyname>Neville</keyname><forenames>Rachel</forenames></author><author><keyname>Peterson</keyname><forenames>Chris</forenames></author><author><keyname>Shipman</keyname><forenames>Patrick</forenames></author><author><keyname>Ziegelmeier</keyname><forenames>Lori</forenames></author></authors><title>Persistent Images: A Stable Vector Representation of Persistent Homology</title><categories>cs.CG math.AT stat.ML</categories><comments>Version 2 contains the following changes: we updated exposition to
  clarify contribution, added theoretical results supporting methodology,
  expanded references to related work, and updated figures</comments><acm-class>F.2.2; I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many data sets can be viewed as a noisy sampling of an underlying topological
space. A suite of tools in topological data analysis allows one to exploit this
structure for the purpose of knowledge discovery. One such tool is persistent
homology which provides a multiscale description of the homological features
within a data set. A useful representation of this homological information is a
persistence diagram (PD). The space of PDs can be given a metric structure
allowing a given diagram to be used as a statistic for the purpose of
comparison against other diagrams. We convert a PD to a persistence image (PI)
and prove stability with respect to small perturbations in the inputs. The PI
is a vector representation allowing the application of vector-based machine
learning tools, such as linear and sparse support vector machines. These tools
help to identify discriminatory features which can have a topological
interpretation. The PIs and PDs derived from randomly sampled topological
spaces are compared by applying the K-medoids clustering algorithm. To further
illustrate the PI technique, linear and sparse support vector machines are
implemented on this data set and classification is performed on additional data
arising from a discrete dynamical system called the linked twist map.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06222</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06222</id><created>2015-07-22</created><authors><author><keyname>Lagorce</keyname><forenames>Xavier</forenames></author><author><keyname>Benosman</keyname><forenames>Ryad</forenames></author></authors><title>STICK: Spike Time Interval Computational Kernel, A Framework for General
  Purpose Computation using Neurons, Precise Timing, Delays, and Synchrony</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been significant research over the past two decades in developing
new platforms for spiking neural computation. Current neural computers are
primarily developed to mimick biology. They use neural networks which can be
trained to perform specific tasks to mainly solve pattern recognition problems.
These machines can do more than simulate biology, they allow us to re-think our
current paradigm of computation. The ultimate goal is to develop brain inspired
general purpose computation architectures that can breach the current
bottleneck introduced by the Von Neumann architecture. This work proposes a new
framework for such a machine. We show that the use of neuron like units with
precise timing representation, synaptic diversity, and temporal delays allows
us to set a complete, scalable compact computation framework. The presented
framework provides both linear and non linear operations, allowing us to
represent and solve any function. We show usability in solving real use cases
from simple differential equations to sets of non-linear differential equations
leading to chaotic attractors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06228</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06228</id><created>2015-07-22</created><updated>2015-11-23</updated><authors><author><keyname>Srivastava</keyname><forenames>Rupesh Kumar</forenames></author><author><keyname>Greff</keyname><forenames>Klaus</forenames></author><author><keyname>Schmidhuber</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Training Very Deep Networks</title><categories>cs.LG cs.NE</categories><comments>11 pages. Extends arXiv:1505.00387. Project webpage is at
  http://people.idsia.ch/~rupesh/very_deep_learning/. in Advances in Neural
  Information Processing Systems 2015</comments><msc-class>68T01</msc-class><acm-class>I.2.6; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theoretical and empirical evidence indicates that the depth of neural
networks is crucial for their success. However, training becomes more difficult
as depth increases, and training of very deep networks remains an open problem.
Here we introduce a new architecture designed to overcome this. Our so-called
highway networks allow unimpeded information flow across many layers on
information highways. They are inspired by Long Short-Term Memory recurrent
networks and use adaptive gating units to regulate the information flow. Even
with hundreds of layers, highway networks can be trained directly through
simple gradient descent. This enables the study of extremely deep and efficient
architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06235</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06235</id><created>2015-07-22</created><authors><author><keyname>Zanibbi</keyname><forenames>Richard</forenames></author><author><keyname>Davila</keyname><forenames>Kenny</forenames></author><author><keyname>Kane</keyname><forenames>Andrew</forenames></author><author><keyname>Tompa</keyname><forenames>Frank</forenames></author></authors><title>The Tangent Search Engine: Improved Similarity Metrics and Scalability
  for Math Formula Search</title><categories>cs.IR</categories><comments>10 pages</comments><acm-class>H.2.4; H.3.3; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the ever-increasing quantity and variety of data worldwide, the Web has
become a rich repository of mathematical formulae. This necessitates the
creation of robust and scalable systems for Mathematical Information Retrieval,
where users search for mathematical information using individual formulae
(query-by-expression) or a combination of keywords and formulae. Often, the
pages that best satisfy users' information needs contain expressions that only
approximately match the query formulae. For users trying to locate or re-find a
specific expression, browse for similar formulae, or who are mathematical
non-experts, the similarity of formulae depends more on the relative positions
of symbols than on deep mathematical semantics.
  We propose the Maximum Subtree Similarity (MSS) metric for
query-by-expression that produces intuitive rankings of formulae based on their
appearance, as represented by the types and relative positions of symbols.
Because it is too expensive to apply the metric against all formulae in large
collections, we first retrieve expressions using an inverted index over tuples
that encode relationships between pairs of symbols, ranking hits using the Dice
coefficient. The top-k formulae are then re-ranked using MSS. Our approach
obtains state-of-the-art performance on the NTCIR-11 Wikipedia formula
retrieval benchmark and is efficient in terms of both index space and overall
retrieval time. Retrieval systems for other graphical forms, including chemical
diagrams, flowcharts, figures, and tables, may also benefit from adopting our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06239</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06239</id><created>2015-07-22</created><authors><author><keyname>Deligiannis</keyname><forenames>Nikos</forenames></author><author><keyname>Mota</keyname><forenames>Joao F. C.</forenames></author><author><keyname>Smart</keyname><forenames>George</forenames></author><author><keyname>Andreopoulos</keyname><forenames>Yiannis</forenames></author></authors><title>Fast Desynchronization For Decentralized Multichannel Medium Access
  Control</title><categories>cs.SY cs.IT cs.MA math.IT math.OC</categories><comments>to appear in IEEE Transactions on Communications</comments><journal-ref>IEEE Transactions on Communications,&quot; Vol. 63, No. 9, pp.
  3336-3349, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed desynchronization algorithms are key to wireless sensor networks
as they allow for medium access control in a decentralized manner. In this
paper, we view desynchronization primitives as iterative methods that solve
optimization problems. In particular, by formalizing a well established
desynchronization algorithm as a gradient descent method, we establish novel
upper bounds on the number of iterations required to reach convergence.
Moreover, by using Nesterov's accelerated gradient method, we propose a novel
desynchronization primitive that provides for faster convergence to the steady
state. Importantly, we propose a novel algorithm that leads to decentralized
time-synchronous multichannel TDMA coordination by formulating this task as an
optimization problem. Our simulations and experiments on a densely-connected
IEEE 802.15.4-based wireless sensor network demonstrate that our scheme
provides for faster convergence to the steady state, robustness to hidden
nodes, higher network throughput and comparable power dissipation with respect
to the recently standardized IEEE 802.15.4e-2012 time-synchronized channel
hopping (TSCH) scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06240</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06240</id><created>2015-07-22</created><updated>2016-02-16</updated><authors><author><keyname>Gawrychowski</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Kosowski</keyname><forenames>Adrian</forenames></author><author><keyname>Uzna&#x144;ski</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Sublinear-Space Distance Labeling using Hubs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a series of new labeling schemes within the framework of so-called
hub labeling (HL, also known as landmark labeling or 2-hop-cover labeling), in
which each node $u$ stores its distance to all nodes from an appropriately
chosen set of hubs $S(u) \subseteq V$. For a queried pair of nodes $(u,v)$, the
length of a shortest $u-v$-path passing through a hub node from $S(u)\cap S(v)$
is then used as an upper bound on the distance between $u$ and $v$.
  We first present a hub labeling which allows us to decode exact distances in
sparse graphs using labels of size sublinear in the number of nodes. For graphs
with at most $n$ nodes and average degree $\Delta$, the tradeoff between label
bit size $L$ and query decoding time $T$ for our approach is given by $L = O(n
\log \log_\Delta T / \log_\Delta T)$, for any $T \leq n$.
  By using similar techniques, we then present a $2$-additive labeling scheme
for general graphs, i.e., one in which the decoder provides a
2-additive-approximation of the distance between any pair of nodes. We achieve
the same label size-time tradeoff $L = O(n \log^2 \log T / \log T)$, for any $T
\leq n$.
  Finally, we propose a $D$-preserving labeling scheme for general graphs,
i.e., one in which the decoder correctly returns the distance value whenever
this value is at least $D$. We provide the first such scheme, in which the
constructed hub sets have asymptotically optimal size $O(n/D)$, for values of
parameter $D = \widetilde O(\sqrt n)$. (For larger values of parameter $D$, the
average hub set size in our construction is still $O(\frac{n}{D})$, but the
largest hub set may be larger by a multiplicative factor of $O(\log \log D)$.)
The distance labels corresponding to our solution are of size $O(\frac{n}{D}
\log D)$, which provides a logarithmic improvement with respect to
$D$-preserving labeling schemes previously known from the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06248</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06248</id><created>2015-07-22</created><authors><author><keyname>Li</keyname><forenames>Yinan</forenames></author><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Ozay</keyname><forenames>Necmiye</forenames></author></authors><title>Computing finite abstractions with robustness margins via local
  reachable set over-approximation</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a method to compute finite abstractions that can be used
for synthesizing robust hybrid control strategies for nonlinear systems. Most
existing methods for computing finite abstractions utilize some global,
analytical function to provide bounds on the reachable sets of nonlinear
systems, which can be conservative and lead to spurious transitions in the
abstract systems. This problem is even more pronounced in the presence of
imperfect measurements and modelling uncertainties, where control synthesis can
easily become infeasible due to added spurious transitions. To mitigate this
problem, we propose to compute finite abstractions with robustness margins by
over-approximating the local reachable sets of nonlinear systems. We do so by
linearizing the nonlinear dynamics into linear affine systems and keeping track
of the linearization error. It is shown that this approach provides tighter
approximations and several numerical examples are used to illustrate of
effectiveness of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06254</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06254</id><created>2015-07-22</created><authors><author><keyname>Cioab&#x103;</keyname><forenames>Sebastian M.</forenames></author><author><keyname>Koolen</keyname><forenames>Jack H.</forenames></author><author><keyname>Li</keyname><forenames>Weiqiang</forenames></author></authors><title>Max-cut and extendability of matchings in distance-regular graphs</title><categories>math.CO cs.DM</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a distance-regular graph of order $v$ and size $e$. In this paper,
we show that the max-cut in $G$ is at most $e(1-1/g)$, where $g$ is the odd
girth of $G$. This result implies that the independence number of $G$ is at
most $\frac{v}{2}(1-1/g)$. We use this fact to also study the extendability of
matchings in distance-regular graphs. A graph $G$ of even order $v$ is called
$t$-extendable if it contains a perfect matching, $t&lt;v/2$ and any matching of
$t$ edges is contained in some perfect matching. The extendability of $G$ is
the maximum $t$ such that $G$ is $t$-extendable. We generalize previous results
on strongly regular graphs and show that all distance-regular graphs with
diameter $D\geq 3$ are $2$-extendable. We also obtain various lower bounds for
the extendability of distance-regular graphs of valency $k$ that depend on $k$,
$\lambda$ and $\mu$, where $\lambda$ is the number of common neighbors of any
two adjacent vertices and $\mu$ is the number of common neighbors of any two
vertices in distance two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06260</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06260</id><created>2015-07-22</created><authors><author><keyname>Jureta</keyname><forenames>Ivan</forenames></author></authors><title>Requirements Problem and Solution Concepts for Adaptive Systems
  Engineering, and their Relationship to Mathematical Optimisation, Decision
  Analysis, and Expected Utility Theory</title><categories>cs.SE</categories><acm-class>D.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Requirements Engineering (RE) focuses on eliciting, modelling, and analyzing
the requirements and environment of a system-to-be in order to design its
specification. The design of the specification, usually called the Requirements
Problem (RP), is a complex problem solving task, as it involves, for each new
system-to-be, the discovery and exploration of, and decision making in, new and
ill-defined problem and solution spaces. The default RP in RE is to design a
specification of the system-to-be which (i) is consistent with given
requirements and conditions of its environment, and (ii) together with
environment conditions satisfies requirements. This paper (i) shows that the
Requirements Problem for Adaptive Systems (RPAS) is different from, and is not
a subclass of the default RP, (ii) gives a formal definition of RPAS, and (iii)
discusses implications for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06266</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06266</id><created>2015-07-22</created><updated>2016-02-23</updated><authors><author><keyname>Dimiccoli</keyname><forenames>Mariella</forenames></author><author><keyname>Jacob</keyname><forenames>Jean-Pascal</forenames></author><author><keyname>Moisan</keyname><forenames>Lionel</forenames></author></authors><title>Particle detection and tracking by a-contrario approach: application to
  fluorescence time-lapse imaging</title><categories>cs.CV</categories><comments>To appear in Journal of Machine Vision and Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a probabilistic approach for the detection and the
tracking of particles on biological images. In presence of very noised and poor
quality data, particles and trajectories can be characterized by an a-contrario
model, that estimates the probability of observing the structures of interest
in random data. This approach, first introduced in the modeling of human visual
perception and then successfully applied in many image processing tasks, leads
to algorithms that do not require a previous learning stage, nor a tedious
parameter tuning and are very robust to noise. Comparative evaluations against
a well established baseline show that the proposed approach outperforms the
state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06268</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06268</id><created>2015-07-22</created><authors><author><keyname>Johnson</keyname><forenames>Oliver</forenames></author></authors><title>A discrete log-Sobolev inequality under a Bakry-Emery type condition</title><categories>math.PR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider probability mass functions $V$ supported on the positive integers
using arguments introduced by Caputo, Dai Pra and Posta, based on a
Bakry--\'{E}mery condition for a Markov birth and death operator with invariant
measure $V$. Under this condition, we prove a modified logarithmic Sobolev
inequality, generalizing and strengthening results of Wu, Bobkov and Ledoux,
and Caputo, Dai Pra and Posta.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06295</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06295</id><created>2015-07-22</created><updated>2015-07-27</updated><authors><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Lemieux</keyname><forenames>Yves</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>The Service-Bond Paradigm - Potentials for a Sustainable, ICT-enabled
  Future</title><categories>cs.CY cs.ET</categories><comments>17 pages and 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The service paradigm as we know it has gone through a long journey of
evolution and improvement, and it seems that a service-oriented vision to
activities in general could serve as a potential platform for the global
transition to a sustainable future. However, it is also apparent that the
services themselves are required to move beyond their traditional definition in
order to prevent any secondary side effect. Here, a new paradigm is proposed
based on bonding between entities involved in a service interaction, service
chaining, or service orchestration. It is purposed to serve as a vehicle to
approach sustainability at the global level in a manner that is thoughtful,
collaborative, and incremental. Time-modulated implementation of the proposed
service-bond paradigm is considered in order to reduce the associated risks and
liabilities. The service bonds are then simply generalized toward representing
bonding among more than two entities. Finally, a practical application of ICT
agents in enabling the service bonds is presented in a use case related to
smart houses. In this use case, some ICT-based agents (federal regulars, among
other ICT agents) are considered to represent and govern services and service
bonds of a household with external entities such as utilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06296</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06296</id><created>2015-07-22</created><authors><author><keyname>Ordentlich</keyname><forenames>Or</forenames></author><author><keyname>Shayevitz</keyname><forenames>Ofer</forenames></author></authors><title>Mutual Information Bounds via Adjacency Events</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mutual information between two jointly distributed random variables $X$
and $Y$ is a functional of the joint distribution $P_{XY}$, which is sometimes
difficult to handle or estimate. A coarser description of the statistical
behavior of $(X,Y)$ is given by the marginal distributions $P_X, P_Y$ and the
adjacency relation induced by the joint distribution, where $x$ and $y$ are
adjacent if $P(x,y)&gt;0$. We derive a lower bound on the mutual information in
terms of these entities. This is achieved by viewing the channel from $X$ to
$Y$ as a probability distribution on a set of possible actions, where an action
determines the output for any possible input, and is independently drawn. We
further derive an upper bound on the mutual information in terms of adjacency
events between the action and the pair $(X,Y)$, where in this case an action
$a$ and a pair $(x,y)$ are adjacent if $y=a(x)$. As an example, we apply our
bounds to the binary deletion channel and show that for the special case of an
i.i.d. input distribution and a range of deletion probabilities, our bounds
both outperform the best known bounds for the mutual information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06332</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06332</id><created>2015-07-22</created><authors><author><keyname>Shih</keyname><forenames>Kevin J.</forenames></author><author><keyname>Mallya</keyname><forenames>Arun</forenames></author><author><keyname>Singh</keyname><forenames>Saurabh</forenames></author><author><keyname>Hoiem</keyname><forenames>Derek</forenames></author></authors><title>Part Localization using Multi-Proposal Consensus for Fine-Grained
  Categorization</title><categories>cs.CV</categories><comments>BMVC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple deep learning framework to simultaneously predict
keypoint locations and their respective visibilities and use those to achieve
state-of-the-art performance for fine-grained classification. We show that by
conditioning the predictions on object proposals with sufficient image support,
our method can do well without complicated spatial reasoning. Instead,
inference methods with robustness to outliers, yield state-of-the-art for
keypoint localization. We demonstrate the effectiveness of our accurate
keypoint localization and visibility prediction on the fine-grained bird
recognition task with and without ground truth bird bounding boxes, and
outperform existing state-of-the-art methods by over 2%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06341</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06341</id><created>2015-07-22</created><updated>2015-09-12</updated><authors><author><keyname>Aravind</keyname><forenames>N. R.</forenames></author><author><keyname>Sandeep</keyname><forenames>R. B.</forenames></author><author><keyname>Sivadasan</keyname><forenames>Naveen</forenames></author></authors><title>Parameterized lower bound and NP-completeness of some $H$-free Edge
  Deletion problems</title><categories>cs.DS</categories><comments>15 pages, COCOA 15 accepted paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a graph $H$, the $H$-free Edge Deletion problem asks whether there exist
at most $k$ edges whose deletion from the input graph $G$ results in a graph
without any induced copy of $H$. We prove that $H$-free Edge Deletion is
NP-complete if $H$ is a graph with at least two edges and $H$ has a component
with maximum number of vertices which is a tree or a regular graph.
Furthermore, we obtain that these NP-complete problems cannot be solved in
parameterized subexponential time, i.e., in time $2^{o(k)}\cdot |G|^{O(1)}$,
unless Exponential Time Hypothesis fails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06346</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06346</id><created>2015-07-22</created><authors><author><keyname>Mattila</keyname><forenames>Robert</forenames></author><author><keyname>Rojas</keyname><forenames>Cristian R.</forenames></author><author><keyname>Wahlberg</keyname><forenames>Bo</forenames></author></authors><title>Evaluation of Spectral Learning for the Identification of Hidden Markov
  Models</title><categories>stat.ML cs.LG math.OC</categories><comments>This paper is accepted and will be published in The Proceedings of
  the 17th IFAC Symposium on System Identification (SYSID 2015), Beijing,
  China, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hidden Markov models have successfully been applied as models of discrete
time series in many fields. Often, when applied in practice, the parameters of
these models have to be estimated. The currently predominating identification
methods, such as maximum-likelihood estimation and especially
expectation-maximization, are iterative and prone to have problems with local
minima. A non-iterative method employing a spectral subspace-like approach has
recently been proposed in the machine learning literature. This paper evaluates
the performance of this algorithm, and compares it to the performance of the
expectation-maximization algorithm, on a number of numerical examples. We find
that the performance is mixed; it successfully identifies some systems with
relatively few available observations, but fails completely for some systems
even when a large amount of observations is available. An open question is how
this discrepancy can be explained. We provide some indications that it could be
related to how well-conditioned some system parameters are.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06352</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06352</id><created>2015-07-22</created><authors><author><keyname>Choi</keyname><forenames>David</forenames></author></authors><title>Co-clustering of Nonsmooth Graphons</title><categories>math.ST cs.SI stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance bounds are given for exploratory co-clustering/ blockmodeling of
bipartite graph data, where we assume the rows and columns of the data matrix
are samples from an arbitrary population. This is equivalent to assuming that
the data is generated from a nonsmooth graphon. It is shown that co-clusters
found by any method can be extended to the row and column populations, or
equivalently that the estimated blockmodel approximates a blocked version of
the generative graphon, with estimation error bounded by $O_P(n^{-1/2})$.
Analogous performance bounds are also given for degree-corrected blockmodels
and random dot product graphs, with error rates depending on the dimensionality
of the latent variable space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06353</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06353</id><created>2015-07-22</created><updated>2015-09-13</updated><authors><author><keyname>Yuzuguzel</keyname><forenames>Hidir</forenames></author><author><keyname>Niemi</keyname><forenames>Jari</forenames></author><author><keyname>Kiranyaz</keyname><forenames>Serkan</forenames></author><author><keyname>Gabbouj</keyname><forenames>Moncef</forenames></author><author><keyname>Heinz</keyname><forenames>Thomas</forenames></author></authors><title>ShakeMe: Key Generation From Shared Motion</title><categories>cs.CR</categories><comments>The paper is accepted to the 13th IEEE International Conference on
  Pervasive Intelligence and Computing (PIComp-2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Devices equipped with accelerometer sensors such as today's mobile devices
can make use of motion to exchange information. A typical example for shared
motion is shaking of two devices which are held together in one hand. Deriving
a shared secret (key) from shared motion, e.g. for device pairing, is an
obvious application for this. Only the keys need to be exchanged between the
peers and neither the motion data nor the features extracted from it. This
makes the pairing fast and easy. For this, each device generates an information
signal (key) independently of each other and, in order to pair, they should be
identical. The key is essentially derived by quantizing certain well
discriminative features extracted from the accelerometer data after an implicit
synchronization. In this paper, we aim at finding a small set of effective
features which enable a significantly simpler quantization procedure than the
prior art. Our tentative results with authentic accelerometer data show that
this is possible with a competent accuracy ($76$%) and key strength (entropy
approximately $15$ bits).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06365</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06365</id><created>2015-07-22</created><authors><author><keyname>Furcy</keyname><forenames>David</forenames></author><author><keyname>Summers</keyname><forenames>Scott M.</forenames></author></authors><title>Optimal self-assembly of finite shapes at temperature 1 in 3D</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Working in a three-dimensional variant of Winfree's abstract Tile Assembly
Model, we show that, for an arbitrary finite, connected shape $X \subset
\mathbb{Z}^2$, there is a tile set that uniquely self-assembles into a 3D
representation of a scaled-up version of $X$ at temperature 1 in 3D with
optimal program-size complexity (the &quot;program-size complexity&quot;, also known as
&quot;tile complexity&quot;, of a shape is the minimum number of tile types required to
uniquely self-assemble it). Moreover, our construction is &quot;just barely&quot; 3D in
the sense that it only places tiles in the $z = 0$ and $z = 1$ planes. Our
result is essentially a just-barely 3D temperature 1 simulation of a similar 2D
temperature 2 result by Soloveichik and Winfree (SICOMP 2007).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06368</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06368</id><created>2015-07-22</created><authors><author><keyname>Wetzels</keyname><forenames>Jos</forenames></author><author><keyname>Bokslag</keyname><forenames>Wouter</forenames></author></authors><title>Simple SIMON: FPGA implementations of the SIMON 64/128 Block Cipher</title><categories>cs.CR</categories><comments>20 pages</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this paper we will present various hardware architecture designs for
implementing the SIMON 64/128 block cipher as a cryptographic component
offering encryption, decryption and self-contained key-scheduling capabilities
and discuss the issues and design options we encountered and the tradeoffs we
made in implementing them. Finally, we will present the results of our hardware
architectures' implementation performances on the Xilinx Spartan-6 FPGA series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06369</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06369</id><created>2015-07-22</created><authors><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>Authorship Patterns in Computer Science Research in the Philippines</title><categories>cs.DL</categories><comments>13 pages, 6 figures</comments><journal-ref>Philippine Computing Journal 5(1):1-13, 2010</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We studied patterns of authorship in computer science~(CS) research in the
Philippines by using data mining and graph theory techniques on archives of
scientific papers presented in the Philippine Computer Science Congresses from
2000 to 2010 involving 326~papers written by 605~authors. We inferred from
these archives various graphs namely, a paper--author bipartite graph, a
co-authorship graph, and two mixing graphs. Our results show that the
scientific articles by Filipino computer scientists were generated at a rate of
33~papers per year, while the papers were written by an average of 2.64~authors
(maximum=13). The frequency distribution of the number of authors per paper
follows a power-law with a power of $\varphi=-2.04$ ($R^2=0.71$). The number of
Filipino CS researchers increases at an annual rate of 60~new scientists. The
researchers have written an average of 1.42~papers (maximum=20) and have
collaborated with 3.70~other computer scientists (maximum=54). The frequency
distribution of the number of papers per author follows a power law with
$\varphi=-1.88$ ($R^2=0.83$). This distribution closely agrees with Lotka's
{\em law of scientific productivity} having $\varphi\approx -2$. The number of
co-authors per author also follows a power-law with $\varphi=-1.65$
($R^2=0.80$). These results suggest that most CS~papers in the country were
written by scientists who prefer to work alone or at most in small groups.
These also suggest that few papers were written by scientists who were involved
in large collaboration efforts. The productivity of the Philippines' CS
researchers, as measured by their number of papers, is positively correlated
with their participation in collaborative research efforts, as measured by
their number of co-authors (Pearson $r=0.7425$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06370</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06370</id><created>2015-07-22</created><updated>2015-10-18</updated><authors><author><keyname>Ma</keyname><forenames>Tengyu</forenames></author><author><keyname>Wigderson</keyname><forenames>Avi</forenames></author></authors><title>Sum-of-Squares Lower Bounds for Sparse PCA</title><categories>cs.LG cs.CC math.ST stat.CO stat.ML stat.TH</categories><comments>to appear at NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper establishes a statistical versus computational trade-off for
solving a basic high-dimensional machine learning problem via a basic convex
relaxation method. Specifically, we consider the {\em Sparse Principal
Component Analysis} (Sparse PCA) problem, and the family of {\em
Sum-of-Squares} (SoS, aka Lasserre/Parillo) convex relaxations. It was well
known that in large dimension $p$, a planted $k$-sparse unit vector can be {\em
in principle} detected using only $n \approx k\log p$ (Gaussian or Bernoulli)
samples, but all {\em efficient} (polynomial time) algorithms known require $n
\approx k^2$ samples. It was also known that this quadratic gap cannot be
improved by the the most basic {\em semi-definite} (SDP, aka spectral)
relaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also
degree-4 SoS algorithms cannot improve this quadratic gap. This average-case
lower bound adds to the small collection of hardness results in machine
learning for this powerful family of convex relaxation algorithms. Moreover,
our design of moments (or &quot;pseudo-expectations&quot;) for this lower bound is quite
different than previous lower bounds. Establishing lower bounds for higher
degree SoS algorithms for remains a challenging problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06380</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06380</id><created>2015-07-23</created><authors><author><keyname>Salvania</keyname><forenames>Abigail C.</forenames></author><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>Information Spread Over an Internet-mediated Social Network: Phases,
  Speed, Width, and Effects of Promotion</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 9 figures, initially appeared in Proceedings (CDROM) of the
  8th National Conference on Information Technology Education, La Carmela de
  Boracay Convention Center, Boracay Island, Malay, Aklan, Philippines, 20-23
  October 2010</comments><journal-ref>Philippine Information Technology Journal 3(2):15-25, 2010</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this study, we looked at the effect of promotion in the speed and width of
spread of information on the Internet by tracking the diffusion of news
articles over a social network. Speed of spread means the number of readers
that the news has reached in a given time, while width of spread means how far
the story has travelled from the news originator within the social network.
After analyzing six stories in a 30-hour time span, we found out that the
lifetime of a story's popularity among the members of the social network has
three phases: Expansion, Front-page, and Saturation. Expansion phase starts
when a story is published and the article spreads from a source node to nodes
within a connected component of the social network. Front-page phase happens
when a news aggregator promotes the story in its front page resulting to the
story's faster rate of spread among the connected nodes while at the same time
spreading the article to nodes outside the original connected component of the
social network. Saturation phase is when the story ages and its rate of spread
within the social network slows down, suggesting popularity saturation among
the nodes. Within these three phases, we observed minimal changes on the width
of information spread as suggested by relatively low increase of the width of
the spread's diameter within the social network. We see that this paper
provides the various stakeholders a first-hand empirical data for modeling,
designing, and improving the current web-based services, specifically the IT
educators for designing and improving academic curricula, and for improving the
current web-enabled deployment of knowledge and online evaluation of skills.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06396</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06396</id><created>2015-07-23</created><updated>2015-11-26</updated><authors><author><keyname>Miridakis</keyname><forenames>Nikolaos I.</forenames></author><author><keyname>Tsiftsis</keyname><forenames>Theodoros A.</forenames></author><author><keyname>Alexandropoulos</keyname><forenames>George C.</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Green Cognitive Relaying: Opportunistically Switching Between Data
  Transmission and Energy Harvesting</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficiency has become an encouragement, and more than this, a
requisite for the design of next-generation wireless communications standards.
In current work, a dual-hop cognitive (secondary) relaying system is
considered, incorporating multiple amplify-and-forward relays, a rather
cost-effective solution. First, the secondary relays sense the wireless
channel, scanning for a primary network activity, and then convey their reports
to a secondary base station (SBS). Afterwards, the SBS, based on these reports
and its own estimation, decides cooperatively the presence of primary
transmission or not. In the former scenario, all the secondary nodes start to
harvest energy from the transmission of primary node(s). In the latter
scenario, the system initiates secondary communication via a best relay
selection policy. Performance evaluation of this system is thoroughly
investigated, by assuming realistic channel conditions, i.e., non-identical
link-distances, Rayleigh fading, and outdated channel estimation. The detection
and outage probabilities as well as the average harvested energy are derived as
new closed-form expressions. In addition, an energy efficiency optimization
problem is analytically formulated and solved, while a necessary condition in
terms of power consumption minimization for each secondary node is presented.
From a green communications standpoint, it turns out that energy harvesting
greatly enhances the resources of secondary nodes, especially when primary
activity is densely present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06397</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06397</id><created>2015-07-23</created><authors><author><keyname>Rezatofighi</keyname><forenames>Seyed Hamid</forenames></author><author><keyname>Gould</keyname><forenames>Stephen</forenames></author><author><keyname>Vo</keyname><forenames>Ba Tuong</forenames></author><author><keyname>Vo</keyname><forenames>Ba-Ngu</forenames></author><author><keyname>Mele</keyname><forenames>Katarina</forenames></author><author><keyname>Hartley</keyname><forenames>Richard</forenames></author></authors><title>Multi-Target Tracking with Time-Varying Clutter Rate and Detection
  Profile: Application to Time-lapse Cell Microscopy Sequences</title><categories>cs.CV</categories><doi>10.1109/TMI.2015.2390647</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantitative analysis of the dynamics of tiny cellular and sub-cellular
structures, known as particles, in time-lapse cell microscopy sequences
requires the development of a reliable multi-target tracking method capable of
tracking numerous similar targets in the presence of high levels of noise, high
target density, complex motion patterns and intricate interactions. In this
paper, we propose a framework for tracking these structures based on the random
finite set Bayesian filtering framework. We focus on challenging biological
applications where image characteristics such as noise and background intensity
change during the acquisition process. Under these conditions, detection
methods usually fail to detect all particles and are often followed by missed
detections and many spurious measurements with unknown and time-varying rates.
To deal with this, we propose a bootstrap filter composed of an estimator and a
tracker. The estimator adaptively estimates the required meta parameters for
the tracker such as clutter rate and the detection probability of the targets,
while the tracker estimates the state of the targets. Our results show that the
proposed approach can outperform state-of-the-art particle trackers on both
synthetic and real data in this regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06411</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06411</id><created>2015-07-23</created><authors><author><keyname>Francois</keyname><forenames>Olivier</forenames></author></authors><title>Arbitrariness of peer review: A Bayesian analysis of the NIPS experiment</title><categories>stat.OT cs.DL stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The principle of peer review is central to the evaluation of research, by
ensuring that only high-quality items are funded or published. But peer review
has also received criticism, as the selection of reviewers may introduce biases
in the system. In 2014, the organizers of the ``Neural Information Processing
Systems\rq\rq{} conference conducted an experiment in which $10\%$ of submitted
manuscripts (166 items) went through the review process twice. Arbitrariness
was measured as the conditional probability for an accepted submission to get
rejected if examined by the second committee. This number was equal to $60\%$,
for a total acceptance rate equal to $22.5\%$. Here we present a Bayesian
analysis of those two numbers, by introducing a hidden parameter which measures
the probability that a submission meets basic quality criteria. The standard
quality criteria usually include novelty, clarity, reproducibility, correctness
and no form of misconduct, and are met by a large proportions of submitted
items. The Bayesian estimate for the hidden parameter was equal to $56\%$
($95\%$CI: $ I = (0.34, 0.83)$), and had a clear interpretation. The result
suggested the total acceptance rate should be increased in order to decrease
arbitrariness estimates in future review processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06427</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06427</id><created>2015-07-23</created><authors><author><keyname>Roland</keyname><forenames>Michael</forenames></author><author><keyname>H&#xf6;lzl</keyname><forenames>Michael</forenames></author></authors><title>Evaluation of Contactless Smartcard Antennas</title><categories>cs.CR cs.CY</categories><comments>University of Applied Sciences Upper Austria, JR-Center u'smile,
  Technical report, 29 pages, 27 figures</comments><acm-class>C.2.0; C.3; H.5.2; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report summarizes the results of our evaluation of antennas of
contactless and dual interface smartcards and our ideas for user-switchable NFC
antennas. We show how to disassemble smartcards with contactless capabilities
in order to obtain the bare chip module and the bare antenna wire. We examine
the design of various smartcard antennas and present concepts to render the
contactless interface unusable. Finally, we present ideas and practical
experiments to make the contactless interface switchable by the end-user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06429</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06429</id><created>2015-07-23</created><authors><author><keyname>Gordo</keyname><forenames>Albert</forenames></author><author><keyname>Gaidon</keyname><forenames>Adrien</forenames></author><author><keyname>Perronnin</keyname><forenames>Florent</forenames></author></authors><title>Deep Fishing: Gradient Features from Deep Nets</title><categories>cs.CV</categories><comments>To appear at BMVC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Networks (ConvNets) have recently improved image recognition
performance thanks to end-to-end learning of deep feed-forward models from raw
pixels. Deep learning is a marked departure from the previous state of the art,
the Fisher Vector (FV), which relied on gradient-based encoding of local
hand-crafted features. In this paper, we discuss a novel connection between
these two approaches. First, we show that one can derive gradient
representations from ConvNets in a similar fashion to the FV. Second, we show
that this gradient representation actually corresponds to a structured matrix
that allows for efficient similarity computation. We experimentally study the
benefits of transferring this representation over the outputs of ConvNet
layers, and find consistent improvements on the Pascal VOC 2007 and 2012
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06452</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06452</id><created>2015-07-23</created><authors><author><keyname>Devooght</keyname><forenames>Robin</forenames></author><author><keyname>Kourtellis</keyname><forenames>Nicolas</forenames></author><author><keyname>Mantrach</keyname><forenames>Amin</forenames></author></authors><title>Dynamic Matrix Factorization with Priors on Unknown Values</title><categories>stat.ML cs.IR cs.LG</categories><comments>in the Proceedings of 21st ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advanced and effective collaborative filtering methods based on explicit
feedback assume that unknown ratings do not follow the same model as the
observed ones (\emph{not missing at random}). In this work, we build on this
assumption, and introduce a novel dynamic matrix factorization framework that
allows to set an explicit prior on unknown values. When new ratings, users, or
items enter the system, we can update the factorization in time independent of
the size of data (number of users, items and ratings). Hence, we can quickly
recommend items even to very recent users. We test our methods on three large
datasets, including two very sparse ones, in static and dynamic conditions. In
each case, we outrank state-of-the-art matrix factorization methods that do not
use a prior on unknown ratings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06462</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06462</id><created>2015-07-23</created><authors><author><keyname>Bruni</keyname><forenames>Roberto</forenames></author><author><keyname>Montanari</keyname><forenames>Ugo</forenames></author><author><keyname>Sammartino</keyname><forenames>Matteo</forenames></author></authors><title>A coalgebraic semantics for causality in Petri nets</title><categories>cs.LO</categories><comments>Accepted by Journal of Logical and Algebraic Methods in Programming</comments><doi>10.1016/j.jlamp.2015.07.003</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we revisit some pioneering efforts to equip Petri nets with
compact operational models for expressing causality. The models we propose have
a bisimilarity relation and a minimal representative for each equivalence
class, and they can be fully explained as coalgebras on a presheaf category on
an index category of partial orders. First, we provide a set-theoretic model in
the form of a a causal case graph, that is a labeled transition system where
states and transitions represent markings and firings of the net, respectively,
and are equipped with causal information. Most importantly, each state has a
poset representing causal dependencies among past events. Our first result
shows the correspondence with behavior structure semantics as proposed by
Trakhtenbrot and Rabinovich. Causal case graphs may be infinitely-branching and
have infinitely many states, but we show how they can be refined to get an
equivalent finitely-branching model. In it, states are equipped with
symmetries, which are essential for the existence of a minimal, often
finite-state, model. The next step is constructing a coalgebraic model. We
exploit the fact that events can be represented as names, and event generation
as name generation. Thus we can apply the Fiore-Turi framework: we model causal
relations as a suitable category of posets with action labels, and generation
of new events with causal dependencies as an endofunctor on this category. Then
we define a well-behaved category of coalgebras. Our coalgebraic model is still
infinite-state, but we exploit the equivalence between coalgebras over a class
of presheaves and History Dependent automata to derive a compact
representation, which is equivalent to our set-theoretical compact model.
Remarkably, state reduction is automatically performed along the equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06469</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06469</id><created>2015-07-23</created><authors><author><keyname>Gupta</keyname><forenames>Rakhi Misuriya</forenames></author></authors><title>MOBISPA: A Reference Framework for Mobile as a Personal Assistant</title><categories>cs.HC</categories><comments>12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile is taking center stage and becoming the device of preference for all
aspects of communication because of our increasingly on the go lifestyles. With
this the demands on mobile capability to execute increasingly complex
operations are also on the rise. However, despite improvements in device
computing power in the last couple of years a mobile device continues to have
limitations. Mobile driven everyday use cases are increasingly raising
expectations that rest on mobile technologies that are still evolving. A number
of fragmented approaches and solutions have been created that address various
requirements unique to mobility, however there is a lack of a single framework
that serves as a unifying reference for industry and solution architectures.
The paper addresses this concern through the specification of a comprehensive
reference framework for mobility that is generic and vendor neutral.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06477</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06477</id><created>2015-07-23</created><authors><author><keyname>Mizuno</keyname><forenames>Takayuki</forenames></author><author><keyname>Ohnishi</keyname><forenames>Takaaki</forenames></author><author><keyname>Watanabe</keyname><forenames>Tsutomu</forenames></author></authors><title>Novel and topical business news and their impact on stock market
  activities</title><categories>q-fin.ST cs.CY physics.soc-ph</categories><comments>8 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an indicator to measure the degree to which a particular news
article is novel, as well as an indicator to measure the degree to which a
particular news item attracts attention from investors. The novelty measure is
obtained by comparing the extent to which a particular news article is similar
to earlier news articles, and an article is regarded as novel if there was no
similar article before it. On the other hand, we say a news item receives a lot
of attention and thus is highly topical if it is simultaneously reported by
many news agencies and read by many investors who receive news from those
agencies. The topicality measure for a news item is obtained by counting the
number of news articles whose content is similar to an original news article
but which are delivered by other news agencies. To check the performance of the
indicators, we empirically examine how these indicators are correlated with
intraday financial market indicators such as the number of transactions and
price volatility. Specifically, we use a dataset consisting of over 90 million
business news articles reported in English and a dataset consisting of
minute-by-minute stock prices on the New York Stock Exchange and the NASDAQ
Stock Market from 2003 to 2014, and show that stock prices and transaction
volumes exhibited a significant response to a news article when it is novel and
topical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06495</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06495</id><created>2015-07-23</created><authors><author><keyname>Bousquet</keyname><forenames>Nicolas</forenames></author><author><keyname>Cai</keyname><forenames>Yang</forenames></author><author><keyname>Hunkenschr&#xf6;der</keyname><forenames>Christoph</forenames></author><author><keyname>Vetta</keyname><forenames>Adrian</forenames></author></authors><title>On the Economic Efficiency of the Combinatorial Clock Auction</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the 1990s spectrum auctions have been implemented world-wide. This has
provided for a practical examination of an assortment of auction mechanisms
and, amongst these, two simultaneous ascending price auctions have proved to be
extremely successful. These are the simultaneous multiround ascending auction
(SMRA) and the combinatorial clock auction (CCA). It has long been known that,
for certain classes of valuation functions, the SMRA provides good theoretical
guarantees on social welfare. However, no such guarantees were known for the
CCA.
  In this paper, we show that CCA does provide strong guarantees on social
welfare provided the price increment and stopping rule are well-chosen. This is
very surprising in that the choice of price increment has been used primarily
to adjust auction duration and the stopping rule has attracted little
attention. The main result is a polylogarithmic approximation guarantee for
social welfare when the maximum number of items demanded $\mathcal{C}$ by a
bidder is fixed. Specifically, we show that either the revenue of the CCA is at
least an $\Omega\Big(\frac{1}{\mathcal{C}^{2}\log n\log^2m}\Big)$-fraction of
the optimal welfare or the welfare of the CCA is at least an
$\Omega\Big(\frac{1}{\log n}\Big)$-fraction of the optimal welfare, where $n$
is the number of bidders and $m$ is the number of items. As a corollary, the
welfare ratio -- the worst case ratio between the social welfare of the optimum
allocation and the social welfare of the CCA allocation -- is at most
$O(\mathcal{C}^2 \cdot \log n \cdot \log^2 m)$. We emphasize that this latter
result requires no assumption on bidders valuation functions. Finally, we prove
that such a dependence on $\mathcal{C}$ is necessary. In particular, we show
that the welfare ratio of the CCA is at least $\Omega \Big(\mathcal{C} \cdot
\frac{\log m}{\log \log m}\Big)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06500</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06500</id><created>2015-07-18</created><authors><author><keyname>Zhuge</keyname><forenames>Hai</forenames></author></authors><title>Mapping Big Data into Knowledge Space with Cognitive
  Cyber-Infrastructure</title><categories>cs.AI</categories><comments>59 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data research has attracted great attention in science, technology,
industry and society. It is developing with the evolving scientific paradigm,
the fourth industrial revolution, and the transformational innovation of
technologies. However, its nature and fundamental challenge have not been
recognized, and its own methodology has not been formed. This paper explores
and answers the following questions: What is big data? What are the basic
methods for representing, managing and analyzing big data? What is the
relationship between big data and knowledge? Can we find a mapping from big
data into knowledge space? What kind of infrastructure is required to support
not only big data management and analysis but also knowledge discovery, sharing
and management? What is the relationship between big data and science paradigm?
What is the nature and fundamental challenge of big data computing? A
multi-dimensional perspective is presented toward a methodology of big data
computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06502</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06502</id><created>2015-07-23</created><authors><author><keyname>Caruso</keyname><forenames>Xavier</forenames><affiliation>IRMAR</affiliation></author></authors><title>Resultants and subresultants of p-adic polynomials</title><categories>math.NT cs.SC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of the stability of the computations of resultants and
subresultants of polynomials defined over complete discrete valuation rings
(e.g. Zp or k[[t]] where k is a field). We prove that Euclide-like algorithms
are highly unstable on average and we explain, in many cases, how one can
stabilize them without sacrifying the complexity. On the way, we completely
determine the distribution of the valuation of the principal subresultants of
two random monic p-adic polynomials having the same degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06504</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06504</id><created>2015-07-23</created><updated>2015-10-26</updated><authors><author><keyname>Jacob</keyname><forenames>Jean-Pascal</forenames></author><author><keyname>Dimiccoli</keyname><forenames>Mariella</forenames></author><author><keyname>Moisan</keyname><forenames>Lionel</forenames></author></authors><title>Active skeleton for bacteria modeling</title><categories>cs.CV</categories><comments>to appear in Computer Methods in Biomechanics and Biomedical
  Engineering: Imaging and Visualization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The investigation of spatio-temporal dynamics of bacterial cells and their
molecular components requires automated image analysis tools to track cell
shape properties and molecular component locations inside the cells. In the
study of bacteria aging, the molecular components of interest are protein
aggregates accumulated near bacteria boundaries. This particular location makes
very ambiguous the correspondence between aggregates and cells, since computing
accurately bacteria boundaries in phase-contrast time-lapse imaging is a
challenging task. This paper proposes an active skeleton formulation for
bacteria modeling which provides several advantages: an easy computation of
shape properties (perimeter, length, thickness, orientation), an improved
boundary accuracy in noisy images, and a natural bacteria-centered coordinate
system that permits the intrinsic location of molecular components inside the
cell. Starting from an initial skeleton estimate, the medial axis of the
bacterium is obtained by minimizing an energy function which incorporates
bacteria shape constraints. Experimental results on biological images and
comparative evaluation of the performances validate the proposed approach for
modeling cigar-shaped bacteria like Escherichia coli. The Image-J plugin of the
proposed method can be found online at http://fluobactracker.inrialpes.fr.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06515</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06515</id><created>2015-07-23</created><authors><author><keyname>Nicotri</keyname><forenames>Stefano</forenames></author><author><keyname>Tinelli</keyname><forenames>Eufemia</forenames></author><author><keyname>Amoroso</keyname><forenames>Nicola</forenames></author><author><keyname>Garuccio</keyname><forenames>Elena</forenames></author><author><keyname>Bellotti</keyname><forenames>Roberto</forenames></author></authors><title>Complex networks and public funding: the case of the 2007-2013 Italian
  program</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>22 pages, 9 figures</comments><journal-ref>EPJ Data Science 2015, 4:8</journal-ref><doi>10.1140/epjds/s13688-015-0047-z</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this paper we apply techniques of complex network analysis to data sources
representing public funding programs and discuss the importance of the
considered indicators for program evaluation. Starting from the Open Data
repository of the 2007-2013 Italian Program Programma Operativo Nazionale
'Ricerca e Competitivit\`a' (PON R&amp;C), we build a set of data models and
perform network analysis over them. We discuss the obtained experimental
results outlining interesting new perspectives that emerge from the application
of the proposed methods to the socio-economical evaluation of funded programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06521</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06521</id><created>2015-07-23</created><authors><author><keyname>Wang</keyname><forenames>Jue</forenames></author><author><keyname>Lee</keyname><forenames>Jemin</forenames></author><author><keyname>Wang</keyname><forenames>Fanggang</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author></authors><title>Jamming-Aided Secure Communication in Massive MIMO Rician Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the artificial noise-aided jamming design for a
transmitter equipped with large antenna array in Rician fading channels. We
figure out that when the number of transmit antennas tends to infinity, whether
the secrecy outage happens in a Rician channel depends on the geometric
locations of eavesdroppers. In this light, we first define and analytically
describe the secrecy outage region (SOR), indicating all possible locations of
an eavesdropper that can cause secrecy outage. After that, the secrecy outage
probability (SOP) is derived, and a jamming-beneficial range, i.e., the
distance range of eavesdroppers which enables uniform jamming to reduce the
SOP, is determined. Then, the optimal power allocation between messages and
artificial noise is investigated for different scenarios. Furthermore, to use
the jamming power more efficiently and further reduce the SOP, we propose
directional jamming that generates jamming signals at selected beams (mapped to
physical angles) only, and power allocation algorithms are proposed for the
cases with and without the information of the suspicious area, i.e., possible
locations of eavesdroppers. We further extend the discussions to multiuser and
multi-cell scenarios. At last, numerical results validate our conclusions and
show the effectiveness of our proposed jamming power allocation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06527</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06527</id><created>2015-07-23</created><updated>2015-08-27</updated><authors><author><keyname>Hausknecht</keyname><forenames>Matthew</forenames></author><author><keyname>Stone</keyname><forenames>Peter</forenames></author></authors><title>Deep Recurrent Q-Learning for Partially Observable MDPs</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Reinforcement Learning has yielded proficient controllers for complex
tasks. However, these controllers have limited memory and rely on being able to
perceive the complete game screen at each decision point. To address these
shortcomings, this article investigates the effects of adding recurrency to a
Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected
layer with a recurrent LSTM. The resulting \textit{Deep Recurrent Q-Network}
(DRQN), although capable of seeing only a single frame at each timestep,
successfully integrates information through time and replicates DQN's
performance on standard Atari games and partially observed equivalents
featuring flickering game screens. Additionally, when trained with partial
observations and evaluated with incrementally more complete observations,
DRQN's performance scales as a function of observability. Conversely, when
trained with full observations and evaluated with partial observations, DRQN's
performance degrades less than DQN's. Thus, given the same length of history,
recurrency is a viable alternative to stacking a history of frames in the DQN's
input layer and while recurrency confers no systematic advantage when learning
to play the game, the recurrent net can better adapt at evaluation time if the
quality of observations changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06529</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06529</id><created>2015-07-23</created><updated>2016-03-02</updated><authors><author><keyname>Malkov</keyname><forenames>Yury A.</forenames></author><author><keyname>Ponomarenko</keyname><forenames>Alexander</forenames></author></authors><title>Growing homophilic networks are natural navigable small worlds</title><categories>physics.soc-ph cs.SI</categories><comments>Updated version. 11 pages, 3 figures + SI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Navigability, an ability to find a logarithmically short path between
elements using only local information, is one of the most fascinating
properties of real-life networks. However, the exact mechanism responsible for
the formation of navigation properties remained unknown. We show that
navigability can be achieved by using only two ingredients present in the
majority of networks: network growth and local homophily, giving a persuasive
answer how the navigation appears in real-life networks. A very simple
algorithm produces hierarchical self-similar optimally wired navigable small
world networks with exponential degree distribution by using only local
information. Adding preferential attachment produces a scale-free network which
has shorter greedy paths, but worse (power law) scaling of the information
extraction locality (algorithmic complexity of a search). Introducing
saturation of the preferential attachment leads to truncated scale-free degree
distribution that offers a good tradeoff between these parameters and can be
useful for practical applications. Several features of the model are observed
in real-life networks, in particular in the brain neural networks, supporting
the earlier suggestions that they are navigable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06535</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06535</id><created>2015-07-23</created><authors><author><keyname>Fawzi</keyname><forenames>Alhussein</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Manitest: Are classifiers really invariant?</title><categories>cs.CV cs.LG stat.ML</categories><comments>BMVC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Invariance to geometric transformations is a highly desirable property of
automatic classifiers in many image recognition tasks. Nevertheless, it is
unclear to which extent state-of-the-art classifiers are invariant to basic
transformations such as rotations and translations. This is mainly due to the
lack of general methods that properly measure such an invariance. In this
paper, we propose a rigorous and systematic approach for quantifying the
invariance to geometric transformations of any classifier. Our key idea is to
cast the problem of assessing a classifier's invariance as the computation of
geodesics along the manifold of transformed images. We propose the Manitest
method, built on the efficient Fast Marching algorithm to compute the
invariance of classifiers. Our new method quantifies in particular the
importance of data augmentation for learning invariance from data, and the
increased invariance of convolutional neural networks with depth. We foresee
that the proposed generic tool for measuring invariance to a large class of
geometric transformations and arbitrary classifiers will have many applications
for evaluating and comparing classifiers based on their invariance, and help
improving the invariance of existing classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06538</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06538</id><created>2015-07-23</created><authors><author><keyname>Tiamiyu</keyname><forenames>Osuolale Abdulrahamon</forenames></author></authors><title>Algorithmization, requirements analysis and architectural challenges of
  TraConDa</title><categories>cs.NI cs.CR</categories><journal-ref>International Journal of Advanced Computer Research (IJACR),
  Volume-5, Issue-19, June-2015 ,pp.145-175</journal-ref><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Globally, there are so much information security threats on Internet that
even when data is encrypted, there is no guarantee that copy would not be
available to third-party, and eventually be decrypted. Thus, trusted routing
mechanism that inhibits availability of (encrypted or not) data being
transferred to third-party is proposed in this paper. Algorithmization,
requirements analysis and architectural challenges for its development are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06541</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06541</id><created>2015-07-23</created><authors><author><keyname>Brandstadt</keyname><forenames>Andreas</forenames></author><author><keyname>Mosca</keyname><forenames>Raffaele</forenames></author></authors><title>Dominating Induced Matchings for $P_8$-free Graphs in Polynomial Time</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G=(V,E)$ be a finite undirected graph. An edge set $E' \subseteq E$ is a
dominating induced matching (d.i.m.) in $G$ if every edge in $E$ is intersected
by exactly one edge of $E'$. The Dominating Induced Matching (DIM) problem asks
for the existence of a d.i.m. in $G$; this problem is also known as the
Efficient Edge Domination problem.
  The DIM problem is related to parallel resource allocation problems, encoding
theory and network routing. It is NP-complete even for very restricted graph
classes such as planar bipartite graphs with maximum degree three and is
solvable in linear time for $P_7$-free graphs. However, its complexity was open
for $P_k$-free graphs for any $k \ge 8$; $P_k$ denotes the chordless path with
$k$ vertices and $k-1$ edges. We show in this paper that the weighted DIM
problem is solvable in polynomial time for $P_8$-free graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06550</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06550</id><created>2015-07-23</created><updated>2015-11-11</updated><authors><author><keyname>Carreira</keyname><forenames>Joao</forenames></author><author><keyname>Agrawal</keyname><forenames>Pulkit</forenames></author><author><keyname>Fragkiadaki</keyname><forenames>Katerina</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Human Pose Estimation with Iterative Error Feedback</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical feature extractors such as Convolutional Networks (ConvNets)
have achieved impressive performance on a variety of classification tasks using
purely feedforward processing. Feedforward architectures can learn rich
representations of the input space but do not explicitly model dependencies in
the output spaces, that are quite structured for tasks such as articulated
human pose estimation or object segmentation. Here we propose a framework that
expands the expressive power of hierarchical feature extractors to encompass
both input and output spaces, by introducing top-down feedback. Instead of
directly predicting the outputs in one go, we use a self-correcting model that
progressively changes an initial solution by feeding back error predictions, in
a process we call Iterative Error Feedback (IEF). IEF shows excellent
performance on the task of articulated pose estimation in the challenging MPII
and LSP benchmarks, matching the state-of-the-art without requiring ground
truth scale annotation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06554</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06554</id><created>2015-07-23</created><authors><author><keyname>Bogaerts</keyname><forenames>Bart</forenames></author><author><keyname>Broeck</keyname><forenames>Guy Van den</forenames></author></authors><title>Knowledge Compilation of Logic Programs Using Approximation Fixpoint
  Theory</title><categories>cs.LO</categories><journal-ref>Theory and Practice of Logic Programming 15 (2015) 464-480</journal-ref><doi>10.1017/S1471068415000162</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To appear in Theory and Practice of Logic Programming (TPLP), Proceedings of
ICLP 2015
  Recent advances in knowledge compilation introduced techniques to compile
\emph{positive} logic programs into propositional logic, essentially exploiting
the constructive nature of the least fixpoint computation. This approach has
several advantages over existing approaches: it maintains logical equivalence,
does not require (expensive) loop-breaking preprocessing or the introduction of
auxiliary variables, and significantly outperforms existing algorithms.
Unfortunately, this technique is limited to \emph{negation-free} programs. In
this paper, we show how to extend it to general logic programs under the
well-founded semantics.
  We develop our work in approximation fixpoint theory, an algebraical
framework that unifies semantics of different logics. As such, our algebraical
results are also applicable to autoepistemic logic, default logic and abstract
dialectical frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06562</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06562</id><created>2015-07-23</created><authors><author><keyname>Varvello</keyname><forenames>Matteo</forenames></author><author><keyname>Schomp</keyname><forenames>Kyle</forenames></author><author><keyname>Naylor</keyname><forenames>David</forenames></author><author><keyname>Blackburn</keyname><forenames>Jeremy</forenames></author><author><keyname>Finamore</keyname><forenames>Alessandro</forenames></author><author><keyname>Papagiannaki</keyname><forenames>Kostantina</forenames></author></authors><title>To HTTP/2, or Not To HTTP/2, That Is The Question</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  As of February, 2015, HTTP/2, the update to the 16-year-old HTTP 1.1, is
officially complete. HTTP/2 aims to improve the Web experience by solving
well-known problems (e.g., head of line blocking and redundant headers), while
introducing new features (e.g., server push and content priority). On paper
HTTP/2 represents the future of the Web. Yet, it is unclear whether the Web
itself will, and should, hop on board. To shed some light on these questions,
we built a measurement platform that monitors HTTP/2 adoption and performance
across the Alexa top 1 million websites on a daily basis. Our system is live
and up-to-date results can be viewed at http://isthewebhttp2yet.com/. In this
paper, we report our initial findings from a 6 month measurement campaign
(November 2014 - May 2015). We find 13,000 websites reporting HTTP/2 support,
but only 600, mostly hosted by Google and Twitter, actually serving content. In
terms of speed, we find no significant benefits from HTTP/2 under stable
network conditions. More benefits appear in a 3G network where current Web
development practices make HTTP/2 more resilient to losses and delay variation
than previously believed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06565</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06565</id><created>2015-07-23</created><authors><author><keyname>Fattahi</keyname><forenames>Ehsan</forenames></author><author><keyname>Waluga</keyname><forenames>Christian</forenames></author><author><keyname>Wohlmuth</keyname><forenames>Barbara</forenames></author><author><keyname>R&#xfc;de</keyname><forenames>Ulrich</forenames></author></authors><title>Large scale lattice Boltzmann simulation for the coupling of free and
  porous media flow</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we investigate the interaction of free and porous media flow by
large scale lattice Boltzmann simulations. We study the transport phenomena at
the porous interface on multiple scales, i.e., we consider both,
computationally generated pore-scale geometries and homogenized models at a
macroscopic scale. The pore-scale results are compared to those obtained by
using different transmission models. Two-domain approaches with sharp interface
conditions, e.g., of Beavers--Joseph--Saffman type, as well as a single-domain
approach with a porosity depending viscosity are taken into account. For the
pore-scale simulations, we use a highly scalable communication-reducing scheme
with a robust second order boundary handling. We comment on computational
aspects of the pore-scale simulation and on how to generate pore-scale
geometries. The two-domain approaches depend sensitively on the choice of the
exact position of the interface, whereas a well-designed single-domain approach
can significantly better recover the averaged pore-scale results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06566</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06566</id><created>2015-07-23</created><authors><author><keyname>Law</keyname><forenames>Mark</forenames></author><author><keyname>Russo</keyname><forenames>Alessandra</forenames></author><author><keyname>Broda</keyname><forenames>Krysia</forenames></author></authors><title>Learning Weak Constraints in Answer Set Programming</title><categories>cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP),
  Proceedings of ICLP 2015</comments><journal-ref>Theory and Practice of Logic Programming 15 (2015) 511-525</journal-ref><doi>10.1017/S1471068415000198</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper contributes to the area of inductive logic programming by
presenting a new learning framework that allows the learning of weak
constraints in Answer Set Programming (ASP). The framework, called Learning
from Ordered Answer Sets, generalises our previous work on learning ASP
programs without weak constraints, by considering a new notion of examples as
ordered pairs of partial answer sets that exemplify which answer sets of a
learned hypothesis (together with a given background knowledge) are preferred
to others. In this new learning task inductive solutions are searched within a
hypothesis space of normal rules, choice rules, and hard and weak constraints.
We propose a new algorithm, ILASP2, which is sound and complete with respect to
our new learning framework. We investigate its applicability to learning
preferences in an interview scheduling problem and also demonstrate that when
restricted to the task of learning ASP programs without weak constraints,
ILASP2 can be much more efficient than our previously proposed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06576</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06576</id><created>2015-07-23</created><authors><author><keyname>Gebser</keyname><forenames>Martin</forenames></author><author><keyname>Harrison</keyname><forenames>Amelia</forenames></author><author><keyname>Kaminski</keyname><forenames>Roland</forenames></author><author><keyname>Lifschitz</keyname><forenames>Vladimir</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author></authors><title>Abstract Gringo</title><categories>cs.PL</categories><journal-ref>Theory and Practice of Logic Programming 15 (2015) 449-463</journal-ref><doi>10.1017/S1471068415000150</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper defines the syntax and semantics of the input language of the ASP
grounder GRINGO. The definition covers several constructs that were not
discussed in earlier work on the semantics of that language, including
intervals, pools, division of integers, aggregates with non-numeric values, and
lparse-style aggregate expressions. The definition is abstract in the sense
that it disregards some details related to representing programs by strings of
ASCII characters. It serves as a specification for GRINGO from Version 4.5 on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06580</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06580</id><created>2015-07-23</created><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Eldan</keyname><forenames>Ronen</forenames></author></authors><title>Multi-scale exploration of convex functions and bandit convex
  optimization</title><categories>math.MG cs.LG math.OC math.PR stat.ML</categories><comments>Preliminary version; 22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a new map from a convex function to a distribution on its
domain, with the property that this distribution is a multi-scale exploration
of the function. We use this map to solve a decade-old open problem in
adversarial bandit convex optimization by showing that the minimax regret for
this problem is $\tilde{O}(\mathrm{poly}(n) \sqrt{T})$, where $n$ is the
dimension and $T$ the number of rounds. This bound is obtained by studying the
dual Bayesian maximin regret via the information ratio analysis of Russo and
Van Roy, and then using the multi-scale exploration to solve the Bayesian
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06593</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06593</id><created>2015-07-23</created><authors><author><keyname>Ganesan</keyname><forenames>Ashwinkumar</forenames></author><author><keyname>Brantley</keyname><forenames>Kiante</forenames></author><author><keyname>Pan</keyname><forenames>Shimei</forenames></author><author><keyname>Chen</keyname><forenames>Jian</forenames></author></authors><title>LDAExplore: Visualizing Topic Models Generated Using Latent Dirichlet
  Allocation</title><categories>cs.IR cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present LDAExplore, a tool to visualize topic distributions in a given
document corpus that are generated using Topic Modeling methods. Latent
Dirichlet Allocation (LDA) is one of the basic methods that is predominantly
used to generate topics. One of the problems with methods like LDA is that
users who apply them may not understand the topics that are generated. Also,
users may find it difficult to search correlated topics and correlated
documents. LDAExplore, tries to alleviate these problems by visualizing topic
and word distributions generated from the document corpus and allowing the user
to interact with them. The system is designed for users, who have minimal
knowledge of LDA or Topic Modelling methods. To evaluate our design, we run a
pilot study which uses the abstracts of 322 Information Visualization papers,
where every abstract is considered a document. The topics generated are then
explored by users. The results show that users are able to find correlated
documents and group them based on topics that are similar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06594</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06594</id><created>2015-07-23</created><updated>2015-09-28</updated><authors><author><keyname>Kelly</keyname><forenames>Jack</forenames></author><author><keyname>Knottenbelt</keyname><forenames>William</forenames></author></authors><title>Neural NILM: Deep Neural Networks Applied to Energy Disaggregation</title><categories>cs.NE</categories><comments>To appear in ACM BuildSys'15, November 4--5, 2015, Seoul</comments><acm-class>I.2.6; I.5.2</acm-class><doi>10.1145/2821650.2821672</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy disaggregation estimates appliance-by-appliance electricity
consumption from a single meter that measures the whole home's electricity
demand. Recently, deep neural networks have driven remarkable improvements in
classification performance in neighbouring machine learning fields such as
image classification and automatic speech recognition. In this paper, we adapt
three deep neural network architectures to energy disaggregation: 1) a form of
recurrent neural network called `long short-term memory' (LSTM); 2) denoising
autoencoders; and 3) a network which regresses the start time, end time and
average power demand of each appliance activation. We use seven metrics to test
the performance of these algorithms on real aggregate power data from five
appliances. Tests are performed against a house not seen during training and
against houses seen during training. We find that all three neural nets achieve
better F1 scores (averaged over all five appliances) than either combinatorial
optimisation or factorial hidden Markov models and that our neural net
algorithms generalise well to an unseen house.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06601</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06601</id><created>2015-07-22</created><authors><author><keyname>Chertkov</keyname><forenames>Misha</forenames></author><author><keyname>Fisher</keyname><forenames>Michael</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author><author><keyname>Bent</keyname><forenames>Russell</forenames></author><author><keyname>Misra</keyname><forenames>Sidhant</forenames></author></authors><title>Pressure Fluctuations in Natural Gas Networks caused by Gas-Electric
  Coupling</title><categories>cs.SY</categories><comments>10 pages, 7 figures</comments><journal-ref>48th Hawaii International Conference on System Sciences, 2738-2747
  (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of hydraulic fracturing technology has dramatically increased
the supply and lowered the cost of natural gas in the United States, driving an
expansion of natural gas-fired generation capacity in several electrical
inter-connections. Gas-fired generators have the capability to ramp quickly and
are often utilized by grid operators to balance intermittency caused by wind
generation. The time-varying output of these generators results in time-varying
natural gas consumption rates that impact the pressure and line-pack of the gas
network. As gas system operators assume nearly constant gas consumption when
estimating pipeline transfer capacity and for planning operations, such
fluctuations are a source of risk to their system. Here, we develop a new
method to assess this risk. We consider a model of gas networks with
consumption modeled through two components: forecasted consumption and small
spatio-temporarily varying consumption due to the gas-fired generators being
used to balance wind. While the forecasted consumption is globally balanced
over longer time scales, the fluctuating consumption causes pressure
fluctuations in the gas system to grow diffusively in time with a diffusion
rate sensitive to the steady but spatially-inhomogeneous forecasted
distribution of mass flow. To motivate our approach, we analyze the effect of
fluctuating gas consumption on a model of the Transco gas pipeline that extends
from the Gulf of Mexico to the Northeast of the United States.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06616</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06616</id><created>2015-07-23</created><updated>2015-08-04</updated><authors><author><keyname>Orlin</keyname><forenames>James B.</forenames></author><author><keyname>Schulz</keyname><forenames>Andreas S.</forenames></author><author><keyname>Udwani</keyname><forenames>Rajan</forenames></author></authors><title>Robust Monotone Submodular Function Maximization</title><categories>cs.DS cs.DM math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Instances of monotone submodular function maximization with cardinality
constraint occur often in practical applications. One example is feature
selection in machine learning, where in many models, adding a new feature to an
existing set of features always improves the modeling power (monotonicity) and
the marginal benefit of adding a new feature decreases as we consider larger
sets (submodularity). However, we would like to choose a robust set of features
such that, there is not too much dependence on a small subset of chosen
features [Krause et al. (08), Globerson &amp; Roweis (06)].
  We consider a formulation of this problem that was formally introduced by
Krause et al. (08). It is not difficult to show that even if we look at
robustness to removing a single element, the problem is at least as hard as the
ordinary maximization problem, which is approximable up to a factor of $1-1/e$.
For this case of single element removal, we give: (i) an algorithm with
parameter $m$, that has asymptotic guarantee $(1-1/e)-\Omega(1/m)$ using
$O(n^{m+1})$ queries and (ii) a fast asymptotically 0.5547 approximate
algorithm. For the general case of subset removal, we give a fast algorithm
with asymptotic guarantee $0.285$. These are the first constant factor results
for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06617</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06617</id><created>2015-07-23</created><updated>2016-03-04</updated><authors><author><keyname>Bohi</keyname><forenames>Amine</forenames></author><author><keyname>Prandi</keyname><forenames>Dario</forenames></author><author><keyname>Guis</keyname><forenames>Vincente</forenames></author><author><keyname>Bouchara</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Gauthier</keyname><forenames>Jean-Paul</forenames></author></authors><title>Fourier descriptors based on the structure of the human primary visual
  cortex with applications to object recognition</title><categories>cs.CV</categories><comments>15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a supervised object recognition method using new
global features and inspired by the model of the human primary visual cortex V1
as the semidiscrete roto-translation group $SE(2,N) = \mathbb R^2\times \mathbb
Z_N$. The proposed technique is based on generalized Fourier descriptors on the
latter group, which are invariant to natural geometric transformations
(rotations, translations). These descriptors are then used to feed an SVM
classifier. We have tested our method against the COIL-100 image database and
the ORL face database, and compared it with other techniques based on
traditional descriptors, global and local. The obtained results have shown that
our approach looks extremely efficient and stable to noise, in presence of
which it outperforms the other techniques analyzed in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06620</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06620</id><created>2015-07-23</created><authors><author><keyname>Sep&#xfa;lveda</keyname><forenames>Alonso</forenames></author><author><keyname>Tizziotti</keyname><forenames>Guilherme</forenames></author></authors><title>Two-point AG codes on the GK maximal curves</title><categories>math.AG cs.IT math.IT</categories><comments>13 pages, 1 figure</comments><msc-class>14H55, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We determine de Weierstrass semigroup of a pair of certain rational points on
the GK-curves. We use this semigroup to obtain two-point AG codes with better
parameters than comparable one-point AG codes arising from these curves. These
parameters are new records in the MinT's tables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06634</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06634</id><created>2015-07-11</created><authors><author><keyname>Li</keyname><forenames>Hongbo</forenames></author><author><keyname>Huang</keyname><forenames>Lei</forenames></author><author><keyname>Shao</keyname><forenames>Changpeng</forenames></author><author><keyname>Dong</keyname><forenames>Lei</forenames></author></authors><title>Three-Dimensional Projective Geometry with Geometric Algebra</title><categories>math.MG cs.CG</categories><msc-class>15A66, 15A72, 51M35, 65D19</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The line geometric model of 3-D projective geometry has the nice property
that the Lie algebra sl(4) of 3-D projective transformations is isomorphic to
the bivector algebra of CL(3,3), and line geometry is closely related to the
classical screw theory for 3-D rigid-body motions. The canonical homomorphism
from SL(4) to Spin(3,3) is not satisfying because it is not surjective, and the
projective transformations of negative determinant do induce orthogonal
transformations in the Pl\&quot;ucker coordinate space of lines.
  This paper presents our contributions in developing a rigorous and convenient
algebraic framework for the study of 3-D projective geometry with Clifford
algebra. To overcome the unsatisfying defects of the Pl\&quot;ucker correspondence,
we propose a group Pin^{sp}(3,3) with Pin(3,3) as its normal subgroup, to
quadruple-cover the group of 3-D projective transformations and polarities. We
construct spinors in factored form that generate 3-D reflections and rigid-body
motions, and extend screw algebra from the Lie algebra of rigid-body motions to
other 6-D Lie subalgebras of sl(4), and construct the corresponding cross
products and virtual works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06667</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06667</id><created>2015-07-23</created><authors><author><keyname>Heath</keyname><forenames>Fenno F.</forenames><suffix>III</suffix></author><author><keyname>Hull</keyname><forenames>Richard</forenames></author><author><keyname>Khabiri</keyname><forenames>Elham</forenames></author><author><keyname>Riemer</keyname><forenames>Matthew</forenames></author><author><keyname>Sukaviriya</keyname><forenames>Noi</forenames></author><author><keyname>Vaculin</keyname><forenames>Roman</forenames></author></authors><title>Alexandria: Extensible Framework for Rapid Exploration of Social Media</title><categories>cs.IR cs.CY cs.HC cs.SI</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Alexandria system under development at IBM Research provides an
extensible framework and platform for supporting a variety of big-data
analytics and visualizations. The system is currently focused on enabling rapid
exploration of text-based social media data. The system provides tools to help
with constructing &quot;domain models&quot; (i.e., families of keywords and extractors to
enable focus on tweets and other social media documents relevant to a project),
to rapidly extract and segment the relevant social media and its authors, to
apply further analytics (such as finding trends and anomalous terms), and
visualizing the results. The system architecture is centered around a variety
of REST-based service APIs to enable flexible orchestration of the system
capabilities; these are especially useful to support knowledge-worker driven
iterative exploration of social phenomena. The architecture also enables rapid
integration of Alexandria capabilities with other social media analytics
system, as has been demonstrated through an integration with IBM Research's
SystemG. This paper describes a prototypical usage scenario for Alexandria,
along with the architecture and key underlying analytics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06672</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06672</id><created>2015-07-23</created><authors><author><keyname>Bazzi</keyname><forenames>Wael M.</forenames></author><author><keyname>Rastegarnia</keyname><forenames>Amir</forenames></author><author><keyname>Khalili</keyname><forenames>Azam</forenames></author></authors><title>A Reliability of Measurement Based Algorithm for Adaptive Estimation in
  Sensor Networks</title><categories>cs.SY</categories><comments>5 pages; 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the issue of reliability of measurements in
distributed adaptive estimation problem. To this aim, we assume a sensor
network with different observation noise variance among the sensors and propose
new estimation method based on incremental distributed least mean-square
(IDLMS) algorithm. The proposed method contains two phases: I) Estimation of
each sensors observation noise variance, and II) Estimation of the desired
parameter using the estimated observation variances. To deal with the
reliability of measurements, in the second phase of the proposed algorithm, the
step-size parameter is adjusted for each sensor according to its observation
noise variance. As our simulation results show, the proposed algorithm
considerably improves the performance of the IDLMS algorithm in the same
condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06673</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06673</id><created>2015-07-23</created><updated>2015-09-01</updated><authors><author><keyname>Vaidya</keyname><forenames>Tavish</forenames></author></authors><title>2001-2013: Survey and Analysis of Major Cyberattacks</title><categories>cs.CY cs.CR</categories><comments>Correction of vulnerabilities exploited in Operation Aurora attacks.
  Zero-day vulnerability in many versions on Microsoft Internet explorer was
  exploited and not in Adobe Reader</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Widespread and extensive use of computers and their interconnections in
almost all sectors like communications, finance, transportation, military,
governance, education, energy etc., have made them attractive targets for
adversaries to spy, disrupt or steal information by pressing of few keystrokes
from any part of the world. This paper presents a survey of major cyberattacks
from 2001 to 2013 and analyzes these attacks to understand the motivation,
targets and technique(s) employed by the attackers. Observed trends in
cyberattacks have also been discussed in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06677</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06677</id><created>2015-07-22</created><authors><author><keyname>Shekhawat</keyname><forenames>Krishnendra</forenames></author></authors><title>Connectivity Algorithm</title><categories>cs.DS</categories><msc-class>05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, for the given adjacency matrix of a graph, we present an
algorithm which checks the connectivity of a graph and computes all of its
connected components. Also, it is mathematically proved that the algorithm
presents all the desired results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06682</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06682</id><created>2015-07-23</created><updated>2015-09-07</updated><authors><author><keyname>Chen</keyname><forenames>Pin-Yu</forenames></author><author><keyname>Lien</keyname><forenames>Chia-Wei</forenames></author><author><keyname>Chu</keyname><forenames>Fu-Jen</forenames></author><author><keyname>Ting</keyname><forenames>Pai-Shun</forenames></author><author><keyname>Cheng</keyname><forenames>Shin-Ming</forenames></author></authors><title>Supervised Collective Classification for Crowdsourcing</title><categories>cs.SI cs.LG stat.ML</categories><comments>to appear in IEEE Global Communications Conference (GLOBECOM)
  Workshop on Networking and Collaboration Issues for the Internet of
  Everything</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing utilizes the wisdom of crowds for collective classification via
information (e.g., labels of an item) provided by labelers. Current
crowdsourcing algorithms are mainly unsupervised methods that are unaware of
the quality of crowdsourced data. In this paper, we propose a supervised
collective classification algorithm that aims to identify reliable labelers
from the training data (e.g., items with known labels). The reliability (i.e.,
weighting factor) of each labeler is determined via a saddle point algorithm.
The results on several crowdsourced data show that supervised methods can
achieve better classification accuracy than unsupervised methods, and our
proposed method outperforms other algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06689</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06689</id><created>2015-07-23</created><updated>2015-10-20</updated><authors><author><keyname>Gaggl</keyname><forenames>Sarah A.</forenames></author><author><keyname>Manthey</keyname><forenames>Norbert</forenames></author><author><keyname>Ronca</keyname><forenames>Alessandro</forenames></author><author><keyname>Wallner</keyname><forenames>Johannes P.</forenames></author><author><keyname>Woltran</keyname><forenames>Stefan</forenames></author></authors><title>Improved Answer-Set Programming Encodings for Abstract Argumentation</title><categories>cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP),
  Proceedings of ICLP 2015</comments><journal-ref>Theory and Practice of Logic Programming 15 (2015) 434-448</journal-ref><doi>10.1017/S1471068415000149</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of efficient solutions for abstract argumentation problems is a
crucial step towards advanced argumentation systems. One of the most prominent
approaches in the literature is to use Answer-Set Programming (ASP) for this
endeavor. In this paper, we present new encodings for three prominent
argumentation semantics using the concept of conditional literals in
disjunctions as provided by the ASP-system clingo. Our new encodings are not
only more succinct than previous versions, but also outperform them on standard
benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06692</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06692</id><created>2015-07-23</created><authors><author><keyname>Wahba</keyname><forenames>Yasmen</forenames></author><author><keyname>ElSalamouny</keyname><forenames>Ehab</forenames></author><author><keyname>ElTaweel</keyname><forenames>Ghada</forenames></author></authors><title>Improving the Performance of Multi-class Intrusion Detection Systems
  using Feature Reduction</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intrusion detection systems (IDS) are widely studied by researchers nowadays
due to the dramatic growth in network-based technologies. Policy violations and
unauthorized access is in turn increasing which makes intrusion detection
systems of great importance. Existing approaches to improve intrusion detection
systems focus on feature selection or reduction since some features are
irrelevant or redundant which when removed improve the accuracy as well as the
learning time. In this paper we propose a hybrid feature selection method using
Correlation-based Feature Selection and Information Gain. In our work we apply
adaptive boosting using na\&quot;ive Bayes as the weak (base) classifier. The key
point in our research is that we are able to improve the detection accuracy
with a reduced number of features while precisely determining the attack.
Experimental results showed that our proposed method achieved high accuracy
compared to methods using only 5-class problem. Correlation is done using
Greedy search strategy and na\&quot;ive Bayes as the classifier on the reduced
NSL-KDD dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06702</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06702</id><created>2015-07-23</created><authors><author><keyname>Firoz</keyname><forenames>Jesun Sahariar</forenames></author><author><keyname>Kanewala</keyname><forenames>Thejaka Amila</forenames></author><author><keyname>Zalewski</keyname><forenames>Marcin</forenames></author><author><keyname>Barnas</keyname><forenames>Martina</forenames></author><author><keyname>Lumsdaine</keyname><forenames>Andrew</forenames></author></authors><title>The Anatomy of Large-Scale Distributed Graph Algorithms</title><categories>cs.DC cs.DS cs.PF cs.SE</categories><acm-class>D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing complexity of the software/hardware stack of modern
supercomputers results in explosion of parameters. The performance analysis
becomes a truly experimental science, even more challenging in the presence of
massive irregularity and data dependency. We analyze how the existing body of
research handles the experimental aspect in the context of distributed graph
algorithms (DGAs). We distinguish algorithm-level contributions, often
prioritized by authors, from runtime-level concerns that are harder to place.
We show that the runtime is such an integral part of DGAs that experimental
results are difficult to interpret and extrapolate without understanding the
properties of the runtime used. We argue that in order to gain understanding
about the impact of runtimes, more information needs to be gathered. To begin
this process, we provide an initial set of recommendations for describing DGA
results based on our analysis of the current state of the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06707</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06707</id><created>2015-07-23</created><authors><author><keyname>Becchetti</keyname><forenames>Luca</forenames></author><author><keyname>Clementi</keyname><forenames>Andrea</forenames></author><author><keyname>Natale</keyname><forenames>Emanuele</forenames></author><author><keyname>Pasquale</keyname><forenames>Francesco</forenames></author></authors><title>Probabilistic Self-Stabilization</title><categories>cs.DC</categories><comments>arXiv admin note: text overlap with arXiv:1501.04822</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By using concrete scenarios, we present and discuss a new concept of
probabilistic Self-Stabilization in Distributed Systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06711</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06711</id><created>2015-07-23</created><updated>2015-07-29</updated><authors><author><keyname>Weng</keyname><forenames>Shitao</forenames></author><author><keyname>Chen</keyname><forenames>Shushan</forenames></author><author><keyname>Yu</keyname><forenames>Lei</forenames></author><author><keyname>Wu</keyname><forenames>Xuewei</forenames></author><author><keyname>Cai</keyname><forenames>Weicheng</forenames></author><author><keyname>Liu</keyname><forenames>Zhi</forenames></author><author><keyname>Li</keyname><forenames>Ming</forenames></author></authors><title>The SYSU System for the Interspeech 2015 Automatic Speaker Verification
  Spoofing and Countermeasures Challenge</title><categories>cs.SD cs.CL</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many existing speaker verification systems are reported to be vulnerable
against different spoofing attacks, for example speaker-adapted speech
synthesis, voice conversion, play back, etc. In order to detect these spoofed
speech signals as a countermeasure, we propose a score level fusion approach
with several different i-vector subsystems. We show that the acoustic level
Mel-frequency cepstral coefficients (MFCC) features, the phase level modified
group delay cepstral coefficients (MGDCC) and the phonetic level phoneme
posterior probability (PPP) tandem features are effective for the
countermeasure. Furthermore, feature level fusion of these features before
i-vector modeling also enhance the performance. A polynomial kernel support
vector machine is adopted as the supervised classifier. In order to enhance the
generalizability of the countermeasure, we also adopted the cosine similarity
and PLDA scoring as one-class classifications methods. By combining the
proposed i-vector subsystems with the OpenSMILE baseline which covers the
acoustic and prosodic information further improves the final performance. The
proposed fusion system achieves 0.29% and 3.26% EER on the development and test
set of the database provided by the INTERSPEECH 2015 automatic speaker
verification spoofing and countermeasures challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06722</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06722</id><created>2015-07-23</created><authors><author><keyname>Lin</keyname><forenames>Yunguo</forenames></author><author><keyname>Li</keyname><forenames>Yongming</forenames></author></authors><title>Exogenous Quantum Operator Logic Based on Density Operators</title><categories>cs.LO quant-ph</categories><comments>26 pages, 1 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although quantum logic by using exogenous approach has been proposed for
reasoning about closed quantum systems, an improvement would be worth to study
quantum logic based on density operators instead of unit vectors in the state
logic point of view. In order to achieve this, we build an exogenous quantum
operator logic(EQOL) based on density operators for reasoning about open
quantum systems. We show that this logic is sound and complete. Just as the
exogenous quantum propositional logic(EQPL), by applying exogenous approach,
EQOL is extended from the classical propositional logic, and is used to
describe the state logic based on density operators. As its applications, we
confirm the entanglement property about Bell states by reasoning and logical
argument, also verify the existence of eavesdropping about the basic BB84
protocol. As a novel type of mathematical formalism for open quantum systems,
we introduce an exogenous quantum Markov chain(EQMC) where its quantum states
are labelled using EQOL formulae. Then, an example is given to illustrate the
termination verification problem of a generalized quantum loop program
described using EQMC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06736</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06736</id><created>2015-07-24</created><updated>2016-01-19</updated><authors><author><keyname>Bah</keyname><forenames>Bubacarr</forenames></author><author><keyname>Ward</keyname><forenames>Rachel</forenames></author></authors><title>The sample complexity of weighted sparse approximation</title><categories>math.NA cs.CC cs.IT math.FA math.IT stat.CO</categories><comments>14 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For Gaussian sampling matrices, we provide bounds on the minimal number of
measurements $m$ required to achieve robust weighted sparse recovery guarantees
in terms of how well a given prior model for the sparsity support aligns with
the true underlying support. Our main contribution is that for a sparse vector
${\bf x} \in \mathbb{R}^N$ supported on an unknown set $\mathcal{S} \subset
\{1, \dots, N\}$ with $|\mathcal{S}|\leq k$, if $\mathcal{S}$ has
\emph{weighted cardinality} $\omega(\mathcal{S}) := \sum_{j \in \mathcal{S}}
\omega_j^2$, and if the weights on $\mathcal{S}^c$ exhibit mild growth,
$\omega_j^2 \geq \gamma \log(j/\omega(\mathcal{S}))$ for $j\in\mathcal{S}^c$
and $\gamma &gt; 0$, then the sample complexity for sparse recovery via weighted
$\ell_1$-minimization using weights $\omega_j$ is linear in the weighted
sparsity level, and $m = \mathcal{O}(\omega(\mathcal{S})/\gamma)$. This main
result is a generalization of special cases including a) the standard sparse
recovery setting where all weights $\omega_j \equiv 1$, and $m =
\mathcal{O}\left(k\log\left(N/k\right)\right)$; b) the setting where the
support is known a priori, and $m = \mathcal{O}(k)$; and c) the setting of
sparse recovery with prior information, and $m$ depends on how well the weights
are aligned with the support set $\mathcal{S}$. We further extend the results
in case c) to the setting of additive noise. Our results are {\em nonuniform}
that is they apply for a fixed support, unknown a priori, and the weights on
$\mathcal{S}$ do not all have to be smaller than the weights on $\mathcal{S}^c$
for our recovery results to hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06737</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06737</id><created>2015-07-24</created><authors><author><keyname>Kang</keyname><forenames>Hyo Seung</forenames></author><author><keyname>Kang</keyname><forenames>Myung Gil</forenames></author><author><keyname>Choi</keyname><forenames>Wan</forenames></author><author><keyname>Nosratinia</keyname><forenames>Aria</forenames></author></authors><title>The Degrees of Freedom of the Interference Channel with a Cognitive
  Relay under Delayed Feedback</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the interference channel with a cognitive relay (ICCR)
under delayed feedback. Three types of delayed feedback are studied: delayed
channel state information at the transmitter (CSIT), delayed output feedback,
and delayed Shannon feedback. Outer bounds are derived for the DoF region of
the two-user multiple-input multiple-output (MIMO) ICCR with delayed feedback
as well as without feedback. For the single-input single-output (SISO)
scenario, optimal schemes are proposed based on retrospective interference
alignment. It is shown that while a cognitive relay without feedback cannot
extend the sum-DoF beyond $1$ in the two-user SISO interference channel,
delayed feedback in the same scenario can extend the sum-DoF to $4/3$. For the
MIMO case, achievable schemes are obtained via extensions of retrospective
interference alignment, leading to DoF regions that meet the respective upper
bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06738</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06738</id><created>2015-07-24</created><authors><author><keyname>Agrawal</keyname><forenames>Shipra</forenames></author><author><keyname>Devanur</keyname><forenames>Nikhil R.</forenames></author></authors><title>Linear Contextual Bandits with Global Constraints and Objective</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the linear contextual bandit problem with global convex
constraints and a concave objective function. In each round, the outcome of
pulling an arm is a vector, that depends linearly on the context of that arm.
The global constraints require the average of these vectors to lie in a certain
convex set. The objective is a concave function of this average vector. This
problem turns out to be a common generalization of classic linear contextual
bandits (linContextual) [Auer 2003], bandits with concave rewards and convex
knapsacks (BwCR) [Agrawal, Devanur 2014], and the online stochastic convex
programming (OSCP) problem [Agrawal, Devanur 2015]. We present algorithms with
near-optimal regret bounds for this problem. Our bounds compare favorably to
results on the unstructured version of the problem [Agrawal et al. 2015,
Badanidiyuru et al. 2014] where the relation between the contexts and the
outcomes could be arbitrary, but the algorithm only competes against a fixed
set of policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06745</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06745</id><created>2015-07-24</created><authors><author><keyname>Wang</keyname><forenames>Tianyu</forenames></author><author><keyname>Sun</keyname><forenames>Yue</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Social Data Offloading in D2D-Enhanced Cellular Networks by Network
  Formation Games</title><categories>cs.GT cs.NI cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, cellular networks are severely overloaded by social-based services,
such as YouTube, Facebook and Twitter, in which thousands of clients subscribe
a common content provider (e.g., a popular singer) and download his/her content
updates all the time. Offloading such traffic through complementary networks,
such as a delay tolerant network formed by device-to-device (D2D)
communications between mobile subscribers, is a promising solution to reduce
the cellular burdens. In the existing solutions, mobile users are assumed to be
volunteers who selfishlessly deliver the content to every other user in
proximity while moving. However, practical users are selfish and they will
evaluate their individual payoffs in the D2D sharing process, which may highly
influence the network performance compared to the case of selfishless users. In
this paper, we take user selfishness into consideration and propose a network
formation game to capture the dynamic characteristics of selfish behaviors. In
the proposed game, we provide the utility function of each user and specify the
conditions under which the subscribers are guaranteed to converge to a stable
network. Then, we propose a practical network formation algorithm in which the
users can decide their D2D sharing strategies based on their historical
records. Simulation results show that user selfishness can highly degrade the
efficiency of data offloading, compared with ideal volunteer users. Also, the
decrease caused by user selfishness can be highly affected by the cost ratio
between the cellular transmission and D2D transmission, the access delays, and
mobility patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06746</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06746</id><created>2015-07-24</created><updated>2015-10-31</updated><authors><author><keyname>Iwata</keyname><forenames>M.</forenames></author><author><keyname>Akiyama</keyname><forenames>E.</forenames></author></authors><title>Heterogeneity of link weight and the evolution of cooperation</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>35 pages, 37 figures, Submitted to Physica A</comments><doi>10.1016/j.physa.2015.12.047</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the effect of &quot;heterogeneity of link weight&quot;,
heterogeneity of the frequency or amount of interactions among individuals, on
the evolution of cooperation. Based on an analysis of the evolutionary
prisoner's dilemma game on a weighted one-dimensional lattice network with
&quot;intra-individual heterogeneity&quot;, we confirm that moderate level of link-weight
heterogeneity can facilitate cooperation. Furthermore, we identify two key
mechanisms by which link-weight heterogeneity promotes the evolution of
cooperation: mechanisms for spread and maintenance of cooperation. We also
derive the corresponding conditions under which the mechanisms can work through
evolutionary dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06753</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06753</id><created>2015-07-24</created><authors><author><keyname>Deb</keyname><forenames>Novarun</forenames></author><author><keyname>Chaki</keyname><forenames>Nabendu</forenames></author><author><keyname>Ghose</keyname><forenames>Aditya</forenames></author></authors><title>Extracting State Transition Models from i* Models</title><categories>cs.SE</categories><comments>41 pages, 13 figures, 5 tables, submitted to the Journal of Systems
  and Software, pending resubmission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  i* models are inherently sequence agnostic. There is an immediate need to
bridge the gap between such a sequence agnostic model and an industry
implemented process modelling standard like Business Process Modelling Notation
(BPMN). This work is an attempt to build State Transition Models from i*
models. In this paper, we first spell out the Naive Algorithm formally, which
is on the lines of Formal Tropos. We demonstrate how the growth of the State
Transition Model Space can be mapped to the problem of finding the number of
possible paths between the Least Upper Bound (LUB) and the Greatest Lower Bound
(GLB) of a k-dimensional hypercube Lattice structure. We formally present the
mathematics for doing a quantitative analysis of the space growth. The Naive
Algorithm has its main drawback in the hyperexponential explosion caused in the
State Transition Model space. This is identified and the Semantic Implosion
Algorithm is proposed which exploits the temporal information embedded within
the i* model of an enterprise to reduce the rate of growth of the State
Transition Model space. A comparative quantitative analysis between the two
approaches concludes the superiority of the Semantic Implosion Algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06763</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06763</id><created>2015-07-24</created><updated>2015-07-26</updated><authors><author><keyname>Okada</keyname><forenames>Rina</forenames></author><author><keyname>Fukuchi</keyname><forenames>Kazuto</forenames></author><author><keyname>Kakizaki</keyname><forenames>Kazuya</forenames></author><author><keyname>Sakuma</keyname><forenames>Jun</forenames></author></authors><title>Differentially Private Analysis of Outliers</title><categories>stat.ML cs.CR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates differentially private analysis of distance-based
outliers. The problem of outlier detection is to find a small number of
instances that are apparently distant from the remaining instances. On the
other hand, the objective of differential privacy is to conceal presence (or
absence) of any particular instance. Outlier detection and privacy protection
are thus intrinsically conflicting tasks. In this paper, instead of reporting
outliers detected, we present two types of differentially private queries that
help to understand behavior of outliers. One is the query to count outliers,
which reports the number of outliers that appear in a given subspace. Our
formal analysis on the exact global sensitivity of outlier counts reveals that
regular global sensitivity based method can make the outputs too noisy,
particularly when the dimensionality of the given subspace is high. Noting that
the counts of outliers are typically expected to be relatively small compared
to the number of data, we introduce a mechanism based on the smooth upper bound
of the local sensitivity. The other is the query to discovery top-$h$ subspaces
containing a large number of outliers. This task can be naively achieved by
issuing count queries to each subspace in turn. However, the variation of
subspaces can grow exponentially in the data dimensionality. This can cause
serious consumption of the privacy budget. For this task, we propose an
exponential mechanism with a customized score function for subspace discovery.
To the best of our knowledge, this study is the first trial to ensure
differential privacy for distance-based outlier analysis. We demonstrated our
methods with synthesized datasets and real datasets. The experimental results
show that out method achieve better utility compared to the global sensitivity
based methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06765</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06765</id><created>2015-07-24</created><authors><author><keyname>Brandstadt</keyname><forenames>Andreas</forenames></author></authors><title>Weighted Efficient Domination for $P_5$-Free Graphs in Linear Time</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a finite undirected graph $G=(V,E)$, a vertex $v \in V$ {\em dominates}
itself and its neighbors. A vertex set $D \subseteq V$ in $G$ is an {\em
efficient dominating set} ({\em e.d.} for short) of $G$ if every vertex of $G$
is dominated by exactly one vertex of $D$. The {\em Efficient Domination} (ED)
problem, which asks for the existence of an e.d. in $G$, is known to be
NP-complete for $P_7$-free graphs but solvable in polynomial time for
$P_5$-free graphs. Very recently, it has been shown by Lokshtanov et al. and
independently by Mosca that ED is solvable in polynomial time for $P_6$-free
graphs.
  In this note, we show that, based on modular decomposition, ED is solvable in
linear time for $P_5$-free graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06768</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06768</id><created>2015-07-24</created><authors><author><keyname>Kuzmickaja</keyname><forenames>Ilona</forenames></author><author><keyname>Wang</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Graziotin</keyname><forenames>Daniel</forenames></author><author><keyname>Dodero</keyname><forenames>Gabriella</forenames></author><author><keyname>Abrahamsson</keyname><forenames>Pekka</forenames></author></authors><title>In Need of Creative Mobile Service Ideas? Forget Adults and Ask Young
  Children</title><categories>cs.CY cs.HC</categories><comments>48 pagers, 3 figures. Accepted for publication at SAGE Open</comments><acm-class>D.2.1; D.2.9; D.4.1; H.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well acknowledged that innovation is a key success factor in mobile
service domain. Having creative ideas is the first critical step in the
innovation process. Many studies suggest that customers are a valuable source
of creative ideas. However, the literature also shows that adults may be
constrained by existing technology frames, which are known to hinder
creativity. Instead young children (aged 7-12) are considered digital natives
yet are free from existing technology frames. This led us to study them as a
potential source for creative mobile service ideas. A set of 41,000 mobile
ideas obtained from a research project in 2006 granted us a unique opportunity
to study the mobile service ideas from young children. We randomly selected two
samples of ideas (N=400 each), one contained the ideas from young children, the
other from adults (aged 17-50). These ideas were evaluated by several
evaluators using an existing creativity framework. The results show that the
mobile service ideas from the young children are significantly more original,
transformational, implementable, and relevant than those from the adults.
Therefore, this study shows that young children are better sources of novel and
quality ideas than adults in the mobile services domain. This study bears
significant contributions to the creativity and innovation research. It also
indicates a new and valuable source for the companies that seek for creative
ideas for innovative products and services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06769</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06769</id><created>2015-07-24</created><authors><author><keyname>Zhang</keyname><forenames>Qian</forenames></author><author><keyname>Jiang</keyname><forenames>Ying</forenames></author><author><keyname>Ding</keyname><forenames>Liping</forenames></author></authors><title>Modelling and Analysis Network Security -- a PVCCS approach</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a probabilistic value-passing CCS (Calculus of
Communicating System) approach to model and analyze a typical network security
scenario with one attacker and one defender. By minimizing this model with
respect to probabilistic bisimulation and abstracting it through
graph-theoretic methods, two algorithms based on backward induction are
designed to compute Nash Equilibrium strategy and Social Optimal strategy
respectively. For each algorithm, the correctness is proved and an
implementation is realized. Finally, this approach is illustrated by a detailed
case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06778</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06778</id><created>2015-07-24</created><authors><author><keyname>Dasseville</keyname><forenames>Ingmar</forenames></author><author><keyname>van der Hallen</keyname><forenames>Matthias</forenames></author><author><keyname>Janssens</keyname><forenames>Gerda</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author></authors><title>Semantics of templates in a compositional framework for building logics</title><categories>cs.LO</categories><journal-ref>Theory and Practice of Logic Programming 15 (2015) 681-695</journal-ref><doi>10.1017/S1471068415000319</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a growing need for abstractions in logic specification languages
such as FO(.) and ASP. One technique to achieve these abstractions are
templates (sometimes called macros). While the semantics of templates are
virtually always described through a syntactical rewriting scheme, we present
an alternative view on templates as second order definitions. To extend the
existing definition construct of FO(.) to second order, we introduce a powerful
compositional framework for defining logics by modular integration of logic
constructs specified as pairs of one syntactical and one semantical inductive
rule. We use the framework to build a logic of nested second order definitions
suitable to express templates. We show that under suitable restrictions, the
view of templates as macros is semantically correct and that adding them does
not extend the descriptive complexity of the base logic, which is in line with
results of existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06796</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06796</id><created>2015-07-24</created><updated>2015-09-24</updated><authors><author><keyname>Keimel</keyname><forenames>Klaus</forenames><affiliation>Technische Universite Darmstadt</affiliation></author></authors><title>Weak upper topologies and duality for cones</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:21) 2015</journal-ref><doi>10.2168/LMCS-11(3:21)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In functional analysis it is well known that every linear functional defined
on the dual of a locally convex vector space which is continuous for the weak
topology is the evaluation at a uniquely determined point of the given vector
space. M. Schroeder and A. Simpson have obtained a similar result for lower
semicontinuous linear functionals on the cone of all Scott-continuous
valuations on a topological space endowed with the weak upper topology, an
asymmetric version of the weak topology. This result has given rise to several
proofs, originally by the Schroeder and Simpson themselves and, more recently,
by the author of these Notes and by J. Goubault-Larrecq. The proofs developed
from very technical arguments to more and more conceptual ones. The present
Note continues on this line, presenting a conceptual approach inspired by
classical functional analysis which may prove useful in other situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06800</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06800</id><created>2015-07-24</created><updated>2015-08-20</updated><authors><author><keyname>Marshall</keyname><forenames>Emily Abernethy</forenames></author><author><keyname>Yepremyan</keyname><forenames>Liana</forenames></author><author><keyname>Gaslowitz</keyname><forenames>Zach</forenames></author></authors><title>The Characterization of planar, 4-connected, K_{2,5}-minor-free graphs</title><categories>math.CO cs.DM</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that every planar, 4-connected, K2;5-minor- free graph is the square
of a cycle of even length at least six.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06802</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06802</id><created>2015-07-24</created><authors><author><keyname>Krijthe</keyname><forenames>Jesse H.</forenames></author><author><keyname>Loog</keyname><forenames>Marco</forenames></author></authors><title>Implicitly Constrained Semi-Supervised Least Squares Classification</title><categories>stat.ML cs.LG</categories><comments>12 pages, 2 figures, 1 table. The Fourteenth International Symposium
  on Intelligent Data Analysis (2015), Saint-Etienne, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel semi-supervised version of the least squares classifier.
This implicitly constrained least squares (ICLS) classifier minimizes the
squared loss on the labeled data among the set of parameters implied by all
possible labelings of the unlabeled data. Unlike other discriminative
semi-supervised methods, our approach does not introduce explicit additional
assumptions into the objective function, but leverages implicit assumptions
already present in the choice of the supervised least squares classifier. We
show this approach can be formulated as a quadratic programming problem and its
solution can be found using a simple gradient descent procedure. We prove that,
in a certain way, our method never leads to performance worse than the
supervised classifier. Experimental results corroborate this theoretical result
in the multidimensional case on benchmark datasets, also in terms of the error
rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06803</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06803</id><created>2015-07-24</created><authors><author><keyname>Romero</keyname><forenames>E.</forenames></author><author><keyname>Mazzanti</keyname><forenames>F.</forenames></author><author><keyname>Delgado</keyname><forenames>J.</forenames></author></authors><title>A Neighbourhood-Based Stopping Criterion for Contrastive Divergence
  Learning</title><categories>cs.NE cs.LG</categories><comments>7 pages. arXiv admin note: substantial text overlap with
  arXiv:1312.6062</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Restricted Boltzmann Machines (RBMs) are general unsupervised learning
devices to ascertain generative models of data distributions. RBMs are often
trained using the Contrastive Divergence learning algorithm (CD), an
approximation to the gradient of the data log-likelihood. A simple
reconstruction error is often used as a stopping criterion for CD, although
several authors
\cite{schulz-et-al-Convergence-Contrastive-Divergence-2010-NIPSw,
fischer-igel-Divergence-Contrastive-Divergence-2010-ICANN} have raised doubts
concerning the feasibility of this procedure. In many cases the evolution curve
of the reconstruction error is monotonic while the log-likelihood is not, thus
indicating that the former is not a good estimator of the optimal stopping
point for learning. However, not many alternatives to the reconstruction error
have been discussed in the literature. In this manuscript we investigate simple
alternatives to the reconstruction error, based on the inclusion of information
contained in neighboring states to the training set, as a stopping criterion
for CD learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06812</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06812</id><created>2015-07-24</created><updated>2015-12-18</updated><authors><author><keyname>Chondros</keyname><forenames>Nikos</forenames></author><author><keyname>Zhang</keyname><forenames>Bingsheng</forenames></author><author><keyname>Zacharias</keyname><forenames>Thomas</forenames></author><author><keyname>Diamantopoulos</keyname><forenames>Panos</forenames></author><author><keyname>Maneas</keyname><forenames>Stathis</forenames></author><author><keyname>Patsonakis</keyname><forenames>Christos</forenames></author><author><keyname>Delis</keyname><forenames>Alex</forenames></author><author><keyname>Kiayias</keyname><forenames>Aggelos</forenames></author><author><keyname>Roussopoulos</keyname><forenames>Mema</forenames></author></authors><title>D-DEMOS: A distributed, end-to-end verifiable, internet voting system</title><categories>cs.CR cs.DC</categories><comments>17 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  E-voting systems have emerged as a powerful technology for improving
democracy by reducing election cost, increasing voter participation, and even
allowing voters to directly verify the entire election procedure. Prior
internet voting systems have single points of failure, which may result in the
compromise of availability, voter secrecy, or integrity of the election
results. In this paper, we present the design, implementation, security
analysis, and evaluation of D-DEMOS, a complete e-voting system that is
distributed, privacy-preserving and end-to-end verifiable. Our system includes
a fully asynchronous vote collection subsystem that provides immediate
assurance to the voter her vote was recorded as cast, without requiring
cryptographic operations on behalf of the voter. We also include a distributed,
replicated and fault-tolerant Bulletin Board component, that stores all
necessary election-related information, and allows any party to read and verify
the complete election process. Finally, we also incorporate trustees, i.e.,
individuals who control election result production while guaranteeing privacy
and end-to-end-verifiability as long as their strong majority is honest. Our
system is the first e-voting system whose voting operation is human verifiable,
i.e., a voter can vote over the web, even when her web client stack is
potentially unsafe, without sacrificing her privacy, and still be assured her
vote was recorded as cast. Additionally, a voter can outsource election
auditing to third parties, still without sacrificing privacy. Finally, as the
number of auditors increases, the probability of election fraud going
undetected is diminished exponentially. We provide a model and security
analysis of the system. We implement a prototype of the complete system, we
measure its performance experimentally, and we demonstrate its ability to
handle large-scale elections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06820</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06820</id><created>2015-07-24</created><authors><author><keyname>Farina</keyname><forenames>Marcello</forenames></author><author><keyname>Carli</keyname><forenames>Ruggero</forenames></author></authors><title>Partition-based Distributed Kalman Filter with plug and play features</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a novel partition-based distributed state estimation
scheme for non-overlapping subsystems based on Kalman filter. The estimation
scheme is designed in order to account, in a rigorous fashion, for dynamic
coupling terms between subsystems, and for the uncertainty related to the state
estimates performed by the neighboring subsystems. The online implementation of
the proposed estimation scheme is scalable, since it involves (i) small-scale
matrix operations to be carried out by the estimator embedded in each subsystem
and (ii) neighbor-to-neighbor transmission of a limited amount of data. We
provide theoretical conditions ensuring the estimation convergence.
Reconfigurability of the proposed estimation scheme is allowed in case of plug
and play operations. Simulation tests are provided to illustrate the
effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06821</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06821</id><created>2015-07-24</created><updated>2015-08-18</updated><authors><author><keyname>Eitel</keyname><forenames>Andreas</forenames></author><author><keyname>Springenberg</keyname><forenames>Jost Tobias</forenames></author><author><keyname>Spinello</keyname><forenames>Luciano</forenames></author><author><keyname>Riedmiller</keyname><forenames>Martin</forenames></author><author><keyname>Burgard</keyname><forenames>Wolfram</forenames></author></authors><title>Multimodal Deep Learning for Robust RGB-D Object Recognition</title><categories>cs.CV cs.LG cs.NE cs.RO</categories><comments>Final version submitted to IROS'2015, results unchanged,
  reformulation of some text passages in abstract and introduction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust object recognition is a crucial ingredient of many, if not all,
real-world robotics applications. This paper leverages recent progress on
Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture
for object recognition. Our architecture is composed of two separate CNN
processing streams - one for each modality - which are consecutively combined
with a late fusion network. We focus on learning with imperfect sensor data, a
typical problem in real-world robotics tasks. For accurate learning, we
introduce a multi-stage training methodology and two crucial ingredients for
handling depth data with CNNs. The first, an effective encoding of depth
information for CNNs that enables learning without the need for large depth
datasets. The second, a data augmentation scheme for robust learning with depth
images by corrupting them with realistic noise patterns. We present
state-of-the-art results on the RGB-D object dataset and show recognition in
challenging RGB-D real-world noisy settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06827</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06827</id><created>2015-07-24</created><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Chen</keyname><forenames>Jiashu</forenames></author><author><keyname>Filos-Ratsikas</keyname><forenames>Aris</forenames></author><author><keyname>Mackenzie</keyname><forenames>Simon</forenames></author><author><keyname>Mattei</keyname><forenames>Nicholas</forenames></author></authors><title>Egalitarianism of Random Assignment Mechanisms</title><categories>cs.GT</categories><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the egalitarian welfare aspects of random assignment mechanisms
when agents have unrestricted cardinal utilities over the objects. We give
bounds on how well different random assignment mechanisms approximate the
optimal egalitarian value and investigate the effect that different well-known
properties like ordinality, envy-freeness, and truthfulness have on the
achievable egalitarian value. Finally, we conduct detailed experiments
analyzing the tradeoffs between efficiency with envy-freeness or truthfulness
using two prominent random assignment mechanisms --- random serial dictatorship
and the probabilistic serial mechanism --- for different classes of utility
functions and distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06829</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06829</id><created>2015-07-24</created><authors><author><keyname>Posch</keyname><forenames>Lisa</forenames></author><author><keyname>Bleier</keyname><forenames>Arnim</forenames></author><author><keyname>Schaer</keyname><forenames>Philipp</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author></authors><title>The Polylingual Labeled Topic Model</title><categories>cs.CL cs.IR cs.LG</categories><comments>Accepted for publication at KI 2015 (38th edition of the German
  Conference on Artificial Intelligence)</comments><acm-class>G.3; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the Polylingual Labeled Topic Model, a model which
combines the characteristics of the existing Polylingual Topic Model and
Labeled LDA. The model accounts for multiple languages with separate topic
distributions for each language while restricting the permitted topics of a
document to a set of predefined labels. We explore the properties of the model
in a two-language setting on a dataset from the social science domain. Our
experiments show that our model outperforms LDA and Labeled LDA in terms of
their held-out perplexity and that it produces semantically coherent topics
which are well interpretable by human subjects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06832</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06832</id><created>2015-07-24</created><authors><author><keyname>Gupta</keyname><forenames>Isha</forenames></author><author><keyname>Serb</keyname><forenames>Alexantrou</forenames></author><author><keyname>Khiat</keyname><forenames>Ali</forenames></author><author><keyname>Zeitler</keyname><forenames>Ralf</forenames></author><author><keyname>Vassanelli</keyname><forenames>Stefano</forenames></author><author><keyname>Prodromakis</keyname><forenames>Themistoklis</forenames></author></authors><title>Memristive integrative sensors for neuronal activity</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of advanced neuronal interfaces offers great promise for linking
brain functions to electronics. A major bottleneck in achieving this is
real-time processing of big data that imposes excessive requirements on
bandwidth, energy and computation capacity; limiting the overall number of
bio-electronic links. Here, we present a novel monitoring system concept that
exploits the intrinsic properties of memristors for processing neural
information in real time. We demonstrate that the inherent voltage thresholds
of solid-state TiOx memristors can be useful for discriminating significant
neural activity, i.e. spiking events, from noise. When compared with a
multi-dimensional, principal component feature space threshold detector, our
system is capable of recording the majority of significant events, without
resorting to computationally heavy off-line processing. We also show a
memristive integrating sensing array that discriminates neuronal activity
recorded in-vitro. We prove that information on spiking event amplitude is
simultaneously transduced and stored as non-volatile resistive state
transitions, allowing for more efficient data compression, demonstrating the
memristors' potential for building scalable, yet energy efficient on-node
processors for big data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06833</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06833</id><created>2015-07-24</created><authors><author><keyname>Matth&#xe9;</keyname><forenames>Maximilian</forenames></author><author><keyname>Gaspar</keyname><forenames>Ivan</forenames></author><author><keyname>Mendes</keyname><forenames>Luciano</forenames></author><author><keyname>Zhang</keyname><forenames>Dan</forenames></author></authors><title>Comparison between GFDM and VOFDM</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document provides a comparison of the transmission techniques used in
Generalized Frequency Division Multiplexing (GFDM) and Vector-OFDM (VOFDM).
Within the document both systems are coarsely described and common and distinct
properties are highlighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06836</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06836</id><created>2015-07-24</created><updated>2015-12-31</updated><authors><author><keyname>Arrighi</keyname><forenames>Pablo</forenames></author><author><keyname>Dowek</keyname><forenames>Gilles</forenames></author></authors><title>Discrete geodesics and cellular automata</title><categories>cs.DM gr-qc</categories><comments>13 pages, 3 figures</comments><msc-class>53C22, 68Q80, 37B15, 37N20, 83C27</msc-class><acm-class>G.1.0; F.1.1; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a dynamical notion of discrete geodesics, understood as
straightest trajectories in discretized curved spacetime. The notion is
generic, as it is formulated in terms of a general deviation function, but
readily specializes to metric spaces such as discretized pseudo-riemannian
manifolds. It is effective: an algorithm for computing these geodesics
naturally follows, which allows numerical validation---as shown by computing
the perihelion shift of a Mercury-like planet. It is consistent, in the
continuum limit, with the standard notion of timelike geodesics in a
pseudo-riemannian manifold. Whether the algorithm fits within the framework of
cellular automata is discussed at length. KEYWORDS: Discrete connection,
parallel transport, general relativity, Regge calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06837</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06837</id><created>2015-07-24</created><authors><author><keyname>Fix</keyname><forenames>Jeremy</forenames></author><author><keyname>Frezza-buet</keyname><forenames>Herve</forenames></author></authors><title>YARBUS : Yet Another Rule Based belief Update System</title><categories>cs.CL cs.AI</categories><comments>Source code at : https://github.com/jeremyfix/dstc</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new rule based system for belief tracking in dialog systems.
Despite the simplicity of the rules being considered, the proposed belief
tracker ranks favourably compared to the previous submissions on the second and
third Dialog State Tracking challenges. The results of this simple tracker
allows to reconsider the performances of previous submissions using more
elaborate techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06838</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06838</id><created>2015-07-24</created><updated>2016-02-19</updated><authors><author><keyname>Castrill&#xf3;n-Santana</keyname><forenames>M.</forenames></author><author><keyname>Lorenzo-Navarro</keyname><forenames>J.</forenames></author><author><keyname>Ram&#xf3;n-Balmaseda</keyname><forenames>E.</forenames></author></authors><title>Descriptors and regions of interest fusion for gender classification in
  the wild. Comparison and combination with Convolutional Neural Networks</title><categories>cs.CV</categories><comments>Revised version containing 12 pages. This revision includes newer
  referenes, results with CNN, fusion of local descriptors amd CNN and corrects
  different typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gender classification (GC) has achieved high accuracy in different
experimental evaluations based mostly on inner facial details. However, these
results do not generalize well in unrestricted datasets and particularly in
cross-database experiments, where the performance drops drastically. In this
paper, we analyze the state-of-the-art GC accuracy on three large datasets:
MORPH, LFW and GROUPS. We discuss their respective difficulties and bias,
concluding that the most challenging and wildest complexity is present in
GROUPS. This dataset covers hard conditions such as low resolution imagery and
cluttered background. Firstly, we analyze in depth the performance of different
descriptors extracted from the face and its local context on this dataset.
Selecting the bests and studying their most suitable combination allows us to
design a solution that beats any previously published results for GROUPS with
the Dago's protocol, reaching an accuracy over 94.2%, reducing the gap with
other simpler datasets. The chosen solution based on local descriptors is later
evaluated in a cross-database scenario with the three mentioned datasets, and
full dataset 5-fold cross validation. The achieved results are compared with a
Convolutional Neural Network approach, achieving rather similar marks. Finally,
a solution is proposed combining both focuses, exhibiting great
complementarity, boosting GC performance to beat previously published results
in GC both cross-database, and full in-database evaluations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06841</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06841</id><created>2015-07-24</created><authors><author><keyname>Zhang</keyname><forenames>Jiawei</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author><author><keyname>Lv</keyname><forenames>Yuanhua</forenames></author></authors><title>Organizational Chart Inference</title><categories>cs.SI cs.CY</categories><comments>10 pages, 9 figures, 1 table. The paper is accepted by KDD 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, to facilitate the communication and cooperation among employees, a
new family of online social networks has been adopted in many companies, which
are called the &quot;enterprise social networks&quot; (ESNs). ESNs can provide employees
with various professional services to help them deal with daily work issues.
Meanwhile, employees in companies are usually organized into different
hierarchies according to the relative ranks of their positions. The company
internal management structure can be outlined with the organizational chart
visually, which is normally confidential to the public out of the privacy and
security concerns. In this paper, we want to study the IOC (Inference of
Organizational Chart) problem to identify company internal organizational chart
based on the heterogeneous online ESN launched in it. IOC is very challenging
to address as, to guarantee smooth operations, the internal organizational
charts of companies need to meet certain structural requirements (about its
depth and width). To solve the IOC problem, a novel unsupervised method Create
(ChArT REcovEr) is proposed in this paper, which consists of 3 steps: (1)
social stratification of ESN users into different social classes, (2)
supervision link inference from managers to subordinates, and (3) consecutive
social classes matching to prune the redundant supervision links. Extensive
experiments conducted on real-world online ESN dataset demonstrate that Create
can perform very well in addressing the IOC problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06852</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06852</id><created>2015-07-24</created><authors><author><keyname>Cristia</keyname><forenames>Maximiliano</forenames></author><author><keyname>Rossi</keyname><forenames>Gianfranco</forenames></author><author><keyname>Frydman</keyname><forenames>Claudia</forenames></author></authors><title>Adding Partial Functions to Constraint Logic Programming with Sets</title><categories>cs.PL</categories><journal-ref>Theory and Practice of Logic Programming 15 (2015) 651-665</journal-ref><doi>10.1017/S1471068415000290</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partial functions are common abstractions in formal specification notations
such as Z, B and Alloy. Conversely, executable programming languages usually
provide little or no support for them. In this paper we propose to add partial
functions as a primitive feature to a Constraint Logic Programming (CLP)
language, namely {log}. Although partial functions could be programmed on top
of {log}, providing them as first-class citizens adds valuable flexibility and
generality to the form of set-theoretic formulas that the language can safely
deal with. In particular, the paper shows how the {log} constraint solver is
naturally extended in order to accommodate for the new primitive constraints
dealing with partial functions. Efficiency of the new version is empirically
assessed by running a number of non-trivial set-theoretical goals involving
partial functions, obtained from specifications written in Z.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06856</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06856</id><created>2015-07-24</created><updated>2015-07-29</updated><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Carmi</keyname><forenames>Paz</forenames></author><author><keyname>Damian</keyname><forenames>Mirela</forenames></author><author><keyname>De Carufel</keyname><forenames>Jean-Lou</forenames></author><author><keyname>Hill</keyname><forenames>Darryl</forenames></author><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Liu</keyname><forenames>Yuyang</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>On the Stretch Factor of Convex Polyhedra whose Vertices are (Almost) on
  a Sphere</title><categories>cs.CG</categories><comments>Corrected some typos</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Let $P$ be a convex polyhedron in $\mathbb{R}^3$. The skeleton of $P$ is the
graph whose vertices and edges are the vertices and edges of $P$, respectively.
We prove that, if these vertices are on the unit-sphere, the skeleton is a
$(0.999 \cdot \pi)$-spanner. If the vertices are very close to this sphere,
then the skeleton is not necessarily a spanner. For the case when the boundary
of $P$ is between two concentric spheres of radii $1$ and $R&gt;1$, and the angles
in all faces are at least $\theta$, we prove that the skeleton is a
$t$-spanner, where $t$ depends only on $R$ and $\theta$. One of the ingredients
in the proof is a tight upper bound on the geometric dilation of a convex cycle
that is contained in an annulus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06857</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06857</id><created>2015-07-24</created><authors><author><keyname>Gidion</keyname><forenames>Gerd</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author><author><keyname>Meadows</keyname><forenames>Ken</forenames></author><author><keyname>Grosch</keyname><forenames>Michael</forenames></author></authors><title>Media Usage in Post-Secondary Education and Implications for Teaching
  and Learning</title><categories>cs.CY</categories><journal-ref>EAI Endorsed Transactions on e-Learning, 14(3):1-17, 2014</journal-ref><doi>10.4108/el.1.4.e3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Web 2.0 has permeated academic life. The use of online information
services in post-secondary education has led to dramatic changes in faculty
teaching methods as well as in the learning and study behavior of students. At
the same time, traditional information media, such as textbooks and printed
handouts, still form the basic pillars of teaching and learning. This paper
reports the results of a survey about media usage in teaching and learning
conducted with Western University students and instructors, highlighting trends
in the usage of new and traditional media in higher education by instructors
and students. In addition, the survey comprises part of an international
research program in which 20 universities from 10 countries are currently
participating. Further, the study will hopefully become a part of the ongoing
discussion of practices and policies that purport to advance the effective use
of media in teaching and learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06858</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06858</id><created>2015-07-24</created><updated>2015-09-19</updated><authors><author><keyname>Potvin</keyname><forenames>Pascal</forenames></author><author><keyname>Nabaee</keyname><forenames>Mahdy</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author><author><keyname>Nguyen</keyname><forenames>Kim-Khoa</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>Micro Service Cloud Computing Pattern for Next Generation Networks</title><categories>cs.DC</categories><comments>12 pages, submitted for EAI International Conference on Smart
  Sustainable City Technologies</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The falling trend in the revenue of traditional telephony services has
attracted attention to new IP based services. The IP Multimedia System (IMS) is
a key architecture which provides the necessary platform for delivery of new
multimedia services. However, current implementations of IMS do not offer
automatic scalability or elastisity for the growing number of customers.
Although the cloud computing paradigm has shown many promising characteristics
for web applications, it is still failing to meet the requirements for
telecommunication applications. In this paper, we present some related cloud
computing patterns and discuss their adaptations for implementation of IMS or
other telecommunication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06865</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06865</id><created>2015-07-24</created><authors><author><keyname>Bilge</keyname><forenames>Yunus Can</forenames></author><author><keyname>&#xc7;a&#x11f;atay</keyname><forenames>Do&#x11f;ukan</forenames></author><author><keyname>Gen&#xe7;</keyname><forenames>Beg&#xfc;m</forenames></author><author><keyname>Sar&#x131;</keyname><forenames>Mecit</forenames></author><author><keyname>Akcan</keyname><forenames>H&#xfc;seyin</forenames></author><author><keyname>Evrendilek</keyname><forenames>Cem</forenames></author></authors><title>All Colors Shortest Path Problem</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All Colors Shortest Path problem defined on an undirected graph aims at
finding a shortest, possibly non-simple, path where every color occurs at least
once, assuming that each vertex in the graph is associated with a color known
in advance. To the best of our knowledge, this paper is the first to define and
investigate this problem. Even though the problem is computationally similar to
generalized minimum spanning tree, and the generalized traveling salesman
problems, allowing for non-simple paths where a node may be visited multiple
times makes All Colors Shortest Path problem novel and computationally unique.
In this paper we prove that All Colors Shortest Path problem is NP-hard, and
does not lend itself to a constant factor approximation. We also propose
several heuristic solutions for this problem based on LP-relaxation, simulated
annealing, ant colony optimization, and genetic algorithm, and provide
extensive simulations for a comparative analysis of them. The heuristics
presented are not the standard implementations of the well known heuristic
algorithms, but rather sophisticated models tailored for the problem in hand.
This fact is acknowledged by the very promising results reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06866</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06866</id><created>2015-07-24</created><authors><author><keyname>Munro</keyname><forenames>J. Ian</forenames></author><author><keyname>Nekrich</keyname><forenames>Yakov</forenames></author></authors><title>Compressed Data Structures for Dynamic Sequences</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of storing a dynamic string $S$ over an alphabet
$\Sigma=\{\,1,\ldots,\sigma\,\}$ in compressed form. Our representation
supports insertions and deletions of symbols and answers three fundamental
queries: $\mathrm{access}(i,S)$ returns the $i$-th symbol in $S$,
$\mathrm{rank}_a(i,S)$ counts how many times a symbol $a$ occurs among the
first $i$ positions in $S$, and $\mathrm{select}_a(i,S)$ finds the position
where a symbol $a$ occurs for the $i$-th time. We present the first
fully-dynamic data structure for arbitrarily large alphabets that achieves
optimal query times for all three operations and supports updates with
worst-case time guarantees. Ours is also the first fully-dynamic data structure
that needs only $nH_k+o(n\log\sigma)$ bits, where $H_k$ is the $k$-th order
entropy and $n$ is the string length. Moreover our representation supports
extraction of a substring $S[i..i+\ell]$ in optimal $O(\log n/\log\log n +
\ell/\log_{\sigma}n)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06873</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06873</id><created>2015-07-24</created><authors><author><keyname>Ahmed</keyname><forenames>Faheem</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author><author><keyname>Bouktif</keyname><forenames>Salah</forenames></author><author><keyname>Campbell</keyname><forenames>Piers</forenames></author></authors><title>Soft Skills and Software Development: A Reflection from the Software
  Industry</title><categories>cs.SE</categories><journal-ref>International Journal of Information Processing and Management,
  4(3):171-191, 2013</journal-ref><doi>10.4156/ijipm.vol14.issue3.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review the literature relating to soft skills and the software engineering
and information systems domain before describing a study based on 650 job
advertisements posted on well-known recruitment sites from a range of
geographical locations including, North America, Europe, Asia and Australia.
The study makes use of nine defined soft skills to assess the level of demand
for each of these skills related to individual job roles within the software
industry. This work reports some of the vital statistics from industry about
the requirements of soft skills in various roles of software development
phases. The work also highlights the variation in the types of skills required
for each of the roles. We found that currently although the software industry
is paying attention to soft skills up to some extent while hiring but there is
a need to further acknowledge the role of these skills in software development.
The objective of this paper is to analyze the software industry soft skills
requirements for various software development positions, such as system
analyst, designer, programmer, and tester. We pose two research questions,
namely, (1) What soft skills are appropriate to different software development
lifecycle roles, and (2) Up to what extend does the software industry consider
soft skills when hiring an employee. The study suggests that there is a further
need of acknowledgment of the significance of soft skills from employers in
software industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06877</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06877</id><created>2015-07-24</created><authors><author><keyname>Doncieux</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Li&#xe9;nard</keyname><forenames>Jean</forenames></author><author><keyname>Girard</keyname><forenames>Beno&#xee;t</forenames></author><author><keyname>Hamdaoui</keyname><forenames>Mohamed</forenames></author><author><keyname>Chaskalovic</keyname><forenames>Jo&#xeb;l</forenames></author></authors><title>Multi-objective analysis of computational models</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational models are of increasing complexity and their behavior may in
particular emerge from the interaction of different parts. Studying such models
becomes then more and more difficult and there is a need for methods and tools
supporting this process. Multi-objective evolutionary algorithms generate a set
of trade-off solutions instead of a single optimal solution. The availability
of a set of solutions that have the specificity to be optimal relative to
carefully chosen objectives allows to perform data mining in order to better
understand model features and regularities. We review the corresponding work,
propose a unifying framework, and highlight its potential use. Typical
questions that such a methodology allows to address are the following: what are
the most critical parameters of the model? What are the relations between the
parameters and the objectives? What are the typical behaviors of the model? Two
examples are provided to illustrate the capabilities of the methodology. The
features of a flapping-wing robot are thus evaluated to find out its
speed-energy relation, together with the criticality of its parameters. A
neurocomputational model of the Basal Ganglia brain nuclei is then considered
and its most salient features according to this methodology are presented and
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06878</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06878</id><created>2015-07-24</created><authors><author><keyname>Gall</keyname><forenames>Fran&#xe7;ois Le</forenames></author><author><keyname>Nakajima</keyname><forenames>Shogo</forenames></author></authors><title>Quantum Algorithm for Triangle Finding in Sparse Graphs</title><categories>quant-ph cs.CC cs.DS</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a quantum algorithm for triangle finding over sparse
graphs that improves over the previous best quantum algorithm for this task by
Buhrman et al. [SIAM Journal on Computing, 2005]. Our algorithm is based on the
recent $\tilde O(n^{5/4})$-query algorithm given by Le Gall [FOCS 2014] for
triangle finding over dense graphs (here $n$ denotes the number of vertices in
the graph). We show in particular that triangle finding can be solved with
$O(n^{5/4-\epsilon})$ queries for some constant $\epsilon&gt;0$ whenever the graph
has at most $O(n^{2-c})$ edges for some constant $c&gt;0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06881</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06881</id><created>2015-07-24</created><authors><author><keyname>Sagari</keyname><forenames>Shweta</forenames></author><author><keyname>Baysting</keyname><forenames>Samuel</forenames></author><author><keyname>Saha</keyname><forenames>Dola</forenames></author><author><keyname>Seskar</keyname><forenames>Ivan</forenames></author><author><keyname>Trappe</keyname><forenames>Wade</forenames></author><author><keyname>Raychaudhuri</keyname><forenames>Dipankar</forenames></author></authors><title>Coordinated Dynamic Spectrum Management of LTE-U and Wi-Fi Networks</title><categories>cs.IT cs.NI cs.PF math.IT</categories><comments>Accepted paper at IEEE DySPAN 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the co-existence of Wi-Fi and LTE in emerging
unlicensed frequency bands which are intended to accommodate multiple radio
access technologies. Wi-Fi and LTE are the two most prominent access
technologies being deployed today, motivating further study of the inter-system
interference arising in such shared spectrum scenarios as well as possible
techniques for enabling improved co-existence. An analytical model for
evaluating the baseline performance of co-existing Wi-Fi and LTE is developed
and used to obtain baseline performance measures. The results show that both
Wi-Fi and LTE networks cause significant interference to each other and that
the degradation is dependent on a number of factors such as power levels and
physical topology. The model-based results are partially validated via
experimental evaluations using USRP based SDR platforms on the ORBIT testbed.
Further, inter-network coordination with logically centralized radio resource
management across Wi-Fi and LTE systems is proposed as a possible solution for
improved co-existence. Numerical results are presented showing significant
gains in both Wi-Fi and LTE performance with the proposed inter-network
coordination approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06882</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06882</id><created>2015-07-24</created><authors><author><keyname>Raza</keyname><forenames>Arif</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Contributors Preference in Open Source Software Usability: An Empirical
  Study</title><categories>cs.SE</categories><journal-ref>International Journal of Software Engineering &amp; Applications
  (IJSEA), 1(2):45-64, 2010</journal-ref><doi>10.5121/ijsea.2010.1204</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fact that the number of users of open source software (OSS) is
practically un-limited and that ultimately the software quality is determined
by end users experience, makes the usability an even more critical quality
attribute than it is for proprietary software. With the sharp increase in use
of open source projects by both individuals and organizations, the level of
usability and related issues must be addressed more seriously. The research
model of this empirical investigation studies and establishes the relationship
between the key usability factors from contributors perspective and OSS
usability. A data set of 78 OSS contributors that includes architects,
designers, developers, testers and users from 22 open source projects of varied
size has been used to study the research model. The results of this study
provide empirical evidence by indicating that the highlighted key factors play
a significant role in improving OSS usability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06886</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06886</id><created>2015-07-24</created><authors><author><keyname>Li</keyname><forenames>Jianing</forenames></author><author><keyname>Deng</keyname><forenames>Yingpu</forenames></author></authors><title>Nonexistence of two classes of generalized bent functions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We obtain new nonexistence results of generalized bent functions from
$\{Z^n}_q$ to $\Z_q$ (called type $[n,q]$) in the case that there exist
cyclotomic integers in $ \Z[\zeta_{q}]$ with absolute value $q^{\frac{n}{2}}$.
This result generalize the previous two scattered nonexistence results
$[n,q]=[1,2\times7]$ of Pei \cite{Pei} and $[3,2\times 23^e]$ of Jiang-Deng
\cite{J-D} to a generalized class. In the last section, we remark that this
method can apply to the GBF from $\Z^n_2$ to $\Z_m$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06888</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06888</id><created>2015-07-24</created><authors><author><keyname>Raza</keyname><forenames>Arif</forenames></author><author><keyname>Zaka-ul-Mustafa</keyname></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Do Personality Profiles Differ in the Pakistani Software Industry and
  Academia - A Case Study</title><categories>cs.SE</categories><journal-ref>International Journal of Software Engineering, 3(4):60-66, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effects of personality profiles and human factors in software engineering
(SE) have been studied from different perspectives, such as: software life
cycle, team performance, software quality attributes, and so on. This study
intends to compare personality profiles of software engineers in academia and
industry. In this survey we have collected personality profiles of software
engineers from academia and the local industry in Pakistan. According to the
Myers- Briggs Type Indicator (MBTI) instrument, the most prominent personality
type among Pakistani academicians is a combination of Introversion, Sensing,
Thinking, and Judging (ISTJ). However the most dominant personality type among
software engineers in the Pakistani software industry is a combination of
Extroversion, Sensing, Thinking, and Judging (ESTJ). The results of study
establish that software engineers working in industry are mostly Extroverts as
compared to those in academia who tend to be Introverts. The dimensions:
Sensing, Thinking, and Judging (STJ), however, remain common in the dominant
personality types of software engineers, both in Pakistani software industry
and academia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06893</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06893</id><created>2015-07-24</created><authors><author><keyname>Raza</keyname><forenames>Arif</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Do Open Source Software Developers Listen to Their Users?</title><categories>cs.SE</categories><journal-ref>First Monday: Peer-Reviewed Open Journal on the Internet,
  17(3):1-9, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In application software, the satisfaction of target users makes the software
more acceptable. Open Source Software (OSS) systems have neither the physical
nor the commercial boundaries of proprietary software, thus users from all over
the world can interact with them. This free access is advantageous, as
increasing numbers of users are able to access OSS; there are more chances of
improvement. This study examines the way users feedback is handled by OSS
developers. In our survey, we have also inquired whether OSS developers consult
professional usability experts to improve their projects. According to the
results, majority of OSS developers neither consider usability as their top
priority nor do they consult usability experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06896</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06896</id><created>2015-07-24</created><authors><author><keyname>Raza</keyname><forenames>Arif</forenames></author><author><keyname>Zaka-u-Mustafa</keyname></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Personality Dimensions and Temperaments of Engineering Professors and
  Students - A Survey</title><categories>cs.SE</categories><journal-ref>Journal of Computing, 3(12):13-20, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research work aims to study personality profiles and temperaments of
Pakistani software engineering professors and students. In this survey we have
collected personality profiles of 18 professors and 92 software engineering
students. According to the Myers-Briggs Type Indicator (MBTI) instrument, the
most prominent personality type among professors as well as among students is a
combination of Introversion, Sensing, Thinking, and Judging (ISTJ). The study
shows ITs (Introverts and Thinking) and IJs (Introverts and Judging) are the
leading temperaments among the professors. About the students data, the results
of the study indicate SJs (Sensing and Judging) and ISs (Introverts and
Sensing) as the dominant temperaments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06897</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06897</id><created>2015-07-24</created><authors><author><keyname>Ahmed</keyname><forenames>Faheem</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>A Business Maturity Model of Software Product Line Engineering</title><categories>cs.SE</categories><journal-ref>Information Systems Frontiers, 13(4):543-560, 2011</journal-ref><doi>10.1007/s10796-010-9230-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent past, software product line engineering has become one of the
most promising practices in software industry with the potential to
substantially increase the software development productivity. Software product
line engineering approach spans the dimensions of business, architecture,
software engineering process and organization. The increasing popularity of
software product line engineering in the software industry necessitates a
process maturity evaluation methodology. Accordingly, this paper presents a
business maturity model of software product line, which is a methodology to
evaluate the current maturity of the business dimension of a software product
line in an organization. This model examines the coordination between product
line engineering and the business aspects of software product line. It
evaluates the maturity of the business dimension of software product line as a
function of how a set of business practices are aligned with product line
engineering in an organization. Using the model presented in this paper, we
conducted two case studies and reported the assessment results. This research
contributes towards establishing a comprehensive and unified strategy for a
process maturity evaluation of software product lines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06900</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06900</id><created>2015-07-24</created><updated>2015-12-29</updated><authors><author><keyname>Mueller</keyname><forenames>Markus P.</forenames></author><author><keyname>Pastena</keyname><forenames>Michele</forenames></author></authors><title>A generalization of majorization that characterizes Shannon entropy</title><categories>cs.IT math-ph math.IT math.MP quant-ph</categories><comments>10 pages; see also arXiv:1409.3258. v2: several minor corrections and
  simplifications; notably, Corollary 3 was already known in Ref. [10]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a binary relation on the finite discrete probability
distributions which generalizes notions of majorization that have been studied
in quantum information theory. Motivated by questions in thermodynamics, our
relation describes the transitions induced by bistochastic maps in the presence
of additional auxiliary systems which may become correlated in the process. We
show that this relation is completely characterized by Shannon entropy H, which
yields an interpretation of H in resource-theoretic terms, and admits a
particularly simple proof of a known characterization of H in terms of natural
information-theoretic properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06901</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06901</id><created>2015-07-24</created><authors><author><keyname>Ahmed</keyname><forenames>Faheem</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>An Architecture Process Maturity Model of Software Product Line
  Engineering</title><categories>cs.SE</categories><journal-ref>Innovations in Systems and Software Engineering: A NASA Journal,
  7(3):191-207, 2011</journal-ref><doi>10.1007/s11334-011-0159-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software architecture has been a key research area in the software
engineering community due to its significant role in creating high quality
software. The trend of developing product lines rather than single products has
made the software product line a viable option in the industry. Software
product line architecture is regarded as one of the crucial components in the
product lines, since all of the resulting products share this common
architecture. The increased popularity of software product lines demands a
process maturity evaluation methodology. Consequently, this paper presents an
architecture process maturity model for software product line engineering to
evaluate the current maturity of the product line architecture development
process in an organization. Assessment questionnaires and a rating methodology
comprise the framework of this model. The objective of the questionnaires is to
collect information about the software product line architecture development
process. Thus, in general this work contributes towards the establishment of a
comprehensive and unified strategy for the process maturity evaluation of
software product line engineering. Furthermore, we conducted two case studies
and reported the assessment results, which show the maturity of the
architecture development process in two organizations
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06906</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06906</id><created>2015-07-24</created><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author><author><keyname>Ho</keyname><forenames>Danny</forenames></author></authors><title>Exploratory Analysis of Quality Practices in Open Source Domain</title><categories>cs.SE</categories><journal-ref>Computer and Information Science, 3(4):35-48, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software quality assurance has been a heated topic for several decades, but
relatively few analyses were performed on open source software (OSS). As OSS
has become very popular in our daily life, many researchers have been keen on
the quality practices in this area. Although quality management presents
distinct patterns compared with those in closed-source software development,
some widely used OSS products have been implemented. Therefore, quality
assurance of OSS projects has attracted increased research focuses. In this
paper, a survey is conducted to reveal the general quality practices in open
source communities. Exploratory analysis has been carried out to disclose those
quality related activities. The results are compared with those from
closed-source environments and the distinguished features of the quality
assurance in OSS projects have been confirmed. Moreover, this study suggests
potential directions for OSS developers to follow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06915</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06915</id><created>2015-07-24</created><authors><author><keyname>Aguado</keyname><forenames>Felicidad</forenames></author><author><keyname>Cabalar</keyname><forenames>Pedro</forenames></author><author><keyname>Pearce</keyname><forenames>David</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Gilberto</forenames></author><author><keyname>Vidal</keyname><forenames>Concepci&#xf3;n</forenames></author></authors><title>A Denotational Semantics for Equilibrium Logic</title><categories>cs.LO</categories><journal-ref>Theory and Practice of Logic Programming 15 (2015) 620-634</journal-ref><doi>10.1017/S1471068415000277</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide an alternative semantics for Equilibrium Logic and
its monotonic basis, the logic of Here-and-There (also known as G\&quot;odel's G3
logic) that relies on the idea of &quot;denotation&quot; of a formula, that is, a
function that collects the set of models of that formula. Using the
three-valued logic G3 as a starting point and an ordering relation (for which
equilibrium/stable models are minimal elements) we provide several elementary
operations for sets of interpretations. By analysing structural properties of
the denotation of formulas, we show some expressiveness results for G3 such as,
for instance, that conjunction is not expressible in terms of the other
connectives. Moreover, the denotational semantics allows us to capture the set
of equilibrium models of a formula with a simple and compact set expression. We
also use this semantics to provide several formal definitions for entailment
relations that are usual in the literature, and further introduce a new one
called &quot;strong entailment&quot;. We say that $\alpha$ strongly entails $\beta$ when
the equilibrium models of $\alpha \wedge \gamma$ are also equilibrium models of
$\beta \wedge \gamma$ for any context $\gamma$. We also provide a
characterisation of strong entailment in terms of the denotational semantics,
and give an example of a sufficient condition that can be applied in some
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06917</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06917</id><created>2015-07-24</created><authors><author><keyname>Du</keyname><forenames>Wei Lin</forenames></author><author><keyname>Ho</keyname><forenames>Danny</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Improving Software Effort Estimation Using Neuro-Fuzzy Model with
  SEER-SEM</title><categories>cs.SE</categories><journal-ref>Global Journal of Computer Science and Technology, 10(12):52-64,
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aims of our research are to evaluate the prediction performance of the
proposed neuro-fuzzy model with System Evaluation and Estimation of Resource
Software Estimation Model (SEER-SEM) in software estimation practices and to
apply the proposed architecture that combines the neuro-fuzzy technique with
different algorithmic models. In this paper, an approach combining the
neuro-fuzzy technique and the SEER-SEM effort estimation algorithm is
described. This proposed model possesses positive characteristics such as
learning ability, decreased sensitivity, effective generalization, and
knowledge integration for introducing the neuro-fuzzy technique. Moreover,
continuous rating values and linguistic values can be inputs of the proposed
model for avoiding the large estimation deviation among similar projects. The
performance of the proposed model is accessed by designing and conducting
evaluation with published projects and industrial data. The evaluation results
indicate that estimation with our proposed neuro-fuzzy model containing
SEER-SEM is improved in comparison with the estimation results that only use
SEER-SEM algorithm. At the same time, the results of this research also
demonstrate that the general neuro-fuzzy framework can function with various
algorithmic models for improving the performance of software effort estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06923</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06923</id><created>2015-07-24</created><authors><author><keyname>Garlapati</keyname><forenames>Abhinav</forenames></author><author><keyname>Raghunathan</keyname><forenames>Aditi</forenames></author><author><keyname>Nagarajan</keyname><forenames>Vaishnavh</forenames></author><author><keyname>Ravindran</keyname><forenames>Balaraman</forenames></author></authors><title>A Reinforcement Learning Approach to Online Learning of Decision Trees</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online decision tree learning algorithms typically examine all features of a
new data point to update model parameters. We propose a novel alternative,
Reinforcement Learning- based Decision Trees (RLDT), that uses Reinforcement
Learning (RL) to actively examine a minimal number of features of a data point
to classify it with high accuracy. Furthermore, RLDT optimizes a long term
return, providing a better alternative to the traditional myopic greedy
approach to growing decision trees. We demonstrate that this approach performs
as well as batch learning algorithms and other online decision tree learning
algorithms, while making significantly fewer queries about the features of the
data points. We also show that RLDT can effectively handle concept drift.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06925</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06925</id><created>2015-07-24</created><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Ho</keyname><forenames>Danny</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>An Empirical Study on the Procedure to Derive Software Quality
  Estimation Models</title><categories>cs.SE</categories><journal-ref>International Journal of Computer Science &amp; Information
  Technology, 2(4):1-16, 2010</journal-ref><doi>10.5121/ijcsit.2010.2401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software quality assurance has been a heated topic for several decades. If
factors that influence software quality can be identified, they may provide
more insight for better software development management. More precise quality
assurance can be achieved by employing resources according to accurate quality
estimation at the early stages of a project. In this paper, a general procedure
is proposed to derive software quality estimation models and various techniques
are presented to accomplish the tasks in respective steps. Several statistical
techniques together with machine learning method are utilized to verify the
effectiveness of software metrics. Moreover, a neuro-fuzzy approach is adopted
to improve the accuracy of the estimation model. This procedure is carried out
based on data from the ISBSG repository to present its empirical value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06927</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06927</id><created>2015-07-24</created><authors><author><keyname>Ahmed</keyname><forenames>Faheem</forenames></author><author><keyname>Campbell</keyname><forenames>Piers</forenames></author><author><keyname>Jaffar</keyname><forenames>Ahmad</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Myths and Realities about Online Forums in Open Source Software
  Development: An Empirical Study</title><categories>cs.SE</categories><journal-ref>The Open Software Engineering Journal, Volume 4, 52-63, 2010</journal-ref><doi>10.2174/1875107X01004010052</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of free and open source software (OSS) is gaining momentum due to the
ever increasing availability and use of the Internet. Organizations are also
now adopting open source software, despite some reservations, in particular
regarding the provision and availability of support. Some of the biggest
concerns about free and open source software are post release software defects
and their rectification, management of dynamic requirements and support to the
users. A common belief is that there is no appropriate support available for
this class of software. A contradictory argument is that due to the active
involvement of Internet users in online forums, there is in fact a large
resource available that communicates and manages the provision of support. The
research model of this empirical investigation examines the evidence available
to assess whether this commonly held belief is based on facts given the current
developments in OSS or simply a myth, which has developed around OSS
development. We analyzed a dataset consisting of 1880 open source software
projects covering a broad range of categories in this investigation. The
results show that online forums play a significant role in managing software
defects, implementation of new requirements and providing support to the users
in open source software and have become a major source of assistance in
maintenance of the open source projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06929</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06929</id><created>2015-07-24</created><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Ho</keyname><forenames>Danny</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Building an OSS Quality Estimation Model with CATREG</title><categories>cs.SE</categories><journal-ref>International Journal on Computer Science and Engineering,
  2(6):1952-1958, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open Source Software (OSS) has been a popular form in software development.
In this paper, we use statistical approaches to derive OSS quality estimation
models. Our objective is to build estimation models for the number of defects
with metrics at project levels. First CATREG (Categorical regression with
optimal scaling) is used to obtain quantifications of the qualitative
variables. Then the independent variables are validated using the stepwise
linear regression. The process is repeated to acquire optimal quantifications
and final regression formula. This modeling process is performed based on data
from the OSS communities and is proved to be practically valuable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06934</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06934</id><created>2015-07-24</created><authors><author><keyname>Xia</keyname><forenames>Wei</forenames></author><author><keyname>Ho</keyname><forenames>Danny</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>A Neuro-Fuzzy Model for Function Point Calibration</title><categories>cs.SE</categories><journal-ref>Transactions on Information Science &amp; Applications, 5(1):22-30,
  2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need to update the calibration of Function Point (FP) complexity weights
is discussed, whose aims are to fit specific software application, to reflect
software industry trend, and to improve cost estimation. Neuro-Fuzzy is a
technique that incorporates the learning ability from neural network and the
ability to capture human knowledge from fuzzy logic. The empirical validation
using ISBSG data repository Release 8 shows a 22% improvement in software
effort estimation after calibration using Neuro-Fuzzy technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06941</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06941</id><created>2015-07-24</created><authors><author><keyname>Ahmed</keyname><forenames>Faheem</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>A Decision Support Tool for Assessing the Maturity of Software Product
  Line Process</title><categories>cs.SE</categories><journal-ref>International Journal of Computing &amp; Information Sciences,
  4(3):97-115, 2006</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The software product line aims at the effective utilization of software
assets, reducing the time required to deliver a product, improving the quality,
and decreasing the cost of software products. Organizations trying to
incorporate this concept require an approach to assess the current maturity
level of the software product line process in order to make management
decisions. A decision support tool for assessing the maturity of the software
product line process is developed to implement the fuzzy logic approach, which
handles the imprecise and uncertain nature of software process variables. The
proposed tool can be used to assess the process maturity level of a software
product line. Such knowledge will enable an organization to make crucial
management decisions. Four case studies were conducted to validate the tool,
and the results of the studies show that the software product line decision
support tool provides a direct mechanism to evaluate the current software
product line process maturity level within an organization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06943</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06943</id><created>2015-07-24</created><authors><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Clues on Software Engineers Learning Styles</title><categories>cs.SE cs.CY</categories><journal-ref>International Journal of Computing &amp; Information Sciences,
  4(1):46-49, 2006</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Myers-Briggs Type Indicator (MBTI) has proved to be a useful instrument
for understanding student learning preferences and has enable comparisons of
the learning preferences for various personality types. Regarding learning
styles, there is no one best combination of characteristics, since each
preference has its own advantages and disadvantages. Therefore, it is a fallacy
to think that professors can devise a single teaching technique that would
always appeal to all students at the same time. The ideas presented in this
paper have been taken into account in two 4th year courses, named Software
Requirements and Software Design in which the students develop their capstone
projects. The results of this investigation may help college instructors to
understanding the preferred leaning style of software engineers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06944</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06944</id><created>2015-07-24</created><authors><author><keyname>Tarau</keyname><forenames>Paul</forenames></author></authors><title>A Logic Programming Playground for Lambda Terms, Combinators, Types and
  Tree-based Arithmetic Computations</title><categories>cs.LO cs.PL</categories><comments>70 pages</comments><msc-class>03B40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With sound unification, Definite Clause Grammars and compact expression of
combinatorial generation algorithms, logic programming is shown to conveniently
host a declarative playground where interesting properties and behaviors emerge
from the interaction of heterogenous but deeply connected computational
objects.
  Compact combinatorial generation algorithms are given for several families of
lambda terms, including open, closed, simply typed and linear terms as well as
type inference and normal order reduction algorithms. We describe a
Prolog-based combined lambda term generator and type-inferrer for closed
well-typed terms of a given size, in de Bruijn notation.
  We introduce a compressed de Bruijn representation of lambda terms and define
its bijections to standard representations. Our compressed terms facilitate
derivation of size-proportionate ranking and unranking algorithms of lambda
terms and their inferred simple types.
  The S and K combinator expressions form a well-known Turing-complete subset
of the lambda calculus. We specify evaluation, type inference and combinatorial
generation algorithms for SK-combinator trees. In the process, we unravel
properties shedding new light on interesting aspects of their structure and
distribution.
  A uniform representation, as binary trees with empty leaves, is given to
expressions built with Rosser's X-combinator, natural numbers, lambda terms and
simple types. Using this shared representation, ranking/unranking algorithm of
lambda terms to tree-based natural numbers are described.
  Our algorithms, expressed as an incrementally developed literate Prolog
program, implement a declarative playground for exploration of representations,
encodings and computations with uniformly represented lambda terms, types,
combinators and tree-based arithmetic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06946</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06946</id><created>2015-07-24</created><authors><author><keyname>Bose</keyname><forenames>Rajesh</forenames></author><author><keyname>Roy</keyname><forenames>Sandip</forenames></author><author><keyname>Sarddar</keyname><forenames>Debabrata</forenames></author></authors><title>Mobile-Based Video Caching Architecture Based on Billboard Manager</title><categories>cs.NI</categories><comments>8 pages, 1 figure, GridCom-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video streaming services are very popular today. Increasingly, users can now
access multimedia applications and video playback wirelessly on their mobile
devices. However, a significant challenge remains in ensuring smooth and
uninterrupted transmission of almost any size of video file over a 3G network,
and as quickly as possible in order to optimize bandwidth consumption. In this
paper, we propose to position our Billboard Manager to provide an optimal
transmission rate to enable smooth video playback to a mobile device user
connected to a 3G network. Our work focuses on serving user requests by mobile
operators from cached resource managed by Billboard Manager, and transmitting
the video files from this pool. The aim is to reduce the load placed on
bandwidth resources of a mobile operator by routing away as much user requests
away from the internet for having to search a video and, subsequently, if
located, have it transferred back to the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06947</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06947</id><created>2015-07-24</created><authors><author><keyname>Sak</keyname><forenames>Ha&#x15f;im</forenames></author><author><keyname>Senior</keyname><forenames>Andrew</forenames></author><author><keyname>Rao</keyname><forenames>Kanishka</forenames></author><author><keyname>Beaufays</keyname><forenames>Fran&#xe7;oise</forenames></author></authors><title>Fast and Accurate Recurrent Neural Network Acoustic Models for Speech
  Recognition</title><categories>cs.CL cs.LG cs.NE stat.ML</categories><comments>To be published in the INTERSPEECH 2015 proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have recently shown that deep Long Short-Term Memory (LSTM) recurrent
neural networks (RNNs) outperform feed forward deep neural networks (DNNs) as
acoustic models for speech recognition. More recently, we have shown that the
performance of sequence trained context dependent (CD) hidden Markov model
(HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained
phone models initialized with connectionist temporal classification (CTC). In
this paper, we present techniques that further improve performance of LSTM RNN
acoustic models for large vocabulary speech recognition. We show that frame
stacking and reduced frame rate lead to more accurate models and faster
decoding. CD phone modeling leads to further improvements. We also present
initial results for LSTM RNN models outputting words directly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06948</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06948</id><created>2015-07-24</created><authors><author><keyname>Ahmed</keyname><forenames>Fahem</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>A Framework for Process Assessment of Software Product Line</title><categories>cs.SE</categories><journal-ref>Journal of Information Technology Theory and Application,
  7(1):135-157, 2005</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software product line has emerged as an attractive phenomenon within
organizations dealing with software development process. It involves assembly
of products from existing core assets, commonly known as components, and
continuous growth in the core assets as production proceeds. Organizations
trying to incorporate the concept of software product line to reduce
development time and cost require certain rules to be followed for successful
development and management, they also require a direct procedure to evaluate
the current maturity level of the process. In this work certain rules for
developing and managing a software product line are put forward. Additionally,
a fuzzy logic based software product line process assessment tool (SPLPAT) has
been designed and implemented on the basis of developed rules for software
product line process assessment. SPLPAT can be used to assess the process
maturity level of software product line, and it provides an opportunity to
handle imprecision and uncertainty present in software process variables. Four
case studies were conducted to validate the framework, and results show that
SPLPAT provides a direct mechanism to evaluate current software product line
process maturity level within an organization. The results of the developed
software product line process assessment approach were compared with the
existing CMM-level of the organization in order to evaluate the reliability of
the presented approach and to find out how effectively an organization can
execute software product line process when it has already achieved a certain
CMM level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06949</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06949</id><created>2015-07-24</created><authors><author><keyname>Kernahan</keyname><forenames>Michael</forenames></author><author><keyname>Capretz</keyname><forenames>Miriam</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>C# Traceability System</title><categories>cs.SE</categories><journal-ref>Transactions on Information Science and Applications,
  6(2):771-778, 2005</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traceability information is a valuable asset that software development teams
can leverage to minimise their risk during production and maintenance of
software projects. When maintainers are added to a software project
post-production, they have to learn the system from scratch and understand its
dynamics before they can begin making appropriate modifications to the source
code. The system outlined in this paper extracts traceability information
directly from the source code of C# projects, and presents it in such a way
that it can be easily used to understand the logic and validate changes to the
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06952</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06952</id><created>2015-07-24</created><authors><author><keyname>Ahmed</keyname><forenames>Faheem</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Framework for Version Control &amp; Dependency Link of Components &amp; Products
  in a Software Product Line</title><categories>cs.SE</categories><journal-ref>Transactions on Computers, 3(6):1782-1787, 2004</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software product line deals with the assembly of products from existing core
assets commonly known as components and continuous growth in the core assets as
we proceed with production. This idea has emerged as vital in terms of software
development from component-based architecture. Since in software product line
one has to deal with number of products and components simultaneous therefore
there is a need to develop a strategy, which will help to store components and
products information in such a way that they can be traced easily for further
development. This storage strategy should reflect a relationship between
products and components so that product history with reference to components
can be traced and vise versa. In this paper we have presented a tree structure
based storage strategy for components and products in software product line.
This strategy will enable us to store the vital information about components
and products with a relationship of their composition and utilization. We
implemented this concept and simulated the software product line environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06953</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06953</id><created>2015-07-24</created><authors><author><keyname>Chalermsook</keyname><forenames>Parinya</forenames></author><author><keyname>Goswami</keyname><forenames>Mayank</forenames></author><author><keyname>Kozma</keyname><forenames>Laszlo</forenames></author><author><keyname>Mehlhorn</keyname><forenames>Kurt</forenames></author><author><keyname>Saranurak</keyname><forenames>Thatchaphol</forenames></author></authors><title>Pattern-avoiding access in binary search trees</title><categories>cs.DS math.CO</categories><comments>To be presented at FOCS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamic optimality conjecture is perhaps the most fundamental open
question about binary search trees (BST). It postulates the existence of an
asymptotically optimal online BST, i.e. one that is constant factor competitive
with any BST on any input access sequence. The two main candidates for dynamic
optimality in the literature are splay trees [Sleator and Tarjan, 1985], and
Greedy [Lucas, 1988; Munro, 2000; Demaine et al. 2009] [..]
  Dynamic optimality is trivial for almost all sequences: the optimum access
cost of most length-n sequences is Theta(n log n), achievable by any balanced
BST. Thus, the obvious missing step towards the conjecture is an understanding
of the &quot;easy&quot; access sequences. [..] The difficulty of proving dynamic
optimality is witnessed by highly restricted special cases that remain
unresolved; one prominent example is the traversal conjecture [Sleator and
Tarjan, 1985], which states that preorder sequences (whose optimum is linear)
are linear-time accessed by splay trees; no online BST is known to satisfy this
conjecture.
  In this paper, we prove two different relaxations of the traversal conjecture
for Greedy: (i) Greedy is almost linear for preorder traversal, (ii) if a
linear-time preprocessing is allowed, Greedy is in fact linear. These
statements are corollaries of our more general results that express the
complexity of access sequences in terms of a pattern avoidance parameter k.
[..] To our knowledge, these are the first upper bounds for Greedy that are not
known to hold for any other online BST. To obtain these results we identify an
input-revealing property of Greedy. Informally, this means that the execution
log partially reveals the structure of the access sequence. This property
facilitates the use of rich technical tools from forbidden submatrix theory.
  [Abridged]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06955</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06955</id><created>2015-07-24</created><updated>2016-02-18</updated><authors><author><keyname>Gruss</keyname><forenames>Daniel</forenames></author><author><keyname>Maurice</keyname><forenames>Cl&#xe9;mentine</forenames></author><author><keyname>Mangard</keyname><forenames>Stefan</forenames></author></authors><title>Rowhammer.js: A Remote Software-Induced Fault Attack in JavaScript</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental assumption in software security is that a memory location can
only be modified by processes that may write to this memory location. However,
a recent study has shown that parasitic effects in DRAM can change the content
of a memory cell without accessing it, but by accessing other memory locations
in a high frequency. This so-called Rowhammer bug occurs in most of today's
memory modules and has fatal consequences for the security of all affected
systems, e.g. privilege escalation attacks.
  All studies and attacks related to Rowhammer so far rely on the availability
of a cache flush instruction in order to cause accesses to DRAM modules at a
sufficiently high frequency. We overcome this limitation by defeating complex
cache replacement policies. We show that caches can be forced into fast cache
eviction to trigger the Rowhammer bug with only regular memory accesses. This
allows to trigger the Rowhammer bug in highly restricted and even scripting
environments.
  We demonstrate a fully automated attack that requires nothing but a website
with JavaScript to trigger faults on remote hardware. Thereby we can gain
unrestricted access to systems of website visitors. We show that the attack
works on off-the-shelf systems. Existing countermeasures fail to protect
against this new Rowhammer attack. We propose a countermeasure that can be
implemented on commodity hardware without any performance penalties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06956</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06956</id><created>2015-07-24</created><authors><author><keyname>Carter</keyname><forenames>David</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>A Three-Dimensional GUI for Windows Explorer</title><categories>cs.HC</categories><journal-ref>Journal of Three Dimensional Images, 18(1):136-141, 2004</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three-dimension will be a characteristic of future user interfaces, although
we are just starting to gain an understanding of how users can navigate and
share information within a virtual 3D environment. Three-dimensional graphical
user interfaces (3D-GUI) raise many issues of design, metaphor and usability.
This research is devoted to designing a 3D-GUI as a front-end tool for a file
management system, in this case, for Microsoft Windows\c{opyright} Explorer; as
well as evaluating the efficiency of a 3D application. The software design was
implemented by extending the Half-Life 3D engine. This extension provides a
directory traversal and basic file management functions, like cut, copy, paste,
delete, and so on. This paper shows the design and implementation of a
real-world application that contains an efficient 3D-GUI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06959</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06959</id><created>2015-07-24</created><authors><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>The Challenges of CASE Design Integration in the Telecommunication
  Application Domain</title><categories>cs.SE</categories><journal-ref>Journal of Integrated Design and Process Science, 7(3):1-10, 2003</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The magnitude of the problems facing the telecommunication software industry
is presently at a point at which software engineers should become deeply
involved. This paper presents a research project on advanced telecommunication
technology carried out in Europe, called BOOST (Broadband Object-Oriented
Service Technology). The project involved cooperative work among
telecommunication companies, research centres and universities from several
countries. The challenges to integrate CASE tools to support software
development within the telecommunication application domain are discussed. A
software process model that encourages component reusability, named the X
model, is described as part of a software life cycle model for the
telecommunication software industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06962</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06962</id><created>2015-07-24</created><authors><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Implications of MBTI in Software Engineering Education</title><categories>cs.SE cs.CY</categories><journal-ref>ACM SIGCSE Bulletin, 34(4):134-137, 2002</journal-ref><doi>10.1145/820127.820185</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of approaches exist to aid the understanding of individual
differences and their effects on teaching and learning. Educators have been
using the Myers-Briggs Type Indicator (MBTI) to understand differences in
learning styles and to develop teaching methods that cater for the various
personality styles. Inspired by the MBTI, we developed a range of practices for
effective teaching and learning in a software engineering course. Our aim is to
reach every student, but in different ways, by devising various teaching
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06970</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06970</id><created>2015-07-24</created><authors><author><keyname>Mania</keyname><forenames>Horia</forenames></author><author><keyname>Pan</keyname><forenames>Xinghao</forenames></author><author><keyname>Papailiopoulos</keyname><forenames>Dimitris</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Perturbed Iterate Analysis for Asynchronous Stochastic Optimization</title><categories>stat.ML cs.DC cs.DS cs.LG math.OC</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and analyze stochastic optimization methods where the input to
each gradient update is perturbed by bounded noise. We show that this framework
forms the basis of a unified approach to analyze asynchronous implementations
of stochastic optimization algorithms.In this framework, asynchronous
stochastic optimization algorithms can be thought of as serial methods
operating on noisy inputs. Using our perturbed iterate framework, we provide
new analyses of the Hogwild! algorithm and asynchronous stochastic coordinate
descent, that are simpler than earlier analyses, remove many assumptions of
previous models, and in some cases yield improved upper bounds on the
convergence rates. We proceed to apply our framework to develop and analyze
KroMagnon: a novel, parallel, sparse stochastic variance-reduced gradient
(SVRG) algorithm. We demonstrate experimentally on a 16-core machine that the
sparse and parallel version of SVRG is in some cases more than four orders of
magnitude faster than the standard SVRG algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06980</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06980</id><created>2015-07-24</created><updated>2015-08-31</updated><authors><author><keyname>Manyam</keyname><forenames>Satyanarayana</forenames></author><author><keyname>Rathinam</keyname><forenames>Sivakumar</forenames></author><author><keyname>Casbeer</keyname><forenames>David</forenames></author><author><keyname>Garcia</keyname><forenames>Eloy</forenames></author></authors><title>Shortest Paths of Bounded Curvature for the Dubins Interval Problem</title><categories>math.OC cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Dubins interval problem aims to find the shortest path of bounded
curvature between two targets such that the departure angle from the first
target and the arrival angle at the second target are constrained to two
respective intervals. We propose a new and a simple algorithm to this problem
based on the minimum principle of Pontryagin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06986</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06986</id><created>2015-07-24</created><authors><author><keyname>Rodrigues</keyname><forenames>Helena Sofia</forenames></author><author><keyname>Fonseca</keyname><forenames>Manuel Jos&#xe9;</forenames></author></authors><title>Viral marketing as epidemiological model</title><categories>physics.soc-ph cs.SI</categories><comments>This is a preprint of a paper whose final and definite form is in
  Proceedings of the 15th International Conference on Computational and
  Mathematical Methods in Science and Engineering, 2015, pages 946 - 955</comments><msc-class>34A34, 92D30, 91F99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In epidemiology, an epidemic is defined as the spread of an infectious
disease to a large number of people in a given population within a short period
of time. In the marketing context, a message is viral when it is broadly sent
and received by the target market through person-to-person transmission. This
specific marketing communication strategy is commonly referred as viral
marketing. Due to this similarity between an epidemic and the viral marketing
process and because the understanding of the critical factors to this
communications strategy effectiveness remain largely unknown, the mathematical
models in epidemiology are presented in this marketing specific field. In this
paper, an epidemiological model SIR (Susceptible- Infected-Recovered) to study
the effects of a viral marketing strategy is presented. It is made a comparison
between the disease parameters and the marketing application, and simulations
using the Matlab software are performed. Finally, some conclusions are given
and their marketing implications are exposed: interactions across the
parameters are found that appear to suggest some recommendations to marketers,
as the profitability of the investment or the need to improve the targeting
criteria of the communications campaigns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.06988</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.06988</id><created>2015-07-24</created><authors><author><keyname>Wang</keyname><forenames>Lihua</forenames></author><author><keyname>Capretz</keyname><forenames>Luz Fernando</forenames></author></authors><title>A Binary Data Stream Scripting Language</title><categories>cs.PL</categories><journal-ref>Transactions on Information Science and Applications,
  3(2):291-298, 2006</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any file is fundamentally a binary data stream. A practical solution was
achieved to interpret binary data stream. A new scripting language named Data
Format Scripting Language (DFSL) was developed to describe the physical layout
of the data in a structural, more intelligible way. On the basis of the
solution, a generic software application was implemented; it parses various
binary data streams according to their respective DFSL scripts and generates
human-readable result and XML document for data sharing. Our solution helps
eliminate the error-prone low-level programming, especially in the hardware
devices or network protocol development/debugging processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07011</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07011</id><created>2015-07-24</created><authors><author><keyname>Christodoulou</keyname><forenames>George</forenames></author><author><keyname>Sgouritsa</keyname><forenames>Alkmini</forenames></author><author><keyname>Tang</keyname><forenames>Bo</forenames></author></authors><title>On the Efficiency of the Proportional Allocation Mechanism for Divisible
  Resources</title><categories>cs.GT</categories><comments>To appear in SAGT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the efficiency of the proportional allocation mechanism, that is
widely used to allocate divisible resources. Each agent submits a bid for each
divisible resource and receives a fraction proportional to her bids. We
quantify the inefficiency of Nash equilibria by studying the Price of Anarchy
(PoA) of the induced game under complete and incomplete information. When
agents' valuations are concave, we show that the Bayesian Nash equilibria can
be arbitrarily inefficient, in contrast to the well-known 4/3 bound for pure
equilibria. Next, we upper bound the PoA over Bayesian equilibria by 2 when
agents' valuations are subadditive, generalizing and strengthening previous
bounds on lattice submodular valuations. Furthermore, we show that this bound
is tight and cannot be improved by any simple or scale-free mechanism. Then we
switch to settings with budget constraints, and we show an improved upper bound
on the PoA over coarse-correlated equilibria. Finally, we prove that the PoA is
exactly 2 for pure equilibria in the polyhedral environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07017</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07017</id><created>2015-07-24</created><updated>2015-11-17</updated><authors><author><keyname>Ogura</keyname><forenames>Masaki</forenames></author><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author></authors><title>Stability of Spreading Processes over Time-Varying Large-Scale Networks</title><categories>cs.SI math.DS math.OC physics.soc-ph q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the dynamics of spreading processes taking place
over time-varying networks. A common approach to model time-varying networks is
via Markovian random graph processes. This modeling approach presents the
following limitation: Markovian random graphs can only replicate switching
patterns with exponential inter-switching times, while in real applications
these times are usually far from exponential. In this paper, we introduce a
flexible and tractable extended family of processes able to replicate, with
arbitrary accuracy, any distribution of inter-switching times. We then study
the stability of spreading processes in this extended family. We first show
that a direct analysis based on It\^o's formula provides stability conditions
in terms of the eigenvalues of a matrix whose size grows exponentially with the
number of edges. To overcome this limitation, we derive alternative stability
conditions involving the eigenvalues of a matrix whose size grows linearly with
the number of nodes. Based on our results, we also show that heuristics based
on aggregated static networks approximate the epidemic threshold more
accurately as the number of nodes grows, or the temporal volatility of the
random graph process is reduced. Finally, we illustrate our findings via
numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07034</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07034</id><created>2015-07-24</created><authors><author><keyname>Fernandez-Granda</keyname><forenames>Carlos</forenames></author></authors><title>Super-Resolution of Point Sources via Convex Programming</title><categories>math.OC cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering a signal consisting of a superposition
of point sources from low-resolution data with a cut-off frequency f. If the
distance between the sources is under 1/f, this problem is not well posed in
the sense that the low-pass data corresponding to two different signals may be
practically the same. We show that minimizing a continuous version of the l1
norm achieves exact recovery as long as the sources are separated by at least
1.26/f. The proof is based on the construction of a dual certificate for the
optimization problem, which can be used to establish that the procedure is
stable to noise. Finally, we illustrate the flexibility of our
optimization-based framework by describing extensions to the demixing of sines
and spikes and to the estimation of point sources that share a common support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07038</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07038</id><created>2015-07-24</created><authors><author><keyname>Alatabbi</keyname><forenames>Ali</forenames></author><author><keyname>Daykin</keyname><forenames>Jacqueline W.</forenames></author><author><keyname>Rahman</keyname><forenames>M. Sohel</forenames></author><author><keyname>Smyth</keyname><forenames>W. F.</forenames></author></authors><title>String Comparison in $V$-Order: New Lexicographic Properties &amp; On-line
  Applications</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $V$-order is a global order on strings related to Unique Maximal
Factorization Families (UMFFs), which are themselves generalizations of Lyndon
words. $V$-order has recently been proposed as an alternative to
lexicographical order in the computation of suffix arrays and in the
suffix-sorting induced by the Burrows-Wheeler transform. Efficient $V$-ordering
of strings thus becomes a matter of considerable interest. In this paper we
present new and surprising results on $V$-order in strings, then go on to
explore the algorithmic consequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07045</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07045</id><created>2015-07-24</created><authors><author><keyname>Kamble</keyname><forenames>Vijay</forenames></author><author><keyname>Shah</keyname><forenames>Nihar</forenames></author><author><keyname>Marn</keyname><forenames>David</forenames></author><author><keyname>Parekh</keyname><forenames>Abhay</forenames></author><author><keyname>Ramachandran</keyname><forenames>Kannan</forenames></author></authors><title>Truth Serums for Massively Crowdsourced Evaluation Tasks</title><categories>cs.GT cs.AI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incentivizing effort and eliciting truthful responses from agents in the
absence of verifiability is a major challenge faced while crowdsourcing many
types of evaluation tasks like labeling images, grading assignments in online
courses, etc. In this paper, we propose new reward mechanisms for such settings
that, unlike most previously studied mechanisms, impose minimal assumptions on
the structure and knowledge of the underlying generating model, can account for
heterogeneity in the agents' abilities, require no extraneous elicitation from
them, and furthermore allow their beliefs to be (almost) arbitrary. Moreover,
these mechanisms have the simple and intuitive structure of output agreement
mechanisms, which, despite not incentivizing truthful behavior, have
nevertheless been quite popular in practice. We achieve this by leveraging a
typical characteristic of many of these settings, which is the existence of a
large number of similar tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07049</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07049</id><created>2015-07-24</created><updated>2015-11-24</updated><authors><author><keyname>McClurg</keyname><forenames>Jedidiah</forenames></author><author><keyname>Hojjat</keyname><forenames>Hossein</forenames></author><author><keyname>Foster</keyname><forenames>Nate</forenames></author><author><keyname>Cerny</keyname><forenames>Pavol</forenames></author></authors><title>Event-driven Network Programming</title><categories>cs.PL</categories><acm-class>D.2.4; F.3.1; C.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software-defined networking (SDN) programs must simultaneously describe
static forwarding behavior and dynamic updates in response to events.
Event-driven updates are critical to get right, but difficult to implement
correctly due to the high degree of concurrency in networks. Existing SDN
platforms offer weak guarantees that often break application invariants,
leading to problems such as dropped packets, degraded performance, security
violations, etc. This paper introduces event-driven consistent updates that are
guaranteed to preserve well-defined behaviors when transitioning between
configurations in response to events. We propose network event structures
(NESs) to model constraints on updates, such as which events can be enabled
simultaneously and causal dependencies between events. We define an extension
of the NetKAT language with mutable state, and give semantics to stateful
programs using NESs. We discuss strategies for implementing NESs using SDN
switches, and prove them correct. Finally, we evaluate our approach
empirically, demonstrating that it gives well-defined consistency guarantees
while avoiding expensive synchronization and packet buffering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07051</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07051</id><created>2015-07-24</created><authors><author><keyname>Suhov</keyname><forenames>Yuri</forenames></author><author><keyname>Sekeh</keyname><forenames>Salimeh Yasaei</forenames></author></authors><title>Weighted cumulative entropies: An extension of CRE and CE</title><categories>cs.IT math.IT math.PR</categories><comments>21 pages</comments><msc-class>62N05, 62B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the weighted cumulative entropies (WCRE and WCE), introduced in
[5], for a system or component lifetime. Representing properties of cumulative
entropies, several bounds and inequalities for the WCRE is proposed
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07056</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07056</id><created>2015-07-24</created><updated>2016-01-16</updated><authors><author><keyname>Siriteanu</keyname><forenames>Constantin</forenames></author><author><keyname>Takemura</keyname><forenames>Akimichi</forenames></author><author><keyname>Koutschan</keyname><forenames>Christoph</forenames></author><author><keyname>Kuriki</keyname><forenames>Satoshi</forenames></author><author><keyname>Richards</keyname><forenames>Donald St. P.</forenames></author><author><keyname>Shin</keyname><forenames>Hyundong</forenames></author></authors><title>Exact ZF Analysis and Computer-Algebra-Aided Evaluation in Rank-1 LoS
  Rician Fading</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Wireless Communications on July
  24th 2015, revised January 15th 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study zero-forcing detection (ZF) for multiple-input/multiple-output
(MIMO) spatial multiplexing under transmit-correlated Rician fading for an
NRXNT channel matrix with rank-1 line-of-sight (LoS) component. By using matrix
transformations and multivariate statistics, our exact analysis yields the
signal-to-noise ratio moment generating function (m.g.f.) as an infinite series
of gamma distribution m.g.f.'s., and analogous series for ZF performance
measures, e.g., outage probability and ergodic capacity. However, their
numerical convergence is inherently problematic with increasing Rician
K-factor, NR, and NT. We circumvent this limitation as follows. First, we
derive differential equations for the performance measures with a novel
automated approach employing a computer-algebra tool which implements Groebner
basis computation and creative telescoping. These differential equations are
then solved with the holonomic gradient method (HGM) from initial conditions
computed with the infinite series. We demonstrate that HGM yields more reliable
performance evaluation than by infinite series alone and more expeditious than
by simulation, for realistic values of K, and even for NR and NT relevant to
large MIMO systems. We envision extending the proposed approaches for exact
analysis and reliable evaluation to more general Rician fading and other
transceiver methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07058</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07058</id><created>2015-07-24</created><authors><author><keyname>Iqbal</keyname><forenames>Azlan</forenames></author><author><keyname>Guid</keyname><forenames>Matej</forenames></author><author><keyname>Colton</keyname><forenames>Simon</forenames></author><author><keyname>Krivec</keyname><forenames>Jana</forenames></author><author><keyname>Azman</keyname><forenames>Shazril</forenames></author><author><keyname>Haghighi</keyname><forenames>Boshra</forenames></author></authors><title>The Digital Synaptic Neural Substrate: A New Approach to Computational
  Creativity</title><categories>cs.AI</categories><comments>39 pages, 5 appendices. Reviewed favorably by several renowned
  journals but the size of the paper was an insurmountable issue</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new artificial intelligence (AI) approach called, the 'Digital
Synaptic Neural Substrate' (DSNS). It uses selected attributes from objects in
various domains (e.g. chess problems, classical music, renowned artworks) and
recombines them in such a way as to generate new attributes that can then, in
principle, be used to create novel objects of creative value to humans relating
to any one of the source domains. This allows some of the burden of creative
content generation to be passed from humans to machines. The approach was
tested in the domain of chess problem composition. We used it to automatically
compose numerous sets of chess problems based on attributes extracted and
recombined from chess problems and tournament games by humans, renowned
paintings, computer-evolved abstract art, photographs of people, and classical
music tracks. The quality of these generated chess problems was then assessed
automatically using an existing and experimentally-validated computational
chess aesthetics model. They were also assessed by human experts in the domain.
The results suggest that attributes collected and recombined from chess and
other domains using the DSNS approach can indeed be used to automatically
generate chess problems of reasonably high aesthetic quality. In particular, a
low quality chess source (i.e. tournament game sequences between weak players)
used in combination with actual photographs of people was able to produce
three-move chess problems of comparable quality or better to those generated
using a high quality chess source (i.e. published compositions by human
experts), and more efficiently as well. Why information from a foreign domain
can be integrated and functional in this way remains an open question for now.
The DSNS approach is, in principle, scalable and applicable to any domain in
which objects have attributes that can be represented using real numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07064</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07064</id><created>2015-07-24</created><authors><author><keyname>Hosseini</keyname><forenames>Hadi</forenames></author><author><keyname>Larson</keyname><forenames>Kate</forenames></author></authors><title>Strategyproof Quota Mechanisms for Multiple Assignment Problems</title><categories>cs.GT</categories><comments>18 pages</comments><msc-class>91A99</msc-class><acm-class>J.4; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of allocating multiple objects to agents without any
explicit market, where each agent may receive more than one object according to
a quota. Under lexicographic preferences, we characterize the set of
strategyproof, non-bossy, and neutral quota mechanisms and show that under a
mild Pareto efficiency condition, serial dictatorship quota mechanisms are the
only mechanisms satisfying these properties. Dropping the neutrality
requirement, this class of quota mechanisms further expands to sequential
dictatorship quota mechanisms. We then extend quota mechanisms to randomized
settings, and show that the random serial dictatorship quota mechanisms (RSDQ)
are envyfree, strategyproof, and ex post efficient for any number of agents and
objects and any quota system, proving that the well-studied Random Serial
Dictatorship (RSD) satisfies envyfreeness when preferences are lexicographic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07067</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07067</id><created>2015-07-25</created><updated>2015-08-21</updated><authors><author><keyname>Ruderman</keyname><forenames>Michael</forenames></author></authors><title>Compensation of Nonlinear Torsion in Flexible Joint Robots: Comparison
  of Two Approaches</title><categories>cs.SY</categories><comments>8 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flexible joint robots, in particularly those which are equipped with
harmonic-drive gears, can feature elasticities with hysteresis. Under heavy
loads and large joint torques the hysteresis lost motion can lead to
significant errors of tracking and positioning of the robotic links. In this
paper, two approaches for compensating the nonlinear joint torsion with
hysteresis are described and compared with each other. Both methods assume the
measured signals available only on the motor side of joint transmissions. The
first approach assumes a rigid-link manipulator model and transforms the
desired link trajectory into that of the motor drives by using the inverse
dynamics and inverse hysteresis map. The second approach relies on the modeling
of motor drives and inverse hysteresis and uses the generalized momenta when
predicting the joint torsion. Both methods are discussed in details along with
a numerical example of two-link planar manipulator under gravity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07068</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07068</id><created>2015-07-25</created><authors><author><keyname>Pal</keyname><forenames>Anirban</forenames></author><author><keyname>Agarwala</keyname><forenames>Abhishek</forenames></author><author><keyname>Raha</keyname><forenames>Soumyendu</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Baidurya</forenames></author></authors><title>Performance metrics in a hybrid MPI-OpenMP based molecular dynamics
  simulation with short-range interactions</title><categories>physics.comp-ph cond-mat.mtrl-sci cs.DC</categories><journal-ref>Journal of Parallel and Distributed Computing, Elsevier, vol. 74,
  no. 3, pp. 2203-2214, 2014</journal-ref><doi>10.1016/j.jpdc.2013.12.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the computational bottlenecks in molecular dynamics (MD) and
describe the challenges in parallelizing the computation intensive tasks. We
present a hybrid algorithm using MPI (Message Passing Interface) with OpenMP
threads for parallelizing a generalized MD computation scheme for systems with
short range interatomic interactions. The algorithm is discussed in the context
of nanoindentation of Chromium films with carbon indenters using the Embedded
Atom Method potential for Cr Cr interaction and the Morse potential for Cr C
interactions. We study the performance of our algorithm for a range of
MPIthread combinations and find the performance to depend strongly on the
computational task and load sharing in the multicore processor. The algorithm
scaled poorly with MPI and our hybrid schemes were observed to outperform the
pure message passing scheme, despite utilizing the same number of processors or
cores in the cluster. Speed-up achieved by our algorithm compared favourably
with that achieved by standard MD packages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07073</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07073</id><created>2015-07-25</created><updated>2015-10-31</updated><authors><author><keyname>Wen</keyname><forenames>Yandong</forenames></author><author><keyname>Liu</keyname><forenames>Weiyang</forenames></author><author><keyname>Yang</keyname><forenames>Meng</forenames></author><author><keyname>Li</keyname><forenames>Zhifeng</forenames></author></authors><title>Efficient Face Alignment via Locality-constrained Representation for
  Robust Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Practical face recognition has been studied in the past decades, but still
remains an open challenge. Current prevailing approaches have already achieved
substantial breakthroughs in recognition accuracy. However, their performance
usually drops dramatically if face samples are severely misaligned. To address
this problem, we propose a highly efficient misalignment-robust
locality-constrained representation (MRLR) algorithm for practical real-time
face recognition. Specifically, the locality constraint that activates the most
correlated atoms and suppresses the uncorrelated ones, is applied to construct
the dictionary for face alignment. Then we simultaneously align the warped face
and update the locality-constrained dictionary, eventually obtaining the final
alignment. Moreover, we make use of the block structure to accelerate the
derived analytical solution. Experimental results on public data sets show that
MRLR significantly outperforms several state-of-the-art approaches in terms of
efficiency and scalability with even better performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07075</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07075</id><created>2015-07-25</created><authors><author><keyname>Prakash</keyname><forenames>Keerthana S.</forenames></author><author><keyname>Prakash</keyname><forenames>R. P.</forenames></author><author><keyname>Binu</keyname><forenames>V. P.</forenames></author></authors><title>A Study of Morphological Filtering Using Graph and Hypergraphs</title><categories>cs.CV</categories><comments>Advance Computing Conference (IACC), 2015 IEEE International,12-13
  June 2015,Banglore India, Page(s):1017 - 1020,Print
  ISBN:978-1-4799-8046-8,IEEE Xplore,2015</comments><doi>10.1109/IADCC.2015.7154858</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mathematical morphology (MM) helps to describe and analyze shapes using set
theory. MM can be effectively applied to binary images which are treated as
sets. Basic morphological operators defined can be used as an effective tool in
image processing. Morphological operators are also developed based on graph and
hypergraph. These operators have found better performance and applications in
image processing. Bino et al. [8], [9] developed the theory of morphological
operators on hypergraph. A hypergraph structure is considered and basic
morphological operation erosion/dilation is defined. Several new operators
opening/closing and filtering are also defined on the hypergraphs. Hypergraph
based filtering have found comparatively better performance with morphological
filters based on graph. In this paper we evaluate the effectiveness of
hypergraph based ASF on binary images. Experimental results shows that
hypergraph based ASF filters have outperformed graph based ASF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07077</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07077</id><created>2015-07-25</created><authors><author><keyname>Abrol</keyname><forenames>V.</forenames></author><author><keyname>Sharma</keyname><forenames>P.</forenames></author><author><keyname>Sao</keyname><forenames>A. K</forenames></author></authors><title>Making sense of randomness: an approach for fast recovery of
  compressively sensed signals</title><categories>cs.IT cs.CV math.IT</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In compressed sensing (CS) framework, a signal is sampled below Nyquist rate,
and the acquired compressed samples are generally random in nature. However,
for efficient estimation of the actual signal, the sensing matrix must preserve
the relative distances among the acquired compressed samples. Provided this
condition is fulfilled, we show that CS samples will preserve the envelope of
the actual signal even at different compression ratios. Exploiting this
envelope preserving property of CS samples, we propose a new fast dictionary
learning (DL) algorithm which is able to extract prototype signals from
compressive samples for efficient sparse representation and recovery of
signals. These prototype signals are orthogonal intrinsic mode functions (IMFs)
extracted using empirical mode decomposition (EMD), which is one of the popular
methods to capture the envelope of a signal. The extracted IMFs are used to
build the dictionary without even comprehending the original signal or the
sensing matrix. Moreover, one can build the dictionary on-line as new CS
samples are available. In particularly, to recover first $L$ signals
($\in\mathbb{R}^n$) at the decoder, one can build the dictionary in just
$\mathcal{O}(nL\log n)$ operations, that is far less as compared to existing
approaches. The efficiency of the proposed approach is demonstrated
experimentally for recovery of speech signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07080</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07080</id><created>2015-07-25</created><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author></authors><title>Range Predecessor and Lempel-Ziv Parsing</title><categories>cs.DS</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Lempel-Ziv parsing of a string (LZ77 for short) is one of the most
important and widely-used algorithmic tools in data compression and string
processing. We show that the Lempel-Ziv parsing of a string of length $n$ on an
alphabet of size $\sigma$ can be computed in $O(n\log\log\sigma)$ time ($O(n)$
time if we allow randomization) using $O(n\log\sigma)$ bits of working space;
that is, using space proportional to that of the input string in bits. The
previous fastest algorithm using $O(n\log\sigma)$ space takes
$O(n(\log\sigma+\log\log n))$ time. We also consider the important rightmost
variant of the problem, where the goal is to associate with each phrase of the
parsing its most recent occurrence in the input string. We solve this problem
in $O(n(1 + (\log\sigma/\sqrt{\log n}))$ time, using the same working space as
above. The previous best solution for rightmost parsing uses
$O(n(1+\log\sigma/\log\log n))$ time and $O(n\log n)$ space. As a bonus, in our
solution for rightmost parsing we provide a faster construction method for
efficient 2D orthogonal range reporting, which is of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07086</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07086</id><created>2015-07-25</created><authors><author><keyname>Spiegelman</keyname><forenames>Alexander</forenames></author><author><keyname>Keidar</keyname><forenames>Idit</forenames></author></authors><title>On Liveness of Dynamic Storage</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic distributed storage algorithms such as DynaStore, Reconfigurable
Paxos, RAMBO, and RDS, do not ensure liveness (wait-freedom) in asynchronous
runs with infinitely many reconfigurations. We prove that this is inherent for
asynchronous dynamic storage algorithms, including ones that use $\Omega$ or
$\diamond S$ oracles. Our result holds even if only one process may fail,
provided that machines that were successfully removed from the system's
configuration may be switched off by an administrator. Intuitively, the
impossibility relies on the fact that a correct process can be suspected to
have failed at any time, i.e., its failure is indistinguishable to other
processes from slow delivery of its messages, and so the system should be able
to reconfigure without waiting for this process to complete its pending
operations.
  To circumvent this result, we define a dynamic eventually perfect failure
detector, and present an algorithm that uses it to emulate wait-free dynamic
atomic storage (with no restrictions on reconfiguration rate). Together, our
results thus draw a sharp line between oracles like $\Omega$ and $\diamond S$,
which allow some correct process to continue to be suspected forever, and a
dynamic eventually perfect one, which does not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07091</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07091</id><created>2015-07-25</created><authors><author><keyname>Bassi</keyname><forenames>Germ&#xe1;n</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author><author><keyname>Shitz</keyname><forenames>Shlomo Shamai</forenames></author></authors><title>The Wiretap Channel with Generalized Feedback: Secure Communication and
  Key Generation</title><categories>cs.IT math.IT</categories><comments>62 pages, 10 figures. Submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a well-known fact that feedback does not increase the capacity of
point-to-point memoryless channels, however, its effect in secure
communications is not fully understood yet. In this work, two achievable
schemes for the wiretap channel with generalized feedback are presented. One of
the schemes, which is based on joint source-channel coding, correlates the
codewords to be transmitted with the feedback signal in the hope of &quot;hiding&quot;
them from the eavesdropper's observations. The other scheme, which uses the
feedback signal to generate a shared secret key between the legitimate users,
encrypts the message to be sent at the bit level. These schemes recover
previous known results from the literature, thus they can be seen as a
generalization and hence unification of several results in the field.
Additionally, we present new capacity results for a class of channels and, for
the Gaussian wiretap channel with noisy feedback, the schemes are shown to
achieve positive secrecy rates even in unfavorable scenarios where the
eavesdropper experiences much better channel conditions than the legitimate
user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07094</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07094</id><created>2015-07-25</created><updated>2015-08-06</updated><authors><author><keyname>Lopes</keyname><forenames>Miles E.</forenames></author></authors><title>Compressed Sensing without Sparsity Assumptions</title><categories>cs.IT math.IT math.ST stat.ME stat.ML stat.TH</categories><comments>This is version 2. Simulations have been added. 40 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of Compressed Sensing asserts that an unknown signal
$x\in\mathbb{R}^p$ can be accurately recovered from an underdetermined set of
$n$ linear measurements with $n\ll p$, provided that $x$ is sufficiently
sparse. However, in applications, the degree of sparsity $\|x\|_0$ is typically
unknown, and the problem of directly estimating $\|x\|_0$ has been a
longstanding gap between theory and practice. A closely related issue is that
$\|x\|_0$ is a highly idealized measure of sparsity, and for real signals with
entries not exactly equal to 0, the value $\|x\|_0=p$ is not a useful
description of compressibility. In our previous conference paper that examined
these problems, Lopes 2013, we considered an alternative measure of &quot;soft&quot;
sparsity, $\|x\|_1^2/\|x\|_2^2$, and designed a procedure to estimate
$\|x\|_1^2/\|x\|_2^2$ that does not rely on sparsity assumptions.
  The present work offers a new deconvolution-based method for estimating
unknown sparsity, which has wider applicability and sharper theoretical
guarantees. Whereas our earlier work was limited to estimating the quantity
$\|x\|_1^2/\|x\|_2^2$, the current paper introduces a family of entropy-based
sparsity measures $s_q(x):=\big(\frac{\|x\|_q}{\|x\|_1}\big)^{\frac{q}{1-q}}$
parameterized by $q\in[0,\infty]$. Two other main advantages of the new
approach are that it handles measurement noise with infinite variance, and that
it yields confidence intervals for $s_q(x)$ with asymptotically exact coverage
probability (whereas our previous intervals were conservative). In addition to
confidence intervals, we also analyze several other aspects of our proposed
estimator $\hat{s}_q(x)$ and show that randomized measurements are an essential
aspect of our procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07096</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07096</id><created>2015-07-25</created><authors><author><keyname>Prakash</keyname><forenames>R. P.</forenames></author><author><keyname>Prakash</keyname><forenames>Keerthana S.</forenames></author><author><keyname>Binu</keyname><forenames>V. P.</forenames></author></authors><title>Thinning Algorithm Using Hypergraph Based Morphological Operators</title><categories>cs.CV</categories><comments>Advance Computing Conference (IACC), 2015 IEEE International,Banglore
  India</comments><journal-ref>IEEE Xplore Advance Computing Conference (IACC) Proceedings Pages:
  1017 - 1020,,Year: 2015</journal-ref><doi>10.1109/IADCC.2015.7154860</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The object recognition is a complex problem in the image processing.
Mathematical morphology is Shape oriented operations, that simplify image data,
preserving their essential shape characteristics and eliminating irrelevancies.
This paper briefly describes morphological operators using hypergraph and its
applications for thinning algorithms. The morphological operators using
hypergraph method is used to preventing errors and irregularities in skeleton,
and is an important step recognizing line objects. The morphological operators
using hypergraph such as dilation, erosion, opening, closing is a novel
approach in image processing and it act as a filter remove the noise and errors
in the images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07105</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07105</id><created>2015-07-25</created><updated>2015-12-13</updated><authors><author><keyname>Heckel</keyname><forenames>Reinhard</forenames></author><author><keyname>Tschannen</keyname><forenames>Michael</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Dimensionality-reduced subspace clustering</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>new results for the noisy case, additional simulation work,
  additional discussions in the main body</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace clustering refers to the problem of clustering unlabeled
high-dimensional data points into a union of low-dimensional linear subspaces,
whose number, orientations, and dimensions are all unknown. In practice one may
have access to dimensionality-reduced observations of the data only, resulting,
e.g., from undersampling due to complexity and speed constraints on the
acquisition device or mechanism. More pertinently, even if the high-dimensional
data set is available it is often desirable to first project the data points
into a lower-dimensional space and to perform clustering there; this reduces
storage requirements and computational cost. The purpose of this paper is to
quantify the impact of dimensionality reduction through random projection on
the performance of three subspace clustering algorithms, all of which are based
on principles from sparse signal recovery. Specifically, we analyze the
thresholding based subspace clustering (TSC) algorithm, the sparse subspace
clustering (SSC) algorithm, and an orthogonal matching pursuit variant thereof
(SSC-OMP). We find, for all three algorithms, that dimensionality reduction
down to the order of the subspace dimensions is possible without incurring
significant performance degradation. Moreover, these results are order-wise
optimal in the sense that reducing the dimensionality further leads to a
fundamentally ill-posed clustering problem. Our findings carry over to the
noisy case as illustrated through analytical results for TSC and simulations
for SSC and SSC-OMP. Extensive experiments on synthetic and real data
complement our theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07109</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07109</id><created>2015-07-25</created><authors><author><keyname>Forelle</keyname><forenames>Michelle</forenames></author><author><keyname>Howard</keyname><forenames>Phil</forenames></author><author><keyname>Monroy-Hern&#xe1;ndez</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>Savage</keyname><forenames>Saiph</forenames></author></authors><title>Political Bots and the Manipulation of Public Opinion in Venezuela</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>8 pages, 3 figures</comments><acm-class>H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social and political bots have a small but strategic role in Venezuelan
political conversations. These automated scripts generate content through
social media platforms and then interact with people. In this preliminary study
on the use of political bots in Venezuela, we analyze the tweeting, following
and retweeting patterns for the accounts of prominent Venezuelan politicians
and prominent Venezuelan bots. We find that bots generate a very small
proportion of all the traffic about political life in Venezuela. Bots are used
to retweet content from Venezuelan politicians but the effect is subtle in that
less than 10 percent of all retweets come from bot-related platforms.
Nonetheless, we find that the most active bots are those used by Venezuela's
radical opposition. Bots are pretending to be political leaders, government
agencies and political parties more than citizens. Finally, bots are promoting
innocuous political events more than attacking opponents or spreading
misinformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07115</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07115</id><created>2015-07-25</created><authors><author><keyname>Shi</keyname><forenames>Qingjiang</forenames></author><author><keyname>Razavayan</keyname><forenames>Meisam</forenames></author><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author></authors><title>SINR Constrained Beamforming for a MIMO Multi-user Downlink System</title><categories>cs.IT math.IT</categories><comments>34 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a multi-input multi-output (MIMO) downlink multi-user channel. A
well-studied problem in such system is the design of linear beamformers for
power minimization with the quality of service (QoS) constraints. The most
representative algorithms for solving this class of problems are the so-called
MMSE-SOCP algorithm [11-12] and the UDD algorithm [9]. The former is based on
alternating optimization of the transmit and receive beamformers, while the
latter is based on the well-known uplink-dowlink duality theory. Despite their
wide applicability, the convergence (to KKT solutions) of both algorithms is
still open in the literature. In this paper, we rigorously establish the
convergence of these algorithms for QoS-constrained power minimization (QCPM)
problem with both single stream and multiple streams per user cases. Key to our
analysis is the development and analysis of a new MMSE-DUAL algorithm, which
connects the MMSE-SOCP and the UDD algorithm. Our numerical experiments show
that 1) all these algorithms can almost always reach points with the same
objective value irrespective of initialization, 2) the MMSE-SOCP/MMSE-DUAL
algorithm works well while the UDD algorithm may fail with an infeasible
initialization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07121</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07121</id><created>2015-07-25</created><updated>2015-09-02</updated><authors><author><keyname>van Zuylen</keyname><forenames>Anke</forenames></author></authors><title>Improved Approximations for Cubic and Cubic Bipartite TSP</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show improved approximation guarantees for the traveling salesman problem
on cubic graphs, and cubic bipartite graphs. For cubic bipartite graphs with n
nodes, we improve on recent results of Karp and Ravi (2014) by giving a simple
&quot;local improvement&quot; algorithm that finds a tour of length at most 5/4 n - 2.
For 2-connected cubic graphs, we show that the techniques of Moemke and
Svensson (2011) can be combined with the techniques of Correa, Larre and Soto
(2012), to obtain a tour of length at most (4/3-1/8754)n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07126</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07126</id><created>2015-07-25</created><updated>2016-02-24</updated><authors><author><keyname>di Bernardo</keyname><forenames>Mario</forenames></author><author><keyname>Fiore</keyname><forenames>Davide</forenames></author><author><keyname>Hogan</keyname><forenames>S. John</forenames></author></authors><title>Contraction analysis of switched Filippov systems via regularization</title><categories>cs.SY</categories><comments>Preprint submitted to Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study incremental stability and convergence of switched
(bimodal) Filippov systems via contraction analysis. In particular, by using
results on regularization of switched dynamical systems we derive sufficient
conditions for convergence of any two trajectories of the Filippov system
between each other within some region of interest: namely that both modes of
the system should be contracting and that the difference of the two modes
evaluated at the switching manifold $\Sigma$ should satisfy an additional
condition. We then apply these conditions to the study of different classes of
Filippov systems including piecewise smooth (PWS) systems, piecewise affine
(PWA) systems and relay feedback systems. We show that the conditions allow the
system to be studied in metrics other than the Euclidean norm. The theoretical
results are illustrated by numerical simulations on a set of representative
examples that confirm their effectiveness and ease of application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07134</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07134</id><created>2015-07-25</created><updated>2015-12-07</updated><authors><author><keyname>Perelman</keyname><forenames>Lina Sela</forenames></author><author><keyname>Abbas</keyname><forenames>Waseem</forenames></author><author><keyname>Koutsoukos</keyname><forenames>Xenofon</forenames></author><author><keyname>Amin</keyname><forenames>Saurabh</forenames></author></authors><title>Sensor placement for fault location identification in water networks: A
  minimum test cover approach</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the optimal sensor placement problem for identification
of pipe failure locations in large-scale urban water systems. We cast the
problem of identifying the locations of pipe failures as the minimum test cover
(MTC) problem, which involves selecting the minimum number of sensors such that
every pipe failure can be uniquely localized. We consider two approaches to
solve the MTC problem, which is NP-hard. In the first approach, we transform
the MTC problem to a minimum set cover (MSC), problem and use the greedy
algorithm that exploits the submodularity property of the MSC to compute the
solution to the MTC problem. In the second approach, we develop the
\textit{augmented greedy} algorithm for solving the MTC problem that does not
require the transformation of the MTC. The augmented greedy algorithm results
in a significant computational improvement while keeping the same approximation
ratio. We propose several metrics to evaluate the performance of the sensor
placement designs. Finally, we present detailed computational experiments for a
number of real water distribution networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07141</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07141</id><created>2015-07-25</created><authors><author><keyname>Wang</keyname><forenames>Ying</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author></authors><title>A QoS-based Power Allocation for Cellular Users with Different
  Modulations</title><categories>cs.NI</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel optimal power allocation method that
features a power limit function and is able to ensure more users reach the
desired Quality-of-Service (QoS). In our model we use sigmoidal-like utility
functions to represent the probability of successful reception of packets at
user equipment (UE)s. Given that each UE has a different channel quality and
different location from base station (BS), it has different CQI and modulation.
For each CQI zone, we evaluate the power threshold which is required to achieve
the minimum QoS for each UE and show that the higher CQI the lower power
threshold is. We present a resource allocation algorithm that gives limited
resources to UEs who have already reached their pre-specified minimum QoS, and
provides more possible resources to UEs who can not reach it. We also compare
this algorithm with the optimal power allocation algorithm in [1] to show the
enhancement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07146</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07146</id><created>2015-07-25</created><authors><author><keyname>Wang</keyname><forenames>Dayong</forenames></author><author><keyname>Wu</keyname><forenames>Pengcheng</forenames></author><author><keyname>Zhao</keyname><forenames>Peilin</forenames></author><author><keyname>Hoi</keyname><forenames>Steven C. H.</forenames></author></authors><title>A Framework of Sparse Online Learning and Its Applications</title><categories>cs.LG</categories><comments>13 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The amount of data in our society has been exploding in the era of big data
today. In this paper, we address several open challenges of big data stream
classification, including high volume, high velocity, high dimensionality, high
sparsity, and high class-imbalance. Many existing studies in data mining
literature solve data stream classification tasks in a batch learning setting,
which suffers from poor efficiency and scalability when dealing with big data.
To overcome the limitations, this paper investigates an online learning
framework for big data stream classification tasks. Unlike some existing online
data stream classification techniques that are often based on first-order
online learning, we propose a framework of Sparse Online Classification (SOC)
for data stream classification, which includes some state-of-the-art
first-order sparse online learning algorithms as special cases and allows us to
derive a new effective second-order online learning algorithm for data stream
classification. In addition, we also propose a new cost-sensitive sparse online
learning algorithm by extending the framework with application to tackle online
anomaly detection tasks where class distribution of data could be very
imbalanced. We also analyze the theoretical bounds of the proposed method, and
finally conduct an extensive set of experiments, in which encouraging results
validate the efficacy of the proposed algorithms in comparison to a family of
state-of-the-art techniques on a variety of data stream classification tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07147</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07147</id><created>2015-07-25</created><authors><author><keyname>Sutton</keyname><forenames>Richard S.</forenames></author></authors><title>True Online Emphatic TD($\lambda$): Quick Reference and Implementation
  Guide</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document is a guide to the implementation of true online emphatic
TD($\lambda$), a model-free temporal-difference algorithm for learning to make
long-term predictions which combines the emphasis idea (Sutton, Mahmood &amp; White
2015) and the true-online idea (van Seijen &amp; Sutton 2014). The setting used
here includes linear function approximation, the possibility of off-policy
training, and all the generality of general value functions, as well as the
emphasis algorithm's notion of &quot;interest&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07148</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07148</id><created>2015-07-25</created><authors><author><keyname>Tomasello</keyname><forenames>Mario Vincenzo</forenames></author><author><keyname>Tessone</keyname><forenames>Claudio Juan</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Quantifying knowledge exchange in R&amp;D networks: a data-driven model</title><categories>physics.soc-ph cs.SI</categories><comments>31 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an agent-based model to reproduce the process of link formation
and to understand the effect of knowledge exchange in collaborative inter-firm
networks of Research and Development (R&amp;D) alliances. In our model, agents form
links based on their previous alliance history and then exchange knowledge with
their partners, thus approaching in a knowledge space. We validate our model
against real data using a two-step approach. Through an inter-firm alliance
dataset, we estimate the model parameters related to the alliance formation, at
the same time reproducing the topology of the resulting collaboration network.
Subsequently, using a dataset on firm patents, we estimate the parameters
related to the process of knowledge exchange. The underlying knowledge space
that we consider in our study is defined by real patent classes, allowing for a
precise quantification of every firm's knowledge position. We find that real
R&amp;D alliances have a duration of around two years, and that the subsequent
knowledge exchange occurs at an extremely low rate - a firm's position is
rather a determinant than a consequence of its R&amp;D alliances. Finally, we
propose an indicator of collaboration performance for the whole network and,
remarkably, we find that the empirical R&amp;D network extracted from our data does
not maximize such an indicator. However, we find that there exist
configurations that can be both realistic and optimized with respect to the
collaboration performance. Effective policies, as suggested by our model, would
incentivize shorter R&amp;D alliances and higher knowledge exchange rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07152</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07152</id><created>2015-07-25</created><authors><author><keyname>Abeliuk</keyname><forenames>Andres</forenames></author><author><keyname>Berbeglia</keyname><forenames>Gerardo</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>Bargaining Mechanisms for One-Way Games</title><categories>cs.GT</categories><comments>An earlier, shorter version of this paper appeared in Proceedings of
  the Twenty-Fourth International joint conference on Artificial Intelligence
  (IJCAI) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce one-way games, a framework motivated by applications in
large-scale power restoration, humanitarian logistics, and integrated
supply-chains. The distinguishable feature of the games is that the payoff of
some player is determined only by her own strategy and does not depend on
actions taken by other players. We show that the equilibrium outcome in one-way
games without payments and the social cost of any ex-post efficient mechanism,
can be far from the optimum. We also show that it is impossible to design a
Bayes-Nash incentive-compatible mechanism for one-way games that is
budget-balanced, individually rational, and efficient. To address this negative
result, we propose a privacy-preserving mechanism that is incentive-compatible
and budget-balanced, satisfies ex-post individual rationality conditions, and
produces an outcome which is more efficient than the equilibrium without
payments. The mechanism is based on a single-offer bargaining and we show that
a randomized multi-offer extension brings no additional benefit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07155</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07155</id><created>2015-07-25</created><authors><author><keyname>Brkic</keyname><forenames>Srdan</forenames></author><author><keyname>Ivanis</keyname><forenames>Predrag</forenames></author><author><keyname>Vasic</keyname><forenames>Bane</forenames></author></authors><title>Majority Logic Decoding under Data-Dependent Logic Gate Failures</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A majority logic decoder made of unreliable logic gates, whose failures are
transient and datadependent, is analyzed. Based on a combinatorial
representation of fault configurations a closed-form expression for the average
bit error rate for an one-step majority logic decoder is derived, for a regular
low-density parity-check (LDPC) code ensemble and the proposed failure model.
The presented analysis framework is then used to establish bounds on the
one-step majority logic decoder performance under the simplified probabilistic
gate-output switching model. Based on the expander property of Tanner graphs of
LDPC codes, it is proven that a version of the faulty parallel bit flipping
decoder can correct a fixed fraction of channel errors in the presence of
data-dependent gate failures. The results are illustrated with numerical
examples of finite geometry codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07159</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07159</id><created>2015-07-25</created><authors><author><keyname>Wang</keyname><forenames>Ying</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author></authors><title>Optimal Power Allocation for LTE Users with Different Modulations</title><categories>cs.NI</categories><comments>5 pages and 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we demonstrate the optimal power allocation for QPSK, 16-QAM,
and 64-QAM modulation schedules and the role of channel quality indicator
(CQI). We used sigmoidal-like utility functions to represent the probability of
successful reception of packets at user equipment (UE). CQI as a feedback to
the base station (BS) indicates the data rate that a downlink channel can
support. With Levenberg-Marquardt (LM) Optimization method, we present utility
functions of different CQI values for standardized 15 Modulation order and
Coding Scheme (MCS) in $3^{rd}$ Generation Partnership Project (3GPP). Finally,
we simulate and show the results of the optimal power allocation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07161</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07161</id><created>2015-07-25</created><updated>2015-10-18</updated><authors><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>An Optimal Resource Allocation with Frequency Reuse in Cellular Networks</title><categories>cs.NI</categories><comments>(c) 2015 IEEE. Personal use of this material is permitted. Permission
  from IEEE must be obtained for all other uses, in any current or future
  media, including reprinting/republishing this material for advertising or
  promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a novel approach for optimal resource allocation
with frequency reuse for users with elastic and inelastic traffic in cellular
networks. In our model, we represent users' applications running on different
user equipments (UE)s by logarithmic and sigmoid utility functions. We applied
utility proportional fairness allocation policy, i.e. the resources are
allocated among users with fairness in utility percentage of the application
running on each mobile station. Our objective is to allocate the cellular
system resources to mobile users optimally from a multi-cell network. In our
model, a minimum quality-of-service (QoS) is guaranteed to every user
subscribing for the mobile service with priority given to users with real-time
applications. We show that the novel resource allocation optimization problem
with frequency reuse is convex and therefore the optimal solution is tractable.
We present a distributed algorithm to allocate the resources optimally from
Mobility Management Entity (MME) to base stations (BS)s sectors. Finally, we
present the simulation results for the performance of our rate allocation
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07184</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07184</id><created>2015-07-26</created><authors><author><keyname>Kabanava</keyname><forenames>Maryia</forenames></author><author><keyname>Kueng</keyname><forenames>Richard</forenames></author><author><keyname>Rauhut</keyname><forenames>Holger</forenames></author><author><keyname>Terstiege</keyname><forenames>Ulrich</forenames></author></authors><title>Stable low-rank matrix recovery via null space properties</title><categories>cs.IT math.IT math.PR quant-ph</categories><comments>26 pages</comments><msc-class>94A20, 94A12, 60B20, 90C25, 81P50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of recovering a matrix of low rank from an incomplete and
possibly noisy set of linear measurements arises in a number of areas. In order
to derive rigorous recovery results, the measurement map is usually modeled
probabilistically. We derive sufficient conditions on the minimal amount of
measurements ensuring recovery via convex optimization. We establish our
results via certain properties of the null space of the measurement map. In the
setting where the measurements are realized as Frobenius inner products with
independent standard Gaussian random matrices we show that $10 r (n_1 + n_2)$
measurements are enough to uniformly and stably recover an $n_1 \times n_2$
matrix of rank at most $r$. We then significantly generalize this result by
only requiring independent mean-zero, variance one entries with four finite
moments at the cost of replacing $10$ by some universal constant. We also study
the case of recovering Hermitian rank-$r$ matrices from measurement matrices
proportional to rank-one projectors. For $m \geq C r n$ rank-one projective
measurements onto independent standard Gaussian vectors, we show that nuclear
norm minimization uniformly and stably reconstructs Hermitian rank-$r$ matrices
with high probability. Next, we partially de-randomize this by establishing an
analogous statement for projectors onto independent elements of a complex
projective 4-designs at the cost of a slightly higher sampling rate $m \geq C
rn \log n$. Moreover, if the Hermitian matrix to be recovered is known to be
positive semidefinite, then we show that the nuclear norm minimization approach
may be replaced by minimizing the $\ell_2$-norm of the residual subject to the
positive semidefinite constraint. Then no estimate of the noise level is
required a priori. We discuss applications in quantum physics and the phase
retrieval problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07190</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07190</id><created>2015-07-26</created><authors><author><keyname>Dong</keyname><forenames>Daoyi</forenames></author><author><keyname>Mabrok</keyname><forenames>Mohamed A.</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Qi</keyname><forenames>Bo</forenames></author><author><keyname>Chen</keyname><forenames>Chunlin</forenames></author><author><keyname>Rabitz</keyname><forenames>Herschel</forenames></author></authors><title>Sampling-based Learning Control for Quantum Systems with Uncertainties</title><categories>quant-ph cs.SY</categories><comments>11 pages, 9 figures, in press, IEEE Transactions on Control Systems
  Technology, 2015</comments><doi>10.1109/TCST.2015.2404292</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust control design for quantum systems has been recognized as a key task
in the development of practical quantum technology. In this paper, we present a
systematic numerical methodology of sampling-based learning control (SLC) for
control design of quantum systems with uncertainties. The SLC method includes
two steps of &quot;training&quot; and &quot;testing&quot;. In the training step, an augmented
system is constructed using artificial samples generated by sampling
uncertainty parameters according to a given distribution. A gradient flow based
learning algorithm is developed to find the control for the augmented system.
In the process of testing, a number of additional samples are tested to
evaluate the control performance where these samples are obtained through
sampling the uncertainty parameters according to a possible distribution. The
SLC method is applied to three significant examples of quantum robust control
including state preparation in a three-level quantum system, robust
entanglement generation in a two-qubit superconducting circuit and quantum
entanglement control in a two-atom system interacting with a quantized field in
a cavity. Numerical results demonstrate the effectiveness of the SLC approach
even when uncertainties are quite large, and show its potential for robust
control design of quantum systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07191</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07191</id><created>2015-07-26</created><updated>2015-07-30</updated><authors><author><keyname>Bahar</keyname><forenames>Gal</forenames></author><author><keyname>Smorodinsky</keyname><forenames>Rann</forenames></author><author><keyname>Tennenholtz</keyname><forenames>Moshe</forenames></author></authors><title>Economic Recommendation Systems</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the on-line Explore and Exploit literature, central to Machine Learning, a
central planner is faced with a set of alternatives, each yielding some unknown
reward. The planner's goal is to learn the optimal alternative as soon as
possible, via experimentation. A typical assumption in this model is that the
planner has full control over the experiment design and implementation. When
experiments are implemented by a society of self-motivated agents the planner
can only recommend experimentation but has no power to enforce it. Kremer et al
(JPE, 2014) introduce the first study of explore and exploit schemes that
account for agents' incentives. In their model it is implicitly assumed that
agents do not see nor communicate with each other. Their main result is a
characterization of an optimal explore and exploit scheme. In this work we
extend Kremer et al (JPE, 2014) by adding a layer of a social network according
to which agents can observe each other. It turns out that when observability is
factored in the scheme proposed by Kremer et al (JPE, 2014) is no longer
incentive compatible. In our main result we provide a tight bound on how many
other agents can each agent observe and still have an incentive-compatible
algorithm and asymptotically optimal outcome. More technically, for a setting
with N agents where the number of nodes with degree greater than N^alpha is
bounded by N^beta and 2*alpha+beta &lt; 1 we construct incentive-compatible
asymptotically optimal mechanism. The bound 2*alpha+beta &lt; 1 is shown to be
tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07199</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07199</id><created>2015-07-26</created><authors><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Task Selection for Bandit-Based Task Assignment in Heterogeneous
  Crowdsourcing</title><categories>cs.LG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1507.05800</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Task selection (picking an appropriate labeling task) and worker selection
(assigning the labeling task to a suitable worker) are two major challenges in
task assignment for crowdsourcing. Recently, worker selection has been
successfully addressed by the bandit-based task assignment (BBTA) method, while
task selection has not been thoroughly investigated yet. In this paper, we
experimentally compare several task selection strategies borrowed from active
learning literature, and show that the least confidence strategy significantly
improves the performance of task assignment in crowdsourcing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07200</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07200</id><created>2015-07-26</created><authors><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author><author><keyname>Micor</keyname><forenames>Jose Rene L.</forenames></author><author><keyname>Mojica</keyname><forenames>Elmer Rico E.</forenames></author></authors><title>A Neural Prototype for a Virtual Chemical Spectrophotometer</title><categories>cs.NE</categories><comments>5 pages, 3 figures, appeared in Proceedings (CDROM) of the 6th
  National Conference on IT in Education (NCITE 2008), University of the
  Philippines Los Ba\~nos, 23-24 October 2008</comments><journal-ref>Philippine Computing Journal, 4(2):39-42, 2009</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  A virtual chemical spectrophotometer for the simultaneous analysis of nickel
(Ni) and cobalt (Co) was developed based on an artificial neural network (ANN).
The developed ANN correlates the respective concentrations of Co and Ni given
the absorbance profile of a Co-Ni mixture based on the Beer's Law. The virtual
chemical spectrometer was trained using a 3-layer jump connection neural
network model (NNM) with 126 input nodes corresponding to the 126 absorbance
readings from 350 nm to 600 nm, 70 nodes in the hidden layer using a logistic
activation function, and 2 nodes in the output layer with a logistic function.
Test result shows that the NNM has correlation coefficients of 0.9953 and
0.9922 when predicting [Co] and [Ni], respectively. We observed, however, that
the NNM has a duality property and that there exists a real-world practical
application in solving the dual problem: Predict the Co-Ni mixture's absorbance
profile given [Co] and [Ni]. It turns out that the dual problem is much harder
to solve because the intended output has a much bigger cardinality than that of
the input. Thus, we trained the dual ANN, a 3-layer jump connection nets with 2
input nodes corresponding to [Co] and [Ni], 70-logistic-activated nodes in the
hidden layer, and 126 output nodes corresponding to the 126 absorbance readings
from 250 nm to 600 nm. Test result shows that the dual NNM has correlation
coefficients that range from 0.9050 through 0.9980 at 356 nm through 578 nm
with the maximum coefficient observed at 480 nm. This means that the dual ANN
can be used to predict the absorbance profile given the respective Co-Ni
concentrations which can be of importance in creating academic models for a
virtual chemical spectrophotometer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07203</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07203</id><created>2015-07-26</created><authors><author><keyname>Ngoho</keyname><forenames>Louie Vincent A.</forenames></author><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>Capturing the Dynamics of Pedestrian Traffic Using a Machine Vision
  System</title><categories>cs.CV</categories><comments>11 pages, 10 figures, appeared in Proceedings (CDROM) of the 7th
  National Conference on IT Education (NCITE 2009), Capitol University, Cagayan
  De Oro City, Philippines, 21-23 October 2009</comments><journal-ref>Philippine Information Technology Journal, 2(2):1-11, 2009</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We developed a machine vision system to automatically capture the dynamics of
pedestrians under four different traffic scenarios. By considering the overhead
view of each pedestrian as a digital object, the system processes the image
sequences to track the pedestrians. Considering the perspective effect of the
camera lens and the projected area of the hallway at the top-view scene, the
distance of each tracked object from its original position to its current
position is approximated every video frame. Using the approximated distance and
the video frame rate (30 frames per second), the respective velocity and
acceleration of each tracked object are later derived. The quantified motion
characteristics of the pedestrians are displayed by the system through
2-dimensional graphs of the kinematics of motion. The system also outputs video
images of the pedestrians with superimposed markers for tracking. These visual
markers were used to visually describe and quantify the behavior of the
pedestrians under different traffic scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07204</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07204</id><created>2015-07-26</created><authors><author><keyname>Shoaib</keyname><forenames>Yasir</forenames></author><author><keyname>Das</keyname><forenames>Olivia</forenames></author></authors><title>Modeling Website Workload Using Neural Networks</title><categories>cs.DC cs.NE</categories><comments>25 pages, 13 figures, 21 references, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, artificial neural networks (ANN) are used for modeling the
number of requests received by 1998 FIFA World Cup website. Modeling is done by
means of time-series forecasting. The log traces of the website, available
through the Internet Traffic Archive (ITA), are processed to obtain two
time-series data sets that are used for finding the following measurements:
requests/day and requests/second. These are modeled by training and simulating
ANN. The method followed to collect and process the data, and perform the
experiments have been detailed in this article. In total, 13 cases have been
tried and their results have been presented, discussed, compared and
summarized. Lastly, future works have also been mentioned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07205</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07205</id><created>2015-07-26</created><authors><author><keyname>Liu</keyname><forenames>Xiaofei</forenames></author><author><keyname>Pequito</keyname><forenames>Sergio</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Sinopoli</keyname><forenames>Bruno</forenames></author><author><keyname>Aguiar</keyname><forenames>A. Pedro</forenames></author></authors><title>Minimum Sensor Placement for Robust Observability of Structured Complex
  Networks</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses problems on the robust structural design of complex
networks. More precisely, we address the problem of deploying the minimum
number of dedicated sensors, i.e., those measuring a single state variable,
that ensure the network to be structurally observable under disruptive
scenarios. The disruptive scenarios considered are as follows: (i) the
malfunction/loss of one arbitrary sensor, and (ii) the failure of connection
(either unidirectional or bidirectional communication) between a pair of
agents. First, we show these problems to be NP-hard, which implies that
efficient algorithms to determine a solution are unlikely to exist. Secondly,
we propose an intuitive two step approach: (1) we achieve an arbitrary minimum
sensor placement ensuring structural observability; (2) we develop a sequential
process to find minimum number of additional sensors required for robust
observability. This step can be solved by recasting it as a weighted set
covering problem. Although this is known to be an NP-hard problem, feasible
approximations can be determined in polynomial-time that can be used to obtain
feasible approximations to the robust structural design problems with
optimality guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07207</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07207</id><created>2015-07-26</created><authors><author><keyname>Pequito</keyname><forenames>Sergio</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Structural Minimum Controllability Problem for Linear Continuous-Time
  Switching Systems</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses a problem on the structural design of control systems,
and explicitly takes into consideration the possible application to large-scale
systems. More precisely, we aim to determine the minimum number of
manipulated/measured state variables ensuring structural controllability/
observability of the linear continuous-time switching system. Further, the
solution can be determined by an efficient procedure, i.e., polynomial in the
number of state variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07217</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07217</id><created>2015-07-26</created><authors><author><keyname>Hari</keyname><forenames>Adiseshu</forenames></author><author><keyname>Niesen</keyname><forenames>Urs</forenames></author><author><keyname>Wilfong</keyname><forenames>Gordon</forenames></author></authors><title>Optimal Path Encoding for Software-Defined Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Packet networks need to maintain state in the form of forwarding tables at
each switch. The cost of this state increases as networks support ever more
sophisticated per-flow routing, traffic engineering, and service chaining.
Per-flow or per-path state at the switches can be eliminated by encoding each
packet's desired path in its header. A key component of such a method is an
efficient encoding of paths through the network. We introduce a mathematical
formulation of this optimal path-encoding problem. We prove that the problem is
APX-hard, by showing that approximating it to within a factor less than 8/7 is
NP-hard. Thus, at best we can hope for a constant-factor approximation
algorithm. We then present such an algorithm, approximating the optimal
path-encoding problem to within a factor 2. Finally, we provide empirical
results illustrating the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07225</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07225</id><created>2015-07-26</created><authors><author><keyname>Yin</keyname><forenames>Yitong</forenames></author><author><keyname>Zhang</keyname><forenames>Chihao</forenames></author></authors><title>Spatial mixing and approximate counting for Potts model on graphs with
  bounded average degree</title><categories>cs.DS</categories><comments>The result of this paper generalizes the result of our previous work
  arXiv:1503.03351</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a notion of contraction function for a family of graphs and
establish its connection to the strong spatial mixing for spin systems. More
specifically, we show that for anti-ferromagnetic Potts model on families of
graphs characterized by a specific contraction function, the model exhibits
strong spatial mixing, and if further the graphs exhibit certain local sparsity
which are very natural and easy to satisfy by typical sparse graphs, then we
also have FPTAS for computing the partition function.
  This new characterization of strong spatial mixing of multi-spin system does
not require maximum degree of the graphs to be bounded, but instead it relates
the decay of correlation of the model to a notion of effective average degree
measured by the contraction of a function on the family of graphs. It also
generalizes other notion of effective average degree which may determine the
strong spatial mixing, such as the connective constant, whose connection to
strong spatial mixing is only known for very simple models and is not
extendable to general spin systems.
  As direct consequences: (1) we obtain FPTAS for the partition function of
$q$-state anti-ferromagnetic Potts model with activity $0\le\beta&lt;1$ on graphs
of maximum degree bounded by $d$ when $q&gt; 3(1-\beta)d+1$, improving the
previous best bound $\beta&gt; 3(1-\beta)d$ and asymptotically approaching the
inapproximability threshold $q=(1-\beta)d$, and (2) we obtain an efficient
sampler (in the same sense of fully polynomial-time almost uniform sampler,
FPAUS) for the Potts model on Erd\H{o}s-R\'enyi random graph
$\mathcal{G}(n,d/n)$ with sufficiently large constant $d$, provided that $q&gt;
3(1-\beta)d+4$. In particular when $\beta=0$, the sampler becomes an FPAUS for
for proper $q$-coloring in $\mathcal{G}(n,d/n)$ with $q&gt; 3d+4$, improving the
current best bound $q&gt; 5.5d$ for FPAUS for $q$-coloring in
$\mathcal{G}(n,d/n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07227</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07227</id><created>2015-07-26</created><authors><author><keyname>Wu</keyname><forenames>Lingfei</forenames></author><author><keyname>Stathopoulos</keyname><forenames>Andreas</forenames></author><author><keyname>Laeuchli</keyname><forenames>Jesse</forenames></author><author><keyname>Kalantzis</keyname><forenames>Vassilis</forenames></author><author><keyname>Gallopoulos</keyname><forenames>Efstratios</forenames></author></authors><title>Estimating the Trace of the Matrix Inverse by Interpolating from the
  Diagonal of an Approximate Inverse</title><categories>cs.NA math.NA quant-ph</categories><comments>19 pages, 15 figures</comments><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determining the trace of a matrix that is implicitly available through a
function is a computationally challenging task that arises in a number of
applications. For the common function of the inverse of a large, sparse matrix,
the standard approach is based on a Monte Carlo method which converges slowly.
We present a different approach by exploiting the pattern correlation between
the diagonal of the inverse of the matrix and the diagonal of some approximate
inverse that can be computed inexpensively. We leverage various sampling and
fitting techniques to fit the diagonal of the approximation to the diagonal of
the inverse. Based on a dynamic evaluation of the variance, the proposed method
can be used as a variance reduction method for Monte Carlo in some cases.
Furthermore, the presented method may serve as a standalone kernel for
providing a fast trace estimate with a small number of samples. An extensive
set of experiments with various technique combinations demonstrates the
effectiveness of our method in some real applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07228</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07228</id><created>2015-07-26</created><updated>2015-08-05</updated><authors><author><keyname>Kwon</keyname><forenames>Keehang</forenames></author></authors><title>Combining Fixed-Point Definitions and Game Semantics in Logic
  Programming</title><categories>cs.LO</categories><comments>7 pages. Some bugs from the previous version are fixed. Presentations
  are simplified</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logic programming with fixed-point definitions is a useful extension of
traditional logic programming. Fixed-point definitions can capture simple model
checking problems and closed-world assumptions. Its operational semantics is
typically based on intuitionistic provability.
  We extend the operational semantics of these languages with game semantics.
This extended semantics has several interesting aspects: in particular, it
gives a logical status to the $read$ predicate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07237</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07237</id><created>2015-07-26</created><authors><author><keyname>Dobzinski</keyname><forenames>Shahar</forenames></author><author><keyname>Mor</keyname><forenames>Ami</forenames></author></authors><title>A Deterministic Algorithm for Maximizing Submodular Functions</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of maximizing a non-negative submodular function was introduced
by Feige, Mirrokni, and Vondrak [FOCS'07] who provided a deterministic
local-search based algorithm that guarantees an approximation ratio of $\frac 1
3$, as well as a randomized $\frac 2 5$-approximation algorithm. An extensive
line of research followed and various algorithms with improving approximation
ratios were developed, all of them are randomized. Finally, Buchbinder et al.
[FOCS'12] presented a randomized $\frac 1 2$-approximation algorithm, which is
the best possible.
  This paper gives the first deterministic algorithm for maximizing a
non-negative submodular function that achieves an approximation ratio better
than $\frac 1 3$. The approximation ratio of our algorithm is $\frac 2 5$. Our
algorithm is based on recursive composition of solutions obtained by the local
search algorithm of Feige et al. We show that the $\frac 2 5$ approximation
ratio can be guaranteed when the recursion depth is $2$, and leave open the
question of whether the approximation ratio improves as the recursion depth
increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07238</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07238</id><created>2015-07-26</created><authors><author><keyname>Katselis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Rojas</keyname><forenames>Cristian R.</forenames></author><author><keyname>Beck</keyname><forenames>Carolyn L.</forenames></author></authors><title>Estimator Selection: End-Performance Metric Aspects</title><categories>cs.IT math.IT stat.ML</categories><comments>arXiv admin note: substantial text overlap with arXiv:1303.4289</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a framework for application-oriented optimal experiment design has
been introduced. In this context, the distance of the estimated system from the
true one is measured in terms of a particular end-performance metric. This
treatment leads to superior unknown system estimates to classical experiment
designs based on usual pointwise functional distances of the estimated system
from the true one. The separation of the system estimator from the experiment
design is done within this new framework by choosing and fixing the estimation
method to either a maximum likelihood (ML) approach or a Bayesian estimator
such as the minimum mean square error (MMSE). Since the MMSE estimator delivers
a system estimate with lower mean square error (MSE) than the ML estimator for
finite-length experiments, it is usually considered the best choice in practice
in signal processing and control applications. Within the application-oriented
framework a related meaningful question is: Are there end-performance metrics
for which the ML estimator outperforms the MMSE when the experiment is
finite-length? In this paper, we affirmatively answer this question based on a
simple linear Gaussian regression example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07242</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07242</id><created>2015-07-26</created><updated>2015-07-28</updated><authors><author><keyname>Wang</keyname><forenames>Dayong</forenames></author><author><keyname>Otto</keyname><forenames>Charles</forenames></author><author><keyname>Jain</keyname><forenames>Anil K.</forenames></author></authors><title>Face Search at Scale: 80 Million Gallery</title><categories>cs.CV</categories><comments>14 pages, 16 figures</comments><report-no>MSU TECHNICAL REPORT MSU-CSE-15-11, JULY 24, 2015</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the prevalence of social media websites, one challenge facing computer
vision researchers is to devise methods to process and search for persons of
interest among the billions of shared photos on these websites. Facebook
revealed in a 2013 white paper that its users have uploaded more than 250
billion photos, and are uploading 350 million new photos each day. Due to this
humongous amount of data, large-scale face search for mining web images is both
important and challenging. Despite significant progress in face recognition,
searching a large collection of unconstrained face images has not been
adequately addressed. To address this challenge, we propose a face search
system which combines a fast search procedure, coupled with a state-of-the-art
commercial off the shelf (COTS) matcher, in a cascaded framework. Given a probe
face, we first filter the large gallery of photos to find the top-k most
similar faces using deep features generated from a convolutional neural
network. The k candidates are re-ranked by combining similarities from deep
features and the COTS matcher. We evaluate the proposed face search system on a
gallery containing 80 million web-downloaded face images. Experimental results
demonstrate that the deep features are competitive with state-of-the-art
methods on unconstrained face recognition benchmarks (LFW and IJB-A). Further,
the proposed face search system offers an excellent trade-off between accuracy
and scalability on datasets consisting of millions of images. Additionally, in
an experiment involving searching for face images of the Tsarnaev brothers,
convicted of the Boston Marathon bombing, the proposed face search system could
find the younger brother's (Dzhokhar Tsarnaev) photo at rank 1 in 1 second on a
5M gallery and at rank 8 in 7 seconds on an 80M gallery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07246</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07246</id><created>2015-07-26</created><updated>2015-08-02</updated><authors><author><keyname>Struth</keyname><forenames>Georg</forenames></author></authors><title>On the Expressive Power of Kleene Algebra with Domain</title><categories>cs.LO</categories><comments>Typos have been corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that antidomain semirings are more expressive than test semirings
and that Kleene algebras with domain are more expressive than Kleene algebras
with tests. It is also shown that Kleene algebras with domain are expressive
for propositional Hoare logic whereas Kleene algebras with tests are not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07256</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07256</id><created>2015-07-26</created><updated>2015-12-29</updated><authors><author><keyname>Bendory</keyname><forenames>Tamir</forenames></author><author><keyname>Bar-Zion</keyname><forenames>Avinoam</forenames></author><author><keyname>Adam</keyname><forenames>Dan</forenames></author><author><keyname>Dekel</keyname><forenames>Shai</forenames></author><author><keyname>Feuer</keyname><forenames>Arie</forenames></author></authors><title>Stable Support Recovery of Stream of Pulses with Application to
  Ultrasound Imaging</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of estimating the delays of a weighted
superposition of pulses, called stream of pulses, in a noisy environment. We
show that the delays can be estimated using a tractable convex optimization
problem with a localization error proportional to the square root of the noise
level. Furthermore, all false detections produced by the algorithm have small
amplitudes. Numerical and in-vitro ultrasound experiments corroborate the
theoretical results and demonstrate their applicability for the ultrasound
imaging signal processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07260</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07260</id><created>2015-07-26</created><authors><author><keyname>Kingravi</keyname><forenames>Hassan A.</forenames></author><author><keyname>Vela</keyname><forenames>Patricio A.</forenames></author><author><keyname>Gray</keyname><forenames>Alexandar</forenames></author></authors><title>Reduced-Set Kernel Principal Components Analysis for Improving the
  Training and Execution Speed of Kernel Machines</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a practical, and theoretically well-founded, approach to
improve the speed of kernel manifold learning algorithms relying on spectral
decomposition. Utilizing recent insights in kernel smoothing and learning with
integral operators, we propose Reduced Set KPCA (RSKPCA), which also suggests
an easy-to-implement method to remove or replace samples with minimal effect on
the empirical operator. A simple data point selection procedure is given to
generate a substitute density for the data, with accuracy that is governed by a
user-tunable parameter . The effect of the approximation on the quality of the
KPCA solution, in terms of spectral and operator errors, can be shown directly
in terms of the density estimate error and as a function of the parameter . We
show in experiments that RSKPCA can improve both training and evaluation time
of KPCA by up to an order of magnitude, and compares favorably to the
widely-used Nystrom and density-weighted Nystrom methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07264</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07264</id><created>2015-07-26</created><updated>2015-08-04</updated><authors><author><keyname>Najd</keyname><forenames>Shayan</forenames></author><author><keyname>Lindley</keyname><forenames>Sam</forenames></author><author><keyname>Svenningsson</keyname><forenames>Josef</forenames></author><author><keyname>Wadler</keyname><forenames>Philip</forenames></author></authors><title>Everything old is new again: Quoted Domain Specific Languages</title><categories>cs.PL</categories><acm-class>D.1.1; D.3.1; D.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new approach to domain specific languages (DSLs), called Quoted
DSLs (QDSLs), that resurrects two old ideas: quotation, from McCarthy's Lisp of
1960, and the subformula property, from Gentzen's natural deduction of 1935.
Quoted terms allow the DSL to share the syntax and type system of the host
language. Normalising quoted terms ensures the subformula property, which
guarantees that one can use higher-order types in the source while guaranteeing
first-order types in the target, and enables using types to guide fusion. We
test our ideas by re-implementing Feldspar, which was originally implemented as
an Embedded DSL (EDSL), as a QDSL; and we compare the QDSL and EDSL variants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07267</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07267</id><created>2015-07-26</created><updated>2015-10-18</updated><authors><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>Network MIMO with Partial Cooperation between Radar and Cellular Systems</title><categories>cs.IT math.IT</categories><comments>(c) 2015 IEEE. Personal use of this material is permitted. Permission
  from IEEE must be obtained for all other uses, in any current or future
  media, including reprinting/republishing this material for advertising or
  promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To meet the growing spectrum demands, future cellular systems are expected to
share the spectrum of other services such as radar. In this paper, we consider
a network multiple-input multiple-output (MIMO) with partial cooperation model
where radar stations cooperate with cellular base stations (BS)s to deliver
messages to intended mobile users. So the radar stations act as BSs in the
cellular system. However, due to the high power transmitted by radar stations
for detection of far targets, the cellular receivers could burnout when
receiving these high radar powers. Therefore, we propose a new projection
method called small singular values space projection (SSVSP) to mitigate these
harmful high power and enable radar stations to collaborate with cellular base
stations. In addition, we formulate the problem into a MIMO interference
channel with general constraints (MIMO-IFC-GC). Finally, we provide a solution
to minimize the weighted sum mean square error minimization problem (WSMMSE)
with enforcing power constraints on both radar and cellular stations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07277</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07277</id><created>2015-07-26</created><authors><author><keyname>Ding</keyname><forenames>Jian-Ya</forenames></author><author><keyname>You</keyname><forenames>Keyou</forenames></author><author><keyname>Song</keyname><forenames>Shiji</forenames></author><author><keyname>Wu</keyname><forenames>Cheng</forenames></author></authors><title>Likelihood Ratio Based Scheduler for Secure Detection in Cyber Physical
  Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with a binary detection problem over a non-secure
network. To satisfy the communication rate constraint and against possible
cyber attacks, which are modeled as deceptive signals injected to the network,
a likelihood ratio based (LRB) scheduler is designed in the sensor side to
smartly select sensor measurements for transmission. By exploring the
scheduler, some sensor measurements are successfully retrieved from the
attacked data at the decision center. We show that even under a moderate
communication rate constraint of secure networks, an optimal LRB scheduler can
achieve a comparable asymptotic detection performance to the standard N-P test
using the full set of measurements, and is strictly better than the random
scheduler. For non-secure networks, the LRB scheduler can also maintain the
detection functionality but suffers graceful performance degradation under
different attack intensities. Finally, we perform simulations to validate our
theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07290</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07290</id><created>2015-07-26</created><authors><author><keyname>Liang</keyname><forenames>Ning</forenames></author><author><keyname>Zhang</keyname><forenames>Wenyi</forenames></author></authors><title>A Mixed-ADC Receiver Architecture for Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures, to appear in IEEE Information Theory Workshop
  (ITW2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the demand for energy-efficient communication solutions in the
next generation cellular network, a mixed-ADC receiver architecture for massive
multiple input multiple output (MIMO) systems is proposed, which differs from
previous works in that herein one-bit analog-to-digital converters (ADCs)
partially replace the conventionally assumed high-resolution ADCs. The
information-theoretic tool of generalized mutual information (GMI) is exploited
to analyze the achievable data rates of the proposed system architecture and an
array of analytical results of engineering interest are obtained. For
deterministic single input multiple output (SIMO) channels, a closed-form
expression of the GMI is derived, based on which the linear combiner is
optimized. Then, the asymptotic behaviors of the GMI in both low and high SNR
regimes are explored, and the analytical results suggest a plausible ADC
assignment scheme. Finally, the analytical framework is applied to the
multi-user access scenario, and the corresponding numerical results demonstrate
that the mixed system architecture with a relatively small number of
high-resolution ADCs is able to achieve a large fraction of the channel
capacity without output quantization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07292</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07292</id><created>2015-07-26</created><authors><author><keyname>Guo</keyname><forenames>Weisi</forenames></author><author><keyname>Asyhari</keyname><forenames>Taufiq</forenames></author><author><keyname>Farsad</keyname><forenames>Nariman</forenames></author><author><keyname>Yilmaz</keyname><forenames>H. Birkan</forenames></author><author><keyname>Li</keyname><forenames>Bin</forenames></author><author><keyname>Eckford</keyname><forenames>Andrew</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author></authors><title>Molecular Communications: Channel Model and Physical Layer Techniques</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article examines recent research in molecular communications from a
telecommunications system design perspective. In particular, it focuses on
channel models and state-of-the-art physical layer techniques. The goal is to
provide a foundation for higher layer research and motivation for research and
development of functional prototypes. In the first part of the article, we
focus on the channel and noise model, comparing molecular and radio-wave
pathloss formulae. In the second part, the article examines, equipped with the
appropriate channel knowledge, the design of appropriate modulation and error
correction coding schemes. The third reviews transmitter and receiver side
signal processing methods that suppress inter-symbol-interference. Taken
together, the three parts present a series of physical layer techniques that
are necessary to producing reliable and practical molecular communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07295</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07295</id><created>2015-07-26</created><authors><author><keyname>Dyagilev</keyname><forenames>Kirill</forenames></author><author><keyname>Saria</keyname><forenames>Suchi</forenames></author></authors><title>Learning (Predictive) Risk Scores in the Presence of Censoring due to
  Interventions</title><categories>cs.AI stat.AP</categories><journal-ref>Machine Learning Journal, Special Issue on on Machine Learning for
  Health and Medicine, pp. 1-26, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large and diverse set of measurements are regularly collected during a
patient's hospital stay to monitor their health status. Tools for integrating
these measurements into severity scores, that accurately track changes in
illness severity, can improve clinicians ability to provide timely
interventions. Existing approaches for creating such scores either 1) rely on
experts to fully specify the severity score, or 2) train a predictive score,
using supervised learning, by regressing against a surrogate marker of severity
such as the presence of downstream adverse events. The first approach does not
extend to diseases where an accurate score cannot be elicited from experts. The
second approach often produces scores that suffer from bias due to
treatment-related censoring (Paxton, 2013). We propose a novel ranking based
framework for disease severity score learning (DSSL). DSSL exploits the
following key observation: while it is challenging for experts to quantify the
disease severity at any given time, it is often easy to compare the disease
severity at two different times. Extending existing ranking algorithms, DSSL
learns a function that maps a vector of patient's measurements to a scalar
severity score such that the resulting score is temporally smooth and
consistent with the expert's ranking of pairs of disease states. We apply DSSL
to the problem of learning a sepsis severity score using a large, real-world
dataset. The learned scores significantly outperform state-of-the-art clinical
scores in ranking patient states by severity and in early detection of future
adverse events. We also show that the learned disease severity trajectories are
consistent with clinical expectations of disease evolution. Further, using
simulated datasets, we show that DSSL exhibits better generalization
performance to changes in treatment patterns compared to the above approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07299</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07299</id><created>2015-07-27</created><updated>2015-11-30</updated><authors><author><keyname>Kitsak</keyname><forenames>Maksim</forenames></author><author><keyname>Elmokashfi</keyname><forenames>Ahmed</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author><author><keyname>Krioukov</keyname><forenames>Dmitri</forenames></author></authors><title>Long-Range Correlations and Memory in the Dynamics of Internet
  Interdomain Routing</title><categories>physics.soc-ph cs.NI</categories><journal-ref>PloS one 10.11 (2015): e0141481</journal-ref><doi>10.1371/journal.pone.0141481</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data transfer is one of the main functions of the Internet. The Internet
consists of a large number of interconnected subnetworks or domains, known as
Autonomous Systems. Due to privacy and other reasons the information about what
route to use to reach devices within other Autonomous Systems is not readily
available to any given Autonomous System. The Border Gateway Protocol is
responsible for discovering and distributing this reachability information to
all Autonomous Systems. Since the topology of the Internet is highly dynamic,
all Autonomous Systems constantly exchange and update this reachability
information in small chunks, known as routing control packets or Border Gateway
Protocol updates. Motivated by scalability and predictability issues with the
dynamics of these updates in the quickly growing Internet, we conduct a
systematic time series analysis of Border Gateway Protocol update rates. We
find that Border Gateway Protocol update time series are extremely volatile,
exhibit long-term correlations and memory effects, similar to seismic time
series, or temperature and stock market price fluctuations. The presented
statistical characterization of Border Gateway Protocol update dynamics could
serve as a ground truth for validation of existing and developing better models
of Internet interdomain routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07301</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07301</id><created>2015-07-27</created><authors><author><keyname>Yu</keyname><forenames>James J. Q.</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author></authors><title>A Social Spider Algorithm for Solving the Non-convex Economic Load
  Dispatch Problem</title><categories>cs.NE</categories><doi>10.1016/j.neucom.2015.07.037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Economic Load Dispatch (ELD) is one of the essential components in power
system control and operation. Although conventional ELD formulation can be
solved using mathematical programming techniques, modern power system
introduces new models of the power units which are non-convex,
non-differentiable, and sometimes non-continuous. In order to solve such
non-convex ELD problems, in this paper we propose a new approach based on the
Social Spider Algorithm (SSA). The classical SSA is modified and enhanced to
adapt to the unique characteristics of ELD problems, e.g., valve-point effects,
multi-fuel operations, prohibited operating zones, and line losses. To
demonstrate the superiority of our proposed approach, five widely-adopted test
systems are employed and the simulation results are compared with the
state-of-the-art algorithms. In addition, the parameter sensitivity is
illustrated by a series of simulations. The simulation results show that SSA
can solve ELD problems effectively and efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07306</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07306</id><created>2015-07-27</created><authors><author><keyname>Nguyen</keyname><forenames>Tam The</forenames></author><author><keyname>Pham</keyname><forenames>Hung Viet</forenames></author><author><keyname>Vu</keyname><forenames>Phong Minh</forenames></author><author><keyname>Nguyen</keyname><forenames>Tung Thanh</forenames></author></authors><title>Learning API Usages from Bytecode: A Statistical Approach</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When developing mobile apps, programmers rely heavily on standard API
frameworks and libraries. However, learning and using those APIs is often
challenging due to the fast-changing nature of API frameworks for mobile
systems, the complexity of API usages, the insufficiency of documentation, and
the unavailability of source code examples. In this paper, we propose a novel
approach to learn API usages from bytecode of Android mobile apps. Our core
contributions include: i) ARUS, a graph-based representation of API usage
scenarios; ii) HAPI, a statistical, generative model of API usages; and iii)
three algorithms to extract ARUS from apps' bytecode, to train HAPI based on
method call sequences extracted from ARUS, and to recommend method calls in
code completion engines using the trained HAPI. Our empirical evaluation
suggests that our approach can learn useful API usage models which can provide
recommendations with higher levels of accuracy than the baseline n-gram model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07330</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07330</id><created>2015-07-27</created><authors><author><keyname>Kume</keyname><forenames>Kenji</forenames></author><author><keyname>Nose-Togawa</keyname><forenames>Naoko</forenames></author></authors><title>Spectral structure of singular spectrum decomposition for time series</title><categories>cs.DS</categories><comments>19 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Singular spectrum analysis (SSA) is a nonparametric and adaptive spectral
decomposition of a time series. The singular value decomposition of the
trajectory matrix and the anti-diagonal averaging leads to a time-series
decomposition. In this algorithm, a single free parameter, window length $K$,
is involved which is the FIR filter length for the time series. There are no
generally accepted criterion for the proper choice of the window length $K$.
Moreover, the proper window length depends on the specific problem which we are
interested in. Thus, it is important to monitor the spectral structure of the
SSA decomposition and its window length dependence in detail for the practical
application. In this paper, based on the filtering interpretation of SSA, it is
shown that the decomposition of the power spectrum for the original time series
is possible with the filters constructed from the eigenvectors of the
lagged-covariance matrix. With this, we can obtain insights into the spectral
structure of the SSA decomposition and it helps us for the proper choice of the
window length in the practical application of SSA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07348</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07348</id><created>2015-07-27</created><authors><author><keyname>Nees</keyname><forenames>Sam</forenames></author><author><keyname>Schwarz</keyname><forenames>Andreas</forenames></author><author><keyname>Kellermann</keyname><forenames>Walter</forenames></author></authors><title>A model for the temporal evolution of the spatial coherence in decaying
  reverberant sound fields</title><categories>cs.SD</categories><comments>Accepted for JASA Express Letters</comments><doi>10.1121/1.4929733</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reverberant sound fields are often modeled as isotropic. However, it has been
observed that spatial properties change during the decay of the sound field
energy, due to non-isotropic attenuation in non-ideal rooms. In this letter, a
model for the spatial coherence between two sensors in a decaying reverberant
sound field is developed for rectangular rooms. The modeled coherence function
depends on room dimensions, surface reflectivity and orientation of the sensor
pair, but is independent of the position of source and sensors in the room. The
model includes the spherically isotropic (diffuse) and cylindrically isotropic
sound field models as special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07362</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07362</id><created>2015-07-27</created><authors><author><keyname>Leroux</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Sutre</keyname><forenames>Gr&#xe9;goire</forenames></author><author><keyname>Totzke</keyname><forenames>Patrick</forenames></author></authors><title>On Boundedness Problems for Pushdown Vector Addition Systems</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study pushdown vector addition systems, which are synchronized products of
pushdown automata with vector addition systems. The question of the boundedness
of the reachability set for this model can be refined into two decision
problems that ask if infinitely many counter values or stack configurations are
reachable, respectively.
  Counter boundedness seems to be the more intricate problem. We show
decidability in exponential time for one-dimensional systems. The proof is via
a small witness property derived from an analysis of derivation trees of
grammar-controlled vector addition systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07363</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07363</id><created>2015-07-27</created><authors><author><keyname>L&#xf6;ndahl</keyname><forenames>Carl</forenames></author></authors><title>A note on the security of the hHB protocol</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a polynomial-time attack on the hHB protocol, showing that the
protocol does not attain the claimed security. Our attack is based on the GRS
attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07368</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07368</id><created>2015-07-27</created><authors><author><keyname>Bshouty</keyname><forenames>Nader H.</forenames></author><author><keyname>Gabizon</keyname><forenames>Ariel</forenames></author></authors><title>Almost Optimal Cover-Free Families</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Roughly speaking, an $(n,(r,s))$-Cover Free Family (CFF) is a small set of
$n$-bit strings such that: &quot;in any $d:=r+s$ indices we see all patterns of
weight $r$&quot;. CFFs have been of interest for a long time both in discrete
mathematics as part of block design theory, and in theoretical computer science
where they have found a variety of applications, for example, in parametrized
algorithms where they were introduced in the recent breakthrough work of Fomin,
Lokshtanov and Saurabh under the name `lopsided universal sets'.
  In this paper we give the first explicit construction of cover-free families
of optimal size up to lower order multiplicative terms, {for any $r$ and $s$}.
In fact, our construction time is almost linear in the size of the family.
Before our work, such a result existed only for $r=d^{o(1)}$. and $r=
\omega(d/(\log\log d\log\log\log d))$. As a sample application, we improve the
running times of parameterized algorithms from the recent work of Gabizon,
Lokshtanov and Pilipczuk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07374</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07374</id><created>2015-07-27</created><authors><author><keyname>Borisyak</keyname><forenames>Maxim</forenames></author><author><keyname>Ustyuzhanin</keyname><forenames>Andrey</forenames></author></authors><title>A genetic algorithm for autonomous navigation in partially observable
  domain</title><categories>cs.LG cs.AI cs.NE</categories><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of autonomous navigation is one of the basic problems for
robotics. Although, in general, it may be challenging when an autonomous
vehicle is placed into partially observable domain. In this paper we consider
simplistic environment model and introduce a navigation algorithm based on
Learning Classifier System.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07382</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07382</id><created>2015-07-27</created><authors><author><keyname>Borisyak</keyname><forenames>Maxim</forenames></author><author><keyname>Zykov</keyname><forenames>Roman</forenames></author><author><keyname>Noskov</keyname><forenames>Artem</forenames></author></authors><title>Application of Kullback-Leibler divergence for short-term user interest
  detection</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical approaches in recommender systems such as collaborative filtering
are concentrated mainly on static user preference extraction. This approach
works well as an example for music recommendations when a user behavior tends
to be stable over long period of time, however the most common situation in
e-commerce is different which requires reactive algorithms based on a
short-term user activity analysis. This paper introduces a small mathematical
framework for short-term user interest detection formulated in terms of item
properties and its application for recommender systems enhancing. The framework
is based on the fundamental concept of information theory --- Kullback-Leibler
divergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07385</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07385</id><created>2015-07-27</created><updated>2016-02-23</updated><authors><author><keyname>Dil</keyname><forenames>B. J.</forenames></author><author><keyname>Gustafsson</keyname><forenames>F.</forenames></author><author><keyname>Hoenders</keyname><forenames>B. J.</forenames></author></authors><title>Fundamental Bounds on Radio Localization Precision in the Far Field</title><categories>cs.IT math.IT physics.class-ph physics.optics</categories><comments>Made some small adjustments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper experimentally and theoretically investigates the fundamental
bounds on radio localization precision of far-field Received Signal Strength
(RSS) measurements. RSS measurements are proportional to power-flow
measurements time-averaged over periods long compared to the coherence time of
the radiation. Our experiments are performed in a novel localization setup
using 2.4GHz quasi-monochromatic radiation, which corresponds to a mean
wavelength of 12.5cm. We experimentally and theoretically show that RSS
measurements are cross-correlated over a minimum distance that approaches the
diffraction limit, which equals half the mean wavelength of the radiation. Our
experiments show that measuring RSS beyond a sampling density of one sample per
half the mean wavelength does not increase localization precision, as the
Root-Mean-Squared-Error (RMSE) converges asymptotically to roughly half the
mean wavelength. This adds to the evidence that the diffraction limit
determines (1) the lower bound on localization precision and (2) the sampling
density that provides optimal localization precision. We experimentally
validate the theoretical relations between Fisher information, Cram\'er-Rao
Lower Bound (CRLB) and uncertainty, where uncertainty is lower bounded by
diffraction as derived from coherence and speckle theory. When we reconcile
Fisher Information with diffraction, the CRLB matches the experimental results
with an accuracy of 97-98%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07395</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07395</id><created>2015-07-27</created><authors><author><keyname>Luzzi</keyname><forenames>Laura</forenames></author><author><keyname>Vehkalahti</keyname><forenames>Roope</forenames></author></authors><title>Almost universal codes achieving ergodic MIMO capacity within a constant
  gap</title><categories>cs.IT math.IT math.NT</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work addresses the question of achieving capacity with lattice codes in
multi-antenna block fading channels when the number of fading blocks tends to
infinity. A design criterion based on the normalized minimum determinant is
proposed for division algebra multiblock space-time codes over fading channels;
this plays a similar role to the Hermite invariant for Gaussian channels. It is
shown that this criterion is sufficient to guarantee transmission rates within
a constant gap from capacity both for slow fading channels and ergodic fading
channels. This performance is achieved both under maximum likelihood decoding
and naive lattice decoding. In the case of independent identically distributed
Rayleigh fading, it is also shown that the error probability vanishes
exponentially fast. In contrast to the standard approach in the literature
which employs random lattice ensembles, the existence results in this paper are
derived from number theory. First the gap to capacity is shown to depend on the
discriminant of the chosen division algebra; then class field theory is applied
to build families of algebras with small discriminants. The key element in the
construction is the choice of a sequence of division algebras whose centers are
number fields with small root discriminants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07396</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07396</id><created>2015-07-27</created><updated>2015-10-02</updated><authors><author><keyname>Huang</keyname><forenames>Chien-Chung</forenames></author><author><keyname>Ott</keyname><forenames>Sebastian</forenames></author></authors><title>A Combinatorial Approximation Algorithm for Graph Balancing with Light
  Hyper Edges</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Makespan minimization in restricted assignment $(R|p_{ij}\in \{p_j,
\infty\}|C_{\max})$ is a classical problem in the field of machine scheduling.
In a landmark paper in 1990 [8], Lenstra, Shmoys, and Tardos gave a
2-approximation algorithm and proved that the problem cannot be approximated
within 1.5 unless P=NP. The upper and lower bounds of the problem have been
essentially unimproved in the intervening 25 years, despite several remarkable
successful attempts in some special cases of the problem [2,4,12] recently.
  In this paper, we consider a special case called graph-balancing with light
hyper edges, where heavy jobs can be assigned to at most two machines while
light jobs can be assigned to any number of machines. For this case, we present
algorithms with approximation ratios strictly better than 2. Specifically,
  Two job sizes: Suppose that light jobs have weight $w$ and heavy jobs have
weight $W$, and $w &lt; W$. We give a $1.5$-approximation algorithm (note that the
current 1.5 lower bound is established in an even more restrictive setting
[1,3]). Indeed, depending on the specific values of $w$ and $W$, sometimes our
algorithm guarantees sub-1.5 approximation ratios.
  Arbitrary job sizes: Suppose that $W$ is the largest given weight, heavy jobs
have weights in the range of $(\beta W, W]$, where $4/7\leq \beta &lt; 1$, and
light jobs have weights in the range of $(0,\beta W]$. We present a
$(5/3+\beta/3)$-approximation algorithm.
  Our algorithms are purely combinatorial, without the need of solving a linear
program as required in most other known approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07402</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07402</id><created>2015-07-27</created><updated>2015-10-06</updated><authors><author><keyname>Chen</keyname><forenames>Antares</forenames></author><author><keyname>Harris</keyname><forenames>David G.</forenames></author><author><keyname>Srinivasan</keyname><forenames>Aravind</forenames></author></authors><title>Partial Resampling to Approximate Covering Integer Programs</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider positive covering integer programs, which generalize set cover
and which have attracted a long line of research developing (randomized)
approximation algorithms. Srinivasan (2006) gave a rounding algorithm based on
the FKG inequality for systems which are &quot;column-sparse.&quot; This algorithm may
return an integer solution in which the variables get assigned large (integral)
values, Kolliopoulos &amp; Young (2005) modified this algorithm to limit the
solution size, at the cost of a worse approximation ratio. We develop a new
rounding scheme based on the Partial Resampling variant of the Lov\'asz Local
Lemma developed by Harris \&amp; Srinivasan (2013). This achieves an approximation
ratio of $1 + \frac{\ln (\Delta_1+1)}{a} + O(\sqrt{\frac{\log
(\Delta_1+1)}{a}})$, where $a$ is the minimum covering constraint and
$\Delta_1$ is the maximum $\ell_1$-norm of any column of the covering matrix
(whose entries are scaled to lie in $[0,1]$), we also show nearly-matching
inapproximability and integrality-gap lower bounds.
  Our approach improves asymptotically, in several different ways, over known
results. First, it replaces $\Delta_0$, the maximum number of nonzeroes in any
column (from the result of Srinivasan) by $\Delta_1$ which is always -- and can
be much -- smaller than $\Delta_0$, this is the first such result in this
context. Second, our algorithm automatically handles multi-criteria programs,
we achieve improved approximation ratios compared to the algorithm of
Srinivasan, and give, for the first time when the number of objective functions
is large, polynomial-time algorithms with good multi-criteria approximations.
We also significantly improve upon the upper-bounds of Kolliopoulos &amp; Young
when the integer variables are required to be within $(1 + \epsilon)$ of some
given upper-bounds, and show nearly-matching inapproximability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07403</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07403</id><created>2015-07-27</created><authors><author><keyname>Taylor</keyname><forenames>Tim</forenames></author></authors><title>Requirements for Open-Ended Evolution in Natural and Artificial Systems</title><categories>cs.NE q-bio.PE</categories><comments>Presented at the EvoEvo Workshop at the European Conference on
  Artificial Life 2015 (ECAL 2015), University of York, UK, July 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open-ended evolutionary dynamics remains an elusive goal for artificial
evolutionary systems. Many ideas exist in the biological literature beyond the
basic Darwinian requirements of variation, differential reproduction and
inheritance. I argue that these ideas can be seen as aspects of five
fundamental requirements for open-ended evolution: (1) robustly reproductive
individuals, (2) a medium allowing the possible existence of a practically
unlimited diversity of individuals and interactions, (3) individuals capable of
producing more complex offspring, (4) mutational pathways to other viable
individuals, and (5) drive for continued evolution. I briefly discuss
implications of this view for the design of artificial systems with greater
evolutionary potential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07411</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07411</id><created>2015-07-27</created><authors><author><keyname>Rodr&#xed;guez-P&#xe9;rez</keyname><forenames>Miguel</forenames></author><author><keyname>Herrer&#xed;a-Alonso</keyname><forenames>Sergio</forenames></author><author><keyname>Fern&#xe1;ndez-Veiga</keyname><forenames>Manuel</forenames></author><author><keyname>L&#xf3;pez-Garc&#xed;a</keyname><forenames>C&#xe1;ndido</forenames></author></authors><title>Improved Opportunistic Sleeping Algorithms for LAN Switches</title><categories>cs.NI</categories><journal-ref>Global Telecommunications Conference, 2009. GLOBECOM 2009. IEEE ,
  vol., no., pp.1,6, Nov. 30 2009-Dec. 4 2009</journal-ref><doi>10.1109/GLOCOM.2009.5425710</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network interfaces in most LAN computing devices are usually severely
under-utilized, wasting energy while waiting for new packets to arrive. In this
paper, we present two algorithms for opportunistically powering down unused
network interfaces in order to save some of that wasted energy. We compare our
proposals to the best known opportunistic method, and show that they provide
much greater power savings inflicting even lower delays to Internet traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07415</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07415</id><created>2015-07-27</created><authors><author><keyname>Rodr&#xed;guez-P&#xe9;rez</keyname><forenames>Miguel</forenames></author><author><keyname>Herrer&#xed;a-Alonso</keyname><forenames>Sergio</forenames></author><author><keyname>Fern&#xe1;ndez-Veiga</keyname><forenames>Manuel</forenames></author><author><keyname>Su&#xe1;rez-Gonz&#xe1;lez</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>L&#xf3;pez-Garc&#xed;a</keyname><forenames>C&#xe1;ndido</forenames></author></authors><title>Achieving Fair Network Equilibria with Delay-based Congestion Control
  Algorithms</title><categories>cs.NI</categories><journal-ref>Communications Letters, IEEE , vol. 12, no. 7, pp. 535, 537, July
  2008</journal-ref><doi>10.1109/LCOMM.2008.080372</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delay-based congestion control algorithms provide higher throughput and
stability than traditional loss-based AIMD algorithms, but they are inherently
unfair against older connections when the queuing and the propagation delay
cannot be measured accurately and independently. This paper presents a novel
measurement algorithm whereby fairness between old and new connections is
preserved. The algorithm does not modify the dynamics of congestion control,
and runs entirely in the server host using locally available information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07419</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07419</id><created>2015-07-27</created><updated>2015-10-29</updated><authors><author><keyname>Schloemann</keyname><forenames>Javier</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Buehrer</keyname><forenames>R. Michael</forenames></author></authors><title>A Tractable Metric for Evaluating Base Station Geometries in Cellular
  Network Localization</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we present a new metric for characterizing the geometric
conditions encountered in cellular positioning based on the angular spread of
the base stations (BSs). The metric is shown to be closely related to the
geometric-dilution-of-precision (GDOP), yet has the benefit of being
characterizable in terms of the network parameters for BS layouts modeled
according to a Poisson point process (PPP). As an additional benefit, the
metric is shown to immediately yield a device's probability of being inside or
outside the convex hull of the BSs, which localization researchers will
widely-recognize as being a strong indicator of localization performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07429</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07429</id><created>2015-07-20</created><updated>2015-07-28</updated><authors><author><keyname>Abdurachmanov</keyname><forenames>David</forenames></author><author><keyname>Degano</keyname><forenames>Alessandro</forenames></author><author><keyname>Elmer</keyname><forenames>Peter</forenames></author><author><keyname>Eulisse</keyname><forenames>Giulio</forenames></author><author><keyname>Mendez</keyname><forenames>David</forenames></author><author><keyname>Muzaffar</keyname><forenames>Shahzad</forenames></author></authors><title>Optimizing CMS build infrastructure via Apache Mesos</title><categories>cs.DC hep-ex</categories><comments>Submitted to proceedings of the 21st International Conference on
  Computing in High Energy and Nuclear Physics (CHEP2015), Okinawa, Japan</comments><doi>10.1088/1742-6596/664/6/062013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Offline Software of the CMS Experiment at the Large Hadron Collider (LHC)
at CERN consists of 6M lines of in-house code, developed over a decade by
nearly 1000 physicists, as well as a comparable amount of general use
open-source code. A critical ingredient to the success of the construction and
early operation of the WLCG was the convergence, around the year 2000, on the
use of a homogeneous environment of commodity x86-64 processors and Linux.
Apache Mesos is a cluster manager that provides efficient resource isolation
and sharing across distributed applications, or frameworks. It can run Hadoop,
Jenkins, Spark, Aurora, and other applications on a dynamically shared pool of
nodes. We present how we migrated our continuos integration system to schedule
jobs on a relatively small Apache Mesos enabled cluster and how this resulted
in better resource usage, higher peak performance and lower latency thanks to
the dynamic scheduling capabilities of Mesos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07430</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07430</id><created>2015-07-20</created><authors><author><keyname>Bauerdick</keyname><forenames>Lothar</forenames></author><author><keyname>Bockelman</keyname><forenames>Brian</forenames></author><author><keyname>Elmer</keyname><forenames>Peter</forenames></author><author><keyname>Gowdy</keyname><forenames>Stephen</forenames></author><author><keyname>Tadel</keyname><forenames>Matevz</forenames></author><author><keyname>Wuerthwein</keyname><forenames>Frank</forenames></author></authors><title>Designing Computing System Architecture and Models for the HL-LHC era</title><categories>cs.DC hep-ex</categories><comments>Submitted to proceedings of the 21st International Conference on
  Computing in High Energy and Nuclear Physics (CHEP2015), Okinawa, Japan</comments><doi>10.1088/1742-6596/664/3/032010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a programme to study the computing model in CMS after
the next long shutdown near the end of the decade.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07458</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07458</id><created>2015-07-27</created><authors><author><keyname>Xu</keyname><forenames>Xun</forenames></author><author><keyname>Hospedales</keyname><forenames>Timothy</forenames></author><author><keyname>Gong</keyname><forenames>Shaogang</forenames></author></authors><title>Discovery of Shared Semantic Spaces for Multi-Scene Video Query and
  Summarization</title><categories>cs.CV</categories><comments>Multi-Scene Traffic Behaviour Analysis ---- Accepted at IEEE
  Transactions on Circuits and Systems for Video Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing rate of public space CCTV installations has generated a need for
automated methods for exploiting video surveillance data including scene
understanding, query, behaviour annotation and summarization. For this reason,
extensive research has been performed on surveillance scene understanding and
analysis. However, most studies have considered single scenes, or groups of
adjacent scenes. The semantic similarity between different but related scenes
(e.g., many different traffic scenes of similar layout) is not generally
exploited to improve any automated surveillance tasks and reduce manual effort.
Exploiting commonality, and sharing any supervised annotations, between
different scenes is however challenging due to: Some scenes are totally
un-related -- and thus any information sharing between them would be
detrimental; while others may only share a subset of common activities -- and
thus information sharing is only useful if it is selective. Moreover,
semantically similar activities which should be modelled together and shared
across scenes may have quite different pixel-level appearance in each scene. To
address these issues we develop a new framework for distributed multiple-scene
global understanding that clusters surveillance scenes by their ability to
explain each other's behaviours; and further discovers which subset of
activities are shared versus scene-specific within each cluster. We show how to
use this structured representation of multiple scenes to improve common
surveillance tasks including scene activity understanding, cross-scene
query-by-example, behaviour classification with reduced supervised labelling
requirements, and video summarization. In each case we demonstrate how our
multi-scene model improves on a collection of standard single scene models and
a flat model of all scenes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07459</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07459</id><created>2015-07-27</created><authors><author><keyname>Oosterwijk</keyname><forenames>Tim</forenames></author></authors><title>On local search and LP and SDP relaxations for k-Set Packing</title><categories>math.CO cs.CC</categories><comments>There is a mistake in the following line of Theorem 17: &quot;As an
  induced subgraph of H with more edges than vertices constitutes an improving
  set&quot;. Therefore, the proofs of Theorem 17, and hence Theorems 19, 23 and 24,
  are false. It is still open whether these theorems are true</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Set packing is a fundamental problem that generalises some well-known
combinatorial optimization problems and knows a lot of applications. It is
equivalent to hypergraph matching and it is strongly related to the maximum
independent set problem. In this thesis we study the k-set packing problem
where given a universe U and a collection C of subsets over U, each of
cardinality k, one needs to find the maximum collection of mutually disjoint
subsets. Local search techniques have proved to be successful in the search for
approximation algorithms, both for the unweighted and the weighted version of
the problem where every subset in C is associated with a weight and the
objective is to maximise the sum of the weights. We make a survey of these
approaches and give some background and intuition behind them. In particular,
we simplify the algebraic proof of the main lemma for the currently best
weighted approximation algorithm of Berman ([Ber00]) into a proof that reveals
more intuition on what is really happening behind the math. The main result is
a new bound of k/3 + 1 + epsilon on the integrality gap for a polynomially
sized LP relaxation for k-set packing by Chan and Lau ([CL10]) and the natural
SDP relaxation [NOTE: see page iii]. We provide detailed proofs of lemmas
needed to prove this new bound and treat some background on related topics like
semidefinite programming and the Lovasz Theta function. Finally we have an
extended discussion in which we suggest some possibilities for future research.
We discuss how the current results from the weighted approximation algorithms
and the LP and SDP relaxations might be improved, the strong relation between
set packing and the independent set problem and the difference between the
weighted and the unweighted version of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07462</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07462</id><created>2015-07-27</created><authors><author><keyname>Smarandache</keyname><forenames>Florentin</forenames></author></authors><title>Unification of Fusion Theories, Rules, Filters, Image Fusion and Target
  Tracking Methods (UFT)</title><categories>cs.AI</categories><comments>79 pages, a diagram. arXiv admin note: substantial text overlap with
  arXiv:cs/0409040, arXiv:0901.1289, arXiv:cs/0410033</comments><acm-class>I.2.6</acm-class><journal-ref>International Journal of Applied Mathematics &amp; Statistics, Vol. 2,
  1-14, 2004</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The author has pledged in various papers, conference or seminar
presentations, and scientific grant applications (between 2004-2015) for the
unification of fusion theories, combinations of fusion rules, image fusion
procedures, filter algorithms, and target tracking methods for more accurate
applications to our real world problems - since neither fusion theory nor
fusion rule fully satisfy all needed applications. For each particular
application, one selects the most appropriate fusion space and fusion model,
then the fusion rules, and the algorithms of implementation. He has worked in
the Unification of the Fusion Theories (UFT), which looks like a cooking
recipe, better one could say like a logical chart for a computer programmer,
but one does not see another method to comprise/unify all things. The
unification scenario presented herein, which is now in an incipient form,
should periodically be updated incorporating new discoveries from the fusion
and engineering research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07479</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07479</id><created>2015-07-27</created><authors><author><keyname>Santos</keyname><forenames>Daniel Ricardo dos</forenames></author><author><keyname>Ranise</keyname><forenames>Silvio</forenames></author><author><keyname>Ponta</keyname><forenames>Serena Elisa</forenames></author></authors><title>Modularity for Security-Sensitive Workflows</title><categories>cs.SE cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An established trend in software engineering insists on using components
(sometimes also called services or packages) to encapsulate a set of related
functionalities or data. By defining interfaces specifying what functionalities
they provide or use, components can be combined with others to form more
complex components. In this way, IT systems can be designed by mostly re-using
existing components and developing new ones to provide new functionalities. In
this paper, we introduce a notion of component and a combination mechanism for
an important class of software artifacts, called security-sensitive workflows.
These are business processes in which execution constraints on the tasks are
complemented with authorization constraints (e.g., Separation of Duty) and
authorization policies (constraining which users can execute which tasks). We
show how well-known workflow execution patterns can be simulated by our
combination mechanism and how authorization constraints can also be imposed
across components. Then, we demonstrate the usefulness of our notion of
component by showing (i) the scalability of a technique for the synthesis of
run-time monitors for security-sensitive workflows and (ii) the design of a
plug-in for the re-use of workflows and related run-time monitors inside an
editor for security-sensitive workflows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07490</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07490</id><created>2015-07-27</created><authors><author><keyname>Aijaz</keyname><forenames>Adnan</forenames></author><author><keyname>Aghvami</keyname><forenames>A. Hamid</forenames></author></authors><title>A Green Perspective on Wi-Fi Offloading</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to increased energy consumption and carbon emissions of the ICT industry,
operators worldwide are focusing on reducing energy consumption of their
networks from financial as well as corporate responsibility perspectives. The
subject of green or energy efficient operation of cellular access network has
attracted a lot of attention in the research community recently. In this
regard, dynamically powering down the radio network equipment has emerged as a
promising solution. In literature, research around this type of techniques has
mainly focused on quantifying the energy saving potential. However, little
efforts have been made towards practical realization of these energy saving
concepts. On the other hand, Wi-Fi networks are undergoing a paradigm shift
towards ubiquity. The main objective of this paper is to provide novel
mechanisms for practically realizing the concept of improving power efficiency
in the cellular access network through opportunistically offloading users to
Wi- Fi networks. These mechanisms are based on the principles of cognitive
network management. Performance evaluation shows the potential of proposed
mechanisms as a viable solution for achieving energy efficiency in the cellular
access network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07492</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07492</id><created>2015-07-27</created><authors><author><keyname>Han</keyname><forenames>Bin</forenames></author><author><keyname>Jiang</keyname><forenames>Qingtang</forenames></author><author><keyname>Shen</keyname><forenames>Zuowei</forenames></author><author><keyname>Zhuang</keyname><forenames>Xiaosheng</forenames></author></authors><title>Symmetric Canonical Quincunx Tight Framelets with High Vanishing Moments
  and Smoothness</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an approach to construct a family of two-dimensional compactly
supported real-valued symmetric quincunx tight framelets $\{\phi;
\psi_1,\psi_2,\psi_3\}$ in $L_2(R^2)$ with arbitrarily high orders of vanishing
moments. Such symmetric quincunx tight framelets are associated with quincunx
tight framelet filter banks $\{a;b_1,b_2,b_3\}$ having increasing orders of
vanishing moments and enjoying the additional double canonical properties: \[
b_1(k_1,k_2)=(-1)^{1+k_1+k_2} a(1-k_1,-k_2), b_3(k_1,k_2)=(-1)^{1+k_1+k_2}
b_2(1-k_1,-k_2). \] For a low-pass filter $a$ which is not a quincunx
orthonormal wavelet filter, we show that a quincunx tight framelet filter bank
$\{a;b_1,\ldots,b_L\}$ with $b_1$ taking the above canonical form must have
$L\ge 3$ high-pass filters. Thus, our family of symmetric double canonical
quincunx tight framelets has the minimum number of generators. Numerical
calculation indicates that this family of symmetric double canonical quincunx
tight framelets can be arbitrarily smooth. Using one-dimensional filters having
linear-phase moments, in this paper we also provide a second approach to
construct multiple canonical quincunx tight framelets with symmetry. In
particular, the second approach yields a family of $6$-multiple canonical
real-valued quincunx tight framelets in $L_2(R^2)$ and a family of double
canonical complex-valued quincunx tight framelets in $L_2(R^2)$ such that both
of them have symmetry and arbitrarily increasing orders of smoothness and
vanishing moments. Several examples are provided to illustrate our general
construction and theoretical results on canonical quincunx tight framelets in
$L_2(R^2)$ with symmetry, high vanishing moments, and smoothness. Symmetric
quincunx tight framelets constructed by both approaches in this paper are of
particular interest for their applications in computer graphics and image
processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07495</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07495</id><created>2015-07-27</created><authors><author><keyname>Meyer</keyname><forenames>David A.</forenames></author><author><keyname>Shakeel</keyname><forenames>Asif</forenames></author></authors><title>Estimating an Activity Driven Hidden Markov Model</title><categories>stat.ML cs.DS cs.LG cs.SI math.ST stat.TH</categories><comments>13 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a Hidden Markov Model (HMM) in which each hidden state has
time-dependent $\textit{activity levels}$ that drive transitions and emissions,
and show how to estimate its parameters. Our construction is motivated by the
problem of inferring human mobility on sub-daily time scales from, for example,
mobile phone records.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07497</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07497</id><created>2015-07-27</created><updated>2016-01-07</updated><authors><author><keyname>Jindal</keyname><forenames>Gorav</forenames></author><author><keyname>Kolev</keyname><forenames>Pavel</forenames></author></authors><title>An Efficient Parallel Algorithm for Spectral Sparsification of Laplacian
  and SDDM Matrix Polynomials</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For &quot;large&quot; class $\mathcal{C}$ of continuous probability density functions
(p.d.f.), we demonstrate that for every $w\in\mathcal{C}$ there is mixture of
discrete Binomial distributions (MDBD) with $T\geq N\sqrt{\phi_{w}/\delta}$
distinct Binomial distributions $B(\cdot,N)$ that $\delta$-approximates a
\emph{discretized} p.d.f. $\hat{w}(i/N)\triangleq
w(i/N)/[\sum_{\ell=0}^{N}w(\ell/N)]$ for all $i\in[3:N-3]$, where
$\phi_{w}\geq\max_{x\in[0,1]}|w(x)|$. Also, we give two efficient parallel
algorithms to find such MDBD.
  Moreover, we propose a sequential algorithm that on input MDBD with $N=2^k$
for $k\in\mathbb{N}_{+}$ that induces a discretized p.d.f. $\beta$, $B=D-M$
that is either Laplacian or SDDM matrix and parameter $\epsilon\in(0,1)$,
outputs in $\hat{O}(\epsilon^{-2}m + \epsilon^{-4}nT)$ time a spectral
sparsifier $D-\hat{M}_{N} \approx_{\epsilon} D-D\sum_{i=0}^{N}\beta_{i}(D^{-1}
M)^i$ of a matrix-polynomial, where $\hat{O}(\cdot)$ notation hides
$\mathrm{poly}(\log n,\log N)$ factors. This improves the Cheng et
al.'s\cite{CCLPT15} algorithm whose run time is $\hat{O}(\epsilon^{-2} m N^2 +
NT)$.
  Furthermore, our algorithm is parallelizable and runs in work
$\hat{O}(\epsilon^{-2}m + \epsilon^{-4}nT)$ and depth $O(\log
N\cdot\mathrm{poly}(\log n)+\log T)$. Our main algorithmic contribution is to
propose the first efficient parallel algorithm that on input continuous p.d.f.
$w\in\mathcal{C}$, matrix $B=D-M$ as above, outputs a spectral sparsifier of
matrix-polynomial whose coefficients approximate component-wise the discretized
p.d.f. $\hat{w}$.
  Our results yield the first efficient and parallel algorithm that runs in
nearly linear work and poly-logarithmic depth and analyzes the long term
behaviour of Markov chains in non-trivial settings. In addition, we strengthen
the Spielman and Peng's\cite{PS14} parallel SDD solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07505</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07505</id><created>2015-07-27</created><updated>2015-09-29</updated><authors><author><keyname>Miao</keyname><forenames>Shun</forenames></author><author><keyname>Wang</keyname><forenames>Z. Jane</forenames></author><author><keyname>Liao</keyname><forenames>Rui</forenames></author></authors><title>MRI-based Motion Estimation via Scatter to Volume Registration</title><categories>cs.CV</categories><doi>10.1109/ISBI.2015.7163944</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Respiratory motion is a major source of error in many image acquisition
applications and image-guided interventions, and motion estimation techniques
have been widely applied to compensate for it. Existing respiratory motion
estimation methods typically reply on breathing motion models learned from
certain training data. However, none of these methods can effectively handle
both intra-subject and inter-subject variations of respiratory motion. In this
paper, we propose a respiratory motion estimation method that directly recovers
motion fields from sparsely spaced dynamic 2-D MRIs without a learned
respiratory motion model. We introduce a scatter-to-volume registration
algorithm to register the dynamic 2-D MRIs with a static 3-D MRI to recover
dense motion fields. The proposed method was validated on 4-D MRIs acquired
from 5 volunteers with breathing pattern variabilities, demonstrating
significant improvements over the state of the art respiratory motion modeling
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07508</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07508</id><created>2015-07-27</created><updated>2015-07-28</updated><authors><author><keyname>Sun</keyname><forenames>Peng</forenames></author><author><keyname>Zhou</keyname><forenames>Haoyin</forenames></author><author><keyname>Lundine</keyname><forenames>Devon</forenames></author><author><keyname>Min</keyname><forenames>James K.</forenames></author><author><keyname>Xiong</keyname><forenames>Guanglei</forenames></author></authors><title>Fast Segmentation of Left Ventricle in CT Images by Explicit Shape
  Regression using Random Pixel Difference Features</title><categories>cs.CV</categories><comments>8 pages, link to a video demo</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, machine learning has been successfully applied to model-based left
ventricle (LV) segmentation. The general framework involves two stages, which
starts with LV localization and is followed by boundary delineation. Both are
driven by supervised learning techniques. When compared to previous
non-learning-based methods, several advantages have been shown, including full
automation and improved accuracy. However, the speed is still slow, in the
order of several seconds, for applications involving a large number of cases or
case loads requiring real-time performance. In this paper, we propose a fast LV
segmentation algorithm by joint localization and boundary delineation via
training explicit shape regressor with random pixel difference features. Tested
on 3D cardiac computed tomography (CT) image volumes, the average running time
of the proposed algorithm is 1.2 milliseconds per case. On a dataset consisting
of 139 CT volumes, a 5-fold cross validation shows the segmentation error is
$1.21 \pm 0.11$ for LV endocardium and $1.23 \pm 0.11$ millimeters for
epicardium. Compared with previous work, the proposed method is more stable
(lower standard deviation) without significant compromise to the accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07516</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07516</id><created>2015-07-27</created><updated>2015-10-07</updated><authors><author><keyname>Seifi</keyname><forenames>Ehsan</forenames></author><author><keyname>Atamanesh</keyname><forenames>Mehran</forenames></author><author><keyname>Khandani</keyname><forenames>Amir K.</forenames></author></authors><title>Media-Based MIMO: A New Frontier in Wireless Communications</title><categories>cs.IT math.IT</categories><comments>26 pages, 11 figures, additional examples are given to further
  explain the idea of Media-Based Modulation. Capacity figure added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of Media-based Modulation (MBM), is based on embedding information
in the variations of the transmission media (channel state). This is in
contrast to legacy wireless systems where data is embedded in a Radio Frequency
(RF) source prior to the transmit antenna. MBM offers several advantages vs.
legacy systems, including &quot;additivity of information over multiple receive
antennas&quot;, and &quot;inherent diversity over a static fading channel&quot;. MBM is
particularly suitable for transmitting high data rates using a single transmit
and multiple receive antennas (Single Input-Multiple Output Media-Based
Modulation, or SIMO-MBM). However, complexity issues limit the amount of data
that can be embedded in the channel state using a single transmit unit. To
address this shortcoming, the current article introduces the idea of Layered
Multiple Input-Multiple Output Media-Based Modulation (LMIMO-MBM). Relying on a
layered structure, LMIMO-MBM can significantly reduce both hardware and
algorithmic complexities, as well as the training overhead, vs. SIMO-MBM.
Simulation results show excellent performance in terms of Symbol Error Rate
(SER) vs. Signal-to-Noise Ratio (SNR). For example, a $4\times 16$ LMIMO-MBM is
capable of transmitting $32$ bits of information per (complex) channel-use,
with SER $ \simeq 10^{-5}$ at $E_b/N_0\simeq -3.5$dB (or SER $ \simeq 10^{-4}$
at $E_b/N_0=-4.5$dB). This performance is achieved using a single transmission
and without adding any redundancy for Forward-Error-Correction (FEC). This
means, in addition to its excellent SER vs. energy/rate performance, MBM
relaxes the need for complex FEC structures, and thereby minimizes the
transmission delay. Overall, LMIMO-MBM provides a promising alternative to MIMO
and Massive MIMO for the realization of 5G wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07548</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07548</id><created>2015-07-25</created><authors><author><keyname>Glass</keyname><forenames>Colin W.</forenames></author><author><keyname>Reiser</keyname><forenames>Steffen</forenames></author><author><keyname>Rutkai</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Deublein</keyname><forenames>Stephan</forenames></author><author><keyname>K&#xf6;ster</keyname><forenames>Andreas</forenames></author><author><keyname>Carri&#xf3;n</keyname><forenames>Gabriela Guevara</forenames></author><author><keyname>Wafai</keyname><forenames>Amer</forenames></author><author><keyname>Horsch</keyname><forenames>Martin</forenames></author><author><keyname>Bernreuther</keyname><forenames>Martin F.</forenames></author><author><keyname>Windmann</keyname><forenames>Thorsten</forenames></author><author><keyname>Hasse</keyname><forenames>Hans</forenames></author><author><keyname>Vrabec</keyname><forenames>Jadran</forenames></author></authors><title>ms2: A molecular simulation tool for thermodynamic properties, new
  version release</title><categories>cs.CE physics.comp-ph</categories><journal-ref>Computer Physics Communications 185 (12): 3302-3306 (2014)</journal-ref><doi>10.1016/j.cpc.2014.07.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new version release (2.0) of the molecular simulation tool ms2 [S. Deublein
et al., Comput. Phys. Commun. 182 (2011) 2350] is presented. Version 2.0 of ms2
features a hybrid parallelization based on MPI and OpenMP for molecular
dynamics simulation to achieve higher scalability. Furthermore, the formalism
by Lustig [R. Lustig, Mol. Phys. 110 (2012) 3041] is implemented, allowing for
a systematic sampling of Massieu potential derivatives in a single simulation
run. Moreover, the Green-Kubo formalism is extended for the sampling of the
electric conductivity and the residence time. To remove the restriction of the
preceding version to electro-neutral molecules, Ewald summation is implemented
to consider ionic long range interactions. Finally, the sampling of the radial
distribution function is added.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07581</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07581</id><created>2015-07-27</created><updated>2015-10-27</updated><authors><author><keyname>Boodaghians</keyname><forenames>Shant</forenames></author><author><keyname>Vetta</keyname><forenames>Adrian</forenames></author></authors><title>Testing Consumer Rationality using Perfect Graphs and Oriented Discs</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a consumer data-set, the axioms of revealed preference proffer a binary
test for rational behaviour. A natural (non-binary) measure of the degree of
rationality exhibited by the consumer is the minimum number of data points
whose removal induces a rationalisable data-set.We study the computational
complexity of the resultant consumer rationality problem in this paper. This
problem is, in the worst case, equivalent (in terms of approximation) to the
directed feedback vertex set problem. Our main result is to obtain an exact
threshold on the number of commodities that separates easy cases and hard
cases. Specifically, for two-commodity markets the consumer rationality problem
is polynomial time solvable; we prove this via a reduction to the vertex cover
problem on perfect graphs. For three-commodity markets, however, the problem is
NP-complete; we prove thisusing a reduction from planar 3-SAT that is based
upon oriented-disc drawings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07583</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07583</id><created>2015-07-27</created><updated>2015-12-22</updated><authors><author><keyname>Richmond</keyname><forenames>David L.</forenames></author><author><keyname>Kainmueller</keyname><forenames>Dagmar</forenames></author><author><keyname>Yang</keyname><forenames>Michael Y.</forenames></author><author><keyname>Myers</keyname><forenames>Eugene W.</forenames></author><author><keyname>Rother</keyname><forenames>Carsten</forenames></author></authors><title>Relating Cascaded Random Forests to Deep Convolutional Neural Networks
  for Semantic Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of pixel-wise semantic segmentation given a small set of
labeled training images. Among two of the most popular techniques to address
this task are Random Forests (RF) and Neural Networks (NN). The main
contribution of this work is to explore the relationship between two special
forms of these techniques: stacked RFs and deep Convolutional Neural Networks
(CNN). We show that there exists a mapping from stacked RF to deep CNN, and an
approximate mapping back. This insight gives two major practical benefits:
Firstly, deep CNNs can be intelligently constructed and initialized, which is
crucial when dealing with a limited amount of training data. Secondly, it can
be utilized to create a new stacked RF with improved performance. Furthermore,
this mapping yields a new CNN architecture, that is well suited for pixel-wise
semantic labeling. We experimentally verify these practical benefits for two
different application scenarios in computer vision and biology, where the
layout of parts is important: Kinect-based body part labeling from depth
images, and somite segmentation in microscopy images of developing zebrafish.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07592</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07592</id><created>2015-07-27</created><authors><author><keyname>Tiamiyu</keyname><forenames>Osuolale Abdulrahamon</forenames></author></authors><title>On the Simulation of Trusted Routing Mechanism</title><categories>cs.NI</categories><report-no>1507.07592</report-no><journal-ref>Actual Problems of Infotelecommunications in Science and
  Education, SPbSUT 2013, pp.879-882</journal-ref><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Among the mechanisms for the data security in computer networks is considered
trusted routing. Its simulation method is chosen and choice of network
simulator is substantiated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07595</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07595</id><created>2015-07-27</created><updated>2016-01-06</updated><authors><author><keyname>Lee</keyname><forenames>Jason D.</forenames></author><author><keyname>Lin</keyname><forenames>Qihang</forenames></author><author><keyname>Ma</keyname><forenames>Tengyu</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author></authors><title>Distributed Stochastic Variance Reduced Gradient Methods and A Lower
  Bound for Communication Complexity</title><categories>math.OC cs.LG stat.ML</categories><comments>significant addition to both theory and experimental results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study distributed optimization algorithms for minimizing the average of
convex functions. The applications include empirical risk minimization problems
in statistical machine learning where the datasets are large and have to be
stored on different machines. We design a distributed stochastic variance
reduced gradient algorithm that, under certain conditions on the condition
number, simultaneously achieves the optimal parallel runtime, amount of
communication and rounds of communication among all distributed first-order
methods up to constant factors. Our method and its accelerated extension also
outperform existing distributed algorithms in terms of the rounds of
communication as long as the condition number is not too large compared to the
size of data in each machine. We also prove a lower bound for the number of
rounds of communication for a broad class of distributed first-order methods
including the proposed algorithms in this paper. We show that our accelerated
distributed stochastic variance reduced gradient algorithm achieves this lower
bound so that it uses the fewest rounds of communication among all distributed
first-order algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07597</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07597</id><created>2015-07-27</created><authors><author><keyname>Cervesato</keyname><forenames>Iliano</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Kaustuv</forenames></author></authors><title>Proceedings Tenth International Workshop on Logical Frameworks and Meta
  Languages: Theory and Practice</title><categories>cs.LO cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 185, 2015</journal-ref><doi>10.4204/EPTCS.185</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume constitutes the proceedings of LFMTP 2015, the Tenth
International Workshop on Logical Frameworks and Meta-Languages: Theory and
Practice, held on August 1st, 2015 in Berlin, Germany. The workshop was a
one-day satellite event of CADE-25, the 25th International Conference on
Automated Deduction. Logical frameworks and meta-languages form a common
substrate for representing, implementing, and reasoning about a wide variety of
deductive systems of interest in logic and computer science. Their design and
implementation and their use in reasoning tasks ranging from the correctness of
software to the properties of formal computational systems have been the focus
of considerable research over the last two decades. This workshop brought
together designers, implementors, and practitioners to discuss various aspects
impinging on the structure and utility of logical frameworks, including the
treatment of variable binding, inductive and co-inductive reasoning techniques
and the expressiveness and lucidity of the reasoning process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07598</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07598</id><created>2015-07-27</created><authors><author><keyname>Lange</keyname><forenames>Kenneth</forenames></author><author><keyname>Keys</keyname><forenames>Kevin L.</forenames></author></authors><title>The proximal distance algorithm</title><categories>math.OC cs.DS</categories><comments>22 pages, 0 figures, 8 tables, modified from conference publication</comments><journal-ref>In Proceedings of the 2014 International Congress of
  Mathematicians. Seoul: Kyung Moon, 4:95-116</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The MM principle is a device for creating optimization algorithms satisfying
the ascent or descent property. The current survey emphasizes the role of the
MM principle in nonlinear programming. For smooth functions, one can construct
an adaptive interior point method based on scaled Bregmann barriers. This
algorithm does not follow the central path. For convex programming subject to
nonsmooth constraints, one can combine an exact penalty method with distance
majorization to create versatile algorithms that are effective even in discrete
optimization. These proximal distance algorithms are highly modular and reduce
to set projections and proximal mappings, both very well-understood techniques
in optimization. We illustrate the possibilities in linear programming, binary
piecewise-linear programming, nonnegative quadratic programming, $\ell_0$
regression, matrix completion, and inverse sparse covariance estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07602</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07602</id><created>2015-07-27</created><authors><author><keyname>Starek</keyname><forenames>Joseph A.</forenames></author><author><keyname>Gomez</keyname><forenames>Javier V.</forenames></author><author><keyname>Schmerling</keyname><forenames>Edward</forenames></author><author><keyname>Janson</keyname><forenames>Lucas</forenames></author><author><keyname>Moreno</keyname><forenames>Luis</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>An Asymptotically-Optimal Sampling-Based Algorithm for Bi-directional
  Motion Planning</title><categories>cs.RO</categories><comments>Accepted to the 2015 IEEE Intelligent Robotics and Systems Conference
  in Hamburg, Germany. This submission represents the long version of the
  conference manuscript, with additional proof details (Section IV) regarding
  the asymptotic optimality of the BFMT* algorithm</comments><doi>10.1109/IROS.2015.7353652</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bi-directional search is a widely used strategy to increase the success and
convergence rates of sampling-based motion planning algorithms. Yet, few
results are available that merge both bi-directional search and asymptotic
optimality into existing optimal planners, such as PRM*, RRT*, and FMT*. The
objective of this paper is to fill this gap. Specifically, this paper presents
a bi-directional, sampling-based, asymptotically-optimal algorithm named
Bi-directional FMT* (BFMT*) that extends the Fast Marching Tree (FMT*)
algorithm to bi-directional search while preserving its key properties, chiefly
lazy search and asymptotic optimality through convergence in probability. BFMT*
performs a two-source, lazy dynamic programming recursion over a set of
randomly-drawn samples, correspondingly generating two search trees: one in
cost-to-come space from the initial configuration and another in cost-to-go
space from the goal configuration. Numerical experiments illustrate the
advantages of BFMT* over its unidirectional counterpart, as well as a number of
other state-of-the-art planners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07605</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07605</id><created>2015-07-27</created><updated>2015-08-28</updated><authors><author><keyname>Rosenberg</keyname><forenames>Gili</forenames></author><author><keyname>Vazifeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Woods</keyname><forenames>Brad</forenames></author><author><keyname>Haber</keyname><forenames>Eldad</forenames></author></authors><title>Building an iterative heuristic solver for a quantum annealer</title><categories>cs.DM cs.ET math.OC quant-ph</categories><comments>21 pages, 4 figures; minor edits</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A quantum annealer heuristically minimizes quadratic unconstrained binary
optimization (QUBO) problems, but is limited by the physical hardware in the
size and density of the problems it can handle. We have developed a
meta-heuristic solver that utilizes D-Wave Systems' quantum annealer (or any
other QUBO problem optimizer) to solve larger or denser problems, by
iteratively solving subproblems, while keeping the rest of the variables fixed.
We present our algorithm, several variants, and the results for the
optimization of standard QUBO problem instances from OR-Library of sizes 500
and 2500 as well as the Palubeckis instances of sizes 3000 to 7000. For
practical use of the solver, we show the dependence of the time to best
solution on the desired gap to the best known solution. In addition, we study
the dependence of the gap and the time to best solution on the size of the
problems solved by the underlying optimizer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07608</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07608</id><created>2015-07-27</created><authors><author><keyname>Kbah</keyname><forenames>Zaid</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author></authors><title>Resource Allocation in Cellular Systems for Applications with Random
  Parameters</title><categories>cs.NI</categories><comments>6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we conduct a study to optimize resource allocation for
adaptive real-time and delay-tolerant applications in cellular systems. To
represent the user applications via several devices and equipment,
sigmoidal-like and logarithm utility functions are used. A fairness
proportional utility functions policy approach is used to allocate the
resources among the user equipment (UE)s in a utility percentage form and to
ensure a minimum level of customer satisfaction for all the subscribers.
Meanwhile, the priority of resources allocation is given to the real-time
applications. We measure the effect of the stochastic variations of the
adaptive real-time applications on the optimal rate allocation process and
compare the results of deterministic and stochastic systems. Our work is
distinct from the other resource allocation optimization works in that we use
bisection method besides convex optimization techniques to optimally allocate
rates, and we present the adaptive real- time applications in a stochastic
form. We use Microsoft Visual Basic for Applications with Arena Simulation
Software interface to simulate the optimization problem. Finally, we present
our optimization algorithm results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07622</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07622</id><created>2015-07-27</created><updated>2016-02-01</updated><authors><author><keyname>Takagi</keyname><forenames>Takuya</forenames></author><author><keyname>Inenaga</keyname><forenames>Shunsuke</forenames></author><author><keyname>Arimura</keyname><forenames>Hiroki</forenames></author></authors><title>Fully-online construction of suffix trees and DAWGs for multiple texts</title><categories>cs.DS</categories><comments>22 pages, 11 figures, LaTeX; Corrected typos, modification of the
  author's affiliation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider fully-online construction of indexing data
structures for multiple texts. Let $T = {T_1, ..., T_K}$ be a collection of
texts. By fully on-line, we mean that a new character can be appended to any
text in $T$ at any time. This is a natural generalization of semi-online
construction of indexing data structures for multiple texts in which, after a
new character is appended to the $k$th text $T_k$, then its previous texts
$T_1, ..., T_{k-1}$ will remain static. Our fully online scenario arises when
we index multi-sensor data. We propose fully online algorithms which construct
the directed acyclic word graph (DAWG) and the generalized suffix tree (GST)
for $T$ in $O(N log \sigma)$ time and $O(N)$ space, where $N$ and $\sigma$
denote the total length of texts in $T$ and the alphabet size, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07628</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07628</id><created>2015-07-27</created><authors><author><keyname>Liu</keyname><forenames>Xishuo</forenames></author><author><keyname>Draper</keyname><forenames>Stark C.</forenames></author></authors><title>LP-decodable multipermutation codes</title><categories>cs.IT math.IT</categories><comments>This work was supported by the National Science Foundation (NSF)
  under Grants CCF-1217058 and by a Natural Sciences and Engineering Research
  Council of Canada (NSERC) Discovery Research Grant. This paper was submitted
  to IEEE Trans. Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a new way of constructing and decoding
multipermutation codes. Multipermutations are permutations of a multiset that
generally consist of duplicate entries. We first introduce a class of binary
matrices called multipermutation matrices, each of which corresponds to a
unique and distinct multipermutation. By enforcing a set of linear constraints
on these matrices, we define a new class of codes that we term LP-decodable
multipermutation codes. In order to decode these codes using a linear program
(LP), thereby enabling soft decoding, we characterize the convex hull of
multipermutation matrices. This characterization allows us to relax the coding
constraints to a polytope and to derive two LP decoding problems. These two
problems are respectively formulated by relaxing the maximum likelihood
decoding problem and the minimum Chebyshev distance decoding problem.
  Because these codes are non-linear, we also study efficient encoding and
decoding algorithms. We first describe an algorithm that maps consecutive
integers, one by one, to an ordered list of multipermutations. Based on this
algorithm, we develop an encoding algorithm for a code proposed by Shieh and
Tsai, a code that falls into our class of LP-decodable multipermutation codes.
Regarding decoding algorithms, we propose an efficient distributed decoding
algorithm based on the alternating direction method of multipliers (ADMM).
Finally, we observe from simulation results that the soft decoding techniques
we introduce can significantly outperform hard decoding techniques that are
based on quantized channel outputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07629</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07629</id><created>2015-07-27</created><authors><author><keyname>Orchard</keyname><forenames>Garrick</forenames></author><author><keyname>Jayawant</keyname><forenames>Ajinkya</forenames></author><author><keyname>Cohen</keyname><forenames>Gregory</forenames></author><author><keyname>Thakor</keyname><forenames>Nitish</forenames></author></authors><title>Converting Static Image Datasets to Spiking Neuromorphic Datasets Using
  Saccades</title><categories>cs.DB q-bio.NC</categories><comments>10 pages, 6 figures in Frontiers in Neuromorphic Engineering, special
  topic on Benchmarks and Challenges for Neuromorphic Engineering, 2015 (under
  review)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Creating datasets for Neuromorphic Vision is a challenging task. A lack of
available recordings from Neuromorphic Vision sensors means that data must
typically be recorded specifically for dataset creation rather than collecting
and labelling existing data. The task is further complicated by a desire to
simultaneously provide traditional frame-based recordings to allow for direct
comparison with traditional Computer Vision algorithms. Here we propose a
method for converting existing Computer Vision static image datasets into
Neuromorphic Vision datasets using an actuated pan-tilt camera platform. Moving
the sensor rather than the scene or image is a more biologically realistic
approach to sensing and eliminates timing artifacts introduced by monitor
updates when simulating motion on a computer monitor. We present conversion of
two popular image datasets (MNIST and Caltech101) which have played important
roles in the development of Computer Vision, and we provide performance metrics
on these datasets using spike-based recognition algorithms. This work
contributes datasets for future use in the field, as well as results from
spike-based algorithms against which future works can compare. Furthermore, by
converting datasets already popular in Computer Vision, we enable more direct
comparison with frame-based approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07632</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07632</id><created>2015-07-27</created><authors><author><keyname>Gallegos</keyname><forenames>Luciano</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Huang</keyname><forenames>Arthur</forenames></author><author><keyname>Garcia</keyname><forenames>David</forenames></author></authors><title>Geography of Emotion: Where in a City are People Happier?</title><categories>cs.CY</categories><comments>submitted to WSDM</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location-sharing services were built upon people's desire to share their
activities and locations with others. By &quot;checking-in&quot; to a place, such as a
restaurant, a park, gym, or train station, people disclose where they are,
thereby providing valuable information about land use and utilization of
services in urban areas. This information may, in turn, be used to design
smarter, happier, more equitable cities. We use data from Foursquare
location-sharing service to identify areas within a major US metropolitan area
with many check-ins, i.e., areas that people like to use. We then use data from
the Twitter microblogging platform to analyze the properties of these areas.
Specifically, we have extracted a large corpus of geo-tagged messages, called
tweets, from a major metropolitan area and linked them US Census data through
their locations. This allows us to measure the sentiment expressed in tweets
that are posted from a specific area, and also use that area's demographic
properties in analysis. Our results reveal that areas with many check-ins are
different from other areas within the metropolitan region. In particular, these
areas have happier tweets, which also encourage people from other areas to
commute longer distances to these places. These findings shed light on human
mobility patterns, as well as how physical environment influences human
emotions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07633</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07633</id><created>2015-07-27</created><updated>2015-08-15</updated><authors><author><keyname>Achlioptas</keyname><forenames>Dimitris</forenames></author><author><keyname>Iliopoulos</keyname><forenames>Fotis</forenames></author></authors><title>Focused Stochastic Local Search and the Lov\'asz Local Lemma</title><categories>cs.DM math.CO math.PR</categories><comments>Generalized the analysis of the Recursive Walk algorithm; corrected
  the proof of Acyclic Edge Coloring result</comments><msc-class>68W20</msc-class><acm-class>F.1.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop tools for analyzing focused stochastic local search algorithms.
These are algorithms which search a state space probabilistically by repeatedly
selecting a constraint that is violated in the current state and moving to a
random nearby state which, hopefully, addresses the violation without
introducing many new ones. A large class of such algorithms arise from the
algorithmization of the Lov\'asz Local Lemma, a non-constructive tool for
proving the existence of satisfying states. Here we give tools that provide a
unified analysis of such algorithms and of many more, expressing them as
instances of a general framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07636</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07636</id><created>2015-07-27</created><authors><author><keyname>Mahadevan</keyname><forenames>Sridhar</forenames></author><author><keyname>Chandar</keyname><forenames>Sarath</forenames></author></authors><title>Reasoning about Linguistic Regularities in Word Embeddings using Matrix
  Manifolds</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has explored methods for learning continuous vector space word
representations reflecting the underlying semantics of words. Simple vector
space arithmetic using cosine distances has been shown to capture certain types
of analogies, such as reasoning about plurals from singulars, past tense from
present tense, etc. In this paper, we introduce a new approach to capture
analogies in continuous word representations, based on modeling not just
individual word vectors, but rather the subspaces spanned by groups of words.
We exploit the property that the set of subspaces in n-dimensional Euclidean
space form a curved manifold space called the Grassmannian, a quotient subgroup
of the Lie group of rotations in n- dimensions. Based on this mathematical
model, we develop a modified cosine distance model based on geodesic kernels
that captures relation-specific distances across word categories. Our
experiments on analogy tasks show that our approach performs significantly
better than the previous approaches for the given task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07646</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07646</id><created>2015-07-28</created><authors><author><keyname>Kanazawa</keyname><forenames>Angjoo</forenames></author><author><keyname>Kovalsky</keyname><forenames>Shahar</forenames></author><author><keyname>Basri</keyname><forenames>Ronen</forenames></author><author><keyname>Jacobs</keyname><forenames>David W.</forenames></author></authors><title>Learning 3D Articulation and Deformation using 2D Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rise of Augmented Reality, Virtual Reality and 3D printing, methods
for acquiring 3D models from the real world are more important then ever. One
approach to generate 3D models is by modifying an existing template 3D mesh to
fit the pose and shape of similar objects in images. To model the pose of an
highly articulated and deformable object, it is essential to understand how an
object class can articulate and deform. In this paper we propose to learn a
class model of articulation and deformation from a set of annotated Internet
images. To do so, we incorporate the idea of local stiffness, which specifies
the amount of distortion allowed for a local region. Our system jointly learns
the stiffness as it deforms a template 3D mesh to fit the pose of the objects
in images. We show that this seemingly complex task can be solved with a
sequence of convex optimization programs. We demonstrate our approach on two
highly articulated and deformable animals, cats and horses. Our approach
obtains significantly more realistic deformations compared to other related
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07648</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07648</id><created>2015-07-28</created><authors><author><keyname>Aziz</keyname><forenames>Rehan Abdul</forenames></author><author><keyname>Chu</keyname><forenames>Geoffrey</forenames></author><author><keyname>Muise</keyname><forenames>Christian</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter</forenames></author></authors><title>Projected Model Counting</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model counting is the task of computing the number of assignments to
variables V that satisfy a given propositional theory F. Model counting is an
essential tool in probabilistic reasoning. In this paper, we introduce the
problem of model counting projected on a subset P of original variables that we
call 'priority' variables. The task is to compute the number of assignments to
P such that there exists an extension to 'non-priority' variables V\P that
satisfies F. Projected model counting arises when some parts of the model are
irrelevant to the counts, in particular when we require additional variables to
model the problem we are counting in SAT. We discuss three different approaches
to projected model counting (two of which are novel), and compare their
performance on different benchmark problems.
  To appear in 18th International Conference on Theory and Applications of
Satisfiability Testing, September 24-27, 2015, Austin, Texas, USA
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07662</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07662</id><created>2015-07-28</created><authors><author><keyname>Chavan</keyname><forenames>Chandrashekhar Pomu</forenames></author><author><keyname>Venkataram</keyname><forenames>Pallapa</forenames></author></authors><title>Designing a Routing Protocol for Ubiquitous Networks Using ECA Scheme</title><categories>cs.NI</categories><comments>18 pages, 13 figures, Fifth International Conference on Advances in
  Computing and Information Technology (ACITY 2015), July 25-26-2015, Chennai,
  India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have designed a novel Event-Condition-Action(ECA) scheme based Ad hoc
On-demand Distance Vector(ECA-AODV) routing protocol for a Ubiquitous
Network(UbiNet). ECA-AODV is designed to make routing decision dynamically and
quicker response to dynamic network conditions as and when event occur. ECA
scheme essentially consists of three modules to make runtime routing decision
quicker. First, event module receive event that occur in a UbiNet and split up
event into event type and event attributes. Second, condition module obtain
event details from event module split up each condition into condition
attributes that matches event and fire the rule as soon as condition hold.
Third, action module make runtime decisions based on event obtained and
condition applied. We have simulated and tested the designed ECA scheme by
considering ubiquitous museum environment as a case study with nodes range from
10 to 100. The simulation results show the time efficient with minimal
operations
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07677</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07677</id><created>2015-07-28</created><authors><author><keyname>Bosansky</keyname><forenames>Branislav</forenames></author><author><keyname>Branzei</keyname><forenames>Simina</forenames></author><author><keyname>Hansen</keyname><forenames>Kristoffer Arnsfelt</forenames></author><author><keyname>Miltersen</keyname><forenames>Peter Bro</forenames></author><author><keyname>Sorensen</keyname><forenames>Troels Bjerre</forenames></author></authors><title>Computation of Stackelberg Equilibria of Finite Sequential Games</title><categories>cs.GT cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Stackelberg equilibrium solution concept describes optimal strategies to
commit to: Player 1 (termed the leader) publicly commits to a strategy and
Player 2 (termed the follower) plays a best response to this strategy (ties are
broken in favor of the leader). We study Stackelberg equilibria in finite
sequential games (or extensive-form games) and provide new exact algorithms,
approximate algorithms, and hardness results for several classes of these
sequential games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07680</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07680</id><created>2015-07-28</created><updated>2015-11-20</updated><authors><author><keyname>Ollivier</keyname><forenames>Yann</forenames></author><author><keyname>Tallec</keyname><forenames>Corentin</forenames></author><author><keyname>Charpiat</keyname><forenames>Guillaume</forenames></author></authors><title>Training recurrent networks online without backtracking</title><categories>cs.NE cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the &quot;NoBackTrack&quot; algorithm to train the parameters of dynamical
systems such as recurrent neural networks. This algorithm works in an online,
memoryless setting, thus requiring no backpropagation through time, and is
scalable, avoiding the large computational and memory cost of maintaining the
full gradient of the current state with respect to the parameters.
  The algorithm essentially maintains, at each time, a single search direction
in parameter space. The evolution of this search direction is partly stochastic
and is constructed in such a way to provide, at every time, an unbiased random
estimate of the gradient of the loss function with respect to the parameters.
Because the gradient estimate is unbiased, on average over time the parameter
is updated as it should.
  The resulting gradient estimate can then be fed to a lightweight Kalman-like
filter to yield an improved algorithm. For recurrent neural networks, the
resulting algorithms scale linearly with the number of parameters.
  Small-scale experiments confirm the suitability of the approach, showing that
the stochastic approximation of the gradient introduced in the algorithm is not
detrimental to learning. In particular, the Kalman-like version of NoBackTrack
is superior to backpropagation through time (BPTT) when the time span of
dependencies in the data is longer than the truncation span for BPTT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07688</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07688</id><created>2015-07-28</created><updated>2016-03-02</updated><authors><author><keyname>Albrecht</keyname><forenames>Stefano V.</forenames></author><author><keyname>Crandall</keyname><forenames>Jacob W.</forenames></author><author><keyname>Ramamoorthy</keyname><forenames>Subramanian</forenames></author></authors><title>Belief and Truth in Hypothesised Behaviours</title><categories>cs.AI cs.GT</categories><comments>44 pages; final manuscript published in Artificial Intelligence (AIJ)</comments><acm-class>I.2.11</acm-class><doi>10.1016/j.artint.2016.02.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a long history in game theory on the topic of Bayesian or &quot;rational&quot;
learning, in which each player maintains beliefs over a set of alternative
behaviours, or types, for the other players. This idea has gained increasing
interest in the artificial intelligence (AI) community, where it is used as a
method to control a single agent in a system composed of multiple agents with
unknown behaviours. The idea is to hypothesise a set of types, each specifying
a possible behaviour for the other agents, and to plan our own actions with
respect to those types which we believe are most likely, given the observed
actions of the agents. The game theory literature studies this idea primarily
in the context of equilibrium attainment. In contrast, many AI applications
have a focus on task completion and payoff maximisation. With this perspective
in mind, we identify and address a spectrum of questions pertaining to belief
and truth in hypothesised types. We formulate three basic ways to incorporate
evidence into posterior beliefs and show when the resulting beliefs are
correct, and when they may fail to be correct. Moreover, we demonstrate that
prior beliefs can have a significant impact on our ability to maximise payoffs
in the long-term, and that they can be computed automatically with consistent
performance effects. Furthermore, we analyse the conditions under which we are
able complete our task optimally, despite inaccuracies in the hypothesised
types. Finally, we show how the correctness of hypothesised types can be
ascertained during the interaction via an automated statistical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07697</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07697</id><created>2015-07-28</created><updated>2015-09-21</updated><authors><author><keyname>Jacobs</keyname><forenames>Bart</forenames><affiliation>Katholieke Universiteit Leuven</affiliation></author><author><keyname>Vogels</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>Katholieke Universiteit Leuven</affiliation></author><author><keyname>Piessens</keyname><forenames>Frank</forenames><affiliation>Katholieke Universiteit Leuven</affiliation></author></authors><title>Featherweight VeriFast</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:19) 2015</journal-ref><doi>10.2168/LMCS-11(3:19)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  VeriFast is a leading research prototype tool for the sound modular
verification of safety and correctness properties of single-threaded and
multithreaded C and Java programs. It has been used as a vehicle for
exploration and validation of novel program verification techniques and for
industrial case studies; it has served well at a number of program verification
competitions; and it has been used for teaching by multiple teachers
independent of the authors. However, until now, while VeriFast's operation has
been described informally in a number of publications, and specific
verification techniques have been formalized, a clear and precise exposition of
how VeriFast works has not yet appeared. In this article we present for the
first time a formal definition and soundness proof of a core subset of the
VeriFast program verification approach. The exposition aims to be both
accessible and rigorous: the text is based on lecture notes for a graduate
course on program verification, and it is backed by an executable
machine-readable definition and machine-checked soundness proof in Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07698</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07698</id><created>2015-07-28</created><authors><author><keyname>Naqvi</keyname><forenames>Syed Hassan Raza</forenames></author><author><keyname>Spagnolini</keyname><forenames>Umberto</forenames></author></authors><title>Interference-Cooperation in Multi-User/Multi-Operator Receivers</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multi-user scenario where users belong to different operators, any
interference mitigation method needs unavoidably some degree of cooperation
among service providers. In this paper we propose a cooperation strategy based
on the exchange of mutual interference among operators, rather than of decoded
data, to let every operator to recover an augmented degree of diversity either
for channel estimation and multi-user detection. In xDSL scenario where
multiple operators share the same cable binder the interference-cooperation
(IC) approach outperforms data-exchange methods and preserves to certain degree
the privacy of the users as signals can be tailored to prevent each operator to
infer parameters (channel and data) of the users from the other operators.
  The IC method is based on Expectation Maximization estimation shaped to
account for the degree of information that each operator can exchange with the
others during the two steps of multi-user channel estimation and multi-user
detection. Convergence of IC is guaranteed into few iterations and it does not
depend on the structure of the interference. IC performance attains those of
centralized receivers (i.e., one fusion-center that collects all the received
signals from all the users/operators), with some loss when in heavily
interfered multi-user channel such as in twisted-pair communications allocated
beyond 50-100MHz spectrum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07716</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07716</id><created>2015-07-28</created><authors><author><keyname>Dai</keyname><forenames>Mingbo</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>A Hierarchical Rate Splitting Strategy for FDD Massive MIMO under
  Imperfect CSIT</title><categories>cs.IT math.IT</categories><comments>Accepted paper at IEEE CAMAD 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multiuser MIMO broadcast channel, the rate performance is affected by
the multiuser interference when the Channel State Information at the
Transmitter (CSIT) is imperfect. To tackle the interference problem, a
Rate-Splitting (RS) approach has been proposed recently, which splits one
user's message into a common and a private part, and superimposes the common
message on top of the private messages. The common message is drawn from a
public codebook and should be decoded by all users. In this paper, we propose a
novel and general framework, denoted as Hierarchical Rate Splitting (HRS), that
is particularly suited to FDD massive MIMO systems. HRS simultaneously
transmits private messages intended to each user and two kinds of common
messages that can be decoded by all users and by a subset of users,
respectively. We analyse the asymptotic sum rate of HRS under imperfect CSIT. A
closed-form power allocation is derived which provides insights into the
effects of system parameters. Finally, simulation results validate the
significant sum rate gain of HRS over various baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07719</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07719</id><created>2015-07-28</created><authors><author><keyname>Crafa</keyname><forenames>Silvia</forenames></author></authors><title>The role of concurrency in an evolutionary view of programming
  abstractions</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we examine how concurrency has been embodied in mainstream
programming languages. In particular, we rely on the evolutionary talking
borrowed from biology to discuss major historical landmarks and crucial
concepts that shaped the development of programming languages. We examine the
general development process, occasionally deepening into some language, trying
to uncover evolutionary lineages related to specific programming traits. We
mainly focus on concurrency, discussing the different abstraction levels
involved in present-day concurrent programming and emphasizing the fact that
they correspond to different levels of explanation. We then comment on the role
of theoretical research on the quest for suitable programming abstractions,
recalling the importance of changing the working framework and the way of
looking every so often. This paper is not meant to be a survey of modern
mainstream programming languages: it would be very incomplete in that sense. It
aims instead at pointing out a number of remarks and connect them under an
evolutionary perspective, in order to grasp a unifying, but not simplistic,
view of the programming languages development process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07727</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07727</id><created>2015-07-28</created><authors><author><keyname>Mihal&#xe1;k</keyname><forenames>Mat&#xfa;&#x161;</forenames></author><author><keyname>Uzna&#x144;ski</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Yordanov</keyname><forenames>Pencho</forenames></author></authors><title>Prime Factorization of the Kirchhoff Polynomial: Compact Enumeration of
  Arborescences</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of enumerating all rooted directed spanning trees
(arborescences) of a directed graph (digraph) $G=(V,E)$ of $n$ vertices. An
arborescence $A$ consisting of edges $e_1,\ldots,e_{n-1}$ can be represented as
a monomial $e_1\cdot e_2 \cdots e_{n-1}$ in variables $e \in E$. All
arborescences $\mathsf{arb}(G)$ of a digraph then define the Kirchhoff
polynomial $\sum_{A \in \mathsf{arb}(G)} \prod_{e\in A} e$. We show how to
compute a compact representation of the Kirchhoff polynomial -- its prime
factorization, and how it relates to combinatorial properties of digraphs such
as strong connectivity and vertex domination. In particular, we provide digraph
decomposition rules that correspond to factorization steps of the polynomial,
and also give necessary and sufficient primality conditions of the resulting
factors expressed by connectivity properties of the corresponding decomposed
components. Thereby, we obtain a linear time algorithm for decomposing a
digraph into components corresponding to factors of the initial polynomial, and
a guarantee that no finer factorization is possible. The decomposition serves
as a starting point for a recursive deletion-contraction algorithm, and also as
a preprocessing phase for iterative enumeration algorithms. Both approaches
produce a compressed output and retain some structural properties in the
resulting polynomial. This proves advantageous in practical applications such
as calculating steady states on digraphs governed by Laplacian dynamics, or
computing the greatest common divisor of Kirchhoff polynomials. Finally, we
initiate the study of a class of digraphs which allow for a practical
enumeration of arborescences. Using our decomposition rules we observe that
various digraphs from real-world applications fall into this class or are
structurally similar to it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07739</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07739</id><created>2015-07-28</created><authors><author><keyname>Anglano</keyname><forenames>Cosimo</forenames></author></authors><title>Forensic Analysis of WhatsApp Messenger on Android Smartphones</title><categories>cs.CR</categories><comments>(c)2014. This manuscript version is made available under the
  CC-BY-NC-ND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0/</comments><journal-ref>Digital Investigation, Vol. 11, No. 3, pp. 201-213, September 2014</journal-ref><doi>10.1016/j.diin.2014.04.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the forensic analysis of the artifacts left on Android devices by
\textit{WhatsApp Messenger}, the client of the WhatsApp instant messaging
system. We provide a complete description of all the artifacts generated by
WhatsApp Messenger, we discuss the decoding and the interpretation of each one
of them, and we show how they can be correlated together to infer various types
of information that cannot be obtained by considering each one of them in
isolation.
  By using the results discussed in this paper, an analyst will be able to
reconstruct the list of contacts and the chronology of the messages that have
been exchanged by users. Furthermore, thanks to the correlation of multiple
artifacts, (s)he will be able to infer information like when a specific contact
has been added, to recover deleted contacts and their time of deletion, to
determine which messages have been deleted, when these messages have been
exchanged, and the users that exchanged them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07749</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07749</id><created>2015-07-28</created><updated>2015-11-11</updated><authors><author><keyname>Ramsey</keyname><forenames>Joseph D.</forenames></author></authors><title>Scaling up Greedy Causal Search for Continuous Variables</title><categories>cs.AI</categories><comments>12 pages, 2 figures, tech report for Center for Causal Discovery</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As standardly implemented in R or the Tetrad program, causal search
algorithms used most widely or effectively by scientists have severe
dimensionality constraints that make them inappropriate for big data problems
without sacrificing accuracy. However, implementation improvements are
possible. We explore optimizations for the Greedy Equivalence Search that allow
search on 50,000-variable problems in 13 minutes for sparse models with 1000
samples on a four-processor, 16G laptop computer. We finish a problem with 1000
samples on 1,000,000 variables in 18 hours for sparse models on a supercomputer
node at the Pittsburgh Supercomputing Center with 40 processors and 384 G RAM.
The same algorithm can be applied to discrete data, with a slower discrete
score, though the discrete implementation currently does not scale as well in
our experiments; we have managed to scale up to about 10,000 variables in
sparse models with 1000 samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07755</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07755</id><created>2015-07-28</created><authors><author><keyname>Ghidini</keyname><forenames>Chiara</forenames></author><author><keyname>Serafini</keyname><forenames>Luciano</forenames></author></authors><title>Distributed First Order Logic</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Distributed First Order Logic (DFOL) is a formalism introduced more than 10
years ago with the purpose of formalising distributed knowledge-based systems.
In these systems, knowledge is scattered in a set of heterogeneous and
intercon- nected modules. The original version of DFOL defined a set of
primitives for the specification of a restricted subset of interconnections
between knowledge mod- ules. Despite this limitation, the ideas originally
introduced in DFOL have influ- enced, in a significant manner, several works in
the areas of distributed knowl- edge representation, heterogeneous knowledge
integration, and semantic web, in the last 10+ years. The development of these
works, together with the growth of interest in logical formalisms for the
representation of distributed knowledge, have, in turn, fostered the
development of a set of extensions and modifications of DFOL which have never
been systematically described and published. In this paper we intend to
sistematize and present, in a clear and comprehensive manner, this completely
revised and extended version of DFOL. This version presents a general mechanism
for the specification of interconnections between knowledge modules which takes
into account the work done in applying the DFOL ideas in various fields of
distributed knowledge representation, heterogeneous knowledge integration, and
semantic web. The resulting general version of DFOL is there- fore proposed as
a clear formal tool for the representations and reasoning about distributed
knowledge-base systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07760</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07760</id><created>2015-07-28</created><authors><author><keyname>Simon</keyname><forenames>Konrad</forenames></author><author><keyname>Sheorey</keyname><forenames>Sameer</forenames></author><author><keyname>Jacobs</keyname><forenames>David</forenames></author><author><keyname>Basri</keyname><forenames>Ronen</forenames></author></authors><title>A Hyperelastic Two-Scale Optimization Model for Shape Matching</title><categories>cs.CG cs.CV cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We suggest a novel shape matching algorithm for three-dimensional surface
meshes of disk or sphere topology. The method is based on the physical theory
of nonlinear elasticity and can hence handle large rotations and deformations.
Deformation boundary conditions that supplement the underlying equations are
usually unknown. Given an initial guess, these are optimized such that the
mechanical boundary forces that are responsible for the deformation are of a
simple nature. We show a heuristic way to approximate the nonlinear
optimization problem by a sequence of convex problems using finite elements.
The deformation cost, i.e, the forces, is measured on a coarse scale while
ICP-like matching is done on the fine scale. We demonstrate the plausibility of
our algorithm on examples taken from different datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07766</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07766</id><created>2015-07-28</created><updated>2015-12-07</updated><authors><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Wang</keyname><forenames>Chang-Jen</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author><author><keyname>Ting</keyname><forenames>Pangan</forenames></author></authors><title>Bayes-Optimal Joint Channel-and-Data Estimation for Massive MIMO with
  Low-Precision ADCs</title><categories>cs.IT math.IT</categories><comments>accepted in IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a multiple-input multiple-output (MIMO) receiver with
very low-precision analog-to-digital convertors (ADCs) with the goal of
developing massive MIMO antenna systems that require minimal cost and power.
Previous studies demonstrated that the training duration should be {\em
relatively long} to obtain acceptable channel state information. To address
this requirement, we adopt a joint channel-and-data (JCD) estimation method
based on Bayes-optimal inference. This method yields minimal mean square errors
with respect to the channels and payload data. We develop a Bayes-optimal JCD
estimator using a recent technique based on approximate message passing. We
then present an analytical framework to study the theoretical performance of
the estimator in the large-system limit. Simulation results confirm our
analytical results, which allow the efficient evaluation of the performance of
quantized massive MIMO systems and provide insights into effective system
design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07775</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07775</id><created>2015-07-28</created><updated>2016-01-12</updated><authors><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Tight uniform continuity bounds for quantum entropies: conditional
  entropy, relative entropy distance and energy constraints</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>12pp, lots of lemmas but no theorem. V2, v3 v4, v5, v6 have ever more
  references and more accurate attribution of previous results. In v6 correct
  proof of the asymptotic continuity of entanglement cost, which had been
  claimed already in v4 and been retracted in v5</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a bouquet of continuity bounds for quantum entropies, falling
broadly into two classes: First, a tight analysis of the Alicki-Fannes
continuity bounds for the conditional von Neumann entropy, reaching almost the
best possible form that depends only on the system dimension and the trace
distance of the states. Almost the same proof can be used to derive similar
continuity bounds for the relative entropy distance from a convex set of states
or positive operators. As applications we give new proofs, with tighter bounds,
of the asymptotic continuity of the relative entropy of entanglement, $E_R$,
and its regularization $E_R^\infty$, as well as of the entanglement of
formation, $E_F$. Using a novel &quot;quantum coupling&quot; of density operators, which
may be of independent interest, we extend the latter to an asymptotic
continuity bound for the regularized entanglement of formation, aka
entanglement cost, $E_C=E_F^\infty$.
  Second, analogous continuity bounds for the von Neumann entropy and
conditional entropy in infinite dimensional systems under an energy constraint,
most importantly systems of multiple quantum harmonic oscillators. While
without an energy bound the entropy is discontinuous, it is well-known to be
continuous on states of bounded energy. However, a quantitative statement to
that effect seems not to have been known. Here, under some regularity
assumptions on the Hamiltonian, we find that, quite intuitively, the Gibbs
entropy at the given energy roughly takes the role of the Hilbert space
dimension in the finite-dimensional Fannes inequality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07789</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07789</id><created>2015-07-28</created><authors><author><keyname>Friedman</keyname><forenames>Eric J.</forenames></author><author><keyname>Landsberg</keyname><forenames>Adam S.</forenames></author></authors><title>Duality and Nonlinear Graph Laplacians</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an iterative algorithm for solving a class of \\nonlinear
Laplacian system of equations in $\tilde{O}(k^2m \log(kn/\epsilon))$
iterations, where $k$ is a measure of nonlinearity, $n$ is the number of
variables, $m$ is the number of nonzero entries in the graph Laplacian $L$,
$\epsilon$ is the solution accuracy and $\tilde{O}()$ neglects (non-leading)
logarithmic terms. This algorithm is a natural nonlinear extension of the one
by of Kelner et. al., which solves a linear Laplacian system of equations in
nearly linear time. Unlike the linear case, in the nonlinear case each
iteration takes $\tilde{O}(n)$ time so the total running time is
$\tilde{O}(k^2mn \log(kn/\epsilon))$. For sparse graphs where $m = O(n)$ and
fixed $k$ this nonlinear algorithm is $\tilde{O}(n^2 \log(n/\epsilon))$ which
is slightly faster than standard methods for solving linear equations, which
require approximately $O(n^{2.38})$ time. Our analysis relies on the
construction of a nonlinear &quot;energy function&quot; and a nonlinear extension of the
duality analysis of Kelner et. al to the nonlinear case without any explicit
references to spectral analysis or electrical flows. These new insights and
results provide tools for more general extensions to spectral theory and
nonlinear applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07800</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07800</id><created>2015-07-28</created><authors><author><keyname>Mata</keyname><forenames>Gadea</forenames></author><author><keyname>Heras</keyname><forenames>J&#xf3;nathan</forenames></author><author><keyname>Morales</keyname><forenames>Miguel</forenames></author><author><keyname>Romero</keyname><forenames>Ana</forenames></author><author><keyname>Rubio</keyname><forenames>Julio</forenames></author></authors><title>SynapCountJ --- a Tool for Analyzing Synaptic Densities in Neurons</title><categories>cs.CV q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quantification of synapses is instrumental to measure the evolution of
synaptic densities of neurons under the effect of some physiological
conditions, neuronal diseases or even drug treatments. However, the manual
quantification of synapses is a tedious, error-prone, time-consuming and
subjective task; therefore, tools that might automate this process are
desirable. In this paper, we present SynapCountJ, an ImageJ plugin, that can
measure synaptic density of individual neurons obtained by immunofluorescence
techniques, and also can be applied for batch processing of neurons that have
been obtained in the same experiment or using the same setting. The procedure
to quantify synapses implemented in SynapCountJ is based on the colocalization
of three images of the same neuron (the neuron marked with two antibody markers
and the structure of the neuron) and is inspired by methods coming from
Computational Algebraic Topology. SynapCountJ provides a procedure to
semi-automatically quantify the number of synapses of neuron cultures; as a
result, the time required for such an analysis is greatly reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07815</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07815</id><created>2015-07-28</created><authors><author><keyname>Lisanti</keyname><forenames>Giuseppe</forenames></author><author><keyname>Karaman</keyname><forenames>Svebor</forenames></author><author><keyname>Pezzatini</keyname><forenames>Daniele</forenames></author><author><keyname>Del Bimbo</keyname><forenames>Alberto</forenames></author></authors><title>A Multi-Camera Image Processing and Visualization System for Train
  Safety Assessment</title><categories>cs.CV</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a machine vision system to efficiently monitor,
analyze and present visual data acquired with a railway overhead gantry
equipped with multiple cameras. This solution aims to improve the safety of
daily life railway transportation in a two- fold manner: (1) by providing
automatic algorithms that can process large imagery of trains (2) by helping
train operators to keep attention on any possible malfunction. The system is
designed with the latest cutting edge, high-rate visible and thermal cameras
that ob- serve a train passing under an railway overhead gantry. The machine
vision system is composed of three principal modules: (1) an automatic wagon
identification system, recognizing the wagon ID according to the UIC
classification of railway coaches; (2) a temperature monitoring system; (3) a
system for the detection, localization and visualization of the pantograph of
the train. These three machine vision modules process batch trains sequences
and their resulting analysis are presented to an operator using a multitouch
user interface. We detail all technical aspects of our multi-camera portal: the
hardware requirements, the software developed to deal with the high-frame rate
cameras and ensure reliable acquisition, the algorithms proposed to solve each
computer vision task, and the multitouch interaction and visualization
interface. We evaluate each component of our system on a dataset recorded in an
ad-hoc railway test-bed, showing the potential of our proposed portal for train
safety assessment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07816</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07816</id><created>2015-07-28</created><authors><author><keyname>Gupta</keyname><forenames>Udit</forenames></author></authors><title>Comparison between security majors in virtual machine and linux
  containers</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtualization started to gain traction in the domain of information
technology in the early 2000s when managing resource distribution was becoming
an uphill task for developers. As a result, tools like VMWare, Hyper V
(hypervisor) started making inroads into the software repository on different
operating systems. VMWare and Hyper V could support multiple virtual machines
running on them with each having their own isolated environment. Due to this
isolation, the security aspects of virtual machines (VMs) did not differ much
from that of physical machines (having a dedicated operating system on
hardware). The advancement made in the domain of linux containers (LXC) has
taken virtualization to an altogether different level where resource
utilization by various applications has been further optimized. But the
container security has assumed primary importance amongst the researchers today
and this paper is inclined towards providing a brief overview about comparisons
between security of container and VMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07826</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07826</id><created>2015-07-28</created><authors><author><keyname>de Arruda</keyname><forenames>Henrique F.</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author></authors><title>Classifying informative and imaginative prose using complex networks</title><categories>cs.CL</categories><journal-ref>Europhysics Letters (EPL) 113 (2016) 28007</journal-ref><doi>10.1209/0295-5075/113/28007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical methods have been widely employed in recent years to grasp many
language properties. The application of such techniques have allowed an
improvement of several linguistic applications, which encompasses machine
translation, automatic summarization and document classification. In the
latter, many approaches have emphasized the semantical content of texts, as it
is the case of bag-of-word language models. This approach has certainly yielded
reasonable performance. However, some potential features such as the structural
organization of texts have been used only on a few studies. In this context, we
probe how features derived from textual structure analysis can be effectively
employed in a classification task. More specifically, we performed a supervised
classification aiming at discriminating informative from imaginative documents.
Using a networked model that describes the local topological/dynamical
properties of function words, we achieved an accuracy rate of up to 95%, which
is much higher than similar networked approaches. A systematic analysis of
feature relevance revealed that symmetry and accessibility measurements are
among the most prominent network measurements. Our results suggest that these
measurements could be used in related language applications, as they play a
complementary role in characterizing texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07830</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07830</id><created>2015-07-28</created><updated>2015-07-29</updated><authors><author><keyname>Yang</keyname><forenames>Yongxin</forenames></author><author><keyname>Hospedales</keyname><forenames>Timothy</forenames></author></authors><title>Zero-Shot Domain Adaptation via Kernel Regression on the Grassmannian</title><categories>cs.LG cs.CV</categories><comments>Accepted to BMVC 2015 Workshop on Differential Geometry in Computer
  Vision (DIFF-CV)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most visual recognition methods implicitly assume the data distribution
remains unchanged from training to testing. However, in practice domain shift
often exists, where real-world factors such as lighting and sensor type change
between train and test, and classifiers do not generalise from source to target
domains. It is impractical to train separate models for all possible situations
because collecting and labelling the data is expensive. Domain adaptation
algorithms aim to ameliorate domain shift, allowing a model trained on a source
to perform well on a different target domain. However, even for the setting of
unsupervised domain adaptation, where the target domain is unlabelled,
collecting data for every possible target domain is still costly. In this
paper, we propose a new domain adaptation method that has no need to access
either data or labels of the target domain when it can be described by a
parametrised vector and there exits several related source domains within the
same parametric space. It greatly reduces the burden of data collection and
annotation, and our experiments show some promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07833</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07833</id><created>2015-07-28</created><updated>2015-10-30</updated><authors><author><keyname>Gupta</keyname><forenames>Yayati</forenames></author><author><keyname>Das</keyname><forenames>Debarati</forenames></author><author><keyname>Iyengar</keyname><forenames>S. R. S.</forenames></author></authors><title>Pseudo-Cores: The Terminus of an Intelligent Viral Meme's Trajectory</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comprehending the virality of a meme can help us in addressing the problems
pertaining to disciplines like epidemiology and digital marketing. Therefore,
it is not surprising that memetics remains a highly analyzed research topic
ever since the mid 1990s. Some scientists choose to investigate the intrinsic
contagiousness of a meme while others study the problem from a network theory
perspective. In this paper, we revisit the idea of a core-periphery structure
and apply it to understand the trajectory of a viral meme in a social network.
We have proposed shell-based hill climbing algorithms to determine the path
from a periphery shell(where the meme originates) to the core of the network.
Further simulations and analysis on the networks behavioral characteristics
helped us unearth specialized shells which we term Pseudo-Cores. These shells
emulate the behavior of the core in terms of size of the cascade triggered. In
our experiments, we have considered two sets for the target nodes, one being
core and the other being any of the pseudo-core. We compare our algorithms
against already existing path finding algorithms and validate the better
performance experimentally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07838</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07838</id><created>2015-07-28</created><updated>2015-11-07</updated><authors><author><keyname>Gupta</keyname><forenames>Yayati</forenames></author><author><keyname>Iyengar</keyname><forenames>S. R. S.</forenames></author><author><keyname>Saini</keyname><forenames>Jaspal Singh</forenames></author><author><keyname>Sridhar</keyname><forenames>Nidhi</forenames></author></authors><title>Shifting Behaviour of Users: Towards Understanding the Fundamental Law
  of Social Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social Networking Sites (SNSs) are powerful marketing and communication
tools. There are hundreds of SNSs that have entered and exited the market over
time. The coexistence of multiple SNSs is a rarely observed phenomenon. Most
coexisting SNSs either serve different purposes for its users or have cultural
differences among them. The introduction of a new SNS with a better set of
features can lead to the demise of an existing SNS, as observed in the
transition from Orkut to Facebook. The paper proposes a model for analyzing the
transition of users from one SNS to another, when a new SNS is introduced in
the system. The game theoretic model proposed considers two major factors in
determining the success of a new SNS. The first being time that an old SNS gets
to stabilise. We study whether the time that a SNS like Facebook received to
monopolize its reach had a distinguishable effect. The second factor is the set
of features showcased by the new SNS. The results of the model are also
experimentally verified with data collected by means of a survey.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07844</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07844</id><created>2015-07-28</created><updated>2015-08-09</updated><authors><author><keyname>Pan</keyname><forenames>Yongping</forenames></author><author><keyname>Pan</keyname><forenames>Lin</forenames></author><author><keyname>Yu</keyname><forenames>Haoyong</forenames></author></authors><title>Composite Learning Control With Application to Inverted Pendulums</title><categories>cs.SY</categories><comments>5 pages, 6 figures, conference submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Composite adaptive control (CAC) that integrates direct and indirect adaptive
control techniques can achieve smaller tracking errors and faster parameter
convergence compared with direct and indirect adaptive control techniques.
However, the condition of persistent excitation (PE) still has to be satisfied
to guarantee parameter convergence in CAC. This paper proposes a novel model
reference composite learning control (MRCLC) strategy for a class of affine
nonlinear systems with parametric uncertainties to guarantee parameter
convergence without the PE condition. In the composite learning, an integral
during a moving-time window is utilized to construct a prediction error, a
linear filter is applied to alleviate the derivation of plant states, and both
the tracking error and the prediction error are applied to update parametric
estimates. It is proven that the closed-loop system achieves global
exponential-like stability under interval excitation rather than PE of
regression functions. The effectiveness of the proposed MRCLC has been verified
by the application to an inverted pendulum control problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07848</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07848</id><created>2015-07-28</created><authors><author><keyname>Juras</keyname><forenames>Martin</forenames></author><author><keyname>Marko</keyname><forenames>Frantisek</forenames></author><author><keyname>Zubkov</keyname><forenames>Alexandr N.</forenames></author></authors><title>Public-key cryptosystem based on invariants of diagonalizable groups</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a public key cryptosystem based on invariants of diagonalizable
groups. Theoretical results about degrees of invariants, which are related to
the security of such cryptosystem, are derived. Further, we derive results on
invariants of superanalogs of tori.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07851</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07851</id><created>2015-07-28</created><updated>2015-10-29</updated><authors><author><keyname>Achara</keyname><forenames>Jagdish Prasad</forenames></author><author><keyname>Acs</keyname><forenames>Gergely</forenames></author><author><keyname>Castelluccia</keyname><forenames>Claude</forenames></author></authors><title>On the Unicity of Smartphone Applications</title><categories>cs.CR</categories><comments>10 pages, 9 Figures, Appeared at ACM CCS Workshop on Privacy in
  Electronic Society (WPES) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prior works have shown that the list of apps installed by a user reveal a lot
about user interests and behavior. These works rely on the semantics of the
installed apps and show that various user traits could be learnt automatically
using off-the-shelf machine-learning techniques. In this work, we focus on the
re-identifiability issue and thoroughly study the unicity of smartphone apps on
a dataset containing 54,893 Android users collected over a period of 7 months.
Our study finds that any 4 apps installed by a user are enough (more than 95%
times) for the re-identification of the user in our dataset. As the complete
list of installed apps is unique for 99% of the users in our dataset, it can be
easily used to track/profile the users by a service such as Twitter that has
access to the whole list of installed apps of users. As our analyzed dataset is
small as compared to the total population of Android users, we also study how
unicity would vary with larger datasets. This work emphasizes the need of
better privacy guards against collection, use and release of the list of
installed apps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07855</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07855</id><created>2015-07-28</created><updated>2015-08-31</updated><authors><author><keyname>Lunardon</keyname><forenames>Guglielmo</forenames></author><author><keyname>Trombetti</keyname><forenames>Rocco</forenames></author><author><keyname>Zhou</keyname><forenames>Yue</forenames></author></authors><title>Generalized Twisted Gabidulin Codes</title><categories>math.CO cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Sheekey constructed a new family of maximal rank distance codes as
sets of $q$-polynomials over $\mathbb{F}_{q^n}$, which are called the twisted
Gabidulin codes. We investigate a generalization of them, which we call
generalized twisted Gabidulin codes. Their Delsarte duals and adjoint codes are
considered. We also completely determine the equivalence between different
members of the generalized twisted Gabidulin codes, from which it follows that
the generalized Gabidulin codes and the twisted Gabidulin codes are both proper
subsets of this new family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07856</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07856</id><created>2015-07-28</created><authors><author><keyname>Narayanaswamy</keyname><forenames>N. S.</forenames></author><author><keyname>Rahul</keyname><forenames>C. S.</forenames></author></authors><title>A Classification of Connected f -factor Problems inside NP</title><categories>cs.CC</categories><comments>15 pages, submitted to FSTTCS 2015</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given an undirected graph G = (V, E) with n vertices, and a function f : V -&gt;
N, we consider the problem of finding a connected f -factor in G. In this work
we design an algorithm to check for the existence of a connected f -factor, for
the case where f (v) &gt;= n/g(n), for all v in V and g(n) is polylogarithmic in
n. The running time of our algorithm is O(n^{2g(n)}. As a consequence of this
algorithm we conclude that the complexity of connected f -factor for the case
we consider is unlikely to be NP-Complete unless the Exponential Time
Hypothesis (ETH) is false. Secondly, under the assumption of the ETH, we show
that it is also unlikely to be in P for g(n) in O((log n)^{1+eps} ) for any
eps&gt; 0. Therefore, our results show that for all eps&gt; 0, connected f -factor
for f (v) &gt;= n/O(log n)^{1+eps}) is in NP-Intermediate unless the ETH is false.
Further, for any constant c &gt; 0, when g(n) = c, our algorithm for connected f
-factor runs in polynomial time. Finally, we extend our algorithm to compute a
minimum weight connected f -factor in edge weighted graphs in the same
asymptotic time bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07870</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07870</id><created>2015-07-25</created><authors><author><keyname>R&#xf6;nnqvist</keyname><forenames>Samuel</forenames></author><author><keyname>Sarlin</keyname><forenames>Peter</forenames></author></authors><title>Detect &amp; Describe: Deep learning of bank stress in the news</title><categories>q-fin.CP cs.AI cs.LG cs.NE q-fin.RM</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  News is a pertinent source of information on financial risks and stress
factors, which nevertheless is challenging to harness due to the sparse and
unstructured nature of natural text. We propose an approach based on
distributional semantics and deep learning with neural networks to model and
link text to a scarce set of bank distress events. Through unsupervised
training, we learn semantic vector representations of news articles as
predictors of distress events. The predictive model that we learn can signal
coinciding stress with an aggregated index at bank or European level, while
crucially allowing for automatic extraction of text descriptions of the events,
based on passages with high stress levels. The method offers insight that
models based on other types of data cannot provide, while offering a general
means for interpreting this type of semantic-predictive model. We model bank
distress with data on 243 events and 6.6M news articles for 101 large European
banks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07872</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07872</id><created>2015-07-28</created><authors><author><keyname>Bujlow</keyname><forenames>Tomasz</forenames></author><author><keyname>Carela-Espa&#xf1;ol</keyname><forenames>Valent&#xed;n</forenames></author><author><keyname>Sol&#xe9;-Pareta</keyname><forenames>Josep</forenames></author><author><keyname>Barlet-Ros</keyname><forenames>Pere</forenames></author></authors><title>Web Tracking: Mechanisms, Implications, and Defenses</title><categories>cs.CY cs.CR</categories><comments>29 pages, 212 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This articles surveys the existing literature on the methods currently used
by web services to track the user online as well as their purposes,
implications, and possible user's defenses. A significant majority of reviewed
articles and web resources are from years 2012-2014. Privacy seems to be the
Achilles' heel of today's web. Web services make continuous efforts to obtain
as much information as they can about the things we search, the sites we visit,
the people with who we contact, and the products we buy. Tracking is usually
performed for commercial purposes. We present 5 main groups of methods used for
user tracking, which are based on sessions, client storage, client cache,
fingerprinting, or yet other approaches. A special focus is placed on
mechanisms that use web caches, operational caches, and fingerprinting, as they
are usually very rich in terms of using various creative methodologies. We also
show how the users can be identified on the web and associated with their real
names, e-mail addresses, phone numbers, or even street addresses. We show why
tracking is being used and its possible implications for the users (price
discrimination, assessing financial credibility, determining insurance
coverage, government surveillance, and identity theft). For each of the
tracking methods, we present possible defenses. Apart from describing the
methods and tools used for keeping the personal data away from being tracked,
we also present several tools that were used for research purposes - their main
goal is to discover how and by which entity the users are being tracked on
their desktop computers or smartphones, provide this information to the users,
and visualize it in an accessible and easy to follow way. Finally, we present
the currently proposed future approaches to track the user and show that they
can potentially pose significant threats to the users' privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07880</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07880</id><created>2015-07-28</created><updated>2016-02-24</updated><authors><author><keyname>Lattimore</keyname><forenames>Tor</forenames></author></authors><title>Optimally Confident UCB: Improved Regret for Finite-Armed Bandits</title><categories>cs.LG math.OC</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I present the first algorithm for stochastic finite-armed bandits that
simultaneously enjoys order-optimal problem-dependent regret and worst-case
regret. Besides the theoretical results, the new algorithm is simple, efficient
and empirically superb. The approach is based on UCB, but with a carefully
chosen confidence parameter that optimally balances the risk of failing
confidence intervals against the cost of excessive optimism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07882</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07882</id><created>2015-07-27</created><authors><author><keyname>Brahmbhatt</keyname><forenames>Samarth</forenames></author><author><keyname>Amor</keyname><forenames>Heni Ben</forenames></author><author><keyname>Christensen</keyname><forenames>Henrik</forenames></author></authors><title>Occlusion-Aware Object Localization, Segmentation and Pose Estimation</title><categories>cs.CV</categories><comments>British Machine Vision Conference 2015 (poster)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a learning approach for localization and segmentation of objects
in an image in a manner that is robust to partial occlusion. Our algorithm
produces a bounding box around the full extent of the object and labels pixels
in the interior that belong to the object. Like existing segmentation aware
detection approaches, we learn an appearance model of the object and consider
regions that do not fit this model as potential occlusions. However, in
addition to the established use of pairwise potentials for encouraging local
consistency, we use higher order potentials which capture information at the
level of im- age segments. We also propose an efficient loss function that
targets both localization and segmentation performance. Our algorithm achieves
13.52% segmentation error and 0.81 area under the false-positive per image vs.
recall curve on average over the challenging CMU Kitchen Occlusion Dataset.
This is a 42.44% decrease in segmentation error and a 16.13% increase in
localization performance compared to the state-of-the-art. Finally, we show
that the visibility labelling produced by our algorithm can make full 3D pose
estimation from a single image robust to occlusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07888</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07888</id><created>2015-07-28</created><authors><author><keyname>Nguyen</keyname><forenames>Thanh</forenames></author><author><keyname>Zhou</keyname><forenames>Hang</forenames></author><author><keyname>Berry</keyname><forenames>Randall A.</forenames></author><author><keyname>Honig</keyname><forenames>Michael L.</forenames></author><author><keyname>Vohra</keyname><forenames>Rakesh</forenames></author></authors><title>The Cost of Free Spectrum</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been growing interest in increasing the amount of radio spectrum
available for unlicensed broad-band wireless access. That includes &quot;prime&quot;
spectrum at lower frequencies, which is also suitable for wide area coverage by
licensed cellular providers. While additional unlicensed spectrum would allow
for market expansion, it could influence competition among providers and
increase congestion (interference) among consumers of wireless services. We
study the value (social welfare and consumer surplus) obtained by adding
unlicensed spectrum to an existing allocation of licensed spectrum among
incumbent service providers. We assume a population of customers who choose a
provider based on the minimum delivered price, given by the weighted sum of the
price of the service and a congestion cost, which depends on the number of
subscribers in a band. We consider models in which this weighting is uniform
across the customer population and where the weighting is either high or low,
reflecting different sensitivities to latency. For the models considered, we
find that the social welfare depends on the amount of additional unlicensed
spectrum, and can actually decrease over a significant range of unlicensed
bandwidths. Furthermore, with nonuniform weighting, introducing unlicensed
spectrum can also reduce consumer welfare.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07901</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07901</id><created>2015-07-28</created><updated>2015-12-23</updated><authors><author><keyname>Dohmatob</keyname><forenames>Elvis</forenames></author></authors><title>A simple and numerically stable primal-dual algorithm for computing
  Nash-equilibria in sequential games with incomplete information</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple primal-dual algorithm for computing approximate
Nash-equilibria in two-person zero-sum sequential games with incomplete
information and perfect recall (like Texas Hold'em Poker). Our algorithm is
numerically stable, performs only basic iterations (i.e matvec multiplications,
clipping, etc., and no calls to external first-order oracles, no matrix
inversions, etc.), and is applicable to a broad class of two-person zero-sum
games including simultaneous games and sequential games with incomplete
information and perfect recall. The applicability to the latter kind of games
is thanks to the sequence-form representation which allows us to encode any
such game as a matrix game with convex polytopial strategy profiles. We prove
that the number of iterations needed to produce a Nash-equilibrium with a given
precision is inversely proportional to the precision. As proof-of-concept, we
present experimental results on matrix games on simplexes and Kuhn Poker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07908</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07908</id><created>2015-07-24</created><authors><author><keyname>Raza</keyname><forenames>Arif</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Addressing User Requirements in Opens Source Software: The Role of
  Online Forums</title><categories>cs.SE</categories><journal-ref>Journal of Computing Science and Engineering, 8(1):57-63, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User satisfaction has always been important in the success of software,
regardless of whether it is closed and proprietary or open source software
(OSS). OSS users are geographically distributed and include technical as well
as novice users. However, it is generally believed that if OSS was more usable,
its popularity would increase tremendously. Hence, users and their requirements
need to be addressed in the priorities of an OSS environment. Online public
forums are a major medium of communication for the OSS community. The research
model of this work studies the relationship between user requirements in open
source software and online public forums. To conduct this research, we used a
dataset consisting of 100 open source software projects in different
categories. The results show that online forums play a significant role in
identifying user requirements and addressing their requests in open source
software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07909</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07909</id><created>2015-07-28</created><updated>2015-08-19</updated><authors><author><keyname>Hafemann</keyname><forenames>Luiz G.</forenames></author><author><keyname>Sabourin</keyname><forenames>Robert</forenames></author><author><keyname>Oliveira</keyname><forenames>Luiz S.</forenames></author></authors><title>Offline Handwritten Signature Verification - Literature Review</title><categories>cs.CV stat.ML</categories><acm-class>I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The area of Handwritten Signature Verification has been broadly researched in
the last decades and still remains as an open research problem. This report
focuses on offline signature verification, characterized by the usage of static
(scanned) images of signatures, where the objective is to discriminate if a
given signature is genuine (produced by the claimed individual), or a forgery
(produced by an impostor). We present an overview of how the problem has been
handled by several researchers in the past few decades and the recent
advancements in the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07911</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07911</id><created>2015-07-28</created><updated>2015-10-05</updated><authors><author><keyname>Barcelo</keyname><forenames>Pablo</forenames><affiliation>University of Chile</affiliation></author><author><keyname>Fontaine</keyname><forenames>Gaelle</forenames><affiliation>University of Chile</affiliation></author><author><keyname>Lin</keyname><forenames>Anthony Widjaja</forenames><affiliation>Yale-NUS College, Singapore</affiliation></author></authors><title>Expressive Path Queries on Graph with Data</title><categories>cs.DB cs.LO</categories><comments>39 pages</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (4:1) 2015</journal-ref><doi>10.2168/LMCS-11(4:1)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph data models have recently become popular owing to their applications,
e.g., in social networks and the semantic web. Typical navigational query
languages over graph databases - such as Conjunctive Regular Path Queries
(CRPQs) - cannot express relevant properties of the interaction between the
underlying data and the topology. Two languages have been recently proposed to
overcome this problem: walk logic (WL) and regular expressions with memory
(REM). In this paper, we begin by investigating fundamental properties of WL
and REM, i.e., complexity of evaluation problems and expressive power. We first
show that the data complexity of WL is nonelementary, which rules out its
practicality. On the other hand, while REM has low data complexity, we point
out that many natural data/topology properties of graphs expressible in WL
cannot be expressed in REM. To this end, we propose register logic, an
extension of REM, which we show to be able to express many natural graph
properties expressible in WL, while at the same time preserving the
elementariness of data complexity of REMs. It is also incomparable to WL in
terms of expressive power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07950</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07950</id><created>2015-07-28</created><authors><author><keyname>Deng</keyname><forenames>Xinyang</forenames></author><author><keyname>Wang</keyname><forenames>Zhen</forenames></author><author><keyname>Liu</keyname><forenames>Qi</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author><author><keyname>Shyr</keyname><forenames>Yu</forenames></author></authors><title>Impact of Preference and Equivocators on Opinion Dynamics with
  Evolutionary Game Framework</title><categories>cs.GT cs.SI physics.soc-ph</categories><comments>15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opinion dynamics, aiming to understand the evolution of collective behavior
through various interaction mechanisms of opinions, represents one of the most
challenges in natural and social science. To elucidate this issue clearly,
binary opinion model becomes a useful framework, where agents can take an
independent opinion. Inspired by the realistic observations, here we propose
two basic interaction mechanisms of binary opinion model: one is the so-called
BSO model in which players benefit from holding the same opinion; the other is
called BDO model in which players benefit from taking different opinions. In
terms of these two basic models, the synthetical effect of opinion preference
and equivocators on the evolution of binary opinion is studied under the
framework of evolutionary game theory (EGT), where the replicator equation (RE)
is employed to mimick the evolution of opinions. By means of numerous
simulations, we show the theoretical equilibrium states of binary opinion
dynamics, and mathematically analyze the stability of each equilibrium state as
well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07951</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07951</id><created>2015-07-28</created><authors><author><keyname>Deng</keyname><forenames>Xinyang</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author><author><keyname>Liu</keyname><forenames>Qi</forenames></author><author><keyname>Wang</keyname><forenames>Zhen</forenames></author></authors><title>A quantum extension to inspection game</title><categories>cs.GT</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum game theory is a new interdisciplinary field between game theory and
physical research. In this paper, we extend the classical inspection game into
a quantum game version by quantizing the strategy space and importing
entanglement between players. Our result shows that the quantum inspection game
has various Nash equilibrium depending on the initial quantum state of the
game. It is also shown that quantization can respectively help each player to
increase his own payoff, yet fails to bring Pareto improvement for the
collective payoff in the quantum inspection game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07955</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07955</id><created>2015-07-28</created><authors><author><keyname>Heim</keyname><forenames>Eric</forenames><affiliation>University of Pittsburgh</affiliation></author><author><keyname>Hauskrecht</keyname><forenames>Milos</forenames><affiliation>University of Pittsburgh</affiliation></author></authors><title>Sparse Multidimensional Patient Modeling using Auxiliary Confidence
  Labels</title><categories>cs.LG</categories><comments>Currently under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we focus on the problem of learning a classification model that
performs inference on patient Electronic Health Records (EHRs). Often, a large
amount of costly expert supervision is required to learn such a model. To
reduce this cost, we obtain confidence labels that indicate how sure an expert
is in the class labels she provides. If meaningful confidence information can
be incorporated into a learning method, fewer patient instances may need to be
labeled to learn an accurate model. In addition, while accuracy of predictions
is important for any inference model, a model of patients must be interpretable
so that clinicians can understand how the model is making decisions. To these
ends, we develop a novel metric learning method called Confidence bAsed MEtric
Learning (CAMEL) that supports inclusion of confidence labels, but also
emphasizes interpretability in three ways. First, our method induces sparsity,
thus producing simple models that use only a few features from patient EHRs.
Second, CAMEL naturally produces confidence scores that can be taken into
consideration when clinicians make treatment decisions. Third, the metrics
learned by CAMEL induce multidimensional spaces where each dimension represents
a different &quot;factor&quot; that clinicians can use to assess patients. In our
experimental evaluation, we show on a real-world clinical data set that our
CAMEL methods are able to learn models that are as or more accurate as other
methods that use the same supervision. Furthermore, we show that when CAMEL
uses confidence scores it is able to learn models as or more accurate as others
we tested while using only 10% of the training instances. Finally, we perform
qualitative assessments on the metrics learned by CAMEL and show that they
identify and clearly articulate important factors in how the model performs
inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07966</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07966</id><created>2015-07-28</created><authors><author><keyname>Deng</keyname><forenames>Xinyang</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author><author><keyname>Liu</keyname><forenames>Qi</forenames></author><author><keyname>Wang</keyname><forenames>Zhen</forenames></author></authors><title>Quantum games of opinion formation based on the Marinatto-Weber quantum
  game scheme</title><categories>cs.GT</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantization becomes a new way to study classical game theory since quantum
strategies and quantum games have been proposed. In previous studies, many
typical game models, such as prisoner's dilemma, battle of the sexes, Hawk-Dove
game, have been investigated by using quantization approaches. In this paper,
several game models of opinion formations have been quantized based on the
Marinatto-Weber quantum game scheme, a frequently used scheme to convert
classical games to quantum versions. Our results show that the quantization can
change fascinatingly the properties of some classical opinion formation game
models so as to generate win-win outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07969</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07969</id><created>2015-07-28</created><updated>2015-08-03</updated><authors><author><keyname>de Carvalho</keyname><forenames>Rogerio Atem</forenames></author><author><keyname>Silva</keyname><forenames>Hudson</forenames></author><author><keyname>Toledo</keyname><forenames>Rafael Ferreira</forenames></author><author><keyname>de Azevedo</keyname><forenames>Milena Silveira</forenames></author></authors><title>TDD for Embedded Systems: A Basic Approach and Toolset</title><categories>cs.SE</categories><comments>07 pages, 05 figures, work in progress</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The evolution of information technology and electronics in general has been
consistently increasing the use of embedded systems. While hardware development
for these systems is already consistent, software development for embedded
systems still lacks a consolidated methodology. This paper describes a process
and toolset for Embedded Systems Validation and Verification using FSM (Finite
State Machines) and TDD (Test Driven Development).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07974</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07974</id><created>2015-07-28</created><authors><author><keyname>Pothier</keyname><forenames>John</forenames></author><author><keyname>Girson</keyname><forenames>Josh</forenames></author><author><keyname>Aeron</keyname><forenames>Shuchin</forenames></author></authors><title>An algorithm for online tensor prediction</title><categories>stat.ML cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method for online prediction and learning of tensors
($N$-way arrays, $N &gt;2$) from sequential measurements. We focus on the specific
case of 3-D tensors and exploit a recently developed framework of structured
tensor decompositions proposed in [1]. In this framework it is possible to
treat 3-D tensors as linear operators and appropriately generalize notions of
rank and positive definiteness to tensors in a natural way. Using these notions
we propose a generalization of the matrix exponentiated gradient descent
algorithm [2] to a tensor exponentiated gradient descent algorithm using an
extension of the notion of von-Neumann divergence to tensors. Then following a
similar construction as in [3], we exploit this algorithm to propose an online
algorithm for learning and prediction of tensors with provable regret
guarantees. Simulations results are presented on semi-synthetic data sets of
ratings evolving in time under local influence over a social network. The
result indicate superior performance compared to other (online) convex tensor
completion methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07984</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07984</id><created>2015-07-28</created><authors><author><keyname>A.</keyname><forenames>Prashanth L.</forenames></author><author><keyname>Prasad</keyname><forenames>H. L.</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author><author><keyname>Chandra</keyname><forenames>Prakash</forenames></author></authors><title>A constrained optimization perspective on actor critic algorithms and
  application to network routing</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel actor-critic algorithm with guaranteed convergence to an
optimal policy for a discounted reward Markov decision process. The actor
incorporates a descent direction that is motivated by the solution of a certain
non-linear optimization problem. We also discuss an extension to incorporate
function approximation and demonstrate the practicality of our algorithms on a
network routing application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07994</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07994</id><created>2015-07-28</created><authors><author><keyname>Tushar</keyname><forenames>Wayes</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Huang</keyname><forenames>Shisheng</forenames></author><author><keyname>Smith</keyname><forenames>David</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Cost Minimization of Charging Stations with Photovoltaics: An Approach
  with EV Classification</title><categories>cs.SY</categories><comments>Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel electric vehicle (EV) classification scheme for a
photovoltaic (PV) powered EV charging station (CS) that reduces the effect of
intermittency of electricity supply as well as reducing the cost of energy
trading of the CS. Since not all EV drivers would like to be environmentally
friendly, all vehicles in the CS are divided into three categories: 1) premium,
2) conservative, and 3) green, according to their charging behavior. Premium
and conservative EVs are considered to be interested only in charging their
batteries, with noticeably higher rate of charging for premium EVs. Green
vehicles are more environmentally friendly, and thus assist the CS to reduce
its cost of energy trading by allowing the CS to use their batteries as
distributed storage. A different charging scheme is proposed for each type of
EV, which is adopted by the CS to encourage more EVs to be green. A basic mixed
integer programming (MIP) technique is used to facilitate the proposed
classification scheme. It is shown that the uncertainty in PV generation can be
effectively compensated, along with minimization of total cost of energy
trading to the CS, by consolidating more green EVs. Real solar and pricing data
are used for performance analysis of the system. It is demonstrated that the
total cost to the CS reduces considerably as the percentage of green vehicles
increases, and also that the contributions of green EVs in winter are greater
than those in summer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.07998</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.07998</id><created>2015-07-28</created><authors><author><keyname>Dai</keyname><forenames>Andrew M.</forenames></author><author><keyname>Olah</keyname><forenames>Christopher</forenames></author><author><keyname>Le</keyname><forenames>Quoc V.</forenames></author></authors><title>Document Embedding with Paragraph Vectors</title><categories>cs.CL cs.AI cs.LG</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Paragraph Vectors has been recently proposed as an unsupervised method for
learning distributed representations for pieces of texts. In their work, the
authors showed that the method can learn an embedding of movie review texts
which can be leveraged for sentiment analysis. That proof of concept, while
encouraging, was rather narrow. Here we consider tasks other than sentiment
analysis, provide a more thorough comparison of Paragraph Vectors to other
document modelling algorithms such as Latent Dirichlet Allocation, and evaluate
performance of the method as we vary the dimensionality of the learned
representation. We benchmarked the models on two document similarity data sets,
one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method
performs significantly better than other methods, and propose a simple
improvement to enhance embedding quality. Somewhat surprisingly, we also show
that much like word embeddings, vector operations on Paragraph Vectors can
perform useful semantic results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08007</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08007</id><created>2015-07-28</created><authors><author><keyname>Eremeev</keyname><forenames>Anton</forenames></author></authors><title>On Proportions of Fit Individuals in Population of Genetic Algorithm
  with Tournament Selection</title><categories>cs.NE</categories><comments>Submited to Theoretical Computer Science journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a fitness-level model of a non-elitist
mutation-only genetic algorithm (GA) with tournament selection. The model
provides upper and lower bounds for the expected proportion of the individuals
with fitness above given thresholds. In the case of GA with bitwise mutation
and OneMax fitness function, the lower bounds are tight when population size
equals one, while the upper bounds are asymptotically tight when population
size tends to infinity.
  The lower bounds on expected proportions of sufficiently fit individuals may
be obtained from the probability distribution of an appropriate Markov chain.
This approach yields polynomial upper bounds on the runtime of an Iterated
version of the GA on 2-SAT problem and on a family of Set Cover problems
proposed by E. Balas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08010</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08010</id><created>2015-07-28</created><updated>2015-10-27</updated><authors><author><keyname>Zhou</keyname><forenames>Zhiyi</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author><author><keyname>Honig</keyname><forenames>Michael L.</forenames></author></authors><title>Allocation of Licensed and Unlicensed Spectrum in Heterogeneous Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the traffic load in cellular networks accelerates, emerging services will
make use of multiple radio access technologies (RATs) over both licensed and
unlicensed frequency bands. This paper studies the spectrum allocation problem
in such heterogeneous networks (HetNets) on a relatively slow timescale, so
that joint allocation across many cells is practical. A queueing model is
introduced for the unlicensed band to capture its lower spectral efficiency,
reliability, and additional delay due to contention and/or listen-before-talk
requirements in contrast to licensed bands. Under mild assumptions, the
spectrum allocation problem is formulated as a bi-convex optimization problem.
Solving this problem gives an effective and computationally efficient solution
for both user association and spectrum allocation over multiple RATs.
Simulation results show that in the heavy-traffic regime, the proposed scheme
significantly outperforms both orthogonal and full-frequency-reuse allocations.
In addition, the solution to the optimization problem matches the intuition
that users with relatively high traffic demand are mostly assigned to the
licensed spectrum, while those with low traffic demand and less exogenous
interference from the unlicensed band are assigned to the unlicensed spectrum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08015</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08015</id><created>2015-07-29</created><authors><author><keyname>Steinfeld</keyname><forenames>Ron</forenames></author><author><keyname>Sakzad</keyname><forenames>Amin</forenames></author></authors><title>On Massive MIMO Physical Layer Cryptosystem</title><categories>cs.IT math.IT</categories><comments>To be presented at ITW 2015, Jeju Island, South Korea. 6 Pages, 1
  Figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a zero-forcing (ZF) attack on the physical layer
cryptography scheme based on massive multiple-input multiple-output (MIMO). The
scheme uses singular value decomposition (SVD) precoder. We show that the
eavesdropper can decrypt/decode the information data under the same condition
as the legitimate receiver. We then study the advantage for decoding by the
legitimate user over the eavesdropper in a generalized scheme using an
arbitrary precoder at the transmitter. On the negative side, we show that if
the eavesdropper uses a number of receive antennas much larger than the number
of legitimate user antennas, then there is no advantage, independent of the
precoding scheme employed at the transmitter. On the positive side, for the
case where the adversary is limited to have the same number of antennas as
legitimate users, we give an $\mathcal{O}\left(n^2\right)$ upper bound on the
advantage and show that this bound can be approached using an inverse precoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08030</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08030</id><created>2015-07-29</created><authors><author><keyname>Cazasnoves</keyname><forenames>Anthony</forenames></author><author><keyname>Buyens</keyname><forenames>Fanny</forenames></author><author><keyname>Sevestre</keyname><forenames>Sylvie</forenames></author></authors><title>Adapted sampling for 3D X-ray computed tomography</title><categories>cs.CV</categories><comments>The 13th International Meeting on Fully Three-Dimensional Image
  Reconstruction in Radiology and Nuclear Medicine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a method to build an adapted mesh representation
of a 3D object for X-Ray tomography reconstruction. Using this representation,
we provide means to reduce the computational cost of reconstruction by way of
iterative algorithms. The adapted sampling of the reconstruction space is
directly obtained from the projection dataset and prior to any reconstruction.
It is built following two stages : firstly, 2D structural information is
extracted from the projection images and is secondly merged in 3D to obtain a
3D pointcloud sampling the interfaces of the object. A relevant mesh is then
built from this cloud by way of tetrahedralization. Critical parameters
selections have been automatized through a statistical framework, thus avoiding
dependence on users expertise. Applying this approach on geometrical shapes and
on a 3D Shepp-Logan phantom, we show the relevance of such a sampling -
obtained in a few seconds - and the drastic decrease in cells number to be
estimated during reconstruction when compared to the usual regular voxel
lattice. A first iterative reconstruction of the Shepp-Logan using this kind of
sampling shows the relevant advantages in terms of low dose or sparse
acquisition sampling contexts. The method can also prove useful for other
applications such as finite element method computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08032</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08032</id><created>2015-07-29</created><authors><author><keyname>Dabbene</keyname><forenames>Fabrizio</forenames><affiliation>LAAS</affiliation></author><author><keyname>Henrion</keyname><forenames>Didier</forenames><affiliation>LAAS</affiliation></author><author><keyname>Lagoa</keyname><forenames>Constantino</forenames><affiliation>ICS</affiliation></author><author><keyname>Shcherbakov</keyname><forenames>Pavel</forenames><affiliation>ICS</affiliation></author></authors><title>Randomized Approximations of the Image Set of Nonlinear Mappings with
  Applications to Filtering</title><categories>math.OC cs.SY</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is twofold: In the first part, we leverage recent
results on scenario design to develop randomized algorithmsfor approximating
the image set of a nonlinear mapping, that is, a (possibly noisy) mapping of a
set via a nonlinear function.We introduce minimum-volume approximations which
have the characteristic of guaranteeing a low probability of violation, i.e.,we
admit for a probability that some points in the image set are not contained in
the approximating set,but this probability is kept below a pre-specified
threshold.In the second part of the paper, this idea is then exploited to
develop a new family of randomized prediction-corrector filters.These filters
represent a natural extension and rapprochement of Gaussian and set-valued
filters,and bear similarities with modern tools such as particle filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08036</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08036</id><created>2015-07-29</created><authors><author><keyname>Alkasem</keyname><forenames>Ameen</forenames></author><author><keyname>Liu</keyname><forenames>Hongwei</forenames></author><author><keyname>Decheng</keyname><forenames>Zuo</forenames></author><author><keyname>Zhao</keyname><forenames>Yao</forenames></author></authors><title>AFDI: A Virtualization-based Accelerated Fault Diagnosis Innovation for
  High Availability Computing</title><categories>cs.SE</categories><comments>8 pages, 14 figures</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Volume 12,
  Issue 3, May 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fault diagnosis has attracted extensive attention for its importance in the
exceedingly fault management framework for cloud virtualization, despite the
fact that fault diagnosis becomes more difficult due to the increasing
scalability and complexity in a heterogeneous environment for a virtualization
technique. Most existing fault diagnoses methods are based on active probing
techniques which can be used to detect the faults rapidly and precisely.
However, most of those methods suffer from the limitation of traffic overhead
and diagnosis of faults, which leads to a reduction in system performance. In
this paper, we propose a new hybrid model named accelerated fault diagnosis
invention (AFDI) to monitor various system metrics for VMs and physical server
hosting, such as CPU, memory, and network usages based on the severity of fault
levels and anomalies. The proposed method takes the advantages of the
multi-valued decision diagram (MDD), A Naive Bayes Classifier (NBC) models and
virtual sensors cloud to achieve high availability for cloud services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08037</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08037</id><created>2015-07-29</created><authors><author><keyname>Tahri</keyname><forenames>Amal</forenames><affiliation>SPIRALS</affiliation></author><author><keyname>Duchien</keyname><forenames>Laurence</forenames><affiliation>SPIRALS</affiliation></author><author><keyname>Pulou</keyname><forenames>Jacques</forenames></author></authors><title>Using Feature Models for Distributed Deployment in Extended Smart Home
  Architecture</title><categories>cs.SE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, smart home is extended beyond the house itself to encompass
connected platforms on the Cloud as well as mobile personal devices. This Smart
Home Extended Architecture (SHEA) helps customers to remain in touch with their
home everywhere and any time. The endless increase of connected devices in the
home and outside within the SHEA multiplies the deployment possibilities for
any application. Therefore, SHEA should be taken from now as the actual target
platform for smart home application deployment. Every home is different and
applications offer different services according to customer preferences. To
manage this variability, we extend the feature modeling from software product
line domain with deployment constraints and we present an example of a model
that could address this deployment challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08042</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08042</id><created>2015-07-29</created><authors><author><keyname>Fu</keyname><forenames>Hu</forenames></author><author><keyname>Immolica</keyname><forenames>Nicole</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author><author><keyname>Strack</keyname><forenames>Philipp</forenames></author></authors><title>Randomization beats Second Price as a Prior-Independent Auction</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing revenue optimal auctions for selling an item to $n$ symmetric
bidders is a fundamental problem in mechanism design. Myerson (1981) shows that
the second price auction with an appropriate reserve price is optimal when
bidders' values are drawn i.i.d. from a known regular distribution. A
cornerstone in the prior-independent revenue maximization literature is a
result by Bulow and Klemperer (1996) showing that the second price auction
without a reserve achieves $(n-1)/n$ of the optimal revenue in the worst case.
  We construct a randomized mechanism that strictly outperforms the second
price auction in this setting. Our mechanism inflates the second highest bid
with a probability that varies with $n$. For two bidders we improve the
performance guarantee from $0.5$ to $0.512$ of the optimal revenue. We also
resolve a question in the design of revenue optimal mechanisms that have access
to a single sample from an unknown distribution. We show that a randomized
mechanism strictly outperforms all deterministic mechanisms in terms of worst
case guarantee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08051</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08051</id><created>2015-07-29</created><authors><author><keyname>Honsell</keyname><forenames>Furio</forenames><affiliation>University of Udine, Italy</affiliation></author><author><keyname>Liquori</keyname><forenames>Luigi</forenames><affiliation>Inria Sophia Antipolis M&#xe9;diterran&#xe9;e, France</affiliation></author><author><keyname>Maksimovi&#x107;</keyname><forenames>Petar</forenames><affiliation>Inria Rennes Bretagne Atlantique, France</affiliation></author><author><keyname>Scagnetto</keyname><forenames>Ivan</forenames><affiliation>University of Udine, Italy</affiliation></author></authors><title>Gluing together Proof Environments: Canonical extensions of LF Type
  Theories featuring Locks</title><categories>cs.LO</categories><comments>In Proceedings LFMTP 2015, arXiv:1507.07597</comments><proxy>EPTCS</proxy><acm-class>D.2.4; F.3.1; F.4.1</acm-class><journal-ref>EPTCS 185, 2015, pp. 3-17</journal-ref><doi>10.4204/EPTCS.185.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two extensions of the LF Constructive Type Theory featuring
monadic locks. A lock is a monadic type construct that captures the effect of
an external call to an oracle. Such calls are the basic tool for gluing
together diverse Type Theories and proof development environments. The oracle
can be invoked either to check that a constraint holds or to provide a suitable
witness. The systems are presented in the canonical style developed by the CMU
School. The first system, CLLFP, is the canonical version of the system LLFP,
presented earlier by the authors. The second system, CLLFP?, features the
possibility of invoking the oracle to obtain a witness satisfying a given
constraint. We discuss encodings of Fitch-Prawitz Set theory, call-by-value
lambda-calculi, and systems of Light Linear Logic. Finally, we show how to use
Fitch-Prawitz Set Theory to define a type system that types precisely the
strongly normalizing terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08052</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08052</id><created>2015-07-29</created><authors><author><keyname>Felty</keyname><forenames>Amy</forenames><affiliation>University of Ottawa</affiliation></author><author><keyname>Momigliano</keyname><forenames>Alberto</forenames><affiliation>Universita degli Studi di Milano</affiliation></author><author><keyname>Pientka</keyname><forenames>Brigitte</forenames><affiliation>McGill University</affiliation></author></authors><title>An Open Challenge Problem Repository for Systems Supporting Binders</title><categories>cs.LO cs.PL</categories><comments>In Proceedings LFMTP 2015, arXiv:1507.07597</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 185, 2015, pp. 18-32</journal-ref><doi>10.4204/EPTCS.185.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variety of logical frameworks support the use of higher-order abstract
syntax in representing formal systems; however, each system has its own set of
benchmarks. Even worse, general proof assistants that provide special libraries
for dealing with binders offer a very limited evaluation of such libraries, and
the examples given often do not exercise and stress-test key aspects that arise
in the presence of binders. In this paper we design an open repository ORBI
(Open challenge problem Repository for systems supporting reasoning with
BInders). We believe the field of reasoning about languages with binders has
matured, and a common set of benchmarks provides an important basis for
evaluation and qualitative comparison of different systems and libraries that
support binders, and it will help to advance the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08053</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08053</id><created>2015-07-29</created><authors><author><keyname>Cave</keyname><forenames>Andrew</forenames><affiliation>McGill University</affiliation></author><author><keyname>Pientka</keyname><forenames>Brigitte</forenames><affiliation>McGill University</affiliation></author></authors><title>A Case Study on Logical Relations using Contextual Types</title><categories>cs.LO cs.PL</categories><comments>In Proceedings LFMTP 2015, arXiv:1507.07597</comments><proxy>EPTCS</proxy><acm-class>F3.1</acm-class><journal-ref>EPTCS 185, 2015, pp. 33-45</journal-ref><doi>10.4204/EPTCS.185.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proofs by logical relations play a key role to establish rich properties such
as normalization or contextual equivalence. They are also challenging to
mechanize. In this paper, we describe the completeness proof of algorithmic
equality for simply typed lambda-terms by Crary where we reason about logically
equivalent terms in the proof environment Beluga. There are three key aspects
we rely upon: 1) we encode lambda-terms together with their operational
semantics and algorithmic equality using higher-order abstract syntax 2) we
directly encode the corresponding logical equivalence of well-typed
lambda-terms using recursive types and higher-order functions 3) we exploit
Beluga's support for contexts and the equational theory of simultaneous
substitutions. This leads to a direct and compact mechanization, demonstrating
Beluga's strength at formalizing logical relations proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08054</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08054</id><created>2015-07-29</created><authors><author><keyname>Perera</keyname><forenames>Roly</forenames><affiliation>University of Glasgow, UK</affiliation></author><author><keyname>Cheney</keyname><forenames>James</forenames><affiliation>University of Edinburgh, UK</affiliation></author></authors><title>Proof-relevant pi-calculus</title><categories>cs.LO</categories><comments>In Proceedings LFMTP 2015, arXiv:1507.07597</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 185, 2015, pp. 46-70</journal-ref><doi>10.4204/EPTCS.185.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formalising the pi-calculus is an illuminating test of the expressiveness of
logical frameworks and mechanised metatheory systems, because of the presence
of name binding, labelled transitions with name extrusion, bisimulation, and
structural congruence. Formalisations have been undertaken in a variety of
systems, primarily focusing on well-studied (and challenging) properties such
as the theory of process bisimulation. We present a formalisation in Agda that
instead explores the theory of concurrent transitions, residuation, and causal
equivalence of traces, which has not previously been formalised for the
pi-calculus. Our formalisation employs de Bruijn indices and dependently-typed
syntax, and aligns the &quot;proved transitions&quot; proposed by Boudol and Castellani
in the context of CCS with the proof terms naturally present in Agda's
representation of the labelled transition relation. Our main contributions are
proofs of the &quot;diamond lemma&quot; for residuation of concurrent transitions and a
formal definition of equivalence of traces up to permutation of transitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08055</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08055</id><created>2015-07-29</created><authors><author><keyname>Saillard</keyname><forenames>Ronan</forenames><affiliation>MINES ParisTech, PSL Research University, France</affiliation></author></authors><title>Rewriting Modulo \beta in the \lambda\Pi-Calculus Modulo</title><categories>cs.LO</categories><comments>In Proceedings LFMTP 2015, arXiv:1507.07597</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 185, 2015, pp. 87-101</journal-ref><doi>10.4204/EPTCS.185.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The lambda-Pi-calculus Modulo is a variant of the lambda-calculus with
dependent types where beta-conversion is extended with user-defined rewrite
rules. It is an expressive logical framework and has been used to encode logics
and type systems in a shallow way. Basic properties such as subject reduction
or uniqueness of types do not hold in general in the lambda-Pi-calculus Modulo.
However, they hold if the rewrite system generated by the rewrite rules
together with beta-reduction is confluent. But this is too restrictive. To
handle the case where non confluence comes from the interference between the
beta-reduction and rewrite rules with lambda-abstraction on their left-hand
side, we introduce a notion of rewriting modulo beta for the lambda-Pi-calculus
Modulo. We prove that confluence of rewriting modulo beta is enough to ensure
subject reduction and uniqueness of types. We achieve our goal by encoding the
lambda-Pi-calculus Modulo into Higher-Order Rewrite System (HRS). As a
consequence, we also make the confluence results for HRSs available for the
lambda-Pi-calculus Modulo.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08056</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08056</id><created>2015-07-29</created><authors><author><keyname>Guenot</keyname><forenames>Nicolas</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Gustafsson</keyname><forenames>Daniel</forenames><affiliation>IT University of Copenhagen</affiliation></author></authors><title>Sequent Calculus and Equational Programming</title><categories>cs.LO</categories><comments>In Proceedings LFMTP 2015, arXiv:1507.07597</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 185, 2015, pp. 102-109</journal-ref><doi>10.4204/EPTCS.185.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proof assistants and programming languages based on type theories usually
come in two flavours: one is based on the standard natural deduction
presentation of type theory and involves eliminators, while the other provides
a syntax in equational style. We show here that the equational approach
corresponds to the use of a focused presentation of a type theory expressed as
a sequent calculus. A typed functional language is presented, based on a
sequent calculus, that we relate to the syntax and internal language of Agda.
In particular, we discuss the use of patterns and case splittings, as well as
rules implementing inductive reasoning and dependent products and sums.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08064</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08064</id><created>2015-07-29</created><authors><author><keyname>Qu</keyname><forenames>Xiaochao</forenames></author><author><keyname>Kim</keyname><forenames>Suah</forenames></author><author><keyname>Cui</keyname><forenames>Run</forenames></author><author><keyname>Kim</keyname><forenames>Hyoung Joong</forenames></author></authors><title>Collaborative Representation Classification Ensemble for Face
  Recognition</title><categories>cs.CV</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative Representation Classification (CRC) for face recognition
attracts a lot attention recently due to its good recognition performance and
fast speed. Compared to Sparse Representation Classification (SRC), CRC
achieves a comparable recognition performance with 10-1000 times faster speed.
In this paper, we propose to ensemble several CRC models to promote the
recognition rate, where each CRC model uses different and divergent randomly
generated biologically-inspired features as the face representation. The
proposed ensemble algorithm calculates an ensemble weight for each CRC model
that guided by the underlying classification rule of CRC. The obtained weights
reflect the confidences of those CRC models where the more confident CRC models
have larger weights. The proposed weighted ensemble method proves to be very
effective and improves the performance of each CRC model significantly.
Extensive experiments are conducted to show the superior performance of the
proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08071</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08071</id><created>2015-07-29</created><authors><author><keyname>Valovich</keyname><forenames>Filipp</forenames></author><author><keyname>Ald&#xe0;</keyname><forenames>Francesco</forenames></author></authors><title>Private Stream Aggregation Revisited</title><categories>cs.CR</categories><comments>33 pages, 2 tables, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we investigate the problem of private statistical analysis in
the distributed and semi-honest setting. In particular, we study properties of
Private Stream Aggregation schemes, first introduced by Shi et al. \cite{2}.
These are computationally secure protocols for the aggregation of data in a
network and have a very small communication cost. We show that such schemes can
be built upon any key-homomorphic \textit{weak} pseudo-random function. Thus,
in contrast to the aforementioned work, our security definition can be achieved
in the \textit{standard model}. In addition, we give a computationally
efficient instantiation of this protocol based on the Decisional Diffie-Hellman
problem. Moreover, we show that every mechanism which preserves
$(\epsilon,\delta)$-differential privacy provides \textit{computational}
$(\epsilon,\delta)$-differential privacy when it is executed through a Private
Stream Aggregation scheme. Finally, we introduce a novel perturbation mechanism
based on the \textit{Skellam distribution} that is suited for the distributed
setting, and compare its performances with those of previous solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08072</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08072</id><created>2015-07-29</created><updated>2016-01-11</updated><authors><author><keyname>Cimini</keyname><forenames>Giulio</forenames></author><author><keyname>Zaccaria</keyname><forenames>Andrea</forenames></author><author><keyname>Gabrielli</keyname><forenames>Andrea</forenames></author></authors><title>Investigating the interplay between fundamentals of national research
  systems: performance, investments and international collaborations</title><categories>physics.soc-ph cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss, at the macro-level of nations, the contribution of research
funding and rate of international collaboration to research performance, with
important implications for the science of science policy. In particular, we
cross-correlate suitable measures of these quantities with a
scientometric-based assessment of scientific success, studying both the average
performance of nations and their temporal dynamics in the space defined by
these variables during the last decade. We find significant differences among
nations in terms of efficiency in turning (financial) input into
bibliometrically measurable output, and we confirm that growth of international
collaboration positively correlate with scientific success, with significant
benefits brought by EU integration policies. Various geo-cultural clusters of
nations naturally emerge from our analysis. We critically discuss the possible
factors that potentially determine the observed patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08073</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08073</id><created>2015-07-29</created><updated>2015-12-03</updated><authors><author><keyname>Yu</keyname><forenames>Jian</forenames></author></authors><title>Communication: Words and Conceptual Systems</title><categories>cs.AI</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Words (phrases or symbols) play a key role in human life. Word (phrase or
symbol) representation is the fundamental problem for knowledge representation
and understanding. A word (phrase or symbol) usually represents a name of a
category. However, it is always a challenge that how to represent a category
can make it easily understood. In this paper, a new representation for a
category is discussed, which can be considered a generalization of classic set.
In order to reduce representation complexity, the economy principle of category
representation is proposed. The proposed category representation provides a
powerful tool for analyzing conceptual systems, relations between words,
communication, knowledge, situations. More specifically, the conceptual system,
word relations and communication are mathematically defined and classified such
as ideal conceptual system, perfect communication and so on; relation between
words and sentences is also studied, which shows that knowledge are words.
Furthermore, how conceptual systems and words depend on situations is
presented, and how truth is defined is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08074</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08074</id><created>2015-07-29</created><authors><author><keyname>Novoselov</keyname><forenames>Sergey</forenames></author><author><keyname>Kozlov</keyname><forenames>Alexandr</forenames></author><author><keyname>Lavrentyeva</keyname><forenames>Galina</forenames></author><author><keyname>Simonchik</keyname><forenames>Konstantin</forenames></author><author><keyname>Shchemelinin</keyname><forenames>Vadim</forenames></author></authors><title>STC Anti-spoofing Systems for the ASVspoof 2015 Challenge</title><categories>cs.SD cs.LG stat.ML</categories><comments>5 pages, 8 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the Speech Technology Center (STC) systems submitted to
Automatic Speaker Verification Spoofing and Countermeasures (ASVspoof)
Challenge 2015. In this work we investigate different acoustic feature spaces
to determine reliable and robust countermeasures against spoofing attacks. In
addition to the commonly used front-end MFCC features we explored features
derived from phase spectrum and features based on applying the multiresolution
wavelet transform. Similar to state-of-the-art ASV systems, we used the
standard TV-JFA approach for probability modelling in spoofing detection
systems. Experiments performed on the development and evaluation datasets of
the Challenge demonstrate that the use of phase-related and wavelet-based
features provides a substantial input into the efficiency of the resulting STC
systems. In our research we also focused on the comparison of the linear (SVM)
and nonlinear (DBN) classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08075</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08075</id><created>2015-07-29</created><authors><author><keyname>Qu</keyname><forenames>Xiaochao</forenames></author><author><keyname>Kim</keyname><forenames>Suah</forenames></author><author><keyname>Cui</keyname><forenames>Run</forenames></author><author><keyname>Kim</keyname><forenames>Hyoung Joong</forenames></author></authors><title>Low Bit-Rate and High Fidelity Reversible Data Hiding</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An accurate predictor is crucial for histogram-shifting (HS) based reversible
data hiding methods. The embedding capacity is increased and the embedding
distortion is decreased simultaneously if the predictor can generate accurate
predictions. In this paper, we propose an accurate linear predictor based on
weighted least squares (WLS) estimation. The robustness of WLS helps the
proposed predictor generate accurate predictions, especially in complex texture
areas of an image, where other predictors usually fail. To further reduce the
embedding distortion, we propose a new embedding method called dynamic
histogram shifting with pixel selection (DHS-PS) that selects not only the
proper histogram bins but also the proper pixel locations to embed the given
data. As a result, the proposed method can obtain very high fidelity marked
images with low bit-rate data embedded. The experimental results show that the
proposed method outperforms the state-of-the-art low bit-rate reversible data
hiding method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08076</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08076</id><created>2015-07-29</created><authors><author><keyname>Li</keyname><forenames>Annan</forenames></author><author><keyname>Shan</keyname><forenames>Shiguang</forenames></author><author><keyname>Chen</keyname><forenames>Xilin</forenames></author><author><keyname>Ma</keyname><forenames>Bingpeng</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author><author><keyname>Gao</keyname><forenames>Wen</forenames></author></authors><title>Cross-pose Face Recognition by Canonical Correlation Analysis</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The pose problem is one of the bottlenecks in automatic face recognition. We
argue that one of the diffculties in this problem is the severe misalignment in
face images or feature vectors with different poses. In this paper, we propose
that this problem can be statistically solved or at least mitigated by
maximizing the intra-subject across-pose correlations via canonical correlation
analysis (CCA). In our method, based on the data set with coupled face images
of the same identities and across two different poses, CCA learns
simultaneously two linear transforms, each for one pose. In the transformed
subspace, the intra-subject correlations between the different poses are
maximized, which implies pose-invariance or pose-robustness is achieved. The
experimental results show that our approach could considerably improve the
recognition performance. And if further enhanced with holistic+local feature
representation, the performance could be comparable to the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08080</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08080</id><created>2015-07-29</created><authors><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Kusters</keyname><forenames>Vincent</forenames></author><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Pilz</keyname><forenames>Alexander</forenames></author><author><keyname>Wettstein</keyname><forenames>Manuel</forenames></author></authors><title>An Optimal Algorithm for Reconstructing Point Set Order Types from
  Radial Orderings</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $P$ of $n$ labeled points in the plane, the radial system of $P$
describes, for each $p\in P$, the radial order of the other points around $p$.
This notion is related to the order type of $P$, which describes the
orientation (clockwise or counterclockwise) of every ordered triple of $P$.
Given only the order type of $P$, it is easy to reconstruct the radial system
of $P$, but the converse is not true. Aichholzer et al. (&quot;Reconstructing Point
Set Order Types from Radial Orderings&quot;, in Proc. ISAAC 2014) defined $T(R)$ to
be the set of order types with radial system $R$ and showed that sometimes
$|T(R)|=n-1$. They give polynomial-time algorithms to compute $T(R)$ when only
given $R$.
  We describe an optimal $O(kn^2)$ time algorithm for computing $T(R)$, where
$k$ is the number of order types reported by the algorithm. The reporting
relies on constructing the convex hulls of all possible point sets with the
given radial system, after which sidedness queries on point triples can be
answered in constant time. This set of convex hulls can be constructed in
linear time. Our results generalize to abstract order types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08082</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08082</id><created>2015-07-29</created><authors><author><keyname>Tascikaraoglu</keyname><forenames>Fatma Yildiz</forenames></author><author><keyname>Lioris</keyname><forenames>Jennie</forenames></author><author><keyname>Muralidharan</keyname><forenames>Ajith</forenames></author><author><keyname>Gouy</keyname><forenames>Martin</forenames></author><author><keyname>Varaiya</keyname><forenames>Pravin</forenames></author></authors><title>PointQ model of an arterial network: calibration and experiments</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The calibration of a PointQ arterial microsimulation model is formulated as a
quadratic programming problem (QP) whose decision variables are link flows,
demands at entry links, and turn movements at intersections, subject to linear
constraints imposed by flow conservation identities and field measurements of a
subset of link flows (counts), demands and turn ratios. The quadratic objective
function is the deviation of the decision variables from their measured values.
The solution to the QP gives estimates of all unmeasured variables and thus
yields a fully specified simulation model. Runs of this simulation model can
then be compared with other field measurements, such as travel times along
routes, to judge the reliability of the calibrated model. A section of the
Huntington-Colorado arterial near I-210 in Los Angeles comprising 73 links and
16 intersections is used to illustrate the procedure. Two experiments are
conducted with the calibrated model to determine the maximum traffic that can
be diverted from the I-210 freeway to the arterial network, with and without
permitting changes in the timing plans. The maximum diversion in both cases is
obtained by solving a linear programming problem. A third experiment compares
the delay and travel time using the existing fixed time control and a max
pressure control. The fourth experiment compares two PointQ models: in the
first model the freeway traffic follows a pre-specified route while the
background traffic moves according to turn ratios, and in the second model turn
ratios are modified in a single commodity model to match the link flows. The
substantial modification of the turn ratios needed suggests that the use of a
single-commodity model as frequently done in CTM models can be misleading...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08085</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08085</id><created>2015-07-29</created><updated>2015-11-29</updated><authors><author><keyname>Zhu</keyname><forenames>Gao</forenames></author><author><keyname>Porikli</keyname><forenames>Fatih</forenames></author><author><keyname>Li</keyname><forenames>Hongdong</forenames></author></authors><title>Tracking Randomly Moving Objects on Edge Box Proposals</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most tracking-by-detection methods employ a local search window around the
predicted object location in the current frame assuming the previous location
is accurate, the trajectory is smooth, and the computational capacity permits a
search radius that can accommodate the maximum speed yet small enough to reduce
mismatches. These, however, may not be valid always, in particular for fast and
irregularly moving objects. Here, we present an object tracker that is not
limited to a local search window and has ability to probe efficiently the
entire frame. Our method generates a small number of &quot;high-quality&quot; proposals
by a novel instance-specific objectness measure and evaluates them against the
object model that can be adopted from an existing tracking-by-detection
approach as a core tracker. During the tracking process, we update the object
model concentrating on hard false-positives supplied by the proposals, which
help suppressing distractors caused by difficult background clutters, and learn
how to re-rank proposals according to the object model. Since we reduce
significantly the number of hypotheses the core tracker evaluates, we can use
richer object descriptors and stronger detector. Our method outperforms most
recent state-of-the-art trackers on popular tracking benchmarks, and provides
improved robustness for fast moving objects as well as for ultra low-frame-rate
videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08087</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08087</id><created>2015-07-29</created><authors><author><keyname>Desouter</keyname><forenames>Benoit</forenames></author><author><keyname>Schrijvers</keyname><forenames>Tom</forenames></author><author><keyname>van Dooren</keyname><forenames>Marko</forenames></author></authors><title>Tabling as a Library with Delimited Control</title><categories>cs.PL</categories><comments>15 pages. To appear in Theory and Practice of Logic Programming
  (TPLP), Proceedings of ICLP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tabling is probably the most widely studied extension of Prolog. But despite
its importance and practicality, tabling is not implemented by most Prolog
systems. Existing approaches require substantial changes to the Prolog engine,
which is an investment out of reach of most systems. To enable more widespread
adoption, we present a new implementation of tabling in under 600 lines of
Prolog code. Our lightweight approach relies on delimited control and provides
reasonable performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08093</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08093</id><created>2015-07-29</created><authors><author><keyname>Kumar</keyname><forenames>Shrawan</forenames></author></authors><title>Property irrelevant predicates</title><categories>cs.PL cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although slicing removes code which has no bearing on property checking.
However even after that, our study has found that there are predicates in
program which have no bearing on property validation, although slicing could
not eliminate them. We have cope up with a criteria to identify such predicates
and then give a process to leverage them in scale up of property checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08094</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08094</id><created>2015-07-29</created><updated>2015-08-18</updated><authors><author><keyname>Schmittner</keyname><forenames>Sebastian E.</forenames></author></authors><title>A SAT-based Public Key Cryptography Scheme</title><categories>cs.CR</categories><comments>7 pages + appendix, 1 figure + 6 in appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A homomorphic public key crypto-scheme based on the Boolean Satisfiability
Problem is proposed. The public key is a SAT formula satisfied by the private
key. Probabilistic encryption generates functions implied to be false by the
public key XOR the message bits. A zero-knowledge proof is used to provide
signatures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08101</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08101</id><created>2015-07-29</created><updated>2016-02-15</updated><authors><author><keyname>Kreutzer</keyname><forenames>Moritz</forenames></author><author><keyname>Thies</keyname><forenames>Jonas</forenames></author><author><keyname>R&#xf6;hrig-Z&#xf6;llner</keyname><forenames>Melven</forenames></author><author><keyname>Pieper</keyname><forenames>Andreas</forenames></author><author><keyname>Shahzad</keyname><forenames>Faisal</forenames></author><author><keyname>Galgon</keyname><forenames>Martin</forenames></author><author><keyname>Basermann</keyname><forenames>Achim</forenames></author><author><keyname>Fehske</keyname><forenames>Holger</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>GHOST: Building blocks for high performance sparse linear algebra on
  heterogeneous systems</title><categories>cs.DC cs.MS</categories><comments>32 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While many of the architectural details of future exascale-class high
performance computer systems are still a matter of intense research, there
appears to be a general consensus that they will be strongly heterogeneous,
featuring &quot;standard&quot; as well as &quot;accelerated&quot; resources. Today, such resources
are available as multicore processors, graphics processing units (GPUs), and
other accelerators such as the Intel Xeon Phi. Any software infrastructure that
claims usefulness for such environments must be able to meet their inherent
challenges: massive multi-level parallelism, topology, asynchronicity, and
abstraction. The &quot;General, Hybrid, and Optimized Sparse Toolkit&quot; (GHOST) is a
collection of building blocks that targets algorithms dealing with sparse
matrix representations on current and future large-scale systems. It implements
the &quot;MPI+X&quot; paradigm, has a pure C interface, and provides hybrid-parallel
numerical kernels, intelligent resource management, and truly heterogeneous
parallelism for multicore CPUs, Nvidia GPUs, and the Intel Xeon Phi. We
describe the details of its design with respect to the challenges posed by
modern heterogeneous supercomputers and recent algorithmic developments.
Implementation details which are indispensable for achieving high efficiency
are pointed out and their necessity is justified by performance measurements or
predictions based on performance models. The library code and several
applications are available as open source. We also provide instructions on how
to make use of GHOST in existing software packages, together with a case study
which demonstrates the applicability and performance of GHOST as a component
within a larger software stack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08104</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08104</id><created>2015-07-29</created><authors><author><keyname>Micenkov&#xe1;</keyname><forenames>Barbora</forenames></author><author><keyname>McWilliams</keyname><forenames>Brian</forenames></author><author><keyname>Assent</keyname><forenames>Ira</forenames></author></authors><title>Learning Representations for Outlier Detection on a Budget</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of detecting a small number of outliers in a large dataset is an
important task in many fields from fraud detection to high-energy physics. Two
approaches have emerged to tackle this problem: unsupervised and supervised.
Supervised approaches require a sufficient amount of labeled data and are
challenged by novel types of outliers and inherent class imbalance, whereas
unsupervised methods do not take advantage of available labeled training
examples and often exhibit poorer predictive performance. We propose BORE (a
Bagged Outlier Representation Ensemble) which uses unsupervised outlier scoring
functions (OSFs) as features in a supervised learning framework. BORE is able
to adapt to arbitrary OSF feature representations, to the imbalance in labeled
data as well as to prediction-time constraints on computational cost. We
demonstrate the good performance of BORE compared to a variety of competing
methods in the non-budgeted and the budgeted outlier detection problem on 12
real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08107</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08107</id><created>2015-07-29</created><authors><author><keyname>Lagr&#xe9;e</keyname><forenames>Paul</forenames></author><author><keyname>Cautis</keyname><forenames>Bogdan</forenames></author><author><keyname>Vahabi</keyname><forenames>Hossein</forenames></author></authors><title>A Network-Aware Approach for Searching As-You-Type in Social Media
  (Extended Version)</title><categories>cs.IR cs.SI</categories><comments>11 pages, To appear in Conference of Information Knowledge and
  Management (CIKM) 2015, Extended Version</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present in this paper a novel approach for as-you-type top-$k$ keyword
search over social media. We adopt a natural &quot;network-aware&quot; interpretation for
information relevance, by which information produced by users who are closer to
the seeker is considered more relevant. In practice, this query model poses new
challenges for effectiveness and efficiency in online search, even when a
complete query is given as input in one keystroke. This is mainly because it
requires a joint exploration of the social space and classic IR indexes such as
inverted lists. We describe a memory-efficient and incremental prefix-based
retrieval algorithm, which also exhibits an anytime behavior, allowing to
output the most likely answer within any chosen running-time limit. We evaluate
it through extensive experiments for several applications and search scenarios,
including searching for posts in micro-blogging (Twitter and Tumblr), as well
as searching for businesses based on reviews in Yelp. They show that our
solution is effective in answering real-time as-you-type searches over social
media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08109</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08109</id><created>2015-07-29</created><authors><author><keyname>Evako</keyname><forenames>Alexander V.</forenames></author></authors><title>Parabolic equations on digital spaces. Solutions on the digital Moebius
  strip and the digital projective plane</title><categories>cs.DM math.AP</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we define a parabolic equation on digital spaces and study its
properties. The equation can be used in investigation of mechanical,
aerodynamic, structural and technological properties of a Moebius strip, which
is used as a basic element of a new configuration of an airplane wing.
Condition for existence of exact solutions by a matrix method and a method of
separation of variables are studied and determined. As examples, numerical
solutions on Moebius strip and projective plane are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08110</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08110</id><created>2015-07-29</created><authors><author><keyname>Fikadu</keyname><forenames>Mulugeta K.</forenames></author><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Cui</keyname><forenames>Qimei</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author></authors><title>Analytic Performance Evaluation of M${-}$QAM Based Decode-and-Forward
  Relay Networks Over Nakagami$-q$ (Hoyt) Fading Channels</title><categories>cs.IT math.IT</categories><comments>16 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is devoted to the analysis of a regenerative multi-node dual-hop
cooperative system over enriched multipath fading channels. Novel analytic
expressions are derived for the symbol-error-rate for $M{-}$ary quadrature
modulated signals in decode-and-forward relay systems over both independent and
identically distributed as well as independent and non-identically distributed
Nakagami${-}$q (Hoyt) fading channels. The derived expressions are based on the
moment-generating-function approach and are given in closed-form in terms of
the generalized Lauricella series. The offered results are validated
extensively through comparisons with respective results from computer
simulations and are useful in the analytic performance evaluation of
regenerative cooperative relay communication systems. To this end, it is shown
that the performance of the cooperative system is, as expected, affected by the
number of employed relays as well as by the value of the fading parameter $q$,
which accounts for \textit{pre-Rayleigh} fading conditions that are often
encountered in mobile cellular radio systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08111</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08111</id><created>2015-07-29</created><authors><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Muhaidat</keyname><forenames>Sami</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author><author><keyname>Ghogho</keyname><forenames>Mounir</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Entropy and Channel Capacity under Optimum Power and Rate Adaptation in
  Generalized Fading Conditions</title><categories>cs.IT math.IT</categories><comments>14 pages, 1 figure</comments><doi>10.1109/LSP.2015.2464221</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate fading characterization and channel capacity determination are of
paramount importance in both conventional and emerging communication systems.
The present work addresses the nonlinearity of the propagation medium and its
effects on the channel capacity. Such fading conditions are first characterized
using information theoretic measures, namely, Shannon entropy, cross entropy
and relative entropy. The corresponding effects on the channel capacity with
and without power adaptation are then analyzed. Closed-form expressions are
derived and validated through comparisons with respective results from computer
simulations. It is shown that the effects of fading nonlinearities are
significantly larger than those of fading parameters such as the scattered-wave
power ratio, and the correlation coefficient between the in-phase and
quadrature components in each cluster of multipath components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08117</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08117</id><created>2015-07-29</created><authors><author><keyname>Singh</keyname><forenames>Pushpendra</forenames></author><author><keyname>Joshi</keyname><forenames>Shiv Dutt</forenames></author></authors><title>Some studies on multidimensional Fourier theory for Hilbert transform,
  analytic signal and space-time series analysis</title><categories>cs.IT math.IT math.NA</categories><comments>13 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose the Fourier frequency vector (FFV), inherently,
associated with multidimensional Fourier transform. With the help of FFV, we
are able to provide physical meaning of so called negative frequencies in
multidimensional Fourier transform (MDFT), which in turn provide
multidimensional spatial and space-time series analysis. The complex
exponential representation of sinusoidal function always yields two
frequencies, negative frequency corresponding to positive frequency and vice
versa, in the multidimensional Fourier spectrum. Thus, using the MDFT, we
propose multidimensional Hilbert transform (MDHT) and associated
multidimensional analytic signal (MDAS) with following properties: (a) the
extra and redundant positive, negative, or both frequencies, introduced due to
complex exponential representation of multidimensional Fourier spectrum, are
suppressed, (b) real part of MDAS is original signal, (c) real and imaginary
part of MDAS are orthogonal, and (d) the magnitude envelope of a original
signal is obtained as the magnitude of its associated MDAS, which is the
instantaneous amplitude of the MDAS. The proposed MDHT and associated DMAS are
generalization of the 1D HT and AS, respectively. We also provide the
decomposition of an image into the AM-FM image model by the Fourier method and
obtain explicit expression for the analytic image computation by 2DDFT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08119</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08119</id><created>2015-07-29</created><authors><author><keyname>M&#xfc;ller</keyname><forenames>Noela S.</forenames></author><author><keyname>Neininger</keyname><forenames>Ralph</forenames></author></authors><title>The CLT Analogue for Cyclic Urns</title><categories>math.PR cs.DM</categories><comments>Extended abstract to be replaced later by a full version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A cyclic urn is an urn model for balls of types $0,\ldots,m-1$ where in each
draw the ball drawn, say of type $j$, is returned to the urn together with a
new ball of type $j+1 \mod m$. The case $m=2$ is the well-known Friedman urn.
The composition vector, i.e., the vector of the numbers of balls of each type
after $n$ steps is, after normalization, known to be asymptotically normal for
$2\le m\le 6$. For $m\ge 7$ the normalized composition vector does not
converge. However, there is an almost sure approximation by a periodic random
vector. In this paper the asymptotic fluctuations around this periodic random
vector are identified. We show that these fluctuations are asymptotically
normal for all $m\ge 7$. However, they are of maximal dimension $m-1$ only when
$6$ does not divide $m$. For $m$ being a multiple of $6$ the fluctuations are
supported by a two-dimensional subspace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08120</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08120</id><created>2015-07-29</created><authors><author><keyname>Lamprecht</keyname><forenames>Daniel</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author><author><keyname>Helic</keyname><forenames>Denis</forenames></author></authors><title>Improving Reachability and Navigability in Recommender Systems</title><categories>cs.IR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate recommender systems from a network perspective
and investigate recommendation networks, where nodes are items (e.g., movies)
and edges are constructed from top-N recommendations (e.g., related movies). In
particular, we focus on evaluating the reachability and navigability of
recommendation networks and investigate the following questions: (i) How well
do recommendation networks support navigation and exploratory search? (ii) What
is the influence of parameters, in particular different recommendation
algorithms and the number of recommendations shown, on reachability and
navigability? and (iii) How can reachability and navigability be improved in
these networks? We tackle these questions by first evaluating the reachability
of recommendation networks by investigating their structural properties.
Second, we evaluate navigability by simulating three different models of
information seeking scenarios. We find that with standard algorithms,
recommender systems are not well suited to navigation and exploration and
propose methods to modify recommendations to improve this. Our work extends
from one-click-based evaluations of recommender systems towards multi-click
analysis (i.e., sequences of dependent clicks) and presents a general,
comprehensive approach to evaluating navigability of arbitrary recommendation
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08121</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08121</id><created>2015-07-29</created><authors><author><keyname>Fikadu</keyname><forenames>Mulugeta K.</forenames></author><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author><author><keyname>Cui</keyname><forenames>Qimei</forenames></author><author><keyname>Muhaidat</keyname><forenames>Sami</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Error Rate and Power Allocation Analysis of Regenerative Networks under
  Generalized Fading Conditions</title><categories>cs.IT math.IT</categories><comments>32 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative communication has been shown to provide significant increase of
transmission reliability and network capacity while expanding coverage in
cellular networks.
  The present work is devoted to the investigation of the end-to-end
performance and power allocation of a maximum-ratio-combining based
regenerative multi-relay cooperative network over non-homogeneous scattering
environment, which is the case in realistic wireless communication scenarios.
Novel analytic expressions are derived for the end-to-end symbol-error-rate of
both $M-$ary Phase-Shift Keying and $M-$ary Quadrature Amplitude Modulation
over independent and non-identically distributed generalized fading channels.
The offered results are expressed in closed-form involving the Lauricella
function and can be readily evaluated with the aid of a proposed computational
algorithm. Simple expressions are also derived for the corresponding
symbol-error-rate at asymptotically high signal-to-noise ratios. The derived
expressions are corroborated with respective results from computer simulations
and are subsequently employed in formulating a power optimization problem that
enhances the system performance under total power constraints within the
multi-relay cooperative system. Furthermore, it is shown that optimum power
allocation provides substantial performance gains over equal power allocation,
particularly, when the source-relay and relay-destination paths are highly
unbalanced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08137</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08137</id><created>2015-07-29</created><updated>2016-02-22</updated><authors><author><keyname>Marti</keyname><forenames>Gautier</forenames></author><author><keyname>Donnat</keyname><forenames>Philippe</forenames></author><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author><author><keyname>Very</keyname><forenames>Philippe</forenames></author></authors><title>HCMapper: An interactive visualization tool to compare partition-based
  flat clustering extracted from pairs of dendrograms</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new visualization tool, dubbed HCMapper, that visually helps to
compare a pair of dendrograms computed on the same dataset by displaying
multiscale partition-based layered structures. The dendrograms are obtained by
hierarchical clustering techniques whose output reflects some hypothesis on the
data and HCMapper is specifically designed to grasp at first glance both
whether the two compared hypotheses broadly agree and the data points on which
they do not concur. Leveraging juxtaposition and explicit encodings, HCMapper
focus on two selected partitions while displaying coarser ones in context areas
for understanding multiscale structure and eventually switching the selected
partitions. HCMapper utility is shown through the example of testing whether
the prices of credit default swap financial time series only undergo
correlation. This use case is detailed in the supplementary material as well as
experiments with code on toy-datasets for reproducible research. HCMapper is
currently released as a visualization tool on the DataGrapple time series and
clustering analysis platorm at www.datagrapple.com.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08139</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08139</id><created>2015-07-29</created><authors><author><keyname>Kang</keyname><forenames>Donggu</forenames></author><author><keyname>Payor</keyname><forenames>James</forenames></author></authors><title>Flow Rounding</title><categories>cs.DS</categories><comments>10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider flow rounding: finding an integral flow from a fractional flow.
Costed flow rounding asks that we find an integral flow with no worse cost.
Randomized flow rounding requires we randomly find an integral flow such that
the expected flow along each edge matches the fractional flow. Both problems
are reduced to cycle canceling, for which we develop an $O(m \log(n^2/m))$
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08150</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08150</id><created>2015-07-29</created><authors><author><keyname>Zaib</keyname><forenames>Alam</forenames></author><author><keyname>Masood</keyname><forenames>Mudassir</forenames></author><author><keyname>Ali</keyname><forenames>Anum</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author></authors><title>Distributed Channel Estimation and Pilot Contamination Analysis for
  Massive MIMO-OFDM Systems</title><categories>cs.IT math.IT</categories><comments>16 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO communication systems, by virtue of utilizing very large number
of antennas, have a potential to yield higher spectral and energy efficiency in
comparison with the conventional MIMO systems. In this paper, we consider
uplink channel estimation in massive MIMO-OFDM systems with frequency selective
channels. With increased number of antennas, the channel estimation problem
becomes very challenging as exceptionally large number of channel parameters
have to be estimated. We propose an efficient distributed linear minimum mean
square error (LMMSE) algorithm that can achieve near optimal channel estimates
at very low complexity by exploiting the strong spatial correlations and
symmetry of large antenna array elements. The proposed method involves solving
a (fixed) reduced dimensional LMMSE problem at each antenna followed by a
repetitive sharing of information through collaboration among neighboring
antenna elements. To further enhance the channel estimates and/or reduce the
number of reserved pilot tones, we propose a data-aided estimation technique
that relies on finding a set of most reliable data carriers. We also analyse
the effect of pilot contamination on the mean square error (MSE) performance of
different channel estimation techniques. Unlike the conventional approaches, we
use stochastic geometry to obtain analytical expression for interference
variance (or power) across OFDM frequency tones and use it to derive the MSE
expressions for different algorithms under both noise and pilot contaminated
regimes. Simulation results validate our analysis and the near optimal MSE
performance of proposed estimation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08153</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08153</id><created>2015-07-29</created><authors><author><keyname>De Masellis</keyname><forenames>Riccardo</forenames></author><author><keyname>Ghidini</keyname><forenames>Chiara</forenames></author><author><keyname>Ranise</keyname><forenames>Silvio</forenames></author></authors><title>A Declarative Framework for Specifying and Enforcing Purpose-aware
  Policies</title><categories>cs.CR cs.LO</categories><comments>Extended version of the paper accepted at the 11th International
  Workshop on Security and Trust Management (STM 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose is crucial for privacy protection as it makes users confident that
their personal data are processed as intended. Available proposals for the
specification and enforcement of purpose-aware policies are unsatisfactory for
their ambiguous semantics of purposes and/or lack of support to the run-time
enforcement of policies.
  In this paper, we propose a declarative framework based on a first-order
temporal logic that allows us to give a precise semantics to purpose-aware
policies and to reuse algorithms for the design of a run-time monitor enforcing
purpose-aware policies. We also show the complexity of the generation and use
of the monitor which, to the best of our knowledge, is the first such a result
in literature on purpose-aware policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08155</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08155</id><created>2015-07-29</created><authors><author><keyname>Qiu</keyname><forenames>Teng</forenames></author><author><keyname>Li</keyname><forenames>Yongjie</forenames></author></authors><title>IT-Dendrogram: A New Member of the In-Tree (IT) Clustering Family</title><categories>stat.ML cs.CV cs.LG stat.ME</categories><comments>13 pages, 6 figures. IT-Dendrogram: An Effective Method to Visualize
  the In-Tree structure by Dendrogram</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Previously, we proposed a physically-inspired method to construct data points
into an effective in-tree (IT) structure, in which the underlying cluster
structure in the dataset is well revealed. Although there are some edges in the
IT structure requiring to be removed, such undesired edges are generally
distinguishable from other edges and thus are easy to be determined. For
instance, when the IT structures for the 2-dimensional (2D) datasets are
graphically presented, those undesired edges can be easily spotted and
interactively determined. However, in practice, there are many datasets that do
not lie in the 2D Euclidean space, thus their IT structures cannot be
graphically presented. But if we can effectively map those IT structures into a
visualized space in which the salient features of those undesired edges are
preserved, then the undesired edges in the IT structures can still be visually
determined in a visualization environment. Previously, this purpose was reached
by our method called IT-map. The outstanding advantage of IT-map is that
clusters can still be found even with the so-called crowding problem in the
embedding.
  In this paper, we propose another method, called IT-Dendrogram, to achieve
the same goal through an effective combination of the IT structure and the
single link hierarchical clustering (SLHC) method. Like IT-map, IT-Dendrogram
can also effectively represent the IT structures in a visualization
environment, whereas using another form, called the Dendrogram. IT-Dendrogram
can serve as another visualization method to determine the undesired edges in
the IT structures and thus benefit the IT-based clustering analysis. This was
demonstrated on several datasets with different shapes, dimensions, and
attributes. Unlike IT-map, IT-Dendrogram can always avoid the crowding problem,
which could help users make more reliable cluster analysis in certain problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08158</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08158</id><created>2015-07-29</created><authors><author><keyname>Drange</keyname><forenames>P&#xe5;l Gr&#xf8;n&#xe5;s</forenames></author><author><keyname>Reidl</keyname><forenames>Felix</forenames></author><author><keyname>Villaamil</keyname><forenames>Fernando S&#xe1;nchez</forenames></author><author><keyname>Sikdar</keyname><forenames>Somnath</forenames></author></authors><title>Fast Biclustering by Dual Parameterization</title><categories>cs.DS</categories><comments>Accepted for presentation at IPEC 2015</comments><acm-class>G.2.2</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  We study two clustering problems, Starforest Editing, the problem of adding
and deleting edges to obtain a disjoint union of stars, and the generalization
Bicluster Editing. We show that, in addition to being NP-hard, none of the
problems can be solved in subexponential time unless the exponential time
hypothesis fails.
  Misra, Panolan, and Saurabh (MFCS 2013) argue that introducing a bound on the
number of connected components in the solution should not make the problem
easier: In particular, they argue that the subexponential time algorithm for
editing to a fixed number of clusters (p-Cluster Editing) by Fomin et al. (J.
Comput. Syst. Sci., 80(7) 2014) is an exception rather than the rule. Here, p
is a secondary parameter, bounding the number of components in the solution.
  However, upon bounding the number of stars or bicliques in the solution, we
obtain algorithms which run in time $2^{5 \sqrt{pk}} + O(n+m)$ for p-Starforest
Editing and $2^{O(p \sqrt{k} \log(pk))} + O(n+m)$ for p-Bicluster Editing. We
obtain a similar result for the more general case of t-Partite p-Cluster
Editing. This is subexponential in k for fixed number of clusters, since p is
then considered a constant.
  Our results even out the number of multivariate subexponential time
algorithms and give reasons to believe that this area warrants further study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08164</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08164</id><created>2015-07-29</created><authors><author><keyname>Foucaud</keyname><forenames>Florent</forenames></author><author><keyname>Mertzios</keyname><forenames>George B.</forenames></author><author><keyname>Naserasr</keyname><forenames>Reza</forenames></author><author><keyname>Parreau</keyname><forenames>Aline</forenames></author><author><keyname>Valicov</keyname><forenames>Petru</forenames></author></authors><title>Identification, location-domination and metric dimension on interval and
  permutation graphs. I. Bounds</title><categories>cs.DM math.CO</categories><comments>formerly part of arXiv:1405.2424</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problems of finding optimal identifying codes, (open)
locating-dominating sets and resolving sets of an interval or a permutation
graph. In these problems, one asks to find a subset of vertices, normally
called a \emph{solution} set, using which all vertices of the graph are
distinguished. The identification can be done by considering the neighborhood
within the solution set, or by employing the distances to the solution
vertices. Normally the goal is to minimize the size of the solution set then.
Here we give tight lower bounds for the size such solution sets when the input
graph is either a interval graph, or a unit interval graph, or a (bipartite)
permutation graph or a cograph. While such lower bounds for the general class
of graphs are in logarithmic order (or order of graph), the improved bounds in
these special classes are of the order of either quadratic root or linear in
terms of number of vertices. Moreover, the results for cographs lead to
linear-time algorithms to solve the considered problems on inputs that are
cographs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08173</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08173</id><created>2015-07-29</created><updated>2016-01-25</updated><authors><author><keyname>Shahid</keyname><forenames>Nauman</forenames></author><author><keyname>Perraudin</keyname><forenames>Nathanael</forenames></author><author><keyname>Kalofolias</keyname><forenames>Vassilis</forenames></author><author><keyname>Puy</keyname><forenames>Gilles</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Fast Robust PCA on Graphs</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining useful clusters from high dimensional data has received significant
attention of the computer vision and pattern recognition community in the
recent years. Linear and non-linear dimensionality reduction has played an
important role to overcome the curse of dimensionality. However, often such
methods are accompanied with three different problems: high computational
complexity (usually associated with the nuclear norm minimization),
non-convexity (for matrix factorization methods) and susceptibility to gross
corruptions in the data. In this paper we propose a principal component
analysis (PCA) based solution that overcomes these three issues and
approximates a low-rank recovery method for high dimensional datasets. We
target the low-rank recovery by enforcing two types of graph smoothness
assumptions, one on the data samples and the other on the features by designing
a convex optimization problem. The resulting algorithm is fast, efficient and
scalable for huge datasets with O(nlog(n)) computational complexity in the
number of data samples. It is also robust to gross corruptions in the dataset
as well as to the model parameters. Clustering experiments on 7 benchmark
datasets with different types of corruptions and background separation
experiments on 3 video datasets show that our proposed model outperforms 10
state-of-the-art dimensionality reduction models. Our theoretical analysis
proves that the proposed model is able to recover approximate low-rank
representations with a bounded error for clusterable data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08184</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08184</id><created>2015-07-29</created><authors><author><keyname>Szasz</keyname><forenames>Teodora</forenames></author><author><keyname>Basarab</keyname><forenames>Adrian</forenames></author><author><keyname>Kouam&#xe9;</keyname><forenames>Denis</forenames></author></authors><title>Beamforming through regularized inverse problems in ultrasound medical
  imaging</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Beamforming in ultrasound imaging has significant impact on the quality of
the final image, controlling its resolution and contrast. Despite its low
spatial resolution and contrast, delay-and-sum is still extensively used
nowadays in clinical applications, due to its real-time capabilities. The most
common alternatives are minimum variance method and its variants, which
overcome the drawbacks of delay-and-sum, at the cost of higher computational
complexity that limits its utilization in real-time applications.
  In this paper, we propose a new way of addressing the beamforming in
ultrasound imaging, by formulating it as a linear inverse problem relating the
reflected echoes to the signal to be recovered. Our approach presents two major
advantages: i) its flexibility in the choice of statistical assumptions on the
signal to be beamformed (Laplacian and Gaussian statistics are tested herein)
and ii) its robustness to a reduced number of pulse emissions. We illustrate
the performance of our approach on both simulated and experimental data, with
\textit{in vivo} examples of carotid and thyroid. Compared to delay-and-sum,
minimimum variance and two other recently published beamforming techniques, our
method offers a better spatial resolution, respectively contrast, when using
Laplacian and Gaussian priors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08187</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08187</id><created>2015-07-29</created><updated>2016-02-25</updated><authors><author><keyname>Ngo</keyname><forenames>Van Chan</forenames><affiliation>ESTASYS</affiliation></author><author><keyname>Legay</keyname><forenames>Axel</forenames><affiliation>ESTASYS</affiliation></author></authors><title>Dependability Analysis of Control Systems using SystemC and Statistical
  Model Checking</title><categories>cs.SE cs.PF</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Petri nets are commonly used for modeling distributed systems in
order to study their performance and dependability. This paper proposes a
realization of stochastic Petri nets in SystemC for modeling large embedded
control systems. Then statistical model checking is used to analyze the
dependability of the constructed model. Our verification framework allows users
to express a wide range of useful properties to be verified which is
illustrated through a case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08198</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08198</id><created>2015-07-29</created><authors><author><keyname>Lioma</keyname><forenames>Christina</forenames></author><author><keyname>Simonsen</keyname><forenames>Jakob Grue</forenames></author><author><keyname>Larsen</keyname><forenames>Birger</forenames></author><author><keyname>Hansen</keyname><forenames>Niels Dalum</forenames></author></authors><title>Non-Compositional Term Dependence for Information Retrieval</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modelling term dependence in IR aims to identify co-occurring terms that are
too heavily dependent on each other to be treated as a bag of words, and to
adapt the indexing and ranking accordingly. Dependent terms are predominantly
identified using lexical frequency statistics, assuming that (a) if terms
co-occur often enough in some corpus, they are semantically dependent; (b) the
more often they co-occur, the more semantically dependent they are. This
assumption is not always correct: the frequency of co-occurring terms can be
separate from the strength of their semantic dependence. E.g. &quot;red tape&quot; might
be overall less frequent than &quot;tape measure&quot; in some corpus, but this does not
mean that &quot;red&quot;+&quot;tape&quot; are less dependent than &quot;tape&quot;+&quot;measure&quot;. This is
especially the case for non-compositional phrases, i.e. phrases whose meaning
cannot be composed from the individual meanings of their terms (such as the
phrase &quot;red tape&quot; meaning bureaucracy). Motivated by this lack of distinction
between the frequency and strength of term dependence in IR, we present a
principled approach for handling term dependence in queries, using both lexical
frequency and semantic evidence. We focus on non-compositional phrases,
extending a recent unsupervised model for their detection [21] to IR. Our
approach, integrated into ranking using Markov Random Fields [31], yields
e?ectiveness gains over competitive TREC baselines, showing that there is still
room for improvement in the very well-studied area of term dependence in IR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08199</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08199</id><created>2015-07-29</created><authors><author><keyname>Aledavood</keyname><forenames>Talayeh</forenames></author><author><keyname>Lehmann</keyname><forenames>Sune</forenames></author><author><keyname>Saram&#xe4;ki</keyname><forenames>Jari</forenames></author></authors><title>On the Digital Daily Cycles of Individuals</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans, like almost all animals, are phase-locked to the diurnal cycle. Most
of us sleep at night and are active through the day. Because we have evolved to
function with this cycle, the circadian rhythm is deeply ingrained and even
detectable at the biochemical level. However, within the broader day-night
pattern, there are individual differences: e.g., some of us are intrinsically
morning-active, while others prefer evenings. In this article, we look at
digital daily cycles: circadian patterns of activity viewed through the lens of
auto-recorded data of communication and online activity. We begin at the
aggregate level, discuss earlier results, and illustrate differences between
population-level daily rhythms in different media. Then we move on to the
individual level, and show that there is a strong individual-level variation
beyond averages: individuals typically have their distinctive daily pattern
that persists in time. We conclude by discussing the driving forces behind
these signature daily patterns, from personal traits (morningness/eveningness)
to variation in activity level and external constraints, and outline
possibilities for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08206</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08206</id><created>2015-07-29</created><updated>2016-01-14</updated><authors><author><keyname>Nicholson</keyname><forenames>Jeremy</forenames></author><author><keyname>Rampersad</keyname><forenames>Narad</forenames></author></authors><title>Initial non-repetitive complexity of infinite words</title><categories>math.CO cs.FL</categories><comments>21 pages; changed &quot;non-repetitive complexity&quot; to &quot;initial
  non-repetitive complexity&quot;</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The initial non-repetitive complexity function of an infinite word x (first
defined by Moothathu) is the function of n that counts the number of distinct
factors of length n that appear at the beginning of x prior to the first
repetition of a length-n factor. We examine general properties of the initial
non-repetitive complexity function, as well as obtain formulas for the initial
non-repetitive complexity of the Thue-Morse word, the Fibonacci word and the
Tribonacci word.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08217</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08217</id><created>2015-07-29</created><authors><author><keyname>Balalaie</keyname><forenames>Armin</forenames><affiliation>Sharif University of Technology</affiliation></author><author><keyname>Heydarnoori</keyname><forenames>Abbas</forenames><affiliation>Sharif University of Technology</affiliation></author><author><keyname>Jamshidi</keyname><forenames>Pooyan</forenames><affiliation>Imperial College London</affiliation></author></authors><title>Migrating to Cloud-Native Architectures Using Microservices: An
  Experience Report</title><categories>cs.SE cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Migration to the cloud has been a popular topic in industry and academia in
recent years. Despite many benefits that the cloud presents, such as high
availability and scalability, most of the on-premise application architectures
are not ready to fully exploit the benefits of this environment, and adapting
them to this environment is a non-trivial task. Microservices have appeared
recently as novel architectural styles that are native to the cloud. These
cloud-native architectures can facilitate migrating on-premise architectures to
fully benefit from the cloud environments because non-functional attributes,
like scalability, are inherent in this style. The existing approaches on cloud
migration does not mostly consider cloud-native architectures as their
first-class citizens. As a result, the final product may not meet its primary
drivers for migration. In this paper, we intend to report our experience and
lessons learned in an ongoing project on migrating a monolithic on-premise
software architecture to microservices. We concluded that microservices is not
a one-fit-all solution as it introduces new complexities to the system, and
many factors, such as distribution complexities, should be considered before
adopting this style. However, if adopted in a context that needs high
flexibility in terms of scalability and availability, it can deliver its
promised benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08219</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08219</id><created>2015-07-29</created><authors><author><keyname>Puppe</keyname><forenames>Clemens</forenames></author><author><keyname>Slinko</keyname><forenames>Arkadii</forenames></author></authors><title>Condorcet Domains, Median Graphs and the Single Crossing Property</title><categories>math.CO cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Condorcet domains are sets of linear orders with the property that, whenever
the preferences of all voters belong to this set, the majority relation has no
cycles. We observe that, without loss of generality, such domain can be assumed
to be closed in the sense that it contains the majority relation of every
profile with an odd number of individuals whose preferences belong to this
domain.
  We show that every closed Condorcet domain is naturally endowed with the
structure of a median graph and that, conversely, every median graph is
associated with a closed Condorcet domain (which may not be a unique one). The
subclass of those Condorcet domains that correspond to linear graphs (chains)
are exactly the preference domains with the classical single crossing property.
As a corollary, we obtain that the domains with the so-called `representative
voter property' (with the exception of a 4-cycle) are the single crossing
domains.
  Maximality of a Condorcet domain imposes additional restrictions on the
underlying median graph. We prove that among all trees only the chains can
induce maximal Condorcet domains, and we characterize the single crossing
domains that in fact do correspond to maximal Condorcet domains.
  Finally, using Nehring's and Puppe's (2007) characterization of monotone
Arrowian aggregation, our analysis yields a rich class of strategy-proof social
choice functions on any closed Condorcet domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08229</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08229</id><created>2015-07-29</created><updated>2015-10-06</updated><authors><author><keyname>Belavkin</keyname><forenames>Roman V.</forenames></author></authors><title>Asymmetric Topologies on Statistical Manifolds</title><categories>cs.IT math.FA math.GN math.IT</categories><comments>8 pages, in Proceedings of the Second International Conference, GSI
  2015, Palaiseau, France, October 28-30, 2015</comments><msc-class>46B20, 46B26, 46E27, 46S99, 52A07, 54E55, 54E99, 94A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asymmetric information distances are used to define asymmetric norms and
quasimetrics on the statistical manifold and its dual space of random
variables. Quasimetric topology, generated by the Kullback-Leibler (KL)
divergence, is considered as the main example, and some of its topological
properties are investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08233</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08233</id><created>2015-07-29</created><authors><author><keyname>Arsenault</keyname><forenames>Marc-Olivier</forenames></author><author><keyname>Gamardo</keyname><forenames>Hanen Garcia</forenames></author><author><keyname>Nguyen</keyname><forenames>Kim-Khoa</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>Session-based Communication for Vital Machine-to-Machine Applications</title><categories>cs.NI</categories><comments>12 pages, S2CT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the machine to machine (M2M) communication has been emerging in
recent years, many vendors' specific proprietary solutions are not suitable for
vital M2M applications. While the main focus of those solutions is management
and provisioning of machines, real-time monitoring and communication control
are also required to handle a variety of access technologies, like WiFi and
LTE, and unleash machine deployment. In this paper, we present a new
architecture addressing these issues by leveraging the IP Multimedia Subsystem
(IMS) deployed in operator's networks for RCS and VoLTE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08234</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08234</id><created>2015-07-29</created><authors><author><keyname>Petersen</keyname><forenames>Casper</forenames></author><author><keyname>Lioma</keyname><forenames>Christina</forenames></author><author><keyname>Simonsen</keyname><forenames>Jakob Grue</forenames></author><author><keyname>Larsen</keyname><forenames>Birger</forenames></author></authors><title>Entropy and Graph Based Modelling of Document Coherence using Discourse
  Entities: An Application</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two novel models of document coherence and their application to
information retrieval (IR). Both models approximate document coherence using
discourse entities, e.g. the subject or object of a sentence. Our fi?rst model
views text as a Markov process generating sequences of discourse entities
(entity n-grams); we use the entropy of these entity n-grams to approximate the
rate at which new information appears in text, reasoning that as more new words
appear, the topic increasingly drifts and text coherence decreases. Our second
model extends the work of Guinaudeau &amp; Strube [28] that represents text as a
graph of discourse entities, linked by diff?erent relations, such as their
distance or adjacency in text. We use several graph topology metrics to
approximate di?fferent aspects of the discourse flow that can indicate
coherence, such as the average clustering or betweenness of discourse entities
in text. Experiments with several instantiations of these models show that: (i)
our models perform on a par with two other well-known models of text coherence
even without any parameter tuning, and (ii) reranking retrieval results
according to their coherence scores gives notable performance gains, con?rming
a relation between document coherence and relevance. This work contributes two
novel models of document coherence, the application of which to IR complements
recent work in the integration of document cohesiveness or comprehensibility to
ranking [5, 56].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08235</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08235</id><created>2015-07-29</created><authors><author><keyname>T&#xf3;thm&#xe9;r&#xe9;sz</keyname><forenames>Lilla</forenames></author></authors><title>Algorithmic aspects of rotor-routing and the notion of linear
  equivalence</title><categories>math.CO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define the analogue of linear equivalence of graph divisors for the
rotor-router model, and use it to prove polynomial time computability of some
problems related to rotor-routing. Using the connection between linear
equivalence for chip-firing and for rotor-routing, we prove that the number of
rotor-router unicycle-orbits equals the order of the Picard group. We also show
that the rotor-router action of the Picard group on the set of spanning
in-arborescences can be interpreted in terms of the linear equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08240</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08240</id><created>2015-07-29</created><updated>2015-10-18</updated><authors><author><keyname>Miao</keyname><forenames>Yajie</forenames></author><author><keyname>Gowayyed</keyname><forenames>Mohammad</forenames></author><author><keyname>Metze</keyname><forenames>Florian</forenames></author></authors><title>EESEN: End-to-End Speech Recognition using Deep RNN Models and
  WFST-based Decoding</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of automatic speech recognition (ASR) has improved
tremendously due to the application of deep neural networks (DNNs). Despite
this progress, building a new ASR system remains a challenging task, requiring
various resources, multiple training stages and significant expertise. This
paper presents our Eesen framework which drastically simplifies the existing
pipeline to build state-of-the-art ASR systems. Acoustic modeling in Eesen
involves learning a single recurrent neural network (RNN) predicting
context-independent targets (phonemes or characters). To remove the need for
pre-generated frame labels, we adopt the connectionist temporal classification
(CTC) objective function to infer the alignments between speech and label
sequences. A distinctive feature of Eesen is a generalized decoding approach
based on weighted finite-state transducers (WFSTs), which enables the efficient
incorporation of lexicons and language models into CTC decoding. Experiments
show that compared with the standard hybrid DNN systems, Eesen achieves
comparable word error rates (WERs), while at the same time speeding up decoding
significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08249</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08249</id><created>2015-07-29</created><updated>2015-10-02</updated><authors><author><keyname>Kliemann</keyname><forenames>Lasse</forenames></author><author><keyname>Sheykhdarabadi</keyname><forenames>Elmira Shirazi</forenames></author><author><keyname>Srivastav</keyname><forenames>Anand</forenames></author></authors><title>Price of Anarchy for Graph Coloring Games with Concave Payoff</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the price of anarchy in a class of graph coloring games (a subclass
of polymatrix common-payoff games). In those games, players are vertices of an
undirected, simple graph, and the strategy space of each player is the set of
colors from $1$ to $k$. A tight bound on the price of anarchy of
$\frac{k}{k-1}$ is known (Hoefer 2007, Kun et al. 2013), for the case that each
player's payoff is the number of her neighbors with different color than
herself. The study of more complex payoff functions was left as an open
problem.
  We compute payoff for a player by determining the distance of her color to
the color of each of her neighbors, applying a non-negative, real-valued,
concave function $f$ to each of those distances, and then summing up the
resulting values. This includes the payoff functions suggested by Kun et al.
(2013) for future work as special cases.
  Denote $f^*$ the maximum value that $f$ attains on the possible distances
$0,\dots,k-1$. We prove an upper bound of $2$ on the price of anarchy for
concave functions $f$ that are non-decreasing or which assume $f^*$ at a
distance on or below $\lfloor\frac{k}{2}\rfloor$. Matching lower bounds are
given for the monotone case and for the case that $f^*$ is assumed in
$\frac{k}{2}$ for even $k$. For general concave functions, we prove an upper
bound of $3$. We use a simple but powerful technique: we obtain an upper bound
of $\lambda \geq 1$ on the price of anarchy if we manage to give a splitting
$\lambda_1 + \dots + \lambda_k = \lambda$ such that $\sum_{s=1}^k \lambda_s
\cdot f(|s-p|) \geq f^*$ for all $p \in \{1,\dots,k\}$. The discovery of
working splittings can be supported by computer experiments. We show how, once
we have an idea what kind of splittings work, this technique helps in giving
simple proofs, which mainly work by case distinctions, algebraic manipulations,
and real calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08254</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08254</id><created>2015-07-29</created><updated>2015-10-26</updated><authors><author><keyname>Bahmani</keyname><forenames>Sohail</forenames></author><author><keyname>Romberg</keyname><forenames>Justin</forenames></author></authors><title>Efficient Compressive Phase Retrieval with Constrained Sensing Vectors</title><categories>cs.IT math.IT math.NA math.OC math.ST stat.TH</categories><comments>Accepted for the 29th Annual Conference on Neural Information
  Processing Systems (NIPS), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a robust and efficient approach to the problem of compressive
phase retrieval in which the goal is to reconstruct a sparse vector from the
magnitude of a number of its linear measurements. The proposed framework relies
on constrained sensing vectors and a two-stage reconstruction method that
consists of two standard convex programs that are solved sequentially.
  In recent years, various methods are proposed for compressive phase
retrieval, but they have suboptimal sample complexity or lack robustness
guarantees. The main obstacle has been that there is no straightforward convex
relaxations for the type of structure in the target. Given a set of
underdetermined measurements, there is a standard framework for recovering a
sparse matrix, and a standard framework for recovering a low-rank matrix.
However, a general, efficient method for recovering a jointly sparse and
low-rank matrix has remained elusive.
  Deviating from the models with generic measurements, in this paper we show
that if the sensing vectors are chosen at random from an incoherent subspace,
then the low-rank and sparse structures of the target signal can be effectively
decoupled. We show that a recovery algorithm that consists of a low-rank
recovery stage followed by a sparse recovery stage will produce an accurate
estimate of the target when the number of measurements is
$\mathsf{O}(k\,\log\frac{d}{k})$, where $k$ and $d$ denote the sparsity level
and the dimension of the input signal. We also evaluate the algorithm through
numerical simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08257</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08257</id><created>2015-07-29</created><authors><author><keyname>Alyoubi</keyname><forenames>Khaled H.</forenames></author><author><keyname>Helmer</keyname><forenames>Sven</forenames></author><author><keyname>Wood</keyname><forenames>Peter T.</forenames></author></authors><title>Ordering Selection Operators Using the Minmax Regret Rule</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimising queries in real-world situations under imperfect conditions is
still a problem that has not been fully solved. We consider finding the optimal
order in which to execute a given set of selection operators under partial
ignorance of their selectivities. The selectivities are modelled as intervals
rather than exact values and we apply a concept from decision theory, the
minimisation of the maximum regret, as a measure of optimality. We show that
the associated decision problem is NP-hard, which renders a brute-force
approach to solving it impractical. Nevertheless, by investigating properties
of the problem and identifying special cases which can be solved in polynomial
time, we gain insight that we use to develop a novel heuristic for solving the
general problem. We also evaluate minmax regret query optimisation
experimentally, showing that it outperforms a currently employed strategy of
optimisers that uses mean values for uncertain parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08258</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08258</id><created>2015-07-29</created><updated>2015-10-27</updated><authors><author><keyname>de Valroger</keyname><forenames>Thibault</forenames></author></authors><title>Perfect Secrecy under Deep Random assumption</title><categories>cs.CR</categories><comments>38 pages main and 41 pages annex</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new idea to design perfectly secure information exchange
protocol, based on so called Deep Randomness, which means randomness relying on
hidden probability distribution. Such idea drives us to introduce a new axiom
in probability theory, thanks to which we can design a protocol, beyond Shannon
limit, enabling two legitimate partners, sharing originally no common private
information, to exchange secret information with accuracy as close as desired
from perfection, and knowledge as close as desired from zero by any unlimitedly
powered opponent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08268</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08268</id><created>2015-07-29</created><updated>2015-10-09</updated><authors><author><keyname>Moshtaghpour</keyname><forenames>Amirafshar</forenames></author><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author><author><keyname>Cambareri</keyname><forenames>Valerio</forenames></author><author><keyname>Degraux</keyname><forenames>Kevin</forenames></author><author><keyname>De Vleeschouwer</keyname><forenames>Christophe</forenames></author></authors><title>Consistent Basis Pursuit for Signal and Matrix Estimates in Quantized
  Compressed Sensing</title><categories>cs.IT math.IT</categories><comments>Keywords: Quantized compressed sensing, quantization, consistency,
  error decay, low-rank, sparsity. 10 pages, 3 figures. Note abbout this
  version: title change, typo corrections, clarification of the context, adding
  a comparison with BPDQ</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the estimation of low-complexity signals when they are
observed through $M$ uniformly quantized compressive observations. Among such
signals, we consider 1-D sparse vectors, low-rank matrices, or compressible
signals that are well approximated by one of these two models. In this context,
we prove the estimation efficiency of a variant of Basis Pursuit Denoise,
called Consistent Basis Pursuit (CoBP), enforcing consistency between the
observations and the re-observed estimate, while promoting its low-complexity
nature. We show that the reconstruction error of CoBP decays like $M^{-1/4}$
when all parameters but $M$ are fixed. Our proof is connected to recent bounds
on the proximity of vectors or matrices when (i) those belong to a set of small
intrinsic &quot;dimension&quot;, as measured by the Gaussian mean width, and (ii) they
share the same quantized (dithered) random projections. By solving CoBP with a
proximal algorithm, we provide some extensive numerical observations that
confirm the theoretical bound as $M$ is increased, displaying even faster error
decay than predicted. The same phenomenon is observed in the special, yet
important case of 1-bit CS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08271</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08271</id><created>2015-07-29</created><updated>2015-08-06</updated><authors><author><keyname>Furmston</keyname><forenames>Thomas</forenames></author><author><keyname>Lever</keyname><forenames>Guy</forenames></author></authors><title>A Gauss-Newton Method for Markov Decision Processes</title><categories>cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate Newton methods are a standard optimization tool which aim to
maintain the benefits of Newton's method, such as a fast rate of convergence,
whilst alleviating its drawbacks, such as computationally expensive calculation
or estimation of the inverse Hessian. In this work we investigate approximate
Newton methods for policy optimization in Markov Decision Processes (MDPs). We
first analyse the structure of the Hessian of the objective function for MDPs.
We show that, like the gradient, the Hessian exhibits useful structure in the
context of MDPs and we use this analysis to motivate two Gauss-Newton Methods
for MDPs. Like the Gauss-Newton method for non-linear least squares, these
methods involve approximating the Hessian by ignoring certain terms in the
Hessian which are difficult to estimate. The approximate Hessians possess
desirable properties, such as negative definiteness, and we demonstrate several
important performance guarantees including guaranteed ascent directions,
invariance to affine transformation of the parameter space, and convergence
guarantees. We finally provide a unifying perspective of key policy search
algorithms, demonstrating that our second Gauss-Newton algorithm is closely
related to both the EM-algorithm and natural gradient ascent applied to MDPs,
but performs significantly better in practice on a range of challenging
domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08282</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08282</id><created>2015-07-29</created><authors><author><keyname>Liddell</keyname><forenames>Torrin M.</forenames></author><author><keyname>DeDeo</keyname><forenames>Simon</forenames></author></authors><title>Common Knowledge on Networks</title><categories>physics.soc-ph cs.SI q-bio.NC</categories><comments>25 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Common knowledge of intentions is crucial to basic social tasks ranging from
cooperative hunting to oligopoly collusion, riots, revolutions, and the
evolution of social norms and human culture. Yet little is known about how
common knowledge leaves a trace on the dynamics of a social network. Here we
show how an individual's network properties---primarily local clustering and
betweenness centrality---provide strong signals of the ability to successfully
participate in common knowledge tasks. These signals are distinct from those
expected when practices are contagious, or when people use less-sophisticated
heuristics that do not yield true coordination. This makes it possible to infer
decision rules from observation. We also find that tasks that require common
knowledge can yield significant inequalities in success, in contrast to the
relative equality that results when practices spread by contagion alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08286</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08286</id><created>2015-07-29</created><authors><author><keyname>Held</keyname><forenames>David</forenames></author><author><keyname>Thrun</keyname><forenames>Sebastian</forenames></author><author><keyname>Savarese</keyname><forenames>Silvio</forenames></author></authors><title>Deep Learning for Single-View Instance Recognition</title><categories>cs.CV cs.LG cs.NE cs.RO</categories><comments>16 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning methods have typically been trained on large datasets in which
many training examples are available. However, many real-world product datasets
have only a small number of images available for each product. We explore the
use of deep learning methods for recognizing object instances when we have only
a single training example per class. We show that feedforward neural networks
outperform state-of-the-art methods for recognizing objects from novel
viewpoints even when trained from just a single image per object. To further
improve our performance on this task, we propose to take advantage of a
supplementary dataset in which we observe a separate set of objects from
multiple viewpoints. We introduce a new approach for training deep learning
methods for instance recognition with limited training data, in which we use an
auxiliary multi-view dataset to train our network to be robust to viewpoint
changes. We find that this approach leads to a more robust classifier for
recognizing objects from novel viewpoints, outperforming previous
state-of-the-art approaches including keypoint-matching, template-based
techniques, and sparse coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08309</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08309</id><created>2015-07-29</created><authors><author><keyname>Li</keyname><forenames>Frank</forenames></author><author><keyname>Shin</keyname><forenames>Richard</forenames></author><author><keyname>Paxson</keyname><forenames>Vern</forenames></author></authors><title>Exploring Privacy Preservation in Outsourced K-Nearest Neighbors with
  Multiple Data Owners</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The k-nearest neighbors (k-NN) algorithm is a popular and effective
classification algorithm. Due to its large storage and computational
requirements, it is suitable for cloud outsourcing. However, k-NN is often run
on sensitive data such as medical records, user images, or personal
information. It is important to protect the privacy of data in an outsourced
k-NN system.
  Prior works have all assumed the data owners (who submit data to the
outsourced k-NN system) are a single trusted party. However, we observe that in
many practical scenarios, there may be multiple mutually distrusting data
owners. In this work, we present the first framing and exploration of privacy
preservation in an outsourced k-NN system with multiple data owners. We
consider the various threat models introduced by this modification. We discover
that under a particularly practical threat model that covers numerous
scenarios, there exists a set of adaptive attacks that breach the data privacy
of any exact k-NN system. The vulnerability is a result of the mathematical
properties of k-NN and its output. Thus, we propose a privacy-preserving
alternative system supporting kernel density estimation using a Gaussian
kernel, a classification algorithm from the same family as k-NN. In many
applications, this similar algorithm serves as a good substitute for k-NN. We
additionally investigate solutions for other threat models, often through
extensions on prior single data owner systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08322</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08322</id><created>2015-07-29</created><authors><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Distributed Mini-Batch SDCA</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an improved analysis of mini-batched stochastic dual coordinate
ascent for regularized empirical loss minimization (i.e. SVM and SVM-type
objectives). Our analysis allows for flexible sampling schemes, including where
data is distribute across machines, and combines a dependence on the smoothness
of the loss and/or the data spread (measured through the spectral norm).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08340</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08340</id><created>2015-07-29</created><authors><author><keyname>Awan</keyname><forenames>Ahsan Javed</forenames></author><author><keyname>Brorsson</keyname><forenames>Mats</forenames></author><author><keyname>Vlassov</keyname><forenames>Vladimir</forenames></author><author><keyname>Ayguade</keyname><forenames>Eduard</forenames></author></authors><title>How Data Volume Affects Spark Based Data Analytics on a Scale-up Server</title><categories>cs.DC cs.AR cs.PF</categories><comments>accepted to 6th International Workshop on Big Data Benchmarks,
  Performance Optimization and Emerging Hardware (BpoE-6) held in conjunction
  with VLDB 2015. arXiv admin note: text overlap with arXiv:1506.07742</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sheer increase in volume of data over the last decade has triggered research
in cluster computing frameworks that enable web enterprises to extract big
insights from big data. While Apache Spark is gaining popularity for exhibiting
superior scale-out performance on the commodity machines, the impact of data
volume on the performance of Spark based data analytics in scale-up
configuration is not well understood. We present a deep-dive analysis of Spark
based applications on a large scale-up server machine. Our analysis reveals
that Spark based data analytics are DRAM bound and do not benefit by using more
than 12 cores for an executor. By enlarging input data size, application
performance degrades significantly due to substantial increase in wait time
during I/O operations and garbage collection, despite 10\% better instruction
retirement rate (due to lower L1 cache misses and higher core utilization). We
match memory behaviour with the garbage collector to improve performance of
applications between 1.6x to 3x.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08345</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08345</id><created>2015-07-29</created><authors><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author><author><keyname>Hermocilla</keyname><forenames>Joseph Anthony C.</forenames></author><author><keyname>Galang</keyname><forenames>John Paul C.</forenames></author><author><keyname>De Sagun</keyname><forenames>Christine C.</forenames></author></authors><title>Perceived Social Loafing in Undergraduate Software Engineering Teams</title><categories>cs.CY</categories><comments>13 pages, appeared in Proceedings (CDROM) of the 6th National
  Conference on IT Education (NCITE 2008), University of the Philippines Los
  Ba\~nos, College, Laguna, Philippines, 23-24 October 2008</comments><journal-ref>Philippine Information Technology Journal 1(2):22-28, 2008</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We surveyed 237 undergraduate students who are enrolled in various subjects
and are members of software engineering teams. Their being a member in a team
is part of the requirements of the course. We found that each of task
visibility, distributive justice, and intrinsic task involvement were
negatively associated with social loafing. We also found out that dominance,
aggression and sucker effect each were positively correlated with social
loafing. We further found out that perception of social loafing exists among
members of software engineering teams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08347</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08347</id><created>2015-07-29</created><authors><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>Inferences in a Virtual Community: Demography, User Preferences, and
  Network Topology</title><categories>cs.SI physics.soc-ph</categories><comments>12 pages, 8 figures, appeared in Proceedings (CDROM) of the 6th
  National Conference on IT Education (NCITE 2008), University of the
  Philippines Los Ba\~nos, College, Laguna, Philippines, 23-24 October 2008</comments><journal-ref>Philippine Information Technology Journal 1(2):2-8, 2008</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper presents a computational procedure for extracting demography data,
mining patterns of human preferences, and measuring the topology of a virtual
network. The network was created from the personal and relationships data of an
online Internet-based community, where persons are considered nodes in the
network, and relationships between persons are considered edges. A community of
Friendster users whose listed hometown is Los Ba\~nos, Laguna was used as a
test bed for the methodology. The method was able to provide the following
demographic, preferential, and topological results about the test bed: (1)
There are more female users (52.34\%) than male (47.66\%); (2) Homophily (i.e.,
birds-of-a-feather adage) is observed in the preferences of users with respect
to age levels, such that they are strongly biased towards being friends with
users of a similar age; (3) There is heterophily in gender preference such that
friendship among users of the opposite gender occurs more often. (4) It
exhibits a small-world characteristic with an average path length of 4.5
(maximum=12) among connected users, shorter than the well-known {\em six
degrees of separation}~\cite{travers69}; And (5) The network exhibits a
scale-free characteristics with heavily-tailed power-law distribution (with the
power $\lambda = -1.02$ and $R^2 = 0.84$) suggesting the presence of many users
acting as the network hubs. The methodology was successful in providing
important data from a virtual community which can be used by several
researchers in the fields of statistics, mathematics, physics, social sciences,
and computer science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08348</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08348</id><created>2015-07-29</created><authors><author><keyname>Manurangsi</keyname><forenames>Pasin</forenames></author><author><keyname>Moshkovitz</keyname><forenames>Dana</forenames></author></authors><title>Approximating Dense Max 2-CSPs</title><categories>cs.DS</categories><acm-class>G.1.6; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a polynomial-time algorithm that approximates
sufficiently high-value Max 2-CSPs on sufficiently dense graphs to within
$O(N^{\varepsilon})$ approximation ratio for any constant $\varepsilon &gt; 0$.
Using this algorithm, we also achieve similar results for free games,
projection games on sufficiently dense random graphs, and the Densest
$k$-Subgraph problem with sufficiently dense optimal solution. Note, however,
that algorithms with similar guarantees to the last algorithm were in fact
discovered prior to our work by Feige et al. and Suzuki and Tokuyama.
  In addition, our idea for the above algorithms yields the following
by-product: a quasi-polynomial time approximation scheme (QPTAS) for
satisfiable dense Max 2-CSPs with better running time than the known
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08349</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08349</id><created>2015-07-29</created><authors><author><keyname>Koch</keyname><forenames>Tobias</forenames></author><author><keyname>Vazquez-Vilar</keyname><forenames>Gonzalo</forenames></author></authors><title>Rate-Distortion Bounds for High-Resolution Vector Quantization via
  Gibbs's Inequality</title><categories>cs.IT math.IT</categories><comments>15 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gibbs's inequality states that the differential entropy of a random variable
with probability density function (pdf) $f$ is less than or equal to its cross
entropy with any other pdf $g$ defined on the same alphabet, i.e., $h(X)\leq
-\mathsf{E}[\log g(X)]$. Using this inequality with a cleverly chosen $g$, we
derive a lower bound on the smallest output entropy that can be achieved by
quantizing a $d$-dimensional source with given expected $r$th-power distortion.
Specialized to the one-dimensional case, and in the limit of vanishing
distortion, this lower bound converges to the output entropy achieved by a
uniform quantizer, thereby recovering the result by Gish and Pierce that
uniform quantizers are asymptotically optimal as the allowed distortion tends
to zero. Our lower bound holds for any $d$-dimensional memoryless source that
has a pdf and whose differential entropy and R\'enyi information dimension are
finite. In contrast to Gish and Pierce, we do not require any additional
constraints on the continuity or decay of the source pdf.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08355</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08355</id><created>2015-07-29</created><updated>2015-10-07</updated><authors><author><keyname>He</keyname><forenames>Xianmang</forenames></author><author><keyname>Xu</keyname><forenames>Liqing</forenames></author><author><keyname>Chen</keyname><forenames>Hao</forenames></author></authors><title>New $q$-ary Quantum MDS Codes with Distances Bigger than $\frac{q}{2}$</title><categories>cs.IT math.IT</categories><comments>19 pages, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constructions of quantum MDS codes have been studied by many authors. We
refer to the table in page 1482 of [3] for known constructions. However there
are only few $q$-ary quantum MDS $[[n,n-2d+2,d]]_q$ codes with minimum
distances $d&gt;\frac{q}{2}$ for sparse lengths $n&gt;q+1$. In the case
$n=\frac{q^2-1}{m}$ where $m|q+1$ or $m|q-1$ there are complete results. In the
case $n=\frac{q^2-1}{m}$ where $m|q^2-1$ is not a factor of $q-1$ or $q+1$,
there is no $q$-ary quantum MDS code with $d&gt; \frac{q}{2}$ has been
constructed. In this paper we propose a direct approch to construct Hermitian
self-orthogonal codes over ${\bf F}_{q^2}$. Thus we give some new $q$-ary
quantum codes in this case. Moreover we present many new $q$-ary quantum MDS
codes with lengths of the form $\frac{w(q^2-1)}{u}$ and minimum distances $d &gt;
\frac{q}{2}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08363</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08363</id><created>2015-07-29</created><authors><author><keyname>Abidi</keyname><forenames>Shaukat</forenames></author><author><keyname>Piccardi</keyname><forenames>Massimo</forenames></author><author><keyname>Williams</keyname><forenames>Mary-Anne</forenames></author></authors><title>Action recognition in still images by latent superpixel classification</title><categories>cs.CV</categories><comments>To appear in the Proceedings of the IEEE International Conference on
  Image Processing. Copyright 2015 IEEE. Please be aware of your obligations
  with respect to copyrighted material</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Action recognition from still images is an important task of computer vision
applications such as image annotation, robotic navigation, video surveillance
and several others. Existing approaches mainly rely on either bag-of-feature
representations or articulated body-part models. However, the relationship
between the action and the image segments is still substantially unexplored.
For this reason, in this paper we propose to approach action recognition by
leveraging an intermediate layer of &quot;superpixels&quot; whose latent classes can act
as attributes of the action. In the proposed approach, the action class is
predicted by a structural model(learnt by Latent Structural SVM) based on
measurements from the image superpixels and their latent classes. Experimental
results over the challenging Stanford 40 Actions dataset report a significant
average accuracy of 74.06% for the positive class and 88.50% for the negative
class, giving evidence to the performance of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08364</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08364</id><created>2015-07-29</created><authors><author><keyname>Segarra</keyname><forenames>Santiago</forenames></author><author><keyname>Marques</keyname><forenames>Antonio G.</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Reconstruction of Graph Signals through Percolation from Seeding Nodes</title><categories>cs.SI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New schemes to recover signals defined in the nodes of a graph are proposed.
Our focus is on reconstructing bandlimited graph signals, which are signals
that admit a sparse representation in a frequency domain related to the
structure of the graph. Most existing formulations focus on estimating an
unknown graph signal by observing its value on a subset of nodes. By contrast,
in this paper, we study the problem of reconstructing a known graph signal
using as input a graph signal that is non-zero only for a small subset of nodes
(seeding nodes). The sparse signal is then percolated (interpolated) across the
graph using a graph filter. Graph filters are a generalization of classical
time-invariant systems and represent linear transformations that can be
implemented distributedly across the nodes of the graph. Three setups are
investigated. In the first one, a single simultaneous injection takes place on
several nodes in the graph. In the second one, successive value injections take
place on a single node. The third one is a generalization where multiple nodes
inject multiple signal values. For noiseless settings, conditions under which
perfect reconstruction is feasible are given, and the corresponding schemes to
recover the desired signal are specified. Scenarios leading to imperfect
reconstruction, either due to insufficient or noisy signal value injections,
are also analyzed. Moreover, connections with classical interpolation in the
time domain are discussed. The last part of the paper presents numerical
experiments that illustrate the results developed through synthetic graph
signals and two real-world signal reconstruction problems: influencing opinions
in a social network and inducing a desired brain state in humans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08373</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08373</id><created>2015-07-30</created><authors><author><keyname>Harandi</keyname><forenames>Mehrtash</forenames></author><author><keyname>Salzmann</keyname><forenames>Mathieu</forenames></author><author><keyname>Porikli</keyname><forenames>Fatih</forenames></author></authors><title>When VLAD met Hilbert</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vectors of Locally Aggregated Descriptors (VLAD) have emerged as powerful
image/video representations that compete with or even outperform
state-of-the-art approaches on many challenging visual recognition tasks. In
this paper, we address two fundamental limitations of VLAD: its requirement for
the local descriptors to have vector form and its restriction to linear
classifiers due to its high-dimensionality. To this end, we introduce a
kernelized version of VLAD. This not only lets us inherently exploit more
sophisticated classification schemes, but also enables us to efficiently
aggregate non-vector descriptors (e.g., tensors) in the VLAD framework.
Furthermore, we propose three approximate formulations that allow us to
accelerate the coding process while still benefiting from the properties of
kernel VLAD. Our experiments demonstrate the effectiveness of our approach at
handling manifold-valued data, such as covariance descriptors, on several
classification tasks. Our results also evidence the benefits of our nonlinear
VLAD descriptors against the linear ones in Euclidean space using several
standard benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08374</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08374</id><created>2015-07-30</created><authors><author><keyname>Rasheed</keyname><forenames>Muhibur</forenames></author><author><keyname>Bajaj</keyname><forenames>Chandrajit</forenames></author></authors><title>Characterization and Construction of a Family of Highly Symmetric
  Spherical Polyhedra with Application in Modeling Self-Assembling Structures</title><categories>cs.CG</categories><comments>25 pages, 12 figures</comments><report-no>UT-TR-2014</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The regular polyhedra have the highest order of 3D symmetries and are
exceptionally at- tractive templates for (self)-assembly using minimal types of
building blocks, from nano-cages and virus capsids to large scale constructions
like glass domes. However, they only represent a small number of possible
spherical layouts which can serve as templates for symmetric assembly. In this
paper, we formalize the necessary and sufficient conditions for symmetric
assembly using exactly one type of building block. All such assemblies
correspond to spherical polyhedra which are edge-transitive and
face-transitive, but not necessarily vertex-transitive. This describes a new
class of polyhedra outside of the well-studied Platonic, Archimedean, Catalan
and and Johnson solids. We show that this new family, dubbed almost-regular
polyhedra, can be pa- rameterized using only two variables and provide an
efficient algorithm to generate an infinite series of such polyhedra.
Additionally, considering the almost-regular polyhedra as templates for the
assembly of 3D spherical shell structures, we developed an efficient polynomial
time shell assembly approximation algorithm for an otherwise NP-hard geometric
optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08375</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08375</id><created>2015-07-30</created><authors><author><keyname>Kaliszyk</keyname><forenames>Cezary</forenames></author><author><keyname>Paskevich</keyname><forenames>Andrei</forenames></author></authors><title>Proceedings Fourth Workshop on Proof eXchange for Theorem Proving</title><categories>cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 186, 2015</journal-ref><doi>10.4204/EPTCS.186</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume of EPTCS contains the proceedings of the Fourth Workshop on Proof
Exchange for Theorem Proving (PxTP 2015), held as part of the International
Conference on Automated Deduction (CADE 2015) on August 2-3, 2015 in Berlin.
The PxTP workshop series brings together researchers working on various aspects
of communication, integration, and cooperation between reasoning systems and
formalisms. These proceedings contain seven regular papers, as well as the
abstracts of the invited talks by Georges Gonthier (Microsoft Research) and
Bart Jacobs (University of Leuven).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08379</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08379</id><created>2015-07-30</created><authors><author><keyname>Wang</keyname><forenames>Mian</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author></authors><title>VMF-SNE: Embedding for Spherical Data</title><categories>cs.LG</categories><comments>5 pages</comments><doi>10.1007/s11040-015-9171-z</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  T-SNE is a well-known approach to embedding high-dimensional data and has
been widely used in data visualization. The basic assumption of t-SNE is that
the data are non-constrained in the Euclidean space and the local proximity can
be modelled by Gaussian distributions. This assumption does not hold for a wide
range of data types in practical applications, for instance spherical data for
which the local proximity is better modelled by the von Mises-Fisher (vMF)
distribution instead of the Gaussian. This paper presents a vMF-SNE embedding
algorithm to embed spherical data. An iterative process is derived to produce
an efficient embedding. The results on a simulation data set demonstrated that
vMF-SNE produces better embeddings than t-SNE for spherical data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08384</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08384</id><created>2015-07-30</created><authors><author><keyname>Feldman</keyname><forenames>Moran</forenames></author><author><keyname>Zenklusen</keyname><forenames>Rico</forenames></author></authors><title>The Submodular Secretary Problem Goes Linear</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the last decade, the matroid secretary problem (MSP) became one of the
most prominent classes of online selection problems. Partially linked to its
numerous applications in mechanism design, substantial interest arose also in
the study of nonlinear versions of MSP, with a focus on the submodular matroid
secretary problem (SMSP). So far, O(1)-competitive algorithms have been
obtained for SMSP over some basic matroid classes. This created some hope that,
analogously to the matroid secretary conjecture, one may even obtain
O(1)-competitive algorithms for SMSP over any matroid. However, up to now, most
questions related to SMSP remained open, including whether SMSP may be
substantially more difficult than MSP; and more generally, to what extend MSP
and SMSP are related.
  Our goal is to address these points by presenting general black-box
reductions from SMSP to MSP. In particular, we show that any O(1)-competitive
algorithm for MSP, even restricted to a particular matroid class, can be
transformed in a black-box way to an O(1)-competitive algorithm for SMSP over
the same matroid class. This implies that the matroid secretary conjecture is
equivalent to the same conjecture for SMSP. Hence, in this sense SMSP is not
harder than MSP. Also, to find O(1)-competitive algorithms for SMSP over a
particular matroid class, it suffices to consider MSP over the same matroid
class. Using our reductions we obtain many first and improved O(1)-competitive
algorithms for SMSP over various matroid classes by leveraging known algorithms
for MSP. Moreover, our reductions imply an O(loglog(rank))-competitive
algorithm for SMSP, thus, matching the currently best asymptotic algorithm for
MSP, and substantially improving on the previously best
O(log(rank))-competitive algorithm for SMSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08396</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08396</id><created>2015-07-30</created><authors><author><keyname>Li</keyname><forenames>Shuangyin</forenames></author><author><keyname>Li</keyname><forenames>Jiefei</forenames></author><author><keyname>Huang</keyname><forenames>Guan</forenames></author><author><keyname>Tan</keyname><forenames>Ruiyang</forenames></author><author><keyname>Pan</keyname><forenames>Rong</forenames></author></authors><title>Tag-Weighted Topic Model For Large-scale Semi-Structured Documents</title><categories>cs.CL cs.IR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To date, there have been massive Semi-Structured Documents (SSDs) during the
evolution of the Internet. These SSDs contain both unstructured features (e.g.,
plain text) and metadata (e.g., tags). Most previous works focused on modeling
the unstructured text, and recently, some other methods have been proposed to
model the unstructured text with specific tags. To build a general model for
SSDs remains an important problem in terms of both model fitness and
efficiency. We propose a novel method to model the SSDs by a so-called
Tag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the
tags and words information, not only to learn the document-topic and topic-word
distributions, but also to infer the tag-topic distributions for text mining
tasks. We present an efficient variational inference method with an EM
algorithm for estimating the model parameters. Meanwhile, we propose three
large-scale solutions for our model under the MapReduce distributed computing
platform for modeling large-scale SSDs. The experimental results show the
effectiveness, efficiency and the robustness by comparing our model with the
state-of-the-art methods in document modeling, tags prediction and text
classification. We also show the performance of the three distributed solutions
in terms of time and accuracy on document modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08398</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08398</id><created>2015-07-30</created><authors><author><keyname>Maingret</keyname><forenames>Baptiste</forenames><affiliation>CITI</affiliation></author><author><keyname>Mou&#xeb;l</keyname><forenames>Fr&#xe9;d&#xe9;ric Le</forenames><affiliation>CITI</affiliation></author><author><keyname>Ponge</keyname><forenames>Julien</forenames><affiliation>CITI</affiliation></author><author><keyname>Stouls</keyname><forenames>Nicolas</forenames><affiliation>CITI</affiliation></author><author><keyname>Cao</keyname><forenames>Jian</forenames><affiliation>CSE</affiliation></author><author><keyname>Loiseau</keyname><forenames>Yannick</forenames><affiliation>LIMOS</affiliation></author></authors><title>Towards a Decoupled Context-Oriented Programming Language for the
  Internet of Things</title><categories>cs.PL</categories><proxy>ccsd</proxy><journal-ref>ACM. 7th International Workshop on Context-Oriented Programming
  (COP'2015) in conjunction with the European Conference on Object-Oriented
  Programming (ECOOP'2015), Jul 2015, Prague, Czech Republic. pp.6, 2015</journal-ref><doi>10.1145/2786545.2786552</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Easily programming behaviors is one major issue of a large and reconfigurable
deployment in the Internet of Things. Such kind of devices often requires to
externalize part of their behavior such as the sensing, the data aggregation or
the code offloading. Most existing context-oriented programming languages
integrate in the same class or close layers the whole behavior. We propose to
abstract and separate the context tracking from the decision process, and to
use event-based handlers to interconnect them. We keep a very easy declarative
and non-layered programming model. We illustrate by defining an extension to
Golo-a JVM-based dynamic language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08408</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08408</id><created>2015-07-30</created><authors><author><keyname>Ghosh</keyname><forenames>Anupam</forenames></author><author><keyname>Talukdar</keyname><forenames>Mainak</forenames></author><author><keyname>Roy</keyname><forenames>Uttam Kumar</forenames></author></authors><title>Stable Drug Designing By Minimizing Drug Protein Interaction Energy
  Using PSO</title><categories>cs.CE q-bio.BM</categories><comments>11 pages, 8 Figures, AICTY 2015, Computer Science &amp; Information
  Technology (CS &amp; IT). http://airccj.org/CSCP/vol5/csit54406.pdf, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Each and every biological function in living organism happens as a result of
protein-protein interactions.The diseases are no exception to this. Identifying
one or more proteins for a particular disease and then designing a suitable
chemical compound (known as drug) to destroy these proteins has been an
interesting topic of research in bio-informatics. In previous methods, drugs
were designed using only seven chemical components and were represented as a
fixed-length tree. But in reality, a drug contains many chemical groups
collectively known as pharmacophore. Moreover, the chemical length of the drug
cannot be determined before designing the drug.In the present work, a Particle
Swarm Optimization (PSO) based methodology has been proposed to find out a
suitable drug for a particular disease so that the drug-protein interaction
becomes stable. In the proposed algorithm, the drug is represented as a
variable length tree and essential functional groups are arranged in different
positions of that drug. Finally, the structure of the drug is obtained and its
docking energy is minimized simultaneously. Also, the orientation of chemical
groups in the drug is tested so that it can bind to a particular active site of
a target protein and the drug fits well inside the active site of target
protein. Here, several inter-molecular forces have been considered for accuracy
of the docking energy. Results show that PSO performs better than the earlier
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08416</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08416</id><created>2015-07-30</created><authors><author><keyname>Chavan</keyname><forenames>Rakesh U.</forenames></author><author><keyname>Chakraborty</keyname><forenames>Debraj</forenames></author><author><keyname>Manjunath</keyname><forenames>D.</forenames></author></authors><title>Stability and Equilibrium Analysis of Laneless Traffic with Local
  Control Laws</title><categories>cs.SY</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new model for traffic on roads with multiple lanes is
developed, where the vehicles do not adhere to a lane discipline. Assuming
identical vehicles, the dynamics is split along two independent directions: the
Y-axis representing the direction of motion and the X-axis representing the
lateral or the direction perpendicular to the direction of motion. Different
influence graphs are used to model the interaction between the vehicles in
these two directions. The instantaneous accelerations of each car, in both X
and Y directions, are functions of the measurements from the neighbouring cars
according to these influence graphs. The stability and equilibrium spacings of
the car formation is analyzed for usual traffic situations such as steady flow,
obstacles, lane changing and rogue drivers arbitrarily changing positions
inside the formation. Conditions are derived under which the formation
maintains stability and the desired intercar spacing for each of these traffic
events. Simulations for some of these scenarios are included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08417</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08417</id><created>2015-07-30</created><authors><author><keyname>D'Angelo</keyname><forenames>Gabriele</forenames></author><author><keyname>Ferretti</keyname><forenames>Stefano</forenames></author></authors><title>Performance evaluation of gossip protocols on complex networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a study on gossip protocols employed to spread data in
unstructured Peer-to-Peer (P2P) network overlays. We consider two classic
protocols (fixed probability and probabilistic broadcast) and compare them with
two new degree dependent gossip protocols that change the probability of
dissemination based on the degree of peers. We identify principal metrics,
provide a theoretical model and perform the assessment evaluation using
parallel and distributed simulation. A main point of this study is that our
large-scale simulations consider implementation technical details, such as the
use of caching and Time To Live (TTL) in message dissemination, that are
usually neglected in simulation, due to the additional overhead they cause.
Outcomes confirm that these technical strategies have an important influence on
the performance of gossip schemes. Moreover, gossip schemes are quite effective
to spread information in P2P overlay networks, whatever their topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08426</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08426</id><created>2015-07-30</created><authors><author><keyname>Lu</keyname><forenames>Xu</forenames></author><author><keyname>Duan</keyname><forenames>Zhenhua</forenames></author><author><keyname>Tian</keyname><forenames>Cong</forenames></author></authors><title>Extending PPTL for Verifying Heap Evolution Properties</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we integrate separation logic with Propositional Projection
Temporal Logic (PPTL) to obtain a two-dimensional logic, namely
PPTL$^{\tiny\mbox{SL}}$. The spatial dimension is realized by a decidable
fragment of separation logic which can be used to describe linked lists, and
the temporal dimension is expressed by PPTL. We show that PPTL and
PPTL$^{\tiny\mbox{SL}}$ are closely related in their syntax structures. That
is, for any PPTL$^{\tiny\mbox{SL}}$ formula in a restricted form, there exists
an &quot;isomorphic&quot; PPTL formula. The &quot;isomorphic&quot; PPTL formulas can be obtained by
first an equisatisfiable translation and then an isomorphic mapping. As a
result, existing theory of PPTL, such as decision procedure for satisfiability
and model checking algorithm, can be reused for PPTL$^{\tiny\mbox{SL}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08429</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08429</id><created>2015-07-30</created><authors><author><keyname>Zhou</keyname><forenames>Shuchang</forenames></author><author><keyname>Wu</keyname><forenames>Yuxin</forenames></author></authors><title>Multilinear Map Layer: Prediction Regularization by Structural
  Constraint</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose and study a technique to impose structural
constraints on the output of a neural network, which can reduce amount of
computation and number of parameters besides improving prediction accuracy when
the output is known to approximately conform to the low-rankness prior. The
technique proceeds by replacing the output layer of neural network with the
so-called MLM layers, which forces the output to be the result of some
Multilinear Map, like a hybrid-Kronecker-dot product or Kronecker Tensor
Product. In particular, given an &quot;autoencoder&quot; model trained on SVHN dataset,
we can construct a new model with MLM layer achieving 62\% reduction in total
number of parameters and reduction of $\ell_2$ reconstruction error from 0.088
to 0.004. Further experiments on other autoencoder model variants trained on
SVHN datasets also demonstrate the efficacy of MLM layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08438</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08438</id><created>2015-07-30</created><updated>2015-07-31</updated><authors><author><keyname>Zhou</keyname><forenames>Pan</forenames></author><author><keyname>Hu</keyname><forenames>Chenhui</forenames></author><author><keyname>Jiang</keyname><forenames>Tao</forenames></author><author><keyname>Wu</keyname><forenames>Dapeng</forenames></author></authors><title>Almost Optimal Energy-Efficient Cognitive Communications in Unknown
  Environments</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1505.06608</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive (Radio) (CR) Communications (CC) are mainly deployed within the
environments of primary (user) communications, where the channel states and
accessibility are usually stochastically distributed (benign or IID). However,
many practical CC are also exposed to disturbing events (contaminated) and
vulnerable jamming attacks (adversarial or non-IID). Thus, the channel state
distribution of spectrum could be stochastic, contaminated or adversarial at
different temporal and spatial locations. Without any a priori, facilitating
optimal CC is a very challenging issue. In this paper, we propose an online
learning algorithm that performs the joint channel sensing, probing and
adaptive channel access for multi-channel CC in general unknown environments.
We take energy-efficient CC (EECC) into our special attention, which is highly
desirable for green wireless communications and demanding to combat with
potential jamming attack who could greatly mar the energy and spectrum
efficiency of CC. The EECC is formulated as a constrained regret minimization
problem with power budget constraints. By tuning a novel exploration parameter,
our algorithms could adaptively find the optimal channel access strategies and
achieve the almost optimal learning performance of EECC in different scenarios
provided with the vanishing long-term power budget violations. We also consider
the important scenario that cooperative learning and information sharing among
multiple CR users to see further performance improvements. The proposed
algorithms are resilient to both oblivious and adaptive jamming attacks with
different intelligence and attacking strength. Extensive numerical results are
conducted to validate our theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08439</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08439</id><created>2015-07-30</created><authors><author><keyname>Kula</keyname><forenames>Maciej</forenames><affiliation>Lyst.com</affiliation></author></authors><title>Metadata Embeddings for User and Item Cold-start Recommendations</title><categories>cs.IR</categories><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I present a hybrid matrix factorisation model representing users and items as
linear combinations of their content features' latent factors. The model
outperforms both collaborative and content-based models in cold-start or sparse
interaction data scenarios (using both user and item metadata), and performs at
least as well as a pure collaborative matrix factorisation model where
interaction data is abundant. Additionally, feature embeddings produced by the
model encode semantic information in a way reminiscent of word embedding
approaches, making them useful for a range of related tasks such as tag
recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08444</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08444</id><created>2015-07-30</created><updated>2015-08-10</updated><authors><author><keyname>Zliobaite</keyname><forenames>Indre</forenames></author><author><keyname>Khokhlov</keyname><forenames>Mikhail</forenames></author></authors><title>Optimal estimates for short horizon travel time prediction in urban
  areas</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing popularity of mobile route planning applications based on GPS
technology provides opportunities for collecting traffic data in urban
environments. One of the main challenges for travel time estimation and
prediction in such a setting is how to aggregate data from vehicles that have
followed different routes, and predict travel time for other routes of
interest. One approach is to predict travel times for route segments, and sum
those estimates to obtain a prediction for the whole route. We study how to
obtain optimal predictions in this scenario. It appears that the optimal
estimate, minimizing the expected mean absolute error, is a combination of the
mean and the median travel times on each segment, where the combination
function depends on the number of segments in the route of interest. We present
a methodology for obtaining such predictions, and demonstrate its effectiveness
with a case study using travel time data from a district of St. Petersburg
collected over one year. The proposed methodology can be applied for real-time
prediction of expected travel times in an urban road network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08445</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08445</id><created>2015-07-30</created><authors><author><keyname>Bansal</keyname><forenames>Ankan</forenames></author><author><keyname>Venkatesh</keyname><forenames>K. S.</forenames></author></authors><title>People Counting in High Density Crowds from Still Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method of estimating the number of people in high density crowds
from still images. The method estimates counts by fusing information from
multiple sources. Most of the existing work on crowd counting deals with very
small crowds (tens of individuals) and use temporal information from videos.
Our method uses only still images to estimate the counts in high density images
(hundreds to thousands of individuals). At this scale, we cannot rely on only
one set of features for count estimation. We, therefore, use multiple sources,
viz. interest points (SIFT), Fourier analysis, wavelet decomposition, GLCM
features and low confidence head detections, to estimate the counts. Each of
these sources gives a separate estimate of the count along with confidences and
other statistical measures which are then combined to obtain the final
estimate. We test our method on an existing dataset of fifty images containing
over 64000 individuals. Further, we added another fifty annotated images of
crowds and tested on the complete dataset of hundred images containing over
87000 individuals. The counts per image range from 81 to 4633. We report the
performance in terms of mean absolute error, which is a measure of accuracy of
the method, and mean normalised absolute error, which is a measure of the
robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08447</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08447</id><created>2015-07-30</created><authors><author><keyname>Mishra</keyname><forenames>Minati</forenames></author></authors><title>Ethical, Legal and Social aspects of Information and Communication
  Technology</title><categories>cs.CY</categories><comments>Proceedings of UGC sponsored Seminar on Ethics and Human Values,Sept
  2007, 66-71</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this era of computers and communication technology where computers and
internet have made their ways to every sphere of life from offices to
residences, reservation counters to banks to post offices, small retail shops
to big organizations, health care units to entertainment industries etc., there
emerged numerous questions regarding the ethical and legal uses of Information
and Communication Technology (ICT). Like any other technological inventions ICT
too has created both positive and negative impacts on the society. This paper
aims at exploring some of these issues in brief.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08449</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08449</id><created>2015-07-30</created><authors><author><keyname>Vilares</keyname><forenames>David</forenames></author><author><keyname>Alonso</keyname><forenames>Miguel A.</forenames></author><author><keyname>G&#xf3;mez-Rodr&#xed;guez</keyname><forenames>Carlos</forenames></author></authors><title>One model, two languages: training bilingual parsers with harmonized
  treebanks</title><categories>cs.CL</categories><comments>6 pages, 2 tables, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an approach to train parsers using bilingual corpora obtained by
merging harmonized treebanks of different languages, producing parsers that
effectively analyze sentences in any of the learned languages, or even
sentences that mix both languages. We test the approach on the Universal
Dependency Treebanks, training with MaltParser and MaltOptimizer.The results
show that these bilingual parsers are more than competitive, as some
combinations not only preserve the performance, but even achieve significant
improvements over the corresponding monolingual parsers. Preliminary
experiments also show the approach to be promising on texts with
code-switching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08452</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08452</id><created>2015-07-30</created><authors><author><keyname>Narayan</keyname><forenames>Shashi</forenames></author><author><keyname>Gardent</keyname><forenames>Claire</forenames></author></authors><title>Unsupervised Sentence Simplification Using Deep Semantics</title><categories>cs.CL</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach to sentence simplification which departs from
previous work in two main ways. First, it requires neither hand written rules
nor a training corpus of aligned standard and simplified sentences. Second,
sentence splitting operates on deep semantic structure. We show (i) that the
unsupervised framework we propose is competitive with four state-of-the-art
supervised systems and (ii) that our semantic based approach allows for a
principled and effective handling of sentence splitting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08462</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08462</id><created>2015-07-30</created><authors><author><keyname>Silva</keyname><forenames>Alonso</forenames><affiliation>LINCS</affiliation></author><author><keyname>Masucci</keyname><forenames>Antonia Maria</forenames></author></authors><title>Defensive Resource Allocation in Social Networks</title><categories>cs.SI cs.GT</categories><comments>arXiv admin note: text overlap with arXiv:1402.5388</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we are interested on the analysis of competing marketing
campaigns between an incumbent who dominates the market and a challenger who
wants to enter the market. We are interested in (a) the simultaneous decision
of how many resources to allocate to their potential customers to advertise
their products for both marketing campaigns, and (b) the optimal allocation on
the situation in which the incumbent knows the entrance of the challenger and
thus can predict its response. Applying results from game theory, we
characterize these optimal strategic resource allocations for the voter model
of social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08467</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08467</id><created>2015-07-30</created><updated>2015-09-18</updated><authors><author><keyname>Jimenez-Romero</keyname><forenames>Cristian</forenames></author><author><keyname>Sousa-Rodrigues</keyname><forenames>David</forenames></author><author><keyname>Johnson</keyname><forenames>Jeffrey H.</forenames></author><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author></authors><title>A Model for Foraging Ants, Controlled by Spiking Neural Networks and
  Double Pheromones</title><categories>cs.NE cs.AI</categories><comments>This work has been accepted for presentation at the UK Workshop on
  Computational Intelligence --- University of Exeter, September 2015
  http://www.ukci2015.ex.ac.uk/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model of an Ant System where ants are controlled by a spiking neural
circuit and a second order pheromone mechanism in a foraging task is presented.
A neural circuit is trained for individual ants and subsequently the ants are
exposed to a virtual environment where a swarm of ants performed a resource
foraging task. The model comprises an associative and unsupervised learning
strategy for the neural circuit of the ant. The neural circuit adapts to the
environment by means of classical conditioning. The initially unknown
environment includes different types of stimuli representing food and obstacles
which, when they come in direct contact with the ant, elicit a reflex response
in the motor neural system of the ant: moving towards or away from the source
of the stimulus. The ants are released on a landscape with multiple food
sources where one ant alone would have difficulty harvesting the landscape to
maximum efficiency. The introduction of a double pheromone mechanism yields
better results than traditional ant colony optimization strategies. Traditional
ant systems include mainly a positive reinforcement pheromone. This approach
uses a second pheromone that acts as a marker for forbidden paths (negative
feedback). This blockade is not permanent and is controlled by the evaporation
rate of the pheromones. The combined action of both pheromones acts as a
collective stigmergic memory of the swarm, which reduces the search space of
the problem. This paper explores how the adaptation and learning abilities
observed in biologically inspired cognitive architectures is synergistically
enhanced by swarm optimization strategies. The model portraits two forms of
artificial intelligent behaviour: at the individual level the spiking neural
network is the main controller and at the collective level the pheromone
distribution is a map towards the solution emerged by the colony.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08475</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08475</id><created>2015-07-30</created><authors><author><keyname>Barroso</keyname><forenames>Ana</forenames></author></authors><title>aDTN - Undetectable Communication in Wireless Delay-tolerant Networks
  (Working Draft)</title><categories>cs.CR cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This document describes a best-effort delay-tolerant communication system
that protects the privacy of users in wireless ad-hoc networks by making their
communication undetectable. The proposed system is a wireless broadcast-based
adaptation of mix networks where each user belongs to at least one group it
trusts, and each group acts as a mix node. Assuming encryption is not broken,
it provides undetectability of all users and messages against external
adversaries, as well as undetectability of users and messages in
non-compromised groups against internal adversaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08476</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08476</id><created>2015-07-30</created><authors><author><keyname>Mill&#xe1;n</keyname><forenames>V&#xed;ctor M. L&#xf3;pez</forenames></author><author><keyname>Cholvi</keyname><forenames>Vicent</forenames></author><author><keyname>Anta</keyname><forenames>Antonio Fern&#xe1;ndez</forenames></author><author><keyname>L&#xf3;pez</keyname><forenames>Luis</forenames></author></authors><title>Resource location based on precomputed partial random walks in dynamic
  networks</title><categories>cs.NI</categories><comments>14 pages, 24 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding a resource residing in a network node (the
\emph{resource location problem}) is a challenge in complex networks due to
aspects as network size, unknown network topology, and network dynamics. The
problem is especially difficult if no requirements on the resource placement
strategy or the network structure are to be imposed, assuming of course that
keeping centralized resource information is not feasible or appropriate. Under
these conditions, random algorithms are useful to search the network. A
possible strategy for static networks, proposed in previous work, uses short
random walks precomputed at each network node as partial walks to construct
longer random walks with associated resource information. In this work, we
adapt the previous mechanisms to dynamic networks, where resource instances may
appear in, and disappear from, network nodes, and the nodes themselves may
leave and join the network, resembling realistic scenarios. We analyze the
resulting resource location mechanisms, providing expressions that accurately
predict average search lengths, which are validated using simulation
experiments. Reduction of average search lengths compared to simple random walk
searches are found to be very large, even in the face of high network
volatility. We also study the cost of the mechanisms, focusing on the overhead
implied by the periodic recomputation of partial walks to refresh the
information on resources, concluding that the proposed mechanisms behave
efficiently and robustly in dynamic networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08482</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08482</id><created>2015-07-30</created><authors><author><keyname>Dunjko</keyname><forenames>Vedran</forenames></author><author><keyname>Taylor</keyname><forenames>Jacob M.</forenames></author><author><keyname>Briegel</keyname><forenames>Hans J.</forenames></author></authors><title>Framework for learning agents in quantum environments</title><categories>quant-ph cs.AI cs.LG</categories><comments>39 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide a broad framework for describing learning agents in
general quantum environments. We analyze the types of classically specified
environments which allow for quantum enhancements in learning, by contrasting
environments to quantum oracles. We show that whether or not quantum
improvements are at all possible depends on the internal structure of the
quantum environment. If the environments are constructed and the internal
structure is appropriately chosen, or if the agent has limited capacities to
influence the internal states of the environment, we show that improvements in
learning times are possible in a broad range of scenarios. Such scenarios we
call luck-favoring settings. The case of constructed environments is
particularly relevant for the class of model-based learning agents, where our
results imply a near-generic improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08492</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08492</id><created>2015-07-30</created><authors><author><keyname>Kougka</keyname><forenames>Georgia</forenames></author><author><keyname>Gounaris</keyname><forenames>Anastasios</forenames></author></authors><title>Cost optimization of data flows based on task re-ordering</title><categories>cs.DB cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analyzing big data in a highly dynamic environment becomes more and more
critical because of the increasingly need for end-to-end processing of this
data. Modern data flows are quite complex and there are not efficient,
cost-based, fully-automated, scalable optimization solutions that can
facilitate flow designers. The state-of-the-art proposals fail to provide near
optimal solutions even for simple data flows. To tackle this problem, we
introduce a set of approximate algorithms for defining the execution order of
the constituent tasks, in order to minimize the total execution cost of a data
flow. We also present the advantages of the parallel execution of data flows.
We validated our proposals in both a real tool and synthetic flows and the
results show that we can achieve significant speed-ups, moving much closer to
optimal solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08495</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08495</id><created>2015-07-30</created><authors><author><keyname>Margenstern</keyname><forenames>Maurice</forenames></author></authors><title>About embedded quarters and points at infinity in the hyperbolic plane</title><categories>cs.CG math.GT</categories><comments>17 pages, 5 figures</comments><msc-class>68R99</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove two results. First, there is a family of sequences of
embedded quarters of the hyperbolic plane such that any sequence converges to a
limit which is an end of the hyperbolic plane. Second, there is no algorithm
which would allow us to check whether two given ends are equal or not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08497</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08497</id><created>2015-07-28</created><updated>2015-08-21</updated><authors><author><keyname>Gasarch</keyname><forenames>William</forenames></author></authors><title>Which Unbounded Protocol for Envy Free Cake Cutting is Better?</title><categories>math.LO cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A division of a cake by n people is envy free if everyone thinks they got the
biggest pieces. Note that peoples tastes can differ. There is a discrete
protocol for envy free division for n=3 which takes at most 5 cuts. For n=4 and
beyond there is a protocol but the number of cuts it takes is unbounded. In
particular the number of cuts depends on peoples tastes. Given any number N
peoples tastes can be set so that the algorithm takes over N cuts. There are
three such algorithms known. Which is better?
  We have devised a way to measure the number of cuts even though it is
unbounded. We use ordinals; therefore, a statement like &quot;this protocol takes at
most 2omega steps&quot; makes sense. We analyse all three discrete algorithms for
envy free cake cutting with this measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08499</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08499</id><created>2015-07-30</created><authors><author><keyname>Garcia-Saavedra</keyname><forenames>Andres</forenames></author><author><keyname>Karzand</keyname><forenames>Mohammad</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author></authors><title>Low Delay Random Linear Coding and Scheduling Over Multiple Interfaces</title><categories>cs.NI cs.IT cs.PF math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multipath transport protocols like MPTCP transfer data across multiple routes
in parallel and deliver it in order at the receiver. When the delay on one or
more of the paths is variable, as is commonly the case, out of order arrivals
are frequent and head of line blocking leads to high latency. This is
exacerbated when packet loss, which is also common with wireless links, is
tackled using ARQ. This paper introduces Stochastic Earliest Delivery Path
First (S-EDPF), a resilient low delay packet scheduler for multipath transport
protocols. S-EDPF takes explicit account of the stochastic nature of paths and
uses this to minimise in-order delivery delay. S-EDPF also takes account of
FEC, jointly scheduling transmission of information and coded packets and in
this way allows lossy links to reduce delay and improve resiliency, rather than
degrading performance as usually occurs with existing multipath systems. We
implement S-EDPF as a multi-platform application that does not require
administration privileges nor modifications to the operating system and has
negligible impact on energy consumption. We present a thorough experimental
evaluation in both controlled environments and into the wild, revealing
dramatic gains in delay performance compared to existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08501</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08501</id><created>2015-07-30</created><authors><author><keyname>Madan</keyname><forenames>Dhiraj</forenames></author><author><keyname>Sen</keyname><forenames>Sandeep</forenames></author></authors><title>Randomised Rounding with Applications</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop new techniques for rounding packing integer programs using
iterative randomized rounding. It is based on a novel application of
multidimensional Brownian motion in $\mathbb{R}^n$. Let $\overset{\sim}{x} \in
{[0,1]}^n$ be a fractional feasible solution of a packing constraint $A x \leq
1,\ \ $ $A \in {\{0,1 \}}^{m\times n}$ that maximizes a linear objective
function. The independent randomized rounding method of Raghavan-Thompson
rounds each variable $x_i$ to 1 with probability $\overset{\sim}{x_i}$ and 0
otherwise. The expected value of the rounded objective function matches the
fractional optimum and no constraint is violated by more than $O(\frac{\log m}
{\log\log m})$.In contrast, our algorithm iteratively transforms
$\overset{\sim}{x}$ to $\hat{x} \in {\{ 0,1\}}^{n}$ using a random walk, such
that the expected values of $\hat{x}_i$'s are consistent with the
Raghavan-Thompson rounding. In addition, it gives us intermediate values $x'$
which can then be used to bias the rounding towards a superior solution.The
reduced dependencies between the constraints of the sparser system can be
exploited using {\it Lovasz Local Lemma}. For $m$ randomly chosen packing
constraints in $n$ variables, with $k$ variables in each inequality, the
constraints are satisfied within $O(\frac{\log (mkp\log m/n) }{\log\log
(mkp\log m/n)})$ with high probability where $p$ is the ratio between the
maximum and minimum coefficients of the linear objective function. Further, we
explore trade-offs between approximation factors and error, and present
applications to well-known problems like circuit-switching, maximum independent
set of rectangles and hypergraph $b$-matching. Our methods apply to the
weighted instances of the problems and are likely to lead to better insights
for even dependent rounding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08514</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08514</id><created>2015-07-30</created><authors><author><keyname>Van Aubel</keyname><forenames>Pol</forenames></author><author><keyname>Bernstein</keyname><forenames>Daniel J.</forenames></author><author><keyname>Niederhagen</keyname><forenames>Ruben</forenames></author></authors><title>Investigating SRAM PUFs in large CPUs and GPUs</title><categories>cs.CR</categories><comments>25 pages, 6 figures. Code in appendix</comments><acm-class>D.4.6; B.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physically unclonable functions (PUFs) provide data that can be used for
cryptographic purposes: on the one hand randomness for the initialization of
random-number generators; on the other hand individual fingerprints for unique
identification of specific hardware components. However, today's off-the-shelf
personal computers advertise randomness and individual fingerprints only in the
form of additional or dedicated hardware.
  This paper introduces a new set of tools to investigate whether intrinsic
PUFs can be found in PC components that are not advertised as containing PUFs.
In particular, this paper investigates AMD64 CPU registers as potential PUF
sources in the operating-system kernel, the bootloader, and the system BIOS;
investigates the CPU cache in the early boot stages; and investigates shared
memory on Nvidia GPUs. This investigation found non-random non-fingerprinting
behavior in several components but revealed usable PUFs in Nvidia GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08539</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08539</id><created>2015-07-30</created><authors><author><keyname>Margan</keyname><forenames>Domagoj</forenames></author><author><keyname>Me&#x161;trovi&#x107;</keyname><forenames>Ana</forenames></author><author><keyname>Martin&#x10d;i&#x107;-Ip&#x161;i&#x107;</keyname><forenames>Sanda</forenames></author></authors><title>Multilayer Network of Language: a Unified Framework for Structural
  Analysis of Linguistic Subsystems</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the focus of complex networks research has shifted from the
analysis of isolated properties of a system toward a more realistic modeling of
multiple phenomena - multilayer networks. Motivated by the prosperity of
multilayer approach in social, transport or trade systems, we propose the
introduction of multilayer networks for language. The multilayer network of
language is a unified framework for modeling linguistic subsystems and their
structural properties enabling the exploration of their mutual interactions.
Various aspects of natural language systems can be represented as complex
networks, whose vertices depict linguistic units, while links model their
relations. The multilayer network of language is defined by three aspects: the
network construction principle, the linguistic subsystem and the language of
interest. More precisely, we construct a word-level (syntax, co-occurrence and
its shuffled counterpart) and a subword level (syllables and graphemes) network
layers, from five variations of original text (in the modeled language). The
obtained results suggest that there are substantial differences between the
networks structures of different language subsystems, which are hidden during
the exploration of an isolated layer. The word-level layers share structural
properties regardless of the language (e.g. Croatian or English), while the
syllabic subword level expresses more language dependent structural properties.
The preserved weighted overlap quantifies the similarity of word-level layers
in weighted and directed networks. Moreover, the analysis of motifs reveals a
close topological structure of the syntactic and syllabic layers for both
languages. The findings corroborate that the multilayer network framework is a
powerful, consistent and systematic approach to model several linguistic
subsystems simultaneously and hence to provide a more unified view on language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08555</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08555</id><created>2015-07-27</created><authors><author><keyname>Bianco</keyname><forenames>Giulia</forenames></author><author><keyname>Gorla</keyname><forenames>Elisa</forenames></author></authors><title>Compression for trace zero points on twisted Edwards curves</title><categories>math.NT cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two optimal representations for the elements of trace zero
subgroups of twisted Edwards curves. For both representations, we provide
efficient compression and decompression algorithms. The efficiency of the
algorithm is compared with the efficiency of similar algorithms on elliptic
curves in Weierstrass form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08559</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08559</id><created>2015-07-30</created><authors><author><keyname>Santhanam</keyname><forenames>Ganesh Ram</forenames></author><author><keyname>Basu</keyname><forenames>Samik</forenames></author><author><keyname>Honavar</keyname><forenames>Vasant</forenames></author></authors><title>CRISNER: A Practically Efficient Reasoner for Qualitative Preferences</title><categories>cs.AI</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present CRISNER (Conditional &amp; Relative Importance Statement Network
PrEference Reasoner), a tool that provides practically efficient as well as
exact reasoning about qualitative preferences in popular ceteris paribus
preference languages such as CP-nets, TCP-nets, CP-theories, etc. The tool uses
a model checking engine to translate preference specifications and queries into
appropriate Kripke models and verifiable properties over them respectively. The
distinguishing features of the tool are: (1) exact and provably correct query
answering for testing dominance, consistency with respect to a preference
specification, and testing equivalence and subsumption of two sets of
preferences; (2) automatic generation of proofs evidencing the correctness of
answer produced by CRISNER to any of the above queries; (3) XML inputs and
outputs that make it portable and pluggable into other applications. We also
describe the extensible architecture of CRISNER, which can be extended to new
reference formalisms based on ceteris paribus semantics that may be developed
in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08565</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08565</id><created>2015-07-29</created><authors><author><keyname>G.</keyname><forenames>Ganesh</forenames></author><author><keyname>Solanky</keyname><forenames>Debanjum Singh</forenames></author><author><keyname>R</keyname><forenames>Govindaraj</forenames></author></authors><title>Collaborative peer production as an alternative to hierarchical internet
  based business systems</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  As we move towards more data intensive, device centric global communication
networks, our ability to usefully harvest these large datastores is degrading.
The widening asymmetry in the explosive growth of data versus our ability to
use it, is forcing us towards centralized analytics. This splintered
concentration of data further consolidates analytical capabilities in the hands
of the few and divides the network into the analysors and the analysed. The
fracturing of the system into opaque datastores and analytics blocks creates a
strong positive feedback loop and has a significant negative impact on the
stability, transparency and freedom of the network. This paper attempted to
identify problems associated with the internet, internet dependent business
models and reviewing available solutions and discuss possible solutions which
became necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08566</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08566</id><created>2015-07-29</created><updated>2015-10-01</updated><authors><author><keyname>Gogineni</keyname><forenames>Vinay Chakravarthi</forenames></author><author><keyname>Chakraborty</keyname><forenames>Mrityunjoy</forenames></author></authors><title>Diffusion Adaptation Over Clustered Multitask Networks Based on the
  Affine Projection Algorithm</title><categories>cs.DC cs.SY math.ST stat.ML stat.TH</categories><comments>Under Communication. arXiv admin note: substantial text overlap with
  arXiv:1311.4894 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed adaptive networks achieve better estimation performance by
exploiting temporal and as well spatial diversity while consuming few
resources. Recent works have studied the single task distributed estimation
problem, in which the nodes estimate a single optimum parameter vector
collaboratively. However, there are many important applications where the
multiple vectors have to estimated simultaneously, in a collaborative manner.
This paper presents multi-task diffusion strategies based on the Affine
Projection Algorithm (APA), usage of APA makes the algorithm robust against the
correlated input. The performance analysis of the proposed multi-task diffusion
APA algorithm is studied in mean and mean square sense. And also a modified
multi-task diffusion strategy is proposed that improves the performance in
terms of convergence rate and steady state EMSE as well. Simulations are
conducted to verify the analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08569</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08569</id><created>2015-07-28</created><authors><author><keyname>Suraki</keyname><forenames>Mohsen Yaghoubi</forenames></author><author><keyname>Suraki</keyname><forenames>Morteza Yaghoubi</forenames></author><author><keyname>SourakiAzad</keyname><forenames>Leila</forenames></author></authors><title>HMIoT: A New Healthcare Model Based on Internet of Things</title><categories>cs.CY</categories><comments>8 pages, 9 figures, Journal</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Volume 12,
  Issue 1, No 1, January 2015 ISSN (Print): 1694-0814 | ISSN (Online):
  1694-0784 www.IJCSI.org</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent century, with developing of equipment, using of the internet and
things connected to the internet is growing. Therefore, the need for informing
in the process of expanding the scope of its application is very necessary and
important. These days, using intelligent and autonomous devices in our daily
lives has become commonplace and the Internet is the most important part of the
relationship between these tools and even at close distances also. Things
connected to the Internet that are currently in use and can be inclusive of all
the sciences as a step to develop and coordinate of them. In this paper we
investigate application and using of Internet of things from the perspective of
various sciences. We show that how this phenomenon can influence on future
health of people.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08571</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08571</id><created>2015-07-30</created><updated>2015-08-07</updated><authors><author><keyname>Ren</keyname><forenames>Wei-Ya</forenames></author><author><keyname>Li</keyname><forenames>Shuo-Hao</forenames></author><author><keyname>Guo</keyname><forenames>Qiang</forenames></author><author><keyname>Li</keyname><forenames>Guo-Hui</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author></authors><title>Agglomerative clustering and collectiveness measure via exponent
  generating function</title><categories>cs.CV cs.GR</categories><comments>11 pages. written on 2015-7-18</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The key in agglomerative clustering is to define the affinity measure between
two sets. A novel agglomerative clustering method is proposed by utilizing the
path integral to define the affinity measure. Firstly, the path integral
descriptor of an edge, a node and a set is computed by path integral and
exponent generating function. Then, the affinity measure between two sets is
obtained by path integral descriptor of sets. Several good properties of the
path integral descriptor is proposed in this paper. In addition, we give the
physical interpretation of the proposed path integral descriptor of a set. The
proposed path integral descriptor of a set can be regard as the collectiveness
measure of a set, which can be a moving system such as human crowd, sheep herd
and so on. Self-driven particle (SDP) model is used to test the ability of the
proposed method in measuring collectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08574</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08574</id><created>2015-07-30</created><updated>2015-10-30</updated><authors><author><keyname>Thomsen</keyname><forenames>Henning</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>de Carvalho</keyname><forenames>Elisabeth</forenames></author><author><keyname>Pratas</keyname><forenames>Nuno K.</forenames></author><author><keyname>Kim</keyname><forenames>Dong Min</forenames></author><author><keyname>Boccardi</keyname><forenames>Federico</forenames></author></authors><title>CoMPflex: CoMP for In-Band Wireless Full Duplex</title><categories>cs.NI cs.IT math.IT</categories><comments>4 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter we consider emulation of a Full Duplex (FD) cellular base
station (BS) by using two spatially separated and coordinated half duplex (HD)
BSs. The proposed system is termed CoMPflex (CoMP for In-Band Wireless Full
Duplex) and at a given instant it serves two HD mobile stations (MSs), one in
the uplink and one in the downlink, respectively. We evaluate the performance
of our scheme by using a geometric extension of the one-dimensional Wyner
model, which takes into account the distances between the devices. The results
show that CoMPflex leads to gains in terms of sum-rate and energy efficiency
with respect to the ordinary FD, as well as with respect to a baseline scheme
based on unidirectional traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08582</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08582</id><created>2015-07-30</created><authors><author><keyname>Pighizzini</keyname><forenames>Giovanni</forenames></author></authors><title>One-Tape Turing Machine Variants and Language Recognition</title><categories>cs.FL cs.CC</categories><comments>20 pages. This article will appear in the Complexity Theory Column of
  the September 2015 issue of SIGACT News</comments><acm-class>F.1.1; F.1.3; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two restricted versions of one-tape Turing machines. Both
characterize the class of context-free languages. In the first version,
proposed by Hibbard in 1967 and called limited automata, each tape cell can be
rewritten only in the first $d$ visits, for a fixed constant $d\geq 2$.
Furthermore, for $d=2$ deterministic limited automata are equivalent to
deterministic pushdown automata, namely they characterize deterministic
context-free languages. Further restricting the possible operations, we
consider strongly limited automata. These models still characterize
context-free languages. However, the deterministic version is less powerful
than the deterministic version of limited automata. In fact, there exist
deterministic context-free languages that are not accepted by any deterministic
strongly limited automaton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08586</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08586</id><created>2015-07-30</created><authors><author><keyname>Wang</keyname><forenames>Yanshan</forenames></author><author><keyname>Li</keyname><forenames>Dingcheng</forenames></author><author><keyname>Liu</keyname><forenames>Hongfang</forenames></author><author><keyname>Choi</keyname><forenames>In-Chan</forenames></author></authors><title>Generalized Ensemble Model for Document Ranking</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A generalized ensemble model (gEnM) for document ranking is proposed in this
paper. The gEnM linearly combines basis document retrieval models and tries to
retrieve relevant documents at high positions. In order to obtain the optimal
linear combination of multiple document retrieval models or rankers, an
optimization program is formulated by directly maximizing the mean average
precision. Both supervised and unsupervised learning algorithms are presented
to solve this program. For the supervised scheme, two approaches are considered
based on the data setting, namely batch and online setting. In the batch
setting, we propose a revised Newton's algorithm, gEnM.BAT, by approximating
the derivative and Hessian matrix. In the online setting, we advocate a
stochastic gradient descent (SGD) based algorithm---gEnM.ON. As for the
unsupervised scheme, an unsupervised ensemble model (UnsEnM) by iteratively
co-learning from each constituent ranker is presented. Experimental study on
benchmark data sets verifies the effectiveness of the proposed algorithms.
Therefore, with appropriate algorithms, the gEnM is a viable option in diverse
practical information retrieval applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08592</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08592</id><created>2015-07-30</created><authors><author><keyname>Bahavarnia</keyname><forenames>MirSaleh</forenames></author></authors><title>Sparse Linear-Quadratic Feedback Design Using Affine Approximation</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a class of $\ell_0$-regularized linear-quadratic (LQ) optimal
control problems. This class of problems is obtained by augmenting a penalizing
sparsity measure to the cost objective of the standard linear-quadratic
regulator (LQR) problem in order to promote sparsity pattern of the state
feedback controller. This class of problems is generally NP hard and
computationally intractable. First, we apply a $\ell_1$-relaxation and consider
the $\ell_1$-regularized LQ version of this class of problems, which is still
nonconvex. Then, we convexify the resulting $\ell_1$-regularized LQ problem by
applying affine approximation techniques. An iterative algorithm is proposed to
solve the $\ell_1$-regularized LQ problem using a series of convexified
$\ell_1$-regularized LQ problems. By means of several numerical experiments, we
show that our proposed algorithm is comparable to the existing algorithms in
the literature, and in some cases it even returns solutions with superior
performance and sparsity pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08599</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08599</id><created>2015-07-30</created><authors><author><keyname>Arag&#xf3;n</keyname><forenames>Pablo</forenames></author><author><keyname>Volkovich</keyname><forenames>Yana</forenames></author><author><keyname>Laniado</keyname><forenames>David</forenames></author><author><keyname>Kaltenbrunner</keyname><forenames>Andreas</forenames></author></authors><title>When a Movement Becomes a Party: The 2015 Barcelona City Council
  Election</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Barcelona en Com\'u, an emerging grassroots movement-party, won the 2015
Barcelona City Council election. This candidacy was devised by activists
involved in the 15M movement in order to turn citizen outrage into political
change. On the one hand, the 15M movement is based on a decentralized
structure. On the other hand, political science literature postulates that
parties historically develop oligarchical leadership structures. This tension
motivates us to examine whether Barcelona en Com\'u preserved a decentralizated
structure or adopted a conventional centralized organization. In this article
we analyse the Twitter networks of the parties that ran for this election by
measuring their hierarchical structure, information efficiency and social
resilience. Our results show that in Barcelona en Com\'u two well-defined
groups co-exist: a cluster dominated by the leader and the collective accounts,
and another cluster formed by the movement activists. While the former group is
highly centralized like the other major parties, the latter one stands out for
its decentralized, cohesive and resilient structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08600</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08600</id><created>2015-07-30</created><authors><author><keyname>Sorokin</keyname><forenames>Alexey</forenames></author></authors><title>Normal forms for linear displacement context-free grammars</title><categories>cs.FL</categories><comments>5 pages, just for educational and referential purposes, therefore no
  references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove several results on normal forms for linear
displacement context-free grammars. The results themselves are rather simple
and use well-known techniques, but they are extensively used in more complex
constructions. Therefore this article mostly serves educational and referential
purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08610</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08610</id><created>2015-07-30</created><authors><author><keyname>Kuramitsu</keyname><forenames>Kimio</forenames></author></authors><title>Fast, Flexible, and Declarative Construction of Abstract Syntax Trees
  with PEGs</title><categories>cs.PL</categories><comments>To appear in Journal of Information Processing (2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address a declarative construction of abstract syntax trees with Parsing
Expression Grammars. AST operators (constructor, connector, and tagging) are
newly defined to specify flexible AST constructions. A new challenge coming
with PEGs is the consistency management of ASTs in backtracking and packrat
parsing. We make the transaction AST machine in order to perform AST operations
in the context of the speculative parsing of PEGs. All the consistency control
is automated by the analysis of AST operators. The proposed approach is
implemented in the Nez parser, written in Java. The performance study shows
that the transactional AST machine requires 25\% approximately more time in
CSV, XML, and C grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08641</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08641</id><created>2015-07-30</created><updated>2016-03-08</updated><authors><author><keyname>Horlemann-Trautmann</keyname><forenames>Anna-Lena</forenames></author><author><keyname>Marshall</keyname><forenames>Kyle</forenames></author></authors><title>New Criteria for MRD and Gabidulin Codes and some Rank-Metric Code
  Constructions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present a new criterion to check if a given rank-metric code
is a maximum rank distance (MRD) code. Moreover, we derive a criterion to check
if a given MRD code is a generalized Gabidulin code. We then use these results
to come up with constructions of linear MRD codes of dimension 2 that are not
generalized Gabidulin codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08685</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08685</id><created>2015-07-30</created><authors><author><keyname>Deshpande</keyname><forenames>Yash</forenames></author><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>Asymptotic Mutual Information for the Two-Groups Stochastic Block Model</title><categories>cs.IT cond-mat.stat-mech math.IT math.ST stat.TH</categories><comments>41 pages, 3 pdf figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an information-theoretic view of the stochastic block model, a
popular statistical model for the large-scale structure of complex networks. A
graph $G$ from such a model is generated by first assigning vertex labels at
random from a finite alphabet, and then connecting vertices with edge
probabilities depending on the labels of the endpoints. In the case of the
symmetric two-group model, we establish an explicit `single-letter'
characterization of the per-vertex mutual information between the vertex labels
and the graph.
  The explicit expression of the mutual information is intimately related to
estimation-theoretic quantities, and --in particular-- reveals a phase
transition at the critical point for community detection. Below the critical
point the per-vertex mutual information is asymptotically the same as if edges
were independent. Correspondingly, no algorithm can estimate the partition
better than random guessing. Conversely, above the threshold, the per-vertex
mutual information is strictly smaller than the independent-edges upper bound.
In this regime there exists a procedure that estimates the vertex labels better
than random guessing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08690</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08690</id><created>2015-07-30</created><authors><author><keyname>Petersen</keyname><forenames>Holger</forenames></author></authors><title>The Complexity of Some Combinatorial Puzzles</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the decision versions of the puzzles Knossos and The Hour-Glass
are complete for NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08694</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08694</id><created>2015-07-30</created><authors><author><keyname>Lim</keyname><forenames>Benjamin</forenames></author></authors><title>Android Tapjacking Vulnerability</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Android is an open source mobile operating system that is developed mainly by
Google. It is used on a significant portion of mobile devices worldwide. In
this paper, I will be looking at an attack commonly known as tapjacking. I will
be taking the attack apart and walking through each individual step required to
implement the attack. I will then explore the various payload options available
to an attacker. Lastly, I will touch on the feasibility of the attack as well
as mitigation strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08696</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08696</id><created>2015-07-30</created><updated>2015-11-17</updated><authors><author><keyname>Fischer</keyname><forenames>Rico</forenames></author><author><keyname>Leitao</keyname><forenames>Jorge C.</forenames></author><author><keyname>Peixoto</keyname><forenames>Tiago P.</forenames></author><author><keyname>Altmann</keyname><forenames>Eduardo G.</forenames></author></authors><title>Sampling motif-constrained ensembles of networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>Updated version, as published in the journal. 7 pages, 5 figures, one
  Supplemental Material</comments><journal-ref>Phys. Rev. Lett. 115, 188701 (2015)</journal-ref><doi>10.1103/PhysRevLett.115.188701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The statistical significance of network properties is conditioned on null
models which satisfy spec- ified properties but that are otherwise random.
Exponential random graph models are a principled theoretical framework to
generate such constrained ensembles, but which often fail in practice, either
due to model inconsistency, or due to the impossibility to sample networks from
them. These problems affect the important case of networks with prescribed
clustering coefficient or number of small connected subgraphs (motifs). In this
paper we use the Wang-Landau method to obtain a multicanonical sampling that
overcomes both these problems. We sample, in polynomial time, net- works with
arbitrary degree sequences from ensembles with imposed motifs counts. Applying
this method to social networks, we investigate the relation between
transitivity and homophily, and we quantify the correlation between different
types of motifs, finding that single motifs can explain up to 60% of the
variation of motif profiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08700</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08700</id><created>2015-07-30</created><authors><author><keyname>Davoodi</keyname><forenames>Faranak</forenames></author></authors><title>PSC: A Pattern-Based Temporal and Spatial Crowdsourcing Platform to
  Improve Performance, Reliability, and Privacy</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a novel spatial crowdsourcing system where the
workers' time availabilities and their spatial locations are known a priori.
Consequently, the tasks assignment to workers is performed not only based on
the current location of the human workers and the tasks available in the
region, but also based on the availability of the workers during the specific
times that a given task should be accepted, processed, and completed. Having
the system determine the daily pattern of the workers (either by predefined
questionnaires when the workers register, or by archiving data from the
worker's mobile devices, or by on the road and real-time entered status data)
eliminates many unsuccessful task assignments and therefore significantly
increases the efficiency of the system. In the original Spatial Crowdsourcing
(SC) framework, the SC-server optimizes the task assignment locally at every
instance of time and whenever a new task, or a new worker, enters the system.
Our new framework (PSC), on the other hand, allows the users to enter their
daily routine, and temporal, spatial, and availability patterns a priori. This
makes the system much more stable and pattern-opportunistic. The PSC servers
can focus on receiving and archiving new entries (e.g., workers, tasks, and
their criteria) during busy times (e.g., when there are many new entries in the
system), and can focus on optimization and computations during quiet times
(e.g., when there are fewer new entries in the system). Having the task
optimization process happen during quiet times, and when there are few changes
to the system, makes the performance more stable and reliable. It also allows
the PSC system to have a global view of the system and and perform global
optimizations to improve the performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08701</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08701</id><created>2015-07-30</created><updated>2015-09-11</updated><authors><author><keyname>Cho</keyname><forenames>Myung</forenames></author><author><keyname>Mishra</keyname><forenames>Kumar Vijay</forenames></author><author><keyname>Cai</keyname><forenames>Jian-Feng</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author></authors><title>Block Iterative Reweighted Algorithms for Super-Resolution of Spectrally
  Sparse Signals</title><categories>cs.IT math.IT</categories><doi>10.1109/LSP.2015.2478854</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose novel algorithms that enhance the performance of recovering
unknown continuous-valued frequencies from undersampled signals. Our iterative
reweighted frequency recovery algorithms employ the support knowledge gained
from earlier steps of our algorithms as block prior information to enhance
frequency recovery. Our methods improve the performance of the atomic norm
minimization which is a useful heuristic in recovering continuous-valued
frequency contents. Numerical results demonstrate that our block iterative
reweighted methods provide both better recovery performance and faster speed
than other known methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08705</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08705</id><created>2015-07-30</created><updated>2015-12-14</updated><authors><author><keyname>Lofgren</keyname><forenames>Peter</forenames></author><author><keyname>Banerjee</keyname><forenames>Siddhartha</forenames></author><author><keyname>Goel</keyname><forenames>Ashish</forenames></author></authors><title>Bidirectional PageRank Estimation: From Average-Case to Worst-Case</title><categories>cs.DS cs.DM</categories><comments>Workshop on Algorithms and Models for the Web-Graph (WAW) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for estimating the Personalized PageRank (PPR)
between a source and target node on undirected graphs, with sublinear
running-time guarantees over the worst-case choice of source and target nodes.
Our work builds on a recent line of work on bidirectional estimators for PPR,
which obtained sublinear running-time guarantees but in an average-case sense,
for a uniformly random choice of target node. Crucially, we show how the
reversibility of random walks on undirected networks can be exploited to
convert average-case to worst-case guarantees. While past bidirectional methods
combine forward random walks with reverse local pushes, our algorithm combines
forward local pushes with reverse random walks. We also discuss how to modify
our methods to estimate random-walk probabilities for any length distribution,
thereby obtaining fast algorithms for estimating general graph diffusions,
including the heat kernel, on undirected networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08707</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08707</id><created>2015-07-30</created><authors><author><keyname>Jobson</keyname><forenames>Adam</forenames></author><author><keyname>Sledd</keyname><forenames>Levi</forenames></author><author><keyname>White</keyname><forenames>Susan C.</forenames></author><author><keyname>Wildstrom</keyname><forenames>D. Jacob</forenames></author></authors><title>Variations on Narrow Dots-and-Boxes and Dots-and-Triangles</title><categories>math.CO cs.GT</categories><comments>8 pages, 11 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We verify a conjecture of Nowakowski and Ottaway that closed $1 \times n$
Dots-and-Triangles is a first-player win when $n \neq 2$. We also prove that in
both the open and closed $1 \times n$ Dots-and-Boxes games where $n$ is even,
the first player can guarantee a tie.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08708</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08708</id><created>2015-07-30</created><updated>2015-08-04</updated><authors><author><keyname>Mu'alem</keyname><forenames>Ahuva</forenames></author><author><keyname>Schapira</keyname><forenames>Michael</forenames></author></authors><title>Setting Lower Bounds on Truthfulness</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present general techniques for proving inapproximability results for
several paradigmatic truthful multidimensional mechanism design problems. In
particular, we demonstrate the strength of our techniques by exhibiting a lower
bound of 2-1/m for the scheduling problem with m unrelated machines (formulated
as a mechanism design problem in the seminal paper of Nisan and Ronen on
Algorithmic Mechanism Design). Our lower bound applies to truthful randomized
mechanisms, regardless of any computational assumptions on the running time of
these mechanisms. Moreover, it holds even for the wider class of
truthfulness-in-expectation mechanisms. This lower bound nearly matches the
known 1.58606 randomized truthful upper bound for the case of two machines (a
non-truthful FPTAS exists).
  Recently, Daskalakis and Weinberg show that there is a polynomial-time
2-approximately optimal Bayesian mechanism for makespan minimization for
unrelated machines. We complement this result by showing an appropriate lower
bound of 1.25 for deterministic incentive compatible Bayesian mechanisms.
  We then show an application of our techniques to the workload-minimization
problem in networks. We prove our lower bounds for this problem in the
inter-domain routing setting presented by Feigenbaum, Papadimitriou, Sami, and
Shenker. Finally, we discuss several notions of non-utilitarian fairness
(Max-Min fairness, Min-Max fairness, and envy minimization) and show how our
techniques can be used to prove lower bounds for these notions. No lower bounds
for truthful mechanisms in multidimensional probabilistic settings were
previously known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08711</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08711</id><created>2015-07-30</created><authors><author><keyname>Harandi</keyname><forenames>Mehrtash</forenames></author><author><keyname>Salzmann</keyname><forenames>Mathieu</forenames></author><author><keyname>Baktashmotlagh</keyname><forenames>Mahsa</forenames></author></authors><title>Beyond Gauss: Image-Set Matching on the Riemannian Manifold of PDFs</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art image-set matching techniques typically implicitly model
each image-set with a Gaussian distribution. Here, we propose to go beyond
these representations and model image-sets as probability distribution
functions (PDFs) using kernel density estimators. To compare and match
image-sets, we exploit Csiszar f-divergences, which bear strong connections to
the geodesic distance defined on the space of PDFs, i.e., the statistical
manifold. Furthermore, we introduce valid positive definite kernels on the
statistical manifolds, which let us make use of more powerful classification
schemes to match image-sets. Finally, we introduce a supervised dimensionality
reduction technique that learns a latent space where f-divergences reflect the
class labels of the data. Our experiments on diverse problems, such as
video-based face recognition and dynamic texture classification, evidence the
benefits of our approach over the state-of-the-art image-set matching methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08715</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08715</id><created>2015-07-30</created><authors><author><keyname>Reis</keyname><forenames>Giselle</forenames><affiliation>INRIA-Saclay</affiliation></author></authors><title>Importing SMT and Connection proofs as expansion trees</title><categories>cs.LO</categories><comments>In Proceedings PxTP 2015, arXiv:1507.08375</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 186, 2015, pp. 3-10</journal-ref><doi>10.4204/EPTCS.186.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different automated theorem provers reason in various deductive systems and,
thus, produce proof objects which are in general not compatible. To understand
and analyze these objects, one needs to study the corresponding proof theory,
and then study the language used to represent proofs, on a prover by prover
basis. In this work we present an implementation that takes SMT and Connection
proof objects from two different provers and imports them both as expansion
trees. By representing the proofs in the same framework, all the algorithms and
tools available for expansion trees (compression, visualization, sequent
calculus proof construction, proof checking, etc.) can be employed uniformly.
The expansion proofs can also be used as a validation tool for the proof
objects produced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08716</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08716</id><created>2015-07-30</created><authors><author><keyname>Heath</keyname><forenames>Quentin</forenames><affiliation>Inria &amp; LIX</affiliation></author><author><keyname>Miller</keyname><forenames>Dale</forenames><affiliation>Inria &amp; LIX</affiliation></author></authors><title>A framework for proof certificates in finite state exploration</title><categories>cs.LO</categories><comments>In Proceedings PxTP 2015, arXiv:1507.08375</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 186, 2015, pp. 11-26</journal-ref><doi>10.4204/EPTCS.186.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model checkers use automated state exploration in order to prove various
properties such as reachability, non-reachability, and bisimulation over state
transition systems. While model checkers have proved valuable for locating
errors in computer models and specifications, they can also be used to prove
properties that might be consumed by other computational logic systems, such as
theorem provers. In such a situation, a prover must be able to trust that the
model checker is correct. Instead of attempting to prove the correctness of a
model checker, we ask that it outputs its &quot;proof evidence&quot; as a formally
defined document--a proof certificate--and that this document is checked by a
trusted proof checker. We describe a framework for defining and checking proof
certificates for a range of model checking problems. The core of this framework
is a (focused) proof system that is augmented with premises that involve &quot;clerk
and expert&quot; predicates. This framework is designed so that soundness can be
guaranteed independently of any concerns for the correctness of the clerk and
expert specifications. To illustrate the flexibility of this framework, we
define and formally check proof certificates for reachability and
non-reachability in graphs, as well as bisimulation and non-bisimulation for
labeled transition systems. Finally, we describe briefly a reference checker
that we have implemented for this framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08717</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08717</id><created>2015-07-30</created><authors><author><keyname>Benzm&#xfc;ller</keyname><forenames>Christoph</forenames><affiliation>Freie Universit&#xe4;t Berlin, Germany</affiliation></author><author><keyname>Claus</keyname><forenames>Maximilian</forenames><affiliation>Freie Universit&#xe4;t Berlin, Germany</affiliation></author><author><keyname>Sultana</keyname><forenames>Nik</forenames><affiliation>Cambridge University, UK</affiliation></author></authors><title>Systematic Verification of the Modal Logic Cube in Isabelle/HOL</title><categories>cs.LO cs.AI</categories><comments>In Proceedings PxTP 2015, arXiv:1507.08375</comments><proxy>EPTCS</proxy><acm-class>I.2.3; I.2.4; F.4.1</acm-class><journal-ref>EPTCS 186, 2015, pp. 27-41</journal-ref><doi>10.4204/EPTCS.186.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an automated verification of the well-known modal logic cube in
Isabelle/HOL, in which we prove the inclusion relations between the cube's
logics using automated reasoning tools. Prior work addresses this problem but
without restriction to the modal logic cube, and using encodings in first-order
logic in combination with first-order automated theorem provers. In contrast,
our solution is more elegant, transparent and effective. It employs an
embedding of quantified modal logic in classical higher-order logic. Automated
reasoning tools, such as Sledgehammer with LEO-II, Satallax and CVC4, Metis and
Nitpick, are employed to achieve full automation. Though successful, the
experiments also motivate some technical improvements in the Isabelle/HOL tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08718</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08718</id><created>2015-07-30</created><authors><author><keyname>Adams</keyname><forenames>Mark</forenames><affiliation>Proof Technologies Ltd, UK and Radboud University, Nijmegen, The Netherlands</affiliation></author></authors><title>The Common HOL Platform</title><categories>cs.LO cs.DL</categories><comments>In Proceedings PxTP 2015, arXiv:1507.08375</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 186, 2015, pp. 42-56</journal-ref><doi>10.4204/EPTCS.186.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Common HOL project aims to facilitate porting source code and proofs
between members of the HOL family of theorem provers. At the heart of the
project is the Common HOL Platform, which defines a standard HOL theory and API
that aims to be compatible with all HOL systems. So far, HOL Light and hol90
have been adapted for conformance, and HOL Zero was originally developed to
conform. In this paper we provide motivation for a platform, give an overview
of the Common HOL Platform's theory and API components, and show how to adapt
legacy systems. We also report on the platform's successful application in the
hand-translation of a few thousand lines of source code from HOL Light to HOL
Zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08719</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08719</id><created>2015-07-30</created><authors><author><keyname>Cauderlier</keyname><forenames>Rapha&#xeb;l</forenames><affiliation>Inria</affiliation></author><author><keyname>Halmagrand</keyname><forenames>Pierre</forenames><affiliation>Inria</affiliation></author></authors><title>Checking Zenon Modulo Proofs in Dedukti</title><categories>cs.LO</categories><comments>In Proceedings PxTP 2015, arXiv:1507.08375</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 186, 2015, pp. 57-73</journal-ref><doi>10.4204/EPTCS.186.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dedukti has been proposed as a universal proof checker. It is a logical
framework based on the lambda Pi calculus modulo that is used as a backend to
verify proofs coming from theorem provers, especially those implementing some
form of rewriting. We present a shallow embedding into Dedukti of proofs
produced by Zenon Modulo, an extension of the tableau-based first-order theorem
prover Zenon to deduction modulo and typing. Zenon Modulo is applied to the
verification of programs in both academic and industrial projects. The purpose
of our embedding is to increase the confidence in automatically generated
proofs by separating untrusted proof search from trusted proof verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08720</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08720</id><created>2015-07-30</created><authors><author><keyname>Assaf</keyname><forenames>Ali</forenames><affiliation>Inria, Ecole Polytechnique</affiliation></author><author><keyname>Burel</keyname><forenames>Guillaume</forenames><affiliation>ENSIIE/C&#xe9;dric</affiliation></author></authors><title>Translating HOL to Dedukti</title><categories>cs.LO</categories><comments>In Proceedings PxTP 2015, arXiv:1507.08375</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 186, 2015, pp. 74-88</journal-ref><doi>10.4204/EPTCS.186.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dedukti is a logical framework based on the lambda-Pi-calculus modulo
rewriting, which extends the lambda-Pi-calculus with rewrite rules. In this
paper, we show how to translate the proofs of a family of HOL proof assistants
to Dedukti. The translation preserves binding, typing, and reduction. We
implemented this translation in an automated tool and used it to successfully
translate the OpenTheory standard library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08721</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08721</id><created>2015-07-30</created><authors><author><keyname>Assaf</keyname><forenames>Ali</forenames><affiliation>Inria, Ecole Polytechnique</affiliation></author><author><keyname>Cauderlier</keyname><forenames>Rapha&#xeb;l</forenames><affiliation>Inria, CNAM</affiliation></author></authors><title>Mixing HOL and Coq in Dedukti (Extended Abstract)</title><categories>cs.LO</categories><comments>In Proceedings PxTP 2015, arXiv:1507.08375</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 186, 2015, pp. 89-96</journal-ref><doi>10.4204/EPTCS.186.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use Dedukti as a logical framework for interoperability. We use automated
tools to translate different developments made in HOL and in Coq to Dedukti,
and we combine them to prove new results. We illustrate our approach with a
concrete example where we instantiate a sorting algorithm written in Coq with
the natural numbers of HOL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08728</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08728</id><created>2015-07-30</created><updated>2016-01-25</updated><authors><author><keyname>Huang</keyname><forenames>Liang-Hao</forenames></author><author><keyname>Hsu</keyname><forenames>Hsiang-Chun</forenames></author><author><keyname>Shen</keyname><forenames>Shan-Hsiang</forenames></author><author><keyname>Yang</keyname><forenames>De-Nian</forenames></author><author><keyname>Chen</keyname><forenames>Wen-Tsuen</forenames></author></authors><title>Multicast Traffic Engineering for Software-Defined Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although Software-Defined Networking (SDN) enables flexible network resource
allocations for traffic engineering, current literature mostly focuses on
unicast communications. Compared to traffic engineering for multiple unicast
flows, multicast traffic engineering for multiple trees is very challenging not
only because minimizing the bandwidth consumption of a single multicast tree by
solving the Steiner tree problem is already NP-Hard, but the Steiner tree
problem does not consider the link capacity constraint for multicast flows and
node capacity constraint to store the forwarding entries in Group Table of
OpenFlow. In this paper, therefore, we first study the hardness results of
scalable multicast traffic engineering in SDN. We prove that scalable multicast
traffic engineering with only the node capacity constraint is NP-Hard and not
approximable within, which is the number of destinations in the largest
multicast group. We then prove that scalable multicast traffic engineering with
both the node and link capacity constraints is NP-Hard and not approximable
within any ratio. To solve the problem, we design an approximation algorithm,
named Multi-Tree Routing and State Assignment Algorithm (MTRSA), for the first
case and extend it to the general multicast traffic engineering problem. The
simulation and implementation results demonstrate that the solutions obtained
by the proposed algorithm outperform the shortest-path trees and Steiner trees.
Most importantly, MTRSA is computation-efficient and can be deployed in SDN
since it can generate the solution with numerous trees in a short time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08733</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08733</id><created>2015-07-30</created><authors><author><keyname>Yamamoto</keyname><forenames>Hirosuke</forenames></author><author><keyname>Tsuchihashi</keyname><forenames>Masato</forenames></author><author><keyname>Honda</keyname><forenames>Junya</forenames></author></authors><title>Almost Instantaneous Fix-to-Variable Length Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory in October
  2014, and revised in July 205</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose almost instantaneous fixed-to-variable-length (AIFV) codes such
that two (resp. $K-1$) code trees are used if code symbols are binary (resp.
$K$-ary for $K \geq 3$), and source symbols are assigned to incomplete internal
nodes in addition to leaves. Although the AIFV codes are not instantaneous
codes, they are devised such that the decoding delay is at most two bits (resp.
one code symbol) in the case of binary (resp. $K$-ary) code alphabet. The AIFV
code can attain better average compression rate than the Huffman code at the
expenses of a little decoding delay and a little large memory size to store
multiple code trees. We also show for the binary and ternary AIFV codes that
the optimal AIFV code can be obtained by solving 0-1 integer programming
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08736</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08736</id><created>2015-07-30</created><authors><author><keyname>Odaibo</keyname><forenames>Stephen G.</forenames></author></authors><title>A Sinc Wavelet Describes the Receptive Fields of Neurons in the Motion
  Cortex</title><categories>q-bio.NC cs.CV cs.IT math.IT physics.bio-ph</categories><comments>This work was presented in part at the 44th Annual Meeting of the
  Society for Neuroscience in Washington, DC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual perception results from a systematic transformation of the information
flowing through the visual system. In the neuronal hierarchy, the response
properties of single neurons are determined by neurons located one level below,
and in turn, determine the responses of neurons located one level above.
Therefore in modeling receptive fields, it is essential to ensure that the
response properties of neurons in a given level can be generated by combining
the response models of neurons in its input levels. However, existing response
models of neurons in the motion cortex do not inherently yield the temporal
frequency filtering gradient (TFFG) property that is known to emerge along the
primary visual cortex (V1) to middle temporal (MT) motion processing stream.
TFFG is the change from predominantly lowpass to predominantly bandpass
temporal frequency filtering character along the V1 to MT pathway (Foster et al
1985; DeAngelis et al 1993; Hawken et al 1996). We devised a new model, the
sinc wavelet model (Odaibo, 2014), which logically and efficiently generates
the TFFG. The model replaces the Gabor function's sine wave carrier with a sinc
(sin(x)/x) function, and has the same or fewer number of parameters as existing
models. Because of its logical consistency with the emergent network property
of TFFG, we conclude that the sinc wavelet is a better model for the receptive
fields of motion cortex neurons. This model will provide new physiological
insights into how the brain represents visual information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08750</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08750</id><created>2015-07-31</created><updated>2015-12-21</updated><authors><author><keyname>Oh</keyname><forenames>Junhyuk</forenames></author><author><keyname>Guo</keyname><forenames>Xiaoxiao</forenames></author><author><keyname>Lee</keyname><forenames>Honglak</forenames></author><author><keyname>Lewis</keyname><forenames>Richard</forenames></author><author><keyname>Singh</keyname><forenames>Satinder</forenames></author></authors><title>Action-Conditional Video Prediction using Deep Networks in Atari Games</title><categories>cs.LG cs.AI cs.CV</categories><comments>Published at NIPS 2015 (Advances in Neural Information Processing
  Systems 28)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by vision-based reinforcement learning (RL) problems, in particular
Atari games from the recent benchmark Aracade Learning Environment (ALE), we
consider spatio-temporal prediction problems where future (image-)frames are
dependent on control variables or actions as well as previous frames. While not
composed of natural scenes, frames in Atari games are high-dimensional in size,
can involve tens of objects with one or more objects being controlled by the
actions directly and many other objects being influenced indirectly, can
involve entry and departure of objects, and can involve deep partial
observability. We propose and evaluate two deep neural network architectures
that consist of encoding, action-conditional transformation, and decoding
layers based on convolutional neural networks and recurrent neural networks.
Experimental results show that the proposed architectures are able to generate
visually-realistic frames that are also useful for control over approximately
100-step action-conditional futures in some games. To the best of our
knowledge, this paper is the first to make and evaluate long-term predictions
on high-dimensional video conditioned by control inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08751</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08751</id><created>2015-07-31</created><updated>2015-08-04</updated><authors><author><keyname>Ong</keyname><forenames>Frank</forenames></author><author><keyname>Lustig</keyname><forenames>Michael</forenames></author></authors><title>Beyond Low Rank + Sparse: Multi-scale Low Rank Matrix Decomposition</title><categories>cs.SY cs.IT cs.NA math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low rank methods allow us to capture globally correlated components within
matrices. The recent low rank + sparse decomposition further enables us to
extract sparse entries along with the globally correlated components. In this
paper, we present a natural generalization and consider the decomposition of
matrices into components of multiple scales. Such decomposition is well
motivated in practice as data matrices often exhibit local correlations in
multiple scales. Concretely, we propose a multi-scale low rank modeling that
represents a data matrix as a sum of block-wise low rank matrices with
increasing scales of block sizes. We then consider the inverse problem of
decomposing the data matrix into its multi-scale low rank components and
approach the problem via a convex formulation. Theoretically, we show that
under an incoherence condition, the convex program recovers the multi-scale low
rank components exactly. Practically, we provide guidance on selecting the
regularization parameters and incorporate cycle spinning to reduce blocking
artifacts. Experimentally, we show that the multi-scale low rank decomposition
provides a more intuitive decomposition than conventional low rank methods and
demonstrate its effectiveness in four applications, including illumination
normalization for face images, motion separation for surveillance videos,
multi-scale modeling of the dynamic contrast enhanced magnetic resonance
imaging and collaborative filtering exploiting age information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08752</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08752</id><created>2015-07-31</created><authors><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>An Optimal Algorithm for Bandit and Zero-Order Convex Optimization with
  Two-Point Feedback</title><categories>cs.LG math.OC stat.ML</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the closely related problems of bandit convex optimization with
two-point feedback, and zero-order stochastic convex optimization with two
function evaluations per round. We provide a simple algorithm and analysis
which is optimal for convex Lipschitz functions. This improves on
\cite{dujww13}, which only provides an optimal result for smooth functions;
Moreover, the algorithm and analysis are simpler, and readily extend to
non-Euclidean problems. The algorithm is based on a small but surprisingly
powerful modification of the gradient estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08754</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08754</id><created>2015-07-31</created><authors><author><keyname>Wu</keyname><forenames>Fa</forenames></author><author><keyname>Hu</keyname><forenames>Peijun</forenames></author><author><keyname>Kong</keyname><forenames>Dexing</forenames></author></authors><title>Flip-Rotate-Pooling Convolution and Split Dropout on Convolution Neural
  Networks for Image Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new version of Dropout called Split Dropout (sDropout)
and rotational convolution techniques to improve CNNs' performance on image
classification. The widely used standard Dropout has advantage of preventing
deep neural networks from overfitting by randomly dropping units during
training. Our sDropout randomly splits the data into two subsets and keeps both
rather than discards one subset. We also introduce two rotational convolution
techniques, i.e. rotate-pooling convolution (RPC) and flip-rotate-pooling
convolution (FRPC) to boost CNNs' performance on the robustness for rotation
transformation. These two techniques encode rotation invariance into the
network without adding extra parameters. Experimental evaluations on
ImageNet2012 classification task demonstrate that sDropout not only enhances
the performance but also converges faster. Additionally, RPC and FRPC make CNNs
more robust for rotation transformations. Overall, FRPC together with sDropout
bring $1.18\%$ (model of Zeiler and Fergus~\cite{zeiler2013visualizing},
10-view, top-1) accuracy increase in ImageNet 2012 classification task compared
to the original network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08761</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08761</id><created>2015-07-31</created><authors><author><keyname>Shahroudy</keyname><forenames>Amir</forenames></author><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Ng</keyname><forenames>Tian-Tsong</forenames></author><author><keyname>Yang</keyname><forenames>Qingxiong</forenames></author></authors><title>Multimodal Multipart Learning for Action Recognition in Depth Videos</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The articulated and complex nature of human actions makes the task of action
recognition difficult. One approach to handle this complexity is dividing it to
the kinetics of body parts and analyzing the actions based on these partial
descriptors. We propose a joint sparse regression based learning method which
utilizes the structured sparsity to model each action as a combination of
multimodal features from a sparse set of body parts. To represent dynamics and
appearance of parts, we employ a heterogeneous set of depth and skeleton based
features. The proper structure of multimodal multipart features are formulated
into the learning framework via the proposed hierarchical mixed norm, to
regularize the structured features of each part and to apply sparsity between
them, in favor of a group feature selection. Our experimental results expose
the effectiveness of the proposed learning method in which it outperforms other
methods in all three tested datasets while saturating one of them by achieving
perfect accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08781</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08781</id><created>2015-07-31</created><authors><author><keyname>Yaroslavsky</keyname><forenames>L.</forenames></author></authors><title>Can compressed sensing beat the Nyquist sampling rate?</title><categories>cs.IT math.IT physics.optics</categories><journal-ref>Opt. Eng. 54(7) 079701 (2015)</journal-ref><doi>10.1117/1.OE.54.7.079701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data saving capability of &quot;Compressed sensing (sampling)&quot; in signal
discretization is disputed and found to be far below the theoretical upper
bound defined by the signal sparsity. On a simple and intuitive example, it is
demonstrated that, in a realistic scenario for signals that are believed to be
sparse, one can achieve a substantially larger saving than compressing sensing
can. It is also shown that frequent assertions in the literature that
&quot;Compressed sensing&quot; can beat the Nyquist sampling approach are misleading
substitution of terms and are rooted in misinterpretation of the sampling
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08788</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08788</id><created>2015-07-31</created><authors><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>Fast Stochastic Algorithms for SVD and PCA: Convergence Properties and
  Convexity</title><categories>cs.LG cs.NA math.NA math.OC stat.ML</categories><comments>35 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the convergence properties of the VR-PCA algorithm introduced by
\cite{shamir2015stochastic} for fast computation of leading singular vectors.
We prove several new results, including a formal analysis of a block version of
the algorithm, and convergence from random initialization. We also make a few
observations of independent interest, such as how pre-initializing with just a
single exact power iteration can significantly improve the runtime of
stochastic methods, and what are the convexity and non-convexity properties of
the underlying optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08792</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08792</id><created>2015-07-31</created><updated>2015-12-31</updated><authors><author><keyname>Sandeep</keyname><forenames>R. B.</forenames></author><author><keyname>Sivadasan</keyname><forenames>Naveen</forenames></author></authors><title>A cubic vertex kernel for Diamond-free Edge Deletion and more</title><categories>cs.DS</categories><comments>A preliminary version of this paper has appeared in the proceedings
  of IPEC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A diamond is a graph obtained by removing an edge from a complete graph on
four vertices. A graph is diamond-free if it does not contain an induced
diamond. The Diamond-free Edge Deletion problem asks whether there exist at
most $k$ edges in the input graph $G$ whose deletion results in a diamond-free
graph. For this problem, a polynomial kernel of $O(k^4$) vertices was found by
Fellows et. al. (Discrete Optimization, 2011).
  In this paper, we give an improved kernel of $O(k^3)$ vertices for
Diamond-free Edge Deletion. Further, we give an $O(k^2)$ vertex kernel for a
related problem {Diamond,K_t}-free Edge Deletion, where $t\geq 4$ is any fixed
integer. To complement our results, we prove that these problems are
NP-complete even for $K_4$-free graphs and can be solved neither in
subexponential time (i.e., $2^{o(|G|)}$) nor in parameterized subexponential
time (i.e., $2^{o(k)}\cdot |G|^{O(1)}$), unless Exponential Time Hypothesis
fails. Our reduction implies the hardness and lower bound for a general class
of problems, where these problems come as a special case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08799</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08799</id><created>2015-07-31</created><updated>2015-09-30</updated><authors><author><keyname>Mandery</keyname><forenames>Christian</forenames></author><author><keyname>Borr&#xe0;s</keyname><forenames>J&#xfa;lia</forenames></author><author><keyname>J&#xf6;chner</keyname><forenames>Mirjam</forenames></author><author><keyname>Asfour</keyname><forenames>Tamim</forenames></author></authors><title>Analyzing Whole-Body Pose Transitions in Multi-Contact Motions</title><categories>cs.RO</categories><comments>8 pages, IEEE-RAS International Conference on Humanoid Robots
  (Humanoids) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When executing whole-body motions, humans are able to use a large variety of
support poses which not only utilize the feet, but also hands, knees and elbows
to enhance stability. While there are many works analyzing the transitions
involved in walking, very few works analyze human motion where more complex
supports occur.
  In this work, we analyze complex support pose transitions in human motion
involving locomotion and manipulation tasks (loco-manipulation). We have
applied a method for the detection of human support contacts from motion
capture data to a large-scale dataset of loco-manipulation motions involving
multi-contact supports, providing a semantic representation of them. Our
results provide a statistical analysis of the used support poses, their
transitions and the time spent in each of them. In addition, our data partially
validates our taxonomy of whole-body support poses presented in our previous
work.
  We believe that this work extends our understanding of human motion for
humanoids, with a long-term objective of developing methods for autonomous
multi-contact motion planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08805</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08805</id><created>2015-07-31</created><updated>2016-03-08</updated><authors><author><keyname>Batselier</keyname><forenames>Kim</forenames></author><author><keyname>Wong</keyname><forenames>Ngai</forenames></author></authors><title>A constructive arbitrary-degree Kronecker product decomposition of
  tensors</title><categories>math.NA cs.NA</categories><comments>Rewrote the paper completely and generalized everything to tensors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the tensor Kronecker product singular value decomposition~(TKPSVD)
that decomposes a real $k$-way tensor $\mathcal{A}$ into a linear combination
of tensor Kronecker products with an arbitrary number of $d$ factors
$\mathcal{A} = \sum_{j=1}^R \sigma_j\, \mathcal{A}^{(d)}_j \otimes \cdots
\otimes \mathcal{A}^{(1)}_j$. We generalize the matrix Kronecker product to
tensors such that each factor $\mathcal{A}^{(i)}_j$ in the TKPSVD is a $k$-way
tensor. The algorithm relies on reshaping and permuting the original tensor
into a $d$-way tensor, after which a polyadic decomposition with orthogonal
rank-1 terms is computed. We prove that for many different structured tensors,
the Kronecker product factors $\mathcal{A}^{(1)}_j,\ldots,\mathcal{A}^{(d)}_j$
are guaranteed to inherit this structure. In addition, we introduce the new
notion of general symmetric tensors, which includes many different structures
such as symmetric, persymmetric, centrosymmetric, Toeplitz and Hankel tensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08810</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08810</id><created>2015-07-31</created><authors><author><keyname>Hu</keyname><forenames>Peng</forenames></author></authors><title>A System Architecture for Software-Defined Industrial Internet of Things</title><categories>cs.NI</categories><comments>To be published by IEEE ICUWB-2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Wireless sensor networks have been a driving force of the Industrial Internet
of Things (IIoT) advancement in the process control and manufacturing industry.
The emergence of IIoT opens great potential for the ubiquitous field device
connectivity and manageability with an integrated and standardized architecture
from low-level device operations to high-level data-centric application
interactions. This technological development requires software definability in
the key architectural elements of IIoT, including wireless field devices, IIoT
gateways, network infrastructure, and IIoT sensor cloud services. In this
paper, a novel software-defined IIoT (SD-IIoT) is proposed in order to solve
essential challenges in a holistic IIoT system, such as reliability, security,
timeliness scalability, and quality of service (QoS). A new IIoT system
architecture is proposed based on the latest networking technologies such as
WirelessHART, WebSocket, IETF constrained application protocol (CoAP) and
software-defined networking (SDN). A new scheme based on CoAP and SDN is
proposed to solve the QoS issues. Computer experiments in a case study are
implemented to show the effectiveness of the proposed system architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08818</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08818</id><created>2015-07-31</created><updated>2015-11-16</updated><authors><author><keyname>Garcia-Gasulla</keyname><forenames>D.</forenames></author><author><keyname>B&#xe9;jar</keyname><forenames>J.</forenames></author><author><keyname>Cort&#xe9;s</keyname><forenames>U.</forenames></author><author><keyname>Ayguad&#xe9;</keyname><forenames>E.</forenames></author><author><keyname>Labarta</keyname><forenames>J.</forenames></author></authors><title>A Visual Embedding for the Unsupervised Extraction of Abstract Semantics</title><categories>cs.CV cs.LG cs.NE</categories><comments>9 pages, 4 figures, submitted to ICLR'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector-space word representations obtained from neural network models have
been shown to enable semantic operations based on vector arithmetic. In this
paper, we explore the existence of similar information on vector
representations of images. For that purpose we define a methodology to obtain
large, sparse vector representations of image classes, and generate vectors
through the state-of-the-art deep learning architecture GoogLeNet for 20K
images obtained from ImageNet. We first evaluate the resultant vector-space
semantics through its correlation with WordNet distances, and find vector
distances to be strongly correlated with linguistic semantics. We then explore
the location of images within the vector space, finding elements close in
WordNet to be clustered together, regardless of significant visual variances
(e.g., 118 dog types). More surprisingly, we find that the space unsupervisedly
separates complex classes without prior knowledge (e.g., living things).
Finally, we consider vector arithmetics, and find them to be related with image
concatenation (e.g., &quot;Horse cart - Horse = Rickshaw&quot;), image overlap (&quot;Panda -
Brown bear = Skunk&quot;) and regularities (&quot;Panda is to Brown bear as Soccer ball
is to Helmet&quot;). These results indicate that image vector embeddings as the one
proposed here contain rich visual semantics usable for learning and reasoning
purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08822</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08822</id><created>2015-07-31</created><updated>2016-01-28</updated><authors><author><keyname>Tsitsvero</keyname><forenames>Mikhail</forenames></author><author><keyname>Barbarossa</keyname><forenames>Sergio</forenames></author><author><keyname>Di Lorenzo</keyname><forenames>Paolo</forenames></author></authors><title>Signals on Graphs: Uncertainty Principle and Sampling</title><categories>cs.DM math.SP</categories><comments>This article is the revised version submitted to the IEEE
  Transactions on Signal Processing on January, 2016; original manuscript was
  submitted on July, 2015. The work includes 16 pages, 9 figures. In the
  revised version we extended Introduction, Section III on uncertainty
  principle and Appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications of current interest, the observations are represented as
a signal defined over a graph. The analysis of such signals requires the
extension of standard signal processing tools. In this work we start providing
a class of graph signals maximally concentrated in the graph domain and its
dual. Then building on this framework, we derive an uncertainty principle for
graph signals and illustrate the conditions for the recovery of band-limited
signals from a subset of their values. We show an interesting link between
uncertainty principle and sampling and propose alternative signal recovery
algorithms. Then, after showing that samples' location plays a key role in the
performance of signal recovery algorithms, we suggest and compare a few
alternative sampling strategies. Finally, we provide the conditions for perfect
recovery of a useful signal corrupted by sparse noise, showing that this
problem is also intrinsically related to graph-frequency localization
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08826</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08826</id><created>2015-07-31</created><authors><author><keyname>Brunelli</keyname><forenames>Matteo</forenames></author></authors><title>Extending the axioms of inconsistency indices for pairwise comparisons</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pairwise comparisons between alternatives are a well-established tool to
decompose decision problems into smaller and more easily tractable
sub-problems. However, due to our limited rationality, the subjective
preferences expressed by decision makers over pairs of alternatives can hardly
ever be consistent. Therefore, several inconsistency indices have been proposed
in the literature to quantify the extent of the deviation from complete
consistency. Only recently, a set of axioms has been proposed to define a
family of functions representing inconsistency indices. The scope of this paper
is to expand the axiomatic framework by adding and justifying a new axiom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08834</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08834</id><created>2015-07-31</created><authors><author><keyname>Keller</keyname><forenames>Matthias</forenames></author><author><keyname>Karl</keyname><forenames>Holger</forenames></author></authors><title>Response-Time-Optimised Service Deployment: MILP Formulations of
  Piece-wise Linear Functions Approximating Non-linear Bivariate Mixed-integer
  Functions</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A current trend in networking and cloud computing is to provide compute
resources at widely dispersed places; this is exemplified by developments such
as Network Function Virtualisation. This paves the way for wide-area service
deployments with improved service quality: e.g, a nearby server can reduce the
user-perceived response times. But always using the nearest server can be a bad
decision if that server is already highly utilised. This paper formalises the
two related problems of allocating resources at different locations and
assigning users to them with the goal of minimising the response times for a
given number of resources to use -- a non-linear capacitated facility location
problem with integrated queuing systems. To efficiently handle the
non-linearity, we introduce five linear problem approximations and adapt the
currently best heuristic for a similar problem to our scenario. All six
approaches are compared in experiments for solution quality and solving time.
Surprisingly, our best optimisation formulation outperforms the heuristic in
both time and quality. Additionally, we evaluate the influence ot resource
distributions in the network on the response time: Cut by half for some
configurations. The presented formulations are applicable to a broader
optimisation domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08838</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08838</id><created>2015-07-31</created><authors><author><keyname>Esiner</keyname><forenames>Ertem</forenames></author><author><keyname>Datta</keyname><forenames>Anwitaman</forenames></author></authors><title>Auditable Versioned Data Storage Outsourcing</title><categories>cs.CR cs.DS</categories><comments>31 Pages, 12 Figures, 2 Tables, 4 Pseudocodes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Auditability is crucial for data outsourcing, facilitating accountability and
identifying data loss or corruption incidents in a timely manner, reducing in
turn the risks from such losses. In recent years, in synch with the growing
trend of outsourcing, a lot of progress has been made in designing
probabilistic (for efficiency) provable data possession (PDP) schemes. However,
even the recent and advanced PDP solutions that do deal with dynamic data, do
so in a limited manner, and for only the latest version of the data. A naive
solution treating different versions in isolation would work, but leads to
tremendous overheads, and is undesirable. In this paper, we present algorithms
to achieve full persistence (all intermediate configurations are preserved and
are modifiable) for an optimized skip list (known as FlexList) so that
versioned data can be audited. The proposed scheme provides deduplication at
the level of logical, variable sized blocks, such that only the altered parts
of the different versions are kept, while the persistent data-structure
facilitates access (read) of any arbitrary version with the same storage and
process efficiency that state-of-the-art dynamic PDP solutions provide for only
the current version, while commit (write) operations incur around 5% additional
time. Furthermore, the time overhead for auditing arbitrary versions in
addition to the latest version is imperceptible even on a low-end server...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08847</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08847</id><created>2015-07-31</created><authors><author><keyname>Yanga</keyname><forenames>Jiachen</forenames></author><author><keyname>Dinga</keyname><forenames>Zhiyong</forenames></author><author><keyname>Guoa</keyname><forenames>Fei</forenames></author><author><keyname>Wanga</keyname><forenames>Huogen</forenames></author><author><keyname>Hughesb</keyname><forenames>Nick</forenames></author></authors><title>A novel multivariate performance optimization method based on sparse
  coding and hyper-predictor learning</title><categories>cs.LG cs.CV cs.NA</categories><doi>10.1016/j.neunet.2015.07.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of optimization multivariate
performance measures, and propose a novel algorithm for it. Different from
traditional machine learning methods which optimize simple loss functions to
learn prediction function, the problem studied in this paper is how to learn
effective hyper-predictor for a tuple of data points, so that a complex loss
function corresponding to a multivariate performance measure can be minimized.
We propose to present the tuple of data points to a tuple of sparse codes via a
dictionary, and then apply a linear function to compare a sparse code against a
give candidate class label. To learn the dictionary, sparse codes, and
parameter of the linear function, we propose a joint optimization problem. In
this problem, the both the reconstruction error and sparsity of sparse code,
and the upper bound of the complex loss function are minimized. Moreover, the
upper bound of the loss function is approximated by the sparse codes and the
linear function parameter. To optimize this problem, we develop an iterative
algorithm based on descent gradient methods to learn the sparse codes and
hyper-predictor parameter alternately. Experiment results on some benchmark
data sets show the advantage of the proposed methods over other
state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08861</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08861</id><created>2015-07-31</created><authors><author><keyname>&#xc7;al&#x131;&#x15f;&#x131;r</keyname><forenames>Fatih</forenames></author><author><keyname>Ulusoy</keyname><forenames>&#xd6;zg&#xfc;r</forenames></author><author><keyname>G&#xfc;d&#xfc;kbay</keyname><forenames>U&#x11f;ur</forenames></author><author><keyname>Ba&#x15f;tan</keyname><forenames>Muhammet</forenames></author></authors><title>Mobile Multi-View Object Image Search</title><categories>cs.MM cs.CV</categories><comments>14 pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High user interaction capability of mobile devices can help improve the
accuracy of mobile visual search systems. At query time, it is possible to
capture multiple views of an object from different viewing angles and at
different scales with the mobile device camera to obtain richer information
about the object compared to a single view and hence return more accurate
results. Motivated by this, we developed a mobile multi-view object image
search system, using a client-server architecture. Multi-view images of objects
acquired by the mobile clients are processed and local features are sent to the
server, which combines the query image representations with early/late fusion
methods based on bag-of-visual-words and sends back the query results. We
performed a comprehensive analysis of early and late fusion approaches using
various similarity functions, on an existing single view and a new multi-view
object image database. The experimental results show that multi-view search
provides significantly better retrieval accuracy compared to single view
search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08863</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08863</id><created>2015-07-31</created><authors><author><keyname>Sabatini</keyname><forenames>Fabio</forenames></author><author><keyname>Sarracino</keyname><forenames>Francesco</forenames></author></authors><title>Keeping up with the e-Joneses: Do online social networks raise social
  comparisons?</title><categories>cs.CY cs.SI physics.soc-ph q-fin.EC</categories><comments>25 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social networks such as Facebook disclose an unprecedented volume of
personal information amplifying the occasions for social comparisons. We test
the hypothesis that the use of social networking sites (SNS) increases people's
dissatisfaction with their income. After addressing endogeneity issues, our
results suggest that SNS users have a higher probability to compare their
achievements with those of others. This effect seems stronger than the one
exerted by TV watching, it is particularly strong for younger people, and it
affects men and women in a similar way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08874</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08874</id><created>2015-07-31</created><updated>2016-02-05</updated><authors><author><keyname>Marciel</keyname><forenames>Miriam</forenames></author><author><keyname>Cuevas</keyname><forenames>Ruben</forenames></author><author><keyname>Banchs</keyname><forenames>Albert</forenames></author><author><keyname>Gonzalez</keyname><forenames>Roberto</forenames></author><author><keyname>Traverso</keyname><forenames>Stefano</forenames></author><author><keyname>Ahmed</keyname><forenames>Mohamed</forenames></author><author><keyname>Azcorra</keyname><forenames>Arturo</forenames></author></authors><title>Understanding the Detection of View Fraud in Video Content Portals</title><categories>cs.CY</categories><comments>To appear in WWW 2016, Montr\'eal, Qu\'ebec, Canada. Please cite the
  conference version of this paper</comments><doi>10.1145/2872427.2882980l</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While substantial effort has been devoted to understand fraudulent activity
in traditional online advertising (search and banner), more recent forms such
as video ads have received little attention. The understanding and
identification of fraudulent activity (i.e., fake views) in video ads for
advertisers, is complicated as they rely exclusively on the detection
mechanisms deployed by video hosting portals. In this context, the development
of independent tools able to monitor and audit the fidelity of these systems
are missing today and needed by both industry and regulators.
  In this paper we present a first set of tools to serve this purpose. Using
our tools, we evaluate the performance of the audit systems of five major
online video portals. Our results reveal that YouTube's detection system
significantly outperforms all the others. Despite this, a systematic evaluation
indicates that it may still be susceptible to simple attacks. Furthermore, we
find that YouTube penalizes its videos' public and monetized view counters
differently, the former being more aggressive. This means that views identified
as fake and discounted from the public view counter are still monetized. We
speculate that even though YouTube's policy puts in lots of effort to
compensate users after an attack is discovered, this practice places the burden
of the risk on the advertisers, who pay to get their ads displayed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08879</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08879</id><created>2015-07-31</created><updated>2015-11-18</updated><authors><author><keyname>Liotta</keyname><forenames>Giuseppe</forenames></author><author><keyname>Montecchiani</keyname><forenames>Fabrizio</forenames></author></authors><title>L-Visibility Drawings of IC-planar Graphs</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An IC-plane graph is a topological graph where every edge is crossed at most
once and no two crossed edges share a vertex. We show that every IC-plane graph
has a visibility drawing where every vertex is an L-shape, and every edge is
either a horizontal or vertical segment. As a byproduct of our drawing
technique, we prove that an IC-plane graph has a RAC drawing in quadratic area
with at most two bends per edge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08888</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08888</id><created>2015-07-31</created><authors><author><keyname>Kuramitsu</keyname><forenames>Kimio</forenames></author></authors><title>Service Dependability with Continuously Revised Assurance Cases by
  Multiple Stakeholders: A Case Study</title><categories>cs.SE</categories><comments>Submitted to Journal of Information Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, assurance cases have received much attentions in the field of
software-based computer systems and IT services. However, software very often
changes and there are no strong regulations for software. These facts are main
two challenges to be addressed in software assurance cases. We propose a
development method of assurance cases by means of continuous revision at every
stage of the system lifecycle, including in-operation and service recovery in
failure cases. The quality of dependability arguments are improved by multiple
stakeholders who check with each other. This paper reported our experience of
the proposed method in a case of the ASPEN education service. The case study
demonstrate that the continuos updates create a significant amount of active
risk communications between stakeholders. This gives us a promising perspective
for the long-term improvement of service dependability with the lifecycle
assurance cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08903</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08903</id><created>2015-07-31</created><authors><author><keyname>Kamalapurkar</keyname><forenames>Rushikesh</forenames></author><author><keyname>Reish</keyname><forenames>Ben</forenames></author><author><keyname>Chowdhary</keyname><forenames>Girish</forenames></author><author><keyname>Dixon</keyname><forenames>Warren E.</forenames></author></authors><title>Concurrent learning for parameter estimation using dynamic
  state-derivative estimators</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A concurrent learning (CL)-based parameter estimator is developed to identify
the unknown parameters in a linearly parameterized uncertain control-affine
nonlinear system. Unlike state-of-the-art CL techniques that assume knowledge
of the state-derivative or rely on numerical smoothing, CL is implemented using
a dynamic state-derivative estimator. A novel purging algorithm is introduced
to discard possibly erroneous data recorded during the transient phase for
concurrent learning. Since purging results in a discontinuous parameter
adaptation law, the closed-loop error system is modeled as a switched system.
Asymptotic convergence of the error states to the origin is established under a
persistent excitation condition, and the error states are shown to be
ultimately bounded under a finite excitation condition. Simulation results are
provided to demonstrate the effectiveness of the developed parameter estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08905</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08905</id><created>2015-07-31</created><updated>2015-10-15</updated><authors><author><keyname>Wang</keyname><forenames>Zhaowen</forenames></author><author><keyname>Liu</keyname><forenames>Ding</forenames></author><author><keyname>Yang</keyname><forenames>Jianchao</forenames></author><author><keyname>Han</keyname><forenames>Wei</forenames></author><author><keyname>Huang</keyname><forenames>Thomas</forenames></author></authors><title>Deep Networks for Image Super-Resolution with Sparse Prior</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning techniques have been successfully applied in many areas of
computer vision, including low-level image restoration problems. For image
super-resolution, several models based on deep neural networks have been
recently proposed and attained superior performance that overshadows all
previous handcrafted models. The question then arises whether large-capacity
and data-driven models have become the dominant solution to the ill-posed
super-resolution problem. In this paper, we argue that domain expertise
represented by the conventional sparse coding model is still valuable, and it
can be combined with the key ingredients of deep learning to achieve further
improved results. We show that a sparse coding model particularly designed for
super-resolution can be incarnated as a neural network, and trained in a
cascaded structure from end to end. The interpretation of the network based on
sparse coding leads to much more efficient and effective training, as well as a
reduced model size. Our model is evaluated on a wide range of images, and shows
clear advantage over existing state-of-the-art methods in terms of both
restoration accuracy and human subjective quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08906</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08906</id><created>2015-07-30</created><updated>2015-09-23</updated><authors><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author><author><keyname>Granqvist</keyname><forenames>Claes G.</forenames></author><author><keyname>Khatri</keyname><forenames>Sunil P.</forenames></author><author><keyname>Peper</keyname><forenames>Ferdinand</forenames></author></authors><title>Zero and negative energy dissipation at information-theoretic erasure</title><categories>cs.ET</categories><comments>accepted for publication and is in press at the Journal of
  Computational Electronics</comments><journal-ref>Journal of Computational Electronics 14 (2015) 1-5</journal-ref><doi>10.1007/s10825-015-0754-5</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce information-theoretic erasure based on Shannon's binary channel
formula. It is pointed out that this type of erasure is a natural
energy-dissipation-free way in which information is lost in
double-potential-well memories, and it may be the reason why the brain can
forget things effortlessly. We also demonstrate a new non-volatile,
charge-based memory scheme wherein the erasure can be associated with even
negative energy dissipation; this implies that the memory's environment is
cooled during information erasure and contradicts Landauer's principle of
erasure dissipation. On the other hand, writing new information into the memory
always requires positive energy dissipation in our schemes. Finally, we show a
simple system where even a classical erasure process yields negative energy
dissipation of arbitrarily large energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08917</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08917</id><created>2015-07-31</created><authors><author><keyname>Hammouda</keyname><forenames>Marwan</forenames></author><author><keyname>Akin</keyname><forenames>Sami</forenames></author><author><keyname>Peissig</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Effective Capacity in Multiple Access Channels with Arbitrary Inputs</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a two-user multiple access fading channel under
quality-of-service (QoS) constraints. We initially formulate the transmission
rates for both transmitters, where the transmitters have arbitrarily
distributed input signals. We assume that the receiver performs successive
decoding with a certain order. Then, we establish the effective capacity region
that provides the maximum allowable sustainable arrival rate region at the
transmitters' buffers under QoS guarantees. Assuming limited transmission power
budgets at the transmitters, we attain the power allocation policies that
maximize the effective capacity region. As for the decoding order at the
receiver, we characterize the optimal decoding order regions in the plane of
channel fading parameters for given power allocation policies. In order to
accomplish the aforementioned objectives, we make use of the relationship
between the minimum mean square error and the first derivative of the mutual
information with respect to the power allocation policies. Through numerical
results, we display the impact of input signal distributions on the effective
capacity region performance of this two-user multiple access fading channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08920</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08920</id><created>2015-07-31</created><updated>2015-10-07</updated><authors><author><keyname>Chen</keyname><forenames>Xiaomin</forenames></author><author><keyname>Akinyemi</keyname><forenames>Ibukunoluwa</forenames></author><author><keyname>Yang</keyname><forenames>Shuang-Hua</forenames></author></authors><title>A control theoretic approach to achieve proportional fairness in 802.11e
  EDCA WLANs</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers proportional fairness amongst ACs in an EDCA WLAN for
provision of distinct QoS requirements and priority parameters. A detailed
theoretical analysis is provided to derive the optimal station attempt
probability which leads to a proportional fair allocation of station
throughputs. The desirable fairness can be achieved using a centralised
adaptive control approach. This approach is based on multivariable statespace
control theory and uses the Linear Quadratic Integral (LQI) controller to
periodically update CWmin till the optimal fair point of operation. Performance
evaluation demonstrates that the control approach has high accuracy performance
and fast convergence speed for general network scenarios. To our knowledge this
might be the first time that a closed-loop control system is designed for EDCA
WLANs to achieve proportional fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08922</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08922</id><created>2015-07-31</created><authors><author><keyname>Chen</keyname><forenames>Xiaomin</forenames></author><author><keyname>Yang</keyname><forenames>Shuang-Hua</forenames></author></authors><title>Achieving proportional fairness with a control theoretic approach in
  error-prone 802.11e WLANs</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter proposes a control theoretic approach to achieve proportional
fairness amongst access categories (ACs) in an error-prone EDCA WLAN for
provision of distinct QoS requirements and priority parameters. The approach
adaptively adjusts the minimum contention window of each AC to derive the
station attempt probability to its optimum which leads to a proportional fair
allocation of station throughputs. Evaluation results demonstrate that the
proposed control approach has high accuracy performance and fast convergence
speed for general network scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08923</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08923</id><created>2015-07-31</created><updated>2015-10-15</updated><authors><author><keyname>Kapelko</keyname><forenames>Rafa&#x142;</forenames></author><author><keyname>Kranakis</keyname><forenames>Evangelos</forenames></author></authors><title>On the Displacement for Covering a Unit Interval with Randomly Placed
  Sensors</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider $n$ mobile sensors placed independently at random with the uniform
distribution on a barrier represented as the unit line segment $[0,1]$. The
sensors have identical sensing radius, say $r$. When a sensor is displaced on
the line a distance equal to $d$ it consumes energy (in movement) which is
proportional to some (fixed) power $a &gt; 0$ of the distance $d$ traveled. The
energy consumption of a system of $n$ sensors thus displaced is defined as the
sum of the energy consumptions for the displacement of the individual sensors.
  We focus on the problem of energy efficient displacement of the sensors so
that in their final placement the sensor system ensures coverage of the barrier
and the energy consumed for the displacement of the sensors to these final
positions is minimized in expectation. In particular, we analyze the problem of
displacing the sensors from their initial positions so as to attain coverage of
the unit interval and derive trade-offs for this displacement as a function of
the sensor range. We obtain several tight bounds in this setting thus
generalizing several of the results of [12] to any power $a &gt;0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08929</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08929</id><created>2015-07-31</created><authors><author><keyname>Shayevitz</keyname><forenames>Ofer</forenames></author><author><keyname>Feder</keyname><forenames>Meir</forenames></author></authors><title>A Simple Proof for the Optimality of Randomized Posterior Matching</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Posterior matching (PM) is a sequential horizon-free feedback communication
scheme introduced by the authors, who also provided a rather involved
optimality proof showing it achieves capacity for a large class of memoryless
channels. Naghshvar et al considered a non-sequential variation of PM with a
fixed number of messages and a random decision-time, and gave a simpler proof
establishing its optimality via a novel Shannon-Jensen divergence argument.
Another simpler optimality proof was given by Li and El Gamal, who considered a
fixed-rate fixed block-length variation of PM with an additional randomization.
Both these works also provided error exponent bounds. However, their simpler
achievability proofs apply only to discrete memoryless channels, and are
restricted to a non-sequential setup with a fixed number of messages. In this
paper, we provide a short and transparent proof for the optimality of the fully
sequential horizon-free PM scheme over general memoryless channels. Borrowing
the key randomization idea of Li and El Gamal, our proof is based on analyzing
the random walk behavior of the shrinking posterior intervals induced by a
reversed iterated function system (RIFS) decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08937</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08937</id><created>2015-07-31</created><authors><author><keyname>Haring</keyname><forenames>Stefan</forenames></author><author><keyname>Hochreiter</keyname><forenames>Ronald</forenames></author></authors><title>Efficient and robust calibration of the Heston option pricing model for
  American options using an improved Cuckoo Search Algorithm</title><categories>cs.NE q-fin.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper an improved Cuckoo Search Algorithm is developed to allow for
an efficient and robust calibration of the Heston option pricing model for
American options. Calibration of stochastic volatility models like the Heston
is significantly harder than classical option pricing models as more parameters
have to be estimated. The difficult task of calibrating one of these models to
American Put options data is the main objective of this paper. Numerical
results are shown to substantiate the suitability of the chosen method to
tackle this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08940</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08940</id><created>2015-07-31</created><authors><author><keyname>Diakonova</keyname><forenames>Marina</forenames></author><author><keyname>Nicosia</keyname><forenames>Vincenzo</forenames></author><author><keyname>Latora</keyname><forenames>Vito</forenames></author><author><keyname>Miguel</keyname><forenames>Maxi San</forenames></author></authors><title>Irreducibility of multilayer network dynamics: the case of the voter
  model</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI nlin.AO</categories><comments>10 pages, 6 figures</comments><journal-ref>New J. Phys. 18, 023010 (2016)</journal-ref><doi>10.1088/1367-2630/18/2/023010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the issue of the reducibility of the dynamics on a multilayer
network to an equivalent process on an aggregated single-layer network. As a
typical example of models for opinion formation in social networks, we
implement the voter model on a two-layer multiplex network, and we study its
dynamics as a function of two control parameters, namely the fraction of edges
simultaneously existing in both layers of the network (edge overlap), and the
fraction of nodes participating in both layers (interlayer connectivity or
degree of multiplexity). We compute the asymptotic value of the number of
active links (interface density) in the thermodynamic limit, and the time to
reach an absorbing state for finite systems, and we compare the numerical
results with the analytical predictions on equivalent single-layer networks
obtained through various possible aggregation procedures. We find a large
region of parameters where the interface density of large multiplexes gives
systematic deviations from that of the aggregates. We show that neither of the
standard aggregation procedures is able to capture the highly nonlinear
increase in the lifetime of a finite size multiplex at small interlayer
connectivity. These results indicate that multiplexity should be appropriately
taken into account when studying voter model dynamics, and that, in general,
single-layer approximations might be not accurate enough to properly understand
processes occurring on multiplex networks, since they might flatten out
relevant dynamical details.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08956</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08956</id><created>2015-07-31</created><authors><author><keyname>Aly</keyname><forenames>Heba</forenames></author><author><keyname>Basalamah</keyname><forenames>Anas</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author></authors><title>Map++: A Crowd-sensing System for Automatic Map Semantics Identification</title><categories>cs.CY</categories><comments>Published in the Eleventh Annual IEEE International Conference on
  Sensing, Communication, and Networking (IEEE SECON 2014)</comments><doi>10.1109/SAHCN.2014.6990394</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital maps have become a part of our daily life with a number of commercial
and free map services. These services have still a huge potential for
enhancement with rich semantic information to support a large class of mapping
applications. In this paper, we present Map++, a system that leverages standard
cell-phone sensors in a crowdsensing approach to automatically enrich digital
maps with different road semantics like tunnels, bumps, bridges, footbridges,
crosswalks, road capacity, among others. Our analysis shows that cell-phones
sensors with humans in vehicles or walking get affected by the different road
features, which can be mined to extend the features of both free and commercial
mapping services. We present the design and implementation of Map++ and
evaluate it in a large city. Our evaluation shows that we can detect the
different semantics accurately with at most 3% false positive rate and 6% false
negative rate for both vehicle and pedestrian-based features. Moreover, we show
that Map++ has a small energy footprint on the cell-phones, highlighting its
promise as a ubiquitous digital maps enriching service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08958</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08958</id><created>2015-07-31</created><authors><author><keyname>Fedorov</keyname><forenames>Roman</forenames></author><author><keyname>Fraternali</keyname><forenames>Piero</forenames></author><author><keyname>Pasini</keyname><forenames>Chiara</forenames></author><author><keyname>Tagliasacchi</keyname><forenames>Marco</forenames></author></authors><title>SnowWatch: Snow Monitoring through Acquisition and Analysis of
  User-Generated Content</title><categories>cs.CV cs.CY</categories><comments>IEEE International Conference on Multimedia and Expo, ICME 2015.
  Accepted and presented technical demo proposal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a system for complementing snow phenomena monitoring with virtual
measurements extracted from public visual content. The proposed system
integrates an automatic acquisition and analysis of photographs and webcam
images depicting Alpine mountains. In particular, the technical demonstration
consists in a web portal that interfaces the whole system with the population.
It acts as an entertaining photo-sharing social web site, acquiring at the same
time visual content necessary for environmental monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08967</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08967</id><created>2015-07-31</created><updated>2015-09-27</updated><authors><author><keyname>Chung</keyname><forenames>Fan</forenames></author><author><keyname>Simpson</keyname><forenames>Olivia</forenames></author></authors><title>Distributed Algorithms for Finding Local Clusters Using Heat Kernel
  Pagerank</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distributed algorithm performs local computations on pieces of input and
communicates the results through given communication links. When processing a
massive graph in a distributed algorithm, local outputs must be configured as a
solution to a graph problem without shared memory and with few rounds of
communication. In this paper we consider the problem of computing a local
cluster in a massive graph in the distributed setting. Computing local clusters
are of certain application-specific interests, such as detecting communities in
social networks or groups of interacting proteins in biological networks. When
the graph models the computer network itself, detecting local clusters can help
to prevent communication bottlenecks. We give a distributed algorithm that
computes a local cluster in time that depends only logarithmically on the size
of the graph in the CONGEST model. In particular, when the value of the optimal
local cluster is known, the algorithm runs in time entirely independent of the
size of the graph and depends only on error bounds for approximation. We also
show that the local cluster problem can be computed in the k-machine
distributed model in sublinear time. The speedup of our local cluster
algorithms is mainly due to the use of our distributed algorithm for heat
kernel pagerank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08968</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08968</id><created>2015-07-31</created><authors><author><keyname>Chung</keyname><forenames>Fan</forenames></author><author><keyname>Simpson</keyname><forenames>Olivia</forenames></author></authors><title>Finding Consensus in Multi-Agent Networks Using Heat Kernel Pagerank</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new and efficient algorithm for determining a consensus value
for a network of agents. Different from existing algorithms, our algorithm
evaluates the consensus value for very large networks using heat kernel
pagerank. We consider two frameworks for the consensus problem, a weighted
average consensus among all agents, and consensus in a leader-following
formation. Using a heat kernel pagerank approximation, we give consensus
algorithms that run in time sublinear in the size of the network, and provide
quantitative analysis of the tradeoff between performance guarantees and error
estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.08979</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.08979</id><created>2015-07-31</created><updated>2016-03-08</updated><authors><author><keyname>Park</keyname><forenames>Jihong</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author><author><keyname>Zander</keyname><forenames>Jens</forenames></author></authors><title>Tractable Resource Management with Uplink Decoupled Millimeter-Wave
  Overlay in Ultra-Dense Cellular Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>to appear in IEEE Transactions on Wireless Communications (17 pages,
  11 figures, 1 table)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The forthcoming 5G cellular network is expected to overlay millimeter-wave
(mmW) transmissions with the incumbent micro-wave ({\mu}W) architecture. The
overall mm-{\mu}W resource management should therefore harmonize with each
other. This paper aims at maximizing the overall downlink (DL) rate with a
minimum uplink (UL) rate constraint, and concludes: mmW tends to focus more on
DL transmissions while {\mu}W has high priority for complementing UL, under
time-division duplex (TDD) mmW operations. Such UL dedication of {\mu}W results
from the limited use of mmW UL bandwidth due to excessive power consumption
and/or high peak-to-average power ratio (PAPR) at mobile users. To further
relieve this UL bottleneck, we propose mmW UL decoupling that allows each
legacy {\mu}W base station (BS) to receive mmW signals. Its impact on mm-{\mu}W
resource management is provided in a tractable way by virtue of a novel
closed-form mm-{\mu}W spectral efficiency (SE) derivation. In an ultra-dense
cellular network (UDN), our derivation verifies mmW (or {\mu}W) SE is a
logarithmic function of BS-to-user density ratio. This strikingly simple yet
practically valid analysis is enabled by exploiting stochastic geometry in
conjunction with real three dimensional (3D) building blockage statistics in
Seoul, Korea.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00019</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00019</id><created>2015-07-31</created><authors><author><keyname>Gashler</keyname><forenames>Michael S.</forenames></author><author><keyname>Kindle</keyname><forenames>Zachariah</forenames></author><author><keyname>Smith</keyname><forenames>Michael R.</forenames></author></authors><title>A Minimal Architecture for General Cognition</title><categories>cs.AI</categories><comments>8 pages, 8 figures, conference, Proceedings of the 2015 International
  Joint Conference on Neural Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A minimalistic cognitive architecture called MANIC is presented. The MANIC
architecture requires only three function approximating models, and one state
machine. Even with so few major components, it is theoretically sufficient to
achieve functional equivalence with all other cognitive architectures, and can
be practically trained. Instead of seeking to transfer architectural
inspiration from biology into artificial intelligence, MANIC seeks to minimize
novelty and follow the most well-established constructs that have evolved
within various sub-fields of data science. From this perspective, MANIC offers
an alternate approach to a long-standing objective of artificial intelligence.
This paper provides a theoretical analysis of the MANIC architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00021</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00021</id><created>2015-07-31</created><updated>2015-09-21</updated><authors><author><keyname>de Br&#xe9;bisson</keyname><forenames>Alexandre</forenames></author><author><keyname>Simon</keyname><forenames>&#xc9;tienne</forenames></author><author><keyname>Auvolat</keyname><forenames>Alex</forenames></author><author><keyname>Vincent</keyname><forenames>Pascal</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Artificial Neural Networks Applied to Taxi Destination Prediction</title><categories>cs.LG cs.NE</categories><comments>ECML/PKDD discovery challenge</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe our first-place solution to the ECML/PKDD discovery challenge on
taxi destination prediction. The task consisted in predicting the destination
of a taxi based on the beginning of its trajectory, represented as a
variable-length sequence of GPS points, and diverse associated
meta-information, such as the departure time, the driver id and client
information. Contrary to most published competitor approaches, we used an
almost fully automated approach based on neural networks and we ranked first
out of 381 teams. The architectures we tried use multi-layer perceptrons,
bidirectional recurrent neural networks and models inspired from recently
introduced memory networks. Our approach could easily be adapted to other
applications in which the goal is to predict a fixed-length output from a
variable-length sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00023</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00023</id><created>2015-07-31</created><updated>2016-01-15</updated><authors><author><keyname>Chatterjee</keyname><forenames>Avhishek</forenames></author><author><keyname>Varshney</keyname><forenames>Lav R.</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Work Capacity of Freelance Markets: Fundamental Limits and Decentralized
  Schemes</title><categories>cs.MA cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing of jobs to online freelance markets is rapidly gaining
popularity. Most crowdsourcing platforms are uncontrolled and offer freedom to
customers and freelancers to choose each other. This works well for unskilled
jobs (e.g., image classification) with no specific quality requirement since
freelancers are functionally identical. For skilled jobs (e.g., software
development) with specific quality requirements, however, this does not ensure
that the maximum number of job requests is satisfied. In this work we determine
the capacity of freelance markets, in terms of maximum satisfied job requests,
and propose centralized schemes that achieve capacity. To ensure decentralized
operation and freedom of choice for customers and freelancers, we propose
simple schemes compatible with the operation of current crowdsourcing platforms
that approximately achieve capacity. Further, for settings where the number of
job requests exceeds capacity, we propose a scheme that is agnostic of that
information, but is optimal and fair in declining jobs without wait.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00027</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00027</id><created>2015-07-31</created><updated>2015-11-07</updated><authors><author><keyname>Novak</keyname><forenames>Petra Kralj</forenames></author><author><keyname>Grcar</keyname><forenames>Miha</forenames></author><author><keyname>Sluban</keyname><forenames>Borut</forenames></author><author><keyname>Mozetic</keyname><forenames>Igor</forenames></author></authors><title>Analysis of Financial News with NewsStream</title><categories>cs.IR</categories><report-no>IJS-DP-11965</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unstructured data, such as news and blogs, can provide valuable insights into
the financial world. We present the NewsStream portal, an intuitive and
easy-to-use tool for news analytics, which supports interactive querying and
visualizations of the documents at different levels of detail. It relies on a
scalable architecture for real-time processing of a continuous stream of
textual data, which incorporates data acquisition, cleaning, natural-language
preprocessing and semantic annotation components. It has been running for over
two years and collected over 18 million news articles and blog posts. The
NewsStream portal can be used to answer the questions when, how often, in what
context, and with what sentiment was a financial entity or term mentioned in a
continuous stream of news and blogs, and therefore providing a complement to
news aggregators. We illustrate some features of our system in three use cases:
relations between the rating agencies and the PIIGS countries, reflection of
financial news on credit default swap (CDS) prices, the emergence of the
Bitcoin digital currency, and visualizing how the world is connected through
news.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00028</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00028</id><created>2015-07-31</created><authors><author><keyname>Xia</keyname><forenames>Wei</forenames></author><author><keyname>Ho</keyname><forenames>Danny</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Calibrating Function Points Using Neuro-Fuzzy Technique</title><categories>cs.SE</categories><comments>21st International Forum on Systems, Software and COCOMO Cost
  Modeling, Washington/DC, 6 pages, 2006. arXiv admin note: substantial text
  overlap with arXiv:1507.06934</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concepts of calibrating Function Points are discussed, whose aims are to
fit specific software application, to reflect software industry trend, and to
improve cost estimation. Neuro-Fuzzy is a technique which incorporates the
learning ability from neural network and the ability to capture human knowledge
from fuzzy logic. The empirical validation using ISBSG data repository Release
8 shows a 22% improvement in software effort estimation after calibration using
Neuro-Fuzzy technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00032</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00032</id><created>2015-07-31</created><authors><author><keyname>Du</keyname><forenames>Wei Lin</forenames></author><author><keyname>Ho</keyname><forenames>Danny</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>A Neuro-Fuzzy Model with SEER-SEM for Software Effort Estimation</title><categories>cs.SE</categories><comments>25th International Forum on COCOMO and Systems/Software Cost
  Modeling, Los Angeles, CA, 7 pages, 2010. arXiv admin note: substantial text
  overlap with arXiv:1507.06917</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software effort estimation is a critical part of software engineering.
Although many techniques and algorithmic models have been developed and
implemented by practitioners, accurate software effort prediction is still a
challenging endeavor. In order to address this issue, a novel soft computing
framework was previously developed. Our study utilizes this novel framework to
develop an approach combining the neuro-fuzzy technique with the System
Evaluation and Estimation of Resource - Software Estimation Model (SEER-SEM).
Moreover, our study assesses the performance of the proposed model by designing
and conducting evaluation with published industrial project data. After
analyzing the performance of our model in comparison to the SEER-SEM effort
estimation model alone, the proposed model demonstrates the ability of
improving the estimation accuracy, especially in its ability to reduce the
large Mean Relative Error (MRE). Furthermore, the results of this research
indicate that the general neuro-fuzzy framework can work with various
algorithmic models for improving the performance of software effort estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00034</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00034</id><created>2015-07-31</created><authors><author><keyname>Huang</keyname><forenames>Xishi</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author><author><keyname>Ho</keyname><forenames>Danny</forenames></author><author><keyname>Ren</keyname><forenames>Jing</forenames></author></authors><title>An Intelligent Approach to Software Cost Prediction</title><categories>cs.SE</categories><comments>18th International Forum on COCOMO and Software Cost Modeling, Los
  Angeles, USA, 10 pages, 2003</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Good software cost prediction is important for effective project management
such as budgeting, project planning and control. In this paper, we present an
intelligent approach to software cost prediction. By integrating the
neuro-fuzzy technique with the well-accepted COCOMO model, our approach can
make the best use of both expert knowledge and historical project data. Its
major advantages include learning ability, good interpretability, and
robustness to imprecise and uncertain inputs. The validation using industry
project data shows that the model greatly improves prediction accuracy in
comparison with the COCOMO model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00036</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00036</id><created>2015-07-31</created><updated>2015-08-10</updated><authors><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author><author><keyname>Olshevsky</keyname><forenames>Alex</forenames></author></authors><title>On performance of consensus protocols subject to noise: role of hitting
  times and network structure</title><categories>math.OC cs.DC cs.SY math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the performance of linear consensus protocols based on repeated
averaging in the presence of additive noise. When the consensus dynamics
corresponds to a reversible Markov chain, we give an exact expression for the
weighted steady-state disagreement in terms of the stationary distribution and
hitting times in an underlying graph. This expression unifies and extends
several results in the existing literature. We show how this result can be used
to characterize the asymptotic mean-square disagreement in certain noisy
opinion dynamics models, as well as the scalability of protocols for formation
control and decentralized clock synchronization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00037</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00037</id><created>2015-07-31</created><authors><author><keyname>Ho</keyname><forenames>Danny</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author><author><keyname>Huang</keyname><forenames>Xishi</forenames></author><author><keyname>Ren</keyname><forenames>Jing</forenames></author></authors><title>Neuro-Fuzzy Algorithmic (NFA) Models and Tools for Estimation</title><categories>cs.SE cs.AI</categories><comments>20th International Forum on COCOMO and Software Cost Modeling, Los
  Angeles, USA, 5 pages, 2005</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate estimation such as cost estimation, quality estimation and risk
analysis is a major issue in management. We propose a patent pending soft
computing framework to tackle this challenging problem. Our generic framework
is independent of the nature and type of estimation. It consists of neural
network, fuzzy logic, and an algorithmic estimation model. We made use of the
Constructive Cost Model (COCOMO), Analysis of Variance (ANOVA), and Function
Point Analysis as the algorithmic models and validated the accuracy of the
Neuro-Fuzzy Algorithmic (NFA) Model in software cost estimation using
industrial project data. Our model produces more accurate estimation than using
an algorithmic model alone. We also discuss the prototypes of our tools that
implement the NFA Model. We conclude with our roadmap and direction to enrich
the model in tackling different estimation challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00040</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00040</id><created>2015-07-31</created><authors><author><keyname>Aly</keyname><forenames>Heba</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author></authors><title>An Analysis of Device-Free and Device-Based WiFi-Localization Systems</title><categories>cs.NI</categories><comments>Published in International Journal of Ambient Computing and
  Intelligence (IJACI) - Volume 6 Issue 1, January 2014</comments><doi>10.4018/ijaci.2014010101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WiFi-based localization became one of the main indoor localization techniques
due to the ubiquity of WiFi connectivity. However, indoor environments exhibit
complex wireless propagation characteristics. Typically, these characteristics
are captured by constructing a fingerprint map for the different locations in
the area of interest. This fingerprint requires significant overhead in manual
construction, and thus has been one of the major drawbacks of WiFi-based
localization. In this paper, we present an automated tool for fingerprint
constructions and leverage it to study novel scenarios for device-based and
device-free WiFi-based localization that are difficult to evaluate in a real
environment. In a particular, we examine the effect of changing the access
points (AP) mounting location, AP technology upgrade, crowd effect on
calibration and operation, among others; on the accuracy of the localization
system. We present the analysis for the two classes of WiFi-based localization:
device-based and device-free. Our analysis highlights factors affecting the
localization system accuracy, how to tune it for better localization, and
provides insights for both researchers and practitioners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00043</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00043</id><created>2015-07-31</created><authors><author><keyname>Gidion</keyname><forenames>Gerd</forenames></author><author><keyname>Grosch</keyname><forenames>Michael</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author><author><keyname>Meadows</keyname><forenames>Ken</forenames></author></authors><title>Media Usage Survey: Overall Comparison of Faculty and Students</title><categories>cs.CY</categories><comments>International Conference on Interactive Collaborative Learning (ICL),
  Dubai, United Arab Emirates, pp. 1014-1020, 2014. arXiv admin note: text
  overlap with arXiv:1507.06857</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in the use of technologies in education have provided
unique opportunities for teaching and learning. This paper describes the
results of a survey conducted at Western University (Canada) in 2013, regarding
the use of media by students and instructors. The results of this study support
the assumption that the media usage of students and instructors include a
mixture of traditional and new media. The main traditional media continue to be
important, and some new media have emerged as seemingly on equal footing or
even more important than the traditional forms of media. Some new media that
have recently been in the public spotlight do not seem to be as important as
expected. These new media may still be emerging but it is not possible to know
their ultimate importance at this point. There was some variation in media
usage across different Faculties but perhaps not as much variation as might
have been expected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00044</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00044</id><created>2015-07-31</created><authors><author><keyname>Gidion</keyname><forenames>Gerd</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author><author><keyname>Meadows</keyname><forenames>Ken N.</forenames></author><author><keyname>Grosch</keyname><forenames>Michael</forenames></author></authors><title>How Students Use Media: A Comparison across Faculties</title><categories>cs.CY</categories><comments>International Conference of Engineering Education and Research,
  Hamilton, Ontario, Canada, McMaster University, pp. 174-177, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pervasiveness of online information services has led to substantial
changes in higher education including changes in faculty members teaching
methods and students study habits. This article presents the results of a
survey about media use for teaching and learning conducted at a large Canadian
university and highlights trends in the use of new and traditional media across
university Faculties. The results of this study support the assumption that
student media usage includes a mixture of traditional and new media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00051</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00051</id><created>2015-07-31</created><authors><author><keyname>Fang</keyname><forenames>Yan</forenames></author><author><keyname>Yashin</keyname><forenames>Victor V.</forenames></author><author><keyname>Chiarulli</keyname><forenames>Donald M.</forenames></author><author><keyname>Levitan</keyname><forenames>Steven P.</forenames></author></authors><title>A Simplified Phase Model for Oscillator Based Computing</title><categories>cs.ET</categories><comments>2015 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)</comments><acm-class>B.7.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building oscillator based computing systems with emerging nano-device
technologies has become a promising solution for unconventional computing tasks
like computer vision and pattern recognition. However, simulation and analysis
of these systems is both time and compute intensive due to the nonlinearity of
new devices and the complex behavior of coupled oscillators. In order to speed
up the simulation of coupled oscillator systems, we propose a simplified phase
model to perform phase and frequency synchronization prediction based on a
synthesis of earlier models. Our model can predict the frequency locking
behavior with several orders of magnitude speedup compared to direct
evaluation, enabling the effective and efficient simulation of the large
numbers of oscillators required for practical computing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00055</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00055</id><created>2015-07-31</created><authors><author><keyname>Gloor</keyname><forenames>Peter A.</forenames></author><author><keyname>Marcos</keyname><forenames>Joao</forenames></author><author><keyname>de Boer</keyname><forenames>Patrick M.</forenames></author><author><keyname>Fuehres</keyname><forenames>Hauke</forenames></author><author><keyname>Lo</keyname><forenames>Wei</forenames></author><author><keyname>Nemoto</keyname><forenames>Keiichi</forenames></author></authors><title>Cultural Anthropology through the Lens of Wikipedia: Historical Leader
  Networks, Gender Bias, and News-based Sentiment</title><categories>cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the differences in historical World View between
Western and Eastern cultures, represented through the English, the Chinese,
Japanese, and German Wikipedia. In particular, we analyze the historical
networks of the World's leaders since the beginning of written history,
comparing them in the different Wikipedias and assessing cultural chauvinism.
We also identify the most influential female leaders of all times in the
English, German, Spanish, and Portuguese Wikipedia. As an additional lens into
the soul of a culture we compare top terms, sentiment, emotionality, and
complexity of the English, Portuguese, Spanish, and German Wikinews.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00059</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00059</id><created>2015-07-31</created><authors><author><keyname>Colaco</keyname><forenames>Zenon</forenames></author><author><keyname>Sridharan</keyname><forenames>Mohan</forenames></author></authors><title>Mixed Logical and Probabilistic Reasoning for Planning and Explanation
  Generation in Robotics</title><categories>cs.RO cs.AI cs.LO</categories><comments>11 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robots assisting humans in complex domains have to represent knowledge and
reason at both the sensorimotor level and the social level. The architecture
described in this paper couples the non-monotonic logical reasoning
capabilities of a declarative language with probabilistic belief revision,
enabling robots to represent and reason with qualitative and quantitative
descriptions of knowledge and degrees of belief. Specifically, incomplete
domain knowledge, including information that holds in all but a few exceptional
situations, is represented as a Answer Set Prolog (ASP) program. The answer set
obtained by solving this program is used for inference, planning, and for
jointly explaining (a) unexpected action outcomes due to exogenous actions and
(b) partial scene descriptions extracted from sensor input. For any given task,
each action in the plan contained in the answer set is executed
probabilistically. The subset of the domain relevant to the action is
identified automatically, and observations extracted from sensor inputs perform
incremental Bayesian updates to a belief distribution defined over this domain
subset, with highly probable beliefs being committed to the ASP program. The
architecture's capabilities are illustrated in simulation and on a mobile robot
in the context of a robot waiter operating in the dining room of a restaurant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00060</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00060</id><created>2015-07-31</created><authors><author><keyname>Sastry</keyname><forenames>Shankar Prasad</forenames></author></authors><title>Snow Globe: An Advancing-Front 3D Delaunay Mesh Refinement Algorithm</title><categories>cs.CG cs.DS</categories><comments>updated manuscript after submission to SIAM SODA 16, updated theorem
  5.6, and added lemma 5.7</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the objectives of a Delaunay mesh refinement algorithm is to produce
meshes with tetrahedral elements having a bounded aspect ratio, which is the
ratio between the radius of the circumscribing and inscribing spheres. The
refinement is carried out by inserting additional Steiner vertices inside the
circumsphere of a poor-quality tetrahedron (to remove the tetrahedron) at a
sufficient distance from existing vertices to guarantee the termination and
size optimality of the algorithm. This technique eliminates tetrahedra whose
ratio of the radius of the circumscribing sphere and the shortest side, the
radius-edge ratio, is large. Slivers, almost-planar tetrahedra, which have a
small radius-edge ratio, but a large aspect ratio, are avoided by small random
perturbations of the Steiner vertices to improve the aspect ratio.
Additionally, geometric constraints, called &quot;petals&quot;, have been shown to
produce smaller high-quality meshes in 2D Delaunay refinement algorithms. In
this paper, we develop a deterministic nondifferentiable optimization routine
to place the Steiner vertex inside geometrical constraints that we call &quot;snow
globes&quot; for 3D Delaunay refinement. We explore why the geometrical constraints
and an ordering on processing of poor-quality tetrahedra result in smaller
meshes. The stricter analysis provides an improved constant associated with the
size optimality of the generated meshes. Aided by the analysis, we present a
modified algorithm to handle boundary encroachment. The final algorithm behaves
like an advancing-front algorithms that are commonly used for quadrilateral and
hexahedral meshing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00068</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00068</id><created>2015-07-31</created><authors><author><keyname>Jiang</keyname><forenames>Heping</forenames></author></authors><title>A New Property of Hamilton Graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Hamilton cycle is a cycle containing every vertex of a graph. A graph is
called Hamiltonian if it contains a Hamilton cycle. The Hamilton cycle problem
is to find the sufficient and necessary condition that a graph is Hamiltonian.
In this paper, we give out some new kind of definitions of the subgraphs and
determine the Hamiltoncity of edges according to the existence of the subgraphs
in a graph, and then obtain a new property of Hamilton graphs as being a
necessary and sufficient condition characterized in the connectivity of the
subgraph that induced from the cycle structure of a given graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00070</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00070</id><created>2015-07-31</created><updated>2015-11-27</updated><authors><author><keyname>Gao</keyname><forenames>Zhen</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Asymptotic Orthogonality Analysis of Time-Domain Sparse Massive MIMO
  Channels</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures;
  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7166308&amp;tag=1</comments><journal-ref>IEEE Communications Letters, vol. 19, no. 10, pp. 1826-1829, Oct.
  2015</journal-ref><doi>10.1109/LCOMM.2015.2460243</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theoretical analysis of downlink massive MIMO usually assumes the ideal
Gaussian channel matrix with asymp- totic orthogonality of channel vectors
associated with different users, since it can provide the favorable propagation
condition. Meanwhile, recent experiments have shown that massive MIMO channels
between a certain user and massive base station antennas appear the spatial
common sparsity (SCS) in both the time domain and angle domain. This motivates
us to investigate whether realistic sparse massive MIMO channels could provide
the favorable propagation condition, and reveal the capacity gap between
massive MIMO systems over realistic sparse channels and that under the ideal
Gaussian channel matrix assumption. This paper theoretically proves that
channel vectors associated with different users in massive MIMO over realistic
sparse channels satisfy the asymptotic orthogonality, which indicates that the
favorable propagation condition can also be provided. Moreover, the simulation
results confirm the theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00077</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00077</id><created>2015-08-01</created><authors><author><keyname>Hong</keyname><forenames>Song-Nam</forenames></author><author><keyname>Maric</keyname><forenames>Ivana</forenames></author><author><keyname>Hui</keyname><forenames>Dennis</forenames></author></authors><title>A Novel Cooperative Strategy for Wireless Multihop Backhaul Networks</title><categories>cs.IT math.IT</categories><comments>Parts of this paper will be presented at GLOBECOM 2015. arXiv admin
  note: text overlap with arXiv:1003.5966 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 5G wireless network architecture will bring dense deployments of base
stations called {\em small cells} for both outdoors and indoors traffic. The
feasibility of their dense deployments depends on the existence of a high
data-rate transport network that can provide high-data backhaul from an
aggregation node where data traffic originates and terminates, to every such
small cell. Due to the limited range of radio signals in the high frequency
bands, multihop wireless connection may need to be established between each
access node and an aggregation node. In this paper, we present a novel
transmission scheme for wireless multihop backhaul for 5G networks. The scheme
consists of 1) {\em group successive relaying} that established a relay
schedule to efficiently exploit half-duplex relays and 2) an optimized
quantize-map-and-forward (QMF) coding scheme that improves the performance of
QMF and reduces the decoding complexity and the delay. We derive an achievable
rate region of the proposed scheme and attain a closed-form expression in the
asymptotic case for several network models of interests. It is shown that the
proposed scheme provides a significant gain over multihop routing (based on
decode-and-forward), which is a solution currently proposed for wireless
multihop backhaul network. Furthermore, the performance gap increases as a
network becomes denser. For the proposed scheme, we then develop
energy-efficient routing that determines {\em groups} of participating relays
for every hop. To reflect the metric used in the routing algorithm, we refer to
it as {\em interference-harnessing} routing. By turning interference into a
useful signal, each relay requires a lower transmission power to achieve a
desired performance compared to other routing schemes. Finally, we present a
low-complexity successive decoder, which makes it feasible to use the proposed
scheme in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00088</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00088</id><created>2015-08-01</created><authors><author><keyname>Shashaank</keyname><forenames>D. S.</forenames></author><author><keyname>Sruthi</keyname><forenames>V.</forenames></author><author><keyname>Vijayalakshimi</keyname><forenames>M. L. S</forenames></author><author><keyname>Garcia</keyname><forenames>Jacob Shomona</forenames></author></authors><title>Turnover Prediction Of Shares using Data Mining techniques : A Case
  Study</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting the turnover of a company in the ever fluctuating Stock market has
always proved to be a precarious situation and most certainly a difficult task
in hand. Data mining is a well-known sphere of Computer Science that aims on
extracting meaningful information from large databases. However, despite the
existence of many algorithms for the purpose of predicting the future trends,
their efficiency is questionable as their predictions suffer from a high error
rate. The objective of this paper is to investigate various classification
algorithms to predict the turnover of different companies based on the Stock
price. The authorized dataset for predicting the turnover was taken from
www.bsc.com and included the stock market values of various companies over the
past 10 years. The algorithms were investigated using the &quot;R&quot; tool. The feature
selection algorithm, Boruta, was run on this dataset to extract the important
and influential features for classification. With these extracted features, the
Total Turnover of the company was predicted using various classification
algorithms like Random Forest, Decision Tree, SVM and Multinomial Regression.
This prediction mechanism was implemented to predict the turnover of a company
on an everyday basis and hence could help navigate through dubious stock market
trades. An accuracy rate of 95% was achieved by the above prediction process.
Moreover, the importance of stock market attributes was established as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00091</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00091</id><created>2015-08-01</created><authors><author><keyname>Yang</keyname><forenames>Yiling</forenames></author><author><keyname>Huang</keyname><forenames>Yu</forenames></author><author><keyname>Cao</keyname><forenames>Jiannong</forenames></author><author><keyname>Lu</keyname><forenames>Jian</forenames></author></authors><title>Understanding the Timed Distributed Trace of a Partially Synchronous
  System at Runtime</title><categories>cs.DC cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has gained broad attention to understand the timed distributed trace of a
cyber-physical system at runtime, which is often achieved by verifying
properties over the observed trace of system execution. However, this
verification is facing severe challenges. First, in realistic settings, the
computing entities only have imperfectly synchronized clocks. A proper timing
model is essential to the interpretation of the trace of system execution.
Second, the specification should be able to express properties with real-time
constraints despite the asynchrony, and the semantics should be interpreted
over the currently-observed and continuously-growing trace. To address these
challenges, we propose PARO - the partially synchronous system observation
framework, which i) adopts the partially synchronous model of time, and
introduces the lattice and the timed automata theories to model the trace of
system execution; ii) adopts a tailored subset of TCTL to specify temporal
properties, and defines the 3-valued semantics to interpret the properties over
the currently-observed finite trace; iii) constructs the timed automaton
corresponding to the trace at runtime, and reduces the satisfaction of the
3-valued semantics over finite traces to that of the classical boolean
semantics over infinite traces. PARO is implemented over MIPA - the open-source
middleware we developed. Performance measurements show the cost-effectiveness
of PARO in different settings of key environmental factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00092</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00092</id><created>2015-08-01</created><authors><author><keyname>Castelluccio</keyname><forenames>Marco</forenames></author><author><keyname>Poggi</keyname><forenames>Giovanni</forenames></author><author><keyname>Sansone</keyname><forenames>Carlo</forenames></author><author><keyname>Verdoliva</keyname><forenames>Luisa</forenames></author></authors><title>Land Use Classification in Remote Sensing Images by Convolutional Neural
  Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the use of convolutional neural networks for the semantic
classification of remote sensing scenes. Two recently proposed architectures,
CaffeNet and GoogLeNet, are adopted, with three different learning modalities.
Besides conventional training from scratch, we resort to pre-trained networks
that are only fine-tuned on the target data, so as to avoid overfitting
problems and reduce design time. Experiments on two remote sensing datasets,
with markedly different characteristics, testify on the effectiveness and wide
applicability of the proposed solution, which guarantees a significant
performance improvement over all state-of-the-art references.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00093</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00093</id><created>2015-08-01</created><authors><author><keyname>Fu</keyname><forenames>Yuxi</forenames></author><author><keyname>Zhu</keyname><forenames>Han</forenames></author></authors><title>The Name-Passing Calculus</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Name-passing calculi are foundational models for mobile computing. Research
into these models has produced a wealth of results ranging from relative
expressiveness to programming pragmatics. The diversity of these results call
for clarification and reorganization. This paper applies a model independent
approach to the study of the name-passing calculi, leading to a uniform
treatment and simplification. The technical tools and the results presented in
the paper form the foundation for a theory of name-passing calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00094</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00094</id><created>2015-08-01</created><authors><author><keyname>Plotnikov</keyname><forenames>Anatoly D.</forenames></author></authors><title>On a logical model of combinatorial problems</title><categories>cs.CC</categories><comments>8 pages, 2 figures</comments><doi>10.4236/oalib.1101479</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The paper proposes a logical model of combinatorial problems, also it gives
an example of a problem of the class NP that can not be solved in polynomial
time on the dimension of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00097</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00097</id><created>2015-08-01</created><authors><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author><author><keyname>Albacea</keyname><forenames>Elizer A.</forenames></author></authors><title>The Interactive Effects of Operators and Parameters to GA Performance
  Under Different Problem Sizes</title><categories>cs.NE</categories><comments>19 pages</comments><journal-ref>Philippine Computing Journal 3(2):26-37, 2008</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The complex effect of genetic algorithm's (GA) operators and parameters to
its performance has been studied extensively by researchers in the past but
none studied their interactive effects while the GA is under different problem
sizes. In this paper, We present the use of experimental model (1)~to
investigate whether the genetic operators and their parameters interact to
affect the offline performance of GA, (2)~to find what combination of genetic
operators and parameter settings will provide the optimum performance for GA,
and (3)~to investigate whether these operator-parameter combination is
dependent on the problem size. We designed a GA to optimize a family of
traveling salesman problems (TSP), with their optimal solutions known for
convenient benchmarking. Our GA was set to use different algorithms in
simulating selection ($\Omega_s$), different algorithms ($\Omega_c$) and
parameters ($p_c$) in simulating crossover, and different parameters ($p_m$) in
simulating mutation. We used several $n$-city TSPs ($n=\{5, 7, 10, 100,
1000\}$) to represent the different problem sizes (i.e., size of the resulting
search space as represented by GA schemata). Using analysis of variance of
3-factor factorial experiments, we found out that GA performance is affected by
$\Omega_s$ at small problem size (5-city TSP) where the algorithm Partially
Matched Crossover significantly outperforms Cycle Crossover at $95\%$
confidence level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00102</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00102</id><created>2015-08-01</created><authors><author><keyname>Angel</keyname><forenames>Axel</forenames></author></authors><title>Towards Distortion-Predictable Embedding of Neural Networks</title><categories>cs.CV</categories><comments>54 pages, 28 figures. Master project at EPFL (Switzerland) in 2015.
  For source code on GitHub, see https://github.com/axel-angel/master-project</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current research in Computer Vision has shown that Convolutional Neural
Networks (CNN) give state-of-the-art performance in many classification tasks
and Computer Vision problems. The embedding of CNN, which is the internal
representation produced by the last layer, can indirectly learn topological and
relational properties. Moreover, by using a suitable loss function, CNN models
can learn invariance to a wide range of non-linear distortions such as
rotation, viewpoint angle or lighting condition. In this work, new insights are
discovered about CNN embeddings and a new loss function is proposed, derived
from the contrastive loss, that creates models with more predicable mappings
and also quantifies distortions. In typical distortion-dependent methods, there
is no simple relation between the features corresponding to one image and the
features of this image distorted. Therefore, these methods require to
feed-forward inputs under every distortions in order to find the corresponding
features representations. Our contribution makes a step towards embeddings
where features of distorted inputs are related and can be derived from each
others by the intensity of the distortion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00105</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00105</id><created>2015-08-01</created><authors><author><keyname>He</keyname><forenames>Biao</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author><author><keyname>Abhayapala</keyname><forenames>Thushara D.</forenames></author></authors><title>Achieving secrecy without knowing the number of eavesdropper antennas</title><categories>cs.IT math.IT</categories><comments>IEEE transactions on wireless communications, accepted to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existing research on physical layer security commonly assumes the number
of eavesdropper antennas to be known. Although this assumption allows one to
easily compute the achievable secrecy rate, it can hardly be realized in
practice. In this paper, we provide an innovative approach to study secure
communication systems without knowing the number of eavesdropper antennas by
introducing the concept of spatial constraint into physical layer security.
Specifically, the eavesdropper is assumed to have a limited spatial region to
place (possibly an infinite number of) antennas. From a practical point of
view, knowing the spatial constraint of the eavesdropper is much easier than
knowing the number of eavesdropper antennas. We derive the achievable secrecy
rates of the spatially-constrained system with and without friendly jamming. We
show that a non-zero secrecy rate is achievable with the help of a friendly
jammer, even if the eavesdropper places an infinite number of antennas in its
spatial region. Furthermore, we find that the achievable secrecy rate does not
monotonically increase with the jamming power, and hence, we obtain the
closed-form solution of the optimal jamming power that maximizes the secrecy
rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00106</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00106</id><created>2015-08-01</created><updated>2015-12-06</updated><authors><author><keyname>Leviant</keyname><forenames>Ira</forenames></author><author><keyname>Reichart</keyname><forenames>Roi</forenames></author></authors><title>Separated by an Un-common Language: Towards Judgment Language Informed
  Vector Space Modeling</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common evaluation practice in the vector space models (VSMs) literature is
to measure the models' ability to predict human judgments about lexical
semantic relations between word pairs. Most existing evaluation sets, however,
consist of scores collected for English word pairs only, ignoring the potential
impact of the judgment language in which word pairs are presented on the human
scores. In this paper we translate two prominent evaluation sets, wordsim353
(association) and SimLex999 (similarity), from English to Italian, German and
Russian and collect scores for each dataset from crowdworkers fluent in its
language. Our analysis reveals that human judgments are strongly impacted by
the judgment language. Moreover, we show that the predictions of monolingual
VSMs do not necessarily best correlate with human judgments made with the
language used for model training, suggesting that models and humans are
affected differently by the language they use when making semantic judgments.
Finally, we show that in a large number of setups, multilingual VSM combination
results in improved correlations with human judgments, suggesting that
multilingualism may partially compensate for the judgment language effect on
human judgments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00109</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00109</id><created>2015-08-01</created><updated>2016-01-02</updated><authors><author><keyname>Liu</keyname><forenames>Yinsheng</forenames></author><author><keyname>Li</keyname><forenames>Geoffrey Ye</forenames></author><author><keyname>Tan</keyname><forenames>Zhenhui</forenames></author><author><keyname>Qiao</keyname><forenames>Deli</forenames></author></authors><title>Single-Carrier Modulation for Large-Scale Antenna Systems</title><categories>cs.IT math.IT</categories><comments>Paper submitted to IEEE Transactions on Vehicular Technologies,
  LSA-SC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale antenna (LSA) has gained a lot of attention due to its great
potential to significantly improve system throughput. In most existing works on
LSA systems, orthogonal frequency division multiplexing (OFDM) is presumed to
deal with frequency selectivity of wireless channels. Although LSA-OFDM is a
natural evolution from multiple-input multiple-output OFDM (MIMO-OFDM), the
drawbacks of LSA-OFDM are inevitable, especially when used for the uplink. In
this paper, we investigate single-carrier (SC) modulation for the uplink
transmission in LSA systems based on a novel waveform recovery theory, where
the receiver is designed to recover the transmit waveform while the
information-bearing symbols can be recovered by directly sampling the recovered
waveform. The waveform recovery adopts the assumption that the antenna number
is infinite and the channels at different antennas are independent. In
practical environments, however, the antenna number is always finite and the
channels at different antennas are also correlated when placing hundreds of
antennas in a small area. Therefore, we will also analyze the impacts of such
non-ideal environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00115</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00115</id><created>2015-08-01</created><authors><author><keyname>Theuns</keyname><forenames>Tom</forenames><affiliation>ICC, Durham University</affiliation></author><author><keyname>Chalk</keyname><forenames>Aidan</forenames><affiliation>ECS, Durham University</affiliation></author><author><keyname>Schaller</keyname><forenames>Matthieu</forenames><affiliation>ICC, Durham University</affiliation></author><author><keyname>Gonnet</keyname><forenames>Pedro</forenames><affiliation>ECS, Durham University</affiliation><affiliation>Google Switzerland</affiliation></author></authors><title>SWIFT: task-based hydrodynamics and gravity for cosmological simulations</title><categories>astro-ph.IM astro-ph.CO cs.DC</categories><comments>Proceedings of the EASC 2015 conference, Edinburgh, UK, April 21-23,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulations of galaxy formation follow the gravitational and hydrodynamical
interactions between gas, stars and dark matter through cosmic time. The huge
dynamic range of such calculations severely limits strong scaling behaviour of
the community codes in use, with load-imbalance, cache inefficiencies and poor
vectorisation limiting performance. The new swift code exploits task-based
parallelism designed for many-core compute nodes interacting via MPI using
asynchronous communication to improve speed and scaling. A graph-based domain
decomposition schedules interdependent tasks over available resources. Strong
scaling tests on realistic particle distributions yield excellent parallel
efficiency, and efficient cache usage provides a large speed-up compared to
current codes even on a single core. SWIFT is designed to be easy to use by
shielding the astronomer from computational details such as the construction of
the tasks or MPI communication. The techniques and algorithms used in SWIFT may
benefit other computational physics areas as well, for example that of
compressible hydrodynamics. For details of this open-source project, see
www.swiftsim.com
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00116</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00116</id><created>2015-08-01</created><authors><author><keyname>Bhardwaj</keyname><forenames>Arjun</forenames></author></authors><title>Extending SROIQ with Constraint Networks and Grounded Circumscription</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developments in semantic web technologies have promoted ontological encoding
of knowledge from diverse domains. However, modelling many practical domains
requires more expressiveness than what the standard description logics (most
prominently SROIQ) support. In this paper, we extend the expressive DL SROIQ
with constraint networks (resulting in the logic SROIQc) and grounded
circumscription (resulting in the logic GC-SROIQ). Applications of constraint
modelling include embedding ontologies with temporal or spatial information,
while those of grounded circumscription include defeasible inference and closed
world reasoning.
  We describe the syntax and semantics of the logic formed by including
constraint modelling constructs in SROIQ, and provide a sound, complete and
terminating tableau algorithm for it. We further provide an intuitive algorithm
for Grounded Circumscription in SROIQc, which adheres to the general framework
of grounded circumscription, and which can be applied to a whole range of
expressive logics for which no such specific algorithm presently exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00123</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00123</id><created>2015-08-01</created><authors><author><keyname>Plotnikov</keyname><forenames>Anatoly D.</forenames></author></authors><title>On non-canonical solving the Satisfiability problem</title><categories>cs.DS</categories><comments>5 pages</comments><journal-ref>International Journal of Automation, Control and Intelligent
  Systems, Vol. 1, No. 3, September 2015, Pub. Date: Aug. 5, 2015, p.73-76</journal-ref><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We study the non-canonical method for solving the Satisfiability problem
which given by a formula in the form of the conjunctive normal form. The
essence of this method consists in counting the number of tuples of Boolean
variables, on which at least one clause of the given formula is false. On this
basis the solution of the problem obtains in the form YES or NO without
constructing tuple, when the answer is YES. It is found that if the clause in
the given formula has pairwise contrary literals, then the problem can be
solved efficiently. However, when in the formula there are a long chain of
clauses with pairwise non-contrary literals, the solution leads to an
exponential calculations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00131</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00131</id><created>2015-08-01</created><authors><author><keyname>Bisti</keyname><forenames>Luca</forenames></author><author><keyname>Lenzini</keyname><forenames>Luciano</forenames></author><author><keyname>Mingozzi</keyname><forenames>Enzo</forenames></author><author><keyname>Vallati</keyname><forenames>Carlo</forenames></author></authors><title>Efficient handoff for mass transit connectivity using IEEE 802.11</title><categories>cs.NI</categories><comments>Infq 2011, Workshop Informatica Quantitativa, 27-29 June 2011,
  Lipari, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years, WiFi standard has been used to develop different kind of
wireless networks due to its flexibility and the availability of cheap off the
shelf hardware. Even if the standard itself lacks mobility support, it has been
used in networks with mobile nodes. When mobility is involved, a fast handoff
is of paramount importance, especially with multimedia applications. The
current IEEE 802.11 standard does not provide any specification about how the
handoff should take place. In this paper we propose a handoff scheme optimized
for networks providing wireless connection to mass transit vehicles. The
structure of the procedure is redesigned to minimize delay as well as assure
reliability. A reliable handoff triggering mechanism is also designed
exploiting the results obtained from a set of preliminary outdoor experiments.
The proposed scheme was implemented and deployed in an experimental testbed,
and several indoor tests were run in order to demonstrate its reliability and
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00140</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00140</id><created>2015-08-01</created><updated>2015-10-30</updated><authors><author><keyname>Douik</keyname><forenames>Ahmed</forenames></author><author><keyname>Dahrouj</keyname><forenames>Hayssam</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Resilient Backhaul Network Design Using Hybrid Radio/Free-Space Optical
  Technology</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The radio-frequency (RF) technology is a scalable solution for the backhaul
planning. However, its performance is limited in terms of data rate and
latency. Free Space Optical (FSO) backhaul, on the other hand, offers a higher
data rate but is sensitive to weather conditions. To combine the advantages of
RF and FSO backhauls, this paper proposes a cost-efficient backhaul network
using the hybrid RF/FSO technology. To ensure a resilient backhaul, the paper
imposes a given degree of redundancy by connecting each node through $K$
link-disjoint paths so as to cope with potential link failures. Hence, the
network planning problem considered in this paper is the one of minimizing the
total deployment cost by choosing the appropriate link type, i.e., either
hybrid RF/FSO or optical fiber (OF), between each couple of base-stations while
guaranteeing $K$ link-disjoint connections, a data rate target, and a
reliability threshold. The paper solves the problem using graph theory
techniques. It reformulates the problem as a maximum weight clique problem in
the planning graph, under a specified realistic assumption about the cost of OF
and hybrid RF/FSO links. Simulation results show the cost of the different
planning and suggest that the proposed heuristic solution has a
close-to-optimal performance for a significant gain in computation complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00142</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00142</id><created>2015-08-01</created><updated>2015-10-14</updated><authors><author><keyname>Feldman</keyname><forenames>Moran</forenames></author><author><keyname>Svensson</keyname><forenames>Ola</forenames></author><author><keyname>Zenklusen</keyname><forenames>Rico</forenames></author></authors><title>Online Contention Resolution Schemes</title><categories>cs.DS cs.DM</categories><comments>33 pages. To appear in SODA 2016</comments><msc-class>68W27 (Primary), 68R05 (Secondary)</msc-class><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new rounding technique designed for online optimization
problems, which is related to contention resolution schemes, a technique
initially introduced in the context of submodular function maximization. Our
rounding technique, which we call online contention resolution schemes (OCRSs),
is applicable to many online selection problems, including Bayesian online
selection, oblivious posted pricing mechanisms, and stochastic probing models.
It allows for handling a wide set of constraints, and shares many strong
properties of offline contention resolution schemes. In particular, OCRSs for
different constraint families can be combined to obtain an OCRS for their
intersection. Moreover, we can approximately maximize submodular functions in
the online settings we consider.
  We, thus, get a broadly applicable framework for several online selection
problems, which improves on previous approaches in terms of the types of
constraints that can be handled, the objective functions that can be dealt
with, and the assumptions on the strength of the adversary. Furthermore, we
resolve two open problems from the literature; namely, we present the first
constant-factor constrained oblivious posted price mechanism for matroid
constraints, and the first constant-factor algorithm for weighted stochastic
probing with deadlines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00144</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00144</id><created>2015-08-01</created><updated>2015-10-07</updated><authors><author><keyname>Grigoryeva</keyname><forenames>Lyudmila</forenames></author><author><keyname>Henriques</keyname><forenames>Julie</forenames></author><author><keyname>Ortega</keyname><forenames>Juan-Pablo</forenames></author></authors><title>Quantitative evaluation of the performance of discrete-time reservoir
  computers in the forecasting, filtering, and reconstruction of stochastic
  stationary signals</title><categories>cs.ET cs.NE math.ST stat.TH</categories><comments>21 pages, 1 figure, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends the notion of information processing capacity for
non-independent input signals in the context of reservoir computing (RC). The
presence of input autocorrelation makes worthwhile the treatment of forecasting
and filtering problems for which we explicitly compute this generalized
capacity as a function of the reservoir parameter values using a streamlined
model. The reservoir model leading to these developments is used to show that,
whenever that approximation is valid, this computational paradigm satisfies the
so called separation and fading memory properties that are usually associated
with good information processing performances. We show that several standard
memory, forecasting, and filtering problems that appear in the parametric
stochastic time series context can be readily formulated and tackled via RC
which, as we show, significantly outperforms standard techniques in some
instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00158</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00158</id><created>2015-08-01</created><authors><author><keyname>Petrosyan</keyname><forenames>Petros A.</forenames></author><author><keyname>Tepanyan</keyname><forenames>Hayk H.</forenames></author></authors><title>Interval edge-colorings of composition of graphs</title><categories>math.CO cs.DM</categories><comments>12 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An edge-coloring of a graph $G$ with consecutive integers
$c_{1},\ldots,c_{t}$ is called an \emph{interval $t$-coloring} if all colors
are used, and the colors of edges incident to any vertex of $G$ are distinct
and form an interval of integers. A graph $G$ is interval colorable if it has
an interval $t$-coloring for some positive integer $t$. The set of all interval
colorable graphs is denoted by $\mathfrak{N}$. In 2004, Giaro and Kubale showed
that if $G,H\in \mathfrak{N}$, then the Cartesian product of these graphs
belongs to $\mathfrak{N}$. In the same year they formulated a similar problem
for the composition of graphs as an open problem. Later, in 2009, the first
author showed that if $G,H\in \mathfrak{N}$ and $H$ is a regular graph, then
$G[H]\in \mathfrak{N}$. In this paper, we prove that if $G\in \mathfrak{N}$ and
$H$ has an interval coloring of a special type, then $G[H]\in \mathfrak{N}$.
Moreover, we show that all regular graphs, complete bipartite graphs and trees
have such a special interval coloring. In particular, this implies that if
$G\in \mathfrak{N}$ and $T$ is a tree, then $G[T]\in \mathfrak{N}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00168</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00168</id><created>2015-08-01</created><authors><author><keyname>Liu</keyname><forenames>Yuanpeng</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Completion Time in Two-user Channels: An Information-Theoretic
  Perspective</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a two-user channel, completion time refers to the number of channel uses
spent by each user to transmit a bit pool with some given size. In this paper,
the information-theoretic formulation of completion time is based on the
concept of constrained rates, where users are allowed to employ different
numbers of channel uses for transmission as opposed to the equal channel use of
the standard information-theoretic formulation. Analogous to the capacity
region, the completion time region characterizes all possible trade-offs among
users' completion times. For a multi-access channel, it is shown that the
completion time region is achieved by operating the channel in two independent
phases: a multi-access phase when both users are transmitting, and a
point-to-point phase when one user has finished and the other is still
transmitting. Using a similar two-phase approach, the completion time region
(or inner and outer bounds) is established for a Gaussian broadcast channel and
a Gaussian interference channel. It is observed that although consisting of two
convex subregions, the completion time region may not be convex in general.
Finally an optimization problem of minimizing the weighted sum completion time
for a Gaussian multi-access channel and a Gaussian broadcast channel is solved,
demonstrating the utility of the completion time approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00169</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00169</id><created>2015-08-01</created><authors><author><keyname>Liu</keyname><forenames>Yuanpeng</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Capacity and Rate Regions of A Class of Broadcast Interference Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a class of broadcast interference channels (BIC) is
investigated, where one of the two broadcast receivers is subject to
interference coming from a point-to-point transmission. For a general discrete
memoryless broadcast interference channel (DM-BIC), an achievable scheme based
on message splitting, superposition and binning is proposed and a concise
representation of the corresponding achievable rate region R is obtained. Two
partial-order broadcast conditions interference-oblivious less noisy and
interference-cognizant less noisy are defined, thereby extending the usual less
noisy condition for a regular broadcast channel by taking interference into
account. Under these conditions, a reduced form of R is shown to be equivalent
to a rate region based on a simpler scheme, where the broadcast transmitter
uses only superposition. Furthermore, if interference is strong for the
interference-oblivious less noisy DM-BIC, the capacity region is given by the
aforementioned two equivalent rate regions. For a Gaussian broadcast
interference channel (GBIC), channel parameters are categorized into three
regimes. For the first two regimes, which are closely related to the two
partial-order broadcast conditions, achievable rate regions are derived by
specializing the corresponding achievable schemes of DM-BICs with Gaussian
input distributions. The entropy power inequality (EPI) based outer bounds are
obtained by combining bounding techniques for a Gaussian broadcast channel
(GBC) and a Gaussian interference channel (GIC). These inner and outer bounds
lead to either exact or approximate characterizations of capacity regions and
sum capacity under various conditions. For the remaining complementing regime,
inner and outer bounds are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00181</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00181</id><created>2015-08-01</created><authors><author><keyname>Shahir</keyname><forenames>Hamed Yaghoubi</forenames></author><author><keyname>Gl&#xe4;sser</keyname><forenames>Uwe</forenames></author><author><keyname>Shahir</keyname><forenames>Amir Yaghoubi</forenames></author><author><keyname>Wehn</keyname><forenames>Hans</forenames></author></authors><title>An Analytic Framework for Maritime Situation Analysis</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maritime domain awareness is critical for protecting sea lanes, ports,
harbors, offshore structures and critical infrastructures against common
threats and illegal activities. Limited surveillance resources constrain
maritime domain awareness and compromise full security coverage at all times.
This situation calls for innovative intelligent systems for interactive
situation analysis to assist marine authorities and security personal in their
routine surveillance operations. In this article, we propose a novel situation
analysis framework to analyze marine traffic data and differentiate various
scenarios of vessel engagement for the purpose of detecting anomalies of
interest for marine vessels that operate over some period of time in relative
proximity to each other. The proposed framework views vessel behavior as
probabilistic processes and uses machine learning to model common vessel
interaction patterns. We represent patterns of interest as left-to-right Hidden
Markov Models and classify such patterns using Support Vector Machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00184</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00184</id><created>2015-08-01</created><authors><author><keyname>Imem</keyname><forenames>Ali Al</forenames></author></authors><title>Comparison and evaluation of digital signature schemes employed in ndn
  network</title><categories>cs.CR</categories><comments>International Journal of Software Engineering &amp; Applications Volume:
  7 - Volume No: 3 - issue: June 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Named Data networking ensure data integrity so that every important data has
to be signed by its owner in order to send it safely inside the network.
Similarly, in NDN we have to assure that none could open the data except
authorized users. Since only the endpoints have the right to sign the data or
check its validity during the verification process, we have considered that the
data could be requested from various types of devices used by different people,
these devices could be anything like a smartphone, PC, sensor node with a
different CPU descriptions, parameters, and memory sizes, however their ability
to check the high traffic of a data during the key generation and verification
period is definitely a hard task and it could exhaust the systems with low
computational resources. RSA and ECDSA as digital signature algorithms have
proven their efficiency against cyber attacks, they are characterized by their
speed to encrypt and decrypt data, in addition to their competence at checking
the data integrity. The main purpose of our research was to find the optimal
algorithm that avoids the systems overhead and offers the best time during the
signature scheme
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00188</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00188</id><created>2015-08-02</created><authors><author><keyname>Luo</keyname><forenames>Feixiong</forenames></author><author><keyname>Cao</keyname><forenames>Guofeng</forenames></author><author><keyname>Mulligan</keyname><forenames>Kevin</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author></authors><title>Explore Spatiotemporal and Demographic Characteristics of Human Mobility
  via Twitter: A Case Study of Chicago</title><categories>cs.SI cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterizing human mobility patterns is essential for understanding human
behaviors and the interactions with socioeconomic and natural environment. With
the continuing advancement of location and Web 2.0 technologies, location-based
social media (LBSM) have been gaining widespread popularity in the past few
years. With an access to locations of users, profiles and the contents of the
social media posts, the LBSM data provided a novel modality of data source for
human mobility study. By exploiting the explicit location footprints and mining
the latent demographic information implied in the LBSM data, the purpose of
this paper is to investigate the spatiotemporal characteristics of human
mobility with a particular focus on the impact of demography. We first collect
geo-tagged Twitter feeds posted in the conterminous United States area, and
organize the collection of feeds using the concept of space-time trajectory
corresponding to each Twitter user. Commonly human mobility measures, including
detected home and activity centers, are derived for each user trajectory. We
then select a subset of Twitter users that have detected home locations in the
city of Chicago as a case study, and apply name analysis to the names provided
in user profiles to learn the implicit demographic information of Twitter
users, including race/ethnicity, gender and age. Finally we explore the
spatiotemporal distribution and mobility characteristics of Chicago Twitter
users, and investigate the demographic impact by comparing the differences
across three demographic dimensions (race/ethnicity, gender and age). We found
that, although the human mobility measures of different demographic groups
generally follow the generic laws (e.g., power law distribution), the
demographic information, particular the race/ethnicity group, significantly
affects the urban human mobility patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00189</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00189</id><created>2015-08-02</created><authors><author><keyname>Sachan</keyname><forenames>Devendra Singh</forenames></author><author><keyname>Kumar</keyname><forenames>Shailesh</forenames></author></authors><title>Class Vectors: Embedding representation of Document Classes</title><categories>cs.CL cs.IR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Distributed representations of words and paragraphs as semantic embeddings in
high dimensional data are used across a number of Natural Language
Understanding tasks such as retrieval, translation, and classification. In this
work, we propose &quot;Class Vectors&quot; - a framework for learning a vector per class
in the same embedding space as the word and paragraph embeddings. Similarity
between these class vectors and word vectors are used as features to classify a
document to a class. In experiment on several sentiment analysis tasks such as
Yelp reviews and Amazon electronic product reviews, class vectors have shown
better or comparable results in classification while learning very meaningful
class embeddings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00192</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00192</id><created>2015-08-02</created><authors><author><keyname>Chen</keyname><forenames>Ling</forenames></author><author><keyname>Yu</keyname><forenames>Ting</forenames></author><author><keyname>Chirkova</keyname><forenames>Rada</forenames></author></authors><title>WaveCluster with Differential Privacy</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WaveCluster is an important family of grid-based clustering algorithms that
are capable of finding clusters of arbitrary shapes. In this paper, we
investigate techniques to perform WaveCluster while ensuring differential
privacy. Our goal is to develop a general technique for achieving differential
privacy on WaveCluster that accommodates different wavelet transforms. We show
that straightforward techniques based on synthetic data generation and
introduction of random noise when quantizing the data, though generally
preserving the distribution of data, often introduce too much noise to preserve
useful clusters. We then propose two optimized techniques, PrivTHR and
PrivTHREM, which can significantly reduce data distortion during two key steps
of WaveCluster: the quantization step and the significant grid identification
step. We conduct extensive experiments based on four datasets that are
particularly interesting in the context of clustering, and show that PrivTHR
and PrivTHREM achieve high utility when privacy budgets are properly allocated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00200</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00200</id><created>2015-08-02</created><authors><author><keyname>Tang</keyname><forenames>Jian</forenames></author><author><keyname>Qu</keyname><forenames>Meng</forenames></author><author><keyname>Mei</keyname><forenames>Qiaozhu</forenames></author></authors><title>PTE: Predictive Text Embedding through Large-scale Heterogeneous Text
  Networks</title><categories>cs.CL cs.LG cs.NE</categories><comments>KDD 2015</comments><acm-class>I.2.6</acm-class><doi>10.1145/2783258.2783307</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector,
have been attracting increasing attention due to their simplicity, scalability,
and effectiveness. However, comparing to sophisticated deep learning
architectures such as convolutional neural networks, these methods usually
yield inferior results when applied to particular machine learning tasks. One
possible reason is that these text embedding methods learn the representation
of text in a fully unsupervised way, without leveraging the labeled information
available for the task. Although the low dimensional representations learned
are applicable to many different tasks, they are not particularly tuned for any
task. In this paper, we fill this gap by proposing a semi-supervised
representation learning method for text data, which we call the
\textit{predictive text embedding} (PTE). Predictive text embedding utilizes
both labeled and unlabeled data to learn the embedding of text. The labeled
information and different levels of word co-occurrence information are first
represented as a large-scale heterogeneous text network, which is then embedded
into a low dimensional space through a principled and efficient algorithm. This
low dimensional embedding not only preserves the semantic closeness of words
and documents, but also has a strong predictive power for the particular task.
Compared to recent supervised approaches based on convolutional neural
networks, predictive text embedding is comparable or more effective, much more
efficient, and has fewer parameters to tune.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00202</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00202</id><created>2015-08-02</created><updated>2015-10-23</updated><authors><author><keyname>Lee</keyname><forenames>Hwangrae</forenames></author><author><keyname>Sturmfels</keyname><forenames>Bernd</forenames></author></authors><title>Duality of Multiple Root Loci</title><categories>math.AG cs.SC math.OC</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multiple root loci among univariate polynomials of degree $n$ are indexed
by partitions of $n$. We study these loci and their conormal varieties. The
projectively dual varieties are joins of such loci where the partitions are
hooks. Our emphasis lies on equations and parametrizations that are useful for
Euclidean distance optimization. We compute the ED degrees for hooks. Among the
dual hypersurfaces are those that demarcate the set of binary forms whose real
rank equals the generic complex rank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00205</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00205</id><created>2015-08-02</created><updated>2015-08-14</updated><authors><author><keyname>Alaa</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Ahuja</keyname><forenames>Kartik</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Evolution of Social Networks: A Microfounded Model</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many societies are organized in networks that are formed by people who meet
and interact over time. In this paper, we present a first model to capture the
micro-foundations of social networks evolution, where boundedly rational agents
of different types join the network; meet other agents stochastically over
time; and consequently decide to form social ties. A basic premise of our model
is that in real-world networks, agents form links by reasoning about the
benefits that agents they meet over time can bestow. We study the evolution of
the emerging networks in terms of friendship and popularity acquisition given
the following exogenous parameters: structural opportunism, type distribution,
homophily, and social gregariousness. We show that the time needed for an agent
to find &quot;friends&quot; is influenced by the exogenous parameters: agents who are
more gregarious, more homophilic, less opportunistic, or belong to a type
&quot;minority&quot; spend a longer time on average searching for friendships. Moreover,
we show that preferential attachment is a consequence of an emerging doubly
preferential meeting process: a process that guides agents of a certain type to
meet more popular similar-type agents with a higher probability, thereby
creating asymmetries in the popularity evolution of different types of agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00212</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00212</id><created>2015-08-02</created><authors><author><keyname>Kowalski</keyname><forenames>Jakub</forenames></author><author><keyname>Szyku&#x142;a</keyname><forenames>Marek</forenames></author></authors><title>Procedural Content Generation for GDL Descriptions of Simplified
  Boardgames</title><categories>cs.AI</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We present initial research towards procedural generation of Simplified
Boardgames and translating them into an efficient GDL code. This is a step
towards establishing Simplified Boardgames as a comparison class for General
Game Playing agents. To generate playable, human readable, and balanced
chess-like games we use an adaptive evolutionary algorithm with the fitness
function based on simulated playouts. In future, we plan to use the proposed
method to diversify and extend the set of GGP tournament games by those with
fully automatically generated rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00213</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00213</id><created>2015-08-02</created><updated>2015-10-26</updated><authors><author><keyname>Tang</keyname><forenames>Yutao</forenames></author><author><keyname>Hong</keyname><forenames>Yiguang</forenames></author><author><keyname>Wang</keyname><forenames>Xinghu</forenames></author></authors><title>Distributed Output Regulation for a Class of Nonlinear Multi-Agent
  Systems with Unknown-Input Leaders</title><categories>math.OC cs.SY</categories><comments>8 pages, 2 figures</comments><msc-class>93B52, 93A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a distributed output regulation problem is formulated for a
class of uncertain nonlinear multi-agent systems subject to local disturbances.
The formulation is given to study a leader-following problem when the leader
contains unknown inputs and its dynamics is different from those of the
followers. Based on the conventional output regulation assumptions and graph
theory, distributed feedback controllers are constructed to make the agents
globally or semi-globally follow the uncertain leader even when the bound of
the leader's inputs is unknown to the followers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00217</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00217</id><created>2015-08-02</created><updated>2015-08-03</updated><authors><author><keyname>Liu</keyname><forenames>Ruoyu</forenames></author><author><keyname>Zhao</keyname><forenames>Yao</forenames></author><author><keyname>Wei</keyname><forenames>Shikui</forenames></author><author><keyname>Zhu</keyname><forenames>Zhenfeng</forenames></author><author><keyname>Liao</keyname><forenames>Lixin</forenames></author><author><keyname>Qiu</keyname><forenames>Shuang</forenames></author></authors><title>Indexing of CNN Features for Large Scale Image Search</title><categories>cs.CV</categories><comments>14 pages, 22 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural network (CNN) feature that represents an image with a
global and high-dimensional vector has shown highly discriminative capability
in image search. Although CNN features are more compact than most of local
representation schemes, it still cannot efficiently deal with large-scale image
search issues due to its non-negligible computational cost and storage usage.
In this paper, we propose a simple but effective image indexing framework to
improve the computational and storage efficiency of CNN features. Instead of
projecting each CNN feature vector into a global hashing code, the proposed
framework adapts Bag-of-Word model and inverted table to global feature
indexing. To this end, two strategies, which are based on semantic information
associated with CNN features, are proposed to convert a global vector to one or
several discrete words. In addition, several strategies for compensating
quantization error are fully investigated under the indexing framework.
Extensive experimental results on two public benchmarks show the superiority of
our framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00230</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00230</id><created>2015-08-02</created><authors><author><keyname>Alsheikh</keyname><forenames>Mohammad Abu</forenames></author><author><keyname>Lin</keyname><forenames>Shaowei</forenames></author><author><keyname>Tan</keyname><forenames>Hwee-Pink</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author></authors><title>Toward a Robust Sparse Data Representation for Wireless Sensor Networks</title><categories>cs.NI cs.LG cs.NE</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing has been successfully used for optimized operations in
wireless sensor networks. However, raw data collected by sensors may be neither
originally sparse nor easily transformed into a sparse data representation.
This paper addresses the problem of transforming source data collected by
sensor nodes into a sparse representation with a few nonzero elements. Our
contributions that address three major issues include: 1) an effective method
that extracts population sparsity of the data, 2) a sparsity ratio guarantee
scheme, and 3) a customized learning algorithm of the sparsifying dictionary.
We introduce an unsupervised neural network to extract an intrinsic sparse
coding of the data. The sparse codes are generated at the activation of the
hidden layer using a sparsity nomination constraint and a shrinking mechanism.
Our analysis using real data samples shows that the proposed method outperforms
conventional sparsity-inducing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00239</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00239</id><created>2015-08-02</created><authors><author><keyname>Liang</keyname><forenames>Dongmei</forenames></author><author><keyname>Cheng</keyname><forenames>Wushan</forenames></author></authors><title>Partial matching face recognition method for rehabilitation nursing
  robots beds</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In order to establish face recognition system in rehabilitation nursing
robots beds and achieve real-time monitor the patient on the bed. We propose a
face recognition method based on partial matching Hu moments which apply for
rehabilitation nursing robots beds. Firstly we using Haar classifier to detect
human faces automatically in dynamic video frames. Secondly we using Otsu
threshold method to extract facial features (eyebrows, eyes, mouth) in the face
image and its Hu moments. Finally, we using Hu moment feature set to achieve
the automatic face recognition. Experimental results show that this method can
efficiently identify face in a dynamic video and it has high practical value
(the accuracy rate is 91% and the average recognition time is 4.3s).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00246</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00246</id><created>2015-08-02</created><updated>2015-08-27</updated><authors><author><keyname>Sekeh</keyname><forenames>Salimeh Yasaei</forenames></author><author><keyname>Borzadran</keyname><forenames>Gholamreza Mohtashami</forenames></author><author><keyname>Roknabadi</keyname><forenames>Abdolhamid Rezaei</forenames></author></authors><title>On double truncated (interval) WCRE and WCE</title><categories>cs.IT math.IT math.PR</categories><comments>9pages</comments><msc-class>62N05, 62B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measure of the weighted cumulative entropy about the predictability of
failure time of a system have been introduced in [3]. Referring properties of
doubly truncated (interval) cumulative residual and past entropy, several
bounds and assertions are proposed in weighted version.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00256</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00256</id><created>2015-08-02</created><authors><author><keyname>Pitaval</keyname><forenames>Renaud-Alexandre</forenames></author><author><keyname>Wei</keyname><forenames>Lu</forenames></author><author><keyname>Tirkkonen</keyname><forenames>Olav</forenames></author><author><keyname>Corander</keyname><forenames>Jukka</forenames></author></authors><title>Volume of Metric Balls in High-Dimensional Complex Grassmann Manifolds</title><categories>cs.IT math.IT</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Volume of metric balls relates to rate-distortion theory and packing bounds
on codes. In this paper, the volume of balls in complex Grassmann manifolds is
evaluated for an arbitrary radius. The ball is defined as a set of hyperplanes
of a fixed dimension with reference to a center of possibly different
dimension, and a generalized chordal distance for unequal dimensional subspaces
is used. First, the volume is reduced to one-dimensional integral
representation. The overall problem boils down to evaluating a determinant of a
matrix of the same size as the subspace dimensionality. Interpreting this
determinant as a characteristic function of the Jacobi ensemble, an asymptotic
analysis is carried out. The obtained asymptotic volume is moreover refined
using moment-matching techniques to provide a tighter approximation in
finite-size regimes. Lastly, the pertinence of the derived results is shown by
rate-distortion analysis of source coding on Grassmann manifolds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00271</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00271</id><created>2015-08-02</created><updated>2015-09-28</updated><authors><author><keyname>Fragkiadaki</keyname><forenames>Katerina</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Felsen</keyname><forenames>Panna</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Recurrent Network Models for Human Dynamics</title><categories>cs.CV</categories><comments>International Conference on Computer Vision 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and
prediction of human body pose in videos and motion capture. The ERD model is a
recurrent neural network that incorporates nonlinear encoder and decoder
networks before and after recurrent layers. We test instantiations of ERD
architectures in the tasks of motion capture (mocap) generation, body pose
labeling and body pose forecasting in videos. Our model handles mocap training
data across multiple subjects and activity domains, and synthesizes novel
motions while avoid drifting for long periods of time. For human pose labeling,
ERD outperforms a per frame body part detector by resolving left-right body
part confusions. For video pose forecasting, ERD predicts body joint
displacements across a temporal horizon of 400ms and outperforms a first order
motion model based on optical flow. ERDs extend previous Long Short Term Memory
(LSTM) models in the literature to jointly learn representations and their
dynamics. Our experiments show such representation learning is crucial for both
labeling and prediction in space-time. We find this is a distinguishing feature
between the spatio-temporal visual domain in comparison to 1D text, speech or
handwriting, where straightforward hard coded representations have shown
excellent results when directly combined with recurrent units.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00278</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00278</id><created>2015-08-02</created><authors><author><keyname>Aghagolzadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Radha</keyname><forenames>Hayder</forenames></author></authors><title>Dictionary and Image Recovery from Incomplete and Random Measurements</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper tackles algorithmic and theoretical aspects of dictionary learning
from incomplete and random block-wise image measurements and the performance of
the adaptive dictionary for sparse image recovery. This problem is related to
blind compressed sensing in which the sparsifying dictionary or basis is viewed
as an unknown variable and subject to estimation during sparse recovery.
However, unlike existing guarantees for a successful blind compressed sensing,
our results do not rely on additional structural constraints on the learned
dictionary or the measured signal. In particular, we rely on the spatial
diversity of compressive measurements to guarantee that the solution is unique
with a high probability. Moreover, our distinguishing goal is to measure and
reduce the estimation error with respect to the ideal dictionary that is based
on the complete image. Using recent results from random matrix theory, we show
that applying a slightly modified dictionary learning algorithm over
compressive measurements results in accurate estimation of the ideal dictionary
for large-scale images. Empirically, we experiment with both space-invariant
and space-varying sensing matrices and demonstrate the critical role of spatial
diversity in measurements. Simulation results confirm that the presented
algorithm outperforms the typical non-adaptive sparse recovery based on
offline-learned universal dictionaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00280</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00280</id><created>2015-08-02</created><authors><author><keyname>Textor</keyname><forenames>Johannes</forenames></author><author><keyname>Idelberger</keyname><forenames>Alexander</forenames></author><author><keyname>Li&#x15b;kiewicz</keyname><forenames>Maciej</forenames></author></authors><title>Learning from Pairwise Marginal Independencies</title><categories>cs.AI</categories><comments>10 pages, 6 figures, 2 tables. Published at the 31st Conference on
  Uncertainty in Artificial Intelligence (UAI 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider graphs that represent pairwise marginal independencies amongst a
set of variables (for instance, the zero entries of a covariance matrix for
normal data). We characterize the directed acyclic graphs (DAGs) that
faithfully explain a given set of independencies, and derive algorithms to
efficiently enumerate such structures. Our results map out the space of
faithful causal models for a given set of pairwise marginal independence
relations. This allows us to show the extent to which causal inference is
possible without using conditional independence tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00282</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00282</id><created>2015-08-02</created><authors><author><keyname>Aghagolzadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Radha</keyname><forenames>Hayder</forenames></author></authors><title>On Hyperspectral Classification in the Compressed Domain</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of hyperspectral pixel classification
based on the recently proposed architectures for compressive whisk-broom
hyperspectral imagers without the need to reconstruct the complete data cube. A
clear advantage of classification in the compressed domain is its suitability
for real-time on-site processing of the sensed data. Moreover, it is assumed
that the training process also takes place in the compressed domain, thus,
isolating the classification unit from the recovery unit at the receiver's
side. We show that, perhaps surprisingly, using distinct measurement matrices
for different pixels results in more accuracy of the learned classifier and
consistent classification performance, supporting the role of information
diversity in learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00285</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00285</id><created>2015-08-02</created><authors><author><keyname>Zou</keyname><forenames>Zhenhua</forenames></author><author><keyname>Gidmark</keyname><forenames>Anders</forenames></author><author><keyname>Charalambous</keyname><forenames>Themistoklis</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author></authors><title>Optimal Radio Frequency Energy Harvesting with Limited Energy Arrival
  Knowledge</title><categories>cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop optimal policies for deciding when a wireless node
with radio frequency (RF) energy harvesting (EH) capabilities should try and
harvest ambient RF energy. While the idea of RF-EH is appealing, it is not
always beneficial to attempt to harvest energy; in environments where the
ambient energy is low, nodes could consume more energy being awake with their
harvesting circuits turned on than what they can extract from the ambient radio
signals; it is then better to enter a sleep mode until the ambient RF energy
increases. Towards this end, we consider a scenario with intermittent energy
arrivals and a wireless node that wakes up for a period of time (herein called
the time-slot) and harvests energy. If enough energy is harvested during the
time-slot, then the harvesting is successful and excess energy is stored;
however, if there does not exist enough energy the harvesting is unsuccessful
and energy is lost.
  We assume that the ambient energy level is constant during the time-slot, and
changes at slot boundaries. The energy level dynamics are described by a
two-state Gilbert-Elliott Markov chain model, where the state of the Markov
chain can only be observed during the harvesting action, and not when in sleep
mode. Two scenarios are studied under this model. In the first scenario, we
assume that we have knowledge of the transition probabilities of the Markov
chain and formulate the problem as a Partially Observable Markov Decision
Process (POMDP), where we find a threshold-based optimal policy. In the second
scenario, we assume that we don't have any knowledge about these parameters and
formulate the problem as a Bayesian adaptive POMDP; to reduce the complexity of
the computations we also propose a heuristic posterior sampling algorithm. The
performance of our approaches is demonstrated via numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00292</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00292</id><created>2015-08-02</created><authors><author><keyname>Ellis</keyname><forenames>John</forenames></author><author><keyname>Stege</keyname><forenames>Ulrike</forenames></author></authors><title>A Provably, Linear Time, In-place and Stable Merge Algorithm via the
  Perfect Shuffle</title><categories>cs.DS</categories><comments>12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reconsider a recently published algorithm (Dalkilic et al.) for merging
lists by way of the perfect shuffle. The original publication gave only
experimental results which, although consistent with linear execution time on
the samples tested, provided no analysis. Here we prove that the time
complexity, in the average case, is indeed linear, although there is an
Omega(n^2) worst case. This is then the first provably linear time merge
algorithm based on the use of the perfect shuffle. We provide a proof of
correctness, extend the algorithm to the general case where the lists are of
unequal length and show how it can be made stable, all aspects not included in
the original presentation and we give a much more concise definition of the
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00299</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00299</id><created>2015-08-02</created><authors><author><keyname>Chen</keyname><forenames>Pin-Yu</forenames></author><author><keyname>Cheng</keyname><forenames>Shin-Ming</forenames></author><author><keyname>Ting</keyname><forenames>Pai-Shun</forenames></author><author><keyname>Lien</keyname><forenames>Chia-Wei</forenames></author><author><keyname>Chu</keyname><forenames>Fu-Jen</forenames></author></authors><title>When Crowdsourcing Meets Mobile Sensing: A Social Network Perspective</title><categories>cs.SI stat.ML</categories><comments>To appear in Oct. IEEE Communications Magazine, feature topic on
  &quot;Social Networks Meet Next Generation Mobile Multimedia Internet&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile sensing is an emerging technology that utilizes agent-participatory
data for decision making or state estimation, including multimedia
applications. This article investigates the structure of mobile sensing schemes
and introduces crowdsourcing methods for mobile sensing. Inspired by social
network, one can establish trust among participatory agents to leverage the
wisdom of crowds for mobile sensing. A prototype of social network inspired
mobile multimedia and sensing application is presented for illustrative
purpose. Numerical experiments on real-world datasets show improved performance
of mobile sensing via crowdsourcing. Challenges for mobile sensing with respect
to Internet layers are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00305</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00305</id><created>2015-08-02</created><authors><author><keyname>Pasupat</keyname><forenames>Panupong</forenames></author><author><keyname>Liang</keyname><forenames>Percy</forenames></author></authors><title>Compositional Semantic Parsing on Semi-Structured Tables</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two important aspects of semantic parsing for question answering are the
breadth of the knowledge source and the depth of logical compositionality.
While existing work trades off one aspect for another, this paper
simultaneously makes progress on both fronts through a new task: answering
complex questions on semi-structured tables using question-answer pairs as
supervision. The central challenge arises from two compounding factors: the
broader domain results in an open-ended set of relations, and the deeper
compositionality results in a combinatorial explosion in the space of logical
forms. We propose a logical-form driven parsing algorithm guided by strong
typing constraints and show that it obtains significant improvements over
natural baselines. For evaluation, we created a new dataset of 22,033 complex
questions on Wikipedia tables, which is made publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00306</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00306</id><created>2015-08-02</created><authors><author><keyname>He</keyname><forenames>Jun</forenames></author><author><keyname>Song</keyname><forenames>Wei</forenames></author></authors><title>SOARAN: A Service-oriented Architecture for Radio Access Network Sharing
  in Evolving Mobile Networks</title><categories>cs.NI</categories><comments>16 pages, 6 figures, Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile networks are undergoing fast evolution to software-defined networking
(SDN) infrastructure in order to accommodate the ever-growing mobile traffic
and overcome the network management nightmares caused by unremitting
acceleration in technology innovations and evolution of the service
market.Enabled by virtualized network functionalities, evolving carrier
wireless networks tend to share radio access network (RAN) among multiple
(virtual) network operators so as to increase network capacity and reduce
expenses.However, existing RAN sharing models are operator-oriented, which
expose extensive resource details, e.g. infrastructure and spectrum,to
participating network operators for resource-sharing purposes. These
old-fashioned models violate the design principles of SDN abstraction and are
infeasible to manage the thriving traffic of on-demand customized services.
This paper presents SOARAN, a service-oriented framework for RAN sharing in
mobile networks evolving from LTE/LTE advanced to software-defined carrier
wireless networks(SD-CWNs), which decouples network operators from radio
resource by providing application-level differentiated services. SOARAN defines
a serial of abstract applications with distinct Quality of Experience (QoE)
requirements. The central controller periodically computes application-level
resource allocation for each radio element with respect to runtime traffic
demands and channel conditions, and disseminate these allocation decisions as
service-oriented policies to respect element. The radio elements then
independently determine flow-level resource allocation within each application
to accomplish these policies. We formulate the application-level resource
allocation as an optimization problem and develop a fast algorithm to solve it
with a provably approximate guarantee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00307</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00307</id><created>2015-08-02</created><authors><author><keyname>Guo</keyname><forenames>Sheng</forenames></author><author><keyname>Huang</keyname><forenames>Weilin</forenames></author><author><keyname>Qiao</keyname><forenames>Yu</forenames></author></authors><title>Local Color Contrastive Descriptor for Image Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image representation and classification are two fundamental tasks towards
multimedia content retrieval and understanding. The idea that shape and texture
information (e.g. edge or orientation) are the key features for visual
representation is ingrained and dominated in current multimedia and computer
vision communities. A number of low-level features have been proposed by
computing local gradients (e.g. SIFT, LBP and HOG), and have achieved great
successes on numerous multimedia applications. In this paper, we present a
simple yet efficient local descriptor for image classification, referred as
Local Color Contrastive Descriptor (LCCD), by leveraging the neural mechanisms
of color contrast. The idea originates from the observation in neural science
that color and shape information are linked inextricably in visual cortical
processing. The color contrast yields key information for visual color
perception and provides strong linkage between color and shape. We propose a
novel contrastive mechanism to compute the color contrast in both spatial
location and multiple channels. The color contrast is computed by measuring
\emph{f}-divergence between the color distributions of two regions. Our
descriptor enriches local image representation with both color and contrast
information. We verified experimentally that it can compensate strongly for the
shape based descriptor (e.g. SIFT), while keeping computationally simple.
Extensive experimental results on image classification show that our descriptor
improves the performance of SIFT substantially by combinations, and achieves
the state-of-the-art performance on three challenging benchmark datasets. It
improves recent Deep Learning model (DeCAF) [1] largely from the accuracy of
40.94% to 49.68% in the large scale SUN397 database. Codes for the LCCD will be
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00315</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00315</id><created>2015-08-03</created><updated>2016-03-02</updated><authors><author><keyname>Friedlander</keyname><forenames>Michael P.</forenames></author><author><keyname>Macedo</keyname><forenames>Ives</forenames></author></authors><title>Low-rank spectral optimization via gauge duality</title><categories>math.OC cs.NA math.NA</categories><comments>24 pages. Fix typo in arXiv title</comments><msc-class>90C15, 90C25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various applications in signal processing and machine learning give rise to
highly structured spectral optimization problems characterized by low-rank
solutions. Two important examples that motivate this work are optimization
problems from phase retrieval and from blind deconvolution, which are designed
to yield rank-1 solutions. An algorithm is described that is based on solving a
certain constrained eigenvalue optimization problem that corresponds to the
gauge dual which, unlike the more typical Lagrange dual, has an especially
simple constraint. The dominant cost at each iteration is the computation of
rightmost eigenpairs of a Hermitian operator. Numerical examples on a range of
problems illustrate the scalability of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00317</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00317</id><created>2015-08-03</created><authors><author><keyname>Mittelman</keyname><forenames>Roni</forenames></author></authors><title>Time-series modeling with undecimated fully convolutional neural
  networks</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new convolutional neural network-based time-series model.
Typical convolutional neural network (CNN) architectures rely on the use of
max-pooling operators in between layers, which leads to reduced resolution at
the top layers. Instead, in this work we consider a fully convolutional network
(FCN) architecture that uses causal filtering operations, and allows for the
rate of the output signal to be the same as that of the input signal. We
furthermore propose an undecimated version of the FCN, which we refer to as the
undecimated fully convolutional neural network (UFCNN), and is motivated by the
undecimated wavelet transform. Our experimental results verify that using the
undecimated version of the FCN is necessary in order to allow for effective
time-series modeling. The UFCNN has several advantages compared to other
time-series models such as the recurrent neural network (RNN) and long
short-term memory (LSTM), since it does not suffer from either the vanishing or
exploding gradients problems, and is therefore easier to train. Convolution
operations can also be implemented more efficiently compared to the recursion
that is involved in RNN-based models. We evaluate the performance of our model
in a synthetic target tracking task using bearing only measurements generated
from a state-space model, a probabilistic modeling of polyphonic music
sequences problem, and a high frequency trading task using a time-series of
ask/bid quotes and their corresponding volumes. Our experimental results using
synthetic and real datasets verify the significant advantages of the UFCNN
compared to the RNN and LSTM baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00330</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00330</id><created>2015-08-03</created><updated>2015-11-01</updated><authors><author><keyname>Liao</keyname><forenames>Zhibin</forenames></author><author><keyname>Carneiro</keyname><forenames>Gustavo</forenames></author></authors><title>On the Importance of Normalisation Layers in Deep Learning with
  Piecewise Linear Activation Units</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep feedforward neural networks with piecewise linear activations are
currently producing the state-of-the-art results in several public datasets.
The combination of deep learning models and piecewise linear activation
functions allows for the estimation of exponentially complex functions with the
use of a large number of subnetworks specialized in the classification of
similar input examples. During the training process, these subnetworks avoid
overfitting with an implicit regularization scheme based on the fact that they
must share their parameters with other subnetworks. Using this framework, we
have made an empirical observation that can improve even more the performance
of such models. We notice that these models assume a balanced initial
distribution of data points with respect to the domain of the piecewise linear
activation function. If that assumption is violated, then the piecewise linear
activation units can degenerate into purely linear activation units, which can
result in a significant reduction of their capacity to learn complex functions.
Furthermore, as the number of model layers increases, this unbalanced initial
distribution makes the model ill-conditioned. Therefore, we propose the
introduction of batch normalisation units into deep feedforward neural networks
with piecewise linear activations, which drives a more balanced use of these
activation units, where each region of the activation function is trained with
a relatively large proportion of training samples. Also, this batch
normalisation promotes the pre-conditioning of very deep learning models. We
show that by introducing maxout and batch normalisation units to the network in
network model results in a model that produces classification results that are
better than or comparable to the current state of the art in CIFAR-10,
CIFAR-100, MNIST, and SVHN datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00335</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00335</id><created>2015-08-03</created><updated>2015-12-21</updated><authors><author><keyname>Sason</keyname><forenames>Igal</forenames></author><author><keyname>Verd&#xfa;</keyname><forenames>Sergio</forenames></author></authors><title>Bounds among $f$-divergences</title><categories>cs.IT math.IT math.PR</categories><comments>Submitted to the IEEE Trans. on Information Theory. 82 pages, 7
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops methods to obtain bounds on an $f$-divergence based on
one or several other $f$-divergences, dealing with pairs of probability
measures defined on arbitrary alphabets. Functional domination is one such
approach, where special emphasis is placed on finding the best possible
constant upper bounding a ratio of $f$-divergences. Another approach used for
the derivation of bounds among $f$-divergences relies on moment inequalities
and the logarithmic-convexity property, which is studied in this paper to
obtain tight bounds on the relative entropy and Bhattacharyya distance in terms
of $\chi^2$ divergences. A rich variety of bounds are shown to hold under
boundedness assumptions on the relative information. Special attention is on
the total variation distance and its relation to the relative information and
relative entropy, including &quot;reverse Pinsker inequalities,&quot; as well as on the
$E_\gamma$ divergence, which generalizes the total variation distance.
Pinsker's inequality is extended for this type of $f$-divergence, a result
which leads to an inequality that links the relative entropy and relative
information spectrum. Integral expressions of the R\'enyi divergence in terms
of the relative information spectrum are derived, leading to bounds on the
R\'enyi divergence in terms of either the variational distance or relative
entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00347</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00347</id><created>2015-08-03</created><authors><author><keyname>Munglani</keyname><forenames>Gautam</forenames></author><author><keyname>Vetter</keyname><forenames>Roman</forenames></author><author><keyname>Wittel</keyname><forenames>Falk K.</forenames></author><author><keyname>Herrmann</keyname><forenames>Hans J.</forenames></author></authors><title>Orthotropic rotation-free thin shell elements</title><categories>math.NA cs.NA physics.comp-ph</categories><comments>10 pages, 8 figures</comments><journal-ref>Comput. Mech. 56, 785-793 (2015)</journal-ref><doi>10.1007/s00466-015-1202-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method to simulate orthotropic behaviour in thin shell finite elements is
proposed. The approach is based on the transformation of shape function
derivatives, resulting in a new orthogonal basis aligned to a specified
preferred direction for all elements. This transformation is carried out solely
in the undeformed state leaving minimal additional impact on the computational
effort expended to simulate orthotropic materials compared to isotropic,
resulting in a straightforward and highly efficient implementation. This method
is implemented for rotation-free triangular shells using the finite element
framework built on the Kirchhoff--Love theory employing subdivision surfaces.
The accuracy of this approach is demonstrated using the deformation of a
pinched hemispherical shell (with a 18{\deg} hole) standard benchmark. To
showcase the efficiency of this implementation, the wrinkling of orthotropic
sheets under shear displacement is analyzed. It is found that orthotropic
subdivision shells are able to capture the wrinkling behavior of sheets
accurately for coarse meshes without the use of an additional wrinkling model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00348</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00348</id><created>2015-08-03</created><authors><author><keyname>Shokri-Ghadikolaei</keyname><forenames>Hossein</forenames></author><author><keyname>Glaropoulos</keyname><forenames>Ioannis</forenames></author><author><keyname>Fodor</keyname><forenames>Viktoria</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Ephremides</keyname><forenames>Antony</forenames></author></authors><title>Green Sensing and Access: Energy-Throughput Tradeoffs in Cognitive
  Networking</title><categories>cs.IT cs.NI math.IT</categories><comments>to be published in IEEE Communications Magazine, 8 pages, 1 table, 6
  figures. arXiv admin note: substantial text overlap with arXiv:1312.0045</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Limited spectrum resources and dramatic growth of high data rate applications
have motivated opportunistic spectrum access utilizing the promising concept of
cognitive networks. Although this concept has emerged primarily to enhance
spectrum utilization and to allow the coexistence of heterogeneous network
technologies, the importance of energy consumption imposes additional
challenges, because energy consumption and communication performance can be at
odds. In this paper, the approaches for energy efficient spectrum sensing and
spectrum handoff, fundamental building blocks of cognitive networks is
investigated. The tradeoff between energy consumption and throughput, under
local as well as under cooperative sensing are characterized, and what further
aspects need to be investigated to achieve energy efficient cognitive operation
under various application requirements are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00349</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00349</id><created>2015-08-03</created><updated>2015-08-03</updated><authors><author><keyname>Vu</keyname><forenames>Tung T.</forenames></author><author><keyname>Kha</keyname><forenames>Ha Hoang</forenames></author><author><keyname>Duong</keyname><forenames>Trung Q.</forenames></author><author><keyname>Vo</keyname><forenames>Nguyen-Son</forenames></author></authors><title>On the Interference Alignment Designs for Secure Multiuser MIMO Systems</title><categories>math.OC cs.IT math.IT</categories><comments>submitted to IET Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose two secure multiuser multiple-input multiple-output
transmission approaches based on interference alignment (IA) in the presence of
an eavesdropper. To deal with the information leakage to the eavesdropper as
well as the interference signals from undesired transmitters (Txs) at desired
receivers (Rxs), our approaches aim to design the transmit precoding and
receive subspace matrices to minimize both the total inter-main-link
interference and the wiretapped signals (WSs). The first proposed IA scheme
focuses on aligning the WSs into proper subspaces while the second one imposes
a new structure on the precoding matrices to force the WSs to zero. When the
channel state information is perfectly known at all Txs, in each proposed IA
scheme, the precoding matrices at Txs and the receive subspaces at Rxs or the
eavesdropper are alternatively selected to minimize the cost function of an
convex optimization problem for every iteration. We provide the feasible
conditions and the proofs of convergence for both IA approaches. The simulation
results indicate that our two IA approaches outperform the conventional IA
algorithm in terms of average secrecy sum rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00354</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00354</id><created>2015-08-03</created><authors><author><keyname>Achanta</keyname><forenames>Sivanand</forenames></author><author><keyname>Vadapalli</keyname><forenames>Anandaswarup</forenames></author><author><keyname>R.</keyname><forenames>Sai Krishna</forenames></author><author><keyname>Gangashetty</keyname><forenames>Suryakanth V.</forenames></author></authors><title>Significance of Maximum Spectral Amplitude in Sub-bands for Spectral
  Envelope Estimation and Its Application to Statistical Parametric Speech
  Synthesis</title><categories>cs.SD cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a technique for spectral envelope estimation using
maximum values in the sub-bands of Fourier magnitude spectrum (MSASB). Most
other methods in the literature parametrize spectral envelope in cepstral
domain such as Mel-generalized cepstrum etc. Such cepstral domain
representations, although compact, are not readily interpretable. This
difficulty is overcome by our method which parametrizes in the spectral domain
itself. In our experiments, spectral envelope estimated using MSASB method was
incorporated in the STRAIGHT vocoder. Both objective and subjective results of
analysis-by-synthesis indicate that the proposed method is comparable to
STRAIGHT. We also evaluate the effectiveness of the proposed parametrization in
a statistical parametric speech synthesis framework using deep neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00375</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00375</id><created>2015-08-03</created><updated>2015-08-09</updated><authors><author><keyname>Haddadi</keyname><forenames>Hamed</forenames></author><author><keyname>Ofli</keyname><forenames>Ferda</forenames></author><author><keyname>Mejova</keyname><forenames>Yelena</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author><author><keyname>Srivastava</keyname><forenames>Jaideep</forenames></author></authors><title>360 Quantified Self</title><categories>cs.HC cs.CY</categories><comments>QCRI Technical Report</comments><report-no>QCRI-TR-2015-004</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wearable devices with a wide range of sensors have contributed to the rise of
the Quantified Self movement, where individuals log everything ranging from the
number of steps they have taken, to their heart rate, to their sleeping
patterns. Sensors do not, however, typically sense the social and ambient
environment of the users, such as general life style attributes or information
about their social network. This means that the users themselves, and the
medical practitioners, privy to the wearable sensor data, only have a narrow
view of the individual, limited mainly to certain aspects of their physical
condition.
  In this paper we describe a number of use cases for how social media can be
used to complement the check-up data and those from sensors to gain a more
holistic view on individuals' health, a perspective we call the 360 Quantified
Self. Health-related information can be obtained from sources as diverse as
food photo sharing, location check-ins, or profile pictures. Additionally,
information from a person's ego network can shed light on the social dimension
of wellbeing which is widely acknowledged to be of utmost importance, even
though they are currently rarely used for medical diagnosis. We articulate a
long-term vision describing the desirable list of technical advances and
variety of data to achieve an integrated system encompassing Electronic Health
Records (EHR), data from wearable devices, alongside information derived from
social media data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00377</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00377</id><created>2015-08-03</created><updated>2015-11-09</updated><authors><author><keyname>&#x10c;ern&#xfd;</keyname><forenames>Martin</forenames></author><author><keyname>Plch</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Marko</keyname><forenames>Mat&#x11b;j</forenames></author><author><keyname>Gemrot</keyname><forenames>Jakub</forenames></author><author><keyname>Ondr&#xe1;&#x10d;ek</keyname><forenames>Petr</forenames></author><author><keyname>Brom</keyname><forenames>Cyril</forenames></author></authors><title>Using Behavior Objects to Manage Complexity in Virtual Worlds</title><categories>cs.AI</categories><comments>Currently under review in IEEE Transactions on Computational
  Intelligence and AI in Games</comments><doi>10.1109/TCIAIG.2016.2528499</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quality of high-level AI of non-player characters (NPCs) in commercial
open-world games (OWGs) has been increasing during the past years. However, due
to constraints specific to the game industry, this increase has been slow and
it has been driven by larger budgets rather than adoption of new complex AI
techniques. Most of the contemporary AI is still expressed as hard-coded
scripts. The complexity and manageability of the script codebase is one of the
key limiting factors for further AI improvements. In this paper we address this
issue. We present behavior objects - a general approach to development of NPC
behaviors for large OWGs. Behavior objects are inspired by object-oriented
programming and extend the concept of smart objects. Our approach promotes
encapsulation of data and code for multiple related behaviors in one place,
hiding internal details and embedding intelligence in the environment. Behavior
objects are a natural abstraction of five different techniques that we have
implemented to manage AI complexity in an upcoming AAA OWG. We report the
details of the implementations in the context of behavior trees and the lessons
learned during development. Our work should serve as inspiration for AI
architecture designers from both the academia and the industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00395</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00395</id><created>2015-08-03</created><authors><author><keyname>Arvind</keyname><forenames>V.</forenames></author><author><keyname>Joglekar</keyname><forenames>Pushkar S</forenames></author><author><keyname>Raja</keyname><forenames>S.</forenames></author></authors><title>Noncommutative Valiant's Classes: Structure and Complete Problems</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore the noncommutative analogues, $\mathrm{VP}_{nc}$ and
$\mathrm{VNP}_{nc}$, of Valiant's algebraic complexity classes and show some
striking connections to classical formal language theory. Our main results are
the following: (1) We show that Dyck polynomials (defined from the Dyck
languages of formal language theory) are complete for the class
$\mathrm{VP}_{nc}$ under $\le_{abp}$ reductions. Likewise, it turns out that
$\mathrm{PAL}$ (Palindrome polynomials defined from palindromes) are complete
for the class $\mathrm{VSKEW}_{nc}$ (defined by polynomial-size skew circuits)
under $\le_{abp}$ reductions. The proof of these results is by suitably
adapting the classical Chomsky-Sch\&quot;{u}tzenberger theorem showing that Dyck
languages are the hardest CFLs. (2) Next, we consider the class
$\mathrm{VNP}_{nc}$. It is known~\cite{HWY10a} that, assuming the
sum-of-squares conjecture, the noncommutative polynomial
$\sum_{w\in\{x_0,x_1\}^n}ww$ requires exponential size circuits. We
unconditionally show that $\sum_{w\in\{x_0,x_1\}^n}ww$ is not
$\mathrm{VNP}_{nc}$-complete under the projection reducibility. As a
consequence, assuming the sum-of-squares conjecture, we exhibit a strictly
infinite hierarchy of p-families under projections inside $\mathrm{VNP}_{nc}$
(analogous to Ladner's theorem~\cite{Ladner75}). In the final section we
discuss some new $\mathrm{VNP}_{nc}$-complete problems under
$\le_{abp}$-reductions. (3) Inside $\mathrm{VP}_{nc}$ too we show there is a
strict hierarchy of p-families (based on the nesting depth of Dyck polynomials)
under the $\le_{abp}$ reducibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00396</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00396</id><created>2015-08-03</created><authors><author><keyname>Choi</keyname><forenames>Pui Tung</forenames></author><author><keyname>Lui</keyname><forenames>Lok Ming</forenames></author></authors><title>A Linear Algorithm for Disk Conformal Parameterization of
  Simply-Connected Open Surfaces</title><categories>cs.CG cs.GR math.DG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Surface parameterization is widely used in computer graphics and geometry
processing. It simplifies challenging tasks such as surface registrations,
morphing, remeshing and texture mapping. In this paper, we present a novel
linear algorithm for computing the disk conformal parameterization of
simply-connected open surfaces. A double covering technique is used to turn a
simply-connected open surface into a genus-0 closed surface, and then a linear
algorithm for parameterization of genus-0 closed surfaces can be applied. The
symmetry of the double covered surface preserves the efficiency of the
computation. A planar parameterization can then be obtained with the aid of a
M\&quot;obius transformation and the stereographic projection. After that, a
normalization step is applied to guarantee the circular boundary. Finally, we
achieve a bijective disk conformal parameterization by a composition of
quasi-conformal mappings. Experimental results demonstrate a significant
improvement in the computational time by over 60%. At the same time, our
proposed method retains comparable accuracy, bijectivity and robustness when
compared with the state-of-the-art approaches. Applications for texture mapping
are considered for illustrating the effectiveness of our proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00413</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00413</id><created>2015-08-03</created><updated>2015-09-10</updated><authors><author><keyname>Cui</keyname><forenames>Liqing</forenames></author><author><keyname>Li</keyname><forenames>Shun</forenames></author><author><keyname>Zhang</keyname><forenames>Wan</forenames></author><author><keyname>Zhang</keyname><forenames>Zhan</forenames></author><author><keyname>Zhu</keyname><forenames>Tingshao</forenames></author></authors><title>Identifying Emotion from Natural Walking</title><categories>cs.CV cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emotion identification from gait aims to automatically determine persons
affective state, it has attracted a great deal of interests and offered immense
potential value in action tendency, health care, psychological detection and
human-computer(robot) interaction.In this paper, we propose a new method of
identifying emotion from natural walking, and analyze the relevance between the
traits of walking and affective states. After obtaining the pure acceleration
data of wrist and ankle, we set a moving average filter window with different
sizes w, then extract 114 features including time-domain, frequency-domain,
power and distribution features from each data slice, and run principal
component analysis (PCA) to reduce dimension. In experiments, we train SVM,
Decision Tree, multilayerperception, Random Tree and Random Forest
classification models, and compare the classification accuracy on data of wrist
and ankle with respect to different w. The performance of emotion
identification on acceleration data of ankle is better than wrist.Comparing
different classification models' results, SVM has best accuracy of identifying
anger and happy could achieve 90:31% and 89:76% respectively, and
identification ratio of anger-happy is 87:10%.The anger-neutral-happy
classification reaches 85%-78%-78%.The results show that it is capable of
identifying personal emotional states through the gait of walking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00430</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00430</id><created>2015-08-03</created><updated>2015-08-04</updated><authors><author><keyname>Yu</keyname><forenames>Mengyang</forenames></author><author><keyname>Liu</keyname><forenames>Li</forenames></author><author><keyname>Shao</keyname><forenames>Ling</forenames></author></authors><title>Kernelized Multiview Projection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional vision algorithms adopt a single type of feature or a simple
concatenation of multiple features, which is always represented in a
high-dimensional space. In this paper, we propose a novel unsupervised spectral
embedding algorithm called Kernelized Multiview Projection (KMP) to better fuse
and embed different feature representations. Computing the kernel matrices from
different features/views, KMP can encode them with the corresponding weights to
achieve a low-dimensional and semantically meaningful subspace where the
distribution of each view is sufficiently smooth and discriminative. More
crucially, KMP is linear for the reproducing kernel Hilbert space (RKHS) and
solves the out-of-sample problem, which allows it to be competent for various
practical applications. Extensive experiments on three popular image datasets
demonstrate the effectiveness of our multiview embedding algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00443</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00443</id><created>2015-08-03</created><authors><author><keyname>Li</keyname><forenames>Jing</forenames></author><author><keyname>Kim</keyname><forenames>Young-Han</forenames></author></authors><title>Partial Decode-Forward Relaying for the Gaussian Two-Hop Relay Network</title><categories>cs.IT math.IT</categories><comments>7 pages (2 columns), 4 figures, submitted to the IEEE Transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multicast capacity of the Gaussian two-hop relay network with one source,
$N$ relays, and $L$ destinations is studied. It is shown that a careful
modification of the partial decode--forward coding scheme, whereby the relays
cooperate through degraded sets of message parts, achieves the cutset upper
bound within $(1/2)\log N$ bits regardless of the channel gains and power
constraints. This scheme improves upon a previous scheme by Chern and Ozgur,
which is also based on partial decode--forward yet has an unbounded gap from
the cutset bound for $L \ge 2$ destinations. When specialized to independent
codes among relays, the proposed scheme achieves within $\log N$ bits from the
cutset bound. The computation of this relaxation involves evaluating mutual
information across $L(N+1)$ cuts out of the total $L 2^N$ possible cuts,
providing a very simple linear-complexity algorithm to approximate the
single-source multicast capacity of the Gaussian two-hop relay network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00451</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00451</id><created>2015-08-03</created><updated>2016-03-03</updated><authors><author><keyname>Houthooft</keyname><forenames>Rein</forenames></author><author><keyname>De Turck</keyname><forenames>Filip</forenames></author></authors><title>Integrated Inference and Learning of Neural Factors in Structural
  Support Vector Machines</title><categories>stat.ML cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tackling pattern recognition problems in areas such as computer vision,
bioinformatics, speech or text recognition is often done best by taking into
account task-specific statistical relations between output variables. In
structured prediction, this internal structure is used to predict multiple
outputs simultaneously, leading to more accurate and coherent predictions.
Structural support vector machines (SSVMs) are nonprobabilistic models that
optimize a joint input-output function through margin-based learning. Because
SSVMs generally disregard the interplay between unary and interaction factors
during the training phase, final parameters are suboptimal. Moreover, its
factors are often restricted to linear combinations of input features, limiting
its generalization power. To improve prediction accuracy, this paper proposes:
(i) Joint inference and learning by integration of back-propagation and
loss-augmented inference in SSVM subgradient descent; (ii) Extending SSVM
factors to neural networks that form highly nonlinear functions of input
features. Image segmentation benchmark results demonstrate improvements over
conventional SSVM training methods in terms of accuracy, highlighting the
feasibility of end-to-end SSVM training with neural factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00455</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00455</id><created>2015-07-29</created><authors><author><keyname>Mangin</keyname><forenames>Cyprien</forenames><affiliation>Univ Paris Diderot &amp; &#xc9;cole Polytechnique</affiliation></author><author><keyname>Sozeau</keyname><forenames>Matthieu</forenames><affiliation>Inria Paris &amp; PPS, Univ Paris Diderot</affiliation></author></authors><title>Equations for Hereditary Substitution in Leivant's Predicative System F:
  A Case Study</title><categories>cs.LO cs.PL</categories><comments>In Proceedings LFMTP 2015, arXiv:1507.07597. www:
  http://equations-fpred.gforge.inria.fr/</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 185, 2015, pp. 71-86</journal-ref><doi>10.4204/EPTCS.185.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a case study of formalizing a normalization proof for
Leivant's Predicative System F using the Equations package. Leivant's
Predicative System F is a stratified version of System F, where type
quantification is annotated with kinds representing universe levels. A weaker
variant of this system was studied by Stump &amp; Eades, employing the hereditary
substitution method to show normalization. We improve on this result by showing
normalization for Leivant's original system using hereditary substitutions and
a novel multiset ordering on types. Our development is done in the Coq proof
assistant using the Equations package, which provides an interface to define
dependently-typed programs with well-founded recursion and full dependent
pattern- matching. Equations allows us to define explicitly the hereditary
substitution function, clarifying its algorithmic behavior in presence of term
and type substitutions. From this definition, consistency can easily be
derived. The algorithmic nature of our development is crucial to reflect
languages with type quantification, enlarging the class of languages on which
reflection methods can be used in the proof assistant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00457</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00457</id><created>2015-08-03</created><authors><author><keyname>Wong</keyname><forenames>Ka-Chun</forenames></author></authors><title>Evolutionary Multimodal Optimization: A Short Survey</title><categories>cs.NE cs.AI q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real world problems always have different multiple solutions. For instance,
optical engineers need to tune the recording parameters to get as many optimal
solutions as possible for multiple trials in the varied-line-spacing
holographic grating design problem. Unfortunately, most traditional
optimization techniques focus on solving for a single optimal solution. They
need to be applied several times; yet all solutions are not guaranteed to be
found. Thus the multimodal optimization problem was proposed. In that problem,
we are interested in not only a single optimal point, but also the others. With
strong parallel search capability, evolutionary algorithms are shown to be
particularly effective in solving this type of problem. In particular, the
evolutionary algorithms for multimodal optimization usually not only locate
multiple optima in a single run, but also preserve their population diversity
throughout a run, resulting in their global optimization ability on multimodal
functions. In addition, the techniques for multimodal optimization are borrowed
as diversity maintenance techniques to other problems. In this chapter, we
describe and review the state-of-the-arts evolutionary algorithms for
multimodal optimization in terms of methodology, benchmarking, and application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00468</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00468</id><created>2015-08-03</created><authors><author><keyname>Wong</keyname><forenames>Ka-Chun</forenames></author></authors><title>Evolutionary Algorithms: Concepts, Designs, and Applications in
  Bioinformatics: Evolutionary Algorithms for Bioinformatics</title><categories>cs.NE q-bio.GN q-bio.QM stat.CO stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since genetic algorithm was proposed by John Holland (Holland J. H., 1975) in
the early 1970s, the study of evolutionary algorithm has emerged as a popular
research field (Civicioglu &amp; Besdok, 2013). Researchers from various scientific
and engineering disciplines have been digging into this field, exploring the
unique power of evolutionary algorithms (Hadka &amp; Reed, 2013). Many applications
have been successfully proposed in the past twenty years. For example,
mechanical design (Lampinen &amp; Zelinka, 1999), electromagnetic optimization
(Rahmat-Samii &amp; Michielssen, 1999), environmental protection (Bertini, Felice,
Moretti, &amp; Pizzuti, 2010), finance (Larkin &amp; Ryan, 2010), musical orchestration
(Esling, Carpentier, &amp; Agon, 2010), pipe routing (Furuholmen, Glette, Hovin, &amp;
Torresen, 2010), and nuclear reactor core design (Sacco, Henderson,
Rios-Coelho, Ali, &amp; Pereira, 2009). In particular, its function optimization
capability was highlighted (Goldberg &amp; Richardson, 1987) because of its high
adaptability to different function landscapes, to which we cannot apply
traditional optimization techniques (Wong, Leung, &amp; Wong, 2009). Here we review
the applications of evolutionary algorithms in bioinformatics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00476</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00476</id><created>2015-08-03</created><authors><author><keyname>Astolfi</keyname><forenames>Daniele</forenames></author><author><keyname>Praly</keyname><forenames>Laurent</forenames></author></authors><title>Integral Action in Output Feedback for multi-input multi-output
  nonlinear systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address a particular problem of output regulation for multi-input
multi-output nonlinear systems. Specifically, we are interested in making the
stability of an equilibrium point and the regulation to zero of an output,
robust to (small) unmodelled discrepancies between design model and actual
system in particular those introducing an offset. We propose a novel procedure
which is intended to be relevant to real life systems, as illustrated by a (non
academic) example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00479</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00479</id><created>2015-07-23</created><updated>2015-12-18</updated><authors><author><keyname>Wang</keyname><forenames>Shuning</forenames></author></authors><title>Simple relay channel</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to an error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1987, Cover proposed an interesting problem in the simple relay channel as
shown in figure 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00481</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00481</id><created>2015-08-03</created><authors><author><keyname>Fujiwara</keyname><forenames>Yuichiro</forenames></author><author><keyname>Colbourn</keyname><forenames>Charles J.</forenames></author></authors><title>A combinatorial approach to X-tolerant compaction circuits</title><categories>cs.IT math.CO math.IT</categories><comments>11 pages, final accepted version for publication in the IEEE
  Transactions on Information Theory</comments><journal-ref>IEEE Transactions on Information Theory, 56 (2010) 3196-3206</journal-ref><doi>10.1109/TIT.2010.2048468</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Test response compaction for integrated circuits (ICs) with scan-based
design-for-testability (DFT) support in the presence of unknown logic values
(Xs) is investigated from a combinatorial viewpoint. The theoretical
foundations of X-codes, employed in an X-tolerant compaction technique called
X-compact, are examined. Through the formulation of a combinatorial model of
X-compact, novel design techniques are developed for X-codes to detect a
specified maximum number of errors in the presence of a specified maximum
number of unknown logic values, while requiring only small fan-out. The special
class of X-codes that results leads to an avoidance problem for configurations
in combinatorial designs. General design methods and nonconstructive existence
theorems to estimate the compaction ratio of an optimal X-compactor are also
derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00488</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00488</id><created>2015-08-03</created><authors><author><keyname>Buntain</keyname><forenames>Cody</forenames></author><author><keyname>Lin</keyname><forenames>Jimmy</forenames></author><author><keyname>Golbeck</keyname><forenames>Jennifer</forenames></author></authors><title>Learning to Discover Key Moments in Social Media Streams</title><categories>cs.SI</categories><acm-class>H.2.8; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces LABurst, a general technique for identifying key
moments, or moments of high impact, in social media streams without the need
for domain-specific information or seed keywords. We leverage machine learning
to model temporal patterns around bursts in Twitter's unfiltered public sample
stream and build a classifier to identify tokens experiencing these bursts. We
show LABurst performs competitively with existing burst detection techniques
while simultaneously providing insight into and detection of unanticipated
moments. To demonstrate our approach's potential, we compare two baseline
event-detection algorithms with our language-agnostic algorithm to detect key
moments across three major sporting competitions: 2013 World Series, 2014 Super
Bowl, and 2014 World Cup. Our results show LABurst outperforms a time series
analysis baseline and is competitive with a domain-specific baseline even
though we operate without any domain knowledge. We then go further by
transferring LABurst's models learned in the sports domain to the task of
identifying earthquakes in Japan and show our method detects large spikes in
earthquake-related tokens within two minutes of the actual event.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00498</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00498</id><created>2015-08-03</created><updated>2015-10-26</updated><authors><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author><author><keyname>Roychowdhury</keyname><forenames>Mrinal Kanti</forenames></author></authors><title>Quantization for uniform distributions on equilateral triangles</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We approximate the uniform measure on an equilateral triangle by a measure
supported on $n$ points. We find the optimal sets of points ($n$-means) and
corresponding approximation (quantization) error for $n\leq4$, give numerical
optimization results for $n\leq 21$, and a bound on the quantization error for
$n\to\infty$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00504</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00504</id><created>2015-07-31</created><authors><author><keyname>Siva</keyname><forenames>Karthik</forenames></author><author><keyname>Tao</keyname><forenames>Jim</forenames></author><author><keyname>Marcolli</keyname><forenames>Matilde</forenames></author></authors><title>Spin Glass Models of Syntax and Language Evolution</title><categories>cs.CL cond-mat.dis-nn physics.soc-ph</categories><comments>19 pages, LaTeX, 20 png figures</comments><msc-class>91F20, 82B20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the SSWL database of syntactic parameters of world languages, and the
MIT Media Lab data on language interactions, we construct a spin glass model of
language evolution. We treat binary syntactic parameters as spin states, with
languages as vertices of a graph, and assigned interaction energies along the
edges. We study a rough model of syntax evolution, under the assumption that a
strong interaction energy tends to cause parameters to align, as in the case of
ferromagnetic materials. We also study how the spin glass model needs to be
modified to account for entailment relations between syntactic parameters. This
modification leads naturally to a generalization of Potts models with external
magnetic field, which consists of a coupling at the vertices of an Ising model
and a Potts model with q=3, that have the same edge interactions. We describe
the results of simulations of the dynamics of these models, in different
temperature and energy regimes. We discuss the linguistic interpretation of the
parameters of the physical model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00506</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00506</id><created>2015-08-03</created><updated>2016-02-17</updated><authors><author><keyname>Sutter</keyname><forenames>Tobias</forenames></author><author><keyname>Ganguly</keyname><forenames>Arnab</forenames></author><author><keyname>Koeppl</keyname><forenames>Heinz</forenames></author></authors><title>A variational approach to path estimation and parameter inference of
  hidden diffusion processes</title><categories>math.OC cs.LG cs.SY math.PR math.ST stat.TH</categories><comments>36 pages, 2 figures, minor changes</comments><msc-class>62M05, 60J60, 60H10, 49J15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a hidden Markov model, where the signal process, given by a
diffusion, is only indirectly observed through some noisy measurements. The
article develops a variational method for approximating the hidden states of
the signal process given the full set of observations. This, in particular,
leads to systematic approximations of the smoothing densities of the signal
process. The paper then demonstrates how an efficient inference scheme, based
on this variational approach to the approximation of the hidden states, can be
designed to estimate the unknown parameters of stochastic differential
equations. Two examples at the end illustrate the efficacy and the accuracy of
the presented method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00507</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00507</id><created>2015-08-03</created><authors><author><keyname>Adel</keyname><forenames>Tameem</forenames></author><author><keyname>Wong</keyname><forenames>Alexander</forenames></author><author><keyname>Stashuk</keyname><forenames>Daniel</forenames></author></authors><title>A Weakly Supervised Learning Approach based on Spectral Graph-Theoretic
  Grouping</title><categories>cs.LG cs.AI</categories><comments>Submitted to IEEE Access</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, a spectral graph-theoretic grouping strategy for weakly
supervised classification is introduced, where a limited number of labelled
samples and a larger set of unlabelled samples are used to construct a larger
annotated training set composed of strongly labelled and weakly labelled
samples. The inherent relationship between the set of strongly labelled samples
and the set of unlabelled samples is established via spectral grouping, with
the unlabelled samples subsequently weakly annotated based on the strongly
labelled samples within the associated spectral groups. A number of similarity
graph models for spectral grouping, including two new similarity graph models
introduced in this study, are explored to investigate their performance in the
context of weakly supervised classification in handling different types of
data. Experimental results using benchmark datasets as well as real EMG
datasets demonstrate that the proposed approach to weakly supervised
classification can provide noticeable improvements in classification
performance, and that the proposed similarity graph models can lead to ultimate
learning results that are either better than or on a par with existing
similarity graph models in the context of spectral grouping for weakly
supervised classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00509</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00509</id><created>2015-08-03</created><updated>2015-08-11</updated><authors><author><keyname>Jahnz</keyname><forenames>Christoph</forenames></author></authors><title>Maintaining prediction quality under the condition of a growing
  knowledge space</title><categories>cs.AI cs.LG</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligence can be understood as an agent's ability to predict its
environment's dynamic by a level of precision which allows it to effectively
foresee opportunities and threats. Under the assumption that such intelligence
relies on a knowledge space any effective reasoning would benefit from a
maximum portion of useful and a minimum portion of misleading knowledge
fragments. It begs the question of how the quality of such knowledge space can
be kept high as the amount of knowledge keeps growing. This article proposes a
mathematical model to describe general principles of how quality of a growing
knowledge space evolves depending on error rate, error propagation and
countermeasures. There is also shown to which extend the quality of a knowledge
space collapses as removal of low quality knowledge fragments occurs too slowly
for a given knowledge space's growth rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00510</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00510</id><created>2015-08-03</created><authors><author><keyname>Geary</keyname><forenames>Cody</forenames></author><author><keyname>Meunier</keyname><forenames>Pierre-&#xc9;tienne</forenames></author><author><keyname>Schabanel</keyname><forenames>Nicolas</forenames></author><author><keyname>Seki</keyname><forenames>Shinnosuke</forenames></author></authors><title>Efficient Universal Computation by Greedy Molecular Folding</title><categories>cs.CG cs.CC cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and study the computational power of Oritatami, a theoretical
model to explore greedy molecular folding, by which the molecule begins to fold
before waiting the end of its production. This model is inspired by our recent
experimental work demonstrating the construction of shapes at the nanoscale by
folding an RNA molecule during its transcription from an engineered sequence of
synthetic DNA. While predicting the most likely conformation is known to be
NP-complete in other models, Oritatami sequences fold optimally in linear time.
Although our model uses only a small subset of the mechanisms known to be
involved in molecular folding, we show that it is capable of efficient
universal computation, implying that any extension of this model will have this
property as well.
  We develop several general design techniques for programming these molecules.
Our main result in this direction is an algorithm in time linear in the
sequence length, that finds a rule for folding the sequence deterministically
into a prescribed set of shapes depending of its environment. This shows the
corresponding problem is fixed-parameter tractable although we proved it is
NP-complete in the number of possible environments. This algorithm was used
effectively to design several key steps of our constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00514</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00514</id><created>2015-08-03</created><authors><author><keyname>Braverman</keyname><forenames>Mark</forenames></author><author><keyname>Gelles</keyname><forenames>Ran</forenames></author><author><keyname>Mao</keyname><forenames>Jieming</forenames></author><author><keyname>Ostrovsky</keyname><forenames>Rafail</forenames></author></authors><title>Coding for interactive communication correcting insertions and deletions</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the question of interactive communication, in which two remote
parties perform a computation while their communication channel is
(adversarially) noisy. We extend here the discussion into a more general and
stronger class of noise, namely, we allow the channel to perform insertions and
deletions of symbols. These types of errors may bring the parties &quot;out of
sync&quot;, so that there is no consensus regarding the current round of the
protocol.
  In this more general noise model, we obtain the first interactive coding
scheme that has a constant rate and resists noise rates of up to
$1/18-\varepsilon$. To this end we develop a novel primitive we name edit
distance tree code. The edit distance tree code is designed to replace the
Hamming distance constraints in Schulman's tree codes (STOC 93), with a
stronger edit distance requirement. However, the straightforward generalization
of tree codes to edit distance does not seem to yield a primitive that suffices
for communication in the presence of synchronization problems. Giving the
&quot;right&quot; definition of edit distance tree codes is a main conceptual
contribution of this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00522</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00522</id><created>2015-08-03</created><updated>2015-09-14</updated><authors><author><keyname>Kech</keyname><forenames>Michael</forenames></author></authors><title>Explicit Frames for Deterministic Phase Retrieval via PhaseLift</title><categories>cs.IT math.IT quant-ph</categories><comments>Minor changes (notation slightly changed, correction of inaccuracies
  and small mistakes)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explicitly give a frame of cardinality $5n-6$ such that every signal in
$\mathbb{C}^n$ can be recovered up to a phase from its associated intensity
measurements via the PhaseLift algorithm. Furthermore, we give explicit linear
measurements with $4r(n-r)+n-2r$ outcomes that enable the recovery of every
positive $n\times n$ matrix of rank at most $r$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00527</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00527</id><created>2015-08-03</created><authors><author><keyname>Mlika</keyname><forenames>Zoubeir</forenames></author><author><keyname>Driouch</keyname><forenames>Elmahdi</forenames></author><author><keyname>Ajib</keyname><forenames>Wessam</forenames></author><author><keyname>Elbiaze</keyname><forenames>Halima</forenames></author></authors><title>On the Base Station Association Problem in HetSNets</title><categories>cs.GT cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dense deployment of small-cell base stations in HetSNets requires
efficient resource allocation techniques. More precisely, the problem of
associating users to SBSs must be revised and carefully studied. This problem
is NP-hard and requires solving an integer optimization problem. In order to
efficiently solve this problem, we model it using non-cooperative game theory.
First, we design two non-cooperative games to solve the problem and show the
existence of pure Nash equilibria (PNE) in both games. These equilibria are
shown to be far from the social optimum. Hence, we propose a better game design
in order to approach this optimum. This new game is proved to have no PNE in
general. However, simulations show, for Rayleigh fading channels, that a PNE
always exists for all instances of the game. In addition, we show that its
prices of anarchy and stability are close to one. We propose a best response
dynamics (BRD) algorithm that converges to a PNE when it exists. Because of the
high information exchange of BRD, a completely distributed algorithm, based on
the theory of learning, is proposed. Simulations show that this algorithm has
tight-to-optimal performance and further it converges to a PNE (when existing)
with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00536</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00536</id><created>2015-08-03</created><updated>2016-02-17</updated><authors><author><keyname>Gao</keyname><forenames>Shuyang</forenames></author><author><keyname>Steeg</keyname><forenames>Greg Ver</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>Estimating Mutual Information by Local Gaussian Approximation</title><categories>cs.IT math.IT physics.data-an</categories><comments>To appear in the 31st Conference on Uncertainty in Artificial
  Intelligence (UAI), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating mutual information (MI) from samples is a fundamental problem in
statistics, machine learning, and data analysis. Recently it was shown that a
popular class of non-parametric MI estimators perform very poorly for strongly
dependent variables and have sample complexity that scales exponentially with
the true MI. This undesired behavior was attributed to the reliance of those
estimators on local uniformity of the underlying (and unknown) probability
density function. Here we present a novel semi-parametric estimator of mutual
information, where at each sample point, densities are {\em locally}
approximated by a Gaussians distribution. We demonstrate that the estimator is
asymptotically unbiased. We also show that the proposed estimator has a
superior performance compared to several baselines, and is able to accurately
measure relationship strengths over many orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00537</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00537</id><created>2015-08-03</created><authors><author><keyname>Nogueira</keyname><forenames>Rodrigo Frassetto</forenames></author><author><keyname>Lotufo</keyname><forenames>Roberto de Alencar</forenames></author><author><keyname>Machado</keyname><forenames>Rubens Campos</forenames></author></authors><title>Evaluating software-based fingerprint liveness detection using
  Convolutional Networks and Local Binary Patterns</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1301.3557 by other authors</comments><journal-ref>Biometric Measurements and Systems for Security and Medical
  Applications (BIOMS) Proceedings, 2014 IEEE Workshop on</journal-ref><doi>10.1109/BIOMS.2014.6951531</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the growing use of biometric authentication systems in the past years,
spoof fingerprint detection has become increasingly important. In this work, we
implement and evaluate two different feature extraction techniques for
software-based fingerprint liveness detection: Convolutional Networks with
random weights and Local Binary Patterns. Both techniques were used in
conjunction with a Support Vector Machine (SVM) classifier. Dataset
Augmentation was used to increase classifier's performance and a variety of
preprocessing operations were tested, such as frequency filtering, contrast
equalization, and region of interest filtering. The experiments were made on
the datasets used in The Liveness Detection Competition of years 2009, 2011 and
2013, which comprise almost 50,000 real and fake fingerprints' images. Our best
method achieves an overall rate of 95.2% of correctly classified samples - an
improvement of 35% in test error when compared with the best previously
published results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00540</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00540</id><created>2015-08-03</created><updated>2015-09-01</updated><authors><author><keyname>Sanli</keyname><forenames>Ceyda</forenames></author><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author></authors><title>Temporal Pattern of Online Communication Spike Trains in Spreading a
  Scientific Rumor: How Often, Who Interacts with Whom?</title><categories>cs.SI cs.IT math.IT</categories><comments>A statistical data analysis &amp; data mining on Social Dynamic Behavior,
  9 pages and 7 figures</comments><journal-ref>Front. Phys. 3:79, (2015)</journal-ref><doi>10.3389/fphy.2015.00079</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study complex time series (spike trains) of online user communication
while spreading messages about the discovery of the Higgs boson in Twitter. We
focus on online social interactions among users such as retweet, mention, and
reply, and construct different types of active (performing an action) and
passive (receiving an action) spike trains for each user. The spike trains are
analyzed by means of local variation, to quantify the temporal behavior of
active and passive users, as a function of their activity and popularity. We
show that the active spike trains are bursty, independently of their activation
frequency. For passive spike trains, in contrast, the local variation of
popular users presents uncorrelated (Poisson random) dynamics. We further
characterize the correlations of the local variation in different interactions.
We obtain high values of correlation, and thus consistent temporal behavior,
between retweets and mentions, but only for popular users, indicating that
creating online attention suggests an alignment in the dynamics of the two
interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00545</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00545</id><created>2015-08-03</created><updated>2015-08-03</updated><authors><author><keyname>Zhao</keyname><forenames>Jun</forenames></author><author><keyname>Ya&#x11f;an</keyname><forenames>Osman</forenames></author><author><keyname>Gligor</keyname><forenames>Virgil</forenames></author></authors><title>Connectivity in Secure Wireless Sensor Networks under Transmission
  Constraints</title><categories>cs.CR cs.DM cs.IT math.IT math.PR physics.soc-ph</categories><comments>Full version of a paper published in Annual Allerton Conference on
  Communication, Control, and Computing (Allerton) 2014</comments><doi>10.1109/ALLERTON.2014.7028605</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless sensor networks (WSNs), the Eschenauer-Gligor (EG) key
pre-distribution scheme is a widely recognized way to secure communications.
Although connectivity properties of secure WSNs with the EG scheme have been
extensively investigated, few results address physical transmission
constraints. These constraints reflect real-world implementations of WSNs in
which two sensors have to be within a certain distance from each other to
communicate. In this paper, we present zero-one laws for connectivity in WSNs
employing the EG scheme under transmission constraints. These laws help specify
the critical transmission ranges for connectivity. Our analytical findings are
confirmed via numerical experiments. In addition to secure WSNs, our
theoretical results are also applied to frequency hopping in wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00603</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00603</id><created>2015-08-03</created><updated>2015-08-13</updated><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Jin</keyname><forenames>Lingfei</forenames></author><author><keyname>Xing</keyname><forenames>Chaoping</forenames></author></authors><title>Efficient list decoding of punctured Reed-Muller codes</title><categories>cs.IT cs.CC math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Reed-Muller (RM) code encoding $n$-variate degree-$d$ polynomials over
$\mathbb{F}_q$ for $d &lt; q$ has relative distance $1-d/q$ and can be list
decoded from a $1-O(\sqrt{d/q})$ fraction of errors. In this work, for $d \ll
q$, we give a length-efficient puncturing of such codes which (almost) retains
the distance and list decodability properties of the Reed-Muller code, but has
much better rate. Specificially, when $q \gtrsim d^2/\epsilon^2$, we given an
explicit rate $\Omega_d(\epsilon)$ puncturing of Reed-Muller codes which have
relative distance at least $(1-\epsilon)$ and efficient list decoding up to
$(1-\sqrt{\epsilon})$ error fraction. This almost matches the performance of
random puncturings which work with the weaker field size requirement $q \gtrsim
d/\epsilon^2$. We can also improve the field size requirement to the optimal
(up to constant factors) $q \gtrsim d/\epsilon$, at the expense of a worse list
decoding radius of $1-\epsilon^{1/3}$ and rate $\Omega_d(\epsilon^2)$. The
first of the above trade-offs is obtained by substituting for the variables
functions with carefully chosen pole orders from an algebraic function field;
this leads to a puncturing for which the RM code is a subcode of a certain
algebraic-geometric code (which is known to be efficiently list decodable). The
second trade-off is obtained by concatenating this construction with a
Reed-Solomon based multiplication friendly pair, and using the list recovery
property of algebraic-geometric codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00614</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00614</id><created>2015-08-03</created><authors><author><keyname>Cseh</keyname><forenames>Agnes</forenames></author><author><keyname>Kavitha</keyname><forenames>Telikepalli</forenames></author></authors><title>Popular Edges and Dominant Matchings</title><categories>cs.DM</categories><msc-class>68W01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a bipartite graph G = (A u B, E) with strict preference lists and and
edge e*, we ask if there exists a popular matching in G that contains the edge
e*. We call this the popular edge problem. A matching M is popular if there is
no matching M' such that the vertices that prefer M' to M outnumber those that
prefer M to M'. It is known that every stable matching is popular; however G
may have no stable matching with the edge e* in it. In this paper we identify
another natural subclass of popular matchings called &quot;dominant matchings&quot; and
show that if there is a popular matching that contains the edge e*, then there
is either a stable matching that contains e* or a dominant matching that
contains e*. This allows us to design a linear time algorithm for the popular
edge problem. We also use dominant matchings to efficiently test if every
popular matching in G is stable or not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00618</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00618</id><created>2015-08-03</created><authors><author><keyname>Hoxha</keyname><forenames>Bardh</forenames></author><author><keyname>Mavridis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Fainekos</keyname><forenames>Georgios</forenames></author></authors><title>ViSpec: A graphical tool for elicitation of MTL requirements</title><categories>cs.SE</categories><comments>Technical report for the paper to be published in the 2015 IEEE/RSJ
  International Conference on Intelligent Robots and Systems held in Hamburg,
  Germany. Includes 10 pages and 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main barriers preventing widespread use of formal methods is the
elicitation of formal specifications. Formal specifications facilitate the
testing and verification process for safety critical robotic systems. However,
handling the intricacies of formal languages is difficult and requires a high
level of expertise in formal logics that many system developers do not have. In
this work, we present a graphical tool designed for the development and
visualization of formal specifications by people that do not have training in
formal logic. The tool enables users to develop specifications using a
graphical formalism which is then automatically translated to Metric Temporal
Logic (MTL). In order to evaluate the effectiveness of our tool, we have also
designed and conducted a usability study with cohorts from the academic student
community and industry. Our results indicate that both groups were able to
define formal requirements with high levels of accuracy. Finally, we present
applications of our tool for defining specifications for operation of robotic
surgery and autonomous quadcopter safe operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00625</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00625</id><created>2015-08-03</created><authors><author><keyname>Asteris</keyname><forenames>Megasthenis</forenames></author><author><keyname>Papailiopoulos</keyname><forenames>Dimitris</forenames></author><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author></authors><title>Sparse PCA via Bipartite Matchings</title><categories>stat.ML cs.DS cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following multi-component sparse PCA problem: given a set of
data points, we seek to extract a small number of sparse components with
disjoint supports that jointly capture the maximum possible variance. These
components can be computed one by one, repeatedly solving the single-component
problem and deflating the input data matrix, but as we show this greedy
procedure is suboptimal. We present a novel algorithm for sparse PCA that
jointly optimizes multiple disjoint components. The extracted features capture
variance that lies within a multiplicative factor arbitrarily close to 1 from
the optimal. Our algorithm is combinatorial and computes the desired components
by solving multiple instances of the bipartite maximum weight matching problem.
Its complexity grows as a low order polynomial in the ambient dimension of the
input data matrix, but exponentially in its rank. However, it can be
effectively applied on a low-dimensional sketch of the data; this allows us to
obtain polynomial-time approximation guarantees via spectral bounds. We
evaluate our algorithm on real data-sets and empirically demonstrate that in
many cases it outperforms existing, deflation-based approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00628</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00628</id><created>2015-08-03</created><authors><author><keyname>Lopes</keyname><forenames>Cristina V.</forenames></author><author><keyname>Ossher</keyname><forenames>Joel</forenames></author></authors><title>How Scale Affects Structure in Java Programs</title><categories>cs.SE cs.PL</categories><comments>ACM Conference on Object-Oriented Programming, Systems, Languages and
  Applications (OOPSLA), October 2015. (Preprint)</comments><doi>10.1145/2858965.2814300</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many internal software metrics and external quality attributes of Java
programs correlate strongly with program size. This knowledge has been used
pervasively in quantitative studies of software through practices such as
normalization on size metrics. This paper reports size-related super- and
sublinear effects that have not been known before. Findings obtained on a very
large collection of Java programs -- 30,911 projects hosted at Google Code as
of Summer 2011 -- unveils how certain characteristics of programs vary
disproportionately with program size, sometimes even non-monotonically. Many of
the specific parameters of nonlinear relations are reported. This result gives
further insights for the differences of &quot;programming in the small&quot; vs.
&quot;programming in the large.&quot; The reported findings carry important consequences
for OO software metrics, and software research in general: metrics that have
been known to correlate with size can now be properly normalized so that all
the information that is left in them is size-independent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00635</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00635</id><created>2015-08-03</created><authors><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author></authors><title>Bayesian mixtures of spatial spline regressions</title><categories>stat.ME cs.LG stat.CO stat.ML</categories><msc-class>62-XX, 62Fxx, 62F15, 62H30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work relates the framework of model-based clustering for spatial
functional data where the data are surfaces. We first introduce a Bayesian
spatial spline regression model with mixed-effects (BSSR) for modeling spatial
function data. The BSSR model is based on Nodal basis functions for spatial
regression and accommodates both common mean behavior for the data through a
fixed-effects part, and variability inter-individuals thanks to a
random-effects part. Then, in order to model populations of spatial functional
data issued from heterogeneous groups, we integrate the BSSR model into a
mixture framework. The resulting model is a Bayesian mixture of spatial spline
regressions with mixed-effects (BMSSR) used for density estimation and
model-based surface clustering. The models, through their Bayesian formulation,
allow to integrate possible prior knowledge on the data structure and
constitute a good alternative to recent mixture of spatial spline regressions
model estimated in a maximum likelihood framework via the
expectation-maximization (EM) algorithm. The Bayesian model inference is
performed by Markov Chain Monte Carlo (MCMC) sampling. We derive two Gibbs
sampler to infer the BSSR and the BMSSR models and apply them on simulated
surfaces and a real problem of handwritten digit recognition using the MNIST
data set. The obtained results highlight the potential benefit of the proposed
Bayesian approaches for modeling surfaces possibly dispersed in particular in
clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00636</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00636</id><created>2015-08-03</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Low-Rank Signal Processing: Design, Algorithms for Dimensionality
  Reduction and Applications</title><categories>cs.IT math.IT</categories><comments>23 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a tutorial on reduced-rank signal processing, design methods and
algorithms for dimensionality reduction, and cover a number of important
applications. A general framework based on linear algebra and linear estimation
is employed to introduce the reader to the fundamentals of reduced-rank signal
processing and to describe how dimensionality reduction is performed on an
observed discrete-time signal. A unified treatment of dimensionality reduction
algorithms is presented with the aid of least squares optimization techniques,
in which several techniques for designing the transformation matrix that
performs dimensionality reduction are reviewed. Among the dimensionality
reduction techniques are those based on the eigen-decomposition of the observed
data vector covariance matrix, Krylov subspace methods, joint and iterative
optimization (JIO) algorithms and JIO with simplified structures and switching
(JIOS) techniques. A number of applications are then considered using a unified
treatment, which includes wireless communications, sensor and array signal
processing, and speech, audio, image and video processing. This tutorial
concludes with a discussion of future research directions and emerging topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00639</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00639</id><created>2015-08-03</created><authors><author><keyname>Vu</keyname><forenames>Tung T.</forenames></author><author><keyname>Kha</keyname><forenames>Ha Hoang</forenames></author><author><keyname>Duong</keyname><forenames>Trung Q.</forenames></author><author><keyname>Vo</keyname><forenames>Nguyen-Son</forenames></author></authors><title>Wiretapped Signal Leakage Minimization for Secure Multiuser MIMO Systems
  via Interference Alignment</title><categories>cs.IT math.IT math.OC</categories><comments>submitted to the international conference on Advanced Technologies
  for Communications (ATC) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, interference alignment (IA) is designed for secure multiuser
multiple-input multiple-output systems in the presence of an eavesdropper. The
proposed IA technique designs the transmit precoding and receiving subspace
matrices to minimize both the total inter-main-link interference and the
wiretapped signals. With perfect channel state information is known at the
transmitters (Txs), the cost function of the optimization problem is
alternatively minimized over the precoding matrices at the Txs and the
receiving subspace matrices at the receivers (Rxs) and the eavesdropper. The
feasible condition, the proof of convergence of the proposed IA approach are
provided. Numerical results reveal a significant improvement in terms of
average secrecy sum rate of our IA algorithm as compared to the conventional IA
design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00641</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00641</id><created>2015-08-03</created><updated>2015-08-29</updated><authors><author><keyname>Tekin</keyname><forenames>Cem</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Staged Multi-armed Bandits</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In conventional multi-armed bandits (MAB) and other reinforcement learning
methods, the learner sequentially chooses actions and obtains a reward (which
can be possibly missing, delayed or erroneous) after each taken action. This
reward is then used by the learner to improve its future decisions. However, in
numerous applications, ranging from personalized patient treatment to
personalized web-based education, the learner does not obtain rewards after
each action, but only after sequences of actions are taken, intermediate
feedbacks are observed, and a final decision is made based on which a reward is
obtained. In this paper, we introduce a new class of reinforcement learning
methods which can operate in such settings. We refer to this class as staged
multi-armed bandits (S-MAB). S-MAB proceeds in rounds, each composed of several
stages; in each stage, the learner chooses an action and observes a feedback
signal. Upon each action selection a feedback signal is observed, whilst the
reward of the selected sequence of actions is only revealed after the learner
selects a stop action that ends the current round. The reward of the round
depends both on the sequence of actions and the sequence of observed feedbacks.
The goal of the learner is to maximize its total expected reward over all
rounds by learning to choose the best sequence of actions based on the feedback
it gets about these actions. First, we define an oracle benchmark, which
sequentially selects the actions that maximize the expected immediate reward.
This benchmark is known to be approximately optimal when the reward sequence
associated with the selected actions is adaptive submodular. Then, we propose
our online learning algorithm, for which we prove that the regret is
logarithmic in the number of rounds and linear in the number of stages with
respect to the oracle benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00654</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00654</id><created>2015-08-04</created><updated>2016-01-31</updated><authors><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Kekatos</keyname><forenames>Vassilis</forenames></author><author><keyname>Conejo</keyname><forenames>Antonio J.</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Ergodic Energy Management Leveraging Resource Variability in
  Distribution Grids</title><categories>cs.SY</categories><comments>11 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contemporary electricity distribution systems are being challenged by the
variability of renewable energy sources. Slow response times and long energy
management periods cannot efficiently integrate intermittent renewable
generation and demand. Yet stochasticity can be judiciously coupled with system
flexibilities to enhance grid operation efficiency. Voltage magnitudes for
instance can transiently exceed regulation limits, while smart inverters can be
overloaded over short time intervals. To implement such a mode of operation, an
ergodic energy management framework is developed here. Considering a
distribution grid with distributed energy sources and a feed-in tariff program,
active power curtailment and reactive power compensation are formulated as a
stochastic optimization problem. Tighter operational constraints are enforced
in an average sense, while looser margins are enforced to be satisfied at all
times. Stochastic dual subgradient solvers are developed based on exact and
approximate grid models of varying complexity. Numerical tests on a real-world
56-bus distribution grid and the IEEE 123-bus test feeder relying on both grid
models corroborate the advantages of the novel schemes over their deterministic
alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00655</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00655</id><created>2015-08-04</created><authors><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Reddi</keyname><forenames>Sashank J.</forenames></author><author><keyname>Poczos</keyname><forenames>Barnabas</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Adaptivity and Computation-Statistics Tradeoffs for Kernel and Distance
  based High Dimensional Two Sample Testing</title><categories>math.ST cs.AI cs.IT cs.LG math.IT stat.ML stat.TH</categories><comments>35 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonparametric two sample testing is a decision theoretic problem that
involves identifying differences between two random variables without making
parametric assumptions about their underlying distributions. We refer to the
most common settings as mean difference alternatives (MDA), for testing
differences only in first moments, and general difference alternatives (GDA),
which is about testing for any difference in distributions. A large number of
test statistics have been proposed for both these settings. This paper connects
three classes of statistics - high dimensional variants of Hotelling's t-test,
statistics based on Reproducing Kernel Hilbert Spaces, and energy statistics
based on pairwise distances. We ask the question: how much statistical power do
popular kernel and distance based tests for GDA have when the unknown
distributions differ in their means, compared to specialized tests for MDA?
  We formally characterize the power of popular tests for GDA like the Maximum
Mean Discrepancy with the Gaussian kernel (gMMD) and bandwidth-dependent
variants of the Energy Distance with the Euclidean norm (eED) in the
high-dimensional MDA regime. Some practically important properties include (a)
eED and gMMD have asymptotically equal power; furthermore they enjoy a free
lunch because, while they are additionally consistent for GDA, they also have
the same power as specialized high-dimensional t-test variants for MDA. All
these tests are asymptotically optimal (including matching constants) under MDA
for spherical covariances, according to simple lower bounds, (b) The power of
gMMD is independent of the kernel bandwidth, as long as it is larger than the
choice made by the median heuristic, (c) There is a clear and smooth
computation-statistics tradeoff for linear-time, subquadratic-time and
quadratic-time versions of these tests, with more computation resulting in
higher power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00657</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00657</id><created>2015-08-04</created><updated>2015-08-11</updated><authors><author><keyname>Ballesteros</keyname><forenames>Miguel</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author></authors><title>Improved Transition-Based Parsing by Modeling Characters instead of
  Words with LSTMs</title><categories>cs.CL</categories><comments>In Proceedings of EMNLP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present extensions to a continuous-state dependency parsing method that
makes it applicable to morphologically rich languages. Starting with a
high-performance transition-based parser that uses long short-term memory
(LSTM) recurrent neural networks to learn representations of the parser state,
we replace lookup-based word representations with representations constructed
from the orthographic representations of the words, also using LSTMs. This
allows statistical sharing across word forms that are similar on the surface.
Experiments for morphologically rich languages show that the parsing model
benefits from incorporating the character-based encodings of words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00663</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00663</id><created>2015-08-04</created><authors><author><keyname>Yu</keyname><forenames>James J. Q.</forenames></author><author><keyname>Lin</keyname><forenames>Junhao</forenames></author><author><keyname>Lam</keyname><forenames>Albert Y. S.</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author></authors><title>Coordinated Electric Vehicle Charging Control with Aggregator Power
  Trading and Indirect Load Control</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the increasing concern on greenhouse gas emmissions and fossil fuel
security, Electric Vehicles (EVs) have attracted much attention in recent
years. However, the increasing popularity of EVs may cause stability issues to
the power grid if their charging behaviors are uncoordinated. In order to
address this problem, we propose a novel coordinated strategy for large-scale
EV charging. We formulate the energy trade among aggregators with locational
marginal pricing to maximize the aggregator profits and to indirectly control
the loads to reduce power network congestion. We first develop a centralized
iterative charging strategy, and then present a distributed optimization-based
heuristic to overcome the high computational complexity and user privacy
issues. To evaluate our proposed approach, a modified IEEE 118 bus testing
system is employed with 10 aggregators serving 30 000 EVs. The simulation
results indicate that our proposed approach can effectively increase the total
profit of aggregators, and enhance the power grid stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00664</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00664</id><created>2015-08-04</created><authors><author><keyname>Ravi</keyname><forenames>Jithin</forenames></author><author><keyname>Dey</keyname><forenames>Bikash Kumar</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Oblivious Transfer over Wireless Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of oblivious transfer (OT) over OFDM and MIMO
wireless communication systems where only the receiver knows the channel state
information. The sender and receiver also have unlimited access to a noise-free
real channel. Using a physical layer approach, based on the properties of the
noisy fading channel, we propose a scheme that enables the transmitter to send
obliviously one-of-two files, i.e., without knowing which one has been actually
requested by the receiver, while also ensuring that the receiver does not get
any information about the other file.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00671</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00671</id><created>2015-08-04</created><authors><author><keyname>Mathur</keyname><forenames>Rajesh</forenames></author><author><keyname>Miles</keyname><forenames>Scott</forenames></author><author><keyname>Du</keyname><forenames>Miao</forenames></author></authors><title>Adaptive Automation: Leveraging Machine Learning to Support
  Uninterrupted Automated Testing of Software Applications</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Checking software application suitability using automated software tools has
become a vital element for most organisations irrespective of whether they
produce in-house software or simply customise off-the-shelf software
applications for internal use. As software solutions become ever more complex,
the industry becomes increasingly dependent on software automation tools, yet
the brittle nature of the available software automation tools limits their
effectiveness. Companies invest significantly in obtaining and implementing
automation software but most of the tools fail to deliver when the cost of
maintaining an effective automation test suite exceeds the cost and time that
would have otherwise been spent on manual testing. A failing in the current
generation of software automation tools is they do not adapt to unexpected
modifications and obstructions without frequent (and time expensive) manual
interference. Such issues are commonly acknowledged amongst industry
practitioners, yet none of the current generation of tools have leveraged the
advances in machine learning and artificial intelligence to address these
problems.
  This paper proposes a framework solution that utilises machine learning
concepts, namely fuzzy matching and error recovery. The suggested solution
applies adaptive techniques to recover from unexpected obstructions that would
otherwise have prevented the script from proceeding. Recovery details are
presented to the user in a report which can be analysed to determine if the
recovery procedure was acceptable and the framework will adapt future runs
based on the decisions of the user. Using this framework, a practitioner can
run the automated suits without human intervention while minimising the risk of
schedule delays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00679</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00679</id><created>2015-08-04</created><authors><author><keyname>Xiao</keyname><forenames>Kexin</forenames></author><author><keyname>Xiao</keyname><forenames>Baicen</forenames></author><author><keyname>Zhang</keyname><forenames>Shutian</forenames></author><author><keyname>Chen</keyname><forenames>Zhiyong</forenames></author><author><keyname>Xia</keyname><forenames>Bin</forenames></author></authors><title>Simplified Multiuser Detection for SCMA with Sum-Product Algorithm</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse code multiple access (SCMA) is a novel non-orthogonal multiple access
technique, which fully exploits the shaping gain of multi-dimensional
codewords. However, the lack of simplified multiuser detection algorithm
prevents further implementation due to the inherently high computation
complexity. In this paper, general SCMA detector algorithms based on
Sum-product algorithm are elaborated. Then two improved algorithms are
proposed, which simplify the detection structure and curtail exponent
operations quantitatively in logarithm domain. Furthermore, to analyze these
detection algorithms fairly, we derive theoretical expression of the average
mutual information (AMI) of SCMA (SCMA-AMI), and employ a statistical method to
calculate SCMA-AMI based specific detection algorithm. Simulation results show
that the performance is almost as well as the based message passing algorithm
in terms of both BER and AMI while the complexity is significantly decreased,
compared to the traditional Max-Log approximation method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00680</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00680</id><created>2015-08-04</created><authors><author><keyname>Xiao</keyname><forenames>Baicen</forenames></author><author><keyname>Xiao</keyname><forenames>Kexin</forenames></author><author><keyname>Zhang</keyname><forenames>Shutian</forenames></author><author><keyname>Chen</keyname><forenames>Zhiyong</forenames></author><author><keyname>Xia</keyname><forenames>Bin</forenames></author><author><keyname>Liu</keyname><forenames>Hui</forenames></author></authors><title>Iterative detection and decoding for SCMA systems with LDPC codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse code multiple access (SCMA) is a promising multiplexing approach to
achieve high system capacity. In this paper, we develop a novel iterative
detection and decoding scheme for SCMA systems combined with Low-density
Parity-check (LDPC) decoding. In particular, we decompose the output of the
message passing algorithm (MPA) based SCMA multiuser detection into intrinsic
part and prior part. Then we design a joint detection and decoding scheme which
iteratively exchanges the intrinsic information between the detector and the
decoder, yielding a satisfied performance gain. Moreover, the proposed scheme
has almost the same complexity compared to the traditional receiver for
LDPC-coded SCMA systems. As numerical results demonstrate, the proposed scheme
has a substantial gain over the traditional SCMA receiver on AWGN channels and
Rayleigh fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00683</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00683</id><created>2015-08-04</created><authors><author><keyname>Grastien</keyname><forenames>Alban</forenames></author></authors><title>Interval Predictability in Discrete Event Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of predictability in partially observable
discrete event systems, i.e., the question whether an observer can predict the
occurrence of a fault. We extend the definition of predictability to consider
the time interval where the fault will occur: the $(i,j)$-predictability does
not only specify that the fault will be predicted before it occurs, but also
that the predictor will be able to predict that its occurrence will occur in
$i$ to $j$ observations from now. We also provide a quadratic algorithm that
decides predictability of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00688</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00688</id><created>2015-08-04</created><authors><author><keyname>Oancea</keyname><forenames>Bogdan</forenames></author><author><keyname>Andrei</keyname><forenames>Tudorel</forenames></author><author><keyname>Dragoescu</keyname><forenames>Raluca Mariana</forenames></author></authors><title>Accelerating R with high performance linear algebra libraries</title><categories>cs.MS</categories><msc-class>68N99</msc-class><acm-class>H.3.4</acm-class><journal-ref>Romanian Statistical Review, No. 3, 2015</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Linear algebra routines are basic building blocks for the statistical
software. In this paper we analyzed how can we can improve R performance for
matrix computations. We benchmarked few matrix operations using the standard
linear algebra libraries included in the R distribution and high performance
libraries like OpenBLAS, GotoBLAS and MKL. Our tests showed the the best
results are obtained with the MKL library, the other two libraries having
similar performances, but lower than MKL
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00689</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00689</id><created>2015-08-04</created><authors><author><keyname>Loeliger</keyname><forenames>Hans-Andrea</forenames></author><author><keyname>Vontobel</keyname><forenames>Pascal O.</forenames></author></authors><title>Factor Graphs for Quantum Probabilities</title><categories>cs.IT cs.AI math.IT math.ST quant-ph stat.TH</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A factor-graph representation of quantum-mechanical probabilities (involving
any number of measurements) is proposed. Unlike standard statistical models,
the proposed representation uses auxiliary variables (state variables) that are
not random variables. All joint probability distributions are marginals of some
complex-valued function q, and it is demonstrated how the basic concepts of
quantum mechanics relate to factorizations and marginals of q.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00690</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00690</id><created>2015-08-04</created><authors><author><keyname>Ivanyos</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Qiao</keyname><forenames>Youming</forenames></author><author><keyname>Subrahmanyam</keyname><forenames>K. V.</forenames></author></authors><title>Non-commutative Edmonds' problem and matrix semi-invariants</title><categories>cs.DS cs.CC math.AC math.RA</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1967, Edmonds introduced the problem of computing the rank over the
rational function field of an $n\times n$ matrix $T$ with integral homogeneous
linear polynomials. In this paper, we consider the non-commutative version of
Edmonds' problem: compute the rank of $T$ over the free skew field. It is known
that this problem relates to the ring of matrix semi-invariants. In particular,
if the nullcone of matrix semi-invariants is defined by elements of degree
$\leq \sigma$, then there follows a $\mathrm{poly}(n, \sigma)$-time randomized
algorithm to decide whether the non-commutative rank of $T$ is $&lt;n$. To our
knowledge, previously the best bound for $\sigma$ was $O(n^2\cdot 4^{n^2})$
over algebraically closed fields of characteristic $0$ (Derksen, 2001).
  In this article we prove the following results:
  (1) We observe that by using an algorithm of Gurvits, and assuming the above
bound $\sigma$ for $R(n, m)$ over $\mathbb{Q}$, deciding whether $T$ has
non-commutative rank $&lt;n$ over $\mathbb{Q}$ can be done deterministically in
time polynomial in the input size and $\sigma$.
  (2) When $\mathbb{F}$ is large enough, we devise a deterministic algorithm
for non-commutative Edmonds' problem in time polynomial in $(n+1)!$, with the
following consequences.
  (2.a) If the commutative rank and the non-commutative rank of $T$ differ by a
constant, then there exists a randomized efficient algorithm that computes the
non-commutative rank of $T$.
  (2.b) We prove that $\sigma\leq (n+1)!$. This not only improves the bound
obtained from Derksen's work over algebraically closed field of characteristic
$0$ but, more importantly, also provides for the first time an explicit bound
on $\sigma$ for matrix semi-invariants over fields of positive characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00691</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00691</id><created>2015-08-04</created><authors><author><keyname>Fu</keyname><forenames>Po-Chun</forenames></author><author><keyname>Li</keyname><forenames>Pei-Rong</forenames></author><author><keyname>Wei</keyname><forenames>Li-Ming</forenames></author><author><keyname>Chen</keyname><forenames>Chang-Lin</forenames></author><author><keyname>Lin</keyname><forenames>Che</forenames></author></authors><title>Deterministic Differential Search Algorithm for Distributed Sensor/Relay
  Networks</title><categories>cs.IT cs.DC cs.NI math.IT</categories><comments>2 pages, 1 figure, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For distributed sensor/relay networks, high reliability and power efficiency
are often required. However, several implementation issues arise in practice.
One such problem is that all the distributed transmitters have limited power
supply since the power source of the transmitters cannot be recharged
continually. To resolve this, distributed beamforming has been proposed as a
viable solution where all distributed transmitters seek to align in phase at
the receiver end. However, it is difficult to implement such transmit
beamforming in a distributed fashion in practice since perfect channel state
information (CSI) need to be made available at all distributed transmitters,
requiring tremendous overhead to feed back CSI from the receiver to all
distributed transmitters.
  In this paper, we propose a novel algorithm that belongs to the category of
deterministic phase adjustment algorithm: the Deterministic Differential Search
Algorithm (DDSA), where the differences between the measured received signal
strength (RSS) are utilized judiciously to help us predict the deterministic
phase adjustment done at distributed transmitters. Numerical simulations
demonstrate rapid convergence to a pre-determined threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00693</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00693</id><created>2015-08-04</created><updated>2015-11-02</updated><authors><author><keyname>Saramaki</keyname><forenames>Jari</forenames></author><author><keyname>Holme</keyname><forenames>Petter</forenames></author></authors><title>Exploring Temporal Networks with Greedy Walks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1140/epjb/e2015-60660-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temporal networks come with a wide variety of heterogeneities, from
burstiness of event sequences to correlations between timings of node and link
activations. In this paper, we set to explore the latter by using greedy walks
as probes of temporal network structure. Given a temporal network (a sequence
of contacts), greedy walks proceed from node to node by always following the
first available contact. Because of this, their structure is particularly
sensitive to temporal-topological patterns involving repeated contacts between
sets of nodes. This becomes evident in their small coverage per step as
compared to a temporal reference model -- in empirical temporal networks,
greedy walks often get stuck within small sets of nodes because of correlated
contact patterns. While this may also happen in static networks that have
pronounced community structure, the use of the temporal reference model takes
the underlying static network structure out of the equation and indicates that
there is a purely temporal reason for the observations. Further analysis of the
structure of greedy walks indicates that burst trains, sequences of repeated
contacts between node pairs, are the dominant factor. However, there are larger
patterns too, as shown with non-backtracking greedy walks. We proceed further
to study the entropy rates of greedy walks, and show that the sequences of
visited nodes are more structured and predictable in original data as compared
to temporally uncorrelated references. Taken together, these results indicate a
richness of correlated temporal-topological patterns in temporal networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00703</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00703</id><created>2015-08-04</created><authors><author><keyname>Goel</keyname><forenames>Naman</forenames></author><author><keyname>Agrawal</keyname><forenames>Divyakant</forenames></author><author><keyname>Chawla</keyname><forenames>Sanjay</forenames></author><author><keyname>Elmagarmid</keyname><forenames>Ahmed</forenames></author></authors><title>Parameter Database : Data-centric Synchronization for Scalable Machine
  Learning</title><categories>cs.DB cs.LG</categories><report-no>QCRI-TR-2015-003</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new data-centric synchronization framework for carrying out of
machine learning (ML) tasks in a distributed environment. Our framework
exploits the iterative nature of ML algorithms and relaxes the application
agnostic bulk synchronization parallel (BSP) paradigm that has previously been
used for distributed machine learning. Data-centric synchronization complements
function-centric synchronization based on using stale updates to increase the
throughput of distributed ML computations. Experiments to validate our
framework suggest that we can attain substantial improvement over BSP while
guaranteeing sequential correctness of ML tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00715</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00715</id><created>2015-08-04</created><authors><author><keyname>Yang</keyname><forenames>Zhilin</forenames></author><author><keyname>Tang</keyname><forenames>Jie</forenames></author></authors><title>Multi-Source Bayesian Embeddings for Learning Social Knowledge Graphs</title><categories>cs.CL cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the semantics of social networks is important for social
mining. However, few works have studied how to bridge the gap between
large-scale social networks and abundant collective knowledge. In this work, we
study the problem of learning social knowledge graphs, which aims to accurately
connect social network vertices to knowledge concepts.
  We propose a multi-source Bayesian embedding model, GenVector, to jointly
incorporate topic models and word/network embeddings. The model leverages
large-scale unlabeled data by incorporating the embeddings, and co-represents
social network vertices and knowledge concepts in a shared latent topic space.
  Experiments on three datasets show that our method outperforms
state-of-the-art methods. We deploy our algorithm on a large-scale academic
social network by linking 39 million researchers to 35 million knowledge
concepts, and decrease the error rate by 67% according to online test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00722</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00722</id><created>2015-08-04</created><authors><author><keyname>Li</keyname><forenames>Shao-Yuan</forenames></author><author><keyname>Jiang</keyname><forenames>Yuan</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Multi-Label Active Learning from Crowds</title><categories>cs.LG cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-label active learning is a hot topic in reducing the label cost by
optimally choosing the most valuable instance to query its label from an
oracle. In this paper, we consider the poolbased multi-label active learning
under the crowdsourcing setting, where during the active query process, instead
of resorting to a high cost oracle for the ground-truth, multiple low cost
imperfect annotators with various expertise are available for labeling. To deal
with this problem, we propose the MAC (Multi-label Active learning from Crowds)
approach which incorporate the local influence of label correlations to build a
probabilistic model over the multi-label classifier and annotators. Based on
this model, we can estimate the labels for instances as well as the expertise
of each annotator. Then we propose the instance selection and annotator
selection criteria that consider the uncertainty/diversity of instances and the
reliability of annotators, such that the most reliable annotator will be
queried for the most valuable instances. Experimental results demonstrate the
effectiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00731</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00731</id><created>2015-08-04</created><updated>2015-08-27</updated><authors><author><keyname>Clifford</keyname><forenames>Rapha&#xeb;l</forenames></author><author><keyname>Fontaine</keyname><forenames>Allyx</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author><author><keyname>Sach</keyname><forenames>Benjamin</forenames></author><author><keyname>Starikovskaya</keyname><forenames>Tatiana</forenames></author></authors><title>The k-mismatch problem revisited</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the complexity of one of the most basic problems in pattern
matching. In the k-mismatch problem we must compute the Hamming distance
between a pattern of length m and every m-length substring of a text of length
n, as long as that Hamming distance is at most k. Where the Hamming distance is
greater than k at some alignment of the pattern and text, we simply output
&quot;No&quot;.
  We study this problem in both the standard offline setting and also as a
streaming problem. In the streaming k-mismatch problem the text arrives one
symbol at a time and we must give an output before processing any future
symbols. Our main results are as follows:
  1) Our first result is a deterministic $O(n k^2\log{k} / m+n \text{polylog}
m)$ time offline algorithm for k-mismatch on a text of length n. This is a
factor of k improvement over the fastest previous result of this form from SODA
2000 by Amihood Amir et al.
  2) We then give a randomised and online algorithm which runs in the same time
complexity but requires only $O(k^2\text{polylog} {m})$ space in total.
  3) Next we give a randomised $(1+\epsilon)$-approximation algorithm for the
streaming k-mismatch problem which uses $O(k^2\text{polylog} m / \epsilon^2)$
space and runs in $O(\text{polylog} m / \epsilon^2)$ worst-case time per
arriving symbol.
  4) Finally we combine our new results to derive a randomised
$O(k^2\text{polylog} {m})$ space algorithm for the streaming k-mismatch problem
which runs in $O(\sqrt{k}\log{k} + \text{polylog} {m})$ worst-case time per
arriving symbol. This improves the best previous space complexity for streaming
k-mismatch from FOCS 2009 by Benny Porat and Ely Porat by a factor of k. We
also improve the time complexity of this previous result by an even greater
factor to match the fastest known offline algorithm (up to logarithmic
factors).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00749</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00749</id><created>2015-08-04</created><authors><author><keyname>Krilavicius</keyname><forenames>Tomas</forenames></author><author><keyname>Zliobaite</keyname><forenames>Indre</forenames></author><author><keyname>Simonavicius</keyname><forenames>Henrikas</forenames></author><author><keyname>Jarusevicius</keyname><forenames>Laimonas</forenames></author></authors><title>Predicting respiratory motion for real-time tumour tracking in
  radiotherapy</title><categories>cs.AI cs.CE physics.med-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose. Radiation therapy is a local treatment aimed at cells in and around
a tumor. The goal of this study is to develop an algorithmic solution for
predicting the position of a target in 3D in real time, aiming for the short
fixed calibration time for each patient at the beginning of the procedure.
Accurate predictions of lung tumor motion are expected to improve the precision
of radiation treatment by controlling the position of a couch or a beam in
order to compensate for respiratory motion during radiation treatment.
  Methods. For developing the algorithmic solution, data mining techniques are
used. A model form from the family of exponential smoothing is assumed, and the
model parameters are fitted by minimizing the absolute disposition error, and
the fluctuations of the prediction signal (jitter). The predictive performance
is evaluated retrospectively on clinical datasets capturing different behavior
(being quiet, talking, laughing), and validated in real-time on a prototype
system with respiratory motion imitation.
  Results. An algorithmic solution for respiratory motion prediction (called
ExSmi) is designed. ExSmi achieves good accuracy of prediction (error $4-9$
mm/s) with acceptable jitter values (5-7 mm/s), as tested on out-of-sample
data. The datasets, the code for algorithms and the experiments are openly
available for research purposes on a dedicated website.
  Conclusions. The developed algorithmic solution performs well to be
prototyped and deployed in applications of radiotherapy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00761</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00761</id><created>2015-08-04</created><authors><author><keyname>Li</keyname><forenames>Shun</forenames></author><author><keyname>Zhu</keyname><forenames>Changye</forenames></author><author><keyname>Cui</keyname><forenames>Liqing</forenames></author><author><keyname>Zhao</keyname><forenames>Nan</forenames></author><author><keyname>Li</keyname><forenames>Baobin</forenames></author><author><keyname>Zhu</keyname><forenames>Tingshao</forenames></author></authors><title>Recognition of Emotions using Kinects</title><categories>cs.CY cs.CV cs.HC</categories><comments>15 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Psychological studies indicate that emotional states are expressed in the way
people walk and the human gait is investigated in terms of its ability to
reveal a person's emotional state. And Microsoft Kinect is a rapidly
developing, inexpensive, portable and no-marker motion capture system. This
paper gives a new referable method to do emotion recognition, by using
Microsoft Kinect to do gait pattern analysis, which has not been reported. $59$
subjects are recruited in this study and their gait patterns are record by two
Kinect cameras. Significant joints selecting, Coordinate system transforming,
Slider window gauss filter, Differential operation, and Data segmentation are
used in data preprocessing. Feature extracting is based on Fourier
transformation. By using the NaiveBayes, RandomForests, libSVM and SMO
classification, the recognition rate of natural and unnatural emotions can
reach above 70%.It is concluded that using the Kinect system can be a new
method in recognition of emotions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00776</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00776</id><created>2015-08-04</created><authors><author><keyname>Gaidon</keyname><forenames>Adrien</forenames></author><author><keyname>Vig</keyname><forenames>Eleonora</forenames></author></authors><title>Online Domain Adaptation for Multi-Object Tracking</title><categories>cs.CV</categories><comments>To appear at BMVC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatically detecting, labeling, and tracking objects in videos depends
first and foremost on accurate category-level object detectors. These might,
however, not always be available in practice, as acquiring high-quality large
scale labeled training datasets is either too costly or impractical for all
possible real-world application scenarios. A scalable solution consists in
re-using object detectors pre-trained on generic datasets. This work is the
first to investigate the problem of on-line domain adaptation of object
detectors for causal multi-object tracking (MOT). We propose to alleviate the
dataset bias by adapting detectors from category to instances, and back: (i) we
jointly learn all target models by adapting them from the pre-trained one, and
(ii) we also adapt the pre-trained model on-line. We introduce an on-line
multi-task learning algorithm to efficiently share parameters and reduce drift,
while gradually improving recall. Our approach is applicable to any linear
object detector, and we evaluate both cheap &quot;mini-Fisher Vectors&quot; and expensive
&quot;off-the-shelf&quot; ConvNet features. We quantitatively measure the benefit of our
domain adaptation strategy on the KITTI tracking benchmark and on a new dataset
(PASCAL-to-KITTI) we introduce to study the domain mismatch problem in MOT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00778</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00778</id><created>2015-08-04</created><authors><author><keyname>Chepoi</keyname><forenames>Victor</forenames></author><author><keyname>Estellon</keyname><forenames>Bertrand</forenames></author><author><keyname>Naves</keyname><forenames>Guyslain</forenames></author></authors><title>Packing and covering with balls on Busemann surfaces</title><categories>math.MG cs.DM</categories><comments>22 pages</comments><msc-class>52A35, 68R05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we prove that for any compact subset $S$ of a Busemann surface
$({\mathcal S},d)$ (in particular, for any simple polygon with geodesic metric)
and any positive number $\delta$, the minimum number of closed balls of radius
$\delta$ with centers at $\mathcal S$ and covering the set $S$ is at most 19
times the maximum number of disjoint closed balls of radius $\delta$ centered
at points of $S$: $\nu(S) \le \rho(S) \le 19\nu(S)$, where $\rho(S)$ and
$\nu(S)$ are the covering and the packing numbers of $S$ by ${\delta}$-balls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00784</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00784</id><created>2015-08-04</created><authors><author><keyname>Han</keyname><forenames>Xiao</forenames></author><author><keyname>Wang</keyname><forenames>Leye</forenames></author><author><keyname>Wen</keyname><forenames>Jiangtao</forenames></author><author><keyname>Cuevas</keyname><forenames>Angel</forenames></author><author><keyname>Chen</keyname><forenames>Chao</forenames></author><author><keyname>Crespi</keyname><forenames>Noel</forenames></author></authors><title>Are You Really Hidden? Predicting Current City from Profile and Social
  Relationship</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy has become a major concern in Online Social Networks (OSNs) due to
threats such as advertising spam, online stalking and identity theft. Although
many users hide or do not fill out their private attributes in OSNs, prior
studies point out that the hidden attributes may be inferred from some other
public information. Thus, users' private information could still be at stake to
be exposed. Hitherto, little work helps users to assess the exposure
probability/risk that the hidden attributes can be correctly predicted, let
alone provides them with pointed countermeasures. In this article, we focus our
study on the exposure risk assessment by a particular privacy-sensitive
attribute - current city - in Facebook. Specifically, we first design a novel
current city prediction approach that discloses users' hidden `current city'
from their self-exposed information. Based on 371,913 Facebook users' data, we
verify that our proposed prediction approach can predict users' current city
more accurately than state-of-the-art approaches. Furthermore, we inspect the
prediction results and model the current city exposure probability via some
measurable characteristics of the self-exposed information. Finally, we
construct an exposure estimator to assess the current city exposure risk for
individual users, given their self-exposed information. Several case studies
are presented to illustrate how to use our proposed estimator for privacy
protection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00792</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00792</id><created>2015-08-04</created><updated>2015-10-08</updated><authors><author><keyname>Mariet</keyname><forenames>Zelda</forenames></author><author><keyname>Sra</keyname><forenames>Suvrit</forenames></author></authors><title>Fixed-point algorithms for learning determinantal point processes</title><categories>cs.LG</categories><comments>ICML, 2015</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Determinantal point processes (DPPs) offer an elegant tool for encoding
probabilities over subsets of a ground set. Discrete DPPs are parametrized by a
positive semidefinite matrix (called the DPP kernel), and estimating this
kernel is key to learning DPPs from observed data. We consider the task of
learning the DPP kernel, and develop for it a surprisingly simple yet effective
new algorithm. Our algorithm offers the following benefits over previous
approaches: (a) it is much simpler; (b) it yields equally good and sometimes
even better local maxima; and (c) it runs an order of magnitude faster on large
problems. We present experimental results on both real and simulated data to
illustrate the numerical performance of our technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00797</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00797</id><created>2015-08-04</created><authors><author><keyname>Chama</keyname><forenames>Namusale</forenames></author><author><keyname>Sofia</keyname><forenames>Rute C.</forenames></author></authors><title>A Discussion on Developing Multihop Routing Metrics Sensitive to Node
  Mobility</title><categories>cs.NI</categories><journal-ref>Journal of Communications, Vol 6, No 1 (2011), 56-67, Feb 2011</journal-ref><doi>10.4304/jcm.6.1.56-67</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is focused on a discussion of parameters and heuristics that are
expected to assist multihop routing in becoming more sensitive to node
mobility. We provide a discussion concerning existing and a few proposed
parameters. Moreover, the work also discusses two new heuristics based on the
notion of link duration. The heuristics are compared based on a meaningful set
of scenarios that attain different mobility aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00801</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00801</id><created>2015-08-04</created><authors><author><keyname>Cavadenti</keyname><forenames>Olivier</forenames></author><author><keyname>Codocedo</keyname><forenames>Victor</forenames></author><author><keyname>Boulicaut</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Kaytoue</keyname><forenames>Mehdi</forenames></author></authors><title>Identifying Avatar Aliases in Starcraft 2</title><categories>cs.AI</categories><comments>Machine Learning and Data Mining for Sports Analytics ECML/PKDD 2015
  workshop, 11 September 2015, Porto, Portugal</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In electronic sports, cyberathletes conceal their online training using
different avatars (virtual identities), allowing them not being recognized by
the opponents they may face in future competitions. In this article, we propose
a method to tackle this avatar aliases identification problem. Our method
trains a classifier on behavioural data and processes the confusion matrix to
output label pairs which concentrate confusion. We experimented with Starcraft
2 and report our first results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00835</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00835</id><created>2015-08-04</created><authors><author><keyname>Papon</keyname><forenames>Jeremie</forenames></author><author><keyname>Schoeler</keyname><forenames>Markus</forenames></author></authors><title>Semantic Pose using Deep Networks Trained on Synthetic RGB-D</title><categories>cs.CV</categories><comments>ICCV 2015 Submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we address the problem of indoor scene understanding from RGB-D
images. Specifically, we propose to find instances of common furniture classes,
their spatial extent, and their pose with respect to generalized class models.
To accomplish this, we use a deep, wide, multi-output convolutional neural
network (CNN) that predicts class, pose, and location of possible objects
simultaneously. To overcome the lack of large annotated RGB-D training sets
(especially those with pose), we use an on-the-fly rendering pipeline that
generates realistic cluttered room scenes in parallel to training. We then
perform transfer learning on the relatively small amount of publicly available
annotated RGB-D data, and find that our model is able to successfully annotate
even highly challenging real scenes. Importantly, our trained network is able
to understand noisy and sparse observations of highly cluttered scenes with a
remarkable degree of accuracy, inferring class and pose from a very limited set
of cues. Additionally, our neural network is only moderately deep and computes
class, pose and position in tandem, so the overall run-time is significantly
faster than existing methods, estimating all output parameters simultaneously
in parallel on a GPU in seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00837</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00837</id><created>2015-08-04</created><authors><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Wang</keyname><forenames>Bolun</forenames></author><author><keyname>Wang</keyname><forenames>Tianyi</forenames></author><author><keyname>Nika</keyname><forenames>Ana</forenames></author><author><keyname>Liu</keyname><forenames>Bingzhe</forenames></author><author><keyname>Zheng</keyname><forenames>Haitao</forenames></author><author><keyname>Zhao</keyname><forenames>Ben Y.</forenames></author></authors><title>Attacks and Defenses in Crowdsourced Mapping Services</title><categories>cs.SI cs.CR</categories><comments>Measure and integration</comments><acm-class>H.3; K.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compared to traditional online maps, crowdsourced maps such as Waze are
unique in providing real-time updates on traffic, congestion, accidents and
points of interest. In this paper, we explore the practical impact of attacks
against crowdsourced map systems, and develop robust defenses against them. Our
experiments show that a single attacker with limited resources can cause havoc
on Waze, reporting false congestion and accidents and automatically rerouting
user traffic. We describe techniques to emulate Waze-enabled vehicles using
lightweight scripts, and how to use these &quot;ghost riders&quot; to compromise user
privacy by remotely tracking precise user movements while avoiding detection. A
single attacker can control groups of ghost riders, overwhelming data from
legitimate users and magnifying the impact of attacks. As defense, we propose a
new approach based on {\em co-location edges}, authenticated records that
attest to the one-time physical co-location of a pair of devices. Over time,
co-location edges combine to form large {\em proximity graphs}, network that
attest to physical interactions between devices. &quot;Ghost-riders&quot; cannot
physically interact with real devices and can be detected using graph
algorithms. We demonstrate the efficacy of this approach using large
simulations, and discuss how they can be used to dramatically reduce the impact
of attacks against crowdsourced mapping services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00842</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00842</id><created>2015-08-04</created><updated>2016-03-06</updated><authors><author><keyname>Chaudhuri</keyname><forenames>Sougata</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Perceptron like Algorithms for Online Learning to Rank</title><categories>cs.LG stat.ML</categories><comments>Substantial modifications to previous version. Added lower bound,
  algorithm achieving lower bound and comparison with existing literature.
  Under review in Journal of Artificial Intelligence Research (JAIR)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Perceptron is a classic online algorithm for learning a classification
function. In this paper, we provide a novel extension of the perceptron
algorithm to the learning to rank problem in information retrieval. We consider
popular listwise performance measures such as Normalized Discounted Cumulative
Gain (NDCG) and Average Precision (AP). A modern perspective on perceptron for
classification is that it is simply an instance of online gradient descent
(OGD), during mistake rounds, using the hinge loss function. Motivated by this
interpretation, we propose a novel family of listwise, large margin ranking
surrogates. Members of this family can be thought of as analogs of the hinge
loss. Exploiting a certain self-bounding property of the proposed family, we
provide a guarantee on the cumulative NDCG (or AP) induced loss incurred by our
perceptron-like algorithm. We show that, if there exists a perfect oracle
ranker which can correctly rank each instance in an online sequence of ranking
data, with some margin, the cumulative loss of perceptron algorithm on that
sequence is bounded by a constant, irrespective of the length of the sequence.
This result is reminiscent of Novikoff's convergence theorem for the
classification perceptron. Moreover, we prove a lower bound on the cumulative
loss achievable by any deterministic algorithm, under the assumption of
existence of perfect oracle ranker. The lower bound shows that our perceptron
bound is not tight, and we propose another, \emph{purely online}, algorithm
which achieves the lower bound. We provide empirical results on simulated and
large commercial datasets to corroborate our theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00851</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00851</id><created>2015-08-04</created><updated>2015-08-05</updated><authors><author><keyname>Schwarz</keyname><forenames>Manfred</forenames></author><author><keyname>Winkler</keyname><forenames>Kyrill</forenames></author><author><keyname>Schmid</keyname><forenames>Ulrich</forenames></author></authors><title>Fast Consensus under Eventually Stabilizing Message Adversaries</title><categories>cs.DC cs.DS</categories><comments>13 pages, 5 figures, updated references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is devoted to deterministic consensus in synchronous dynamic
networks with unidirectional links, which are under the control of an
omniscient message adversary. Motivated by unpredictable node/system
initialization times and long-lasting periods of massive transient faults, we
consider message adversaries that guarantee periods of less erratic message
loss only eventually: We present a tight bound of $2D+1$ for the termination
time of consensus under a message adversary that eventually guarantees a single
vertex-stable root component with dynamic network diameter $D$, as well as a
simple algorithm that matches this bound. It effectively halves the termination
time $4D+1$ achieved by an existing consensus algorithm, which also works under
our message adversary. We also introduce a generalized, considerably stronger
variant of our message adversary, and show that our new algorithm, unlike the
existing one, still works correctly under it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00860</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00860</id><created>2015-08-04</created><authors><author><keyname>Ozols</keyname><forenames>Maris</forenames></author></authors><title>How to combine three quantum states</title><categories>quant-ph cs.IT math-ph math.IT math.MP math.RT</categories><comments>24 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We devise a ternary operation for combining three quantum states: it consists
of permuting the input systems in a continuous fashion and then discarding all
but one of them. This generalizes a binary operation recently studied by
Audenaert et al. [arXiv:1503.04213] in the context of entropy power
inequalities. In fact, our ternary operation continuously interpolates between
all such nested binary operations. Our construction is based on a unitary
version of Cayley's theorem: we use representation theory to show that any
finite group can be naturally embedded in a continuous subgroup of the unitary
group. Formally, this amounts to characterizing when a linear combination of
certain permutations is unitary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00864</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00864</id><created>2015-08-04</created><authors><author><keyname>Roohitavaf</keyname><forenames>Mohammad</forenames></author><author><keyname>Kulkarni</keyname><forenames>Sandeep</forenames></author></authors><title>Stabilization and Fault-Tolerance in Presence of Unchangeable
  Environment Actions</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the problem of adding fault-tolerance to an existing concurrent
protocol in the presence of {\em unchangeable environment actions}. Such
unchangeable actions occur in practice due to several reasons. One instance
includes the case where only a subset of the components/processes can be
revised and other components/processes must be as is. Another instance includes
cyber-physical systems where revising physical components may be undesirable or
impossible. These actions differ from faults in that they are simultaneously
{\em assistive} and {\em disruptive}, whereas faults are only disruptive. For
example, if these actions are a part of a physical component, their execution
is essential for the normal operation of the system. However, they can
potentially disrupt actions taken by other components for dealing with faults.
Also, one can typically assume that fault actions will stop for a long enough
time for the program to make progress. Such an assumption is impossible in this
context.
  We present algorithms for adding stabilizing fault-tolerance, failsafe
fault-tolerance and masking fault-tolerance. Interestingly, we observe that the
previous approaches for adding stabilizing fault-tolerance and masking
fault-tolerance cannot be easily extended in this context. However, we find
that the overall complexity of adding these levels of fault-tolerance remains
in P (in the state space of the program). We also demonstrate that our
algorithms are sound and complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00879</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00879</id><created>2015-08-04</created><authors><author><keyname>Agrawal</keyname><forenames>Ankit</forenames></author></authors><title>Qualitative Decision Methods for Multi-Attribute Decision Making</title><categories>cs.AI</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fundamental problem underlying all multi-criteria decision analysis
(MCDA) problems is that of dominance between any two alternatives: &quot;Given two
alternatives A and B, each described by a set criteria, is A preferred to B
with respect to a set of decision maker (DM) preferences over the criteria?&quot;.
Depending on the application in which MCDA is performed, the alternatives may
represent strategies and policies for business, potential locations for setting
up new facilities, designs of buildings, etc. The general objective of MCDA is
to enable the DM to order all alternatives in order of the stated preferences,
and choose the ones that are best, i.e., optimal with respect to the
preferences over the criteria. This article presents and summarizes a recently
developed MCDA framework that orders the set of alternatives when the relative
importance preferences are incomplete, imprecise, or qualitative in nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00883</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00883</id><created>2015-08-04</created><authors><author><keyname>Showers</keyname><forenames>Eric</forenames></author><author><keyname>Tindall</keyname><forenames>Nathan</forenames></author><author><keyname>Davies</keyname><forenames>Todd</forenames></author></authors><title>Equality of Participation Online Versus Face to Face: Condensed Analysis
  of the Community Forum Deliberative Methods Demonstration</title><categories>cs.CY cs.HC</categories><comments>14 pages, 10 tables, to appear in Efthimios Tambouris, Panos
  Panagiotopoulos, {\O}ystein S{\ae}b{\o}, Konstantinos Tarabanis, Michela
  Milano, Theresa Pardo, and Maria Wimmer (Editors), Electronic Participation:
  Proceedings of the 7th IFIP WG 8.5 International Conference, ePart 2015
  (Thessaloniki, August 30-September 2), Springer LNCS Vol. 9249, 2015</comments><acm-class>H.1.2; H.3.1; H.4.3; H.5.3; J.1; K.4.1</acm-class><doi>10.1007/978-3-319-22500-5_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online deliberation may provide a more cost-effective and/or less inhibiting
environment for public participation than face to face (F2F). But do online
methods bias participation toward certain individuals or groups? We compare F2F
versus online participation in an experiment affording within-participants and
cross-modal comparisons. For English speakers required to have Internet access
as a condition of participation, we find no negative effects of online modes on
equality of participation (EoP) related to gender, age, or educational level.
Asynchronous online discussion appears to improve EoP for gender relative to
F2F. Data suggest a dampening effect of online environments on black
participants, as well as amplification for whites. Synchronous online voice
communication EoP is on par with F2F across individuals. But individual-level
EoP is much lower in the online forum, and greater online forum participation
predicts greater F2F participation for individuals. Measured rates of
participation are compared to self-reported experiences, and other findings are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00893</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00893</id><created>2015-08-03</created><authors><author><keyname>Fadil</keyname><forenames>Oussama</forenames></author><author><keyname>Soloff</keyname><forenames>Jake</forenames></author></authors><title>Information Cascades and Online Rating Games</title><categories>cs.SI q-fin.EC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Through mathematical analysis and simulations, online ratings and their
impact on businesses are characterized through two parameters: an inherent and
objective restaurant quality factor, and the accuracy of customers' gut feeling
about a business. Within this model, it is found that online ratings are seldom
accurate mainly because of the low or high accuracy in customers' gut feelings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00914</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00914</id><created>2015-08-04</created><authors><author><keyname>Machado</keyname><forenames>Roberto Assis</forenames></author><author><keyname>Pineiro</keyname><forenames>Jerry Anderson</forenames></author><author><keyname>Firer</keyname><forenames>Marcelo</forenames></author></authors><title>Characterization of metrics induced by hierarchical posets</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider metrics determined by hierarchical posets and give explicit
formulae for the main parameters of a linear code (packing, covering and
Chebyshev radii and minimal distance), in terms of the corresponding Hamming
parameters. We also give ten characterizations of hierarchical poset metrics,
including new characterizations and simple new proofs to the known ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00927</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00927</id><created>2015-08-04</created><updated>2015-08-14</updated><authors><author><keyname>Azari</keyname><forenames>Amin</forenames></author><author><keyname>Lahouti</keyname><forenames>Farshad</forenames></author><author><keyname>Weber</keyname><forenames>Tobias</forenames></author></authors><title>Degrees of Freedom for Instantaneous-Relay Aided Interference Channel:
  Bounds and Achievable Schemes</title><categories>cs.IT math.IT</categories><comments>A crucial problem in the paper has been found and the paper is
  withdrawn to be reviewed again</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The K-user flat fading MIMO interference channel with J instantaneous relays
(KICJR) is considered. In the KICJR, the effective channel between sources and
destinations including the relays has certain structure and is non-generic. For
non-generic channels, the achievable degrees of freedom (DoF) is still unknown.
Lee and Wang showed that by using the aligned interference neutralization
scheme 3/2 degrees of freedom is achievable in a 2IC1R system, which is 50%
more than the 2-user interference channel. But the DoF performance and
achievable schemes for other KICJR networks are not investigated in literature.
In this paper we devise an achievable scheme called restricted interference
alignment for instantaneous-relay aided interference channels. Also, to find
insights to the maximum achievable degrees of freedom we develop linear
beamforming based on the mean square error (MSE) minimization as an achievable
scheme. Furthermore, we present upper-bounds on the maximum achievable degrees
of freedom by investigating the properness of the interference alignment
equation system. The numerical results show that the DoF performance of the
proposed restricted interference alignment scheme and the MSE-based beamforming
match the upper-bounds determined from the properness of the interference
alignment equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00942</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00942</id><created>2015-08-04</created><updated>2016-01-10</updated><authors><author><keyname>Michelusi</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Boedicker</keyname><forenames>James</forenames></author><author><keyname>El-Naggar</keyname><forenames>Mohamed Y.</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Queuing models for abstracting interactions in Bacterial communities</title><categories>cs.ET</categories><comments>IEEE Journal on Selected Areas in Communications (Bonus Issue on
  Emerging Technologies -- invited)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microbial communities play a significant role in bioremediation,plant
growth,human and animal digestion,global elemental cycles including the
carbon-cycle,and water treatment.They are also posed to be the engines of
renewable energy via microbial fuel cells which can reverse the process of
electrosynthesis.Microbial communication regulates many virulence mechanisms
used by bacteria.Thus,it is of fundamental importance to understand
interactions in microbial communities and to develop predictive tools that help
control them,in order to aid the design of systems exploiting bacterial
capabilities.This position paper explores how abstractions from
communications,networking and information theory can play a role in
understanding and modeling bacterial interactions.In particular,two forms of
interactions in bacterial systems will be examined:electron transfer and quorum
sensing.While the diffusion of chemical signals has been heavily
studied,electron transfer occurring in living cells and its role in cell-cell
interaction is less understood.Recent experimental observations open up new
frontiers in the design of microbial systems based on electron transfer,which
may coexist with the more well-known interaction strategies based on molecular
diffusion.In quorum sensing,the concentration of certain signature chemical
compounds emitted by the bacteria is used to estimate the bacterial population
size,so as to activate collective behaviors.In this position paper,queuing
models for electron transfer are summarized and adapted to provide new models
for quorum sensing.These models are stochastic,and thus capture the inherent
randomness exhibited by cell colonies in nature.It is shown that queuing models
allow the characterization of the state of a single cell as a function of
interactions with other cells and the environment,while being amenable to
complexity reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00945</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00945</id><created>2015-08-04</created><updated>2016-03-02</updated><authors><author><keyname>Honorio</keyname><forenames>Jean</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi</forenames></author></authors><title>Structured Prediction: From Gaussian Perturbations to Linear-Time
  Principled Algorithms</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Margin-based structured prediction commonly uses a maximum loss over all
possible structured outputs \cite{Altun03,Collins04b,Taskar03}. In natural
language processing, recent work \cite{Zhang14,Zhang15} has proposed the use of
the maximum loss over random structured outputs sampled independently from some
proposal distribution. This method is linear-time in the number of random
structured outputs and trivially parallelizable. We study this family of loss
functions in the PAC-Bayes framework under Gaussian perturbations
\cite{McAllester07}. Under some technical conditions and up to statistical
accuracy, we show that this family of loss functions produces a tighter upper
bound of the Gibbs decoder distortion than commonly used methods. Thus, using
the maximum loss over random structured outputs is a principled way of learning
the parameter of structured prediction models. Besides explaining the
experimental success of \cite{Zhang14,Zhang15}, our theoretical results show
that more general techniques are possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00949</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00949</id><created>2015-08-04</created><authors><author><keyname>Zhu</keyname><forenames>Weiping</forenames></author></authors><title>Loss Rate Estimators and the Properties for the Tree Topology</title><categories>cs.IT math.IT</categories><comments>17 pages, 1 figure. arXiv admin note: text overlap with
  arXiv:1205.6244</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large number of explicit estimators are proposed in this paper for loss
rate estimation in a network of the tree topology. All of the estimators are
proved to be unbiased and consistent instead of asymptotic unbiased as that
obtained in [1] for a specific estimator. In addition, a set of formulae are
derived for the variances of various maximum likelihood estimators that unveil
the connection between the path of interest and the subtrees connecting the
path to observers. Using the formulae, we are able to not only rank the
estimators proposed so far, including those proposed in this paper, but also
identify the errors made in previous works. More importantly, using the
formulae we can easily identify the most efficient explicit estimator from a
pool that makes model selection feasible in loss tomography
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00952</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00952</id><created>2015-08-04</created><authors><author><keyname>Srinivasan</keyname><forenames>Akshay</forenames></author><author><keyname>Todorov</keyname><forenames>Emanuel</forenames></author></authors><title>Graphical Newton</title><categories>math.OC cs.RO cs.SY</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Computing the Newton step for a generic function $f: \mathbb{R}^n \rightarrow
\mathbb{R}$ takes $O(n^{3})$ flops. In this paper we explore avenues for
reducing this bound, when the computational structure of $f$ is known. We show
that the Newton step can be computed in time linear in the size of the
computational-graph, and cubic in its tree-width. We also generalize the
algorithm to constrained systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00962</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00962</id><created>2015-08-04</created><updated>2015-11-22</updated><authors><author><keyname>Cong</keyname><forenames>Yirui</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author></authors><title>Event-Trigger Based Robust-Optimal Control for Energy Harvesting
  Transmitter</title><categories>cs.IT math.IT</categories><comments>This paper is submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies online transmission policies for an energy harvesting
transmitter. Unlike the existing online policies which more or less require the
knowledge on the future behavior of the energy and data arrival processes, we
consider a practical but significantly more challenging scenario where the
energy and data arrival processes are assumed to be totally unknown. Our design
is formulated as a robust-optimal control problem which aims to optimize the
worst-case performance. The transmission policy is designed only based on the
current battery energy level and the data queue length directly monitored by
the transmitter itself. Specifically, we apply an event-trigger approach in
which the transmitter continuously monitors the battery energy and data queue
length, and triggers an event when a significant change occurs in either of
them. Once an event is triggered, the transmission policy is updated by the
solution to the robust-optimal control problem. We consider both the
transmission time and throughput as the performance metrics and formulated two
optimization problems. The solutions are given in either a simple analytical
form or an easy-to-implement algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00964</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00964</id><created>2015-08-04</created><updated>2016-01-28</updated><authors><author><keyname>Lee</keyname><forenames>Namyoon</forenames></author></authors><title>MAP Support Detection for Greedy Sparse Signal Recovery Algorithms in
  Compressive Sensing</title><categories>cs.IT cs.LG math.IT</categories><comments>12 pages, Submitted to IEEE TSP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A reliable support detection is essential for a greedy algorithm to
reconstruct a sparse signal accurately from compressed and noisy measurements.
This paper proposes a novel support detection method for greedy algorithms,
which is referred to as &quot;\textit{maximum a posteriori (MAP) support
detection}&quot;. Unlike existing support detection methods that identify support
indices with the largest correlation value in magnitude per iteration, the
proposed method selects them with the largest likelihood ratios computed under
the true and null support hypotheses by simultaneously exploiting the
distributions of sensing matrix, sparse signal, and noise. Leveraging this
technique, MAP-Matching Pursuit (MAP-MP) is first presented to show the
advantages of exploiting the proposed support detection method, and a
sufficient condition for perfect signal recovery is derived for the case when
the sparse signal is binary. Subsequently, a set of iterative greedy
algorithms, called MAP-generalized Orthogonal Matching Pursuit (MAP-gOMP),
MAP-Compressive Sampling Matching Pursuit (MAP-CoSaMP), and MAP-Subspace
Pursuit (MAP-SP) are presented to demonstrate the applicability of the proposed
support detection method to existing greedy algorithms. From empirical results,
it is shown that the proposed greedy algorithms with highly reliable support
detection can be better, faster, and easier to implement than basis pursuit via
linear programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00966</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00966</id><created>2015-08-04</created><authors><author><keyname>Sun</keyname><forenames>Yankui</forenames></author><author><keyname>Zhang</keyname><forenames>Tian</forenames></author><author><keyname>Zhao</keyname><forenames>Yue</forenames></author><author><keyname>He</keyname><forenames>Yufan</forenames></author></authors><title>3D Automatic Segmentation Method for Retinal Optical Coherence
  Tomography Volume Data Using Boundary Surface Enhancement</title><categories>cs.CV</categories><comments>27 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the introduction of spectral-domain optical coherence tomography
(SDOCT), much larger image datasets are routinely acquired compared to what was
possible using the previous generation of time-domain OCT. Thus, there is a
critical need for the development of 3D segmentation methods for processing
these data. We present here a novel 3D automatic segmentation method for
retinal OCT volume data. Briefly, to segment a boundary surface, two OCT volume
datasets are obtained by using a 3D smoothing filter and a 3D differential
filter. Their linear combination is then calculated to generate new volume data
with an enhanced boundary surface, where pixel intensity, boundary position
information, and intensity changes on both sides of the boundary surface are
used simultaneously. Next, preliminary discrete boundary points are detected
from the A-Scans of the volume data. Finally, surface smoothness constraints
and a dynamic threshold are applied to obtain a smoothed boundary surface by
correcting a small number of error points. Our method can extract retinal layer
boundary surfaces sequentially with a decreasing search region of volume data.
We performed automatic segmentation on eight human OCT volume datasets acquired
from a commercial Spectralis OCT system, where each volume of data consisted of
97 OCT images with a resolution of 496 512; experimental results show that this
method can accurately segment seven layer boundary surfaces in normal as well
as some abnormal eyes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00973</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00973</id><created>2015-08-05</created><authors><author><keyname>Chen</keyname><forenames>Peixian</forenames></author><author><keyname>Zhang</keyname><forenames>Nevin L.</forenames></author><author><keyname>Poon</keyname><forenames>Leonard K. M.</forenames></author><author><keyname>Chen</keyname><forenames>Zhourong</forenames></author></authors><title>Progressive EM for Latent Tree Models and Hierarchical Topic Detection</title><categories>cs.LG cs.CL cs.IR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical latent tree analysis (HLTA) is recently proposed as a new method
for topic detection. It differs fundamentally from the LDA-based methods in
terms of topic definition, topic-document relationship, and learning method. It
has been shown to discover significantly more coherent topics and better topic
hierarchies. However, HLTA relies on the Expectation-Maximization (EM)
algorithm for parameter estimation and hence is not efficient enough to deal
with large datasets. In this paper, we propose a method to drastically speed up
HLTA using a technique inspired by recent advances in the moments method.
Empirical experiments show that our method greatly improves the efficiency of
HLTA. It is as efficient as the state-of-the-art LDA-based method for
hierarchical topic detection and finds substantially better topics and topic
hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00974</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00974</id><created>2015-08-05</created><authors><author><keyname>Zhang</keyname><forenames>Tao</forenames></author><author><keyname>Ge</keyname><forenames>Gennian</forenames></author></authors><title>Quantum Block and Synchronizable Codes Derived from Certain Classes of
  Polynomials</title><categories>cs.IT math.IT</categories><comments>9 pages. arXiv admin note: text overlap with arXiv:1403.6192,
  arXiv:1311.3416 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One central theme in quantum error-correction is to construct quantum codes
that have a large minimum distance. In this paper, we first present a
construction of classical codes based on certain class of polynomials. Through
these classical codes, we are able to obtain some new quantum codes. It turns
out that some of quantum codes exhibited here have better parameters than the
ones available in the literature. Meanwhile, we give a new class of quantum
synchronizable codes with highest possible tolerance against misalignment from
duadic codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00978</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00978</id><created>2015-08-05</created><authors><author><keyname>Zhang</keyname><forenames>Tao</forenames></author><author><keyname>Ge</keyname><forenames>Gennian</forenames></author></authors><title>Quantum Codes from Generalized Reed-Solomon Codes and Matrix-Product
  Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the central tasks in quantum error-correction is to construct quantum
codes that have good parameters. In this paper, we construct three new classes
of quantum MDS codes from classical Hermitian self-orthogonal generalized
Reed-Solomon codes. We also present some classes of quantum codes from
matrix-product codes. It turns out that many of our quantum codes are new in
the sense that the parameters of quantum codes cannot be obtained from all
previous constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00982</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00982</id><created>2015-08-05</created><authors><author><keyname>He</keyname><forenames>Peng</forenames></author><author><keyname>Mao</keyname><forenames>Yuming</forenames></author><author><keyname>Liu</keyname><forenames>Qiang</forenames></author><author><keyname>Yang</keyname><forenames>Kun</forenames></author></authors><title>Improving Reliability Performance of Diffusion-based Molecular
  Communication With Adaptive Threshold Variation Algorithm</title><categories>cs.IT cs.ET math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we investigate the communication reliability for
diffusion-based molecular communication, using the indicator of bit error rate
(BER). A molecular classified model is established to divide molecules into
three parts, which are the signal, inter-symbol interference (ISI) and noise.
We expand each part separately using molecular absorbing probability, and
connect them by a traditional-like formula. Based on the classified model, we
do a theoretical analysis to prove the feasibility of improving the BER
performance. Accordingly, an adaptive threshold variation (ATV) algorithm is
designed in demodulation to implement the goal, which makes the receiver adapt
the channel condition properly through learning process. Moreover, the
complexity of ATV is calculated and its performance in various noisy channel is
discussed. An expression of Signal to Interference plus Noise Ratio (SINR) is
defined to verify the system performance. We test some important parameters of
the channel model, as well as the ATV algorithm in the simulation section. The
results have shown the performance gain of the proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00984</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00984</id><created>2015-08-05</created><authors><author><keyname>Hartono</keyname><forenames>Pitoyo</forenames></author></authors><title>Dimension Reduction with Non-degrading Generalization</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visualizing high dimensional data by projecting them into two or three
dimensional space is one of the most effective ways to intuitively understand
the data's underlying characteristics, for example their class neighborhood
structure. While data visualization in low dimensional space can be efficient
for revealing the data's underlying characteristics, classifying a new sample
in the reduced-dimensional space is not always beneficial because of the loss
of information in expressing the data. It is possible to classify the data in
the high dimensional space, while visualizing them in the low dimensional
space, but in this case, the visualization is often meaningless because it
fails to illustrate the underlying characteristics that are crucial for the
classification process.
  In this paper, the performance-preserving property of the previously proposed
Restricted Radial Basis Function Network in reducing the dimension of labeled
data is explained. Here, it is argued through empirical experiments that the
internal representation of the Restricted Radial Basis Function Network, which
during the supervised learning process organizes a visualizable two dimensional
map, does not only preserve the topographical structure of high dimensional
data but also captures their class neighborhood structures that are important
for classifying them. Hence, unlike many of the existing dimension reduction
methods, the Restricted Radial Basis Function Network offers two dimensional
visualization that is strongly correlated with the classification process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00986</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00986</id><created>2015-08-05</created><authors><author><keyname>Wang</keyname><forenames>Zhuoran</forenames></author><author><keyname>Crook</keyname><forenames>Paul A.</forenames></author><author><keyname>Tang</keyname><forenames>Wenshuo</forenames></author><author><keyname>Lemon</keyname><forenames>Oliver</forenames></author></authors><title>On the Linear Belief Compression of POMDPs: A re-examination of current
  methods</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Belief compression improves the tractability of large-scale partially
observable Markov decision processes (POMDPs) by finding projections from
high-dimensional belief space onto low-dimensional approximations, where
solving to obtain action selection policies requires fewer computations. This
paper develops a unified theoretical framework to analyse three existing linear
belief compression approaches, including value-directed compression and two
non-negative matrix factorisation (NMF) based algorithms. The results indicate
that all the three known belief compression methods have their own critical
deficiencies. Therefore, projective NMF belief compression is proposed (P-NMF),
aiming to overcome the drawbacks of the existing techniques. The performance of
the proposed algorithm is examined on four POMDP problems of reasonably large
scale, in comparison with existing techniques. Additionally, the
competitiveness of belief compression is compared empirically to a
state-of-the-art heuristic search based POMDP solver and their relative merits
in solving large-scale POMDPs are investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00987</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00987</id><created>2015-08-05</created><authors><author><keyname>Liu</keyname><forenames>Xiaodong</forenames></author><author><keyname>Liao</keyname><forenames>Xiangke</forenames></author><author><keyname>Li</keyname><forenames>Shanshan</forenames></author><author><keyname>Zhang</keyname><forenames>Jingying</forenames></author><author><keyname>Shao</keyname><forenames>Lisong</forenames></author><author><keyname>Huang</keyname><forenames>Chenlin</forenames></author><author><keyname>Xiao</keyname><forenames>Liquan</forenames></author></authors><title>On the Shoulders of Giants: Incremental Influence Maximization in
  Evolving Social Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying the most influential individuals can provide invaluable help in
developing and deploying effective viral marketing strategies. Previous studies
mainly focus on designing efficient algorithms or heuristics to find top-K
influential nodes on a given static social network. While, as a matter of fact,
real-world social networks keep evolving over time and a recalculation upon the
changed network inevitably leads to a long running time, significantly
affecting the efficiency. In this paper, we observe from real-world traces that
the evolution of social network follows the preferential attachment rule and
the influential nodes are mainly selected from high-degree nodes. Such
observations shed light on the design of IncInf, an incremental approach that
can efficiently locate the top-K influential individuals in evolving social
networks based on previous information instead of calculation from scratch. In
particular, IncInf quantitatively analyzes the influence spread changes of
nodes by localizing the impact of topology evolution to only local regions, and
a pruning strategy is further proposed to effectively narrow the search space
into nodes experiencing major increases or with high degrees. We carried out
extensive experiments on real-world dynamic social networks including Facebook,
NetHEPT, and Flickr. Experimental results demonstrate that, compared with the
state-of-the-art static heuristic, IncInf achieves as much as 21X speedup in
execution time while maintaining matching performance in terms of influence
spread.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00996</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00996</id><created>2015-08-05</created><authors><author><keyname>Tayel</keyname><forenames>Mazhar B.</forenames></author><author><keyname>AlSaba</keyname><forenames>Eslam I</forenames></author></authors><title>Robust and Sensitive Method of Lyapunov Exponent for Heart Rate
  Variability</title><categories>cs.SY</categories><comments>19 pages, 11 figures, extended paper of CSIP2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heart Rate Variability (HRV) plays an important role for reporting several
cardiological and non-cardiological diseases. Also, the HRV has a prognostic
value and is therefore quite important in modelling the cardiac risk. The
nature of the HRV is chaotic, stochastic and it remains highly controversial.
Because the HRV has utmost importance, it needs a sensitive tool to analyze the
variability. In previous work, Rosenstein and Wolf had used the Lyapunov
exponent as a quantitative measure for HRV detection sensitivity. However, the
two methods diverge in determining the HRV sensitivity. This paper introduces a
modification to both the Rosenstein and Wolf methods to overcome their
drawbacks. The introduced Mazhar-Eslam algorithm increases the sensitivity to
HRV detection with better accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.00998</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.00998</id><created>2015-08-05</created><updated>2015-12-11</updated><authors><author><keyname>Bianco</keyname><forenames>Simone</forenames></author><author><keyname>Cusano</keyname><forenames>Claudio</forenames></author><author><keyname>Schettini</keyname><forenames>Raimondo</forenames></author></authors><title>Single and Multiple Illuminant Estimation Using Convolutional Neural
  Networks</title><categories>cs.CV</categories><comments>Submitted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a method for the estimation of the color of the
illuminant in RAW images. The method includes a Convolutional Neural Network
that has been specially designed to produce multiple local estimates. A
multiple illuminant detector determines whether or not the local outputs of the
network must be aggregated into a single estimate. We evaluated our method on
standard datasets with single and multiple illuminants, obtaining lower
estimation errors with respect to those obtained by other general purpose
methods in the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01006</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01006</id><created>2015-08-05</created><updated>2015-12-24</updated><authors><author><keyname>Zhang</keyname><forenames>Dongxu</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author></authors><title>Relation Classification via Recurrent Neural Network</title><categories>cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning has gained much success in sentence-level relation
classification. For example, convolutional neural networks (CNN) have delivered
competitive performance without much effort on feature engineering as the
conventional pattern-based methods. Thus a lot of works have been produced
based on CNN structures. However, a key issue that has not been well addressed
by the CNN-based method is the lack of capability to learn temporal features,
especially long-distance dependency between nominal pairs. In this paper, we
propose a simple framework based on recurrent neural networks (RNN) and compare
it with CNN-based model. To show the limitation of popular used SemEval-2010
Task 8 dataset, we introduce another dataset refined from MIMLRE(Angeli et al.,
2014). Experiments on two different datasets strongly indicates that the
RNN-based model can deliver better performance on relation classification, and
it is particularly capable of learning long-distance relation patterns. This
makes it suitable for real-world applications where complicated expressions are
often involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01008</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01008</id><created>2015-08-05</created><authors><author><keyname>Chung</keyname><forenames>Jaeyong</forenames></author><author><keyname>Shin</keyname><forenames>Taehwan</forenames></author><author><keyname>Kang</keyname><forenames>Yongshin</forenames></author></authors><title>INsight: A Neuromorphic Computing System for Evaluation of Large Neural
  Networks</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks have been demonstrated impressive results in various
cognitive tasks such as object detection and image classification. In order to
execute large networks, Von Neumann computers store the large number of weight
parameters in external memories, and processing elements are timed-shared,
which leads to power-hungry I/O operations and processing bottlenecks. This
paper describes a neuromorphic computing system that is designed from the
ground up for the energy-efficient evaluation of large-scale neural networks.
The computing system consists of a non-conventional compiler, a neuromorphic
architecture, and a space-efficient microarchitecture that leverages existing
integrated circuit design methodologies. The compiler factorizes a trained,
feedforward network into a sparsely connected network, compresses the weights
linearly, and generates a time delay neural network reducing the number of
connections. The connections and units in the simplified network are mapped to
silicon synapses and neurons. We demonstrate an implementation of the
neuromorphic computing system based on a field-programmable gate array that
performs the MNIST hand-written digit classification with 97.64% accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01011</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01011</id><created>2015-08-05</created><authors><author><keyname>Zhang</keyname><forenames>Dongxu</forenames></author><author><keyname>Luo</keyname><forenames>Tianyi</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author><author><keyname>Liu</keyname><forenames>Rong</forenames></author></authors><title>Learning from LDA using Deep Neural Networks</title><categories>cs.LG cs.CL cs.IR cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian
model for topic inference. In spite of its great success, inferring the latent
topic distribution with LDA is time-consuming. Motivated by the transfer
learning approach proposed by~\newcite{hinton2015distilling}, we present a
novel method that uses LDA to supervise the training of a deep neural network
(DNN), so that the DNN can approximate the costly LDA inference with less
computation. Our experiments on a document classification task show that a
simple DNN can learn the LDA behavior pretty well, while the inference is
speeded up tens or hundreds of times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01014</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01014</id><created>2015-08-05</created><authors><author><keyname>Knop</keyname><forenames>Du&#x161;an</forenames></author><author><keyname>Masa&#x159;&#xed;k</keyname><forenames>Tom&#xe1;&#x161;</forenames></author></authors><title>Computational complexity of distance edge labeling</title><categories>cs.DM</categories><comments>21 pages, IWOCA 2015</comments><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of Distance Edge Labeling is a variant of Distance Vertex
Labeling (also known as $L_{2,1}$ labeling) that has been studied for more than
twenty years and has many applications, such as frequency assignment.
  The Distance Edge Labeling problem asks whether the edges of a given graph
can be labeled such that the labels of adjacent edges differ by at least two
and the labels of edges of distance two differ by at least one. Labels are
chosen from the set $\{0,1,\dots,\lambda\}$ for $\lambda$ fixed.
  We present a full classification of its computational complexity - a
dichotomy between the polynomially solvable cases and the remaining cases which
are NP-complete. We characterise graphs with $\lambda \le 4$ which leads to a
polynomial-time algorithm recognizing the class and we show NP-completeness for
$\lambda \ge 5$ by several reductions from Monotone Not All Equal 3-SAT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01023</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01023</id><created>2015-08-05</created><authors><author><keyname>Cao</keyname><forenames>Bokai</forenames></author><author><keyname>Kong</keyname><forenames>Xiangnan</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>A review of heterogeneous data mining for brain disorders</title><categories>cs.LG cs.CE cs.DB q-bio.NC stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With rapid advances in neuroimaging techniques, the research on brain
disorder identification has become an emerging area in the data mining
community. Brain disorder data poses many unique challenges for data mining
research. For example, the raw data generated by neuroimaging experiments is in
tensor representations, with typical characteristics of high dimensionality,
structural complexity and nonlinear separability. Furthermore, brain
connectivity networks can be constructed from the tensor data, embedding subtle
interactions between brain regions. Other clinical measures are usually
available reflecting the disease status from different perspectives. It is
expected that integrating complementary information in the tensor data and the
brain network data, and incorporating other clinical parameters will be
potentially transformative for investigating disease mechanisms and for
informing therapeutic interventions. Many research efforts have been devoted to
this area. They have achieved great success in various applications, such as
tensor-based modeling, subgraph pattern mining, multi-view feature analysis. In
this paper, we review some recent data mining methods that are used for
analyzing brain disorders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01041</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01041</id><created>2015-08-05</created><authors><author><keyname>Jensen</keyname><forenames>K. E.</forenames></author><author><keyname>Szabo</keyname><forenames>P.</forenames></author><author><keyname>Okkels</keyname><forenames>F.</forenames></author></authors><title>Implementation of the Log-Conformation Formulation for Two-Dimensional
  Viscoelastic Flow</title><categories>cs.CE physics.flu-dyn</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have implemented the log-conformation method for two-dimensional
viscoelastic flow in COMSOL, a commercial high-level finite element package.
The code is verified for an Oldroyd-B fluid flowing past a confined cylinder.
We are also able to describe the well-known bistability of the viscoelastic
flow in a cross-slot geometry for a FENE-CR fluid, and we describe the changes
required for performing simulations with the Phan-Thien-Tanner (PTT), Giesekus
and FENE-P models. Finally, we calculate the flow of a FENE-CR fluid in a
geometry with three in- and outlets. The implementation is included in the
supplementary material, and we hope that it can inspire new as well as
experienced researchers in the field of differential constitutive equations for
viscoelastic flow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01045</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01045</id><created>2015-08-05</created><authors><author><keyname>Lonsing</keyname><forenames>Florian</forenames></author><author><keyname>Seidl</keyname><forenames>Martina</forenames></author><author><keyname>Van Gelder</keyname><forenames>Allen</forenames></author></authors><title>The QBF Gallery: Behind the Scenes</title><categories>cs.LO</categories><comments>28 pages with additional appendix of 7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last few years, much progress has been made in the theory and
practice of solving quantified Boolean formulas (QBF). Novel solvers have been
presented that either successfully enhance established techniques or implement
novel solving paradigms. Powerful preprocessors have been realized that tune
the encoding of a formula to make it easier to solve. Frameworks for
certification and solution extraction emerged that allow for a detailed
interpretation of a QBF solver's results, and new types of QBF encodings were
presented for various application problems.
  To capture these developments the QBF Gallery was established in 2013. The
QBF Gallery aims at providing a forum to assess QBF tools and to collect new,
expressive benchmarks that allow for documenting the status quo and that
indicate promising research directions. The collected benchmarks became the
basis for the experiments conducted in the context of the QBF Gallery 2013 and
QBF Gallery 2014. In the latter, QBF solvers were evaluated in a competitive
setting as part of the FLoC Olympic Games. In contrast to 2014, the edition of
the QBF Gallery in 2013 was not competitive and hence no prizes were awarded.
  In this paper, we report on the setup of the QBF Gallery. To this end, we
conducted numerous experiments which allowed us not only to assess the quality
of the tools, but also the quality of the benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01053</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01053</id><created>2015-08-05</created><authors><author><keyname>S&#xe1;ndez</keyname><forenames>Manuel Jes&#xfa;s Rivas</forenames></author></authors><title>A Review of Technical Problems when Conducting an Investigation in Cloud
  Based Environments</title><categories>cs.CR cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is a relatively new technology which is quickly becoming one
of the most important technological advances for computer science. This
technology has had a significant growth in recent years. It is now more
affordable and cloud platforms are becoming more stable. Businesses are
successfully migrating their systems to a cloud infrastructure, obtaining
technological and economic benefits. However, others still remain reluctant to
do it due to both security concerns and the loss of control over their
infrastructures and data that the migration entails. At the same time that new
technologies progress, its benefits appeal to criminals too. They can not only
steal data from clouds, but they can also hide data in clouds, which has
provoked an increased in the number of cybercrimes and their economic impacts.
Their victims range from children and adults to companies and even countries.
On the other hand, digital forensics have negatively suffered the impact of the
boom of cloud computing due to its dynamic nature. The tools and procedures
that were successfully proved and used in digital investigations are now
becoming irrelevant, making it an urging necessity to develop new forensics
capabilities for conducting an investigation in this new environment. As a
consequence of these needs a new area has emerged, Cloud Forensics, which is
the result of the intersection between cloud computing and digital forensics.
  Keywords: Cloud forensics, cloud computing, forensics investigation, forensic
challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01055</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01055</id><created>2015-08-05</created><authors><author><keyname>Fedorov</keyname><forenames>Roman</forenames></author><author><keyname>Camerada</keyname><forenames>Alessandro</forenames></author><author><keyname>Fraternali</keyname><forenames>Piero</forenames></author><author><keyname>Tagliasacchi</keyname><forenames>Marco</forenames></author></authors><title>Estimating snow cover from publicly available images</title><categories>cs.MM cs.CV</categories><comments>submitted to IEEE Transactions on Multimedia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of estimating snow cover in mountainous
regions, that is, the spatial extent of the earth surface covered by snow. We
argue that publicly available visual content, in the form of user generated
photographs and image feeds from outdoor webcams, can both be leveraged as
additional measurement sources, complementing existing ground, satellite and
airborne sensor data. To this end, we describe two content acquisition and
processing pipelines that are tailored to such sources, addressing the specific
challenges posed by each of them, e.g., identifying the mountain peaks,
filtering out images taken in bad weather conditions, handling varying
illumination conditions. The final outcome is summarized in a snow cover index,
which indicates for a specific mountain and day of the year, the fraction of
visible area covered by snow, possibly at different elevations. We created a
manually labelled dataset to assess the accuracy of the image snow covered area
estimation, achieving 90.0% precision at 91.1% recall. In addition, we show
that seasonal trends related to air temperature are captured by the snow cover
index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01056</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01056</id><created>2015-08-05</created><updated>2015-11-17</updated><authors><author><keyname>Arrigo</keyname><forenames>Francesca</forenames></author><author><keyname>Benzi</keyname><forenames>Michele</forenames></author></authors><title>Edge modification criteria for enhancing the communicability of digraphs</title><categories>cs.SI math.NA physics.soc-ph</categories><comments>26 pages, 11 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce new broadcast and receive communicability indices that can be
used as global measures of how effectively information is spread in a directed
network. Furthermore, we describe fast and effective criteria for the selection
of edges to be added to (or deleted from) a given directed network so as to
enhance these network communicability measures. Numerical experiments
illustrate the effectiveness of the proposed techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01057</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01057</id><created>2015-08-05</created><authors><author><keyname>Xenaki</keyname><forenames>Spyridoula D.</forenames></author><author><keyname>Koutroumbas</keyname><forenames>Konstantinos D.</forenames></author><author><keyname>Rontogiannis</keyname><forenames>Athanasios A.</forenames></author></authors><title>On the convergence of the sparse possibilistic c-means algorithm</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a convergence proof for the recently proposed sparse
possibilistic c-means (SPCM) algorithm is provided, utilizing the celebrated
Zangwill convergence theorem. It is shown that the iterative sequence generated
by SPCM converges to a stationary point or there exists a subsequence of it
that converges to a stationary point of the cost function of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01059</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01059</id><created>2015-08-05</created><authors><author><keyname>Avigdor-Elgrabli</keyname><forenames>Noa</forenames></author><author><keyname>Blocq</keyname><forenames>Gideon</forenames></author><author><keyname>Gamzu</keyname><forenames>Iftah</forenames></author></authors><title>Offline and Online Models of Budget Allocation for Maximizing Influence
  Spread</title><categories>cs.DS cs.GT cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The research of influence propagation in social networks via word-of-mouth
processes has been given considerable attention in recent years. Arguably, the
most fundamental problem in this domain is the influence maximization problem,
where the goal is to identify a small seed set of individuals that can trigger
a large cascade of influence in the network. While there has been significant
progress regarding this problem and its variants, one basic shortcoming of the
underlying models is that they lack the flexibility in the way the overall
budget is allocated to different individuals. Indeed, budget allocation is a
critical issue in advertising and viral marketing. Taking the other point of
view, known models allowing flexible budget allocation do not take into account
the influence spread in social networks.
  We introduce a generalized model that captures both budgets and influence
propagation simultaneously. For the offline setting, we identify a large family
of natural budget-based propagation functions that admits a tight approximation
guarantee. This family extends most of the previously studied influence models,
including the well-known Triggering model. We establish that any function in
this family implies an instance of a monotone submodular function maximization
over the integer lattice subject to a knapsack constraint. This problem is
known to admit an optimal $1-1/e \approx 0.632$-approximation. For the online
setting, in which an unknown subset of agents arrive in a random order and the
algorithm needs to make an irrevocable budget allocation in each step, we
develop a $1/(15e) \approx 0.025$-competitive algorithm. This setting extends
the celebrated secretary problem, and its variant, the submodular knapsack
secretary problem. Notably, our algorithm improves over the best known
approximation for the latter problem, even though it applies to a more general
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01067</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01067</id><created>2015-08-05</created><authors><author><keyname>Su</keyname><forenames>Jing</forenames></author><author><keyname>Boydell</keyname><forenames>Ois&#xed;n</forenames></author><author><keyname>Greene</keyname><forenames>Derek</forenames></author><author><keyname>Lynch</keyname><forenames>Gerard</forenames></author></authors><title>Topic Stability over Noisy Sources</title><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topic modelling techniques such as LDA have recently been applied to speech
transcripts and OCR output. These corpora may contain noisy or erroneous texts
which may undermine topic stability. Therefore, it is important to know how
well a topic modelling algorithm will perform when applied to noisy data. In
this paper we show that different types of textual noise will have diverse
effects on the stability of different topic models. From these observations, we
propose guidelines for text corpus generation, with a focus on automatic speech
transcription. We also suggest topic model selection methods for noisy corpora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01071</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01071</id><created>2015-08-05</created><authors><author><keyname>Carvajal</keyname><forenames>Rodrigo</forenames></author><author><keyname>Ag&#xfc;ero</keyname><forenames>Juan C.</forenames></author><author><keyname>Godoy</keyname><forenames>Boris I.</forenames></author><author><keyname>Katselis</keyname><forenames>Dimitrios</forenames></author></authors><title>A MAP approach for $\ell_q$-norm regularized sparse parameter estimation
  using the EM algorithm</title><categories>cs.SY stat.ML</categories><comments>Accepted to IEEE Machine Learning for Signal Processing Conference
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, Bayesian parameter estimation through the consideration of the
Maximum A Posteriori (MAP) criterion is revisited under the prism of the
Expectation-Maximization (EM) algorithm. By incorporating a sparsity-promoting
penalty term in the cost function of the estimation problem through the use of
an appropriate prior distribution, we show how the EM algorithm can be used to
efficiently solve the corresponding optimization problem. To this end, we rely
on variance-mean Gaussian mixtures (VMGM) to describe the prior distribution,
while we incorporate many nice features of these mixtures to our estimation
problem. The corresponding MAP estimation problem is completely expressed in
terms of the EM algorithm, which allows for handling nonlinearities and hidden
variables that cannot be easily handled with traditional methods. For
comparison purposes, we also develop a Coordinate Descent algorithm for the
$\ell_q$-norm penalized problem and present the performance results via
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01076</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01076</id><created>2015-08-05</created><authors><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Biedl</keyname><forenames>Therese</forenames></author><author><keyname>Hackl</keyname><forenames>Thomas</forenames></author><author><keyname>Held</keyname><forenames>Martin</forenames></author><author><keyname>Huber</keyname><forenames>Stefan</forenames></author><author><keyname>Palfrader</keyname><forenames>Peter</forenames></author><author><keyname>Vogtenhuber</keyname><forenames>Birgit</forenames></author></authors><title>Representing Directed Trees as Straight Skeletons</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The straight skeleton of a polygon is the geometric graph obtained by tracing
the vertices during a mitered offsetting process. It is known that the straight
skeleton of a simple polygon is a tree, and one can naturally derive directions
on the edges of the tree from the propagation of the shrinking process.
  In this paper, we ask the reverse question: Given a tree with directed edges,
can it be the straight skeleton of a polygon? And if so, can we find a suitable
simple polygon? We answer these questions for all directed trees where the
order of edges around each node is fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01081</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01081</id><created>2015-08-05</created><authors><author><keyname>Nesi</keyname><forenames>Paolo</forenames></author><author><keyname>Pantaleo</keyname><forenames>Gianni</forenames></author></authors><title>Detection of Critical Number of People in Interlocked Doors for Security
  Access Control by Exploiting a Microwave Transceiver-Array</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Counting the number of people is something many security application focus
on, when dealing with controlling accesses in restricted areas, as it occurs
with banks, airports, railway stations and governmental offices. This paper
presents an automated solution for detecting the presence of more than one
person into interlocked doors adopted in many accesses. In most cases,
interlocked doors are small areas where other pieces of information and sensors
are placed in order to detect the presence of guns, explosive, etc. The general
goals and the required environmental condition, allowed us to implement a
detection system at lower costs and complexity, with respect to other existing
techniques. The system consists of a fixed array of microwave transceiver
modules, whose received signals are processed to collect information related to
a sort of volume occupied in the interlocked door cabin. The proposed solution
has been statistically validated by using statistical analysis. The whole
solution has been also implemented to be used in a real time environment and
thus validated against real experimental measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01083</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01083</id><created>2015-08-05</created><authors><author><keyname>Bellini</keyname><forenames>Pierfrancesco</forenames></author><author><keyname>Nesi</keyname><forenames>Paolo</forenames></author><author><keyname>Rauch</keyname><forenames>Nadia</forenames></author></authors><title>Ontology Bulding vs Data Harvesting and Cleaning for Smart-city Services</title><categories>cs.DB cs.AI cs.CY</categories><comments>DMS 2014, Distributed Multimedia Systems</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Presently, a very large number of public and private data sets are available
around the local governments. In most cases, they are not semantically
interoperable and a huge human effort is needed to create integrated ontologies
and knowledge base for smart city. Smart City ontology is not yet standardized,
and a lot of research work is needed to identify models that can easily support
the data reconciliation, the management of the complexity and reasoning. In
this paper, a system for data ingestion and reconciliation smart cities related
aspects as road graph, services available on the roads, traffic sensors etc.,
is proposed. The system allows managing a big volume of data coming from a
variety of sources considering both static and dynamic data. These data are
mapped to smart-city ontology and stored into an RDF-Store where they are
available for applications via SPARQL queries to provide new services to the
users. The paper presents the process adopted to produce the ontology and the
knowledge base and the mechanisms adopted for the verification, reconciliation
and validation. Some examples about the possible usage of the coherent
knowledge base produced are also offered and are accessible from the RDF-Store.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01084</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01084</id><created>2015-08-05</created><authors><author><keyname>Anselmi</keyname><forenames>Fabio</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author><author><keyname>Tan</keyname><forenames>Cheston</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author></authors><title>Deep Convolutional Networks are Hierarchical Kernel Machines</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In i-theory a typical layer of a hierarchical architecture consists of HW
modules pooling the dot products of the inputs to the layer with the
transformations of a few templates under a group. Such layers include as
special cases the convolutional layers of Deep Convolutional Networks (DCNs) as
well as the non-convolutional layers (when the group contains only the
identity). Rectifying nonlinearities -- which are used by present-day DCNs --
are one of the several nonlinearities admitted by i-theory for the HW module.
We discuss here the equivalence between group averages of linear combinations
of rectifying nonlinearities and an associated kernel. This property implies
that present-day DCNs can be exactly equivalent to a hierarchy of kernel
machines with pooling and non-pooling layers. Finally, we describe a conjecture
for theoretically understanding hierarchies of such modules. A main consequence
of the conjecture is that hierarchies of trained HW modules minimize memory
requirements while computing a selective and invariant representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01086</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01086</id><created>2015-08-05</created><authors><author><keyname>Bellini</keyname><forenames>Pierfrancesco</forenames></author><author><keyname>Benigni</keyname><forenames>Monica</forenames></author><author><keyname>Billero</keyname><forenames>Riccardo</forenames></author><author><keyname>Nesi</keyname><forenames>Paolo</forenames></author><author><keyname>Rauch</keyname><forenames>Nadia</forenames></author></authors><title>Km4City Ontology Building vs Data Harvesting and Cleaning for Smart-city
  Services</title><categories>cs.DB cs.AI cs.CY</categories><doi>10.1016/j.jvlc.2014.10.023</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Presently, a very large number of public and private data sets are available
from local governments. In most cases, they are not semantically interoperable
and a huge human effort would be needed to create integrated ontologies and
knowledge base for smart city. Smart City ontology is not yet standardized, and
a lot of research work is needed to identify models that can easily support the
data reconciliation, the management of the complexity, to allow the data
reasoning. In this paper, a system for data ingestion and reconciliation of
smart cities related aspects as road graph, services available on the roads,
traffic sensors etc., is proposed. The system allows managing a big data volume
of data coming from a variety of sources considering both static and dynamic
data. These data are mapped to a smart-city ontology, called KM4City (Knowledge
Model for City), and stored into an RDF-Store where they are available for
applications via SPARQL queries to provide new services to the users via
specific applications of public administration and enterprises. The paper
presents the process adopted to produce the ontology and the big data
architecture for the knowledge base feeding on the basis of open and private
data, and the mechanisms adopted for the data verification, reconciliation and
validation. Some examples about the possible usage of the coherent big data
knowledge base produced are also offered and are accessible from the RDF-Store
and related services. The article also presented the work performed about
reconciliation algorithms and their comparative assessment and selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01087</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01087</id><created>2015-08-05</created><authors><author><keyname>Sofia</keyname><forenames>Rute C.</forenames></author><author><keyname>Mendes</keyname><forenames>Paulo</forenames></author><author><keyname>Dam&#xe1;sio</keyname><forenames>Jos&#xe9; Manuel</forenames></author><author><keyname>Henriques</keyname><forenames>Sara</forenames></author><author><keyname>Giglietto</keyname><forenames>Fabio</forenames></author><author><keyname>Giambitto</keyname><forenames>Erica</forenames></author><author><keyname>Bogliolo</keyname><forenames>Alessadro</forenames></author></authors><title>Moving Towards a Socially-Driven Internet Architectural Design</title><categories>cs.CY</categories><journal-ref>ACM SIGCOMM CCR Newsletter, 42:3, 2012</journal-ref><doi>10.1145/2317307.2317316</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides an interdisciplinary perspective concerning the role of
prosumers on future Internet design based on the current trend of Internet user
empowerment. The paper debates the prosumer role, and addresses models to
develop a symmetric Internet architecture and supply-chain based on the
integration of social capital aspects. It has as goal to ignite the discussion
concerning a socially-driven Internet architectural design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01097</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01097</id><created>2015-08-05</created><authors><author><keyname>Froese</keyname><forenames>Vincent</forenames></author><author><keyname>Kanj</keyname><forenames>Iyad</forenames></author><author><keyname>Nichterlein</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author></authors><title>Finding Points in General Position</title><categories>cs.CG</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study computational aspects of the General Position Subset Selection
problem defined as follows: Given a set of points in the plane, find a
maximum-cardinality subset of points in general position. We prove that General
Position Subset Selection is NP-hard, APX-hard, and give several
fixed-parameter tractability results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01104</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01104</id><created>2015-08-05</created><authors><author><keyname>Mayer</keyname><forenames>Martin</forenames></author><author><keyname>Goertz</keyname><forenames>Norbert</forenames></author></authors><title>Bayesian Optimal Approximate Message Passing to Recover Structured
  Sparse Signals</title><categories>cs.IT math.IT</categories><comments>13 pages, 9 figures, 1 table. Submitted to IEEE Transactions on
  Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel compressed sensing recovery algorithm - termed Bayesian
Optimal Structured Signal Approximate Message Passing (BOSSAMP) - that jointly
exploits the prior distribution and the structured sparsity of a signal that
shall be recovered from noisy linear measurements. Structured sparsity is
inherent to group sparse and jointly sparse signals. Our algorithm is based on
approximate message passing that poses a low complexity recovery algorithm
whose Bayesian optimal version allows to specify a prior distribution for each
signal component. We utilize this feature in order to establish an
iteration-wise extrinsic group update step, in which likelihood ratios of
neighboring group elements provide soft information about a specific group
element. Doing so, the recovery of structured signals is drastically improved.
We derive the extrinsic group update step for a sparse binary and a sparse
Gaussian signal prior, where the nonzero entries are either one or Gaussian
distributed, respectively. We also explain how BOSSAMP is applicable to
arbitrary sparse signals. Simulations demonstrate that our approach exhibits
superior performance compared to the current state of the art, while it retains
a simple iterative implementation with low computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01108</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01108</id><created>2015-08-05</created><authors><author><keyname>Cusano</keyname><forenames>Claudio</forenames></author><author><keyname>Napoletano</keyname><forenames>Paolo</forenames></author><author><keyname>Schettini</keyname><forenames>Raimondo</forenames></author></authors><title>Evaluating color texture descriptors under large variations of
  controlled lighting conditions</title><categories>cs.CV</categories><comments>Submitted to the Journal of the Optical Society of America A</comments><doi>10.1364/JOSAA.33.000017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recognition of color texture under varying lighting conditions is still
an open issue. Several features have been proposed for this purpose, ranging
from traditional statistical descriptors to features extracted with neural
networks. Still, it is not completely clear under what circumstances a feature
performs better than the others. In this paper we report an extensive
comparison of old and new texture features, with and without a color
normalization step, with a particular focus on how they are affected by small
and large variation in the lighting conditions. The evaluation is performed on
a new texture database including 68 samples of raw food acquired under 46
conditions that present single and combined variations of light color,
direction and intensity. The database allows to systematically investigate the
robustness of texture descriptors across a large range of variations of imaging
conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01110</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01110</id><created>2015-08-05</created><authors><author><keyname>Burichenko</keyname><forenames>V. P.</forenames></author></authors><title>Symmetries of matrix multiplication algorithms. I</title><categories>cs.CC cs.DS math.GR</categories><msc-class>68Q25, 20C</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work the algorithms of fast multiplication of matrices are
considered. To any algorithm there associated a certain group of automorphisms.
These automorphism groups are found for some well-known algorithms, including
algorithms of Hopcroft, Laderman, and Pan. The automorphism group is isomorphic
to $S_3\times Z_2$ and $S_4$ for Hopcroft anf Laderman algorithms,
respectively. The studying of symmetry of algorithms may be a fruitful idea for
finding fast algorithms, by an analogy with well-known optimization problems
for codes, lattices, and graphs.
  {\em Keywords}: Strassen algorithm, symmetry, fast matrix multiplication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01115</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01115</id><created>2015-08-05</created><authors><author><keyname>Li</keyname><forenames>Xin</forenames></author></authors><title>Improved Constructions of Two-Source Extractors</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent breakthrough \cite{CZ15}, Chattopadhyay and Zuckerman gave an
explicit two-source extractor for min-entropy $k \geq \log^C n$ for some large
enough constant $C$. However, their extractor only outputs one bit. In this
paper, we improve the output of the two-source extractor to $k^{\Omega(1)}$,
while the error remains $n^{-\Omega(1)}$.
  Our improvement is obtained by giving a better extractor for $(q, t, \gamma)$
non-oblivious bit-fixing sources, which can output $t^{\Omega(1)}$ bits instead
of one bit as in \cite{CZ15}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01127</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01127</id><created>2015-08-05</created><authors><author><keyname>Hatzel</keyname><forenames>Meike</forenames></author><author><keyname>Wagner</keyname><forenames>Christoph</forenames></author><author><keyname>Peters</keyname><forenames>Kirstin</forenames></author><author><keyname>Nestmann</keyname><forenames>Uwe</forenames></author></authors><title>Encoding CSP into CCS (Extended Version)</title><categories>cs.LO</categories><comments>Extended version of the article &quot;Encoding CSP into CCS&quot; in
  EXPRESS/SOS'15</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study encodings from CSP into asynchronous CCS with name passing and
matching, so in fact, the asynchronous pi-calculus. By doing so, we discuss two
different ways to map the multi-way synchronisation mechanism of CSP into the
two-way synchronisation mechanism of CCS. Both encodings satisfy the criteria
of Gorla except for compositionality, as both use an additional top-level
context. Following the work of Parrow and Sj\&quot;odin, the first encoding uses a
central coordinator and establishes a variant of weak bisimilarity between
source terms and their translations. The second encoding is decentralised, and
thus more efficient, but ensures only a form of coupled similarity between
source terms and their translations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01128</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01128</id><created>2015-08-05</created><authors><author><keyname>Mansoor</keyname><forenames>Awais</forenames></author><author><keyname>Cerrolaza</keyname><forenames>Juan J.</forenames></author><author><keyname>Avery</keyname><forenames>Robert A.</forenames></author><author><keyname>Linguraru</keyname><forenames>Marius G.</forenames></author></authors><title>Partitioned Shape Modeling with On-the-Fly Sparse Appearance Learning
  for Anterior Visual Pathway Segmentation</title><categories>cs.CV</categories><comments>8 pages; 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MRI quantification of cranial nerves such as anterior visual pathway (AVP) in
MRI is challenging due to their thin small size, structural variation along its
path, and adjacent anatomic structures. Segmentation of pathologically abnormal
optic nerve (e.g. optic nerve glioma) poses additional challenges due to
changes in its shape at unpredictable locations. In this work, we propose a
partitioned joint statistical shape model approach with sparse appearance
learning for the segmentation of healthy and pathological AVP. Our main
contributions are: (1) optimally partitioned statistical shape models for the
AVP based on regional shape variations for greater local flexibility of
statistical shape model; (2) refinement model to accommodate pathological
regions as well as areas of subtle variation by training the model on-the-fly
using the initial segmentation obtained in (1); (3) hierarchical deformable
framework to incorporate scale information in partitioned shape and appearance
models. Our method, entitled PAScAL (PArtitioned Shape and Appearance
Learning), was evaluated on 21 MRI scans (15 healthy + 6 glioma cases) from
pediatric patients (ages 2-17). The experimental results show that the proposed
localized shape and sparse appearance-based learning approach significantly
outperforms segmentation approaches in the analysis of pathological data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01130</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01130</id><created>2015-08-05</created><authors><author><keyname>Christodoulou</keyname><forenames>George</forenames></author><author><keyname>Sgouritsa</keyname><forenames>Alkmini</forenames></author><author><keyname>Tang</keyname><forenames>Bo</forenames></author></authors><title>On the Efficiency of All-Pay Mechanisms</title><categories>cs.GT</categories><comments>26 pages, 2 figures, European Symposium on Algorithms(ESA) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the inefficiency of mixed equilibria, expressed as the price of
anarchy, of all-pay auctions in three different environments: combinatorial,
multi-unit and single-item auctions. First, we consider item-bidding
combinatorial auctions where m all-pay auctions run in parallel, one for each
good. For fractionally subadditive valuations, we strengthen the upper bound
from 2 [Syrgkanis and Tardos STOC'13] to 1.82 by proving some structural
properties that characterize the mixed Nash equilibria of the game. Next, we
design an all-pay mechanism with a randomized allocation rule for the multi-
unit auction. We show that, for bidders with submodular valuations, the
mechanism admits a unique, 75% efficient, pure Nash equilibrium. The efficiency
of this mechanism outperforms all the known bounds on the price of anarchy of
mechanisms used for multi-unit auctions. Finally, we analyze single-item
all-pay auctions motivated by their connection to contests and show tight
bounds on the price of anarchy of social welfare, revenue and maximum bid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01134</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01134</id><created>2015-08-05</created><authors><author><keyname>Mrowinski</keyname><forenames>Maciej J.</forenames></author><author><keyname>Fronczak</keyname><forenames>Agata</forenames></author><author><keyname>Fronczak</keyname><forenames>Piotr</forenames></author><author><keyname>Nedic</keyname><forenames>Olgica</forenames></author><author><keyname>Ausloos</keyname><forenames>Marcel</forenames></author></authors><title>Review times in peer review: quantitative analysis of editorial
  workflows</title><categories>physics.soc-ph cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine selected aspects of peer review and suggest possible improvements.
To this end, we analyse a dataset containing information about 300 papers
submitted to the Biochemistry and Biotechnology section of the Journal of the
Serbian Chemical Society. After separating the peer review process into stages
that each review has to go through, we use a weighted directed graph to
describe it in a probabilistic manner and test the impact of some modifications
of the editorial policy on the efficiency of the whole process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01158</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01158</id><created>2015-08-05</created><updated>2015-08-06</updated><authors><author><keyname>Solera</keyname><forenames>Francesco</forenames></author><author><keyname>Calderara</keyname><forenames>Simone</forenames></author><author><keyname>Cucchiara</keyname><forenames>Rita</forenames></author></authors><title>Socially Constrained Structural Learning for Groups Detection in Crowd</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern crowd theories agree that collective behavior is the result of the
underlying interactions among small groups of individuals. In this work, we
propose a novel algorithm for detecting social groups in crowds by means of a
Correlation Clustering procedure on people trajectories. The affinity between
crowd members is learned through an online formulation of the Structural SVM
framework and a set of specifically designed features characterizing both their
physical and social identity, inspired by Proxemic theory, Granger causality,
DTW and Heat-maps. To adhere to sociological observations, we introduce a loss
function (G-MITRE) able to deal with the complexity of evaluating group
detection performances. We show our algorithm achieves state-of-the-art results
when relying on both ground truth trajectories and tracklets previously
extracted by available detector/tracker systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01161</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01161</id><created>2015-08-05</created><authors><author><keyname>Li</keyname><forenames>Ying</forenames></author><author><keyname>Xie</keyname><forenames>Kun</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author></authors><title>Pushing towards the Limit of Sampling Rate: Adaptive Chasing Sampling</title><categories>cs.IT math.IT</categories><comments>9 pages, IEEE MASS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measurement samples are often taken in various monitoring applications. To
reduce the sensing cost, it is desirable to achieve better sensing quality
while using fewer samples. Compressive Sensing (CS) technique finds its role
when the signal to be sampled meets certain sparsity requirements. In this
paper we investigate the possibility and basic techniques that could further
reduce the number of samples involved in conventional CS theory by exploiting
learning-based non-uniform adaptive sampling.
  Based on a typical signal sensing application, we illustrate and evaluate the
performance of two of our algorithms, Individual Chasing and Centroid Chasing,
for signals of different distribution features. Our proposed learning-based
adaptive sampling schemes complement existing efforts in CS fields and do not
depend on any specific signal reconstruction technique. Compared to
conventional sparse sampling methods, the simulation results demonstrate that
our algorithms allow $46\%$ less number of samples for accurate signal
reconstruction and achieve up to $57\%$ smaller signal reconstruction error
under the same noise condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01162</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01162</id><created>2015-08-04</created><updated>2016-02-25</updated><authors><author><keyname>Zhang</keyname><forenames>Xiuchang</forenames></author><author><keyname>Ruiz</keyname><forenames>H. S.</forenames></author><author><keyname>Zhong</keyname><forenames>Z.</forenames></author><author><keyname>Coombs</keyname><forenames>T. A.</forenames></author></authors><title>Implementation of Resistive Type Superconducting Fault Current Limiters
  in Electrical Grids: Performance Analysis and Measuring of Optimal Locations</title><categories>cond-mat.supr-con cs.SY physics.ins-det</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past few years there has been a significant rise in the short-circuit
current levels in transmission and distribution networks, it due to the
increasing demands on power and the addition of sources of distributed
generations. It leads to the need of integration of novel protection systems
such as the superconducting fault current limiters (SFCLs), ... . SFCL models
on the electric distribution networks largely rely on the insertion of a step
or exponential resistance that is determined by a predefined quenching time.
However, beyond the framework of these models, the study of the performance,
reliability, and location strategy for the installation of sole or multiple
SFCLs in power grids still lacks of proper development leading to the utter
need of comprehensive and systematic studies on this issue. In this paper, we
expand the scope of the aforementioned models by considering the actual
behaviour of a SFCL in terms of the temperature dynamic power-law dependence
between the electrical field and the current density. Our results are compared
with step-resistance models for the sake of discussion and clarity of the
conclusions. Both SFCL models were integrated into a power system model built
based on the UK power standard, and the impact of these protection strategies
on the performance of the overall electricity network was studied. As a
representative renewable energy source, a 90 MVA wind farm was considered for
the simulations. Three fault conditions have been simulated, and the figures
for the fault current reduction predicted by both fault current limiting models
have been compared in terms of multiple current measuring points and allocation
strategies...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01167</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01167</id><created>2015-08-05</created><authors><author><keyname>Roberto</keyname><forenames>Elizabeth</forenames></author></authors><title>Measuring Inequality and Segregation</title><categories>stat.ME cs.IT math.IT physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, I introduce the Divergence Index, a conceptually intuitive and
methodologically rigorous measure of inequality and segregation. The index
measures the difference between a distribution of interest and another
empirical, theoretical, or normative distribution. The Divergence Index
provides flexibility in specifying a theoretically meaningful basis for
evaluating inequality. It evaluates how surprising an empirical distribution is
given a theoretical distribution that represents equality. I demonstrate the
unique features of the new measure, as well as deriving its mathematical
equivalence with Theil's Inequality Index and the Information Theory Index. I
compare the dynamics of the measures using simulated data, and an empirical
analysis of racial residential segregation in the Detroit, MI, metro area. The
Information Theory Index has become the gold standard for decomposition
analyses of segregation. I show that although the Information Theory Index can
be decomposed for subareas, it is misleading to interpret the results as
segregation. The Divergence Index addresses the limitations of existing
measures and accurately decomposes segregation across contexts and nested
levels of geography. By creating an alternative measure, I provide a distinct
lens, which enables richer, deeper, more accurate understandings of inequality
and segregation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01168</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01168</id><created>2015-08-03</created><authors><author><keyname>Vu</keyname><forenames>Tung T.</forenames></author><author><keyname>Kha</keyname><forenames>Ha Hoang</forenames></author><author><keyname>Duong</keyname><forenames>Trung Q.</forenames></author><author><keyname>Vo</keyname><forenames>Nguyen-Son</forenames></author></authors><title>Particle Swarm Optimization for Weighted Sum Rate Maximization in MIMO
  Broadcast Channels</title><categories>cs.IT cs.NE math.IT math.OC</categories><comments>submitted to Wireless Personal Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the downlink multiple-input-multipleoutput
(MIMO) broadcast channels in which a base transceiver station (BTS) broadcasts
multiple data streams to K MIMO mobile stations (MSs) simultaneously. In order
to maximize the weighted sum-rate (WSR) of the system subject to the
transmitted power constraint, the design problem is to find the pre-coding
matrices at BTS and the decoding matrices at MSs. However, such a design
problem is typically a nonlinear and nonconvex optimization and, thus, it is
quite hard to obtain the analytical solutions. To tackle with the mathematical
difficulties, we propose an efficient stochastic optimization algorithm to
optimize the transceiver matrices. Specifically, we utilize the linear minimum
mean square error (MMSE) Wiener filters at MSs. Then, we introduce the
constrained particle swarm optimization (PSO) algorithm to jointly optimize the
precoding and decoding matrices. Numerical experiments are exhibited to
validate the effectiveness of the proposed algorithm in terms of convergence,
computational complexity and total WSR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01169</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01169</id><created>2015-08-03</created><authors><author><keyname>Vu</keyname><forenames>Tung T.</forenames></author><author><keyname>Kha</keyname><forenames>Ha Hoang</forenames></author><author><keyname>Duong</keyname><forenames>Trung Q.</forenames></author></authors><title>Interference Alignment Designs for Secure Multiuser MIMO Systems: Rank
  Constrained Rank Minimization Approach</title><categories>cs.IT math.IT math.OC</categories><comments>submitted to the 3rd international conference on Computing,
  Management and Telecommunications (ComManTel 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we formulate the interference alignment (IA) problem for a
multiuser multiple-input multiple-output (MIMO) system in the presence of an
eavesdropper as a rank constrained rank minimization (RCRM) problem. The aim of
the proposed rank minimization IA schemes is to find the precoding and receiver
subspace matrices to align interference and wiretapped signals into the lowest
dimension subspaces while keeping the desired signal subspace spanning full
available spatial dimensions. To deal with the nonconvexity of the rank
function, we present two convex relaxations of the RCRM problem, namely nuclear
norm (NN) and reweighted nuclear norm (RNN), and transform the rank constraints
to equivalent and tractable ones. We then derive a coordinate decent approach
to obtain the solutions for IA schemes. The simulation results show that our
proposed IA designs outperform the conventional IA design in terms of average
secrecy sum rate. On the other hand, our proposed designs perform the same or
better than other secure IA schemes which account for low interference and
wiretapped signal power rather than for low dimensions of interference and
wiretapped signal matrices in the systems which achieve the perfect IA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01171</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01171</id><created>2015-08-05</created><authors><author><keyname>Afrati</keyname><forenames>Foto</forenames></author><author><keyname>Dolev</keyname><forenames>Shlomi</forenames></author><author><keyname>Sharma</keyname><forenames>Shantanu</forenames></author><author><keyname>Ullman</keyname><forenames>Jeffrey D.</forenames></author></authors><title>Meta-MapReduce: A Technique for Reducing Communication in MapReduce
  Computations</title><categories>cs.DB cs.DC</categories><comments>This is a version that appeared as a brief announcement in 17th
  International Symposium on Stabilization, Safety, and Security of Distributed
  Systems (SSS, 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The federation of cloud and big data activities is the next challenge where
MapReduce should be modified to avoid (big) data migration across remote
(cloud) sites. This is exactly our scope of research, where only the very
essential data for obtaining the result is transmitted, reducing communication,
processing and preserving data privacy as much as possible. We propose an
algorithmic technique for MapReduce algorithms, called Meta-MapReduce, that
decreases the communication cost by allowing us to process and move metadata to
clouds and from the map to reduce phases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01176</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01176</id><created>2015-08-05</created><authors><author><keyname>Orchard</keyname><forenames>Garrick</forenames></author><author><keyname>Meyer</keyname><forenames>Cedric</forenames></author><author><keyname>Etienne-Cummings</keyname><forenames>Ralph</forenames></author><author><keyname>Posch</keyname><forenames>Christoph</forenames></author><author><keyname>Thakor</keyname><forenames>Nitish</forenames></author><author><keyname>Benosman</keyname><forenames>Ryad</forenames></author></authors><title>HFirst: A Temporal Approach to Object Recognition</title><categories>cs.CV</categories><comments>13 pages, 10 figures</comments><journal-ref>Pattern Analysis and Machine Intelligence, IEEE Transactions on,
  vol.37, no.10, pp.2028-2040, Oct 2015</journal-ref><doi>10.1109/TPAMI.2015.2392947</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a spiking hierarchical model for object recognition
which utilizes the precise timing information inherently present in the output
of biologically inspired asynchronous Address Event Representation (AER) vision
sensors. The asynchronous nature of these systems frees computation and
communication from the rigid predetermined timing enforced by system clocks in
conventional systems. Freedom from rigid timing constraints opens the
possibility of using true timing to our advantage in computation. We show not
only how timing can be used in object recognition, but also how it can in fact
simplify computation. Specifically, we rely on a simple
temporal-winner-take-all rather than more computationally intensive synchronous
operations typically used in biologically inspired neural networks for object
recognition. This approach to visual computation represents a major paradigm
shift from conventional clocked systems and can find application in other
sensory modalities and computational tasks. We showcase effectiveness of the
approach by achieving the highest reported accuracy to date (97.5\%$\pm$3.5\%)
for a previously published four class card pip recognition task and an accuracy
of 84.9\%$\pm$1.9\% for a new more difficult 36 class character recognition
task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01177</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01177</id><created>2015-08-05</created><authors><author><keyname>Bernardi</keyname><forenames>Lucas</forenames></author><author><keyname>Kamps</keyname><forenames>Jaap</forenames></author><author><keyname>Kiseleva</keyname><forenames>Julia</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Melanie JI</forenames></author></authors><title>The Continuous Cold Start Problem in e-Commerce Recommender Systems</title><categories>cs.IR</categories><comments>6 pages, 3 figures. 2nd Workshop on New Trends in Content-Based
  Recommender Systems, RecSys 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many e-commerce websites use recommender systems to recommend items to users.
When a user or item is new, the system may fail because not enough information
is available on this user or item. Various solutions to this `cold-start
problem' have been proposed in the literature. However, many real-life
e-commerce applications suffer from an aggravated, recurring version of
cold-start even for known users or items, since many users visit the website
rarely, change their interests over time, or exhibit different personas. This
paper exposes the `Continuous Cold Start' (CoCoS) problem and its consequences
for content- and context-based recommendation from the viewpoint of typical
e-commerce applications, illustrated with examples from a major travel
recommendation website, Booking.com.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01182</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01182</id><created>2015-08-05</created><authors><author><keyname>Li</keyname><forenames>Ying</forenames></author><author><keyname>Guo</keyname><forenames>Katherine</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Soljanin</keyname><forenames>Emina</forenames></author><author><keyname>Woo</keyname><forenames>Thomas</forenames></author></authors><title>SEARS: Space Efficient And Reliable Storage System in the Cloud</title><categories>cs.DC</categories><comments>4 pages, IEEE LCN 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's cloud storage services must offer storage reliability and fast data
retrieval for large amount of data without sacrificing storage cost. We present
SEARS, a cloud-based storage system which integrates erasure coding and data
deduplication to support efficient and reliable data storage with fast user
response time. With proper association of data to storage server clusters,
SEARS provides flexible mixing of different configurations, suitable for
real-time and archival applications.
  Our prototype implementation of SEARS over Amazon EC2 shows that it
outperforms existing storage systems in storage efficiency and file retrieval
time. For 3 MB files, SEARS delivers retrieval time of $2.5$ s compared to $7$
s with existing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01190</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01190</id><created>2015-08-05</created><authors><author><keyname>D&#xf6;ring</keyname><forenames>Robert</forenames></author></authors><title>Evaluation des Algorithmus DIBADAWN zum Detektieren von Bruecken und
  Gelenkpunkten in 802.11 Maschennetzen</title><categories>cs.NI</categories><comments>Bachelor of Science thesis</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The topic is the evaluation of the algorithm DIBADAWN to detect bridges and
articulation points in an wireless mess network. Therefore this algorithm is
implemented and evaluated per simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01191</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01191</id><created>2015-08-05</created><authors><author><keyname>Fueloep</keyname><forenames>J.</forenames></author><author><keyname>Koczkodaj</keyname><forenames>W. W.</forenames></author><author><keyname>Szarek</keyname><forenames>S. J.</forenames></author></authors><title>A different perspective on a scale for pairwise comparisons</title><categories>cs.AI</categories><comments>16 pages, 1 figure; the mathematical theory has been provided for the
  use of small scale (1 to 3) for pairwise comparisons (but not only)</comments><journal-ref>Logic Journal of the IGPL Volume: 18 Issue: 6 Pages: 859-869
  Published: DEC 2010</journal-ref><doi>10.1093/jigpal/jzp062</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the major challenges for collective intelligence is inconsistency,
which is unavoidable whenever subjective assessments are involved. Pairwise
comparisons allow one to represent such subjective assessments and to process
them by analyzing, quantifying and identifying the inconsistencies.
  We propose using smaller scales for pairwise comparisons and provide
mathematical and practical justifications for this change. Our postulate's aim
is to initiate a paradigm shift in the search for a better scale construction
for pairwise comparisons. Beyond pairwise comparisons, the results presented
may be relevant to other methods using subjective scales.
  Keywords: pairwise comparisons, collective intelligence, scale, subjective
assessment, inaccuracy, inconsistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01192</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01192</id><created>2015-08-05</created><authors><author><keyname>Stanton</keyname><forenames>Andrew</forenames></author><author><keyname>Thart</keyname><forenames>Amanda</forenames></author><author><keyname>Jain</keyname><forenames>Ashish</forenames></author><author><keyname>Vyas</keyname><forenames>Priyank</forenames></author><author><keyname>Chatterjee</keyname><forenames>Arpan</forenames></author><author><keyname>Shakarian</keyname><forenames>Paulo</forenames></author></authors><title>Mining for Causal Relationships: A Data-Driven Study of the Islamic
  State</title><categories>cs.CY cs.AI</categories><journal-ref>Final version presented at KDD 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Islamic State of Iraq and al-Sham (ISIS) is a dominant insurgent group
operating in Iraq and Syria that rose to prominence when it took over Mosul in
June, 2014. In this paper, we present a data-driven approach to analyzing this
group using a dataset consisting of 2200 incidents of military activity
surrounding ISIS and the forces that oppose it (including Iraqi, Syrian, and
the American-led coalition). We combine ideas from logic programming and causal
reasoning to mine for association rules for which we present evidence of
causality. We present relationships that link ISIS vehicle-bourne improvised
explosive device (VBIED) activity in Syria with military operations in Iraq,
coalition air strikes, and ISIS IED activity, as well as rules that may serve
as indicators of spikes in indirect fire, suicide attacks, and arrests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01211</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01211</id><created>2015-08-05</created><updated>2015-08-19</updated><authors><author><keyname>Chan</keyname><forenames>William</forenames></author><author><keyname>Jaitly</keyname><forenames>Navdeep</forenames></author><author><keyname>Le</keyname><forenames>Quoc V.</forenames></author><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author></authors><title>Listen, Attend and Spell</title><categories>cs.CL cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Listen, Attend and Spell (LAS), a neural network that learns to
transcribe speech utterances to characters. Unlike traditional DNN-HMM models,
this model learns all the components of a speech recognizer jointly. Our system
has two components: a listener and a speller. The listener is a pyramidal
recurrent network encoder that accepts filter bank spectra as inputs. The
speller is an attention-based recurrent network decoder that emits characters
as outputs. The network produces character sequences without making any
independence assumptions between the characters. This is the key improvement of
LAS over previous end-to-end CTC models. On a subset of the Google voice search
task, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a
language model, and 10.3% with language model rescoring over the top 32 beams.
By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01216</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01216</id><created>2015-08-05</created><authors><author><keyname>Azari</keyname><forenames>Amin</forenames></author><author><keyname>Harsini</keyname><forenames>Jalil S.</forenames></author><author><keyname>Lahouti</keyname><forenames>Farshad</forenames></author></authors><title>Power Allocation in Multi-hop OFDM Transmission Systems with
  Amplify-and-Forward Relaying: A Unified Approach</title><categories>cs.IT math.IT</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a unified approach for power allocation (PA) problem in
multi-hop orthogonal frequency division multiplexing (OFDM) amplify-and-forward
(AF) relaying systems has been developed. In the proposed approach, we consider
short and long-term individual and total power constraints at the source and
relays, and devise decentralized low complexity PA algorithms when wireless
links are subject to channel path-loss and small-scale Rayleigh fading. In
particular, aiming at improving the instantaneous rate of multi-hop
transmission systems with AF relaying, we develop (i) a near-optimal iterative
PA algorithm based on the exact analysis of the received SNR at the
destination; (ii) a low complexity-suboptimal iterative PA algorithm based on
an approximate expression at high-SNR regime; and (iii) a low complexity-non
iterative PA scheme with limited performance loss. Since the PA problem in
multi-hop systems is too complex to solve with known optimization solvers, in
the proposed formulations, we adopted a two-stage approach, including a power
distribution phase among distinct subcarriers, and a power allocation phase
among different relays. The individual PA phases are then appropriately linked
through an iterative method which tries to compensate the performance loss
caused by the distinct two-stage approach. Simulation results show the superior
performance of the proposed power allocation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01235</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01235</id><created>2015-08-05</created><authors><author><keyname>Pourhabib</keyname><forenames>Arash</forenames></author></authors><title>Empirical Similarity for Absent Data Generation in Imbalanced
  Classification</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When the training data in a two-class classification problem is overwhelmed
by one class, most classification techniques fail to correctly identify the
data points belonging to the underrepresented class. We propose
Similarity-based Imbalanced Classification (SBIC) that learns patterns in the
training data based on an empirical similarity function. To take the imbalanced
structure of the training data into account, SBIC utilizes the concept of
absent data, i.e. data from the minority class which can help better find the
boundary between the two classes. SBIC simultaneously optimizes the weights of
the empirical similarity function and finds the locations of absent data
points. As such, SBIC uses an embedded mechanism for synthetic data generation
which does not modify the training dataset, but alters the algorithm to suit
imbalanced datasets. Therefore, SBIC uses the ideas of both major schools of
thoughts in imbalanced classification: Like cost-sensitive approaches SBIC
operates on an algorithm level to handle imbalanced structures; and similar to
synthetic data generation approaches, it utilizes the properties of unobserved
data points from the minority class. The application of SBIC to imbalanced
datasets suggests it is comparable to, and in some cases outperforms, other
commonly used classification techniques for imbalanced datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01239</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01239</id><created>2015-08-05</created><updated>2015-12-21</updated><authors><author><keyname>Joglekar</keyname><forenames>Manas</forenames></author><author><keyname>Re</keyname><forenames>Christopher</forenames></author></authors><title>It's all a matter of degree: Using degree information to optimize
  multiway joins</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We optimize multiway equijoins on relational tables using degree information.
We give a new bound that uses degree information to more tightly bound the
maximum output size of a query. On real data, our bound on the number of
triangles in a social network can be up to $95$ times tighter than existing
worst case bounds. We show that using only a constant amount of degree
information, we are able to obtain join algorithms with a running time that has
a smaller exponent than existing algorithms--{\em for any database instance}.
We also show that this degree information can be obtained in nearly linear
time, which yields asymptotically faster algorithms in the serial setting and
lower communication algorithms in the MapReduce setting.
  In the serial setting, the data complexity of join processing can be
expressed as a function $O(\IN^x + \OUT)$ in terms of input size $\IN$ and
output size $\OUT$ in which $x$ depends on the query. An upper bound for $x$ is
given by fractional hypertreewidth. We are interested in situations in which we
can get algorithms for which $x$ is strictly smaller than the fractional
hypertreewidth. We say that a join can be processed in subquadratic time if $x
&lt; 2$. Building on the AYZ algorithm for processing cycle joins in quadratic
time, for a restricted class of joins which we call $1$-series-parallel graphs,
we obtain a complete decision procedure for identifying subquadratic
solvability (subject to the $3$-SUM problem requiring quadratic time). Our
$3$-SUM based quadratic lower bound is tight, making it the only known tight
bound for joins that does not require any assumption about the matrix
multiplication exponent $\omega$. We also give a MapReduce algorithm that meets
our improved communication bound and handles essentially optimal parallelism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01244</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01244</id><created>2015-08-05</created><updated>2015-09-05</updated><authors><author><keyname>Huang</keyname><forenames>Qiong</forenames></author><author><keyname>Veeraraghavan</keyname><forenames>Ashok</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>TabletGaze: Unconstrained Appearance-based Gaze Estimation in Mobile
  Tablets</title><categories>cs.CV</categories><comments>13 pages, 17 figures, submitted to journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study gaze estimation on tablets; our key design goal is uncalibrated gaze
estimation using the front-facing camera during natural use of tablets, where
the posture and method of holding the tablet is not constrained. We collected
the first large unconstrained gaze dataset of tablet users, labeled Rice
TabletGaze dataset. The dataset consists of 51 subjects, each with 4 different
postures and 35 gaze locations. Subjects vary in race, gender and in their need
for prescription glasses, all of which might impact gaze estimation accuracy.
Driven by our observations on the collected data, we present a TabletGaze
algorithm for automatic gaze estimation using multi-level HoG feature and
Random Forests regressor. The TabletGaze algorithm achieves a mean error of
3.17 cm. We perform extensive evaluation on the impact of various factors such
as dataset size, race, wearing glasses and user posture on the gaze estimation
accuracy and make important observations about the impact of these factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01256</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01256</id><created>2015-08-05</created><authors><author><keyname>Mendoza-Smith</keyname><forenames>Rodrigo</forenames></author><author><keyname>Tanner</keyname><forenames>Jared</forenames></author></authors><title>Expander $\ell_0$-Decoding</title><categories>math.NA cs.IT math.CO math.IT</categories><comments>14 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce two new algorithms, Serial-$\ell_0$ and Parallel-$\ell_0$ for
solving a large underdetermined linear system of equations $y = Ax \in
\mathbb{R}^m$ when it is known that $x \in \mathbb{R}^n$ has at most $k &lt; m$
nonzero entries and that $A$ is the adjacency matrix of an unbalanced left
$d$-regular expander graph. The matrices in this class are sparse and allow a
highly efficient implementation. A number of algorithms have been designed to
work exclusively under this setting, composing the branch of combinatorial
compressed-sensing (CCS).
  Serial-$\ell_0$ and Parallel-$\ell_0$ iteratively minimise $\|y - A\hat
x\|_0$ by successfully combining two desirable features of previous CCS
algorithms: the information-preserving strategy of ER, and the parallel
updating mechanism of SMP. We are able to link these elements and guarantee
convergence in $\mathcal{O}(dn \log k)$ operations by assuming that the signal
is dissociated, meaning that all of the $2^k$ subset sums of the support of $x$
are pairwise different. However, we observe empirically that the signal need
not be exactly dissociated in practice. Moreover, we observe Serial-$\ell_0$
and Parallel-$\ell_0$ to be able to solve large scale problems with a larger
fraction of nonzeros than other algorithms when the number of measurements is
substantially less than the signal length; in particular, they are able to
reliably solve for a $k$-sparse vector $x\in\mathbb{R}^n$ from $m$ expander
measurements with $n/m=10^3$ and $k/m$ up to four times greater than what is
achievable by $\ell_1$-regularization from dense Gaussian measurements.
Additionally, Serial-$\ell_0$ and Parallel-$\ell_0$ are observed to be able to
solve large problems sizes in substantially less time than other algorithms for
compressed sensing. In particular, Parallel-$\ell_0$ is structured to take
advantage of massively parallel architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01274</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01274</id><created>2015-08-06</created><authors><author><keyname>Zhu</keyname><forenames>Weiping</forenames></author></authors><title>Divide and Conquer: Loss Tomography in General Topology</title><categories>cs.NI</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Loss tomography has received considerable attention in recent years and a
large number of estimators based on maximum likelihood (ML) or Bayesian
principles have been proposed for the tree topology. In contrast, there has
been no maximum likelihood estimator (MLE) proposed for the general topology
although there has been enormous interest to extend the estimators proposed for
the tree topology to the general one. In this paper, we propose a MLE for the
general topology that consists of a divide and conquer strategy, two
estimators, and an estimation order. The three components together ensure the
estimate obtained for each link is the maximum likelihood one. Apart from the
MLE, the statistical properties of the estimators are presented, including
minimum-variance unbiasedness, consistence and asymptotically efficiency. In
addition to the proof of the superiority of the proposed estimator over the
previous ones, a simulation study is conducted to verify the theoretical
result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01288</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01288</id><created>2015-08-06</created><authors><author><keyname>Komuravelli</keyname><forenames>Anvesh</forenames></author><author><keyname>Bjorner</keyname><forenames>Nikolaj</forenames></author><author><keyname>Gurfinkel</keyname><forenames>Arie</forenames></author><author><keyname>McMillan</keyname><forenames>Kenneth L.</forenames></author></authors><title>Compositional Verification of Procedural Programs using Horn Clauses
  over Integers and Arrays</title><categories>cs.LO cs.PL cs.SE</categories><comments>8 pages, FMCAD 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a compositional SMT-based algorithm for safety of procedural C
programs that takes the heap into consideration as well. Existing SMT-based
approaches are either largely restricted to handling linear arithmetic
operations and properties, or are non-compositional. We use Constrained Horn
Clauses (CHCs) to represent the verification conditions where the memory
operations are modeled using the extensional theory of arrays (ARR). First, we
describe an exponential time quantifier elimination (QE) algorithm for ARR
which can introduce new quantifiers of the index and value sorts. Second, we
adapt the QE algorithm to efficiently obtain under-approximations using models,
resulting in a polynomial time Model Based Projection (MBP) algorithm. Third,
we integrate the MBP algorithm into the framework of compositional reasoning of
procedural programs using may and must summaries recently proposed by us. Our
solutions to the CHCs are currently restricted to quantifier-free formulas.
Finally, we describe our practical experience over SV-COMP'15 benchmarks using
an implementation in the tool SPACER.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01292</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01292</id><created>2015-08-06</created><updated>2015-11-23</updated><authors><author><keyname>Kalinovskii</keyname><forenames>Ilya</forenames></author><author><keyname>Spitsyn</keyname><forenames>Vladimir</forenames></author></authors><title>Compact Convolutional Neural Network Cascade for Face Detection</title><categories>cs.CV</categories><comments>Demo video and test results available at
  http://github.com/Bkmz21/FD-Evaluation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of faces detection in images or video streams is a classical
problem of computer vision. The multiple solutions of this problem have been
proposed, but the question of their optimality is still open. Many algorithms
achieve a high quality face detection, but at the cost of high computational
complexity. This restricts their application in the real-time systems. This
paper presents a new solution of the frontal face detection problem based on
compact convolutional neural networks cascade. The test results on FDDB dataset
show that it is competitive with state-of-the-art algorithms. This proposed
detector is implemented using three technologies: SSE/AVX/AVX2 instruction sets
for Intel CPUs, Nvidia CUDA, OpenCL. The detection speed of our approach
considerably exceeds all the existing CPU-based and GPU-based algorithms.
Because of high computational efficiency, our detector can processing 4K Ultra
HD video stream in real time (up to 27 fps) on mobile platforms (Intel Ivy
Bridge CPUs and Nvidia Kepler GPUs) in searching objects with the dimension
60x60 pixels or higher. At the same time its performance weakly dependent on
the background and number of objects in scene. This is achieved by the
asynchronous computation of stages in the cascade.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01295</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01295</id><created>2015-08-06</created><authors><author><keyname>Kittichokechai</keyname><forenames>Kittipong</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Secret key-based Identification and Authentication with a Privacy
  Constraint</title><categories>cs.IT cs.CR math.IT</categories><comments>33 pages, 3 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of identification and authentication based on secret
key generation from some user-generated source data (e.g., a biometric source).
The goal is to reliably identify users pre-enrolled in a database as well as
authenticate them based on the estimated secret key while preserving the
privacy of the enrolled data and of the generated keys. We characterize the
optimal tradeoff between the identification rate, the compression rate of the
users' source data, information leakage rate, and secret key rate. In
particular, we provide a coding strategy based on layered random binning which
is shown to be optimal. In addition, we study a related secure
identification/authentication problem where an adversary tries to deceive the
system using its own data. Here the optimal tradeoff between the identification
rate, compression rate, leakage rate, and exponent of the maximum false
acceptance probability is provided. The results reveal a close connection
between the optimal secret key rate and the false acceptance exponent of the
identification/authentication system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01300</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01300</id><created>2015-08-06</created><authors><author><keyname>Fusco</keyname><forenames>Emanuele G.</forenames></author><author><keyname>Pelc</keyname><forenames>Andrzej</forenames></author></authors><title>Knowledge, Level of Symmetry, and Time of Leader Election</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the time needed for deterministic leader election in the ${\cal
LOCAL}$ model, where in every round a node can exchange any messages with its
neighbors and perform any local computations. The topology of the network is
unknown and nodes are unlabeled, but ports at each node have arbitrary fixed
labelings which, together with the topology of the network, can create
asymmetries to be exploited in leader election. We consider two versions of the
leader election problem: strong LE in which exactly one leader has to be
elected, if this is possible, while all nodes must terminate declaring that
leader election is impossible otherwise, and weak LE, which differs from strong
LE in that no requirement on the behavior of nodes is imposed, if leader
election is impossible. We show that the time of leader election depends on
three parameters of the network: its diameter $D$, its size $n$, and its level
of symmetry $\lambda$, which, when leader election is feasible, is the smallest
depth at which some node has a unique view of the network. It also depends on
the knowledge by the nodes, or lack of it, of parameters $D$ and $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01303</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01303</id><created>2015-08-06</created><updated>2015-08-31</updated><authors><author><keyname>Holme</keyname><forenames>Petter</forenames></author></authors><title>Modern temporal network theory: A colloquium</title><categories>physics.soc-ph cs.SI</categories><comments>Final accepted version</comments><journal-ref>Eur. Phys. J. B 88, 234 (2015)</journal-ref><doi>10.1140/epjb/e2015-60657-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The power of any kind of network approach lies in the ability to simplify a
complex system so that one can better understand its function as a whole.
Sometimes it is beneficial, however, to include more information than in a
simple graph of only nodes and links. Adding information about times of
interactions can make predictions and mechanistic understanding more accurate.
The drawback, however, is that there are not so many methods available, partly
because temporal networks is a relatively young field, partly because it more
difficult to develop such methods compared to for static networks. In this
colloquium, we review the methods to analyze and model temporal networks and
processes taking place on them, focusing mainly on the last three years. This
includes the spreading of infectious disease, opinions, rumors, in social
networks; information packets in computer networks; various types of signaling
in biology, and more. We also discuss future directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01306</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01306</id><created>2015-08-06</created><updated>2015-09-02</updated><authors><author><keyname>Minock</keyname><forenames>Michael</forenames></author><author><keyname>Everling</keyname><forenames>Nils</forenames></author></authors><title>Replication and Generalization of PRECISE</title><categories>cs.CL cs.AI cs.DB</categories><acm-class>H.5.2; I.2.1; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report describes an initial replication study of the PRECISE system and
develops a clearer, more formal description of the approach. Based on our
evaluation, we conclude that the PRECISE results do not fully replicate.
However the formalization developed here suggests a road map to further enhance
and extend the approach pioneered by PRECISE.
  After a long, productive discussion with Ana-Maria Popescu (one of the
authors of PRECISE) we got more clarity on the PRECISE approach and how the
lexicon was authored for the GEO evaluation. Based on this we built a more
direct implementation over a repaired formalism. Although our new evaluation is
not yet complete, it is clear that the system is performing much better now. We
will continue developing our ideas and implementation and generate a future
report/publication that more accurately evaluates PRECISE like approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01308</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01308</id><created>2015-08-06</created><authors><author><keyname>Duran</keyname><forenames>Joan</forenames></author><author><keyname>Moeller</keyname><forenames>Michael</forenames></author><author><keyname>Sbert</keyname><forenames>Catalina</forenames></author><author><keyname>Cremers</keyname><forenames>Daniel</forenames></author></authors><title>Collaborative Total Variation: A General Framework for Vectorial TV
  Models</title><categories>cs.CV math.HO math.NA math.OC</categories><msc-class>15A60, 65F22, 65K10, 68U10, 90C25, 90C46, 94A08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even after over two decades, the total variation (TV) remains one of the most
popular regularizations for image processing problems and has sparked a
tremendous amount of research, particularly to move from scalar to
vector-valued functions. In this paper, we consider the gradient of a color
image as a three dimensional matrix or tensor with dimensions corresponding to
the spatial extend, the differences to other pixels, and the spectral channels.
The smoothness of this tensor is then measured by taking different norms along
the different dimensions. Depending on the type of these norms one obtains very
different properties of the regularization, leading to novel models for color
images. We call this class of regularizations collaborative total variation
(CTV). On the theoretical side, we characterize the dual norm, the
subdifferential and the proximal mapping of the proposed regularizers. We
further prove, with the help of the generalized concept of singular vectors,
that an $\ell^{\infty}$ channel coupling makes the most prior assumptions and
has the greatest potential to reduce color artifacts. Our practical
contributions consist of an extensive experimental section where we compare the
performance of a large number of collaborative TV methods for inverse problems
like denoising, deblurring and inpainting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01310</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01310</id><created>2015-08-06</created><authors><author><keyname>Akbarzadeh</keyname><forenames>Afsane</forenames></author><author><keyname>Shadkam</keyname><forenames>Elham</forenames></author></authors><title>The study of cuckoo optimization algorithm for production planning
  problem</title><categories>math.OC cs.NE</categories><doi>10.5121/ijcax.2015.2301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constrained Nonlinear programming problems are hard problems, and one of the
most widely used and common problems for production planning problem to
optimize. In this study, one of the mathematical models of production planning
is survey and the problem solved by cuckoo algorithm. Cuckoo Algorithm is
efficient method to solve continues non linear problem. Moreover, mentioned
models of production planning solved with Genetic algorithm and Lingo software
and the results will compared. The Cuckoo Algorithm is suitable choice for
optimization in convergence of solution
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01321</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01321</id><created>2015-08-06</created><authors><author><keyname>Moncada</keyname><forenames>Fatima M.</forenames></author><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>On Gobbledygook and Mood of the Philippine Senate: An Exploratory Study
  on the Readability and Sentiment of Selected Philippine Senators' Microposts</title><categories>cs.CL cs.CY</categories><comments>13 pages, 6 figures, submitted to the Asia Pacific Journal on
  Education, Arts, and Sciences</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper presents the findings of a readability assessment and sentiment
analysis of selected six Philippine senators' microposts over the popular
Twitter microblog. Using the Simple Measure of Gobbledygook (SMOG), tweets of
Senators Cayetano, Defensor-Santiago, Pangilinan, Marcos, Guingona, and
Escudero were assessed. A sentiment analysis was also done to determine the
polarity of the senators' respective microposts. Results showed that on the
average, the six senators are tweeting at an eight to ten SMOG level. This
means that, at least a sixth grader will be able to understand the senators'
tweets. Moreover, their tweets are mostly neutral and their sentiments vary in
unison at some period of time. This could mean that a senator's tweet sentiment
is affected by specific Philippine-based events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01324</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01324</id><created>2015-08-06</created><authors><author><keyname>Dolev</keyname><forenames>Shlomi</forenames></author><author><keyname>Krzywiecki</keyname><forenames>Lukasz</forenames></author><author><keyname>Panwar</keyname><forenames>Nisha</forenames></author><author><keyname>Segal</keyname><forenames>Michael</forenames></author></authors><title>Vehicle to Vehicle Authentication</title><categories>cs.CR cs.NI</categories><comments>This is a version that appeared as a brief announcement in 17th
  International Symposium on Stabilization, Safety, and Security of Distributed
  Systems (SSS, 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent future, vehicles will establish a spontaneous connection over a
wireless radio channel, coordinating actions and information. Vehicles will
exchange warning messages over the wireless radio channel through Dedicated
Short Range Communication (IEEE 1609) over the Wireless Access in Vehicular
Environment (802.11p). Unfortunately, the wireless communication among vehicles
is vulnerable to security threats that may lead to very serious safety hazards.
Therefore, the warning messages being exchanged must incorporate an authentic
factor such that recipient is willing to verify and accept the message in a
timely manner
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01326</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01326</id><created>2015-08-06</created><authors><author><keyname>Abdollahi</keyname><forenames>Neda</forenames></author><author><keyname>Jafari</keyname><forenames>Mohammad</forenames></author><author><keyname>Bayat</keyname><forenames>Morteza</forenames></author><author><keyname>Amiri</keyname><forenames>Ali</forenames></author><author><keyname>Fathy</keyname><forenames>Mahmood</forenames></author></authors><title>An efficient parallel algorithm for computing determinant of non square
  matrices</title><categories>cs.DC</categories><comments>12 pages,4 tables in International journal of Distributed and
  parallel systems(IJDPS),July 2015,vol 6, no 4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most significant challenges in Computing Determinant of
Rectangular Matrices is high time complexity of its algorithm. Among all
definitions of determinant of rectangular matrices, used definition has special
features which make it more notable. But in this definition, C(n m) sub
matrices of the order m*m needed to be generated that put this problem in NP
hard class. On the other hand, any row or column reduction operation may hardly
lead to diminish the volume of calculation. Therefore, in this paper we try to
present the parallel algorithm which can decrease the time complexity of
computing the determinant of non-square matrices to O(pow(n,2)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01330</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01330</id><created>2015-08-06</created><authors><author><keyname>Abuseta</keyname><forenames>Yousef</forenames></author><author><keyname>Swesi</keyname><forenames>Khaled</forenames></author></authors><title>Design Patterns for Self Adaptive Systems Engineering</title><categories>cs.SE</categories><comments>18 pages, 11 figures</comments><journal-ref>International Journal of Software Engineering &amp; Applications
  (IJSEA), Vol.6, No.4, July 2015</journal-ref><doi>10.5121/ijsea.2015.6402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self adaptation has been proposed to overcome the complexity of today's
software systems which results from the uncertainty issue. Aspects of
uncertainty include changing systems goals, changing resource availability and
dynamic operating conditions. Feedback control loops have been recognized as
vital elements for engineering self-adaptive systems. However, despite their
importance, there is still a lack of systematic way of the design of the
interactions between the different components comprising one particular
feedback control loop as well as the interactions between components from
different control loops . Most existing approaches are either domain specific
or too abstract to be useful. In addition, the issue of multiple control loops
is often neglected and consequently self adaptive systems are often designed
around a single loop. In this paper we propose a set of design patterns for
modeling and designing self adaptive software systems based on MAPE-K Control
loop of IBM architecture blueprint which takes into account the multiple
control loops issue. A case study is presented to illustrate the applicability
of the proposed design patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01340</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01340</id><created>2015-08-06</created><authors><author><keyname>Boull&#xe9;</keyname><forenames>Marc</forenames></author></authors><title>Universal Approximation of Edge Density in Large Graphs</title><categories>cs.SI cs.DB stat.ML</categories><acm-class>H.2.8; I.5.3; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel way to summarize the structure of large
graphs, based on non-parametric estimation of edge density in directed
multigraphs. Following coclustering approach, we use a clustering of the
vertices, with a piecewise constant estimation of the density of the edges
across the clusters, and address the problem of automatically and reliably
inferring the number of clusters, which is the granularity of the coclustering.
We use a model selection technique with data-dependent prior and obtain an
exact evaluation criterion for the posterior probability of edge density
estimation models. We demonstrate, both theoretically and empirically, that our
data-dependent modeling technique is consistent, resilient to noise, valid non
asymptotically and asymptotically behaves as an universal approximator of the
true edge density in directed multigraphs. We evaluate our method using
artificial graphs and present its practical interest on real world graphs. The
method is both robust and scalable. It is able to extract insightful patterns
in the unsupervised learning setting and to provide state of the art accuracy
when used as a preparation step for supervised learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01345</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01345</id><created>2015-08-06</created><authors><author><keyname>Korkmaz</keyname><forenames>Fatih</forenames></author><author><keyname>Topalo&#x11f;lu</keyname><forenames>&#x130;smail</forenames></author><author><keyname>Mamur</keyname><forenames>Hayati</forenames></author></authors><title>Fuzzy Logic Based Direct Torque Control Of Induction Motor With Space
  Vector Modulation</title><categories>cs.SY cs.AI</categories><comments>10 pages</comments><doi>10.5121/ijscai.2013.2603</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The induction motors have wide range of applications for due to its
well-known advantages like brushless structures, low costs and robust
performances. Over the past years, many kind of control methods are proposed
for the induction motors and direct torque control has gained huge importance
inside of them due to fast dynamic torque responses and simple control
structures. However, the direct torque control method has still some handicaps
against the other control methods and most of the important of these handicaps
is high torque ripple. This paper suggests a new approach, Fuzzy logic based
space vector modulation, on the direct torque controlled induction motors and
aim of the approach is to overcome high torque ripple disadvantages of
conventional direct torque control. In order to test and compare the proposed
direct torque control method with conventional direct torque control method
simulations, in Matlab/Simulink,have been carried out in different working
conditions. The simulation results showed that a significant improvement in the
dynamic torque and speed responses when compared to the conventional direct
torque control method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01346</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01346</id><created>2015-08-06</created><authors><author><keyname>Pal</keyname><forenames>Alok Ranjan</forenames></author><author><keyname>Saha</keyname><forenames>Diganta</forenames></author></authors><title>Word sense disambiguation: a survey</title><categories>cs.CL</categories><comments>International Journal of Control Theory and Computer Modeling
  (IJCTCM) Vol.5, No.3, July 2015</comments><doi>10.5121/ijctcm.2015.5301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we made a survey on Word Sense Disambiguation (WSD). Near
about in all major languages around the world, research in WSD has been
conducted upto different extents. In this paper, we have gone through a survey
regarding the different approaches adopted in different research works, the
State of the Art in the performance in this domain, recent works in different
Indian languages and finally a survey in Bengali language. We have made a
survey on different competitions in this field and the bench mark results,
obtained from those competitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01349</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01349</id><created>2015-08-06</created><authors><author><keyname>Pal</keyname><forenames>Alok Ranjan</forenames></author><author><keyname>Saha</keyname><forenames>Diganta</forenames></author><author><keyname>Dash</keyname><forenames>Niladri Sekhar</forenames></author></authors><title>Automatic classification of bengali sentences based on sense definitions
  present in bengali wordnet</title><categories>cs.CL</categories><comments>International Journal of Control Theory and Computer Modeling
  (IJCTCM) Vol.5, No.1, January 2015</comments><doi>10.5121/ijctcm.2015.5101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the sense definition of words available in the Bengali WordNet, an
attempt is made to classify the Bengali sentences automatically into different
groups in accordance with their underlying senses. The input sentences are
collected from 50 different categories of the Bengali text corpus developed in
the TDIL project of the Govt. of India, while information about the different
senses of particular ambiguous lexical item is collected from Bengali WordNet.
In an experimental basis we have used Naive Bayes probabilistic model as a
useful classifier of sentences. We have applied the algorithm over 1747
sentences that contain a particular Bengali lexical item which, because of its
ambiguous nature, is able to trigger different senses that render sentences in
different meanings. In our experiment we have achieved around 84% accurate
result on the sense classification over the total input sentences. We have
analyzed those residual sentences that did not comply with our experiment and
did affect the results to note that in many cases, wrong syntactic structures
and less semantic information are the main hurdles in semantic classification
of sentences. The applicational relevance of this study is attested in
automatic text classification, machine learning, information extraction, and
word sense disambiguation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01352</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01352</id><created>2015-08-06</created><authors><author><keyname>Shekhtman</keyname><forenames>Louis</forenames></author><author><keyname>Shai</keyname><forenames>Saray</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author></authors><title>Resilience of Networks Formed of Interdependent Modular Networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1088/1367-2630/17/12/123007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many infrastructure networks have a modular structure and are also
interdependent. While significant research has explored the resilience of
interdependent networks, there has been no analysis of the effects of
modularity. Here we develop a theoretical framework for attacks on
interdependent modular networks and support our results by simulations. We
focus on the case where each network has the same number of communities and the
dependency links are restricted to be between pairs of communities of different
networks. This is very realistic for infrastructure across cities. Each city
has its own infrastructures and different infrastructures are dependent within
the city. However, each infrastructure is connected within and between cities.
For example, a power grid will connect many cities as will a communication
network, yet a power station and communication tower that are interdependent
will likely be in the same city. It has been shown that single networks are
very susceptible to the failure of the interconnected nodes (between
communities) Shai et al. and that attacks on these nodes are more crippling
than attacks based on betweenness da Cunha et al. In our example of cities
these nodes have long range links which are more likely to fail. For both
treelike and looplike interdependent modular networks we find distinct regimes
depending on the number of modules, $m$. (i) In the case where there are fewer
modules with strong intraconnections, the system first separates into modules
in an abrupt first-order transition and then each module undergoes a second
percolation transition. (ii) When there are more modules with many
interconnections between them, the system undergoes a single transition.
Overall, we find that modular structure can influence the type of transitions
observed in interdependent networks and should be considered in attempts to
make interdependent networks more resilient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01354</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01354</id><created>2015-08-06</created><authors><author><keyname>Amelio</keyname><forenames>A.</forenames></author><author><keyname>Pizzuti</keyname><forenames>C.</forenames></author></authors><title>Mining and Analyzing the Italian Parliament: Party Structure and
  Evolution</title><categories>cs.SI cs.CY</categories><comments>27 pages, 14 figures</comments><acm-class>H.2.8</acm-class><journal-ref>Recommendation and Search in Social Networks, Lecture Notes in
  Social Networks 2015, pp. 249-279</journal-ref><doi>10.1007/978-3-319-14379-8_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The roll calls of the Italian Parliament in the XVI legislature are studied
by employing multidimensional scaling, hierarchical clustering, and network
analysis. In order to detect changes in voting behavior, the roll calls have
been divided in seven periods of six months each. All the methods employed
pointed out an increasing fragmentation of the political parties endorsing the
previous government that culminated in its downfall. By using the concept of
modularity at different resolution levels, we identify the community structure
of Parliament and its evolution in each of the considered time periods. The
analysis performed revealed as a valuable tool in detecting trends and drifts
of Parliamentarians. It showed its effectiveness at identifying political
parties and at providing insights on the temporal evolution of groups and their
cohesiveness, without having at disposal any knowledge about political
membership of Representatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01360</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01360</id><created>2015-08-06</created><authors><author><keyname>Anisimov</keyname><forenames>Anatoly V.</forenames></author><author><keyname>Zavadskyi</keyname><forenames>Igor O.</forenames></author></authors><title>Variable-length Splittable Codes with Multiple Delimiters</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variable-length splittable codes are derived from encoding sequences of
ordered integer pairs, where one of the pair's components is upper bounded by
some constant, and the other one is any positive integer. Each pair is encoded
by the concatenation of two fixed independent prefix encoding functions applied
to the corresponding components of a pair. The codeword of such a sequence of
pairs consists of the sequential concatenation of corresponding pair's
encodings. We call such codes splittable. We show that Fibonacci codes of
higher orders and codes with multiple delimiters of the form 011...10 are
splittable. Completeness and universality of multi-delimiter codes are proved.
Encoding of integers by multi-delimiter codes is considered in detail. For
these codes, a fast byte aligned decoding algorithm is constructed. The
comparative compression performance of Fibonacci codes and different
multi-delimiter codes is presented. By many useful properties, multi-delimiter
codes are superior to Fibonacci codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01372</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01372</id><created>2015-08-06</created><authors><author><keyname>M&#xfc;hlenthaler</keyname><forenames>Moritz</forenames></author></authors><title>Degree-constrained Subgraph Reconfiguration is in P</title><categories>cs.DM math.CO</categories><comments>Full version of the paper published at Mathematical Foundations of
  Computer Science (MFCS) 2015</comments><msc-class>68R10 (primary), 05C70, 05C40 (secondary)</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The degree-constrained subgraph problem asks for a subgraph of a given graph
such that the degree of each vertex is within some specified bounds. We study
the following reconfiguration variant of this problem: Given two solutions to a
degree-constrained subgraph instance, can we transform one solution into the
other by adding and removing individual edges, such that each intermediate
subgraph satisfies the degree constraints and contains at least a certain
minimum number of edges? This problem is a generalization of the matching
reconfiguration problem, which is known to be in P. We show that even in the
more general setting the reconfiguration problem is in P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01375</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01375</id><created>2015-08-06</created><updated>2015-09-23</updated><authors><author><keyname>Elias</keyname><forenames>Yara</forenames></author><author><keyname>Lauter</keyname><forenames>Kristin E.</forenames></author><author><keyname>Ozman</keyname><forenames>Ekin</forenames></author><author><keyname>Stange</keyname><forenames>Katherine E.</forenames></author></authors><title>Ring-LWE Cryptography for the Number Theorist</title><categories>math.NT cs.CR</categories><comments>20 Pages</comments><msc-class>11T71, 94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we survey the status of attacks on the ring and polynomial
learning with errors problems (RLWE and PLWE). Recent work on the security of
these problems [Eisentr\&quot;ager-Hallgren-Lauter, Elias-Lauter-Ozman-Stange] gives
rise to interesting questions about number fields. We extend these attacks and
survey related open problems in number theory, including spectral distortion of
an algebraic number and its relationship to Mahler measure, the monogenic
property for the ring of integers of a number field, and the size of elements
of small order modulo q.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01376</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01376</id><created>2015-08-06</created><authors><author><keyname>Zehmakan</keyname><forenames>Abdolahad Noori</forenames></author></authors><title>Bin Packing Problem: Two Approximation Algorithms</title><categories>cs.DS cs.DM</categories><comments>12 pages, 10 figures, 1 table in International Journal in Foundations
  of Computer Science &amp; Technology (IJFCST, July 2015, Volume 5, Number 4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bin Packing Problem is one of the most important optimization problems.
In recent years, due to its NP-hard nature, several approximation algorithms
have been presented. It is proved that the best algorithm for the Bin Packing
Problem has the approximation ratio 3/2 and the time order O(n), unless P=NP.
In this paper, first, a 3/2-approximation algorithm is presented, then a
modification to FFD algorithm is proposed to decrease time order to linear.
Finally, these suggested approximation algorithms are compared with some other
approximation algorithms. The experimental results show the suggested
algorithms perform efficiently. In summary, the main goal of the research is
presenting methods which not only enjoy the best theoretical criteria, but also
perform considerably efficient in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01412</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01412</id><created>2015-08-06</created><authors><author><keyname>McGilvary</keyname><forenames>Gary A.</forenames></author><author><keyname>Atkinson</keyname><forenames>Malcolm</forenames></author><author><keyname>Gesing</keyname><forenames>Sandra</forenames></author><author><keyname>Aguilera</keyname><forenames>Alvaro</forenames></author><author><keyname>Grunzke</keyname><forenames>Richard</forenames></author><author><keyname>Sciacca</keyname><forenames>Eva</forenames></author></authors><title>Enhanced Usability of Managing Workflows in an Industrial Data Gateway</title><categories>cs.DC cs.SE</categories><comments>Proceedings of the 1st International Workshop on Interoperable
  Infrastructures for Interdisciplinary Big Data Sciences, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Grid and Cloud User Support Environment (gUSE) enables users convenient
and easy access to grid and cloud infrastructures by providing a general
purpose, workflow-oriented graphical user interface to create and run workflows
on various Distributed Computing Infrastructures (DCIs). Its arrangements for
creating and modifying existing workflows are, however, non-intuitive and
cumbersome due to the technologies and architecture employed by gUSE. In this
paper, we outline the first integrated web-based workflow editor for gUSE with
the aim of improving the user experience for those with industrial data
workflows and the wider gUSE community. We report initial assessments of the
editor's utility based on users' feedback. We argue that combining access to
diverse scalable resources with improved workflow creation tools is important
for all big data applications and research infrastructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01420</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01420</id><created>2015-08-06</created><authors><author><keyname>Marujo</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Port&#xea;lo</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Ling</keyname><forenames>Wang</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author><author><keyname>Neto</keyname><forenames>Jo&#xe3;o P.</forenames></author><author><keyname>Gershman</keyname><forenames>Anatole</forenames></author><author><keyname>Carbonell</keyname><forenames>Jaime</forenames></author><author><keyname>Trancoso</keyname><forenames>Isabel</forenames></author><author><keyname>Raj</keyname><forenames>Bhiksha</forenames></author></authors><title>Privacy-Preserving Multi-Document Summarization</title><categories>cs.IR cs.CL cs.CR</categories><comments>4 pages, In Proceedings of 2nd ACM SIGIR Workshop on
  Privacy-Preserving Information Retrieval, August 2015. arXiv admin note: text
  overlap with arXiv:1407.5416</comments><acm-class>H.3; I.2.7; K.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art extractive multi-document summarization systems are usually
designed without any concern about privacy issues, meaning that all documents
are open to third parties. In this paper we propose a privacy-preserving
approach to multi-document summarization. Our approach enables other parties to
obtain summaries without learning anything else about the original documents'
content. We use a hashing scheme known as Secure Binary Embeddings to convert
documents representation containing key phrases and bag-of-words into bit
strings, allowing the computation of approximate distances, instead of exact
ones. Our experiments indicate that our system yields similar results to its
non-private counterpart on standard multi-document evaluation datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01430</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01430</id><created>2015-08-06</created><authors><author><keyname>Salam</keyname><forenames>Mohammad Abdus</forenames></author><author><keyname>Sarkodee-Adoo</keyname><forenames>Alfred</forenames></author></authors><title>Referencing Tool for Reputation and Trust in Wireless Sensor Networks</title><categories>cs.NI cs.CR</categories><comments>13 pages</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.7, No.4, pp. 139-151, July 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Presently, there are not many literatures on the characterization of
reputation and trust in wireless sensor networks (WSNs) which can be referenced
by scientists, researchers and students. Although some research documents
include information on reputation and trust, characterization of these features
are not adequately covered. In this paper, reputation and trust are divided
into various classes or categories and a method of referencing the information
is provided. This method used results in providing researchers with a tool that
makes it easier to reference these features on reputation and trust in a much
easier way than if referencing has to be directed to several uncoordinated
resources. Although the outcome of this work proves beneficial to research in
the characterization of reputation and trust in WSNs, more work needs to be
done in extending the benefits to other network systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01441</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01441</id><created>2015-08-06</created><authors><author><keyname>Enright</keyname><forenames>Jessica</forenames></author><author><keyname>Stewart</keyname><forenames>Lorna</forenames></author></authors><title>Equivalence of the filament and overlap graphs of subtrees of limited
  trees</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that subtree overlap graphs are equivalent to subtree filament
graphs. We show the equivalence of subtree filament and subtree overlap classes
of limited host trees, and relate this to the $\mathcal{G}$-mixed
characterisations used by Gavril (2000). As a consequence of this result, we
have that caterpillar overlap graphs are equivalent to interval filament
graphs.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="81000" completeListSize="102538">1122234|82001</resumptionToken>
</ListRecords>
</OAI-PMH>
