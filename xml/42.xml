<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T01:08:56Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|41001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7361</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7361</id><created>2013-01-30</created><updated>2013-04-23</updated><authors><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author><author><keyname>Brafman</keyname><forenames>Ronen I.</forenames></author><author><keyname>Geib</keyname><forenames>Christopher W.</forenames></author></authors><title>Structured Reachability Analysis for Markov Decision Processes</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1998-PG-24-32</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research in decision theoretic planning has focussed on making the
solution of Markov decision processes (MDPs) more feasible. We develop a family
of algorithms for structured reachability analysis of MDPs that are suitable
when an initial state (or set of states) is known. Using compact, structured
representations of MDPs (e.g., Bayesian networks), our methods, which vary in
the tradeoff between complexity and accuracy, produce structured descriptions
of (estimated) reachable states that can be used to eliminate variables or
variable values from the problem description, reducing the size of the MDP and
making it easier to solve. One contribution of our work is the extension of
ideas from GRAPHPLAN to deal with the distributed nature of action
representations typically embodied within Bayes nets and the problem of
correlated action effects. We also demonstrate that our algorithm can be made
more complete by using k-ary constraints instead of binary constraints. Another
contribution is the illustration of how the compact representation of
reachability constraints can be exploited by several existing (exact and
approximate) abstraction algorithms for MDPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7362</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7362</id><created>2013-01-30</created><authors><author><keyname>Boyen</keyname><forenames>Xavier</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Tractable Inference for Complex Stochastic Processes</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-33-42</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The monitoring and control of any dynamic system depends crucially on the
ability to reason about its current status and its future trajectory. In the
case of a stochastic system, these tasks typically involve the use of a belief
state- a probability distribution over the state of the process at a given
point in time. Unfortunately, the state spaces of complex processes are very
large, making an explicit representation of a belief state intractable. Even in
dynamic Bayesian networks (DBNs), where the process itself can be represented
compactly, the representation of the belief state is intractable. We
investigate the idea of maintaining a compact approximation to the true belief
state, and analyze the conditions under which the errors due to the
approximations taken over the lifetime of the process do not accumulate to make
our answers completely irrelevant. We show that the error in a belief state
contracts exponentially as the process evolves. Thus, even with multiple
approximations, the error in our process remains bounded indefinitely. We show
how the additional structure of a DBN can be used to design our approximation
scheme, improving its performance significantly. We demonstrate the
applicability of our ideas in the context of a monitoring task, showing that
orders of magnitude faster inference can be achieved with only a small
degradation in accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7363</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7363</id><created>2013-01-30</created><authors><author><keyname>Breese</keyname><forenames>John S.</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Kadie</keyname><forenames>Carl</forenames></author></authors><title>Empirical Analysis of Predictive Algorithms for Collaborative Filtering</title><categories>cs.IR cs.LG</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-43-52</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative filtering or recommender systems use a database about user
preferences to predict additional topics or products a new user might like. In
this paper we describe several algorithms designed for this task, including
techniques based on correlation coefficients, vector-based similarity
calculations, and statistical Bayesian methods. We compare the predictive
accuracy of the various methods in a set of representative problem domains. We
use two basic classes of evaluation metrics. The first characterizes accuracy
over a set of individual predictions in terms of average absolute deviation.
The second estimates the utility of a ranked list of suggested items. This
metric uses an estimate of the probability that a user will see a
recommendation in an ordered list. Experiments were run for datasets associated
with 3 application areas, 4 experimental protocols, and the 2 evaluation
metrics for the various algorithms. Results indicate that for a wide range of
conditions, Bayesian networks with decision trees at each node and correlation
methods outperform Bayesian-clustering and vector-similarity methods. Between
correlation and Bayesian networks, the preferred method depends on the nature
of the dataset, nature of the application (ranked versus one-by-one
presentation), and the availability of votes with which to make predictions.
Other considerations include the size of database, speed of predictions, and
learning time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7364</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7364</id><created>2013-01-30</created><authors><author><keyname>de Campos</keyname><forenames>Luis M.</forenames></author><author><keyname>Fernandez-Luna</keyname><forenames>Juan M.</forenames></author><author><keyname>Huete</keyname><forenames>Juan F.</forenames></author></authors><title>Query Expansion in Information Retrieval Systems using a Bayesian
  Network-Based Thesaurus</title><categories>cs.IR cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-53-60</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information Retrieval (IR) is concerned with the identification of documents
in a collection that are relevant to a given information need, usually
represented as a query containing terms or keywords, which are supposed to be a
good description of what the user is looking for. IR systems may improve their
effectiveness (i.e., increasing the number of relevant documents retrieved) by
using a process of query expansion, which automatically adds new terms to the
original query posed by an user. In this paper we develop a method of query
expansion based on Bayesian networks. Using a learning algorithm, we construct
a Bayesian network that represents some of the relationships among the terms
appearing in a given document collection; this network is then used as a
thesaurus (specific for that collection). We also report the results obtained
by our method on three standard test collections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7365</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7365</id><created>2013-01-30</created><authors><author><keyname>Castel</keyname><forenames>Charles</forenames></author><author><keyname>Cossart</keyname><forenames>Corine</forenames></author><author><keyname>Tessier</keyname><forenames>Catherine</forenames></author></authors><title>Dealing with Uncertainty in Situation Assessment: towards a Symbolic
  Approach</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-61-68</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The situation assessment problem is considered, in terms of object,
condition, activity, and plan recognition, based on data coming from the
real-word {em via} various sensors. It is shown that uncertainty issues are
linked both to the models and to the matching algorithm. Three different types
of uncertainties are identified, and within each one, the numerical and the
symbolic cases are distinguished. The emphasis is then put on purely symbolic
uncertainties: it is shown that they can be dealt with within a purely symbolic
framework resulting from a transposition of classical numerical estimation
tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7366</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7366</id><created>2013-01-30</created><authors><author><keyname>Castillo</keyname><forenames>Enrique F.</forenames></author><author><keyname>Ferr&#xe1;ndiz</keyname><forenames>Juan</forenames></author><author><keyname>Sanmartin</keyname><forenames>Pilar</forenames></author></authors><title>Marginalizing in Undirected Graph and Hypergraph Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-69-78</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph G or hypergraph X model for a given set of
variables V, we introduce two marginalization operators for obtaining the
undirected graph GA or hypergraph HA associated with a given subset A c V such
that the marginal distribution of A factorizes according to GA or HA,
respectively. Finally, we illustrate the method by its application to some
practical examples. With them we show that hypergraph models allow defining a
finer factorization or performing a more precise conditional independence
analysis than undirected graph models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7367</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7367</id><created>2013-01-30</created><authors><author><keyname>Chajewska</keyname><forenames>Urszula</forenames></author><author><keyname>Getoor</keyname><forenames>Lise</forenames></author><author><keyname>Norman</keyname><forenames>Joseph</forenames></author><author><keyname>Shahar</keyname><forenames>Yuval</forenames></author></authors><title>Utility Elicitation as a Classification Problem</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-79-88</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the application of classification techniques to utility
elicitation. In a decision problem, two sets of parameters must generally be
elicited: the probabilities and the utilities. While the prior and conditional
probabilities in the model do not change from user to user, the utility models
do. Thus it is necessary to elicit a utility model separately for each new
user. Elicitation is long and tedious, particularly if the outcome space is
large and not decomposable. There are two common approaches to utility function
elicitation. The first is to base the determination of the users utility
function solely ON elicitation OF qualitative preferences.The second makes
assumptions about the form AND decomposability OF the utility function.Here we
take a different approach: we attempt TO identify the new USERs utility
function based on classification relative to a database of previously collected
utility functions. We do this by identifying clusters of utility functions that
minimize an appropriate distance measure. Having identified the clusters, we
develop a classification scheme that requires many fewer and simpler
assessments than full utility elicitation and is more robust than utility
elicitation based solely on preferences. We have tested our algorithm on a
small database of utility functions in a prenatal diagnosis domain and the
results are quite promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7368</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7368</id><created>2013-01-30</created><authors><author><keyname>Cozman</keyname><forenames>Fabio Gagliardi</forenames></author></authors><title>Irrelevance and Independence Relations in Quasi-Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-89-96</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes irrelevance and independence relations in graphical
models associated with convex sets of probability distributions (called
Quasi-Bayesian networks). The basic question in Quasi-Bayesian networks is, How
can irrelevance/independence relations in Quasi-Bayesian networks be detected,
enforced and exploited? This paper addresses these questions through Walley's
definitions of irrelevance and independence. Novel algorithms and results are
presented for inferences with the so-called natural extensions using fractional
linear programming, and the properties of the so-called type-1 extensions are
clarified through a new generalization of d-separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7369</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7369</id><created>2013-01-30</created><authors><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>Dynamic Jointrees</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-97-104</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that one can ignore parts of a belief network when computing
answers to certain probabilistic queries. It is also well known that the
ignorable parts (if any) depend on the specific query of interest and,
therefore, may change as the query changes. Algorithms based on jointrees,
however, do not seem to take computational advantage of these facts given that
they typically construct jointrees for worst-case queries; that is, queries for
which every part of the belief network is considered relevant. To address this
limitation, we propose in this paper a method for reconfiguring jointrees
dynamically as the query changes. The reconfiguration process aims at
maintaining a jointree which corresponds to the underlying belief network after
it has been pruned given the current query. Our reconfiguration method is
marked by three characteristics: (a) it is based on a non-classical definition
of jointrees; (b) it is relatively efficient; and (c) it can reuse some of the
computations performed before a jointree is reconfigured. We present
preliminary experimental results which demonstrate significant savings over
using static jointrees when query changes are considerable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7370</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7370</id><created>2013-01-30</created><authors><author><keyname>Desjardins</keyname><forenames>Benoit</forenames></author></authors><title>On the Semi-Markov Equivalence of Causal Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-105-112</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The variability of structure in a finite Markov equivalence class of causally
sufficient models represented by directed acyclic graphs has been fully
characterized. Without causal sufficiency, an infinite semi-Markov equivalence
class of models has only been characterized by the fact that each model in the
equivalence class entails the same marginal statistical dependencies. In this
paper, we study the variability of structure of causal models within a
semi-Markov equivalence class and propose a systematic approach to construct
models entailing any specific marginal statistical dependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7371</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7371</id><created>2013-01-30</created><authors><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Fargier</keyname><forenames>Helene</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>Comparative Uncertainty, Belief Functions and Accepted Beliefs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-113-120</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper relates comparative belief structures and a general view of belief
management in the setting of deductively closed logical representations of
accepted beliefs. We show that the range of compatibility between the classical
deductive closure and uncertain reasoning covers precisely the nonmonotonic
'preferential' inference system of Kraus, Lehmann and Magidor and nothing else.
In terms of uncertain reasoning any possibility or necessity measure gives
birth to a structure of accepted beliefs. The classes of probability functions
and of Shafer's belief functions which yield belief sets prove to be very
special ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7372</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7372</id><created>2013-01-30</created><authors><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author><author><keyname>Sabbadin</keyname><forenames>Regis</forenames></author></authors><title>Qualitative Decision Theory with Sugeno Integrals</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-121-128</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an axiomatic framework for qualitative decision under
uncertainty in a finite setting. The corresponding utility is expressed by a
sup-min expression, called Sugeno (or fuzzy) integral. Technically speaking,
Sugeno integral is a median, which is indeed a qualitative counterpart to the
averaging operation underlying expected utility. The axiomatic justification of
Sugeno integral-based utility is expressed in terms of preference between acts
as in Savage decision theory. Pessimistic and optimistic qualitative utilities,
based on necessity and possibility measures, previously introduced by two of
the authors, can be retrieved in this setting by adding appropriate axioms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7373</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7373</id><created>2013-01-30</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author></authors><title>The Bayesian Structural EM Algorithm</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-129-138</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years there has been a flurry of works on learning Bayesian
networks from data. One of the hard problems in this area is how to effectively
learn the structure of a belief network from incomplete data- that is, in the
presence of missing values or hidden variables. In a recent paper, I introduced
an algorithm called Structural EM that combines the standard Expectation
Maximization (EM) algorithm, which optimizes parameters, with structure search
for model selection. That algorithm learns networks based on penalized
likelihood scores, which include the BIC/MDL score and various approximations
to the Bayesian score. In this paper, I extend Structural EM to deal directly
with Bayesian model selection. I prove the convergence of the resulting
algorithm and show how to apply it for learning a large class of probabilistic
models, including Bayesian networks and some variants thereof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7374</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7374</id><created>2013-01-30</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author><author><keyname>Russell</keyname><forenames>Stuart</forenames></author></authors><title>Learning the Structure of Dynamic Probabilistic Networks</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-139-147</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic probabilistic networks are a compact representation of complex
stochastic processes. In this paper we examine how to learn the structure of a
DPN from data. We extend structure scoring rules for standard probabilistic
networks to the dynamic case, and show how to search for structure when some of
the variables are hidden. Finally, we examine two applications where such a
technology might be useful: predicting and classifying dynamic behaviors, and
learning causal orderings in biological processes. We provide empirical results
that demonstrate the applicability of our methods in both domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7375</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7375</id><created>2013-01-30</created><authors><author><keyname>Gammerman</keyname><forenames>Alex</forenames></author><author><keyname>Vovk</keyname><forenames>Volodya</forenames></author><author><keyname>Vapnik</keyname><forenames>Vladimir</forenames></author></authors><title>Learning by Transduction</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-148-155</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method for predicting a classification of an object given
classifications of the objects in the training set, assuming that the pairs
object/classification are generated by an i.i.d. process from a continuous
probability distribution. Our method is a modification of Vapnik's
support-vector machine; its main novelty is that it gives not only the
prediction itself but also a practicable measure of the evidence found in
support of that prediction. We also describe a procedure for assigning degrees
of confidence to predictions made by the support vector machine. Some
experimental results are presented, and possible extensions of the algorithms
are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7376</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7376</id><created>2013-01-30</created><authors><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author></authors><title>Graphical Models and Exponential Families</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-156-165</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a classification of graphical models according to their
representation as subfamilies of exponential families. Undirected graphical
models with no hidden variables are linear exponential families (LEFs),
directed acyclic graphical models and chain graphs with no hidden variables,
including Bayesian networks with several families of local distributions, are
curved exponential families (CEFs) and graphical models with hidden variables
are stratified exponential families (SEFs). An SEF is a finite union of CEFs
satisfying a frontier condition. In addition, we illustrate how one can
automatically generate independence and non-independence constraints on the
distributions over the observable variables implied by a Bayesian network with
hidden variables. The relevance of these results for model selection is
examined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7377</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7377</id><created>2013-01-30</created><authors><author><keyname>Glymour</keyname><forenames>Clark</forenames></author></authors><title>Psychological and Normative Theories of Causal Power and the
  Probabilities of Causes</title><categories>cs.AI stat.ME</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-166-172</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper (1)shows that the best supported current psychological theory
(Cheng, 1997) of how human subjects judge the causal power or influence of
variations in presence or absence of one feature on another, given data on
their covariation, tacitly uses a Bayes network which is either a noisy or gate
(for causes that promote the effect) or a noisy and gate (for causes that
inhibit the effect); (2)generalizes Chengs theory to arbitrary acyclic networks
of noisy or and noisy and gates; (3)gives various sufficient conditions for the
estimation of the parameters in such networks when there are independent,
unobserved causes; (4)distinguishes direct causal influence of one feature on
another (influence along a path with one edge) from total influence (influence
along all paths from one variable to another) and gives sufficient conditions
for estimating each when there are unobserved causes of the outcome variable;
(5)describes the relation between Cheng models and a simplified version of the
Rubin framework for representing causal relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7378</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7378</id><created>2013-01-30</created><authors><author><keyname>Grunwald</keyname><forenames>Peter D</forenames></author><author><keyname>Kontkanen</keyname><forenames>Petri</forenames></author><author><keyname>Myllymaki</keyname><forenames>Petri</forenames></author><author><keyname>Silander</keyname><forenames>Tomi</forenames></author><author><keyname>Tirri</keyname><forenames>Henry</forenames></author></authors><title>Minimum Encoding Approaches for Predictive Modeling</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-183-192</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze differences between two information-theoretically motivated
approaches to statistical inference and model selection: the Minimum
Description Length (MDL) principle, and the Minimum Message Length (MML)
principle. Based on this analysis, we present two revised versions of MML: a
pointwise estimator which gives the MML-optimal single parameter model, and a
volumewise estimator which gives the MML-optimal region in the parameter space.
Our empirical results suggest that with small data sets, the MDL approach
yields more accurate predictions than the MML estimators. The empirical results
also demonstrate that the revised MML estimators introduced here perform better
than the original MML estimator suggested by Wallace and Freeman.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7379</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7379</id><created>2013-01-30</created><authors><author><keyname>Ha</keyname><forenames>Vu A.</forenames></author><author><keyname>Haddawy</keyname><forenames>Peter</forenames></author></authors><title>Towards Case-Based Preference Elicitation: Similarity Measures on
  Preference Structures</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-193-201</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While decision theory provides an appealing normative framework for
representing rich preference structures, eliciting utility or value functions
typically incurs a large cost. For many applications involving interactive
systems this overhead precludes the use of formal decision-theoretic models of
preference. Instead of performing elicitation in a vacuum, it would be useful
if we could augment directly elicited preferences with some appropriate default
information. In this paper we propose a case-based approach to alleviating the
preference elicitation bottleneck. Assuming the existence of a population of
users from whom we have elicited complete or incomplete preference structures,
we propose eliciting the preferences of a new user interactively and
incrementally, using the closest existing preference structures as potential
defaults. Since a notion of closeness demands a measure of distance among
preference structures, this paper takes the first step of studying various
distance measures over fully and partially specified preference structures. We
explore the use of Euclidean distance, Spearmans footrule, and define a new
measure, the probabilistic distance. We provide computational techniques for
all three measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7380</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7380</id><created>2013-01-30</created><authors><author><keyname>Hansen</keyname><forenames>Eric A.</forenames></author></authors><title>Solving POMDPs by Searching in Policy Space</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-211-219</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most algorithms for solving POMDPs iteratively improve a value function that
implicitly represents a policy and are said to search in value function space.
This paper presents an approach to solving POMDPs that represents a policy
explicitly as a finite-state controller and iteratively improves the controller
by search in policy space. Two related algorithms illustrate this approach. The
first is a policy iteration algorithm that can outperform value iteration in
solving infinitehorizon POMDPs. It provides the foundation for a new heuristic
search algorithm that promises further speedup by focusing computational effort
on regions of the problem space that are reachable, or likely to be reached,
from a start state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7381</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7381</id><created>2013-01-30</created><authors><author><keyname>Hauskrecht</keyname><forenames>Milos</forenames></author><author><keyname>Meuleau</keyname><forenames>Nicolas</forenames></author><author><keyname>Kaelbling</keyname><forenames>Leslie Pack</forenames></author><author><keyname>Dean</keyname><forenames>Thomas L.</forenames></author><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author></authors><title>Hierarchical Solution of Markov Decision Processes using Macro-actions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-220-229</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the use of temporally abstract actions, or macro-actions, in
the solution of Markov decision processes. Unlike current models that combine
both primitive actions and macro-actions and leave the state space unchanged,
we propose a hierarchical model (using an abstract MDP) that works with
macro-actions only, and that significantly reduces the size of the state space.
This is achieved by treating macroactions as local policies that act in certain
regions of state space, and by restricting states in the abstract MDP to those
at the boundaries of regions. The abstract MDP approximates the original and
can be solved more efficiently. We discuss several ways in which macro-actions
can be generated to ensure good solution quality. Finally, we consider ways in
which macro-actions can be reused to solve multiple, related MDPs; and we show
that this can justify the computational overhead of macro-action generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7382</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7382</id><created>2013-01-30</created><updated>2015-05-16</updated><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author></authors><title>Inferring Informational Goals from Free-Text Queries: A Bayesian
  Approach</title><categories>cs.IR cs.AI cs.CL</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1998-PG-230-237</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People using consumer software applications typically do not use technical
jargon when querying an online database of help topics. Rather, they attempt to
communicate their goals with common words and phrases that describe software
functionality in terms of structure and objects they understand. We describe a
Bayesian approach to modeling the relationship between words in a user's query
for assistance and the informational goals of the user. After reviewing the
general method, we describe several extensions that center on integrating
additional distinctions and structure about language usage and user goals into
the Bayesian models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7383</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7383</id><created>2013-01-30</created><authors><author><keyname>Hoos</keyname><forenames>Holger H.</forenames></author><author><keyname>Stutzle</keyname><forenames>Thomas</forenames></author></authors><title>Evaluating Las Vegas Algorithms - Pitfalls and Remedies</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-238-245</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic search algorithms are among the most sucessful approaches for
solving hard combinatorial problems. A large class of stochastic search
approaches can be cast into the framework of Las Vegas Algorithms (LVAs). As
the run-time behavior of LVAs is characterized by random variables, the
detailed knowledge of run-time distributions provides important information for
the analysis of these algorithms. In this paper we propose a novel methodology
for evaluating the performance of LVAs, based on the identification of
empirical run-time distributions. We exemplify our approach by applying it to
Stochastic Local Search (SLS) algorithms for the satisfiability problem (SAT)
in propositional logic. We point out pitfalls arising from the use of improper
empirical methods and discuss the benefits of the proposed methodology for
evaluating and comparing LVAs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7384</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7384</id><created>2013-01-30</created><authors><author><keyname>Horsch</keyname><forenames>Michael C.</forenames></author><author><keyname>Poole</keyname><forenames>David L.</forenames></author></authors><title>An Anytime Algorithm for Decision Making under Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-246-255</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an anytime algorithm which computes policies for decision problems
represented as multi-stage influence diagrams. Our algorithm constructs
policies incrementally, starting from a policy which makes no use of the
available information. The incremental process constructs policies which
includes more of the information available to the decision maker at each step.
While the process converges to the optimal policy, our approach is designed for
situations in which computing the optimal policy is infeasible. We provide
examples of the process on several large decision problems, showing that, for
these examples, the process constructs valuable (but sub-optimal) policies
before the optimal policy would be available by traditional methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7385</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7385</id><created>2013-01-30</created><authors><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author><author><keyname>Breese</keyname><forenames>John S.</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Hovel</keyname><forenames>David</forenames></author><author><keyname>Rommelse</keyname><forenames>Koos</forenames></author></authors><title>The Lumiere Project: Bayesian User Modeling for Inferring the Goals and
  Needs of Software Users</title><categories>cs.AI cs.HC</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-256-265</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Lumiere Project centers on harnessing probability and utility to provide
assistance to computer software users. We review work on Bayesian user models
that can be employed to infer a users needs by considering a user's background,
actions, and queries. Several problems were tackled in Lumiere research,
including (1) the construction of Bayesian models for reasoning about the
time-varying goals of computer users from their observed actions and queries,
(2) gaining access to a stream of events from software applications, (3)
developing a language for transforming system events into observational
variables represented in Bayesian user models, (4) developing persistent
profiles to capture changes in a user expertise, and (5) the development of an
overall architecture for an intelligent user interface. Lumiere prototypes
served as the basis for the Office Assistant in the Microsoft Office '97 suite
of productivity applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7386</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7386</id><created>2013-01-30</created><authors><author><keyname>Ibarguengoytia</keyname><forenames>Pablo H.</forenames></author><author><keyname>Sucar</keyname><forenames>Luis Enrique</forenames></author><author><keyname>Vadera</keyname><forenames>Sunil</forenames></author></authors><title>Any Time Probabilistic Reasoning for Sensor Validation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-266-273</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many real time applications, it is important to validate the information
received from the sensors before entering higher levels of reasoning. This
paper presents an any time probabilistic algorithm for validating the
information provided by sensors. The system consists of two Bayesian network
models. The first one is a model of the dependencies between sensors and it is
used to validate each sensor. It provides a list of potentially faulty sensors.
To isolate the real faults, a second Bayesian network is used, which relates
the potential faults with the real faults. This second model is also used to
make the validation algorithm any time, by validating first the sensors that
provide more information. To select the next sensor to validate, and measure
the quality of the results at each stage, an entropy function is used. This
function captures in a single quantity both the certainty and specificity
measures of any time algorithms. Together, both models constitute a mechanism
for validating sensors in an any time fashion, providing at each step the
probability of correct/faulty for each sensor, and the total quality of the
results. The algorithm has been tested in the validation of temperature sensors
of a power plant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7387</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7387</id><created>2013-01-30</created><authors><author><keyname>Jaeger</keyname><forenames>Manfred</forenames></author></authors><title>Measure Selection: Notions of Rationality and Representation
  Independence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-274-281</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We take another look at the general problem of selecting a preferred
probability measure among those that comply with some given constraints. The
dominant role that entropy maximization has obtained in this context is
questioned by arguing that the minimum information principle on which it is
based could be supplanted by an at least as plausible &quot;likelihood of evidence&quot;
principle. We then review a method for turning given selection functions into
representation independent variants, and discuss the tradeoffs involved in this
transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7388</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7388</id><created>2013-01-30</created><authors><author><keyname>Jaffray</keyname><forenames>Jean-Yves</forenames></author></authors><title>Implementing Resolute Choice Under Uncertainty</title><categories>cs.GT cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-282-288</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The adaptation to situations of sequential choice under uncertainty of
decision criteria which deviate from (subjective) expected utility raises the
problem of ensuring the selection of a nondominated strategy. In particular,
when following the suggestion of Machina and McClennen of giving up
separability (also known as consequentialism), which requires the choice of a
substrategy in a subtree to depend only on data relevant to that subtree, one
must renounce to the use of dynamic programming, since Bellman's principle is
no longer valid. An interpretation of McClennen's resolute choice, based on
cooperation between the successive Selves of the decision maker, is proposed.
Implementations of resolute choice which prevent Money Pumps negative prices of
information or, more generally, choices of dominated strategies, while
remaining computationally tractable, are proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7389</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7389</id><created>2013-01-30</created><authors><author><keyname>Jarkass</keyname><forenames>Iman</forenames></author><author><keyname>Rombaut</keyname><forenames>Michele</forenames></author></authors><title>Dealing with Uncertainty on the Initial State of a Petri Net</title><categories>cs.AI cs.SY</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-289-295</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a method to find the actual state of a complex dynamic
system from information coming from the sensors on the system himself, or on
its environment. The nominal evolution of the system is a priori known and can
be modeled (by an expert, for example), by different methods. In this paper,
the Petri nets have been chosen. Contrary to the usual use of the Petri nets,
the initial state of the system is unknown. So a degree of belief is bound to
each places, or set of places. The theory used to model this uncertainty is the
Dempster-Shafer's one which is well adapted to this type of problems. From the
given Petri net characterizing the nominal evolution of the dynamic system, and
from the observation inputs, the proposed method allows to determine according
to the reliability of the model and the inputs, the state of the system at any
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7390</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7390</id><created>2013-01-30</created><authors><author><keyname>Jiang</keyname><forenames>Wenxin</forenames></author><author><keyname>Tanner</keyname><forenames>Martin A.</forenames></author></authors><title>Hierarchical Mixtures-of-Experts for Exponential Family Regression
  Models with Generalized Linear Mean Functions: A Survey of Approximation and
  Consistency Results</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-296-303</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a class of hierarchical mixtures-of-experts (HME) models where
exponential family regression models with generalized linear mean functions of
the form psi(ga+fx^Tfgb) are mixed. Here psi(...) is the inverse link function.
Suppose the true response y follows an exponential family regression model with
mean function belonging to a class of smooth functions of the form psi(h(fx))
where h(...)in W_2^infty (a Sobolev class over [0,1]^{s}). It is shown that the
HME probability density functions can approximate the true density, at a rate
of O(m^{-2/s}) in L_p norm, and at a rate of O(m^{-4/s}) in Kullback-Leibler
divergence. These rates can be achieved within the family of HME structures
with no more than s-layers, where s is the dimension of the predictor fx. It is
also shown that likelihood-based inference based on HME is consistent in
recovering the truth, in the sense that as the sample size n and the number of
experts m both increase, the mean square error of the predicted mean response
goes to zero. Conditions for such results to hold are stated and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7391</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7391</id><created>2013-01-30</created><authors><author><keyname>Kearns</keyname><forenames>Michael</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author></authors><title>Exact Inference of Hidden Structure from Sample Data in Noisy-OR
  Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-304-310</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the literature on graphical models, there has been increased attention
paid to the problems of learning hidden structure (see Heckerman [H96] for
survey) and causal mechanisms from sample data [H96, P88, S93, P95, F98]. In
most settings we should expect the former to be difficult, and the latter
potentially impossible without experimental intervention. In this work, we
examine some restricted settings in which perfectly reconstruct the hidden
structure solely on the basis of observed sample data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7392</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7392</id><created>2013-01-30</created><authors><author><keyname>Kearns</keyname><forenames>Michael</forenames></author><author><keyname>Saul</keyname><forenames>Lawrence</forenames></author></authors><title>Large Deviation Methods for Approximate Probabilistic Inference</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-311-319</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study two-layer belief networks of binary random variables in which the
conditional probabilities Pr[childlparents] depend monotonically on weighted
sums of the parents. In large networks where exact probabilistic inference is
intractable, we show how to compute upper and lower bounds on many
probabilities of interest. In particular, using methods from large deviation
theory, we derive rigorous bounds on marginal probabilities such as
Pr[children] and prove rates of convergence for the accuracy of our bounds as a
function of network size. Our results apply to networks with generic transfer
function parameterizations of the conditional probability tables, such as
sigmoid and noisy-OR. They also explicitly illustrate the types of averaging
behavior that can simplify the problem of inference in large networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7393</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7393</id><created>2013-01-30</created><authors><author><keyname>Lawrence</keyname><forenames>Neil D.</forenames></author><author><keyname>Bishop</keyname><forenames>Christopher M.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Mixture Representations for Inference and Learning in Boltzmann Machines</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-320-327</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boltzmann machines are undirected graphical models with two-state stochastic
variables, in which the logarithms of the clique potentials are quadratic
functions of the node states. They have been widely studied in the neural
computing literature, although their practical applicability has been limited
by the difficulty of finding an effective learning algorithm. One
well-established approach, known as mean field theory, represents the
stochastic distribution using a factorized approximation. However, the
corresponding learning algorithm often fails to find a good solution. We
conjecture that this is due to the implicit uni-modality of the mean field
approximation which is therefore unable to capture multi-modality in the true
distribution. In this paper we use variational methods to approximate the
stochastic distribution using multi-modal mixtures of factorized distributions.
We present results for both inference and learning to demonstrate the
effectiveness of this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7394</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7394</id><created>2013-01-30</created><authors><author><keyname>Lepar</keyname><forenames>Vasilica</forenames></author><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author></authors><title>A Comparison of Lauritzen-Spiegelhalter, Hugin, and Shenoy-Shafer
  Architectures for Computing Marginals of Probability Distributions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-328-337</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decade, several architectures have been proposed for exact
computation of marginals using local computation. In this paper, we compare
three architectures - Lauritzen-Spiegelhalter, Hugin, and Shenoy-Shafer - from
the perspective of graphical structure for message propagation, message-passing
scheme, computational efficiency, and storage efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7395</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7395</id><created>2013-01-30</created><authors><author><keyname>Liu</keyname><forenames>Chao-Lin</forenames></author><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author></authors><title>Incremental Tradeoff Resolution in Qualitative Probabilistic Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-338-345</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Qualitative probabilistic reasoning in a Bayesian network often reveals
tradeoffs: relationships that are ambiguous due to competing qualitative
influences. We present two techniques that combine qualitative and numeric
probabilistic reasoning to resolve such tradeoffs, inferring the qualitative
relationship between nodes in a Bayesian network. The first approach
incrementally marginalizes nodes that contribute to the ambiguous qualitative
relationships. The second approach evaluates approximate Bayesian networks for
bounds of probability distributions, and uses these bounds to determinate
qualitative relationships in question. This approach is also incremental in
that the algorithm refines the state spaces of random variables for tighter
bounds until the qualitative relationships are resolved. Both approaches
provide systematic methods for tradeoff resolution at potentially lower
computational cost than application of purely numeric methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7396</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7396</id><created>2013-01-30</created><authors><author><keyname>Liu</keyname><forenames>Chao-Lin</forenames></author><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author></authors><title>Using Qualitative Relationships for Bounding Probability Distributions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-346-353</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We exploit qualitative probabilistic relationships among variables for
computing bounds of conditional probability distributions of interest in
Bayesian networks. Using the signs of qualitative relationships, we can
implement abstraction operations that are guaranteed to bound the distributions
of interest in the desired direction. By evaluating incrementally improved
approximate networks, our algorithm obtains monotonically tightening bounds
that converge to exact distributions. For supermodular utility functions, the
tightening bounds monotonically reduce the set of admissible decision
alternatives as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7397</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7397</id><created>2013-01-30</created><authors><author><keyname>Lukasiewicz</keyname><forenames>Thomas</forenames></author></authors><title>Magic Inference Rules for Probabilistic Deduction under Taxonomic
  Knowledge</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-354-361</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present locally complete inference rules for probabilistic deduction from
taxonomic and probabilistic knowledge-bases over conjunctive events. Crucially,
in contrast to similar inference rules in the literature, our inference rules
are locally complete for conjunctive events and under additional taxonomic
knowledge. We discover that our inference rules are extremely complex and that
it is at first glance not clear at all where the deduced tightest bounds come
from. Moreover, analyzing the global completeness of our inference rules, we
find examples of globally very incomplete probabilistic deductions. More
generally, we even show that all systems of inference rules for taxonomic and
probabilistic knowledge-bases over conjunctive events are globally incomplete.
We conclude that probabilistic deduction by the iterative application of
inference rules on interval restrictions for conditional probabilities, even
though considered very promising in the literature so far, seems very limited
in its field of application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7398</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7398</id><created>2013-01-30</created><authors><author><keyname>Madsen</keyname><forenames>Anders L.</forenames></author><author><keyname>Jensen</keyname><forenames>Finn Verner</forenames></author></authors><title>Lazy Propagation in Junction Trees</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-362-369</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The efficiency of algorithms using secondary structures for probabilistic
inference in Bayesian networks can be improved by exploiting independence
relations induced by evidence and the direction of the links in the original
network. In this paper we present an algorithm that on-line exploits
independence relations induced by evidence and the direction of the links in
the original network to reduce both time and space costs. Instead of
multiplying the conditional probability distributions for the various cliques,
we determine on-line which potentials to multiply when a message is to be
produced. The performance improvement of the algorithm is emphasized through
empirical evaluations involving large real world Bayesian networks, and we
compare the method with the HUGIN and Shafer-Shenoy inference algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7399</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7399</id><created>2013-01-30</created><authors><author><keyname>Mahoney</keyname><forenames>Suzanne M.</forenames></author><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author></authors><title>Constructing Situation Specific Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-370-378</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a process for constructing situation-specific belief
networks from a knowledge base of network fragments. A situation-specific
network is a minimal query complete network constructed from a knowledge base
in response to a query for the probability distribution on a set of target
variables given evidence and context variables. We present definitions of query
completeness and situation-specific networks. We describe conditions on the
knowledge base that guarantee query completeness. The relationship of our work
to earlier work on KBMC is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7401</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7401</id><created>2013-01-30</created><updated>2015-05-16</updated><authors><author><keyname>Meila</keyname><forenames>Marina</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>An Experimental Comparison of Several Clustering and Initialization
  Methods</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1998-PG-386-395</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine methods for clustering in high dimensions. In the first part of
the paper, we perform an experimental comparison between three batch clustering
algorithms: the Expectation-Maximization (EM) algorithm, a winner take all
version of the EM algorithm reminiscent of the K-means algorithm, and
model-based hierarchical agglomerative clustering. We learn naive-Bayes models
with a hidden root node, using high-dimensional discrete-variable data sets
(both real and synthetic). We find that the EM algorithm significantly
outperforms the other methods, and proceed to investigate the effect of various
initialization schemes on the final solution produced by the EM algorithm. The
initializations that we consider are (1) parameters sampled from an
uninformative prior, (2) random perturbations of the marginal distribution of
the data, and (3) the output of hierarchical agglomerative clustering. Although
the methods are substantially different, they lead to learned models that are
strikingly similar in quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7402</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7402</id><created>2013-01-30</created><authors><author><keyname>Monney</keyname><forenames>Paul-Andre</forenames></author></authors><title>From Likelihood to Plausibility</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-396-403</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several authors have explained that the likelihood ratio measures the
strength of the evidence represented by observations in statistical problems.
This idea works fine when the goal is to evaluate the strength of the available
evidence for a simple hypothesis versus another simple hypothesis. However, the
applicability of this idea is limited to simple hypotheses because the
likelihood function is primarily defined on points (simple hypotheses) of the
parameter space. In this paper we define a general weight of evidence that is
applicable to both simple and composite hypotheses. It is based on the
Dempster-Shafer concept of plausibility and is shown to be a generalization of
the likelihood ratio. Functional models are of a fundamental importance for the
general weight of evidence proposed in this paper. The relevant concepts and
ideas are explained by means of a familiar urn problem and the general analysis
of a real-world medical problem is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7403</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7403</id><created>2013-01-30</created><authors><author><keyname>Monti</keyname><forenames>Stefano</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>A Multivariate Discretization Method for Learning Bayesian Networks from
  Mixed Data</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-404-413</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of discretization in the context of
learning Bayesian networks (BNs) from data containing both continuous and
discrete variables. We describe a new technique for &lt;EM&gt;multivariate&lt;/EM&gt;
discretization, whereby each continuous variable is discretized while taking
into account its interaction with the other variables. The technique is based
on the use of a Bayesian scoring metric that scores the discretization policy
for a continuous variable given a BN structure and the observed data. Since the
metric is relative to the BN structure currently being evaluated, the
discretization of a variable needs to be dynamically adjusted as the BN
structure changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7404</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7404</id><created>2013-01-30</created><authors><author><keyname>Ng</keyname><forenames>Benson Hin Kwong</forenames></author><author><keyname>Wong</keyname><forenames>Kam-Fai</forenames></author><author><keyname>Low</keyname><forenames>Boon-Toh</forenames></author></authors><title>Resolving Conflicting Arguments under Uncertainties</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-414-421</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed knowledge based applications in open domain rely on common sense
information which is bound to be uncertain and incomplete. To draw the useful
conclusions from ambiguous data, one must address uncertainties and conflicts
incurred in a holistic view. No integrated frameworks are viable without an
in-depth analysis of conflicts incurred by uncertainties. In this paper, we
give such an analysis and based on the result, propose an integrated framework.
Our framework extends definite argumentation theory to model uncertainty. It
supports three views over conflicting and uncertain knowledge. Thus, knowledge
engineers can draw different conclusions depending on the application context
(i.e. view). We also give an illustrative example on strategical decision
support to show the practical usefulness of our framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7405</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7405</id><created>2013-01-30</created><authors><author><keyname>Parr</keyname><forenames>Ron</forenames></author></authors><title>Flexible Decomposition Algorithms for Weakly Coupled Markov Decision
  Problems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-422-430</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents two new approaches to decomposing and solving large
Markov decision problems (MDPs), a partial decoupling method and a complete
decoupling method. In these approaches, a large, stochastic decision problem is
divided into smaller pieces. The first approach builds a cache of policies for
each part of the problem independently, and then combines the pieces in a
separate, light-weight step. A second approach also divides the problem into
smaller pieces, but information is communicated between the different problem
pieces, allowing intelligent decisions to be made about which piece requires
the most attention. Both approaches can be used to find optimal policies or
approximately optimal policies with provable bounds. These algorithms also
provide a framework for the efficient transfer of knowledge across problems
that share similar structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7406</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7406</id><created>2013-01-30</created><authors><author><keyname>Pennock</keyname><forenames>David M.</forenames></author></authors><title>Logarithmic Time Parallel Bayesian Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-431-438</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I present a parallel algorithm for exact probabilistic inference in Bayesian
networks. For polytree networks with n variables, the worst-case time
complexity is O(log n) on a CREW PRAM (concurrent-read, exclusive-write
parallel random-access machine) with n processors, for any constant number of
evidence variables. For arbitrary networks, the time complexity is O(r^{3w}*log
n) for n processors, or O(w*log n) for r^{3w}*n processors, where r is the
maximum range of any variable, and w is the induced width (the maximum clique
size), after moralizing and triangulating the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7407</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7407</id><created>2013-01-30</created><authors><author><keyname>Peot</keyname><forenames>Mark Alan</forenames></author><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>Learning From What You Don't Observe</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-439-446</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process of diagnosis involves learning about the state of a system from
various observations of symptoms or findings about the system. Sophisticated
Bayesian (and other) algorithms have been developed to revise and maintain
beliefs about the system as observations are made. Nonetheless, diagnostic
models have tended to ignore some common sense reasoning exploited by human
diagnosticians; In particular, one can learn from which observations have not
been made, in the spirit of conversational implicature. There are two concepts
that we describe to extract information from the observations not made. First,
some symptoms, if present, are more likely to be reported before others.
Second, most human diagnosticians and expert systems are economical in their
data-gathering, searching first where they are more likely to find symptoms
present. Thus, there is a desirable bias toward reporting symptoms that are
present. We develop a simple model for these concepts that can significantly
improve diagnostic inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7408</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7408</id><created>2013-01-30</created><authors><author><keyname>Poole</keyname><forenames>David L.</forenames></author></authors><title>Context-Specific Approximation in Probabilistic Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-447-454</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is evidence that the numbers in probabilistic inference don't really
matter. This paper considers the idea that we can make a probabilistic model
simpler by making fewer distinctions. Unfortunately, the level of a Bayesian
network seems too coarse; it is unlikely that a parent will make little
difference for all values of the other parents. In this paper we consider an
approximation scheme where distinctions can be ignored in some contexts, but
not in other contexts. We elaborate on a notion of a parent context that allows
a structured context-specific decomposition of a probability distribution and
the associated probabilistic inference scheme called probabilistic partial
evaluation (Poole 1997). This paper shows a way to simplify a probabilistic
model by ignoring distinctions which have similar probabilities, a method to
exploit the simpler model, a bound on the resulting errors, and some
preliminary empirical results on simple networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7409</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7409</id><created>2013-01-30</created><authors><author><keyname>Rish</keyname><forenames>Irina</forenames></author><author><keyname>Kask</keyname><forenames>Kalev</forenames></author><author><keyname>Dechter</keyname><forenames>Rina</forenames></author></authors><title>Empirical Evaluation of Approximation Algorithms for Probabilistic
  Decoding</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-455-463</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was recently shown that the problem of decoding messages transmitted
through a noisy channel can be formulated as a belief updating task over a
probabilistic network [McEliece]. Moreover, it was observed that iterative
application of the (linear time) Pearl's belief propagation algorithm designed
for polytrees outperformed state of the art decoding algorithms, even though
the corresponding networks may have many cycles. This paper demonstrates
empirically that an approximation algorithm approx-mpe for solving the most
probable explanation (MPE) problem, developed within the recently proposed
mini-bucket elimination framework [Dechter96], outperforms iterative belief
propagation on classes of coding networks that have bounded induced width. Our
experiments suggest that approximate MPE decoders can be good competitors to
the approximate belief updating decoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7410</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7410</id><created>2013-01-30</created><authors><author><keyname>Sebastiani</keyname><forenames>Paola</forenames></author><author><keyname>Ramoni</keyname><forenames>Marco</forenames></author></authors><title>Decision Theoretic Foundations of Graphical Model Selection</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-464-471</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a decision theoretic formulation of learning the
graphical structure of a Bayesian Belief Network from data. This framework
subsumes the standard Bayesian approach of choosing the model with the largest
posterior probability as the solution of a decision problem with a 0-1 loss
function and allows the use of more general loss functions able to trade-off
the complexity of the selected model and the error of choosing an
oversimplified model. A new class of loss functions, called disintegrable, is
introduced, to allow the decision problem to match the decomposability of the
graphical model. With this class of loss functions, the optimal solution to the
decision problem can be found using an efficient bottom-up search strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7411</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7411</id><created>2013-01-30</created><authors><author><keyname>Settimi</keyname><forenames>Raffaella</forenames></author><author><keyname>Smith</keyname><forenames>Jim Q.</forenames></author></authors><title>On the Geometry of Bayesian Graphical Models with Hidden Variables</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-472-479</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the geometry of the likelihood of the unknown
parameters in a simple class of Bayesian directed graphs with hidden variables.
This enables us, before any numerical algorithms are employed, to obtain
certain insights in the nature of the unidentifiability inherent in such
models, the way posterior densities will be sensitive to prior densities and
the typical geometrical form these posterior densities might take. Many of
these insights carry over into more complicated Bayesian networks with
systematic missing data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7412</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7412</id><created>2013-01-30</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>Bayes-Ball: The Rational Pastime (for Determining Irrelevance and
  Requisite Information in Belief Networks and Influence Diagrams)</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-480-487</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the benefits of belief networks and influence diagrams is that so much
knowledge is captured in the graphical structure. In particular, statements of
conditional irrelevance (or independence) can be verified in time linear in the
size of the graph. To resolve a particular inference query or decision problem,
only some of the possible states and probability distributions must be
specified, the &quot;requisite information.&quot;
  This paper presents a new, simple, and efficient &quot;Bayes-ball&quot; algorithm which
is well-suited to both new students of belief networks and state of the art
implementations. The Bayes-ball algorithm determines irrelevant sets and
requisite information more efficiently than existing methods, and is linear in
the size of the graph for belief networks and influence diagrams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7413</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7413</id><created>2013-01-30</created><authors><author><keyname>Singer</keyname><forenames>Yoram</forenames></author></authors><title>Switching Portfolios</title><categories>q-fin.PM cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-488-495</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A constant rebalanced portfolio is an asset allocation algorithm which keeps
the same distribution of wealth among a set of assets along a period of time.
Recently, there has been work on on-line portfolio selection algorithms which
are competitive with the best constant rebalanced portfolio determined in
hindsight. By their nature, these algorithms employ the assumption that high
returns can be achieved using a fixed asset allocation strategy. However, stock
markets are far from being stationary and in many cases the wealth achieved by
a constant rebalanced portfolio is much smaller than the wealth achieved by an
ad-hoc investment strategy that adapts to changes in the market. In this paper
we present an efficient Bayesian portfolio selection algorithm that is able to
track a changing market. We also describe a simple extension of the algorithm
for the case of a general transaction cost, including the transactions cost
models recently investigated by Blum and kalai. We provide a simple analysis of
the competitiveness of the algorithm and check its performance on real stock
data from the New York Stock Exchange accumulated during a 22-year period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7414</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7414</id><created>2013-01-30</created><authors><author><keyname>Studeny</keyname><forenames>Milan</forenames></author></authors><title>Bayesian Networks from the Point of View of Chain Graphs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-496-503</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  AThe paper gives a few arguments in favour of the use of chain graphs for
description of probabilistic conditional independence structures. Every
Bayesian network model can be equivalently introduced by means of a
factorization formula with respect to a chain graph which is Markov equivalent
to the Bayesian network. A graphical characterization of such graphs is given.
The class of equivalent graphs can be represented by a distinguished graph
which is called the largest chain graph. The factorization formula with respect
to the largest chain graph is a basis of a proposal of how to represent the
corresponding (discrete) probability distribution in a computer (i.e.
parametrize it). This way does not depend on the choice of a particular
Bayesian network from the class of equivalent networks and seems to be the most
efficient way from the point of view of memory demands. A separation criterion
for reading independency statements from a chain graph is formulated in a
simpler way. It resembles the well-known d-separation criterion for Bayesian
networks and can be implemented locally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7415</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7415</id><created>2013-01-30</created><updated>2015-05-16</updated><authors><author><keyname>Thiesson</keyname><forenames>Bo</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author><author><keyname>Chickering</keyname><forenames>David Maxwell</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Learning Mixtures of DAG Models</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1998-PG-504-513</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe computationally efficient methods for learning mixtures in which
each component is a directed acyclic graphical model (mixtures of DAGs or
MDAGs). We argue that simple search-and-score algorithms are infeasible for a
variety of problems, and introduce a feasible approach in which parameter and
structure search is interleaved and expected data is treated as real data. Our
approach can be viewed as a combination of (1) the Cheeseman--Stutz asymptotic
approximation for model posterior probability and (2) the
Expectation--Maximization algorithm. We evaluate our procedure for selecting
among MDAGs on synthetic and real examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7416</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7416</id><created>2013-01-30</created><authors><author><keyname>Zhang</keyname><forenames>Nevin Lianwen</forenames></author></authors><title>Probabilistic Inference in Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-514-522</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is about reducing influence diagram (ID) evaluation into Bayesian
network (BN) inference problems. Such reduction is interesting because it
enables one to readily use one's favorite BN inference algorithm to efficiently
evaluate IDs. Two such reduction methods have been proposed previously (Cooper
1988, Shachter and Peot 1992). This paper proposes a new method. The BN
inference problems induced by the mew method are much easier to solve than
those induced by the two previous methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7417</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7417</id><created>2013-01-30</created><authors><author><keyname>Zhang</keyname><forenames>Nevin Lianwen</forenames></author><author><keyname>Lee</keyname><forenames>Stephen S.</forenames></author></authors><title>Planning with Partially Observable Markov Decision Processes: Advances
  in Exact Solution Method</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-523-530</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is much interest in using partially observable Markov decision
processes (POMDPs) as a formal model for planning in stochastic domains. This
paper is concerned with finding optimal policies for POMDPs. We propose several
improvements to incremental pruning, presently the most efficient exact
algorithm for solving POMDPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7418</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7418</id><created>2013-01-30</created><authors><author><keyname>Zhang</keyname><forenames>Weixiong</forenames></author></authors><title>Flexible and Approximate Computation through State-Space Reduction</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-531-538</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the real world, insufficient information, limited computation resources,
and complex problem structures often force an autonomous agent to make a
decision in time less than that required to solve the problem at hand
completely. Flexible and approximate computations are two approaches to
decision making under limited computation resources. Flexible computation helps
an agent to flexibly allocate limited computation resources so that the overall
system utility is maximized. Approximate computation enables an agent to find
the best satisfactory solution within a deadline. In this paper, we present two
state-space reduction methods for flexible and approximate computation:
quantitative reduction to deal with inaccurate heuristic information, and
structural reduction to handle complex problem structures. These two methods
can be applied successively to continuously improve solution quality if more
computation is available. Our results show that these reduction methods are
effective and efficient, finding better solutions with less computation than
some existing well-known methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7443</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7443</id><created>2013-01-27</created><authors><author><keyname>Schaer</keyname><forenames>Philipp</forenames></author><author><keyname>Lueke</keyname><forenames>Thomas</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author><author><keyname>Mutschke</keyname><forenames>Peter</forenames></author></authors><title>An OAI-PMH-based Web Service for the Generation of Co-Author Networks</title><categories>cs.DL</categories><comments>2 pages, 1 figure, 13th International Symposium Of Information
  Science 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We will present a new component of our technical framework that was built to
provide a brought range of reusable web services for the enhancement of typical
scientific retrieval processes. The proposed component computes betweenness of
authors in co-authorship networks extracted from publicly available metadata
that was harvested using OAI-PMH.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7455</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7455</id><created>2013-01-30</created><authors><author><keyname>Gionis</keyname><forenames>Aristides</forenames></author><author><keyname>Terzi</keyname><forenames>Evimaria</forenames></author><author><keyname>Tsaparas</keyname><forenames>Panayiotis</forenames></author></authors><title>Opinion Maximization in Social Networks</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Siam International Conference on Data Mining (SDM), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process of opinion formation through synthesis and contrast of different
viewpoints has been the subject of many studies in economics and social
sciences. Today, this process manifests itself also in online social networks
and social media. The key characteristic of successful promotion campaigns is
that they take into consideration such opinion-formation dynamics in order to
create a overall favorable opinion about a specific information item, such as a
person, a product, or an idea.
  In this paper, we adopt a well-established model for social-opinion dynamics
and formalize the campaign-design problem as the problem of identifying a set
of target individuals whose positive opinion about an information item will
maximize the overall positive opinion for the item in the social network. We
call this problem CAMPAIGN. We study the complexity of the CAMPAIGN problem,
and design algorithms for solving it. Our experiments on real data demonstrate
the efficiency and practical utility of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7457</identifier>
 <datestamp>2013-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7457</id><created>2013-01-30</created><authors><author><keyname>Srivastava</keyname><forenames>Madhur</forenames></author></authors><title>Introduction of Intellectual Property Courses in STEM Curriculum</title><categories>cs.CY</categories><comments>4 pages, 1 figure, 2013 Integrated STEM Education Conference (ISEC)</comments><doi>10.1109/ISECon.2013.6525228</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper explains and emphasizes the need to include mandatory Intellectual
Property (IP) courses in the undergraduate and graduate curriculum of STEM
students. It prescribes the proposed content of IP to be covered at
undergraduate level, and graduate depending on the degree pursued by the STEM
student. More importantly, this paper advocates for an Integrated PhD/JD degree
to produce highly skilled patent attorneys and agents, to overcome the problem
of poor quality of patents, and the high number of patent related litigations.
A framework is suggested which would enable STEM departments and law schools to
successfully offer this integrated degree, and students to successfully pursue
and complete this degree program in a realistic manner; without compromising
any stake holder's interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7462</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7462</id><created>2013-01-30</created><authors><author><keyname>Alkassar</keyname><forenames>Eyad</forenames></author><author><keyname>B&#xf6;hme</keyname><forenames>Sascha</forenames></author><author><keyname>Mehlhorn</keyname><forenames>Kurt</forenames></author><author><keyname>Rizkallah</keyname><forenames>Christine</forenames></author></authors><title>A Framework for the Verification of Certifying Computations</title><categories>cs.LO cs.DS cs.FL</categories><comments>A preliminary version appeared under the title &quot;Verification of
  Certifying Computations&quot; in CAV 2011, LCNS Vol 6806, pages 67 - 82. This
  paper is currently under review in the Journal of Automated Reasoning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal verification of complex algorithms is challenging. Verifying their
implementations goes beyond the state of the art of current automatic
verification tools and usually involves intricate mathematical theorems.
Certifying algorithms compute in addition to each output a witness certifying
that the output is correct. A checker for such a witness is usually much
simpler than the original algorithm - yet it is all the user has to trust. The
verification of checkers is feasible with current tools and leads to
computations that can be completely trusted. We describe a framework to
seamlessly verify certifying computations. We use the automatic verifier VCC
for establishing the correctness of the checker and the interactive theorem
prover Isabelle/HOL for high-level mathematical properties of algorithms. We
demonstrate the effectiveness of our approach by presenting the verification of
typical examples of the industrial-level and widespread algorithmic library
LEDA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7464</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7464</id><created>2013-01-30</created><updated>2013-02-25</updated><authors><author><keyname>Chen</keyname><forenames>Tsung-Yi</forenames></author><author><keyname>Williamson</keyname><forenames>Adam R.</forenames></author><author><keyname>Wesel</keyname><forenames>Richard D.</forenames></author></authors><title>Variable-Length Coding with Feedback: Finite-Length Codewords and
  Periodic Decoding</title><categories>cs.IT math.IT</categories><comments>8 pages. A shorten version is submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theoretical analysis has long indicated that feedback improves the error
exponent but not the capacity of single-user memoryless channels. Recently
Polyanskiy et al. studied the benefit of variable-length feedback with
termination (VLFT) codes in the non-asymptotic regime. In that work,
achievability is based on an infinite length random code and decoding is
attempted at every symbol. The coding rate backoff from capacity due to channel
dispersion is greatly reduced with feedback, allowing capacity to be approached
with surprisingly small expected latency. This paper is mainly concerned with
VLFT codes based on finite-length codes and decoding attempts only at certain
specified decoding times. The penalties of using a finite block-length $N$ and
a sequence of specified decoding times are studied. This paper shows that
properly scaling $N$ with the expected latency can achieve the same performance
up to constant terms as with $N = \infty$. The penalty introduced by periodic
decoding times is a linear term of the interval between decoding times and
hence the performance approaches capacity as the expected latency grows if the
interval between decoding times grows sub-linearly with the expected latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7465</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7465</id><created>2013-01-30</created><updated>2015-04-15</updated><authors><author><keyname>Peretz</keyname><forenames>Ron</forenames></author></authors><title>Effective Martingales with Restricted Wagers</title><categories>math.LO cs.LO</categories><msc-class>68Q30 (Primary), 91A20 (Secondary)</msc-class><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classic model of computable randomness considers martingales that take
real or rational values. Recent work by Bienvenu et al. (2012) and Teutsch
(2014) shows that fundamental features of the classic model change when the
martingales take integer values.
  We compare the prediction power of martingales whose wagers belong to three
different subsets of rational numbers: (a) all rational numbers, (b) rational
numbers excluding a punctured neighbourhood of 0, and (c) integers. We also
consider three different success criteria: (i) accumulating an infinite amount
of money, (ii) consuming an infinite amount of money, and (iii) making the
accumulated capital oscillate.
  The nine combinations of (a)--(c) and (i)--(iii) define nine notions of
computable randomness. We provide a complete characterization of the relations
between these notions, and show that they form five linearly ordered classes.
  Our results solve outstanding questions raised in Bienvenu et al. (2012),
Teutsch (2014), and Chalcraft et al. (2012), and strengthen existing results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7471</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7471</id><created>2013-01-30</created><authors><author><keyname>Klosik</keyname><forenames>David F.</forenames></author><author><keyname>Bornholdt</keyname><forenames>Stefan</forenames></author></authors><title>The citation wake of publications detects Nobel laureates' papers</title><categories>physics.soc-ph cs.DL</categories><comments>11 pages, 3 figures</comments><doi>10.1371/journal.pone.0113184</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For several decades, a leading paradigm of how to quantitatively assess
scientific research has been the analysis of the aggregated citation
information in a set of scientific publications. Although the representation of
this information as a citation network has already been coined in the 1960s, it
needed the systematic indexing of scientific literature to allow for impact
metrics that actually made use of this network as a whole improving on the then
prevailing metrics that were almost exclusively based on the number of direct
citations. However, besides focusing on the assignment of credit, the paper
citation network can also be studied in terms of the proliferation of
scientific ideas. Here we introduce a simple measure based on the
shortest-paths in the paper's in-component or, simply speaking, on the shape
and size of the wake of a paper within the citation network. Applied to a
citation network containing Physical Review publications from more than a
century, our approach is able to detect seminal articles which have introduced
concepts of obvious importance to the further development of physics. We
observe a large fraction of papers co-authored by Nobel Prize laureates in
physics among the top-ranked publications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7473</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7473</id><created>2013-01-30</created><updated>2013-03-27</updated><authors><author><keyname>Martius</keyname><forenames>Georg</forenames></author><author><keyname>Der</keyname><forenames>Ralf</forenames></author><author><keyname>Ay</keyname><forenames>Nihat</forenames></author></authors><title>Information driven self-organization of complex robotic behaviors</title><categories>cs.RO cs.IT cs.LG math.IT</categories><comments>29 pages, 12 figures</comments><msc-class>94A15, 94A17, 37N35, 68T05, 68T40</msc-class><acm-class>I.2.9; H.1.1; I.2.6</acm-class><journal-ref>PLoS ONE 8(5): e63400</journal-ref><doi>10.1371/journal.pone.0063400</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Information theory is a powerful tool to express principles to drive
autonomous systems because it is domain invariant and allows for an intuitive
interpretation. This paper studies the use of the predictive information (PI),
also called excess entropy or effective measure complexity, of the sensorimotor
process as a driving force to generate behavior. We study nonlinear and
nonstationary systems and introduce the time-local predicting information
(TiPI) which allows us to derive exact results together with explicit update
rules for the parameters of the controller in the dynamical systems framework.
In this way the information principle, formulated at the level of behavior, is
translated to the dynamics of the synapses. We underpin our results with a
number of case studies with high-dimensional robotic systems. We show the
spontaneous cooperativity in a complex physical system with decentralized
control. Moreover, a jointly controlled humanoid robot develops a high
behavioral variety depending on its physics and the environment it is
dynamically embedded into. The behavior can be decomposed into a succession of
low-dimensional modes that increasingly explore the behavior space. This is a
promising way to avoid the curse of dimensionality which hinders learning
systems to scale well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7482</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7482</id><created>2013-01-30</created><authors><author><keyname>Jones</keyname><forenames>Austin</forenames></author><author><keyname>Schwager</keyname><forenames>Mac</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Technical Report: A Receding Horizon Algorithm for Informative Path
  Planning with Temporal Logic Constraints</title><categories>cs.RO</categories><comments>Extended version of paper accepted to 2013 IEEE International
  Conference on Robotics and Automation (ICRA)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical report is an extended version of the paper 'A Receding Horizon
Algorithm for Informative Path Planning with Temporal Logic Constraints'
accepted to the 2013 IEEE International Conference on Robotics and Automation
(ICRA). This paper considers the problem of finding the most informative path
for a sensing robot under temporal logic constraints, a richer set of
constraints than have previously been considered in information gathering. An
algorithm for informative path planning is presented that leverages tools from
information theory and formal control synthesis, and is proven to give a path
that satisfies the given temporal logic constraints. The algorithm uses a
receding horizon approach in order to provide a reactive, on-line solution
while mitigating computational complexity. Statistics compiled from multiple
simulation studies indicate that this algorithm performs better than a baseline
exhaustive search approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7491</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7491</id><created>2013-01-30</created><authors><author><keyname>Mahdavifar</keyname><forenames>Hessam</forenames></author><author><keyname>El-Khamy</keyname><forenames>Mostafa</forenames></author><author><keyname>Lee</keyname><forenames>Jungwon</forenames></author><author><keyname>Kang</keyname><forenames>Inyup</forenames></author></authors><title>On the Construction and Decoding of Concatenated Polar Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A scheme for concatenating the recently invented polar codes with interleaved
block codes is considered. By concatenating binary polar codes with interleaved
Reed-Solomon codes, we prove that the proposed concatenation scheme captures
the capacity-achieving property of polar codes, while having a significantly
better error-decay rate. We show that for any $\epsilon &gt; 0$, and total frame
length $N$, the parameters of the scheme can be set such that the frame error
probability is less than $2^{-N^{1-\epsilon}}$, while the scheme is still
capacity achieving. This improves upon $2^{-N^{0.5-\eps}}$, the frame error
probability of Arikan's polar codes. We also propose decoding algorithms for
concatenated polar codes, which significantly improve the error-rate
performance at finite block lengths while preserving the low decoding
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7496</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7496</id><created>2013-01-30</created><authors><author><keyname>Nguyen</keyname><forenames>Huy</forenames></author><author><keyname>Scalosub</keyname><forenames>Gabriel</forenames></author><author><keyname>Zheng</keyname><forenames>Rong</forenames></author></authors><title>On Quality of Monitoring for Multi-channel Wireless Infrastructure
  Networks</title><categories>cs.NI</categories><comments>Accepted for publication in IEEE TMC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Passive monitoring utilizing distributed wireless sniffers is an effective
technique to monitor activities in wireless infrastructure networks for fault
diagnosis, resource management and critical path analysis. In this paper, we
introduce a quality of monitoring (QoM) metric defined by the expected number
of active users monitored, and investigate the problem of maximizing QoM by
judiciously assigning sniffers to channels based on the knowledge of user
activities in a multi-channel wireless network. Two types of capture models are
considered. The user-centric model assumes frame-level capturing capability of
sniffers such that the activities of different users can be distinguished while
the sniffer-centric model only utilizes the binary channel information (active
or not) at a sniffer. For the user-centric model, we show that the implied
optimization problem is NP-hard, but a constant approximation ratio can be
attained via polynomial complexity algorithms. For the sniffer-centric model,
we devise stochastic inference schemes to transform the problem into the
user-centric domain, where we are able to apply our polynomial approximation
algorithms. The effectiveness of our proposed schemes and algorithms is further
evaluated using both synthetic data as well as real-world traces from an
operational WLAN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7503</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7503</id><created>2013-01-30</created><authors><author><keyname>Yugawa</keyname><forenames>Daichi</forenames></author><author><keyname>Wadayama</keyname><forenames>Tadashi</forenames></author></authors><title>Finite Length Analysis on Listing Failure Probability of Invertible
  Bloom Lookup Tables</title><categories>cs.IT math.IT</categories><comments>5pages, Submitted to ISIT 2013</comments><doi>10.1587/transfun.E97.A.2309</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Invertible Bloom Lookup Tables (IBLT) is a data structure which supports
insertion, deletion, retrieval and listing operations of the key-value pair.
The IBLT can be used to realize efficient set reconciliation for database
synchronization. The most notable feature of the IBLT is the complete listing
operation of the key-value pairs based on the algorithm similar to the peeling
algorithm for low-density generator-matrix (LDGM) codes. In this paper, we will
present a stopping set (SS) analysis for the IBLT which reveals finite length
behaviors of the listing failure probability. The key of the analysis is
enumeration of the number of stopping matrices of given size. We derived a
novel recursive formula useful for computationally efficient enumeration. An
upper bound on the listing failure probability based on the union bound
accurately captures the error floor behaviors. It will be shown that, in the
error floor region, the dominant SS have size 2. We propose a simple
modification on hash functions, which are called SS avoiding hash functions,
for preventing occurrences of the SS of size 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7504</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7504</id><created>2013-01-30</created><updated>2013-07-16</updated><authors><author><keyname>Sason</keyname><forenames>Igal</forenames></author></authors><title>Improved Lower Bounds on the Total Variation Distance for the Poisson
  Approximation</title><categories>cs.IT math.IT</categories><comments>To appear in the Statistics and Probability Letters, final version:
  July 16, 2013. This work was presented in part at the 2013 Information Theory
  and Applications (ITA) Workshop in San-Diego, February 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New lower bounds on the total variation distance between the distribution of
a sum of independent Bernoulli random variables and the Poisson random variable
(with the same mean) are derived via the Chen-Stein method. The new bounds rely
on a non-trivial modification of the analysis by Barbour and Hall (1984) which
surprisingly gives a significant improvement. A use of the new lower bounds is
addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7506</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7506</id><created>2013-01-30</created><authors><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Zhu</keyname><forenames>Jia</forenames></author><author><keyname>Zheng</keyname><forenames>Baoyu</forenames></author></authors><title>A Fully Distributed Opportunistic Network Coding Scheme for Cellular
  Relay Networks</title><categories>cs.IT math.IT</categories><comments>in Proceedings of the 2013 IEEE Wireless Communications and
  Networking Conference (IEEE WCNC 2013), Shanghai China, Apr. 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an opportunistic network coding (ONC) scheme in
cellular relay networks, which operates depending on whether the relay decodes
source messages successfully or not. A fully distributed method is presented to
implement the proposed opportunistic network coding scheme without the need of
any feedback between two network nodes. We consider the use of proposed ONC for
cellular downlink transmissions and derive its closed-form outage probability
expression considering cochannel interference in a Rayleigh fading environment.
Numerical results show that the proposed ONC scheme outperforms the traditional
non-cooperation in terms of outage probability. We also develop the
diversity-multiplexing tradeoff (DMT) of proposed ONC and show that the ONC
scheme obtains the full diversity and an increased multiplexing gain as
compared with the conventional cooperation protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7512</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7512</id><created>2013-01-30</created><updated>2014-03-06</updated><authors><author><keyname>Chen</keyname><forenames>Danny Z.</forenames></author><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Wang</keyname><forenames>Haitao</forenames></author></authors><title>Efficient Algorithms for One-Dimensional k-Center Problems</title><categories>cs.CG cs.DS</categories><comments>13 pages, 3 figures. Thanks to Amir Tamir, discussions on previous
  work are updated in this version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding k centers for n weighted points on a real
line. This (weighted) k-center problem was solved in O(n log n) time previously
by using Cole's parametric search and other complicated approaches. In this
paper, we present an easier O(n log n) time algorithm that avoids the
parametric search, and in certain special cases our algorithm solves the
problem in O(n) time. In addition, our techniques involve developing
interesting data structures for processing queries that find a lowest point in
the common intersection of a certain subset of half-planes. This subproblem is
interesting in its own right and our solution for it may find other
applications as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7515</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7515</id><created>2013-01-31</created><authors><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Zhu</keyname><forenames>Jia</forenames></author><author><keyname>Zheng</keyname><forenames>Baoyu</forenames></author></authors><title>Energy Efficiency of Network Cooperation for Cellular Uplink
  Transmissions</title><categories>cs.IT math.IT</categories><comments>in Proceedings of the 2013 IEEE International Conference on
  Communications (IEEE ICC 2013), Budapest, Hungary, June 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a growing interest in energy efficient or so-called &quot;green&quot; wireless
communication to reduce the energy consumption in cellular networks. Since
today's wireless terminals are typically equipped with multiple network access
interfaces such as Bluetooth, Wi-Fi, and cellular networks, this paper
investigates user terminals cooperating with each other in transmitting their
data packets to a base station (BS) by exploiting the multiple network access
interfaces, referred to as inter-network cooperation, to improve the energy
efficiency in cellular uplink transmission. Given target outage probability and
data rate requirements, we develop a closed-form expression of energy
efficiency in Bits-per-Joule for the inter-network cooperation by taking into
account the path loss, fading, and thermal noise effects. Numerical results
show that when the cooperating users move towards to each other, the proposed
inter-network cooperation significantly improves the energy efficiency as
compared with the traditional non-cooperation and intra-network cooperation.
This implies that given a certain amount of bits to be transmitted, the
inter-network cooperation requires less energy than the traditional
non-cooperation and intra-network cooperation, showing the energy saving
benefit of inter-network cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7517</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7517</id><created>2013-01-31</created><authors><author><keyname>Chanda</keyname><forenames>Abhishek</forenames></author><author><keyname>Westphal</keyname><forenames>Cedric</forenames></author><author><keyname>Raychaudhuri</keyname><forenames>Dipankar</forenames></author></authors><title>Content Based Traffic Engineering in Software Defined Information
  Centric Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a content centric network architecture which uses
software defined networking principles to implement efficient metadata driven
services by extracting content metadata at the network layer. The ability to
access content metadata transparently enables a number of new services in the
network. Specific examples discussed here include: a metadata driven traffic
engineering scheme which uses prior knowledge of content length to optimize
content delivery, a metadata driven content firewall which is more resilient
than traditional firewalls and differentiated treatment of content based on the
type of content being accessed. A detailed outline of an implementation of the
proposed architecture is presented along with some basic evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7519</identifier>
 <datestamp>2013-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7519</id><created>2013-01-31</created><updated>2013-04-26</updated><authors><author><keyname>Wadayama</keyname><forenames>Tadashi</forenames></author></authors><title>Non-Adaptive Group Testing based on Sparse Pooling Graphs</title><categories>cs.IT math.IT</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an information theoretic analysis on non-adaptive group
testing schemes based on sparse pooling graphs is presented. The binary status
of the objects to be tested are modeled by i.i.d. Bernoulli random variables
with probability p. An (l, r, n)-regular pooling graph is a bipartite graph
with left node degree l and right node degree r, where n is the number of left
nodes. Two scenarios are considered: a noiseless setting and a noisy one. The
main contributions of this paper are direct part theorems that give conditions
for the existence of an estimator achieving arbitrary small estimation error
probability. The direct part theorems are proved by averaging an upper bound on
estimation error probability of the typical set estimator over an (l,r,
n)-regular pooling graph ensemble. Numerical results indicate sharp threshold
behaviors in the asymptotic regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7521</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7521</id><created>2013-01-31</created><updated>2013-03-01</updated><authors><author><keyname>Husainov</keyname><forenames>Ahmet A.</forenames></author><author><keyname>Bushmeleva</keyname><forenames>E. S.</forenames></author><author><keyname>Trishina</keyname><forenames>T. A.</forenames></author></authors><title>Homology Groups of Pipeline Petri Nets</title><categories>cs.LO math.AT</categories><comments>13 pages</comments><msc-class>68Q10, 68Q85, 18G10, 18G35, 55U10</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We study homology groups of elementary Petri nets for the pipeline systems.
We show that the integral homology groups of these nets in dimensions 0 and 1
equal the group of integers, and they are zero in other dimensions. We prove
that directed homology groups of elementary Petri nets are zero in all
dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7530</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7530</id><created>2013-01-31</created><authors><author><keyname>Gosselet</keyname><forenames>Pierre</forenames><affiliation>LMT</affiliation></author><author><keyname>Rey</keyname><forenames>Christian</forenames><affiliation>LMT</affiliation></author><author><keyname>Pebrel</keyname><forenames>Julien</forenames><affiliation>LMT</affiliation></author></authors><title>Total and selective reuse of Krylov subspaces for the resolution of
  sequences of nonlinear structural problems</title><categories>math.NA cs.NA</categories><comments>International Journal for Numerical Methods in Engineering (2013) 24
  pages</comments><proxy>ccsd</proxy><doi>10.1002/nme.4441</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the definition and optimization of augmentation spaces
for faster convergence of the conjugate gradient method in the resolution of
sequences of linear systems. Using advanced convergence results from the
literature, we present a procedure based on a selection of relevant
approximations of the eigenspaces for extracting, selecting and reusing
information from the Krylov subspaces generated by previous solutions in order
to accelerate the current iteration. Assessments of the method are proposed in
the cases of both linear and nonlinear structural problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7531</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7531</id><created>2013-01-31</created><authors><author><keyname>Abid</keyname><forenames>Nouha</forenames><affiliation>LAAS</affiliation></author><author><keyname>Zilio</keyname><forenames>Silvano Dal</forenames><affiliation>LAAS</affiliation></author><author><keyname>Botlan</keyname><forenames>Didier Le</forenames><affiliation>LAAS</affiliation></author></authors><title>A Verified Approach for Checking Real-Time Specification Patterns</title><categories>cs.LO</categories><comments>An extended version of this paper appears as Research Report LAAS No.
  11365, June 2011. VECoS 2012, 6th International Workshop on Verification and
  Evaluation of Computer and Communication Systems, France (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a verified approach to the formal verification of timed properties
using model-checking techniques. We focus on properties expressed using
real-time specification patterns, which can be viewed as a subset of timed
temporal logics that includes properties commonly found during the analysis of
reactive systems. Our model-checking approach is based on the use of observers
in order to transform the verification of timed patterns into the verification
of simpler LTL formulas. While the use of observers for model-checking is quite
common, our contribution is original in several ways. First, we define a formal
framework to verify that our observers are correct and non-intrusive. Second,
we define different classes of observers for each pattern and use a pragmatic
approach in order to select the most efficient candidate in practice. This
approach is implemented in an integrated verification tool chain for the Fiacre
language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7533</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7533</id><created>2013-01-31</created><authors><author><keyname>Saad</keyname><forenames>Rodrigo Tacla</forenames><affiliation>LAAS</affiliation></author><author><keyname>Zilio</keyname><forenames>Silvano Dal</forenames><affiliation>LAAS</affiliation></author><author><keyname>Berthomieu</keyname><forenames>Bernard</forenames><affiliation>LAAS</affiliation></author></authors><title>An Experiment on Parallel Model Checking of a CTL Fragment</title><categories>cs.LO cs.DC</categories><comments>10th International Symposium, ATVA 2012, Automated Technology for
  Verification and Analysis, Thiruvananthapuram : India (2012)</comments><proxy>ccsd</proxy><report-no>Rapport LAAS n&amp;deg; 12400</report-no><doi>10.1007/978-3-642-33386-6_23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a parallel algorithm for local, on the fly, model checking of a
fragment of CTL that is well-suited for modern, multi-core architectures. This
model-checking algorithm takes bene t from a parallel state space construction
algorithm, which we described in a previous work, and shares the same basic set
of principles: there are no assumptions on the models that can be analyzed; no
restrictions on the way states are distributed; and no restrictions on the way
work is shared among processors. We evaluate the performance of diff erent
versions of our algorithm and compare our results with those obtained using
other parallel model checking tools. One of the most novel contributions of
this work is to study a space-e fficient variant for CTL model-checking that
does not require to store the whole transition graph but that operates on a
reverse spanning tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7534</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7534</id><created>2013-01-31</created><authors><author><keyname>Abid</keyname><forenames>Nouha</forenames><affiliation>LAAS</affiliation></author><author><keyname>Zilio</keyname><forenames>Silvano Dal</forenames><affiliation>LAAS</affiliation></author><author><keyname>Botlan</keyname><forenames>Didier Le</forenames><affiliation>LAAS</affiliation></author></authors><title>Real-Time Specification Patterns and Tools</title><categories>cs.SE</categories><comments>An extended version of this paper appears as Research Report LAAS No.
  11364, June 2011. 17th International Workshop on Formal Methods for
  Industrial Critical Systems, FMICS 2012, Paris : France (2012)</comments><proxy>ccsd</proxy><doi>10.1007/978-3-642-32469-7_1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An issue limiting the adoption of model checking technologies by the industry
is the ability, for non-experts, to express their requirements using the
property languages supported by verification tools. This has motivated the
definition of dedicated assertion languages for expressing temporal properties
at a higher level. However, only a limited number of these formalisms support
the definition of timing constraints. In this paper, we propose a set of
specification patterns that can be used to express real-time requirements
commonly found in the design of reactive systems. We also provide an integrated
model checking tool chain for the verification of timed requirements on TTS, an
extension of Timed Petri Nets with data variables and priorities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7542</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7542</id><created>2013-01-31</created><authors><author><keyname>Fujii</keyname><forenames>Yuki</forenames></author><author><keyname>Wadayama</keyname><forenames>Tadashi</forenames></author></authors><title>An Analysis on Minimum s-t Cut Capacity of Random Graphs with Specified
  Degree Distribution</title><categories>cs.IT math.IT</categories><comments>5 pages, Submitted to ISIT 2013. arXiv admin note: substantial text
  overlap with arXiv:1202.0876</comments><doi>10.1587/transfun.E97.A.2317</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity (or maximum flow) of an unicast network is known to be equal to
the minimum s-t cut capacity due to the max-flow min-cut theorem. If the
topology of a network (or link capacities) is dynamically changing or unknown,
it is not so trivial to predict statistical properties on the maximum flow of
the network. In this paper, we present a probabilistic analysis for evaluating
the accumulate distribution of the minimum s-t cut capacity on random graphs.
The graph ensemble treated in this paper consists of weighted graphs with
arbitrary specified degree distribution. The main contribution of our work is a
lower bound for the accumulate distribution of the minimum s-t cut capacity.
From some computer experiments, it is observed that the lower bound derived
here reflects the actual statistical behavior of the minimum s-t cut capacity
of random graphs with specified degrees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7564</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7564</id><created>2013-01-31</created><authors><author><keyname>Kova&#x10d;evi&#x107;</keyname><forenames>Mladen</forenames></author><author><keyname>Vukobratovi&#x107;</keyname><forenames>Dejan</forenames></author></authors><title>Multiset Codes for Permutation Channels</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><msc-class>94B60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the notion of multiset codes as relevant to the problem
of reliable information transmission over permutation channels. The motivation
for studying permutation channels comes from the effect of out of order
delivery of packets in some types of packet networks. The proposed codes are a
generalization of the so-called subset codes, recently proposed by the authors.
Some of the basic properties of multiset codes are established, among which
their equivalence to integer codes under the Manhattan metric. The presented
coding-theoretic framework follows closely the one proposed by Koetter and
Kschischang for the operator channels. The two mathematical models are similar
in many respects, and the basic idea is presented in a way which admits a
unified view on coding for these types of channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7566</identifier>
 <datestamp>2013-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7566</id><created>2013-01-31</created><updated>2013-05-07</updated><authors><author><keyname>Sonee</keyname><forenames>Amir</forenames></author><author><keyname>Hodtani</keyname><forenames>Ghosheh Abed</forenames></author></authors><title>On the Capacity of Special Classes of Gaussian Relay Networks with
  Orthogonal Components and Noncausal State Information at Source</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to an error in
  derivation of a relation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study relay networks with orthogonal components in presence
of noncausal channel state information (CSI) available at the source. We
propose an upper bound on the capacity of the discrete memoryless model (DM)for
the case in which just the source component intended for the destination is
encoded against the CSI known non-causally at the source. Also, we derive
capacity for two special classes of the Gaussian structure of the model. The
first class is the one for which we have obtained the upper bound and the
second class is the one in which all of the source components intended for the
relays and destination are encoded against the noncausal CSI, however, no
interference at the relays and destination exists in this case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7592</identifier>
 <datestamp>2013-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7592</id><created>2013-01-31</created><updated>2013-05-28</updated><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author><author><keyname>Markakis</keyname><forenames>Evangelos</forenames></author><author><keyname>Simon</keyname><forenames>Sunil</forenames></author></authors><title>Paradoxes in Social Networks with Multiple Products</title><categories>cs.GT cs.SI</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, we introduced in arXiv:1105.2434 a model for product adoption in
social networks with multiple products, where the agents, influenced by their
neighbours, can adopt one out of several alternatives. We identify and analyze
here four types of paradoxes that can arise in these networks. To this end, we
use social network games that we recently introduced in arxiv:1202.2209. These
paradoxes shed light on possible inefficiencies arising when one modifies the
sets of products available to the agents forming a social network. One of the
paradoxes corresponds to the well-known Braess paradox in congestion games and
shows that by adding more choices to a node, the network may end up in a
situation that is worse for everybody. We exhibit a dual version of this, where
removing available choices from someone can eventually make everybody better
off. The other paradoxes that we identify show that by adding or removing a
product from the choice set of some node may lead to permanent instability.
Finally, we also identify conditions under which some of these paradoxes cannot
arise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7597</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7597</id><created>2013-01-31</created><updated>2014-01-04</updated><authors><author><keyname>Kita</keyname><forenames>Nanao</forenames></author></authors><title>The Third Proof of Lov\'asz's Cathedral Theorem</title><categories>math.CO cs.DM</categories><comments>22 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ with a perfect matching is called saturated if $G+e$ has more
perfect matchings than $G$ for any edge $e$ that is not in $G$. Lov\'asz gave a
characterization of the saturated graphs called the cathedral theorem, with
some applications to the enumeration problem of perfect matchings, and later
Szigeti gave another proof. In this paper, we give a new proof with our
preceding works which revealed canonical structures of general graphs with
perfect matchings. Here, the cathedral theorem is derived in quite a natural
way, providing more refined or generalized properties. Moreover, the new proof
shows that it can be proved without using the Gallai-Edmonds structure theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7602</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7602</id><created>2013-01-31</created><updated>2013-03-10</updated><authors><author><keyname>Lin</keyname><forenames>Min Chih</forenames></author><author><keyname>Mizrahi</keyname><forenames>Michel J.</forenames></author><author><keyname>Szwarcfiter</keyname><forenames>Jayme L.</forenames></author></authors><title>Exact algorithms for dominating induced matchings</title><categories>cs.DM math.CO</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Say that an edge of a graph G dominates itself and every other edge adjacent
to it. An edge dominating set of a graph G = (V,E) is a subset of edges E' of E
which dominates all edges of G. In particular, if every edge of G is dominated
by exactly one edge of E' then E' is a dominating induced matching. It is known
that not every graph admits a dominating induced matching, while the problem to
decide if it does admit is NP-complete. In this paper we consider the problem
of finding a minimum weighted dominating induced matching, if any, of a graph
with weighted edges. We describe two exact algorithms for general graphs. The
algorithms are efficient in the cases where G admits a known vertex dominating
set of small size, or when G contains a polynomial number of maximal
independent sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7619</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7619</id><created>2013-01-31</created><authors><author><keyname>Bazerque</keyname><forenames>Juan Andres</forenames></author><author><keyname>Mateos</keyname><forenames>Gonzalo</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Rank regularization and Bayesian inference for tensor completion and
  extrapolation</title><categories>cs.IT cs.LG math.IT stat.ML</categories><comments>12 pages, submitted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2013.2278516</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel regularizer of the PARAFAC decomposition factors capturing the
tensor's rank is proposed in this paper, as the key enabler for completion of
three-way data arrays with missing entries. Set in a Bayesian framework, the
tensor completion method incorporates prior information to enhance its
smoothing and prediction capabilities. This probabilistic approach can
naturally accommodate general models for the data distribution, lending itself
to various fitting criteria that yield optimum estimates in the
maximum-a-posteriori sense. In particular, two algorithms are devised for
Gaussian- and Poisson-distributed data, that minimize the rank-regularized
least-squares error and Kullback-Leibler divergence, respectively. The proposed
technique is able to recover the &quot;ground-truth'' tensor rank when tested on
synthetic data, and to complete brain imaging and yeast gene expression
datasets with 50% and 15% of missing entries respectively, resulting in
recovery errors at -10dB and -15dB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7627</identifier>
 <datestamp>2013-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7627</id><created>2013-01-31</created><authors><author><keyname>Mateos</keyname><forenames>Gonzalo</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Load curve data cleansing and imputation via sparsity and low rank</title><categories>math.OC cs.IT cs.SY math.IT</categories><comments>8 figures, submitted to IEEE Transactions on Smart Grid - Special
  issue on &quot;Optimization methods and algorithms applied to smart grid&quot;</comments><doi>10.1109/TSG.2013.2259853</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The smart grid vision is to build an intelligent power network with an
unprecedented level of situational awareness and controllability over its
services and infrastructure. This paper advocates statistical inference methods
to robustify power monitoring tasks against the outlier effects owing to faulty
readings and malicious attacks, as well as against missing data due to privacy
concerns and communication errors. In this context, a novel load cleansing and
imputation scheme is developed leveraging the low intrinsic-dimensionality of
spatiotemporal load profiles and the sparse nature of &quot;bad data.'' A robust
estimator based on principal components pursuit (PCP) is adopted, which effects
a twofold sparsity-promoting regularization through an $\ell_1$-norm of the
outliers, and the nuclear norm of the nominal load profiles. Upon recasting the
non-separable nuclear norm into a form amenable to decentralized optimization,
a distributed (D-) PCP algorithm is developed to carry out the imputation and
cleansing tasks using networked devices comprising the so-termed advanced
metering infrastructure. If D-PCP converges and a qualification inequality is
satisfied, the novel distributed estimator provably attains the performance of
its centralized PCP counterpart, which has access to all networkwide data.
Computer simulations and tests with real load curve data corroborate the
convergence and effectiveness of the novel D-PCP algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7630</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7630</id><created>2013-01-31</created><authors><author><keyname>Dong</keyname><forenames>Yunquan</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author></authors><title>An Extended Fano's Inequality for the Finite Blocklength Coding</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures, submitted to IEEE ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fano's inequality reveals the relation between the conditional entropy and
the probability of error . It has been the key tool in proving the converse of
coding theorems in the past sixty years. In this paper, an extended Fano's
inequality is proposed, which is tighter and more applicable for codings in the
finite blocklength regime. Lower bounds on the mutual information and an upper
bound on the codebook size are also given, which are shown to be tighter than
the original Fano's inequality. Especially, the extended Fano's inequality is
tight for some symmetric channels such as the $q$-ary symmetric channels (QSC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7641</identifier>
 <datestamp>2013-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7641</id><created>2013-01-31</created><updated>2013-06-06</updated><authors><author><keyname>Ngo</keyname><forenames>Anh Cat Le</forenames></author><author><keyname>Ang</keyname><forenames>Kenneth Li-Minn</forenames></author><author><keyname>Qiu</keyname><forenames>Guoping</forenames></author><author><keyname>Seng</keyname><forenames>Jasmine Kah-Phooi</forenames></author></authors><title>Multi-scale Discriminant Saliency with Wavelet-based Hidden Markov Tree
  Modelling</title><categories>cs.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1301.3964</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bottom-up saliency, an early stage of humans' visual attention, can be
considered as a binary classification problem between centre and surround
classes. Discriminant power of features for the classification is measured as
mutual information between distributions of image features and corresponding
classes . As the estimated discrepancy very much depends on considered scale
level, multi-scale structure and discriminant power are integrated by employing
discrete wavelet features and Hidden Markov Tree (HMT). With wavelet
coefficients and Hidden Markov Tree parameters, quad-tree like label structures
are constructed and utilized in maximum a posterior probability (MAP) of hidden
class variables at corresponding dyadic sub-squares. Then, a saliency value for
each square block at each scale level is computed with discriminant power
principle. Finally, across multiple scales is integrated the final saliency map
by an information maximization rule. Both standard quantitative tools such as
NSS, LCC, AUC and qualitative assessments are used for evaluating the proposed
multi-scale discriminant saliency (MDIS) method against the well-know
information based approach AIM on its released image collection with
eye-tracking data. Simulation results are presented and analysed to verify the
validity of MDIS as well as point out its limitation for further research
direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7643</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7643</id><created>2013-01-31</created><updated>2013-02-07</updated><authors><author><keyname>Halevi</keyname><forenames>Tzipora</forenames></author><author><keyname>Lewis</keyname><forenames>Jim</forenames></author><author><keyname>Memon</keyname><forenames>Nasir</forenames></author></authors><title>Phishing, Personality Traits and Facebook</title><categories>cs.HC cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phishing attacks have become an increasing threat to online users. Recent
research has begun to focus on the factors that cause people to respond to
them. Our study examines the correlation between the Big Five personality
traits and email phishing response. We also examine how these factors affect
users behavior on Facebook, including posting personal information and choosing
Facebook privacy settings.
  Our research shows that when using a prize phishing email, we find a strong
correlation between gender and the response to the phishing email. In addition,
we find that the neuroticism is the factor most correlated to responding to
this email. Our study also found that people who score high on the openness
factor tend to both post more information on Facebook as well as have less
strict privacy settings, which may cause them to be susceptible to privacy
attacks. In addition, our work detected no correlation between the participants
estimate of being vulnerable to phishing attacks and actually being phished,
which suggests susceptibility to phishing is not due to lack of awareness of
the phishing risks and that realtime response to phishing is hard to predict in
advance by online users.
  We believe that better understanding of the traits which contribute to online
vulnerability can help develop methods for increasing users privacy and
security in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7657</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7657</id><created>2013-01-31</created><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Lo</keyname><forenames>Ernest S.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Energy-Efficient Power Allocation in OFDM Systems with Wireless
  Information and Power Transfer</title><categories>cs.IT math.IT</categories><comments>6 pages, Accepted for presentation at the IEEE International
  Conference on Communications (ICC) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers an orthogonal frequency division multiplexing (OFDM)
downlink point-to-point system with simultaneous wireless information and power
transfer. It is assumed that the receiver is able to harvest energy from noise,
interference, and the desired signals.
  We study the design of power allocation algorithms maximizing the energy
efficiency of data transmission (bit/Joule delivered to the receiver). In
particular, the algorithm design is formulated as a high-dimensional non-convex
optimization problem which takes into account the circuit power consumption,
the minimum required data rate, and a constraint on the minimum power delivered
to the receiver. Subsequently, by exploiting the properties of nonlinear
fractional programming, the considered non-convex optimization problem, whose
objective function is in fractional form, is transformed into an equivalent
optimization problem having an objective function in subtractive form, which
enables the derivation of an efficient iterative power allocation algorithm. In
each iteration, the optimal power allocation solution is derived based on dual
decomposition and a one-dimensional search. Simulation results illustrate that
the proposed iterative power allocation algorithm converges to the optimal
solution, and unveil the trade-off between energy efficiency, system capacity,
and wireless power transfer: (1) In the low transmit power regime, maximizing
the system capacity may maximize the energy efficiency. (2) Wireless power
transfer can enhance the energy efficiency, especially in the interference
limited regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7661</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7661</id><created>2013-01-31</created><authors><author><keyname>Ngo</keyname><forenames>Anh Cat Le</forenames></author><author><keyname>Qiu</keyname><forenames>Guoping</forenames></author><author><keyname>Underwood</keyname><forenames>Geoff</forenames></author><author><keyname>Ang</keyname><forenames>Kenneth Li-Minn</forenames></author><author><keyname>Seng</keyname><forenames>Jasmine Kah-Phooi</forenames></author></authors><title>Fast non parametric entropy estimation for spatial-temporal saliency
  method</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper formulates bottom-up visual saliency as center surround
conditional entropy and presents a fast and efficient technique for the
computation of such a saliency map. It is shown that the new saliency
formulation is consistent with self-information based saliency,
decision-theoretic saliency and Bayesian definition of surprises but also faces
the same significant computational challenge of estimating probability density
in very high dimensional spaces with limited samples. We have developed a fast
and efficient nonparametric method to make the practical implementation of
these types of saliency maps possible. By aligning pixels from the center and
surround regions and treating their location coordinates as random variables,
we use a k-d partitioning method to efficiently estimating the center surround
conditional entropy. We present experimental results on two publicly available
eye tracking still image databases and show that the new technique is
competitive with state of the art bottom-up saliency computational methods. We
have also extended the technique to compute spatiotemporal visual saliency of
video and evaluate the bottom-up spatiotemporal saliency against eye tracking
data on a video taken onboard a moving vehicle with the driver's eye being
tracked by a head mounted eye-tracker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7664</identifier>
 <datestamp>2013-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7664</id><created>2013-01-31</created><updated>2013-10-02</updated><authors><author><keyname>Kamalapurkar</keyname><forenames>Rushikesh</forenames></author><author><keyname>Dinh</keyname><forenames>Huyen</forenames></author><author><keyname>Bhasin</keyname><forenames>Shubhendu</forenames></author><author><keyname>Dixon</keyname><forenames>Warren</forenames></author></authors><title>Approximate Optimal Trajectory Tracking for Continuous Time Nonlinear
  Systems</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate dynamic programming has been investigated and used as a method to
approximately solve optimal regulation problems. However, the extension of this
technique to optimal tracking problems for continuous time nonlinear systems
has remained a non-trivial open problem. The control development in this paper
guarantees ultimately bounded tracking of a desired trajectory, while also
ensuring that the controller converges to an approximate optimal policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7669</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7669</id><created>2013-01-31</created><authors><author><keyname>Wielemaker</keyname><forenames>Jan</forenames></author></authors><title>Extending the logical update view with transaction support</title><categories>cs.PL cs.DB</categories><comments>Appeared in CICLOPS 2012. 9 Pages, 0 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the database update view was standardised in the Prolog ISO standard,
the so called logical update view is available in all actively maintained
Prolog systems. While this update view provided a well defined update semantics
and allows for efficient handling of dynamic code, it does not help in
maintaining consistency of the dynamic database. With the introduction of
multiple threads and deployment of Prolog in continuously running server
applications, consistency of the dynamic database becomes important.
  In this article, we propose an extension to the generation-based
implementation of the logical update view that supports transactions.
Generation-based transactions have been implemented according to this
description in the SWI-Prolog RDF store. The aim of this paper is to motivate
transactions, outline an implementation and generate discussion on the
desirable semantics and interface prior to implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7673</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7673</id><created>2013-01-31</created><authors><author><keyname>Zhou</keyname><forenames>Neng-Fa</forenames></author><author><keyname>Fruhman</keyname><forenames>Jonathan</forenames></author></authors><title>Toward a Dynamic Programming Solution for the 4-peg Tower of Hanoi
  Problem with Configurations</title><categories>cs.PL cs.AI</categories><comments>Appeared in CICLOPS 2012. 15 Pages, 2 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Frame-Stewart algorithm for the 4-peg variant of the Tower of Hanoi,
introduced in 1941, partitions disks into intermediate towers before moving the
remaining disks to their destination. Algorithms that partition the disks have
not been proven to be optimal, although they have been verified for up to 30
disks. This paper presents a dynamic programming approach to this algorithm,
using tabling in B-Prolog. This study uses a variation of the problem,
involving configurations of disks, in order to contrast the tabling approach
with the approaches utilized by other solvers. A comparison of different
partitioning locations for the Frame-Stewart algorithm indicates that, although
certain partitions are optimal for the classic problem, they need to be
modified for certain configurations, and that random configurations might
require an entirely new algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7676</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7676</id><created>2013-01-31</created><authors><author><keyname>Monnet</keyname><forenames>Anthony</forenames></author><author><keyname>Villemaire</keyname><forenames>Roger</forenames></author></authors><title>Efficient Partial Order CDCL Using Assertion Level Choice Heuristics</title><categories>cs.AI cs.LO</categories><comments>Appeared in CICLOPS 2012. 15 Pages, 1 Figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We previously designed Partial Order Conflict Driven Clause Learning
(PO-CDCL), a variation of the satisfiability solving CDCL algorithm with a
partial order on decision levels, and showed that it can speed up the solving
on problems with a high independence between decision levels. In this paper, we
more thoroughly analyze the reasons of the efficiency of PO-CDCL. Of particular
importance is that the partial order introduces several candidates for the
assertion level. By evaluating different heuristics for this choice, we show
that the assertion level selection has an important impact on solving and that
a carefully designed heuristic can significantly improve performances on
relevant benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7680</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7680</id><created>2013-01-31</created><authors><author><keyname>Santos</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Rocha</keyname><forenames>Ricardo</forenames></author></authors><title>Efficient Support for Mode-Directed Tabling in the YapTab Tabling System</title><categories>cs.PL</categories><comments>Appeared in CICLOPS 2012. 15 Pages, 12 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mode-directed tabling is an extension to the tabling technique that supports
the definition of mode operators for specifying how answers are inserted into
the table space. In this paper, we focus our discussion on the efficient
support for mode directed-tabling in the YapTab tabling system. We discuss 7
different mode operators and explain how we have extended and optimized
YapTab's table space organization to support them. Initial experimental results
show that our implementation compares favorably with the B-Prolog and XSB
state-of-the-art Prolog tabling systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7690</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7690</id><created>2013-01-31</created><authors><author><keyname>Vieira</keyname><forenames>Rui</forenames></author><author><keyname>Rocha</keyname><forenames>Ricardo</forenames></author><author><keyname>Silva</keyname><forenames>Fernando</forenames></author></authors><title>On Comparing Alternative Splitting Strategies for Or-Parallel Prolog
  Execution on Multicores</title><categories>cs.PL</categories><comments>Appeared in CICLOPS 2012. 15 Pages, 6 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many or-parallel Prolog models exploiting implicit parallelism have been
proposed in the past. Arguably, one of the most successful models is
environment copying for shared memory architectures. With the increasing
availability and popularity of multicore architectures, it makes sense to
recover the body of knowledge there is in this area and re-engineer prior
computational models to evaluate their performance on newer architectures. In
this work, we focus on the implementation of splitting strategies for
or-parallel Prolog execution on multicores and, for that, we develop a
framework, on top of the YapOr system, that integrates and supports five
alternative splitting strategies. Our implementation shares the underlying
execution environment and most of the data structures used to implement
or-parallelism in YapOr. In particular, we took advantage of YapOr's
infrastructure for incremental copying and scheduling support, which we used
with minimal modifications. We thus argue that all these common support
features allow us to make a first and fair comparison between these five
alternative splitting strategies and, therefore, better understand their
advantages and weaknesses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7693</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7693</id><created>2013-01-31</created><updated>2013-11-06</updated><authors><author><keyname>Tamo</keyname><forenames>Itzhak</forenames></author><author><keyname>Papailiopoulos</keyname><forenames>Dimitris S.</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author></authors><title>Optimal Locally Repairable Codes and Connections to Matroid Theory</title><categories>cs.IT math.IT</categories><comments>Submitted for publication, a shorter version was presented at ISIT
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Petabyte-scale distributed storage systems are currently transitioning to
erasure codes to achieve higher storage efficiency. Classical codes like
Reed-Solomon are highly sub-optimal for distributed environments due to their
high overhead in single-failure events. Locally Repairable Codes (LRCs) form a
new family of codes that are repair efficient. In particular, LRCs minimize the
number of nodes participating in single node repairs during which they generate
small network traffic. Two large-scale distributed storage systems have already
implemented different types of LRCs: Windows Azure Storage and the Hadoop
Distributed File System RAID used by Facebook. The fundamental bounds for LRCs,
namely the best possible distance for a given code locality, were recently
discovered, but few explicit constructions exist. In this work, we present an
explicit and optimal LRCs that are simple to construct. Our construction is
based on grouping Reed-Solomon (RS) coded symbols to obtain RS coded symbols
over a larger finite field. We then partition these RS symbols in small groups,
and re-encode them using a simple local code that offers low repair locality.
For the analysis of the optimality of the code, we derive a new result on the
matroid represented by the code generator matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7694</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7694</id><created>2013-01-31</created><authors><author><keyname>Drey</keyname><forenames>Zo&#xe9;</forenames></author><author><keyname>Morales</keyname><forenames>Jos&#xe9; F.</forenames></author><author><keyname>Hermenegildo</keyname><forenames>Manuel V.</forenames></author></authors><title>Reversible Language Extensions and their Application in Debugging</title><categories>cs.PL</categories><comments>Appeared in CICLOPS 2012. 15 Pages, 7 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A range of methodologies and techniques are available to guide the design and
implementation of language extensions and domain-specific languages. A simple
yet powerful technique is based on source-to-source transformations interleaved
across the compilation passes of a base language. Despite being a successful
approach, it has the main drawback that the input source code is lost in the
process. When considering the whole workflow of program development (warning
and error reporting, debugging, or even program analysis), program translations
are no more powerful than a glorified macro language. In this paper, we propose
an augmented approach to language extensions for Prolog, where symbolic
annotations are included in the target program. These annotations allow
selectively reversing the translated code. We illustrate the approach by
showing that coupling it with minimal extensions to a generic Prolog debugger
allows us to provide users with a familiar, source-level view during the
debugging of programs which use a variety of language extensions, such as
functional notation, DCGs, or CLP{Q,R}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7699</identifier>
 <datestamp>2013-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7699</id><created>2013-01-31</created><updated>2013-05-10</updated><authors><author><keyname>Machado</keyname><forenames>Rui</forenames></author><author><keyname>Abreu</keyname><forenames>Salvador</forenames></author><author><keyname>Diaz</keyname><forenames>Daniel</forenames></author></authors><title>Parallel Local Search: Experiments with a PGAS-based programming model</title><categories>cs.PL cs.DC</categories><comments>Appeared in CICLOPS 2012. 17 Pages, 4 Figures. arXiv admin note: text
  overlap with arXiv:1212.4287 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local search is a successful approach for solving combinatorial optimization
and constraint satisfaction problems. With the progressing move toward multi
and many-core systems, GPUs and the quest for Exascale systems, parallelism has
become mainstream as the number of cores continues to increase. New programming
models are required and need to be better understood as well as data structures
and algorithms. Such is the case for local search algorithms when run on
hundreds or thousands of processing units. In this paper, we discuss some
experiments we have been doing with Adaptive Search and present a new parallel
version of it based on GPI, a recent API and programming model for the
development of scalable parallel applications. Our experiments on different
problems show interesting speedups and, more importantly, a deeper
interpretation of the parallelization of Local Search methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7700</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7700</id><created>2013-01-31</created><updated>2013-08-02</updated><authors><author><keyname>Arul</keyname><forenames>Arjun</forenames></author><author><keyname>Reichert</keyname><forenames>Julien</forenames></author></authors><title>The Complexity of Robot Games on the Integer Line</title><categories>cs.LO cs.GT</categories><comments>In Proceedings QAPL 2013, arXiv:1306.2413</comments><journal-ref>EPTCS 117, 2013, pp. 132-148</journal-ref><doi>10.4204/EPTCS.117.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In robot games on Z, two players add integers to a counter. Each player has a
finite set from which he picks the integer to add, and the objective of the
first player is to let the counter reach 0. We present an exponential-time
algorithm for deciding the winner of a robot game given the initial counter
value, and prove a matching lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7702</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7702</id><created>2013-01-31</created><authors><author><keyname>Arias</keyname><forenames>Emilio Jes&#xfa;s Gallego</forenames></author><author><keyname>Haemmerl&#xe9;</keyname><forenames>R&#xe9;my</forenames></author><author><keyname>Hermenegildo</keyname><forenames>Manuel V.</forenames></author><author><keyname>Morales</keyname><forenames>Jos&#xe9; F.</forenames></author></authors><title>The Ciao clp(FD) Library. A Modular CLP Extension for Prolog</title><categories>cs.PL</categories><comments>Appeared in CICLOPS 2012. 15 Pages, 5 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new free library for Constraint Logic Programming over Finite
Domains, included with the Ciao Prolog system. The library is entirely written
in Prolog, leveraging on Ciao's module system and code transformation
capabilities in order to achieve a highly modular design without compromising
performance. We describe the interface, implementation, and design rationale of
each modular component.
  The library meets several design goals: a high level of modularity, allowing
the individual components to be replaced by different versions;
high-efficiency, being competitive with other FD implementations; a glass-box
approach, so the user can specify new constraints at different levels; and a
Prolog implementation, in order to ease the integration with Ciao's code
analysis components.
  The core is built upon two small libraries which implement integer ranges and
closures. On top of that, a finite domain variable datatype is defined, taking
care of constraint reexecution depending on range changes. These three
libraries form what we call the FD kernel of the library.
  This FD kernel is used in turn to implement several higher-level finite
domain constraints, specified using indexicals. Together with a labeling module
this layer forms what we name \emph{the FD solver}. A final level integrates
the clp(FD) paradigm with our FD solver. This is achieved using attributed
variables and a compiler from the clp(FD) language to the set of constraints
provided by the solver.
  It should be noted that the user of the library is encouraged to work in any
of those levels as seen convenient: from writing a new range module to
enriching the set of FD constraints by writing new indexicals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7724</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7724</id><created>2013-01-31</created><updated>2014-09-02</updated><authors><author><keyname>Carlsson</keyname><forenames>Gunnar</forenames></author><author><keyname>M&#xe9;moli</keyname><forenames>Facundo</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author><author><keyname>Segarra</keyname><forenames>Santiago</forenames></author></authors><title>Axiomatic Construction of Hierarchical Clustering in Asymmetric Networks</title><categories>cs.LG cs.SI stat.ML</categories><comments>This is a largely extended version of the previous conference
  submission under the same title. The current version contains the material in
  the previous version (published in ICASSP 2013) as well as material presented
  at the Asilomar Conference on Signal, Systems, and Computers 2013, GlobalSIP
  2013, and ICML 2014. Also, unpublished material is included in the current
  version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers networks where relationships between nodes are
represented by directed dissimilarities. The goal is to study methods for the
determination of hierarchical clusters, i.e., a family of nested partitions
indexed by a connectivity parameter, induced by the given dissimilarity
structures. Our construction of hierarchical clustering methods is based on
defining admissible methods to be those methods that abide by the axioms of
value - nodes in a network with two nodes are clustered together at the maximum
of the two dissimilarities between them - and transformation - when
dissimilarities are reduced, the network may become more clustered but not
less. Several admissible methods are constructed and two particular methods,
termed reciprocal and nonreciprocal clustering, are shown to provide upper and
lower bounds in the space of admissible methods. Alternative clustering
methodologies and axioms are further considered. Allowing the outcome of
hierarchical clustering to be asymmetric, so that it matches the asymmetry of
the original data, leads to the inception of quasi-clustering methods. The
existence of a unique quasi-clustering method is shown. Allowing clustering in
a two-node network to proceed at the minimum of the two dissimilarities
generates an alternative axiomatic construction. There is a unique clustering
method in this case too. The paper also develops algorithms for the computation
of hierarchical clusters using matrix powers on a min-max dioid algebra and
studies the stability of the methods proposed. We proved that most of the
methods introduced in this paper are such that similar networks yield similar
hierarchical clustering results. Algorithms are exemplified through their
application to networks describing internal migration within states of the
United States (U.S.) and the interrelation between sectors of the U.S. economy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7738</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7738</id><created>2013-01-31</created><updated>2013-02-19</updated><authors><author><keyname>Coelho</keyname><forenames>Fl&#xe1;vio Code&#xe7;o</forenames></author><author><keyname>Souza</keyname><forenames>Renato Rocha</forenames></author><author><keyname>Justen</keyname><forenames>&#xc1;lvaro</forenames></author><author><keyname>Amieiro</keyname><forenames>Fl&#xe1;vio</forenames></author><author><keyname>Mello</keyname><forenames>Heliana</forenames></author></authors><title>PyPLN: a Distributed Platform for Natural Language Processing</title><categories>cs.CL cs.IR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents a distributed platform for Natural Language Processing
called PyPLN. PyPLN leverages a vast array of NLP and text processing open
source tools, managing the distribution of the workload on a variety of
configurations: from a single server to a cluster of linux servers. PyPLN is
developed using Python 2.7.3 but makes it very easy to incorporate other
softwares for specific tasks as long as a linux version is available. PyPLN
facilitates analyses both at document and corpus level, simplifying management
and publication of corpora and analytical results through an easy to use web
interface. In the current (beta) release, it supports English and Portuguese
languages with support to other languages planned for future releases. To
support the Portuguese language PyPLN uses the PALAVRAS parser\citep{Bick2000}.
Currently PyPLN offers the following features: Text extraction with encoding
normalization (to UTF-8), part-of-speech tagging, token frequency, semantic
annotation, n-gram extraction, word and sentence repertoire, and full-text
search across corpora. The platform is licensed as GPL-v3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7744</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7744</id><created>2013-01-31</created><updated>2014-04-09</updated><authors><author><keyname>Schatz</keyname><forenames>Martin D.</forenames></author><author><keyname>Low</keyname><forenames>Tze Meng</forenames></author><author><keyname>van de Geijn</keyname><forenames>Robert A.</forenames></author><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author></authors><title>Exploiting Symmetry in Tensors for High Performance: Multiplication with
  Symmetric Tensors</title><categories>math.NA cs.MS</categories><msc-class>15-02 (Primary)</msc-class><journal-ref>SIAM Journal on Scientific Computing, Vol. 36, No. 5, pp.
  C453-C479, September 2014</journal-ref><doi>10.1137/130907215</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symmetric tensor operations arise in a wide variety of computations. However,
the benefits of exploiting symmetry in order to reduce storage and computation
is in conflict with a desire to simplify memory access patterns. In this paper,
we propose a blocked data structure (Blocked Compact Symmetric Storage) wherein
we consider the tensor by blocks and store only the unique blocks of a
symmetric tensor. We propose an algorithm-by-blocks, already shown of benefit
for matrix computations, that exploits this storage format by utilizing a
series of temporary tensors to avoid redundant computation. Further, partial
symmetry within temporaries is exploited to further avoid redundant storage and
redundant computation. A detailed analysis shows that, relative to storing and
computing with tensors without taking advantage of symmetry and partial
symmetry, storage requirements are reduced by a factor of $ O\left( m! \right)$
and computational requirements by a factor of $O\left( (m+1)!/2^m \right)$,
where $ m $ is the order of the tensor. However, as the analysis shows, care
must be taken in choosing the correct block size to ensure these storage and
computational benefits are achieved (particularly for low-order tensors). An
implementation demonstrates that storage is greatly reduced and the complexity
introduced by storing and computing with tensors by blocks is manageable.
Preliminary results demonstrate that computational time is also reduced. The
paper concludes with a discussion of how insights in this paper point to
opportunities for generalizing recent advances in the domain of linear algebra
libraries to the field of multi-linear computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0017</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0017</id><created>2013-01-31</created><authors><author><keyname>Hussain</keyname><forenames>Heather S.</forenames></author><author><keyname>Matsutani</keyname><forenames>Megumi M.</forenames></author><author><keyname>Annaswamy</keyname><forenames>Anuradha M.</forenames></author><author><keyname>Lavretsky</keyname><forenames>Eugene</forenames></author></authors><title>Adaptive Control of Scalar Plants in the Presence of Unmodeled Dynamics</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust adaptive control of scalar plants in the presence of unmodeled
dynamics is established in this paper. It is shown that implementation of a
projection algorithm with standard adaptive control of a scalar plant ensures
global boundedness of the overall adaptive system for a class of unmodeled
dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0019</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0019</id><created>2013-01-31</created><updated>2013-07-01</updated><authors><author><keyname>Amjad</keyname><forenames>Rana Ali</forenames></author><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author></authors><title>Fixed-to-Variable Length Distribution Matching</title><categories>cs.IT math.IT</categories><comments>5 pages, essentially the ISIT 2013 version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fixed-to-variable length (f2v) matchers are used to reversibly transform an
input sequence of independent and uniformly distributed bits into an output
sequence of bits that are (approximately) independent and distributed according
to a target distribution. The degree of approximation is measured by the
informational divergence between the output distribution and the target
distribution. An algorithm is developed that efficiently finds optimal f2v
codes. It is shown that by encoding the input bits blockwise, the informational
divergence per bit approaches zero as the block length approaches infinity. A
relation to data compression by Tunstall coding is established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0033</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0033</id><created>2013-01-31</created><authors><author><keyname>de la Cruz</keyname><forenames>J.</forenames></author></authors><title>On extremal self-dual codes of length 120</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the only primes which may divide the order of the automorphism
group of a putative binary self-dual doubly-even [120, 60, 24] code are 2, 3,
5, 7, 19, 23 and 29. Furthermore we prove that automorphisms of prime order $p
\geq 5$ have a unique cycle structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0050</identifier>
 <datestamp>2013-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0050</id><created>2013-01-31</created><updated>2013-05-03</updated><authors><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author><author><keyname>Kuzuoka</keyname><forenames>Shigeaki</forenames></author></authors><title>Universal Wyner-Ziv Coding for Distortion Constrained General
  Side-Information</title><categories>cs.IT math.IT</categories><comments>34 pages, 5 figures, v2 fixed a minor gap in the proof</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the Wyner-Ziv coding in which the statistics of the principal
source is known but the statistics of the channel generating the
side-information is unknown except that it is in a certain class. The class
consists of channels such that the distortion between the principal source and
the side-information is smaller than a threshold, but channels may be neither
stationary nor ergodic. In this situation, we define a new rate-distortion
function as the minimum rate such that there exists a Wyner-Ziv code that is
universal for every channel in the class. Then, we show an upper bound and a
lower bound on the rate-distortion function, and derive a matching condition
such that the upper and lower bounds coincide. The relation between the new
rate-distortion function and the rate-distortion function of the Heegard-Berger
problem is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0059</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0059</id><created>2013-01-31</created><authors><author><keyname>Graves</keyname><forenames>Eric</forenames></author><author><keyname>Wong</keyname><forenames>Tan</forenames></author></authors><title>A coding approach to guarantee information integrity against a Byzantine
  relay</title><categories>cs.IT math.IT</categories><comments>5 Pages, submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a random coding scheme with which two nodes can exchange
information with guaranteed integrity over a two-way Byzantine relay. This
coding scheme is employed to obtain an inner bound on the capacity region with
guaranteed information integrity. No pre-shared secret or secret transmission
is needed for the proposed scheme. Hence the inner bound obtained is generally
larger than those achieved based on secret transmission schemes. This approach
advocates the separation of supporting information integrity and secrecy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0067</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0067</id><created>2013-01-31</created><authors><author><keyname>Adler</keyname><forenames>Ilan</forenames></author><author><keyname>Verma</keyname><forenames>Sushil</forenames></author></authors><title>A direct reduction of PPAD Lemke-verified linear complementarity
  problems to bimatrix games</title><categories>cs.CC</categories><comments>19 pages</comments><msc-class>03D15, 68Q15, 68Q17, 90C20, 90C33, 91A05, 91-08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The linear complementarity problem, LCP(q,M), is defined as follows. For
given M,q find z such that q+Mz&gt;=0, z&gt;=0, z(q + M z)=0,or certify that there is
no such z. It is well known that the problem of finding a Nash equilibrium for
a bimatrix game (2-NASH) can be formulated as a linear complementarity problem
(LCP). In addition, 2-NASH is known to be complete in the complexity class PPAD
(Polynomial-time Parity Argument Directed). However, the ingeniously
constructed reduction (which is designed for any PPAD problem) is very
complicated, so while of great theoretical significance, it is not practical
for actually solving an LCP via 2-NASH, and it may not provide the potential
insight that can be gained from studying the game obtained from a problem
formulated as an LCP (e.g. market equilibrium). The main goal of this paper is
the construction of a simple explicit reduction of any LCP(q,M) that can be
verified as belonging to PPAD via the graph induced by the generic Lemke
algorithm with some positive covering vector d, to a symmetric 2-NASH. In
particular, any endpoint of this graph (with the exception of the initial point
of the algorithm) corresponds to either a solution or to a so-called secondary
ray. Thus, an LCP problem is verified as belonging to PPAD if any secondary ray
can be used to construct, in polynomial time, a certificate that there is no
solution to the problem. We achieve our goal by showing that for any M,q and a
positive d satisfying a certain nondegeneracy assumption with respect to M, we
can simply and directly construct a symmetric 2-NASH whose Nash equilibria
correspond one-to-one to the end points of the graph induced by LCP(q,M) and
the Lemke algorithm with a covering vector d. We note that for a given M the
reduction works for all positive d with the exception of a subset of measure 0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0070</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0070</id><created>2013-01-31</created><authors><author><keyname>Perevalov</keyname><forenames>Eugene</forenames></author><author><keyname>Grace</keyname><forenames>David</forenames></author></authors><title>Towards the full information chain theory: solution methods for optimal
  information acquisition problem</title><categories>physics.data-an cs.IT math.IT</categories><comments>33 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:1301.2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When additional information sources are available in decision making problems
that allow stochastic optimization formulations, an important question is how
to optimally use the information the sources are capable of providing. A
framework that relates information accuracy determined by the source's
knowledge structure to its relevance determined by the problem being solved was
proposed in a companion paper. There, the problem of optimal information
acquisition was formulated as that of minimization of the expected loss of the
solution subject to constraints dictated by the information source knowledge
structure and depth. Approximate solution methods for this problem are
developed making use of probability metrics method and its application for
scenario reduction in stochastic optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0072</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0072</id><created>2013-01-31</created><authors><author><keyname>Marcus</keyname><forenames>Shoshana</forenames></author><author><keyname>Sokol</keyname><forenames>Dina</forenames></author></authors><title>Dynamic 2D Dictionary Matching in Small Space</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dictionary matching problem preprocesses a set of patterns and finds all
occurrences of each of the patterns in a text when it is provided. We focus on
the dynamic setting, in which patterns can be inserted to and removed from the
dictionary, without reprocessing the entire dictionary. This article presents
the first algorithm that performs \emph{dynamic} dictionary matching on
two-dimensional data within small space. The time complexity of our algorithm
is almost linear. The only slowdown is incurred by querying the compressed
self-index that replaces the dictionary. The dictionary is updated in time
proportional to the size of the pattern that is being inserted to or removed
from the dictionary. Our algorithm is suitable for rectangular patterns that
are of uniform size in one dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0077</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0077</id><created>2013-02-01</created><authors><author><keyname>Yang</keyname><forenames>Zai</forenames></author><author><keyname>Zhang</keyname><forenames>Cishen</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author></authors><title>Sparse MRI for motion correction</title><categories>cs.CV physics.bio-ph physics.med-ph</categories><comments>To appear in Proceedings of ISBI 2013. 4 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MR image sparsity/compressibility has been widely exploited for imaging
acceleration with the development of compressed sensing. A sparsity-based
approach to rigid-body motion correction is presented for the first time in
this paper. A motion is sought after such that the compensated MR image is
maximally sparse/compressible among the infinite candidates. Iterative
algorithms are proposed that jointly estimate the motion and the image content.
The proposed method has a lot of merits, such as no need of additional data and
loose requirement for the sampling sequence. Promising results are presented to
demonstrate its performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0081</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0081</id><created>2013-02-01</created><authors><author><keyname>Yang</keyname><forenames>Zai</forenames></author><author><keyname>Zhang</keyname><forenames>Cishen</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author></authors><title>Robust Compressive Phase Retrieval via L1 Minimization With Application
  to Image Reconstruction</title><categories>physics.comp-ph cs.IT math.IT math.OC</categories><comments>Submitted to ICIP 2013. 5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phase retrieval refers to a classical nonconvex problem of recovering a
signal from its Fourier magnitude measurements. Inspired by the compressed
sensing technique, signal sparsity is exploited in recent studies of phase
retrieval to reduce the required number of measurements, known as compressive
phase retrieval (CPR). In this paper, l1 minimization problems are formulated
for CPR to exploit the signal sparsity and alternating direction algorithms are
presented for problem solving. For real-valued, nonnegative image
reconstruction, the image of interest is shown to be an optimal solution of the
formulated l1 minimization in the noise free case. Numerical simulations
demonstrate that the proposed approach is fast, accurate and robust to
measurements noises.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0082</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0082</id><created>2013-02-01</created><authors><author><keyname>Poczos</keyname><forenames>Barnabas</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Distribution-Free Distribution Regression</title><categories>stat.ML cs.LG math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  `Distribution regression' refers to the situation where a response Y depends
on a covariate P where P is a probability distribution. The model is Y=f(P) +
mu where f is an unknown regression function and mu is a random error.
Typically, we do not observe P directly, but rather, we observe a sample from
P. In this paper we develop theory and methods for distribution-free versions
of distribution regression. This means that we do not make distributional
assumptions about the error term mu and covariate P. We prove that when the
effective dimension is small enough (as measured by the doubling dimension),
then the excess prediction risk converges to zero with a polynomial rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0084</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0084</id><created>2013-02-01</created><updated>2014-08-11</updated><authors><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author><author><keyname>Wu</keyname><forenames>Yihong</forenames></author></authors><title>Peak-to-average power ratio of good codes for Gaussian channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a problem of forward error-correction for the additive white
Gaussian noise (AWGN) channel. For finite blocklength codes the backoff from
the channel capacity is inversely proportional to the square root of the
blocklength. In this paper it is shown that codes achieving this tradeoff must
necessarily have peak-to-average power ratio (PAPR) proportional to logarithm
of the blocklength. This is extended to codes approaching capacity slower, and
to PAPR measured at the output of an OFDM modulator. As a by-product the
convergence of (Smith's) amplitude-constrained AWGN capacity to Shannon's
classical formula is characterized in the regime of large amplitudes. This
converse-type result builds upon recent contributions in the study of empirical
output distributions of good channel codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0103</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0103</id><created>2013-02-01</created><updated>2013-02-19</updated><authors><author><keyname>Rusu</keyname><forenames>Florin</forenames></author><author><keyname>Cheng</keyname><forenames>Yu</forenames></author></authors><title>A Survey on Array Storage, Query Languages, and Systems</title><categories>cs.DB</categories><comments>44 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since scientific investigation is one of the most important providers of
massive amounts of ordered data, there is a renewed interest in array data
processing in the context of Big Data. To the best of our knowledge, a unified
resource that summarizes and analyzes array processing research over its long
existence is currently missing. In this survey, we provide a guide for past,
present, and future research in array processing. The survey is organized along
three main topics. Array storage discusses all the aspects related to array
partitioning into chunks. The identification of a reduced set of array
operators to form the foundation for an array query language is analyzed across
multiple such proposals. Lastly, we survey real systems for array processing.
The result is a thorough survey on array data storage and processing that
should be consulted by anyone interested in this research topic, independent of
experience level. The survey is not complete though. We greatly appreciate
pointers towards any work we might have forgotten to mention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0126</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0126</id><created>2013-02-01</created><authors><author><keyname>Angelopoulos</keyname><forenames>Nicos</forenames></author><author><keyname>Bagnara</keyname><forenames>Roberto</forenames></author></authors><title>Proceedings of the 12th International Colloquium on Implementation of
  Constraint and LOgic Programming Systems</title><categories>cs.PL cs.AI</categories><comments>1 invited talk, 9 papers and 1 panel discussion</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers presented at CICLOPS'12: 12th International
Colloquium on Implementation of Constraint and LOgic Programming Systems held
on Tueseday September 4th, 2012 in Budapest.
  The program included 1 invited talk, 9 technical presentations and a panel
discussion on Prolog open standards (open.pl). Each programme paper was
reviewed by 3 reviewers.
  CICLOPS'12 continues a tradition of successful workshops on Implementations
of Logic Programming Systems, previously held in Budapest (1993) and Ithaca
(1994), the Compulog Net workshops on Parallelism and Implementation
Technologies held in Madrid (1993 and 1994), Utrecht (1995) and Bonn (1996),
the Workshop on Parallelism and Implementation Technology for (Constraint)
Logic Programming Languages held in Port Jefferson (1997), Manchester (1998),
Las Cruces (1999), and London (2000), and more recently the Colloquium on
Implementation of Constraint and LOgic Programming Systems in Paphos (2001),
Copenhagen (2002), Mumbai (2003), Saint Malo (2004), Sitges (2005), Seattle
(2006), Porto (2007), Udine (2008), Pasadena (2009), Edinburgh (2010) -
together with WLPE, Lexington (2011).
  We would like to thank all the authors, Tom Schrijvers for his invited talk,
the programme committee members, and the ICLP 2012 organisers. We would like to
also thank arXiv.org for providing permanent hosting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0136</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0136</id><created>2013-02-01</created><authors><author><keyname>Stowell</keyname><forenames>Dan</forenames></author><author><keyname>Chew</keyname><forenames>Elaine</forenames></author></authors><title>Maximum a posteriori estimation of piecewise arcs in tempo time-series</title><categories>cs.SD</categories><comments>Submitted to postprint volume for Computer Music Modeling and
  Retrieval (CMMR) 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In musical performances with expressive tempo modulation, the tempo variation
can be modelled as a sequence of tempo arcs. Previous authors have used this
idea to estimate series of piecewise arc segments from data. In this paper we
describe a probabilistic model for a time-series process of this nature, and
use this to perform inference of single- and multi-level arc processes from
data. We describe an efficient Viterbi-like process for MAP inference of arcs.
Our approach is score-agnostic, and together with efficient inference allows
for online analysis of performances including improvisations, and can predict
immediate future tempo trajectories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0164</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0164</id><created>2013-02-01</created><authors><author><keyname>Ward</keyname><forenames>Jonathan</forenames></author><author><keyname>Grindrod</keyname><forenames>Peter</forenames></author></authors><title>Aperiodic dynamics in a deterministic model of attitude formation in
  social groups</title><categories>physics.soc-ph cs.SI nlin.AO</categories><doi>10.1016/j.physd.2014.05.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Homophily and social influence are the fundamental mechanisms that drive the
evolution of attitudes, beliefs and behaviour within social groups. Homophily
relates the similarity between pairs of individuals' attitudinal states to
their frequency of interaction, and hence structural tie strength, while social
influence causes the convergence of individuals' states during interaction.
Building on these basic elements, we propose a new mathematical modelling
framework to describe the evolution of attitudes within a group of interacting
agents. Specifically, our model describes sub-conscious attitudes that have an
activator-inhibitor relationship. We consider a homogeneous population using a
deterministic, continuous-time dynamical system. Surprisingly, the combined
effects of homophily and social influence do not necessarily lead to group
consensus or global monoculture. We observe that sub-group formation and
polarisation-like effects may be transient, the long-time dynamics being
quasi-periodic with sensitive dependence to initial conditions. This is due to
the interplay between the evolving interaction network and Turing instability
associated with the attitudinal state dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0189</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0189</id><created>2013-02-01</created><authors><author><keyname>Zhang</keyname><forenames>Pan</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>M&#xe9;zard</keyname><forenames>Marc</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Non-adaptive pooling strategies for detection of rare faulty items</title><categories>cs.IT cond-mat.stat-mech math.IT q-bio.GN q-bio.QM</categories><comments>5 pages</comments><journal-ref>IEEE International Conference on Communications Workshops (ICC
  2013), Pages: 1409 - 1414, (2013)</journal-ref><doi>10.1109/ICCW.2013.6649458</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study non-adaptive pooling strategies for detection of rare faulty items.
Given a binary sparse N-dimensional signal x, how to construct a sparse binary
MxN pooling matrix F such that the signal can be reconstructed from the
smallest possible number M of measurements y=Fx? We show that a very low number
of measurements is possible for random spatially coupled design of pools F. Our
design might find application in genetic screening or compressed genotyping. We
show that our results are robust with respect to the uncertainty in the matrix
F when some elements are mistaken.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0210</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0210</id><created>2013-02-01</created><authors><author><keyname>Chen</keyname><forenames>Ming-Hung</forenames></author><author><keyname>Wang</keyname><forenames>Shi-Chen</forenames></author><author><keyname>Chou</keyname><forenames>Cheng-Fu</forenames></author></authors><title>Deadline is not Enough: How to Achieve Importance-aware Server-centric
  Data Centers via a Cross Layer Approach</title><categories>cs.DC cs.NI</categories><comments>This version is finished on 2012/7/30. New versions which include
  improved rate-based importance-aware protocol and DCTCP-based
  importance-aware protocol are considered to be submitted before release</comments><msc-class>68M12</msc-class><acm-class>C.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's datacenters face important challenges for providing low-latency
high-quality interactive services to meet user's expectation. For improving the
application throughput, recent research works have embedded application
deadline information into design of network flow schedule to meet the latency
requirement. Here, arises a critical question: does application-level
throughput mean providing better quality service? We note that there are
usually a set of semantic related responses (or flows) for answering a query;
and, some responses are highly correlative with the query while others do not.
Thus, this observation motivates us to associate the importance of the contents
with the application flows (or responses) in order to enhance the service
quality. We first model the application importance maximization problem in a
generic network and in a server-centric network. Since both of them are too
complicated to be deployed in the real world, we propose the importance-aware
delivery protocol, which is a distributed event-driven rate-based delivery
control protocol, for server-centric datacenter networks. The proposed protocol
is able to make use of the multiple disjoin paths of server-centric network,
and jointly consider flow importance, flow size, and deadline to maximize the
goodput of most-related semantic data of a query. Through real-data-based or
synthetic simulations, the results show that our proposed protocol
significantly outperforms D3 and MPTCP in terms of the precision at K and the
sum of application-level importance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0212</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0212</id><created>2013-02-01</created><authors><author><keyname>Yin</keyname><forenames>Xin</forenames></author><author><keyname>Song</keyname><forenames>Zhao</forenames></author><author><keyname>Dorman</keyname><forenames>Karin</forenames></author><author><keyname>Ramamoorthy</keyname><forenames>Aditya</forenames></author></authors><title>PREMIER - PRobabilistic Error-correction using Markov Inference in
  Errored Reads</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present a flexible, probabilistic and reference-free method
of error correction for high throughput DNA sequencing data. The key is to
exploit the high coverage of sequencing data and model short sequence outputs
as independent realizations of a Hidden Markov Model (HMM). We pose the problem
of error correction of reads as one of maximum likelihood sequence detection
over this HMM. While time and memory considerations rule out an implementation
of the optimal Baum-Welch algorithm (for parameter estimation) and the optimal
Viterbi algorithm (for error correction), we propose low-complexity approximate
versions of both. Specifically, we propose an approximate Viterbi and a
sequential decoding based algorithm for the error correction. Our results show
that when compared with Reptile, a state-of-the-art error correction method,
our methods consistently achieve superior performances on both simulated and
real data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0215</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0215</id><created>2013-02-01</created><updated>2013-05-13</updated><authors><author><keyname>Hou</keyname><forenames>Jie</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>Informational Divergence Approximations to Product Distributions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum rate needed to accurately approximate a product distribution
based on an unnormalized informational divergence is shown to be a mutual
information. This result subsumes results of Wyner on common information and
Han-Verd\'{u} on resolvability. The result also extends to cases where the
source distribution is unknown but the entropy is known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0216</identifier>
 <datestamp>2013-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0216</id><created>2013-01-31</created><updated>2013-08-22</updated><authors><author><keyname>Dobrev</keyname><forenames>Dimiter</forenames></author></authors><title>Comparison between the two definitions of AI</title><categories>cs.AI</categories><comments>added four new sections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two different definitions of the Artificial Intelligence concept have been
proposed in papers [1] and [2]. The first definition is informal. It says that
any program that is cleverer than a human being, is acknowledged as Artificial
Intelligence. The second definition is formal because it avoids reference to
the concept of human being. The readers of papers [1] and [2] might be left
with the impression that both definitions are equivalent and the definition in
[2] is simply a formal version of that in [1]. This paper will compare both
definitions of Artificial Intelligence and, hopefully, will bring a better
understanding of the concept.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0226</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0226</id><created>2013-02-01</created><authors><author><keyname>Riverso</keyname><forenames>Stefano</forenames></author><author><keyname>Farina</keyname><forenames>Marcello</forenames></author><author><keyname>Ferrari-Trecate</keyname><forenames>Giancarlo</forenames></author></authors><title>Plug-and-Play Decentralized Model Predictive Control</title><categories>cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1210.6927</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a linear system structured into physically coupled
subsystems and propose a decentralized control scheme capable to guarantee
asymptotic stability and satisfaction of constraints on system inputs and
states. The design procedure is totally decentralized, since the synthesis of a
local controller uses only information on a subsystem and its neighbors, i.e.
subsystems coupled to it. We first derive tests for checking if a subsystem can
be plugged into (or unplugged from) an existing plant without spoiling overall
stability and constraint satisfaction. When this is possible, we show how to
automatize the design of local controllers so that it can be carried out in
parallel by smart actuators equipped with computational resources and capable
to exchange information with neighboring subsystems. In particular, local
controllers exploit tube-based Model Predictive Control (MPC) in order to
guarantee robustness with respect to physical coupling among subsystems.
Finally, an application of the proposed control design procedure to frequency
control in power networks is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0234</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0234</id><created>2013-02-01</created><authors><author><keyname>Wang</keyname><forenames>Lin</forenames></author><author><keyname>Anta</keyname><forenames>Antonio Fern&#xe1;ndez</forenames></author><author><keyname>Zhang</keyname><forenames>Fa</forenames></author><author><keyname>Hou</keyname><forenames>Chenying</forenames></author><author><keyname>Liu</keyname><forenames>Zhiyong</forenames></author></authors><title>Routing for Energy Minimization with Discrete Cost Functions</title><categories>cs.NI</categories><comments>14 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy saving is becoming an important issue in the design and use of
computer networks. In this work we propose a problem that considers the use of
rate adaptation as the energy saving strategy in networks. The problem is
modeled as an integral demand-routing problem in a network with discrete cost
functions at the links. The discreteness of the cost function comes from the
different states (bandwidths) at which links can operate and, in particular,
from the energy consumed at each state. This in its turn leads to the
non-convexity of the cost function, and thus adds complexity to solve this
problem. We formulate this routing problem as an integer program, and we show
that the general case of this problem is NP-hard, and even hard to approximate.
For the special case when the step ratio of the cost function is bounded, we
show that effective approximations can be obtained. Our main algorithm executes
two processes in sequence: relaxation and rounding. The relaxation process
eliminates the non-convexity of the cost function, so that the problem is
transformed into a fractional convex program solvable in polynomial time. After
that, a randomized rounding process is used to get a feasible solution for the
original problem. This algorithm provides a constant approximation ratio for
uniform demands and an approximation ratio of $O(\log^{\beta-1} d)$ for
non-uniform demands, where $\beta$ is a constant and $d$ is the largest demand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0249</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0249</id><created>2013-02-01</created><authors><author><keyname>Eksin</keyname><forenames>Ceyhun</forenames></author><author><keyname>Molavi</keyname><forenames>Pooya</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Bayesian Quadratic Network Game Filters</title><categories>cs.SY cs.IT cs.SI math.IT</categories><doi>10.1109/TSP.2014.2309073</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A repeated network game where agents have quadratic utilities that depend on
information externalities -- an unknown underlying state -- as well as payoff
externalities -- the actions of all other agents in the network -- is
considered. Agents play Bayesian Nash Equilibrium strategies with respect to
their beliefs on the state of the world and the actions of all other nodes in
the network. These beliefs are refined over subsequent stages based on the
observed actions of neighboring peers. This paper introduces the Quadratic
Network Game (QNG) filter that agents can run locally to update their beliefs,
select corresponding optimal actions, and eventually learn a sufficient
statistic of the network's state. The QNG filter is demonstrated on a Cournot
market competition game and a coordination game to implement navigation of an
autonomous team.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0250</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0250</id><created>2013-02-01</created><authors><author><keyname>Little</keyname><forenames>Andrew T.</forenames></author><author><keyname>Tucker</keyname><forenames>Joshua A.</forenames></author><author><keyname>LaGatta</keyname><forenames>Tom</forenames></author></authors><title>Elections, Protest, and Alternation of Power</title><categories>physics.soc-ph cs.GT cs.SI math.PR</categories><comments>41 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite many examples to the contrary, most models of elections assume that
rules determining the winner will be followed. We present a model where
elections are solely a public signal of the incumbent popularity, and citizens
can protests against leaders that do not step down from power. In this minimal
setup, rule-based alternation of power as well as &quot;semi-democratic&quot; alternation
of power independent of electoral rules can both arise in equilibrium.
Compliance with electoral rules requires there to be multiple equilibria in the
protest game, where the electoral rule serves as a focal point spurring protest
against losers that do not step down voluntarily. Such multiplicity is possible
when elections are informative and citizens not too polarized. Extensions to
the model are consistent with the facts that protests often center around
accusations of electoral fraud and that in the democratic case turnover is
peaceful while semi-democratic turnover often requires citizens to actually
take to the streets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0264</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0264</id><created>2013-02-01</created><authors><author><keyname>Ghaffari</keyname><forenames>Mohsen</forenames></author><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author><author><keyname>Khabbazian</keyname><forenames>Majid</forenames></author></authors><title>A Bound on the Throughput of Radio Networks</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the well-studied radio network model: a synchronous model with a
graph G=(V,E) with |V|=n where in each round, each node either transmits a
packet, with length B=Omega(log n) bits, or listens. Each node receives a
packet iff it is listening and exactly one of its neighbors is transmitting. We
consider the problem of k-message broadcast, where k messages, each with
Theta(B) bits, are placed in an arbitrary nodes of the graph and the goal is to
deliver all messages to all the nodes. We present a simple proof showing that
there exist a radio network with radius 2 where for any k, broadcasting k
messages requires at least Omega(k log n) rounds. That is, in this network,
regardless of the algorithm, the maximum achievable broadcast throughput is
O(1/log n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0265</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0265</id><created>2013-02-01</created><authors><author><keyname>Mahdavifar</keyname><forenames>Hessam</forenames></author><author><keyname>El-Khamy</keyname><forenames>Mostafa</forenames></author><author><keyname>Lee</keyname><forenames>Jungwon</forenames></author><author><keyname>Kang</keyname><forenames>Inyup</forenames></author></authors><title>Compound Polar Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A capacity-achieving scheme based on polar codes is proposed for reliable
communication over multi-channels which can be directly applied to
bit-interleaved coded modulation schemes. We start by reviewing the
ground-breaking work of polar codes and then discuss our proposed scheme.
Instead of encoding separately across the individual underlying channels, which
requires multiple encoders and decoders, we take advantage of the recursive
structure of polar codes to construct a unified scheme with a single encoder
and decoder that can be used over the multi-channels. We prove that the scheme
achieves the capacity over this multi-channel. Numerical analysis and
simulation results for BICM channels at finite block lengths shows a
considerable improvement in the probability of error comparing to a
conventional separated scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0272</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0272</id><created>2013-02-01</created><authors><author><keyname>AlGhamdi</keyname><forenames>Rayed</forenames></author><author><keyname>Nguyen</keyname><forenames>Ann</forenames></author><author><keyname>Jones</keyname><forenames>Vicki</forenames></author></authors><title>A Study of Influential Factors in the Adoption and Diffusion of B2C
  E-Commerce</title><categories>cs.CY cs.SI</categories><journal-ref>International Journal of Advanced Computer Science and
  Applications 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper looks at the present standing of ecommerce in Saudi Arabia, as
well as the challenges and strengths of Business to Customers (B2C) electronic
commerce. Many studies have been conducted around the world in order to gain a
better understanding of the demands, needs and effectiveness of online
commerce. A study was undertaken to review the literature identifying the
factors influencing the adoption and diffusion of B2C e-commerce. It found four
distinct categories: businesses, customers, environmental and governmental
support, which must all be considered when creating an e-commerce
infrastructure. A concept matrix was used to provide a comparison of important
factors in different parts of the world. The study found that e-commerce in
Saudi Arabia was lacking in Governmental support as well as relevant
involvement by both customers and retailers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0274</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0274</id><created>2013-02-01</created><authors><author><keyname>Peruani</keyname><forenames>Fernando</forenames></author><author><keyname>Tabourier</keyname><forenames>Lionel</forenames></author></authors><title>Directedness of information flow in mobile phone communication networks</title><categories>physics.soc-ph cs.SI</categories><journal-ref>PLoS ONE 6(12), e28860 (2011)</journal-ref><doi>10.1371/journal.pone.0028860</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Without having direct access to the information that is being exchanged,
traces of information flow can be obtained by looking at temporal sequences of
user interactions. These sequences can be represented as causality trees whose
statistics result from a complex interplay between the topology of the
underlying (social) network and the time correlations among the communications.
Here, we study causality trees in mobile-phone data, which can be represented
as a dynamical directed network. This representation of the data reveals the
existence of super-spreaders and super-receivers. We show that the tree
statistics, respectively the information spreading process, are extremely
sensitive to the in-out degree correlation exhibited by the users. We also
learn that a given information, e.g., a rumor, would require users to
retransmit it for more than 30 hours in order to cover a macroscopic fraction
of the system. Our analysis indicates that topological node-node correlations
of the underlying social network, while allowing the existence of information
loops, they also promote information spreading. Temporal correlations, and
therefore causality effects, are only visible as local phenomena and during
short time scales. These results are obtained through a combination of theory
and data analysis techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0275</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0275</id><created>2013-02-01</created><updated>2014-07-11</updated><authors><author><keyname>Harsha</keyname><forenames>Prahladh</forenames></author><author><keyname>Jain</keyname><forenames>Rahul</forenames></author></authors><title>A strong direct product theorem for the tribes function via the
  smooth-rectangle bound</title><categories>cs.CC</categories><comments>16 pages (including title page), fixed error in earlier version</comments><journal-ref>In Proc. 33rd IARCS Conf. on Foundations of Software Technology &amp;
  Theoretical Computer Science (FSTTCS) (Guwahati, India, 12-14 December),
  pages 141-152, 2013</journal-ref><doi>10.4230/LIPIcs.FSTTCS.2013.141</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main result of this paper is an optimal strong direct product result for
the two-party public-coin randomized communication complexity of the Tribes
function. This is proved by providing an alternate proof of the optimal lower
bound of \Omega(n) for the randomised communication complexity of the Tribes
function using the so-called smooth-rectangle bound, introduced by Jain and
Klauck [JK10]. The optimal \Omega(n) lower bound for Tribes was originally
proved by Jayram, Kumar and Sivakumar [JKS03], using a more powerful lower
bound technique, namely the information complexity bound. The information
complexity bound is known to be at least as strong a lower bound method as the
smooth-rectangle bound [KLL+12]. On the other hand, we are not aware of any
function or relation for which the smooth-rectangle bound is (asymptotically)
smaller than its public-coin randomized communication complexity. The optimal
direct product for Tribes is obtained by combining our smooth-rectangle bound
for tribes with the strong direct product result of Jain and Yao [JY12] in
terms of smooth-rectangle bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0286</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0286</id><created>2013-02-01</created><authors><author><keyname>Fuhrman</keyname><forenames>Marco</forenames><affiliation>Dipartimento Di Matematica</affiliation></author><author><keyname>Hu</keyname><forenames>Ying</forenames><affiliation>IRMAR</affiliation></author><author><keyname>Tessitore</keyname><forenames>Gianmario</forenames></author></authors><title>Stochastic maximum principle for optimal control of SPDEs</title><categories>math.OC cs.SY math.PR</categories><comments>This is the long version of arXiv:1206.2119</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a version of the maximum principle, in the sense of Pontryagin, for
the optimal control of a stochastic partial differential equation driven by a
finite dimensional Wiener process. The equation is formulated in a
semi-abstract form that allows direct applications to a large class of
controlled stochastic parabolic equations. We allow for a diffusion coefficient
dependent on the control parameter, and the space of control actions is
general, so that in particular we need to introduce two adjoint processes. The
second adjoint process takes values in a suitable space of operators on $L^4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0290</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0290</id><created>2013-02-01</created><authors><author><keyname>Gosset</keyname><forenames>David</forenames></author><author><keyname>Nagaj</keyname><forenames>Daniel</forenames></author></authors><title>Quantum 3-SAT is QMA1-complete</title><categories>quant-ph cs.CC</categories><journal-ref>2013 IEEE 54TH annual symposium on Foundations of Computer Science
  (FOCS 2013), Pages: 756-765 (2013)</journal-ref><doi>10.1109/FOCS.2013.86</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum satisfiability is a constraint satisfaction problem that generalizes
classical boolean satisfiability. In the quantum k-SAT problem, each constraint
is specified by a k-local projector and is satisfied by any state in its
nullspace. Bravyi showed that quantum 2-SAT can be solved efficiently on a
classical computer and that quantum k-SAT with k greater than or equal to 4 is
QMA1-complete. Quantum 3-SAT was known to be contained in QMA1, but its
computational hardness was unknown until now. We prove that quantum 3-SAT is
QMA1-hard, and therefore complete for this complexity class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0296</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0296</id><created>2013-02-01</created><updated>2014-12-17</updated><authors><author><keyname>Naderializadeh</keyname><forenames>Navid</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>Interference Networks with No CSIT: Impact of Topology</title><categories>cs.IT math.IT</categories><comments>Accepted for Publication in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider partially-connected $K$-user interference networks, where the
transmitters have no knowledge about the channel gain values, but they are
aware of network topology (or connectivity). We introduce several linear
algebraic and graph theoretic concepts to derive new topology-based outer
bounds and inner bounds on the symmetric degrees-of-freedom (DoF) of these
networks. We evaluate our bounds for two classes of networks to demonstrate
their tightness for most networks in these classes, quantify the gain of our
inner bounds over benchmark interference management strategies, and illustrate
the effect of network topology on these gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0304</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0304</id><created>2013-02-01</created><authors><author><keyname>Dujmovic</keyname><forenames>Vida</forenames></author></authors><title>Graph Layouts via Layered Separators</title><categories>cs.CG cs.DS</categories><msc-class>05C</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A k-queue layout of a graph consists of a total order of the vertices, and a
partition of the edges into k sets such that no two edges that are in the same
set are nested with respect to the vertex ordering. A k-track layout of a graph
consists of a vertex k-colouring, and a total order of each vertex colour
class, such that between each pair of colour classes no two edges cross. The
queue-number (track-number) of a graph G, is the minimum k such that G has a
k-queue (k-track) layout.
  This paper proves that every n-vertex planar graph has track number and queue
number at most O(log n). This improves the result of Di Battista, Frati and
Pach [Foundations of Computer Science, (FOCS '10), pp. 365--374] who proved the
first sub-polynomial bounds on the queue number and track number of planar
graphs. Specifically, they obtained O(log^2 n) queue number and O(log^8 n)
track number bounds for planar graphs.
  The result also implies that every planar graph has a 3D crossing-free grid
drawing in O(n log n) volume. The proof uses a non-standard type of graph
separators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0309</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0309</id><created>2013-02-01</created><updated>2013-10-06</updated><authors><author><keyname>Bailis</keyname><forenames>Peter</forenames></author><author><keyname>Davidson</keyname><forenames>Aaron</forenames></author><author><keyname>Fekete</keyname><forenames>Alan</forenames></author><author><keyname>Ghodsi</keyname><forenames>Ali</forenames></author><author><keyname>Hellerstein</keyname><forenames>Joseph M.</forenames></author><author><keyname>Stoica</keyname><forenames>Ion</forenames></author></authors><title>Highly Available Transactions: Virtues and Limitations (Extended
  Version)</title><categories>cs.DB</categories><comments>Extended version of &quot;Highly Available Transactions: Virtues and
  Limitations&quot; to appear in VLDB 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  To minimize network latency and remain online during server failures and
network partitions, many modern distributed data storage systems eschew
transactional functionality, which provides strong semantic guarantees for
groups of multiple operations over multiple data items. In this work, we
consider the problem of providing Highly Available Transactions (HATs):
transactional guarantees that do not suffer unavailability during system
partitions or incur high network latency. We introduce a taxonomy of highly
available systems and analyze existing ACID isolation and distributed data
consistency guarantees to identify which can and cannot be achieved in HAT
systems. This unifies the literature on weak transactional isolation, replica
consistency, and highly available systems. We analytically and experimentally
quantify the availability and performance benefits of HATs--often two to three
orders of magnitude over wide-area networks--and discuss their necessary
semantic compromises.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0315</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0315</id><created>2013-02-01</created><authors><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Mahdavi</keyname><forenames>Mehrdad</forenames></author></authors><title>Sparse Multiple Kernel Learning with Geometric Convergence Rate</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of sparse multiple kernel learning (MKL),
where the goal is to efficiently learn a combination of a fixed small number of
kernels from a large pool that could lead to a kernel classifier with a small
prediction error. We develop an efficient algorithm based on the greedy
coordinate descent algorithm, that is able to achieve a geometric convergence
rate under appropriate conditions. The convergence rate is achieved by
measuring the size of functional gradients by an empirical $\ell_2$ norm that
depends on the empirical data distribution. This is in contrast to previous
algorithms that use a functional norm to measure the size of gradients, which
is independent from the data samples. We also establish a generalization error
bound of the learned sparse kernel classifier using the technique of local
Rademacher complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0317</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0317</id><created>2013-02-01</created><authors><author><keyname>Krzhizhanovskaya</keyname><forenames>V. V.</forenames></author><author><keyname>Melnikova</keyname><forenames>N. B.</forenames></author><author><keyname>Chirkin</keyname><forenames>A. M.</forenames></author><author><keyname>Ivanov</keyname><forenames>S. V.</forenames></author><author><keyname>Boukhanovsky</keyname><forenames>A. V.</forenames></author><author><keyname>Sloot</keyname><forenames>P. M. A.</forenames></author></authors><title>Distributed simulation of city inundation by coupled surface and
  subsurface porous flow for urban flood decision support system</title><categories>cs.CE</categories><comments>Pre-print submitted to the 2013 International Conference on
  Computational Science</comments><journal-ref>Procedia Computer Science, Volume 18, 2013, Pages 1046-1056</journal-ref><doi>10.1016/j.procs.2013.05.270</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a decision support system for flood early warning and disaster
management. It includes the models for data-driven meteorological predictions,
for simulation of atmospheric pressure, wind, long sea waves and seiches; a
module for optimization of flood barrier gates operation; models for stability
assessment of levees and embankments, for simulation of city inundation
dynamics and citizens evacuation scenarios. The novelty of this paper is a
coupled distributed simulation of surface and subsurface flows that can predict
inundation of low-lying inland zones far from the submerged waterfront areas,
as observed in St. Petersburg city during the floods. All the models are
wrapped as software services in the CLAVIRE platform for urgent computing,
which provides workflow management and resource orchestration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0320</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0320</id><created>2013-02-01</created><updated>2013-06-22</updated><authors><author><keyname>Lin</keyname><forenames>Xingqin</forenames></author><author><keyname>Viswanathan</keyname><forenames>Harish</forenames></author></authors><title>Dynamic Spectrum Refarming with Overlay for Legacy Devices</title><categories>cs.NI</categories><comments>12 pages, 14 figures, submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The explosive growth in data traffic is resulting in a spectrum crunch
forcing many wireless network operators to look towards refarming their 2G
spectrum and deploy more spectrally efficient Long Term Evolution (LTE)
technology. However, mobile network operators face a challenge when it comes to
spectrum refarming because 2G technologies such as Global System for Mobile
(GSM) is still widely used for low bandwidth machine-to-machine (M2M) devices.
M2M devices typically have long life cycles, e.g. smart meters, and it is
expensive to migrate these devices to newer technology since a truck roll will
typically be required to the site where a device is deployed. Furthermore, with
cost of 2G modules several times less than that of LTE, even newly deployed M2M
devices tend to adopt 2G technology. Nevertheless, operators are keen to either
force their 2G M2M customers to migrate so that they can refarm the spectrum or
set aside a portion of the 2G spectrum for continuing operating 2G and only
refarm the rest for LTE. In this paper we propose a novel solution to provide
GSM connectivity within an LTE carrier through an efficient overlay by
reserving a few physical resource blocks for GSM. With this approach, operators
can refarm their 2G spectrum to LTE efficiently while still providing some GSM
connectivity to their low data rate M2M customers. Furthermore, spectrum can be
dynamically shared between LTE and GSM. An approach similar to that proposed in
this paper can also be applied for other narrow band technology overlays over
LTE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0321</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0321</id><created>2013-02-01</created><authors><author><keyname>Tan</keyname><forenames>Jin</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author></authors><title>Signal reconstruction in linear mixing systems with different error
  metrics</title><categories>cs.IT math.IT</categories><comments>ITA 2013. arXiv admin note: substantial text overlap with
  arXiv:1207.1760</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of reconstructing a signal from noisy measurements in
linear mixing systems. The reconstruction performance is usually quantified by
standard error metrics such as squared error, whereas we consider any additive
error metric. Under the assumption that relaxed belief propagation (BP) can
compute the posterior in the large system limit, we propose a simple, fast, and
highly general algorithm that reconstructs the signal by minimizing the
user-defined error metric. For two example metrics, we provide performance
analysis and convincing numerical results. Finally, our algorithm can be
adjusted to minimize the $\ell_\infty$ error, which is not additive.
Interestingly, $\ell_{\infty}$ minimization only requires to apply a Wiener
filter to the output of relaxed BP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0324</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0324</id><created>2013-02-01</created><authors><author><keyname>Muzhou</keyname><forenames>Hou</forenames></author><author><keyname>Lee</keyname><forenames>Moon Ho</forenames></author></authors><title>A New Constructive Method to Optimize Neural Network Architecture and
  Generalization</title><categories>cs.NE</categories><msc-class>41A99, 65D15</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, after analyzing the reasons of poor generalization and
overfitting in neural networks, we consider some noise data as a singular value
of a continuous function - jump discontinuity point. The continuous part can be
approximated with the simplest neural networks, which have good generalization
performance and optimal network architecture, by traditional algorithms such as
constructive algorithm for feed-forward neural networks with incremental
training, BP algorithm, ELM algorithm, various constructive algorithm, RBF
approximation and SVM. At the same time, we will construct RBF neural networks
to fit the singular value with every error in, and we prove that a function
with jumping discontinuity points can be approximated by the simplest neural
networks with a decay RBF neural networks in by each error, and a function with
jumping discontinuity point can be constructively approximated by a decay RBF
neural networks in by each error and the constructive part have no
generalization influence to the whole machine learning system which will
optimize neural network architecture and generalization performance, reduce the
overfitting phenomenon by avoid fitting the noisy data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0328</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0328</id><created>2013-02-01</created><updated>2014-04-09</updated><authors><author><keyname>Archer</keyname><forenames>Evan</forenames></author><author><keyname>Park</keyname><forenames>Il Memming</forenames></author><author><keyname>Pillow</keyname><forenames>Jonathan</forenames></author></authors><title>Bayesian Entropy Estimation for Countable Discrete Distributions</title><categories>cs.IT math.IT</categories><comments>38 pages LaTeX. Revised and resubmitted to JMLR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating Shannon's entropy $H$ from discrete
data, in cases where the number of possible symbols is unknown or even
countably infinite. The Pitman-Yor process, a generalization of Dirichlet
process, provides a tractable prior distribution over the space of countably
infinite discrete distributions, and has found major applications in Bayesian
non-parametric statistics and machine learning. Here we show that it also
provides a natural family of priors for Bayesian entropy estimation, due to the
fact that moments of the induced posterior distribution over $H$ can be
computed analytically. We derive formulas for the posterior mean (Bayes' least
squares estimate) and variance under Dirichlet and Pitman-Yor process priors.
Moreover, we show that a fixed Dirichlet or Pitman-Yor process prior implies a
narrow prior distribution over $H$, meaning the prior strongly determines the
entropy estimate in the under-sampled regime. We derive a family of continuous
mixing measures such that the resulting mixture of Pitman-Yor processes
produces an approximately flat prior over $H$. We show that the resulting
Pitman-Yor Mixture (PYM) entropy estimator is consistent for a large class of
distributions. We explore the theoretical properties of the resulting
estimator, and show that it performs well both in simulation and in application
to real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0334</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0334</id><created>2013-02-01</created><authors><author><keyname>Buehrer</keyname><forenames>Daniel</forenames></author><author><keyname>Lee</keyname><forenames>Chee-Hwa</forenames></author></authors><title>Class Algebra for Ontology Reasoning</title><categories>cs.AI</categories><comments>pp.2-13</comments><journal-ref>Proc. of TOOLS Asia 99 (Technology of Object-Oriented Languages
  and Systems, 1999 International Conference), IEEE Press</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Class algebra provides a natural framework for sharing of ISA hierarchies
between users that may be unaware of each other's definitions. This permits
data from relational databases, object-oriented databases, and tagged XML
documents to be unioned into one distributed ontology, sharable by all users
without the need for prior negotiation or the development of a &quot;standard&quot;
ontology for each field. Moreover, class algebra produces a functional
correspondence between a class's class algebraic definition (i.e. its &quot;intent&quot;)
and the set of all instances which satisfy the expression (i.e. its &quot;extent&quot;).
The framework thus provides assistance in quickly locating examples and
counterexamples of various definitions. This kind of information is very
valuable when developing models of the real world, and serves as an invaluable
tool assisting in the proof of theorems concerning these class algebra
expressions. Finally, the relative frequencies of objects in the ISA hierarchy
can produce a useful Boolean algebra of probabilities. The probabilities can be
used by traditional information-theoretic classification methodologies to
obtain optimal ways of classifying objects in the database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0336</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0336</id><created>2013-02-01</created><updated>2013-10-15</updated><authors><author><keyname>Guntuboyina</keyname><forenames>Adityanand</forenames></author><author><keyname>Saha</keyname><forenames>Sujayam</forenames></author><author><keyname>Schiebinger</keyname><forenames>Geoffrey</forenames></author></authors><title>Sharp Inequalities for $f$-divergences</title><categories>math.ST cs.IT math.IT math.OC math.PR stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $f$-divergences are a general class of divergences between probability
measures which include as special cases many commonly used divergences in
probability, mathematical statistics and information theory such as
Kullback-Leibler divergence, chi-squared divergence, squared Hellinger
distance, total variation distance etc. In this paper, we study the problem of
maximizing or minimizing an $f$-divergence between two probability measures
subject to a finite number of constraints on other $f$-divergences. We show
that these infinite-dimensional optimization problems can all be reduced to
optimization problems over small finite dimensional spaces which are tractable.
Our results lead to a comprehensive and unified treatment of the problem of
obtaining sharp inequalities between $f$-divergences. We demonstrate that many
of the existing results on inequalities between $f$-divergences can be obtained
as special cases of our results and we also improve on some existing non-sharp
inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0337</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0337</id><created>2013-02-01</created><authors><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author></authors><title>Perancangan basisdata sistem informasi penggajian</title><categories>cs.DB</categories><comments>18 pages</comments><journal-ref>MATRIK. 8 (2006) 135-152</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this research is to design database scheme of information
system at XYZ University. By using database design methods (conceptual scheme,
logical scheme, &amp; physical scheme) the writer designs payroll information
system. The physical scheme is compatible with Borland Delphi Database Engine
Scheme to support the implementation of the I.S. After 3 (three) steps we get 7
(seven) tables, dan 6 (six) forms. By using this shemce, the system can produce
several reports quickly, accurately, efficiently, and effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0338</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0338</id><created>2013-02-01</created><authors><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author><author><keyname>Emigawaty</keyname></author></authors><title>Analisis laporan tugas akhir mahasiswa Diploma I dari sudut pandang
  kaidah ilmiah dan penggunaan teknologi informasi</title><categories>cs.OH</categories><comments>18 pages, scientific journal</comments><journal-ref>MATRIK. 11 (2009) 19-36</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purposes of this research are: 1) to analyze final report from scientific
role, 2) the use of information technology (IT), and 3) to conduct academic
athmosphere in research area. This research gives contributions to study
program of MI-DI, such as: 1) to know the pattern of student final report from
scientific role and the use of IT, 2) give input to study program for next
final report scheme, and 3) can be used for next research reference. If we look
to the quality of final report, there are several focuses to be prepared on
tittle, literature review, methodology, results of report, discussion and also
conclusion. But for the use of IT is already good but the varian is decrease.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0347</identifier>
 <datestamp>2013-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0347</id><created>2013-02-02</created><updated>2013-09-07</updated><authors><author><keyname>Rastaghi</keyname><forenames>Roohallah</forenames></author></authors><title>An Efficient CCA2-Secure Variant of the McEliece Cryptosystem in the
  Standard Model</title><categories>cs.CR cs.IT math.IT</categories><comments>Submited. arXiv admin note: text overlap with arXiv:1205.5224 by
  other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a few chosen-ciphertext secure (CCA2-secure) variants of the
McEliece public-key encryption (PKE) scheme in the standard model were
introduced. All the proposed schemes are based on encryption repetition
paradigm and use general transformation from CPA-secure scheme to a CCA2-secure
one. Therefore, the resulting encryption scheme needs \textit{separate}
encryption and has \textit{large} key size compared to the original scheme,
which complex public key size problem in the code-based PKE schemes. Thus, the
proposed schemes are not sufficiently efficient to be used in practice.
  In this work, we propose an efficient CCA2-secure variant of the McEliece PKE
scheme in the standard model. The main novelty is that, unlike previous
approaches, our approach is a generic conversion and can be applied to
\textit{any} one-way trapdoor function (OW-TDF), the lowest-level security
notion in the context of public-key cryptography, resolving a big fundamental
and central problem that has remained unsolved in the past two decades.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0351</identifier>
 <datestamp>2013-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0351</id><created>2013-02-02</created><updated>2013-06-11</updated><authors><author><keyname>Saxena</keyname><forenames>Gaurav</forenames></author><author><keyname>Narula</keyname><forenames>Ruchi</forenames></author><author><keyname>Mishra</keyname><forenames>Manish</forenames></author></authors><title>New Dimension Value Introduction for In-Memory What-If Analysis</title><categories>cs.DB</categories><comments>Changes from previous version: 1. Changed references format 2. Added
  a few more references 3. Added an algorithm to create sub-cube 4. Modified
  algorithm to process a query for a few errors 5. Rephrased sentences for
  clarity</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  OLAP systems operate on historical data and provide answers to analysts
queries. Recent in-memory implementations provide significant performance
improvement for real time ad-hoc analysis. Philosophy and techniques of what-if
analysis on data warehouse and in-memory data store based OLAP systems have
been covered in great detail before but exploration of new dimension value
(attribute) introduction has been limited in the context of what-if analysis.
We extend the approach of Andrey Balmin et al of using select modify operator
on data graph to introduce new values for dimensions and measures in a
read-only in-memory data store as scenarios. Our system constructs scenarios
without materializing the rows and stores the row information as queries. The
rows associated with the scenarios are constructed as and when required by an
ad-hoc query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0366</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0366</id><created>2013-02-02</created><authors><author><keyname>Sawerwain</keyname><forenames>Marek</forenames><affiliation>Institute of Control &amp; Computation Engineering, University of Zielona G&#xf3;ra, Poland</affiliation></author><author><keyname>Wi&#x15b;niewska</keyname><forenames>Joanna</forenames><affiliation>Institute of Information Systems, Faculty of Cybernetics, Military University of Technology, Poland</affiliation></author></authors><title>Transfers of entanglement qudit states in quantum networks</title><categories>quant-ph cs.OH physics.comp-ph</categories><comments>10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The issue of quantum states' transfer -- in particular, for so-called Perfect
State Transfer (PST) -- in the networks represented by the spin chains seems to
be one of the major concerns in quantum computing. Especially, in the context
of future communication methods that can be used in broadly defined computer
science. The chapter presents a definition of Hamiltonian describing the
dynamics of quantum data transfer in one-dimensional spin chain, which is able
to transfer the state of unknown qudits. The main part of the chapter is the
discussion about possibility of entangled states' perfect transfer, in
particular, for the generalized Bell states for qudits. One of the sections
also contains the results of numerical experiments for the transmission of
quantum entangled state in a noisy quantum channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0378</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0378</id><created>2013-02-02</created><authors><author><keyname>Ablinger</keyname><forenames>Jakob</forenames></author><author><keyname>Bl&#xfc;mlein</keyname><forenames>Johannes</forenames></author><author><keyname>Schneider</keyname><forenames>Carsten</forenames></author></authors><title>Analytic and Algorithmic Aspects of Generalized Harmonic Sums and
  Polylogarithms</title><categories>math-ph cs.SC hep-ph hep-th math.MP</categories><comments>75 pages, 1 figure</comments><report-no>DESY 12--210,DO--TH 13/01,SFB/CPP-12-91,LPN 12-125</report-no><doi>10.1063/1.4811117</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent three--loop calculations of massive Feynman integrals within
Quantum Chromodynamics (QCD) and, e.g., in recent combinatorial problems the
so-called generalized harmonic sums (in short $S$-sums) arise. They are
characterized by rational (or real) numerator weights also different from $\pm
1$. In this article we explore the algorithmic and analytic properties of these
sums systematically. We work out the Mellin and inverse Mellin transform which
connects the sums under consideration with the associated Poincar\'{e} iterated
integrals, also called generalized harmonic polylogarithms. In this regard, we
obtain explicit analytic continuations by means of asymptotic expansions of the
$S$-sums which started to occur frequently in current QCD calculations. In
addition, we derive algebraic and structural relations, like differentiation
w.r.t. the external summation index and different multi-argument relations, for
the compactification of $S$-sum expressions. Finally, we calculate algebraic
relations for infinite $S$-sums, or equivalently for generalized harmonic
polylogarithms evaluated at special values. The corresponding algorithms and
relations are encoded in the computer algebra package {\tt HarmonicSums}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0380</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0380</id><created>2013-02-02</created><updated>2015-12-01</updated><authors><author><keyname>Roux</keyname><forenames>St&#xe9;phane Le</forenames><affiliation>TU Darmstadt</affiliation></author><author><keyname>Pauly</keyname><forenames>Arno</forenames><affiliation>University of Cambridge</affiliation></author></authors><title>Finite choice, convex choice and finding roots</title><categories>math.LO cs.CG</categories><comments>An earlier version was titled &quot;Closed choice: Cardinality vs convex
  dimension&quot;</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (4:6) 2015</journal-ref><doi>10.2168/LMCS-11(4:6)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate choice principles in the Weihrauch lattice for finite sets on
the one hand, and convex sets on the other hand. Increasing cardinality and
increasing dimension both correspond to increasing Weihrauch degrees. Moreover,
we demonstrate that the dimension of convex sets can be characterized by the
cardinality of finite sets encodable into them. Precisely, choice from an n+1
point set is reducible to choice from a convex set of dimension n, but not
reducible to choice from a convex set of dimension n-1. Furthermore we consider
searching for zeros of continuous functions. We provide an algorithm producing
3n real numbers containing all zeros of a continuous function with up to n
local minima. This demonstrates that having finitely many zeros is a strictly
weaker condition than having finitely many local extrema. We can prove 3n to be
optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0386</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0386</id><created>2013-02-02</created><authors><author><keyname>Koos</keyname><forenames>Sylvain</forenames></author><author><keyname>Cully</keyname><forenames>Antoine</forenames></author><author><keyname>Mouret</keyname><forenames>Jean-Baptiste</forenames></author></authors><title>Fast Damage Recovery in Robotics with the T-Resilience Algorithm</title><categories>cs.RO cs.AI cs.LG</categories><doi>10.1177/0278364913499192</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Damage recovery is critical for autonomous robots that need to operate for a
long time without assistance. Most current methods are complex and costly
because they require anticipating each potential damage in order to have a
contingency plan ready. As an alternative, we introduce the T-resilience
algorithm, a new algorithm that allows robots to quickly and autonomously
discover compensatory behaviors in unanticipated situations. This algorithm
equips the robot with a self-model and discovers new behaviors by learning to
avoid those that perform differently in the self-model and in reality. Our
algorithm thus does not identify the damaged parts but it implicitly searches
for efficient behaviors that do not use them. We evaluate the T-Resilience
algorithm on a hexapod robot that needs to adapt to leg removal, broken legs
and motor failures; we compare it to stochastic local search, policy gradient
and the self-modeling algorithm proposed by Bongard et al. The behavior of the
robot is assessed on-board thanks to a RGB-D sensor and a SLAM algorithm. Using
only 25 tests on the robot and an overall running time of 20 minutes,
T-Resilience consistently leads to substantially better results than the other
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0393</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0393</id><created>2013-02-02</created><authors><author><keyname>Coecke</keyname><forenames>Bob</forenames></author><author><keyname>Grefenstette</keyname><forenames>Edward</forenames></author><author><keyname>Sadrzadeh</keyname><forenames>Mehrnoosh</forenames></author></authors><title>Lambek vs. Lambek: Functorial Vector Space Semantics and String Diagrams
  for Lambek Calculus</title><categories>math.LO cs.CL math.CT</categories><comments>29 pages, pending publication in Annals of Pure and Applied Logic</comments><msc-class>16B50, 18A10, 68T50</msc-class><acm-class>G.1.3; F.4.1; F.4.2; I.2.7</acm-class><doi>10.1016/j.apal.2013.05.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Distributional Compositional Categorical (DisCoCat) model is a
mathematical framework that provides compositional semantics for meanings of
natural language sentences. It consists of a computational procedure for
constructing meanings of sentences, given their grammatical structure in terms
of compositional type-logic, and given the empirically derived meanings of
their words. For the particular case that the meaning of words is modelled
within a distributional vector space model, its experimental predictions,
derived from real large scale data, have outperformed other empirically
validated methods that could build vectors for a full sentence. This success
can be attributed to a conceptually motivated mathematical underpinning, by
integrating qualitative compositional type-logic and quantitative modelling of
meaning within a category-theoretic mathematical framework.
  The type-logic used in the DisCoCat model is Lambek's pregroup grammar.
Pregroup types form a posetal compact closed category, which can be passed, in
a functorial manner, on to the compact closed structure of vector spaces,
linear maps and tensor product. The diagrammatic versions of the equational
reasoning in compact closed categories can be interpreted as the flow of word
meanings within sentences. Pregroups simplify Lambek's previous type-logic, the
Lambek calculus, which has been extensively used to formalise and reason about
various linguistic phenomena. The apparent reliance of the DisCoCat on
pregroups has been seen as a shortcoming. This paper addresses this concern, by
pointing out that one may as well realise a functorial passage from the
original type-logic of Lambek, a monoidal bi-closed category, to vector spaces,
or to any other model of meaning organised within a monoidal bi-closed
category. The corresponding string diagram calculus, due to Baez and Stay, now
depicts the flow of word meanings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0394</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0394</id><created>2013-02-02</created><authors><author><keyname>Liu</keyname><forenames>Xiaogang</forenames></author><author><keyname>Luo</keyname><forenames>Yuan</forenames></author></authors><title>The weight distributions of some cyclic codes with three or four
  nonzeros over F3</title><categories>cs.IT math.IT</categories><comments>22 pages, 3 tables</comments><msc-class>68P30 (Primary),</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Because of efficient encoding and decoding algorithms, cyclic codes are an
important family of linear block codes, and have applications in communica-
tion and storage systems. However, their weight distributions are known only
for a few cases mainly on the codes with one or two nonzeros. In this paper,
the weight distributions of two classes of cyclic codes with three or four
nonzeros are determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0398</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0398</id><created>2013-02-02</created><authors><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author><author><keyname>Landon-Cardinal</keyname><forenames>Olivier</forenames></author><author><keyname>Hayden</keyname><forenames>Patrick</forenames></author></authors><title>Towards efficient decoding of classical-quantum polar codes</title><categories>quant-ph cs.IT math.IT</categories><comments>21 pages, 2 figures, submission to the 8th Conference on the Theory
  of Quantum Computation, Communication, and Cryptography</comments><journal-ref>8th Conference on the Theory of Quantum Computation, Communication
  and Cryptography (TQC 2013), pages 157-177 (May 2013)</journal-ref><doi>10.4230/LIPIcs.TQC.2013.157</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Known strategies for sending bits at the capacity rate over a general channel
with classical input and quantum output (a cq channel) require the decoder to
implement impractically complicated collective measurements. Here, we show that
a fully collective strategy is not necessary in order to recover all of the
information bits. In fact, when coding for a large number N uses of a cq
channel W, N I(W_acc) of the bits can be recovered by a non-collective strategy
which amounts to coherent quantum processing of the results of product
measurements, where I(W_acc) is the accessible information of the channel W. In
order to decode the other N (I(W) - I(W_acc)) bits, where I(W) is the Holevo
rate, our conclusion is that the receiver should employ collective
measurements. We also present two other results: 1) collective Fuchs-Caves
measurements (quantum likelihood ratio measurements) can be used at the
receiver to achieve the Holevo rate and 2) we give an explicit form of the
Helstrom measurements used in small-size polar codes. The main approach used to
demonstrate these results is a quantum extension of Arikan's polar codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0404</identifier>
 <datestamp>2013-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0404</id><created>2013-02-02</created><updated>2013-05-27</updated><authors><author><keyname>Chudnovsky</keyname><forenames>Maria</forenames></author><author><keyname>Maceli</keyname><forenames>Peter</forenames></author></authors><title>Simplicial vertices in graphs with no induced four-edge path or
  four-edge antipath, and the $H_6$-conjecture</title><categories>math.CO cs.DM</categories><comments>15 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathcal{G}$ be the class of all graphs with no induced four-edge path
or four-edge antipath. Hayward and Nastos \cite{MS} conjectured that every
prime graph in $\mathcal{G}$ not isomorphic to the cycle of length five is
either a split graph or contains a certain useful arrangement of simplicial and
antisimplicial vertices. In this paper we give a counterexample to their
conjecture, and prove a slightly weaker version. Additionally, applying a
result of the first author and Seymour \cite{grow} we give a short proof of
Fouquet's result \cite{C5} on the structure of the subclass of bull-free graphs
contained in $\mathcal{G}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0405</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0405</id><created>2013-02-02</created><updated>2015-12-07</updated><authors><author><keyname>Chudnovsky</keyname><forenames>Maria</forenames></author><author><keyname>Maceli</keyname><forenames>Peter</forenames></author><author><keyname>Penev</keyname><forenames>Irena</forenames></author></authors><title>Excluding four-edge paths and their complements</title><categories>math.CO cs.DM</categories><comments>28 pages, the paper arXiv:1410.0871 resulted from the merging of this
  manuscript together with 'On $(P_5, \overline{P_5})$-free graphs', by L.
  Esperet, L. Lemoine, and F. Maffray (2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that a graph G contains no induced four-edge path and no induced
complement of a four-edge path if and only if G is obtained from five-cycles
and split graphs by repeatedly applying the following operations: substitution,
split graph unification, and split graph unification in the complement (&quot;split
graph unification&quot; is a new class-preserving operation that is introduced in
this paper).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0406</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0406</id><created>2013-02-02</created><authors><author><keyname>Kar</keyname><forenames>Purushottam</forenames></author></authors><title>Generalization Guarantees for a Binary Classification Framework for
  Two-Stage Multiple Kernel Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present generalization bounds for the TS-MKL framework for two stage
multiple kernel learning. We also present bounds for sparse kernel learning
formulations within the TS-MKL framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0413</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0413</id><created>2013-02-02</created><authors><author><keyname>Moreira</keyname><forenames>Catarina</forenames></author><author><keyname>Calado</keyname><forenames>P&#xe1;vel</forenames></author><author><keyname>Martins</keyname><forenames>Bruno</forenames></author></authors><title>Learning to Rank for Expert Search in Digital Libraries of Academic
  Publications</title><categories>cs.IR cs.DL</categories><journal-ref>Progress in Artificial Intelligence, Lecture Notes in Computer
  Science, Springer Berlin Heidelberg. In Proceedings of the 15th Portuguese
  Conference on Artificial Intelligence, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of expert finding has been getting increasing attention in
information retrieval literature. However, the current state-of-the-art is
still lacking in principled approaches for combining different sources of
evidence in an optimal way. This paper explores the usage of learning to rank
methods as a principled approach for combining multiple estimators of
expertise, derived from the textual contents, from the graph-structure with the
citation patterns for the community of experts, and from profile information
about the experts. Experiments made over a dataset of academic publications,
for the area of Computer Science, attest for the adequacy of the proposed
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0418</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0418</id><created>2013-02-02</created><authors><author><keyname>Gur</keyname><forenames>Tom</forenames></author><author><keyname>Raz</keyname><forenames>Ran</forenames></author></authors><title>Arthur-Merlin Streaming Complexity</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the power of Arthur-Merlin probabilistic proof systems in the data
stream model. We show a canonical $\mathcal{AM}$ streaming algorithm for a wide
class of data stream problems. The algorithm offers a tradeoff between the
length of the proof and the space complexity that is needed to verify it.
  As an application, we give an $\mathcal{AM}$ streaming algorithm for the
\emph{Distinct Elements} problem. Given a data stream of length $m$ over
alphabet of size $n$, the algorithm uses $\tilde O(s)$ space and a proof of
size $\tilde O(w)$, for every $s,w$ such that $s \cdot w \ge n$ (where $\tilde
O$ hides a $\polylog(m,n)$ factor). We also prove a lower bound, showing that
every $\mathcal{MA}$ streaming algorithm for the \emph{Distinct Elements}
problem that uses $s$ bits of space and a proof of size $w$, satisfies $s \cdot
w = \Omega(n)$.
  As a part of the proof of the lower bound for the \emph{Distinct Elements}
problem, we show a new lower bound of $\Omega(\sqrt n)$ on the $\mathcal{MA}$
communication complexity of the \emph{Gap Hamming Distance} problem, and prove
its tightness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0420</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0420</id><created>2013-02-02</created><updated>2013-10-16</updated><authors><author><keyname>Couto</keyname><forenames>Francisco M</forenames></author><author><keyname>Faria</keyname><forenames>Daniel</forenames></author><author><keyname>Tavares</keyname><forenames>Bruno</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Pedro</forenames></author><author><keyname>Verissimo</keyname><forenames>Paulo</forenames></author></authors><title>Benchmarking some Portuguese S&amp;T system research units: 2nd Edition</title><categories>cs.DL cs.IR</categories><comments>26 pages, 20 figures F. Couto, D. Faria, B. Tavares, P.
  Gon\c{c}alves, and P. Verissimo, Benchmarking some portuguese S\&amp;T system
  research units: 2nd edition, DI/FCUL TR 13-03, Department of Informatics,
  University of Lisbon, February 2013</comments><report-no>DI--FCUL--TR--2013--03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing use of productivity and impact metrics for evaluation and
comparison, not only of individual researchers but also of institutions,
universities and even countries, has prompted the development of bibliometrics.
Currently, metrics are becoming widely accepted as an easy and balanced way to
assist the peer review and evaluation of scientists and/or research units,
provided they have adequate precision and recall.
  This paper presents a benchmarking study of a selected list of representative
Portuguese research units, based on a fairly complete set of parameters:
bibliometric parameters, number of competitive projects and number of PhDs
produced. The study aimed at collecting productivity and impact data from the
selected research units in comparable conditions i.e., using objective metrics
based on public information, retrievable on-line and/or from official sources
and thus verifiable and repeatable. The study has thus focused on the activity
of the 2003-06 period, where such data was available from the latest official
evaluation.
  The main advantage of our study was the application of automatic tools,
achieving relevant results at a reduced cost. Moreover, the results over the
selected units suggest that this kind of analyses will be very useful to
benchmark scientific productivity and impact, and assist peer review.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0422</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0422</id><created>2013-02-02</created><authors><author><keyname>Wang</keyname><forenames>L.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Set-Membership Constrained Conjugate Gradient Beamforming Algorithms</title><categories>cs.IT math.IT</categories><comments>9 figures</comments><journal-ref>IET Signal Processing, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work a constrained adaptive filtering strategy based on conjugate
gradient (CG) and set-membership (SM) techniques is presented for adaptive
beamforming. A constraint on the magnitude of the array output is imposed to
derive an adaptive algorithm that performs data-selective updates when
calculating the beamformer's parameters. We consider a linearly constrained
minimum variance (LCMV) optimization problem with the bounded constraint based
on this strategy and propose a CG type algorithm for implementation. The
proposed algorithm has data-selective updates, a variable forgetting factor and
performs one iteration per update to reduce the computational complexity. The
updated parameters construct a space of feasible solutions that enforce the
constraints. We also introduce two time-varying bounding schemes to measure the
quality of the parameters that could be included in the parameter space. A
comprehensive complexity and performance analysis between the proposed and
existing algorithms are provided. Simulations are performed to show the
enhanced convergence and tracking performance of the proposed algorithm as
compared to existing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0432</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0432</id><created>2013-02-02</created><updated>2014-01-19</updated><authors><author><keyname>Tang</keyname><forenames>Ping Tak Peter</forenames></author><author><keyname>Polizzi</keyname><forenames>Eric</forenames></author></authors><title>FEAST as a Subspace Iteration Eigensolver Accelerated by Approximate
  Spectral Projection</title><categories>math.NA cs.MS cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The calculation of a segment of eigenvalues and their corresponding
eigenvectors of a Hermitian matrix or matrix pencil has many applications. A
new density-matrix-based algorithm has been proposed recently and a software
package FEAST has been developed. The density-matrix approach allows FEAST's
implementation to exploit a key strength of modern computer architectures,
namely, multiple levels of parallelism. Consequently, the software package has
been well received, especially in the electronic structure community.
Nevertheless, theoretical analysis of FEAST has lagged. For instance, the FEAST
algorithm has not been proven to converge. This paper offers a detailed
numerical analysis of FEAST. In particular, we show that the FEAST algorithm
can be understood as an accelerated subspace iteration algorithm in conjunction
with the Rayleigh-Ritz procedure. The novelty of FEAST lies in its accelerator
which is a rational matrix function that approximates the spectral projector
onto the eigenspace in question. Analysis of the numerical nature of this
approximate spectral projector and the resulting subspaces generated in the
FEAST algorithm establishes the algorithm's convergence. This paper shows that
FEAST is resilient against rounding errors and establishes properties that can
be leveraged to enhance the algorithm's robustness. Finally, we propose an
extension of FEAST to handle non-Hermitian problems and suggest some future
research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0435</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0435</id><created>2013-02-02</created><updated>2013-02-06</updated><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Wang</keyname><forenames>James Z.</forenames></author><author><keyname>Li</keyname><forenames>Jia</forenames></author></authors><title>Parallel D2-Clustering: Large-Scale Clustering of Discrete Distributions</title><categories>cs.LG cs.CV</categories><acm-class>I.5.3; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The discrete distribution clustering algorithm, namely D2-clustering, has
demonstrated its usefulness in image classification and annotation where each
object is represented by a bag of weighed vectors. The high computational
complexity of the algorithm, however, limits its applications to large-scale
problems. We present a parallel D2-clustering algorithm with substantially
improved scalability. A hierarchical structure for parallel computing is
devised to achieve a balance between the individual-node computation and the
integration process of the algorithm. Additionally, it is shown that even with
a single CPU, the hierarchical structure results in significant speed-up.
Experiments on real-world large-scale image data, Youtube video data, and
protein sequence data demonstrate the efficiency and wide applicability of the
parallel D2-clustering algorithm. The loss in clustering accuracy is minor in
comparison with the original sequential algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0439</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0439</id><created>2013-02-02</created><updated>2013-02-07</updated><authors><author><keyname>Shearer</keyname><forenames>Paul</forenames></author><author><keyname>Gilbert</keyname><forenames>Anna C.</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Correcting Camera Shake by Incremental Sparse Approximation</title><categories>cs.CV cs.GR</categories><comments>5 pages, 3 figures. Conference submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of deblurring an image when the blur kernel is unknown remains
challenging after decades of work. Recently there has been rapid progress on
correcting irregular blur patterns caused by camera shake, but there is still
much room for improvement. We propose a new blind deconvolution method using
incremental sparse edge approximation to recover images blurred by camera
shake. We estimate the blur kernel first from only the strongest edges in the
image, then gradually refine this estimate by allowing for weaker and weaker
edges. Our method competes with the benchmark deblurring performance of the
state-of-the-art while being significantly faster and easier to generalize.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0446</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0446</id><created>2013-02-02</created><authors><author><keyname>Song</keyname><forenames>Mingli</forenames></author><author><keyname>Tao</keyname><forenames>Dachent</forenames></author><author><keyname>Maybank</keyname><forenames>Stephen J.</forenames></author></authors><title>Sparse Camera Network for Visual Surveillance -- A Comprehensive Survey</title><categories>cs.CV</categories><comments>41 pages, 5 figures, journal submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Technological advances in sensor manufacture, communication, and computing
are stimulating the development of new applications that are transforming
traditional vision systems into pervasive intelligent camera networks. The
analysis of visual cues in multi-camera networks enables a wide range of
applications, from smart home and office automation to large area surveillance
and traffic surveillance. While dense camera networks - in which most cameras
have large overlapping fields of view - are well studied, we are mainly
concerned with sparse camera networks. A sparse camera network undertakes large
area surveillance using as few cameras as possible, and most cameras have
non-overlapping fields of view with one another. The task is challenging due to
the lack of knowledge about the topological structure of the network,
variations in the appearance and motion of specific tracking targets in
different views, and the difficulties of understanding composite events in the
network. In this review paper, we present a comprehensive survey of recent
research results to address the problems of intra-camera tracking, topological
structure learning, target appearance modeling, and global activity
understanding in sparse camera networks. A number of current open research
issues are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0447</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0447</id><created>2013-02-02</created><authors><author><keyname>Harjes</keyname><forenames>Kristine</forenames></author><author><keyname>Naumov</keyname><forenames>Pavel</forenames></author></authors><title>Functional Dependence in Strategic Games</title><categories>math.LO cs.GT cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies properties of functional dependencies between strategies of
players in Nash equilibria of multi-player strategic games. The main focus is
on the properties of functional dependencies in the context of a fixed
dependency graph for pay-off functions. A logical system describing properties
of functional dependence for any given graph is proposed and is proven to be
complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0449</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0449</id><created>2013-02-02</created><updated>2013-10-10</updated><authors><author><keyname>Fardad</keyname><forenames>Makan</forenames></author><author><keyname>Lin</keyname><forenames>Fu</forenames></author><author><keyname>Jovanovi&#x107;</keyname><forenames>Mihailo R.</forenames></author></authors><title>Design of optimal sparse interconnection graphs for synchronization of
  oscillator networks</title><categories>math.OC cs.SY</categories><comments>Submitted to IEEE Transactions on Automatic Control</comments><journal-ref>IEEE Trans. Automat. Control (2014), vol. 59, no. 9, pp. 2457-2462</journal-ref><doi>10.1109/TAC.2014.2301577</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the optimal design of a conductance network as a means for
synchronizing a given set of oscillators. Synchronization is achieved when all
oscillator voltages reach consensus, and performance is quantified by the
mean-square deviation from the consensus value. We formulate optimization
problems that address the trade-off between synchronization performance and the
number and strength of oscillator couplings. We promote the sparsity of the
coupling network by penalizing the number of interconnection links. For
identical oscillators, we establish convexity of the optimization problem and
demonstrate that the design problem can be formulated as a semidefinite
program. Finally, for special classes of oscillator networks we derive explicit
analytical expressions for the optimal conductance values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0450</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0450</id><created>2013-02-02</created><updated>2013-05-29</updated><authors><author><keyname>Lin</keyname><forenames>Fu</forenames></author><author><keyname>Fardad</keyname><forenames>Makan</forenames></author><author><keyname>Jovanovi&#x107;</keyname><forenames>Mihailo R.</forenames></author></authors><title>Algorithms for leader selection in stochastically forced consensus
  networks</title><categories>math.OC cs.RO cs.SY</categories><comments>Submitted to IEEE Transactions on Automatic Control</comments><journal-ref>IEEE Trans. Automat. Control (2014), vol. 59, no. 7, pp. 1789-1802</journal-ref><doi>10.1109/TAC.2014.2314223</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in assigning a pre-specified number of nodes as leaders in
order to minimize the mean-square deviation from consensus in stochastically
forced networks. This problem arises in several applications including control
of vehicular formations and localization in sensor networks. For networks with
leaders subject to noise, we show that the Boolean constraints (a node is
either a leader or it is not) are the only source of nonconvexity. By relaxing
these constraints to their convex hull we obtain a lower bound on the global
optimal value. We also use a simple but efficient greedy algorithm to identify
leaders and to compute an upper bound. For networks with leaders that perfectly
follow their desired trajectories, we identify an additional source of
nonconvexity in the form of a rank constraint. Removal of the rank constraint
and relaxation of the Boolean constraints yields a semidefinite program for
which we develop a customized algorithm well-suited for large networks. Several
examples ranging from regular lattices to random graphs are provided to
illustrate the effectiveness of the developed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0459</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0459</id><created>2013-02-03</created><authors><author><keyname>Sadeghi</keyname><forenames>Mohammad-Reza</forenames></author><author><keyname>Sakzad</keyname><forenames>Amin</forenames></author></authors><title>On the performance of 1-level LDPC lattices</title><categories>cs.IT math.IT</categories><comments>1 figure, submitted to IWCIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The low-density parity-check (LDPC) lattices perform very well in high
dimensions under generalized min-sum iterative decoding algorithm. In this work
we focus on 1-level LDPC lattices. We show that these lattices are the same as
lattices constructed based on Construction A and low-density lattice-code
(LDLC) lattices. In spite of having slightly lower coding gain, 1-level regular
LDPC lattices have remarkable performances. The lower complexity nature of the
decoding algorithm for these type of lattices allows us to run it for higher
dimensions easily. Our simulation results show that a 1-level LDPC lattice of
size 10000 can work as close as 1.1 dB at normalized error probability (NEP) of
$10^{-5}$.This can also be reported as 0.6 dB at symbol error rate (SER) of
$10^{-5}$ with sum-product algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0463</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0463</id><created>2013-02-03</created><updated>2013-08-23</updated><authors><author><keyname>Wang</keyname><forenames>Xue-Wen</forenames></author><author><keyname>Zhang</keyname><forenames>Li-Jie</forenames></author><author><keyname>Yang</keyname><forenames>Guo-Hong</forenames></author><author><keyname>Xu</keyname><forenames>Xin-Jian</forenames></author></authors><title>Modeling citation networks based on vigorousness and dormancy</title><categories>physics.soc-ph cond-mat.stat-mech cs.DL cs.SI</categories><comments>ws-tex, 11 pages, 5 figures</comments><journal-ref>Modern Physics Letters B 27, 1350155 (2013)</journal-ref><doi>10.1142/S0217984913501558</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In citation networks, the activity of papers usually decreases with age and
dormant papers may be discovered and become fashionable again. To model this
phenomenon, a competition mechanism is suggested which incorporates two
factors: vigorousness and dormancy. Based on this idea, a citation network
model is proposed, in which a node has two discrete stage: vigorous and
dormant. Vigorous nodes can be deactivated and dormant nodes may be activated
and become vigorous. The evolution of the network couples addition of new nodes
and state transitions of old ones. Both analytical calculation and numerical
simulation show that the degree distribution of nodes in generated networks
displays a good right-skewed behavior. Particularly, scale-free networks are
obtained as the deactivated vertex is target selected and exponential networks
are realized for the random-selected case. Moreover, the measurement of four
real-world citation networks achieves a good agreement with the stochastic
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0487</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0487</id><created>2013-02-03</created><authors><author><keyname>Gurumoorthy</keyname><forenames>Karthik S.</forenames></author></authors><title>On the dynamic compressibility of sets</title><categories>cs.CC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a new notion of compressibility of a set of numbers through the
dynamics of a polynomial function. We provide approaches to solve the problem
by reducing it to the multi-criteria traveling salesman problem through a
series of transformations. We then establish computational complexity results
by giving some NP-completeness proofs. We also discuss about a notion of
$\epsilon$ K-compressibility of a set, with regard to lossy compression and
deduce the necessary condition for the given set to be $\epsilon$
K-compressible. Finally, we conclude by providing a list of open problems
solutions to which could extend the applicability the our technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0488</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0488</id><created>2013-02-03</created><authors><author><keyname>Rodaro</keyname><forenames>Emanuele</forenames></author><author><keyname>Yeldan</keyname><forenames>&#xd6;znur</forenames></author></authors><title>A multi-lane traffic simulation model via continuous cellular automata</title><categories>cs.MA nlin.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffic models based on cellular automata have high computational efficiency
because of their simplicity in describing unrealistic vehicular behavior and
the versatility of cellular automata to be implemented on parallel processing.
On the other hand, the other microscopic traffic models such as car-following
models are computationally more expensive, but they have more realistic driver
behaviors and detailed vehicle characteristics. We propose a new class between
these two categories, defining a traffic model based on continuous cellular
automata where we combine the efficiency of cellular automata models with the
accuracy of the other microscopic models. More precisely, we introduce a
stochastic cellular automata traffic model in which the space is not
coarse-grain but continuous. The continuity also allows us to embed a
multi-agent fuzzy system proposed to handle uncertainties in decision making on
road traffic. Therefore, we simulate different driver behaviors and study the
effect of various compositions of vehicles within the traffic stream from the
macroscopic point of view. The experimental results show that our model is able
to reproduce the typical traffic flow phenomena showing a variety of effects
due to the heterogeneity of traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0490</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0490</id><created>2013-02-03</created><authors><author><keyname>Satpathi</keyname><forenames>Siddhartha</forenames></author><author><keyname>Das</keyname><forenames>Rajib Lochan</forenames></author><author><keyname>Chakraborty</keyname><forenames>Mrityunjoy</forenames></author></authors><title>Improved Bounds on RIP for Generalized Orthogonal Matching Pursuit</title><categories>cs.IT math.IT</categories><comments>8 pages, 1 figure</comments><doi>10.1109/LSP.2013.2279977</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized Orthogonal Matching Pursuit (gOMP) is a natural extension of OMP
algorithm where unlike OMP, it may select $N (\geq1)$ atoms in each iteration.
In this paper, we demonstrate that gOMP can successfully reconstruct a
$K$-sparse signal from a compressed measurement $ {\bf y}={\bf \Phi x}$ by
$K^{th}$ iteration if the sensing matrix ${\bf \Phi}$ satisfies restricted
isometry property (RIP) of order $NK$ where $\delta_{NK} &lt; \frac
{\sqrt{N}}{\sqrt{K}+2\sqrt{N}}$. Our bound offers an improvement over the very
recent result shown in \cite{wang_2012b}. Moreover, we present another bound
for gOMP of order $NK+1$ with $\delta_{NK+1} &lt; \frac
{\sqrt{N}}{\sqrt{K}+\sqrt{N}}$ which exactly relates to the near optimal bound
of $\delta_{K+1} &lt; \frac {1}{\sqrt{K}+1}$ for OMP (N=1) as shown in
\cite{wang_2012a}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0494</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0494</id><created>2013-02-03</created><updated>2013-04-15</updated><authors><author><keyname>Qin</keyname><forenames>Binjie</forenames></author><author><keyname>Shen</keyname><forenames>Zhuangming</forenames></author><author><keyname>Zhou</keyname><forenames>Zien</forenames></author><author><keyname>Zhou</keyname><forenames>Jiawei</forenames></author><author><keyname>Sun</keyname><forenames>Jiuai</forenames></author><author><keyname>Zhang</keyname><forenames>Hui</forenames></author><author><keyname>Hu</keyname><forenames>Mingxing</forenames></author><author><keyname>Lv</keyname><forenames>Yisong</forenames></author></authors><title>Local Structure Matching Driven by Joint-Saliency-Structure Adaptive
  Kernel Regression</title><categories>cs.CV</categories><comments>12 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  For nonrigid image registration, matching the particular structures (or the
outliers) that have missing correspondence and/or local large deformations, can
be more difficult than matching the common structures with small deformations
in the two images. Most existing works depend heavily on the outlier
segmentation to remove the outlier effect in the registration. Moreover, these
works do not handle simultaneously the missing correspondences and local large
deformations. In this paper, we defined the nonrigid image registration as a
local adaptive kernel regression which locally reconstruct the moving image's
dense deformation vectors from the sparse deformation vectors in the
multi-resolution block matching. The kernel function of the kernel regression
adapts its shape and orientation to the reference image's structure to gather
more deformation vector samples of the same structure for the iterative
regression computation, whereby the moving image's local deformations could be
compliant with the reference image's local structures. To estimate the local
deformations around the outliers, we use joint saliency map that highlights the
corresponding saliency structures (called Joint Saliency Structures, JSSs) in
the two images to guide the dense deformation reconstruction by emphasizing
those JSSs' sparse deformation vectors in the kernel regression. The
experimental results demonstrate that by using local JSS adaptive kernel
regression, the proposed method achieves almost the best performance in
alignment of all challenging image pairs with outlier structures compared with
other five state-of-the-art nonrigid registration algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0511</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0511</id><created>2013-02-03</created><updated>2014-09-26</updated><authors><author><keyname>Callegari</keyname><forenames>Sergio</forenames></author><author><keyname>Setti</keyname><forenames>Gianluca</forenames></author><author><keyname>Langlois</keyname><forenames>Peter J.</forenames></author></authors><title>A CMOS Tailed Tent Map for the Generation of Uniformly Distributed
  Chaotic Sequences</title><categories>nlin.CD cs.OH</categories><journal-ref>Proceedings of 1997 IEEE International Symposium on Circuits and
  Systems, 1997. ISCAS '97., vol 2. pages 781 - 784</journal-ref><doi>10.1109/ISCAS.1997.621829</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the design of a modified tent map characterized by a
uniform probability density function. The use of this map is proposed as an
alternative to the tent map and the Bernoulli shift. It is shown that practical
circuits implementing the latter two maps may possess parasitic stable
equilibria, fact which would prevent the desired chaotic behavior of the
system. On the other hand, commonly used strategies to avoid the parasitic
equilibria onset also affect the uniformity of the probability density
function. Conversely, the use of the proposed tailed tent map allows to assure
a certain degree of parameter deviation robustness, without compromising on the
statistical properties of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0522</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0522</id><created>2013-02-03</created><updated>2013-07-04</updated><authors><author><keyname>Mulholland</keyname><forenames>Ian P.</forenames></author><author><keyname>Flanagan</keyname><forenames>Mark F.</forenames></author><author><keyname>Paolini</keyname><forenames>Enrico</forenames></author></authors><title>Minimum Distance Distribution of Irregular Generalized LDPC Code
  Ensembles</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure. Submitted to the IEEE International Symposium on
  Information Theory (ISIT) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the minimum distance distribution of irregular generalized
LDPC (GLDPC) code ensembles is investigated. Two classes of GLDPC code
ensembles are analyzed; in one case, the Tanner graph is regular from the
variable node perspective, and in the other case the Tanner graph is completely
unstructured and irregular. In particular, for the former ensemble class we
determine exactly which ensembles have minimum distance growing linearly with
the block length with probability approaching unity with increasing block
length. This work extends previous results concerning LDPC and regular GLDPC
codes to the case where a hybrid mixture of check node types is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0533</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0533</id><created>2013-02-03</created><authors><author><keyname>Wang</keyname><forenames>L.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Low-Complexity Reduced-Rank Beamforming Algorithms</title><categories>cs.IT math.IT</categories><comments>7 figures</comments><journal-ref>IEEE Transactions on Aerospace and Electronic Systems, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A reduced-rank framework with set-membership filtering (SMF) techniques is
presented for adaptive beamforming problems encountered in radar systems. We
develop and analyze stochastic gradient (SG) and recursive least squares
(RLS)-type adaptive algorithms, which achieve an enhanced convergence and
tracking performance with low computational cost as compared to existing
techniques. Simulations show that the proposed algorithms have a superior
performance to prior methods, while the complexity is lower.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0540</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0540</id><created>2013-02-03</created><authors><author><keyname>Georgiou</keyname><forenames>Harris V.</forenames></author><author><keyname>Mavroforakis</keyname><forenames>Michael E.</forenames></author></authors><title>A game-theoretic framework for classifier ensembles using weighted
  majority voting with local accuracy estimates</title><categories>cs.LG</categories><comments>21 pages, 9 tables, 1 figure, 68 references</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, a novel approach for the optimal combination of binary
classifiers is proposed. The classifier combination problem is approached from
a Game Theory perspective. The proposed framework of adapted weighted majority
rules (WMR) is tested against common rank-based, Bayesian and simple majority
models, as well as two soft-output averaging rules. Experiments with ensembles
of Support Vector Machines (SVM), Ordinary Binary Tree Classifiers (OBTC) and
weighted k-nearest-neighbor (w/k-NN) models on benchmark datasets indicate that
this new adaptive WMR model, employing local accuracy estimators and the
analytically computed optimal weights outperform all the other simple
combination rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0558</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0558</id><created>2013-02-03</created><updated>2014-11-25</updated><authors><author><keyname>Cardillo</keyname><forenames>Alessio</forenames></author><author><keyname>Petri</keyname><forenames>Giovanni</forenames></author><author><keyname>Nicosia</keyname><forenames>Vincenzo</forenames></author><author><keyname>Sinatra</keyname><forenames>Roberta</forenames></author><author><keyname>G&#xf3;mez-Garde&#xf1;es</keyname><forenames>Jes&#xfa;s</forenames></author><author><keyname>Latora</keyname><forenames>Vito</forenames></author></authors><title>Evolutionary dynamics of time-resolved social interactions</title><categories>physics.soc-ph cs.SI</categories><comments>13 pages, 9 figures. Final version accepted for publication</comments><journal-ref>Physical Review E 90, 052825 (2014)</journal-ref><doi>10.1103/PhysRevE.90.052825</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation among unrelated individuals is frequently observed in social
groups when their members combine efforts and resources to obtain a shared
benefit that is unachievable by an individual alone. However, understanding why
cooperation arises despite the natural tendency of individuals towards selfish
behavior is still an open problem and represents one of the most fascinating
challenges in evolutionary dynamics. Recently, the structural characterization
of the networks in which social interactions take place has shed some light on
the mechanisms by which cooperative behavior emerges and eventually overcomes
the natural temptation to defect. In particular, it has been found that the
heterogeneity in the number of social ties and the presence of tightly knit
communities lead to a significant increase in cooperation as compared with the
unstructured and homogeneous connection patterns considered in classical
evolutionary dynamics. Here, we investigate the role of social-ties dynamics
for the emergence of cooperation in a family of social dilemmas. Social
interactions are in fact intrinsically dynamic, fluctuating, and intermittent
over time, and they can be represented by time-varying networks. By considering
two experimental data sets of human interactions with detailed time
information, we show that the temporal dynamics of social ties has a dramatic
impact on the evolution of cooperation: the dynamics of pairwise interactions
favors selfish behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0561</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0561</id><created>2013-02-03</created><updated>2014-06-23</updated><authors><author><keyname>Adcock</keyname><forenames>Ben</forenames></author><author><keyname>Hansen</keyname><forenames>Anders C.</forenames></author><author><keyname>Poon</keyname><forenames>Clarice</forenames></author><author><keyname>Roman</keyname><forenames>Bogdan</forenames></author></authors><title>Breaking the coherence barrier: A new theory for compressed sensing</title><categories>cs.IT math.IT math.NA</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper provides an extension of compressed sensing which bridges a
substantial gap between existing theory and its current use in real-world
applications. It introduces a mathematical framework that generalizes the three
standard pillars of compressed sensing - namely, sparsity, incoherence and
uniform random subsampling - to three new concepts: asymptotic sparsity,
asymptotic incoherence and multilevel random sampling. The new theorems show
that compressed sensing is also possible, and reveals several advantages, under
these substantially relaxed conditions. The importance of this is threefold.
First, inverse problems to which compressed sensing is currently applied are
typically coherent. The new theory provides the first comprehensive
mathematical explanation for a range of empirical usages of compressed sensing
in real-world applications, such as medical imaging, microscopy, spectroscopy
and others. Second, in showing that compressed sensing does not require
incoherence, but instead that asymptotic incoherence is sufficient, the new
theory offers markedly greater flexibility in the design of sensing mechanisms.
Third, by using asymptotic incoherence and multi-level sampling to exploit not
just sparsity, but also structure, i.e. asymptotic sparsity, the new theory
shows that substantially improved reconstructions can be obtained from fewer
measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0566</identifier>
 <datestamp>2013-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0566</id><created>2013-02-03</created><updated>2013-09-18</updated><authors><author><keyname>Chen</keyname><forenames>Taolue</forenames></author><author><keyname>Sun</keyname><forenames>Xiaoming</forenames></author><author><keyname>Yu</keyname><forenames>Nengkun</forenames></author></authors><title>Orbit Problem Revisited</title><categories>cs.CC</categories><comments>Comments are welcome. We delete the first part since the overlap with
  arxiv:1303.2981 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we revisit the {\em orbit problem}, which was studied in
\cite{HAR69,SHA79,KL86}. In \cite{KL86}, Kannan and Lipton proved that this
problem is decidable in polynomial time. In this paper, we study the {\em
approximate orbit problem}, and show that this problem is decidable except for
one case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0569</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0569</id><created>2013-02-03</created><authors><author><keyname>Zhou</keyname><forenames>Zhengchun</forenames></author><author><keyname>Ding</keyname><forenames>Cunsheng</forenames></author></authors><title>A Class of Three-Weight Cyclic Codes</title><categories>cs.IT math.IT</categories><comments>11 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes are a subclass of linear codes and have applications in consumer
electronics, data storage systems, and communication systems as they have
efficient encoding and decoding algorithms. In this paper, a class of
three-weight cyclic codes over $\gf(p)$ whose duals have two zeros is
presented, where $p$ is an odd prime. The weight distribution of this class of
cyclic codes is settled. Some of the cyclic codes are optimal. The duals of a
subclass of the cyclic codes are also studied and proved to be optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0579</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0579</id><created>2013-02-03</created><updated>2013-10-16</updated><authors><author><keyname>Daskin</keyname><forenames>Anmer</forenames></author><author><keyname>Grama</keyname><forenames>Ananth</forenames></author><author><keyname>Kais</keyname><forenames>Sabre</forenames></author></authors><title>A Universal Quantum Circuit Scheme For Finding Complex Eigenvalues</title><categories>quant-ph cs.IT math.IT</categories><doi>10.1007/s11128-013-0654-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general quantum circuit design for finding eigenvalues of
non-unitary matrices on quantum computers using the iterative phase estimation
algorithm. In particular, we show how the method can be used for the simulation
of resonance states for quantum systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0581</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0581</id><created>2013-02-03</created><updated>2014-03-20</updated><authors><author><keyname>Dowty</keyname><forenames>James G.</forenames></author></authors><title>SMML estimators for exponential families with continuous sufficient
  statistics</title><categories>cs.IT math.IT math.ST stat.ML stat.TH</categories><comments>Revised to include new insights and results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum message length principle is an information theoretic criterion
that links data compression with statistical inference. This paper studies the
strict minimum message length (SMML) estimator for $d$-dimensional exponential
families with continuous sufficient statistics, for all $d \ge 1$. The
partition of an SMML estimator is shown to consist of convex polytopes (i.e.
convex polygons when $d=2$) which can be described explicitly in terms of the
assertions and coding probabilities. While this result is known, we give a new
proof based on the calculus of variations, and this approach gives some
interesting new inequalities for SMML estimators. We also use this result to
construct an SMML estimator for a $2$-dimensional normal random variable with
known variance and a normal prior on its mean.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0585</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0585</id><created>2013-02-04</created><updated>2013-06-30</updated><authors><author><keyname>Liu</keyname><forenames>Liang</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Chua</keyname><forenames>Kee-Chaing</forenames></author></authors><title>Wireless Information and Power Transfer: A Dynamic Power Splitting
  Approach</title><categories>cs.IT math.IT</categories><comments>accepted by IEEE Transactions on Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scavenging energy from ambient radio signals, namely wireless energy
harvesting (WEH), has recently drawn significant attention. In this paper, we
consider a point-to-point wireless link over the flat-fading channel, where the
receiver replenishes energy via WEH from the signals sent by the transmitter.
We consider a SISO (single-input single-output) system where the single-antenna
receiver cannot decode information and harvest energy independently from the
same signal received. Under this practical constraint, we propose a dynamic
power splitting (DPS) scheme, where the received signal is split into two
streams with adjustable power levels for information decoding and energy
harvesting separately based on the instantaneous channel condition that is
assumed to be known at the receiver. We derive the optimal power splitting rule
at the receiver to achieve various trade-offs between the maximum ergodic
capacity for information transfer and the maximum average harvested energy for
power transfer. Moreover, for the case when the channel state information is
also known at the transmitter, we investigate the joint optimization of
transmitter power control and receiver power splitting. Finally, we extend the
result for DPS to the SIMO (single-input multiple-output) system and
investigate a low-complexity power splitting scheme termed antenna switching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0608</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0608</id><created>2013-02-04</created><authors><author><keyname>Torres-Salinas</keyname><forenames>Daniel</forenames></author><author><keyname>Robinson-Garcia</keyname><forenames>Nicolas</forenames></author><author><keyname>Jim&#xe9;nez-Contreras</keyname><forenames>Evaristo</forenames></author><author><keyname>Herrera</keyname><forenames>Francisco</forenames></author><author><keyname>L&#xf3;pez-C&#xf3;zar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>On the use of Biplot analysis for multivariate bibliometric and
  scientific indicators</title><categories>cs.DL</categories><comments>Accepted for publication in Journal of the American Society for
  Information Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bibliometric mapping and visualization techniques represent one of the main
pillars in the field of scientometrics. Traditionally, the main methodologies
employed for representing data are Multi-Dimensional Scaling, Principal
Component Analysis or Correspondence Analysis. In this paper we aim at
presenting a visualization methodology known as Biplot analysis for
representing bibliometric and science and technology indicators. A Biplot is a
graphical representation of multivariate data, where the elements of a data
matrix are represented according to dots and vectors associated with the rows
and columns of the matrix. In this paper we explore the possibilities of
applying the Biplot analysis in the research policy area. More specifically we
will first describe and introduce the reader to this methodology and secondly,
we will analyze its strengths and weaknesses through three different study
cases: countries, universities and scientific fields. For this, we use a Biplot
analysis known as JK-Biplot. Finally we compare the Biplot representation with
other multivariate analysis techniques. We conclude that Biplot analysis could
be a useful technique in scientometrics when studying multivariate data and an
easy-to-read tool for research decision makers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0614</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0614</id><created>2013-02-04</created><updated>2013-12-01</updated><authors><author><keyname>Karadimitrakis</keyname><forenames>Apostolos</forenames></author><author><keyname>Moustakas</keyname><forenames>Aris L.</forenames></author><author><keyname>Vivo</keyname><forenames>Pierpaolo</forenames></author></authors><title>Outage Capacity for the Optical MIMO Channel</title><categories>cs.IT math.IT</categories><comments>Revised version includes more details, proofs and a closed-form
  expression for the outage probability</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MIMO processing techniques in fiber optical communications have been proposed
as a promising approach to meet increasing demand for information throughput.
In this context, the multiple channels correspond to the multiple modes and/or
multiple cores in the fiber. In this paper we characterize the distribution of
the mutual information with Gaussian input in a simple channel model for this
system. Assuming significant cross talk between cores, negligible
backscattering and near-lossless propagation in the fiber, we model the
transmission channel as a random complex unitary matrix. The loss in the
transmission may be parameterized by a number of unutilized channels in the
fiber. We analyze the system in a dual fashion. First, we evaluate a
closed-form expression for the outage probability, which is handy for small
matrices. We also apply the asymptotic approach, in particular the Coulomb gas
method from statistical mechanics, to obtain closed-form results for the
ergodic mutual information, its variance as well as the outage probability for
Gaussian input in the limit of large number of cores/modes. By comparing our
analytic results to simulations, we see that, despite the fact that this method
is nominally valid for large number of modes, our method is quite accurate even
for small to modest number of channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0621</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0621</id><created>2013-02-04</created><updated>2013-06-27</updated><authors><author><keyname>Ng</keyname><forenames>Chun-Ho</forenames></author><author><keyname>Lee</keyname><forenames>Patrick P. C.</forenames></author></authors><title>RevDedup: A Reverse Deduplication Storage System Optimized for Reads to
  Latest Backups</title><categories>cs.DC cs.OS</categories><comments>A 7-page version appeared in APSys'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scaling up the backup storage for an ever-increasing volume of virtual
machine (VM) images is a critical issue in virtualization environments. While
deduplication is known to effectively eliminate duplicates for VM image
storage, it also introduces fragmentation that will degrade read performance.
We propose RevDedup, a deduplication system that optimizes reads to latest VM
image backups using an idea called reverse deduplication. In contrast with
conventional deduplication that removes duplicates from new data, RevDedup
removes duplicates from old data, thereby shifting fragmentation to old data
while keeping the layout of new data as sequential as possible. We evaluate our
RevDedup prototype using microbenchmark and real-world workloads. For a 12-week
span of real-world VM images from 160 users, RevDedup achieves high
deduplication efficiency with around 97% of saving, and high backup and read
throughput on the order of 1GB/s. RevDedup also incurs small metadata overhead
in backup/read operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0629</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0629</id><created>2013-02-04</created><authors><author><keyname>ALmomani</keyname><forenames>Ammar</forenames></author><author><keyname>Gupta</keyname><forenames>B. B.</forenames></author><author><keyname>Wan</keyname><forenames>Tat-Chee</forenames></author><author><keyname>Altaher</keyname><forenames>Altyeb</forenames></author><author><keyname>Manickam</keyname><forenames>Selvakumar</forenames></author></authors><title>Phishing Dynamic Evolving Neural Fuzzy Framework for Online Detection
  Zero-day Phishing Email</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phishing is a kind of attack in which criminals use spoofed emails and
fraudulent web sites to trick financial organization and customers. Criminals
try to lure online users by convincing them to reveal the username, passwords,
credit card number and updating account information or fill billing
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0634</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0634</id><created>2013-02-04</created><updated>2014-04-14</updated><authors><author><keyname>Himpe</keyname><forenames>Christian</forenames></author><author><keyname>Ohlberger</keyname><forenames>Mario</forenames></author></authors><title>Cross-Gramian-Based Combined State and Parameter Reduction for
  Large-Scale Control Systems</title><categories>math.OC cs.SY math.DS</categories><comments>Preprint</comments><msc-class>93B11, 93B30, 93C10</msc-class><acm-class>G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces the empirical cross gramian for
multiple-input-multiple-output systems. The cross gramian is a tool for
reducing the state space of control systems, which conjoins controllability and
observability information into a single matrix and does not require balancing.
Its empirical gramian variant extends the application of the cross gramian to
nonlinear systems. Furthermore, for parametrized systems, the empirical
gramians can also be utilized for sensitivity analysis or parameter
identification and thus for parameter reduction. This work also introduces the
empirical joint gramian, which is derived from the empirical cross gramian. The
joint gramian not only allows a reduction of the parameter space, but also the
combined state and parameter space reduction, which is tested on a linear and a
nonlinear control system. Controllability- and observability-based combined
reduction methods are also presented, which are benchmarked against the joint
gramian.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0635</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0635</id><created>2013-02-04</created><authors><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Rodrigues</keyname><forenames>Miguel R. D.</forenames></author><author><keyname>Wassell</keyname><forenames>Ian</forenames></author></authors><title>Projection Design For Statistical Compressive Sensing: A Tight Frame
  Based Approach</title><categories>cs.IT math.IT</categories><comments>31 pages, 11 figures, accepted by IEEE Transaction on Signal
  Processing</comments><doi>10.1109/TSP.2013.2245661</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a framework to design sensing matrices for
compressive sensing applications that lead to good mean squared error (MSE)
performance subject to sensing cost constraints. By capitalizing on the MSE of
the oracle estimator, whose performance has been shown to act as a benchmark to
the performance of standard sparse recovery algorithms, we use the fact that a
Parseval tight frame is the closest design - in the Frobenius norm sense - to
the solution of a convex relaxation of the optimization problem that relates to
the minimization of the MSE of the oracle estimator with respect to the
equivalent sensing matrix, subject to sensing energy constraints. Based on this
result, we then propose two sensing matrix designs that exhibit two key
properties: i) the designs are closed form rather than iterative; ii) the
designs exhibit superior performance in relation to other designs in the
literature, which is revealed by our numerical investigation in various
scenarios with different sparse recovery algorithms including basis pursuit
de-noise (BPDN), the Dantzig selector and orthogonal matching pursuit (OMP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0677</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0677</id><created>2013-02-04</created><updated>2014-01-15</updated><authors><author><keyname>Saito</keyname><forenames>Kodai</forenames></author><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>Two types of well followed users in the followership networks of Twitter</title><categories>cs.SI physics.soc-ph</categories><comments>4 Figures and 8 Tables</comments><journal-ref>PLOS ONE, 9(1), e84265 (2014)</journal-ref><doi>10.1371/journal.pone.0084265</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Twitter blogosphere, the number of followers is probably the most
basic and succinct quantity for measuring popularity of users. However, the
number of followers can be manipulated in various ways; we can even buy
follows. Therefore, alternative popularity measures for Twitter users on the
basis of, for example, users' tweets and retweets, have been developed. In the
present work, we take a purely network approach to this fundamental question.
First, we find that two relatively distinct types of users possessing a large
number of followers exist, in particular for Japanese, Russian, and Korean
users among the seven language groups that we examined. A first type of user
follows a small number of other users. A second type of user follows
approximately the same number of other users as the number of follows that the
user receives. Then, we compare local (i.e., egocentric) followership networks
around the two types of users with many followers. We show that the second
type, which is presumably uninfluential users despite its large number of
followers, is characterized by high link reciprocity, a large number of friends
(i.e., those whom a user follows) for the followers, followers' high link
reciprocity, large clustering coefficient, large fraction of the second type of
users among the followers, and a small PageRank. Our network-based results
support that the number of followers used alone is a misleading measure of
user's popularity. We propose that the number of friends, which is simple to
measure, also helps us to assess the popularity of Twitter users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0689</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0689</id><created>2013-02-04</created><authors><author><keyname>Ngo</keyname><forenames>Anh Cat Le</forenames></author><author><keyname>Ang</keyname><forenames>Li-Minn</forenames></author><author><keyname>Qiu</keyname><forenames>Guoping</forenames></author><author><keyname>Seng</keyname><forenames>Kah-Phooi</forenames></author></authors><title>Multi-scale Visual Attention &amp; Saliency Modelling with Decision Theory</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bottom-up saliency, an early human visual processing, behaves like binary
classification of interest and null hypothesis. Its discriminant power, mutual
information of image features and class distribution, is closely related to
saliency value by the well-known centre-surround theory. As classification
accuracy very much depends on window sizes, the discriminant saliency (power)
varies according to sampling scales. Discriminating power estimation in
multi-scales framework needs integrating with wavelet transformation and then
estimating statistical discrepancy of two consecutive scales (centre-surround
windows) by Hidden Markov Tree (HMT) model. Finally, multi-scale discriminant
saliency (MDIS) maps are combined by the maximum information rule to synthesize
a final saliency map. All MDIS maps are evaluated with standard quantitative
tools (NSS,LCC,AUC) on N.Bruce's database with ground truth data as
eye-tracking locations ; as well assessed qualitatively by visual examination
of individual cases. For evaluating MDIS against well-known AIM saliency
method, simulations are needed and described in details with several
interesting conclusions, drawn for further research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0692</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0692</id><created>2013-02-04</created><authors><author><keyname>Holme</keyname><forenames>Petter</forenames></author></authors><title>Epidemiologically optimal static networks from temporal network data</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><doi>10.1371/journal.pcbi.1003142</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network epidemiology's most important assumption is that the contact
structure over which infectious diseases propagate can be represented as a
static network. However, contacts are highly dynamic, changing at many time
scales. In this paper, we investigate conceptually simple methods to construct
static graphs for network epidemiology from temporal contact data. We evaluate
these methods on empirical and synthetic model data. For almost all our cases,
the network representation that captures most relevant information is a
so-called exponential-threshold network. In these, each contact contributes
with a weight decreasing exponentially with time, and there is an edge between
a pair of vertices if the weight between them exceeds a threshold. Networks of
aggregated contacts over an optimally chosen time window perform almost as good
as the exponential-threshold networks. On the other hand, networks of
accumulated contacts over the entire sampling time, and networks of concurrent
partnerships, perform worse. We discuss these observations in the context of
the temporal and topological structure of the data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0707</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0707</id><created>2013-02-04</created><authors><author><keyname>Kim</keyname><forenames>Juhoon</forenames></author><author><keyname>Chatzis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Siebke</keyname><forenames>Matthias</forenames></author><author><keyname>Feldmann</keyname><forenames>Anja</forenames></author></authors><title>Tigers vs Lions: Towards Characterizing Solitary and Group User Behavior
  in MMORPG</title><categories>cs.NI</categories><comments>Also appears as TU-Berlin technical report 2013-01, ISSN: 1436-9915</comments><report-no>2013-01 (ISSN: 1436-9915)</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of Internet technologies enables software developers to build
virtual worlds such as Massively Multi-Player Online Role-Playing Games
(MMORPGs). The population of such games shows super-linear growing tendency. It
is estimated that the number of Internet users subscribed in MMORPGs is more
than 22 million worldwide [1]. However, only little is known about the
characteristics of traffic generated by such games as well as the behavior of
their subscribers. In this paper, we characterize the traffic behavior of World
of Warcraft, the most subscribed MMORPG in the world, by analyzing Internet
traffic data sets collected from a European tier-1 ISP in two different time
periods. We find that World of Warcraft is an influential application regarding
the time spent by users (1.76 and 4.17 Hours/day on average in our
measurement), while its traffic share is comparatively low (&lt; 1 %). In this
respect, we look at the World of Warcraft subscriber's gaming behavior by
categorizing them into two different types of users (solitary users and group
users) and compare these two groups in relation to the playing behavior
(duration as the metric) and the in-game behavior (distance as the metric).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0710</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0710</id><created>2013-02-04</created><authors><author><keyname>Teixeira</keyname><forenames>Ana L.</forenames></author><author><keyname>Santos</keyname><forenames>Rui C.</forenames></author><author><keyname>Leal</keyname><forenames>Joao P.</forenames></author><author><keyname>Simoes</keyname><forenames>Jose A. Martinho</forenames></author><author><keyname>Falcao</keyname><forenames>Andre O.</forenames></author></authors><title>ThermInfo: Collecting, Retrieving, and Estimating Reliable
  Thermochemical Data</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard enthalpies of formation are used for assessing the efficiency and
safety of chemical processes in the chemical industry. However, the number of
compounds for which the enthalpies of formation are available is many orders of
magnitude smaller than the number of known compounds. Thermochemical data
prediction methods are therefore clearly needed. Several commercial and free
chemical databases are currently available, the NIST WebBook being the most
used free source. To overcome this problem a cheminformatics system was
designed and built with two main objectives in mind: collecting and retrieving
critically evaluated thermochemical values, and estimating new data. In its
present version, by using cheminformatics techniques, ThermInfo allows the
retrieval of the value of a thermochemical property, such as a gas-phase
standard enthalpy of formation, by inputting, for example, the molecular
structure or the name of a compound. The same inputs can also be used to
estimate data (presently restricted to non-polycyclic hydrocarbons) by using
the Extended Laidler Bond Additivity (ELBA) method. The information system is
publicly available at http://www.therminfo.com or
http://therminfo.lasige.di.fc.ul.pt. ThermInfo's strength lies in the data
quality, availability (free access), search capabilities, and, in particular,
prediction ability, based on a user-friendly interface that accepts inputs in
several formats.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0723</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0723</id><created>2013-02-04</created><updated>2013-02-05</updated><authors><author><keyname>Cao</keyname><forenames>Nannan</forenames></author><author><keyname>Low</keyname><forenames>Kian Hsiang</forenames></author><author><keyname>Dolan</keyname><forenames>John M.</forenames></author></authors><title>Multi-Robot Informative Path Planning for Active Sensing of
  Environmental Phenomena: A Tale of Two Algorithms</title><categories>cs.LG cs.AI cs.MA cs.RO</categories><comments>12th International Conference on Autonomous Agents and Multiagent
  Systems (AAMAS 2013), Extended version with proofs, 15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key problem of robotic environmental sensing and monitoring is that of
active sensing: How can a team of robots plan the most informative observation
paths to minimize the uncertainty in modeling and predicting an environmental
phenomenon? This paper presents two principled approaches to efficient
information-theoretic path planning based on entropy and mutual information
criteria for in situ active sensing of an important broad class of
widely-occurring environmental phenomena called anisotropic fields. Our
proposed algorithms are novel in addressing a trade-off between active sensing
performance and time efficiency. An important practical consequence is that our
algorithms can exploit the spatial correlation structure of Gaussian
process-based anisotropic fields to improve time efficiency while preserving
near-optimal active sensing performance. We analyze the time complexity of our
algorithms and prove analytically that they scale better than state-of-the-art
algorithms with increasing planning horizon length. We provide theoretical
guarantees on the active sensing performance of our algorithms for a class of
exploration tasks called transect sampling, which, in particular, can be
improved with longer planning time and/or lower spatial correlation along the
transect. Empirical evaluation on real-world anisotropic field data shows that
our algorithms can perform better or at least as well as the state-of-the-art
algorithms while often incurring a few orders of magnitude less computational
time, even when the field conditions are less favorable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0739</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0739</id><created>2013-02-04</created><authors><author><keyname>Lee</keyname><forenames>Conrad</forenames></author><author><keyname>Cunningham</keyname><forenames>P&#xe1;draig</forenames></author></authors><title>Benchmarking community detection methods on social media data</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Benchmarking the performance of community detection methods on empirical
social network data has been identified as critical for improving these
methods. In particular, while most current research focuses on detecting
communities in data that has been digitally extracted from large social media
and telecommunications services, most evaluation of this research is based on
small, hand-curated datasets. We argue that these two types of networks differ
so significantly that by evaluating algorithms solely on the former, we know
little about how well they perform on the latter. To address this problem, we
consider the difficulties that arise in constructing benchmarks based on
digitally extracted network data, and propose a task-based strategy which we
feel addresses these difficulties. To demonstrate that our scheme is effective,
we use it to carry out a substantial benchmark based on Facebook data. The
benchmark reveals that some of the most popular algorithms fail to detect
fine-grained community structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0744</identifier>
 <datestamp>2013-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0744</id><created>2013-02-04</created><updated>2013-05-27</updated><authors><author><keyname>Kamath</keyname><forenames>Govinda M.</forenames></author><author><keyname>Silberstein</keyname><forenames>Natalia</forenames></author><author><keyname>Prakash</keyname><forenames>N.</forenames></author><author><keyname>Rawat</keyname><forenames>Ankit S.</forenames></author><author><keyname>Lalitha</keyname><forenames>V.</forenames></author><author><keyname>Koyluoglu</keyname><forenames>O. Ozan</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijay</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Explicit MBR All-Symbol Locality Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Node failures are inevitable in distributed storage systems (DSS). To enable
efficient repair when faced with such failures, two main techniques are known:
Regenerating codes, i.e., codes that minimize the total repair bandwidth; and
codes with locality, which minimize the number of nodes participating in the
repair process. This paper focuses on regenerating codes with locality, using
pre-coding based on Gabidulin codes, and presents constructions that utilize
minimum bandwidth regenerating (MBR) local codes. The constructions achieve
maximum resilience (i.e., optimal minimum distance) and have maximum capacity
(i.e., maximum rate). Finally, the same pre-coding mechanism can be combined
with a subclass of fractional-repetition codes to enable maximum resilience and
repair-by-transfer simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0745</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0745</id><created>2013-02-04</created><authors><author><keyname>Alur</keyname><forenames>Rajeev</forenames></author><author><keyname>Forejt</keyname><forenames>Vojtech</forenames></author><author><keyname>Moarref</keyname><forenames>Salar</forenames></author><author><keyname>Trivedi</keyname><forenames>Ashutosh</forenames></author></authors><title>Safe Schedulability of Bounded-Rate Multi-Mode Systems</title><categories>cs.LO cs.GT</categories><comments>Technical report for a paper presented at HSCC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bounded-rate multi-mode systems (BMMS) are hybrid systems that can switch
freely among a finite set of modes, and whose dynamics is specified by a finite
number of real-valued variables with mode-dependent rates that can vary within
given bounded sets. The schedulability problem for BMMS is defined as an
infinite-round game between two players---the scheduler and the
environment---where in each round the scheduler proposes a time and a mode
while the environment chooses an allowable rate for that mode, and the state of
the system changes linearly in the direction of the rate vector. The goal of
the scheduler is to keep the state of the system within a pre-specified safe
set using a non-Zeno schedule, while the goal of the environment is the
opposite. Green scheduling under uncertainty is a paradigmatic example of BMMS
where a winning strategy of the scheduler corresponds to a robust
energy-optimal policy. We present an algorithm to decide whether the scheduler
has a winning strategy from an arbitrary starting state, and give an algorithm
to compute such a winning strategy, if it exists. We show that the
schedulability problem for BMMS is co-NP complete in general, but for two
variables it is in PTIME. We also study the discrete schedulability problem
where the environment has only finitely many choices of rate vectors in each
mode and the scheduler can make decisions only at multiples of a given clock
period, and show it to be EXPTIME-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0749</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0749</id><created>2013-02-04</created><authors><author><keyname>Lee</keyname><forenames>Namyoon</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Multi-Way Information Exchange Over Completely-Connected Interference
  Networks with a Multi-Antenna Relay</title><categories>cs.IT math.IT</categories><comments>Short version is submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a fully-connected interference network with a relay in
which multiple users equipped with a single antenna want to exchange multiple
unicast messages with other users in the network by sharing the relay equipped
with multiple antennas. For such a network, the degrees of freedom (DoF) are
derived by considering various message exchange scenarios: a multi-user
fully-connected Y channel, a two-pair two-way interference channel with the
relay, and a two-pair two-way X channel with the relay. Further, considering
distributed relays employing a single antenna in the two-way interference
channel and the three-user fully-connected Y channel, achievable sum-DoF are
also derived in the two-way interference channel and the three-user
fully-connected Y channel. A major implication of the derived DoF results is
that a relay with multiple antennas or multiple relays employing a single
antenna increases the capacity scaling law of the multi-user interference
network when multiple directional information flows are considered, even if the
networks are fully-connected and all nodes operate in half-duplex. These
results reveal that the relay is useful in the multi-way interference network
with practical considerations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0750</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0750</id><created>2013-02-04</created><authors><author><keyname>Maia</keyname><forenames>Eva</forenames></author><author><keyname>Moreira</keyname><forenames>Nelma</forenames></author><author><keyname>Reis</keyname><forenames>Rog&#xe9;rio</forenames></author></authors><title>Incomplete Transition Complexity of Basic Operations on Finite Languages</title><categories>cs.FL</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The state complexity of basic operations on finite languages (considering
complete DFAs) has been in studied the literature. In this paper we study the
incomplete (deterministic) state and transition complexity on finite languages
of boolean operations, concatenation, star, and reversal. For all operations we
give tight upper bounds for both description measures. We correct the published
state complexity of concatenation for complete DFAs and provide a tight upper
bound for the case when the right automaton is larger than the left one. For
all binary operations the tightness is proved using family languages with a
variable alphabet size. In general the operational complexities depend not only
on the complexities of the operands but also on other refined measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0753</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0753</id><created>2013-02-04</created><authors><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author></authors><title>Rooted Trees with Probabilities Revisited</title><categories>cs.IT math.IT</categories><comments>This an evolving draft currently in the form of slides</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rooted trees with probabilities are convenient to represent a class of random
processes with memory. They allow to describe and analyze variable length codes
for data compression and distribution matching. In this work, the Leaf-Average
Node-Sum Interchange Theorem (LANSIT) and the well-known applications to path
length and leaf entropy are re-stated. The LANSIT is then applied to
informational divergence. Next, the differential LANSIT is derived, which
allows to write normalized functionals of leaf distributions as an average of
functionals of branching distributions. Joint distributions of random variables
and the corresponding conditional distributions are special cases of leaf
distributions and branching distributions. Using the differential LANSIT,
Pinsker's inequality is formulated for rooted trees with probabilities, with an
application to the approximation of product distributions. In particular, it is
shown that if the normalized informational divergence of a distribution and a
product distribution approaches zero, then the entropy rate approaches the
entropy rate of the product distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0756</identifier>
 <datestamp>2013-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0756</id><created>2013-02-04</created><updated>2013-09-19</updated><authors><author><keyname>Scutari</keyname><forenames>Gesualdo</forenames></author><author><keyname>Facchinei</keyname><forenames>Francisco</forenames></author><author><keyname>Song</keyname><forenames>Peiran</forenames></author><author><keyname>Palomar</keyname><forenames>Daniel P.</forenames></author><author><keyname>Pang</keyname><forenames>Jong-Shi</forenames></author></authors><title>Decomposition by Partial Linearization: Parallel Optimization of
  Multi-Agent Systems</title><categories>cs.IT math.IT math.OC</categories><comments>submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel decomposition framework for the distributed optimization
of general nonconvex sum-utility functions arising naturally in the system
design of wireless multiuser interfering systems. Our main contributions are:
i) the development of the first class of (inexact) Jacobi best-response
algorithms with provable convergence, where all the users simultaneously and
iteratively solve a suitably convexified version of the original sum-utility
optimization problem; ii) the derivation of a general dynamic pricing mechanism
that provides a unified view of existing pricing schemes that are based,
instead, on heuristics; and iii) a framework that can be easily particularized
to well-known applications, giving rise to very efficient practical (Jacobi or
Gauss-Seidel) algorithms that outperform existing adhoc methods proposed for
very specific problems. Interestingly, our framework contains as special cases
well-known gradient algorithms for nonconvex sum-utility problems, and many
blockcoordinate descent schemes for convex functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0778</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0778</id><created>2013-02-04</created><authors><author><keyname>Buliga</keyname><forenames>Marius</forenames></author></authors><title>On graphic lambda calculus and the dual of the graphic beta move</title><categories>math.GT cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a short description of graphic lambda calculus, with special emphasis
on a duality suggested by the two different appearances of knot diagrams, in
lambda calculus and emergent algebra sectors of the graphic lambda calculus
respectively. This duality leads to the introduction of the dual of the graphic
beta move. While the graphic beta move corresponds to beta reduction in untyped
lambda calculus, the dual graphic beta move appears in relation to emergent
algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0780</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0780</id><created>2013-02-04</created><authors><author><keyname>Burger</keyname><forenames>Mathias</forenames></author><author><keyname>De Persis</keyname><forenames>Claudio</forenames></author></authors><title>Internal models for nonlinear output agreement and optimal flow control</title><categories>cs.SY math.OC</categories><comments>14 pages; submitted to the 9th IFAC Symposium on Nonlinear Control
  Systems (NOLCOS2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of output agreement in networks of nonlinear
dynamical systems under time-varying disturbances. Necessary and sufficient
conditions for output agreement are derived for the class of incrementally
passive systems. Following this, it is shown that the optimal distribution
problem in dynamic inventory systems with time-varying supply and demand can be
cast as a special version of the output agreement problem. We show in
particular that the time-varying optimal distribution problem can be solved by
applying an internal model controller to the dual variables of a certain convex
network optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0785</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0785</id><created>2013-02-04</created><authors><author><keyname>Gale</keyname><forenames>Ella</forenames></author><author><keyname>Matthews</keyname><forenames>Oliver</forenames></author><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Beyond Markov Chains, Towards Adaptive Memristor Network-based Music
  Generation</title><categories>cs.ET cs.AI cs.NE cs.SD</categories><comments>22 pages, 13 pages, conference paper</comments><msc-class>68Txx</msc-class><acm-class>H.5.5; J.5; C.2.1; F.1.1; G.2.2; I.6; C.1.3; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We undertook a study of the use of a memristor network for music generation,
making use of the memristor's memory to go beyond the Markov hypothesis. Seed
transition matrices are created and populated using memristor equations, and
which are shown to generate musical melodies and change in style over time as a
result of feedback into the transition matrix. The spiking properties of simple
memristor networks are demonstrated and discussed with reference to
applications of music making. The limitations of simulating composing memristor
networks in von Neumann hardware is discussed and a hardware solution based on
physical memristor properties is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0792</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0792</id><created>2013-02-04</created><updated>2014-06-19</updated><authors><author><keyname>Cohen</keyname><forenames>Edith</forenames></author><author><keyname>Hassidim</keyname><forenames>Avinatan</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Raz</keyname><forenames>Danny</forenames></author><author><keyname>Tzur</keyname><forenames>Yoav</forenames></author></authors><title>Probe Scheduling for Efficient Detection of Silent Failures</title><categories>cs.NI cs.DS</categories><comments>23 Pages, 3 figures, A partial version (without some of the proofs)
  Performance Evaluation 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most discovery systems for silent failures work in two phases: a continuous
monitoring phase that detects presence of failures through probe packets and a
localization phase that pinpoints the faulty element(s). This separation is
important because localization requires significantly more resources than
detection and should be initiated only when a fault is present.
  We focus on improving the efficiency of the detection phase, where the goal
is to balance the overhead with the cost associated with longer failure
detection times. We formulate a general model which unifies the treatment of
probe scheduling mechanisms, stochastic or deterministic, and different cost
objectives - minimizing average detection time (SUM) or worst-case detection
time (MAX).
  We then focus on two classes of schedules. {\em Memoryless schedules} -- a
subclass of stochastic schedules which is simple and suitable for distributed
deployment. We show that the optimal memorlyess schedulers can be efficiently
computed by convex programs (for SUM objectives) or linear programs (for MAX
objectives), and surprisingly perhaps, are guaranteed to have expected
detection times that are not too far off the (NP hard) stochastic optima. {\em
Deterministic schedules} allow us to bound the maximum (rather than expected)
cost of undetected faults, but like stochastic schedules, are NP hard to
optimize. We develop novel efficient deterministic schedulers with provable
approximation ratios.
  An extensive simulation study on real networks, demonstrates significant
performance gains of our memoryless and deterministic schedulers over previous
approaches. Our unified treatment also facilitates a clear comparison between
different objectives and scheduling mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0797</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0797</id><created>2013-02-04</created><authors><author><keyname>Gale</keyname><forenames>Ella</forenames></author><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Comparison of Ant-Inspired Gatherer Allocation Approaches using
  Memristor-Based Environmental Models</title><categories>cs.NE</categories><comments>11 pages, 3 figures, conference paper</comments><acm-class>B.7.1; I.2.9; I.2.8; I.6.0</acm-class><journal-ref>Bio-Inspired Models of Networks, Information, and Computing
  Systems, Lecture Notes of the Institute for Computer Sciences, Social
  Informatics and Telecommunications Engineering, Volume 103, 2012, pp 73-84</journal-ref><doi>10.1007/978-3-642-32711-7_6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memristors are used to compare three gathering techniques in an
already-mapped environment where resource locations are known. The All Site
model, which apportions gatherers based on the modeled memristance of that
path, proves to be good at increasing overall efficiency and decreasing time to
fully deplete an environment, however it only works well when the resources are
of similar quality. The Leaf Cutter method, based on Leaf Cutter Ant behaviour,
assigns all gatherers first to the best resource, and once depleted, uses the
All Site model to spread them out amongst the rest. The Leaf Cutter model is
better at increasing resource influx in the short-term and vastly out-performs
the All Site model in a more varied environments. It is demonstrated that
memristor based abstractions of gatherer models provide potential methods for
both the comparison and implementation of agent controls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0806</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0806</id><created>2013-02-04</created><authors><author><keyname>Chen</keyname><forenames>Jinyuan</forenames></author><author><keyname>Yang</keyname><forenames>Sheng</forenames></author><author><keyname>Elia</keyname><forenames>Petros</forenames></author></authors><title>On the Fundamental Feedback-vs-Performance Tradeoff over the MISO-BC
  with Imperfect and Delayed CSIT</title><categories>cs.IT math.IT</categories><comments>An initial version of this paper has been reported as Research Report
  No. RR-12-275 at EURECOM, December 7, 2012. This paper was submitted in part
  to the ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers the multiuser multiple-input single-output (MISO)
broadcast channel (BC), where a transmitter with M antennas transmits
information to K single-antenna users, and where - as expected - the quality
and timeliness of channel state information at the transmitter (CSIT) is
imperfect. Motivated by the fundamental question of how much feedback is
necessary to achieve a certain performance, this work seeks to establish bounds
on the tradeoff between degrees-of-freedom (DoF) performance and CSIT feedback
quality. Specifically, this work provides a novel DoF region outer bound for
the general K-user MISO BC with partial current CSIT, which naturally bridges
the gap between the case of having no current CSIT (only delayed CSIT, or no
CSIT) and the case with full CSIT. The work then characterizes the minimum CSIT
feedback that is necessary for any point of the sum DoF, which is optimal for
the case with M &gt;= K, and the case with M=2, K=3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0820</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0820</id><created>2013-02-04</created><authors><author><keyname>AlGhamdi</keyname><forenames>Rayed</forenames></author><author><keyname>Nguyen</keyname><forenames>Anne</forenames></author><author><keyname>Jones</keyname><forenames>Vicki</forenames></author></authors><title>Wheel of B2C E-commerce Development in Saudi Arabia</title><categories>cs.CY</categories><comments>Conference paper, First International Conference on Robot
  Intelligence Technology and Applications (RiTA 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online retailing (a model of B2C e-commerce) is growing world-wide, with
companies in many countries showing increased sales and productivity as a
result. It has great potential within the global economy. This paper looks at
the current status of online retailing in Saudi Arabia, with particular focus
on what inhibits or enables both the customers and retailers. It also analyses
the status of Government involvement and proposes a layered model, known as the
Wheel of Online Retailing which illustrates how Government intervention can
benefit the e-commerce in Saudi Arabia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0870</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0870</id><created>2013-02-04</created><authors><author><keyname>Baingana</keyname><forenames>Brian</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Centrality-constrained graph embedding</title><categories>stat.ML cs.CV math.OC</categories><comments>Submitted to ICASSP May, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual rendering of graphs is a key task in the mapping of complex network
data. Although most graph drawing algorithms emphasize aesthetic appeal,
certain applications such as travel-time maps place more importance on
visualization of structural network properties. The present paper advocates a
graph embedding approach with centrality considerations to comply with node
hierarchy. The problem is formulated as one of constrained multi-dimensional
scaling (MDS), and it is solved via block coordinate descent iterations with
successive approximations and guaranteed convergence to a KKT point. In
addition, a regularization term enforcing graph smoothness is incorporated with
the goal of reducing edge crossings. Experimental results demonstrate that the
algorithm converges, and can be used to efficiently embed large graphs on the
order of thousands of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0891</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0891</id><created>2013-02-04</created><updated>2014-12-25</updated><authors><author><keyname>Abdulla</keyname><forenames>Mouhamed</forenames></author><author><keyname>Shayan</keyname><forenames>Yousef R.</forenames></author></authors><title>Large-Scale Fading Behavior for a Cellular Network with Uniform Spatial
  Distribution</title><categories>cs.IT cs.NI math.IT</categories><comments>17 pages, 13 figures, 3 tables</comments><journal-ref>Wiley's Wireless Communications and Mobile Computing Journal, pp.
  1-17, Dec. 2014</journal-ref><doi>10.1002/WCM.2565</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale fading (LSF) between interacting nodes is a fundamental element
in radio communications, responsible for weakening the propagation, and thus
worsening the service quality. Given the importance of channel-losses in
general, and the inevitability of random spatial geometry in real-life wireless
networks, it was then natural to merge these two paradigms together in order to
obtain an improved stochastical model for the LSF indicator. Therefore, in
exact closed-form notation, we generically derived the LSF distribution between
a prepositioned reference base-station and an arbitrary node for a
multi-cellular random network model. In fact, we provided an explicit and
definitive formulation that considered at once: the lattice profile, the users'
random geometry, the effect of the far-field phenomenon, the path-loss
behavior, and the stochastic impact of channel scatters. The veracity and
accuracy of the theoretical analysis were also confirmed through Monte Carlo
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0892</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0892</id><created>2013-02-04</created><authors><author><keyname>Braverman</keyname><forenames>Mark</forenames></author><author><keyname>Oshri</keyname><forenames>Gal</forenames></author></authors><title>Search using queries on indistinguishable items</title><categories>cs.DS</categories><comments>A version of this paper was presented in STACS'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of determining a set S of k indistinguishable
integers in the range [1,n]. The algorithm is allowed to query an integer $q\in
[1,n]$, and receive a response comparing this integer to an integer randomly
chosen from S. The algorithm has no control over which element of S the query q
is compared to. We show tight bounds for this problem. In particular, we show
that in the natural regime where $k\le n$, the optimal number of queries to
attain $n^{-\Omega(1)}$ error probability is $\Theta(k^3 \log n)$. In the
regime where $k&gt;n$, the optimal number of queries is $\Theta(n^2 k \log n)$.
  Our main technical tools include the use of information theory to derive the
lower bounds, and the application of noisy binary search in the spirit of
Feige, Raghavan, Peleg, and Upfal (1994). In particular, our lower bound
technique is likely to be applicable in other situations that involve search
under uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0895</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0895</id><created>2013-02-04</created><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author><author><keyname>Zhang</keyname><forenames>Cun-Hui</forenames></author></authors><title>Exact Sparse Recovery with L0 Projections</title><categories>stat.ML cs.IT cs.LG math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications concern sparse signals, for example, detecting anomalies
from the differences between consecutive images taken by surveillance cameras.
This paper focuses on the problem of recovering a K-sparse signal x in N
dimensions. In the mainstream framework of compressed sensing (CS), the vector
x is recovered from M non-adaptive linear measurements y = xS, where S (of size
N x M) is typically a Gaussian (or Gaussian-like) design matrix, through some
optimization procedure such as linear programming (LP).
  In our proposed method, the design matrix S is generated from an
$\alpha$-stable distribution with $\alpha\approx 0$. Our decoding algorithm
mainly requires one linear scan of the coordinates, followed by a few
iterations on a small number of coordinates which are &quot;undetermined&quot; in the
previous iteration. Comparisons with two strong baselines, linear programming
(LP) and orthogonal matching pursuit (OMP), demonstrate that our algorithm can
be significantly faster in decoding speed and more accurate in recovery
quality, for the task of exact spare recovery. Our procedure is robust against
measurement noise. Even when there are no sufficient measurements, our
algorithm can still reliably recover a significant portion of the nonzero
coordinates.
  To provide the intuition for understanding our method, we also analyze the
procedure by assuming an idealistic setting. Interestingly, when K=2, the
&quot;idealized&quot; algorithm achieves exact recovery with merely 3 measurements,
regardless of N. For general K, the required sample size of the &quot;idealized&quot;
algorithm is about 5K.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0897</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0897</id><created>2013-02-04</created><updated>2013-02-05</updated><authors><author><keyname>Santagati</keyname><forenames>G. Enrico</forenames></author><author><keyname>Melodia</keyname><forenames>Tommaso</forenames></author><author><keyname>Galluccio</keyname><forenames>Laura</forenames></author><author><keyname>Palazzo</keyname><forenames>Sergio</forenames></author></authors><title>Distributed MAC and Rate Adaptation for Ultrasonically Networked
  Implantable Sensors</title><categories>cs.NI</categories><comments>9 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of miniaturized biomedical devices implanted in the human body and
wirelessly internetworked is promising a significant leap forward in medical
treatment of many pervasive diseases. Recognizing the well-understood
limitations of traditional radio-frequency wireless communications in
interconnecting devices within the human body, in this paper we propose for the
first time to develop network protocols for implantable devices based on
ultrasonic transmissions. We start off by assessing the feasibility of using
ultrasonic propagation in human body tissues and by deriving an accurate
channel model for ultrasonic intra-body communications. Then, we propose a new
ultrasonic transmission and multiple access technique, which we refer to as
Ultrasonic WideBand (UsWB). UsWB is based on the idea of transmitting
information bits spread over very short pulses following a time-hopping
pattern. The short impulse duration results in limited reflection and
scattering effects, and its low duty cycle reduces the thermal and mechanical
effects, which are detrimental for human health. We then develop a multiple
access technique with distributed control to enable efficient simultaneous
access by interfering devices based on minimal and localized information
exchange and on measurements at the receiver only. Finally, we demonstrate the
performance of UsWB through a multi-scale simulator that models the proposed
communication system at the acoustic wave level, at the physical (bit) level,
and at the network (packet) level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0907</identifier>
 <datestamp>2013-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0907</id><created>2013-02-04</created><updated>2013-06-05</updated><authors><author><keyname>DeDeo</keyname><forenames>Simon</forenames></author><author><keyname>Hawkins</keyname><forenames>Robert X. D.</forenames></author><author><keyname>Klingenstein</keyname><forenames>Sara</forenames></author><author><keyname>Hitchcock</keyname><forenames>Tim</forenames></author></authors><title>Bootstrap Methods for the Empirical Study of Decision-Making and
  Information Flows in Social Systems</title><categories>cs.IT cs.SI math.IT physics.soc-ph stat.ME</categories><comments>32 pages, 8 figures, 5 tables. Matched published version. Code for
  NSB, naive, and bootstrap estimation of entropy, mutual information, and
  other quantities available at http://thoth-python.org</comments><journal-ref>Entropy 2013, 15(6), 2246-2276</journal-ref><doi>10.3390/e15062246</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the statistical bootstrap for the estimation of
information-theoretic quantities from data, with particular reference to its
use in the study of large-scale social phenomena. Our methods allow one to
preserve, approximately, the underlying axiomatic relationships of information
theory---in particular, consistency under arbitrary coarse-graining---that
motivate use of these quantities in the first place, while providing
reliability comparable to the state of the art for Bayesian estimators. We show
how information-theoretic quantities allow for rigorous empirical study of the
decision-making capacities of rational agents and the time-asymmetric flows of
information in distributed systems. We provide illustrative examples by
reference to ongoing collaborative work on the semantic structure of the
British Criminal Court system and the conflict dynamics of the contemporary
Afghanistan insurgency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0908</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0908</id><created>2013-02-01</created><authors><author><keyname>Farhi</keyname><forenames>Nadir</forenames></author><author><keyname>Goursat</keyname><forenames>Maurice</forenames></author><author><keyname>Quadrat</keyname><forenames>Jean-Pierre</forenames></author></authors><title>The Traffic Phases of Road Networks</title><categories>math.OC cs.SY math.DS</categories><comments>37 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the relation between the average traffic flow and the vehicle
density on road networks that we call 2D-traffic fundamental diagram. We show
that this diagram presents mainly four phases. We analyze different cases.
First, the case of a junction managed with a priority rule is presented, four
traffic phases are identified and described, and a good analytic approximation
of the fundamental diagram is obtained by computing a generalized eigenvalue of
the dynamics of the system. Then, the model is extended to the case of two
junctions, and finally to a regular city. The system still presents mainly four
phases. The role of a critical circuit of non-priority roads appears clearly in
the two junctions case. In Section 4, we use traffic light controls to improve
the traffic diagram. We present the improvements obtained by open-loop, local
feedback, and global feedback strategies. A comparison based on the response
times to reach the stationary regime is also given. Finally, we show the
importance of the design of the junction. It appears that if the junction is
enough large, the traffic is almost not slowed down by the junction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0914</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0914</id><created>2013-02-04</created><updated>2014-03-28</updated><authors><author><keyname>Ngo</keyname><forenames>Hung Q.</forenames></author><author><keyname>Nguyen</keyname><forenames>Dung T.</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author><author><keyname>Rudra</keyname><forenames>Atri</forenames></author></authors><title>Beyond Worst-Case Analysis for Joins with Minesweeper</title><categories>cs.DB</categories><comments>[This is the full version of our PODS'2014 paper.]</comments><acm-class>H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new algorithm, Minesweeper, that is able to satisfy stronger
runtime guarantees than previous join algorithms (colloquially, `beyond
worst-case guarantees') for data in indexed search trees. Our first
contribution is developing a framework to measure this stronger notion of
complexity, which we call {\it certificate complexity}, that extends notions of
Barbay et al. and Demaine et al.; a certificate is a set of propositional
formulae that certifies that the output is correct. This notion captures a
natural class of join algorithms. In addition, the certificate allows us to
define a strictly stronger notion of runtime complexity than traditional
worst-case guarantees. Our second contribution is to develop a dichotomy
theorem for the certificate-based notion of complexity. Roughly, we show that
Minesweeper evaluates $\beta$-acyclic queries in time linear in the certificate
plus the output size, while for any $\beta$-cyclic query there is some instance
that takes superlinear time in the certificate (and for which the output is no
larger than the certificate size). We also extend our certificate-complexity
analysis to queries with bounded treewidth and the triangle query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0939</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0939</id><created>2013-02-05</created><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author></authors><title>Security and Privacy Issues in Wireless Mesh Networks: A Survey</title><categories>cs.CR cs.NI</categories><comments>62 pages, 12 figures, 6 tables. This chapter is an extension of the
  author's previous submission in arXiv submission: arXiv:1102.1226. There are
  some text overlaps with the previous submission</comments><journal-ref>Book Chapter in Wireless Networks and Security- Issues, Challenges
  and Research Trends, Editors: Shafiullah Khan, Al-Sakib Khan Pathan.
  Springer, 2013, pp. 189-27, February 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This book chapter identifies various security threats in wireless mesh
network (WMN). Keeping in mind the critical requirement of security and user
privacy in WMNs, this chapter provides a comprehensive overview of various
possible attacks on different layers of the communication protocol stack for
WMNs and their corresponding defense mechanisms. First, it identifies the
security vulnerabilities in the physical, link, network, transport, application
layers. Furthermore, various possible attacks on the key management protocols,
user authentication and access control protocols, and user privacy preservation
protocols are presented. After enumerating various possible attacks, the
chapter provides a detailed discussion on various existing security mechanisms
and protocols to defend against and wherever possible prevent the possible
attacks. Comparative analyses are also presented on the security schemes with
regards to the cryptographic schemes used, key management strategies deployed,
use of any trusted third party, computation and communication overhead involved
etc. The chapter then presents a brief discussion on various trust management
approaches for WMNs since trust and reputation-based schemes are increasingly
becoming popular for enforcing security in wireless networks. A number of open
problems in security and privacy issues for WMNs are subsequently discussed
before the chapter is finally concluded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0948</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0948</id><created>2013-02-05</created><updated>2014-04-22</updated><authors><author><keyname>Skowron</keyname><forenames>Piotr</forenames></author><author><keyname>Rzadca</keyname><forenames>Krzysztof</forenames></author></authors><title>Non-monetary fair scheduling---a cooperative game theory approach</title><categories>cs.DC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multi-organizational system in which each organization
contributes processors to the global pool but also jobs to be processed on the
common resources. The fairness of the scheduling algorithm is essential for the
stability and even for the existence of such systems (as organizations may
refuse to join an unfair system).
  We consider on-line, non-clairvoyant scheduling of sequential jobs. The
started jobs cannot be stopped, canceled, preempted, or moved to other
processors. We consider identical processors, but most of our results can be
extended to related or unrelated processors.
  We model the fair scheduling problem as a cooperative game and we use the
Shapley value to determine the ideal fair schedule. In contrast to the current
literature, we do not use money to assess the relative utilities of jobs.
Instead, to calculate the contribution of an organization, we determine how the
presence of this organization influences the performance of other
organizations. Our approach can be used with arbitrary utility function (e.g.,
flow time, tardiness, resource utilization), but we argue that the utility
function should be strategy resilient. The organizations should be discouraged
from splitting, merging or delaying their jobs. We present the unique (to
within a multiplicative and additive constants) strategy resilient utility
function.
  We show that the problem of fair scheduling is NP-hard and hard to
approximate. However, for unit-size jobs, we present an FPRAS. Also, we show
that the problem parametrized with the number of organizations is FPT. Although
for the large number of the organizations the problem is computationally hard,
the presented exponential algorithm can be used as a fairness benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0951</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0951</id><created>2013-02-05</created><authors><author><keyname>Muramatsu</keyname><forenames>Jun</forenames></author></authors><title>Channel Coding and Lossy Source Coding Using a Constrained Random Number
  Generator</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory, 42 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic encoders for channel coding and lossy source coding are introduced
with a rate close to the fundamental limits, where the only restriction is that
the channel input alphabet and the reproduction alphabet of the lossy source
code are finite. Random numbers, which satisfy a condition specified by a
function and its value, are used to construct stochastic encoders. The proof of
the theorems is based on the hash property of an ensemble of functions, where
the results are extended to general channels/sources and alternative formulas
are introduced for channel capacity and the rate-distortion region. Since an
ensemble of sparse matrices has a hash property, we can construct a code by
using sparse matrices, where the sum-product algorithm can be used for encoding
and decoding by assuming that channels/sources are memoryless.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0952</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0952</id><created>2013-02-05</created><authors><author><keyname>Zhou</keyname><forenames>Zhengchun</forenames></author><author><keyname>Ding</keyname><forenames>Cunsheng</forenames></author><author><keyname>Luo</keyname><forenames>Jinquan</forenames></author><author><keyname>Zhang</keyname><forenames>Aixian</forenames></author></authors><title>A Family of Five-Weight Cyclic Codes and Their Weight Enumerators</title><categories>cs.IT math.IT</categories><comments>14 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes are a subclass of linear codes and have applications in consumer
electronics, data storage systems, and communication systems as they have
efficient encoding and decoding algorithms. In this paper, a family of $p$-ary
cyclic codes whose duals have three zeros are proposed. The weight distribution
of this family of cyclic codes is determined. It turns out that the proposed
cyclic codes have five nonzero weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0962</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0962</id><created>2013-02-05</created><authors><author><keyname>Kaur</keyname><forenames>Savinderjit</forenames><affiliation>Department of Information Technology, UIET, PU, Chandigarh, India</affiliation></author><author><keyname>Mangat</keyname><forenames>Veenu</forenames><affiliation>Department of Information Technology, UIET, PU, Chandigarh, India</affiliation></author></authors><title>Improved Accuracy of PSO and DE using Normalization: an Application to
  Stock Price Prediction</title><categories>cs.NE cs.LG</categories><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 3, No. 9, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data Mining is being actively applied to stock market since 1980s. It has
been used to predict stock prices, stock indexes, for portfolio management,
trend detection and for developing recommender systems. The various algorithms
which have been used for the same include ANN, SVM, ARIMA, GARCH etc. Different
hybrid models have been developed by combining these algorithms with other
algorithms like roughest, fuzzy logic, GA, PSO, DE, ACO etc. to improve the
efficiency. This paper proposes DE-SVM model (Differential EvolutionSupport
vector Machine) for stock price prediction. DE has been used to select best
free parameters combination for SVM to improve results. The paper also compares
the results of prediction with the outputs of SVM alone and PSO-SVM model
(Particle Swarm Optimization). The effect of normalization of data on the
accuracy of prediction has also been studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0963</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0963</id><created>2013-02-05</created><authors><author><keyname>Paisitkriangkrai</keyname><forenames>Sakrapee</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Shi</keyname><forenames>Qinfeng</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>RandomBoost: Simplified Multi-class Boosting through Randomization</title><categories>cs.LG</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel boosting approach to multi-class classification problems,
in which multiple classes are distinguished by a set of random projection
matrices in essence. The approach uses random projections to alleviate the
proliferation of binary classifiers typically required to perform multi-class
classification. The result is a multi-class classifier with a single
vector-valued parameter, irrespective of the number of classes involved. Two
variants of this approach are proposed. The first method randomly projects the
original data into new spaces, while the second method randomly projects the
outputs of learned weak classifiers. These methods are not only conceptually
simple but also effective and easy to implement. A series of experiments on
synthetic, machine learning and visual recognition data sets demonstrate that
our proposed methods compare favorably to existing multi-class boosting
algorithms in terms of both the convergence rate and classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0965</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0965</id><created>2013-02-05</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Sharma</keyname><forenames>Tanu</forenames></author><author><keyname>Sharma</keyname><forenames>Ritu</forenames></author></authors><title>Adaptive Energy Aware Data Aggregation Tree for Wireless Sensor Networks</title><categories>cs.NI</categories><comments>12 pages, 8 figures, International Journal of Hybrid Information
  Technology Vol. 6, No. 1, January, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To meet the demands of wireless sensor networks (WSNs) where data are usually
aggregated at a single source prior to transmitting to any distant user, there
is a need to establish a tree structure inside to aggregate data. In this
paper, an adaptive energy aware data aggregation tree (AEDT) is proposed. The
proposed tree uses the maximum energy available node as the data aggregator
node. The tree incorporates sleep and awake technology where the communicating
node and the parent node are only in awake state rest all the nodes go to sleep
state saving the network energy and enhancing the network lifetime. When the
traffic load crosses the threshold value, then the packets are accepted
adaptively according to the communication capacity of the parent node. The
proposed tree maintains a memory table which stores the value of each selected
path. Path selection is based on shortest path algorithm where the node with
highest available energy is always selected as forwarding node. By simulation
results, we show that our proposed tree enhances network lifetime minimizes
energy consumption and achieves good delivery ratio with reduced delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0971</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0971</id><created>2013-02-05</created><authors><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author></authors><title>Validasi data dengan menggunakan objek lookup pada borland delphi 7.0</title><categories>cs.DB</categories><comments>16 pages</comments><journal-ref>MATRIK. 7 (2005) 1-16</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing an application with some tables must concern the validation of
input (specially in Table Child). In order to maximize the accuracy and data
input validation. Its called lookup (took data from other dataset). There are
two ways to look up data from Table Parent: 1) Using Objects (DBLookupComboBox
and DBookupListBox), or 2) Arranging the properties of data types fields (shown
by using DBGrid). In this article is using Borland Delphi software (Inprise
product). The method is offered using 5 (five) practise steps: 1) Relational
Database Scheme, 2) Form Design, 3) Object DatabasesRelationships Scheme, 4)
Properties and Field Type Arrangement, and 5) Procedures. The result of this
paper are: 1) The relationship that using lookup objects are valid, and 2)
Delphi Lookup Objects can be used for 1-1, 1-N, and M-N relationship.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0973</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0973</id><created>2013-02-05</created><authors><author><keyname>Avanzini</keyname><forenames>Martin</forenames></author><author><keyname>Moser</keyname><forenames>Georg</forenames></author></authors><title>A Combination Framework for Complexity</title><categories>cs.CC</categories><acm-class>F.1.3; F.3.2; F.4.1; F.4.2</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we present a combination framework for polynomial complexity
analysis of term rewrite systems. The framework covers both derivational and
runtime complexity analysis. We present generalisations of powerful complexity
techniques, notably a generalisation of complexity pairs and (weak) dependency
pairs. Finally, we also present a novel technique, called dependency graph
decomposition, that in the dependency pair setting greatly increases
modularity. We employ the framework in the automated complexity tool TCT. TCT
implements a majority of the techniques found in the literature, witnessing
that our framework is general enough to capture a very brought setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0974</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0974</id><created>2013-02-05</created><authors><author><keyname>Rupnik</keyname><forenames>Jan</forenames></author><author><keyname>Skraba</keyname><forenames>Primoz</forenames></author><author><keyname>Shawe-Taylor</keyname><forenames>John</forenames></author><author><keyname>Guettes</keyname><forenames>Sabrina</forenames></author></authors><title>A Comparison of Relaxations of Multiset Cannonical Correlation Analysis
  and Applications</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Canonical correlation analysis is a statistical technique that is used to
find relations between two sets of variables. An important extension in pattern
analysis is to consider more than two sets of variables. This problem can be
expressed as a quadratically constrained quadratic program (QCQP), commonly
referred to Multi-set Canonical Correlation Analysis (MCCA). This is a
non-convex problem and so greedy algorithms converge to local optima without
any guarantees on global optimality. In this paper, we show that despite being
highly structured, finding the optimal solution is NP-Hard. This motivates our
relaxation of the QCQP to a semidefinite program (SDP). The SDP is convex, can
be solved reasonably efficiently and comes with both absolute and
output-sensitive approximation quality. In addition to theoretical guarantees,
we do an extensive comparison of the QCQP method and the SDP relaxation on a
variety of synthetic and real world data. Finally, we present two useful
extensions: we incorporate kernel methods and computing multiple sets of
canonical vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.0975</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.0975</id><created>2013-02-05</created><authors><author><keyname>Ramezanian</keyname><forenames>Rasoul</forenames></author></authors><title>A Constructive Epistemic Logic with Public Announcement
  (Non-Predetermined Possibilities)</title><categories>cs.LO</categories><comments>12 page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We argue that the notion of epistemic \emph{possible worlds} in
constructivism (intuitionism) is not as the same as it is in classic view, and
there are possibilities, called non-predetermined worlds, which are ignored in
(classic) Epistemic Logic. Regarding non-predetermined possibilities, we
propose a constructive epistemic logic and prove soundness and completeness
theorems for it. We extend the proposed logic by adding a public announcement
operator. To declare the significance of our work, we formulate the well-known
Surprise Exam Paradox, $\mathbf{SEP}$, via the proposed constructive epistemic
logic and then put forward a solution for the paradox. We clarify that the
puzzle in the $\mathbf{SEP}$ is because of students'(wrong) assumption that the
day of the exam is necessarily predetermined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1005</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1005</id><created>2013-02-05</created><authors><author><keyname>Merrikh-Bayat</keyname><forenames>Farshad</forenames></author><author><keyname>Mirebrahimi</keyname><forenames>Nafiseh</forenames></author><author><keyname>Bayat</keyname><forenames>Farhad</forenames></author></authors><title>Circuit proposition for copying the value of a resistor into a
  memristive device supported by HSPICE simulation</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memristor is the fourth fundamental passive circuit element with potential
applications in development of analog memories, artificial brains (with the
capacity of hardware training) and neuro-science. In most of these applications
the memristance of the device should be set to the desired value, which is
currently performed by trial and error. The aim of this paper is to propose a
circuit for copying the value of the given resistor into a memristive device.
HSPICE simulations are also presented to confirm the efficiency of the proposed
circuit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1007</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1007</id><created>2013-02-05</created><authors><author><keyname>Jassim</keyname><forenames>Firas Ajil</forenames></author></authors><title>Image Denoising Using Interquartile Range Filter with Local Averaging</title><categories>cs.CV</categories><comments>5 pages, 8 figures, 2 tables</comments><journal-ref>International Journal of Soft Computing and Engineering (IJSCE)
  ISSN: 2231-2307, Volume-2, Issue-6, January 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Image denoising is one of the fundamental problems in image processing. In
this paper, a novel approach to suppress noise from the image is conducted by
applying the interquartile range (IQR) which is one of the statistical methods
used to detect outlier effect from a dataset. A window of size kXk was
implemented to support IQR filter. Each pixel outside the IQR range of the kXk
window is treated as noisy pixel. The estimation of the noisy pixels was
obtained by local averaging. The essential advantage of applying IQR filter is
to preserve edge sharpness better of the original image. A variety of test
images have been used to support the proposed filter and PSNR was calculated
and compared with median filter. The experimental results on standard test
images demonstrate this filter is simpler and better performing than median
filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1008</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1008</id><created>2013-02-05</created><authors><author><keyname>Rezaee</keyname><forenames>Mohsen</forenames></author><author><keyname>Guillaud</keyname><forenames>Maxime</forenames></author><author><keyname>Lindqvist</keyname><forenames>Fredrik</forenames></author></authors><title>CSIT Sharing over Finite Capacity Backhaul for Spatial Interference
  Alignment</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular systems that employ time division duplexing (TDD) transmission are
good candidates for implementation of interference alignment (IA) in the
downlink since channel reciprocity enables the estimation of the channel state
by the base stations (BS) in the uplink phase. However, the interfering BSs
need to share their channel estimates via backhaul links of finite capacity. A
quantization scheme is proposed which reduces the amount of information
exchange (compared to conventional methods) required to achieve IA in a TDD
system. The scaling (with the transmit power) of the number of bits to be
exchanged between the BSs that is sufficient to preserve the multiplexing gain
of IA is derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1010</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1010</id><created>2013-02-05</created><authors><author><keyname>Santy</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Nelissen</keyname><forenames>Geoffrey</forenames></author><author><keyname>Goossens</keyname><forenames>Jo&#xeb;l</forenames></author></authors><title>Improving Mixed-Criticality System Consistency and Behavior on
  Multiprocessor Platforms by Means of Multi-Moded Approaches</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research in the domain of real-time scheduling theory has tackled the
problem of scheduling mixed-criticality systems upon uniprocessor or
multiprocessor platforms, with the main objective being to respect the
timeliness of the most critical tasks, at the expense of the requirements of
the less critical ones. In particular, the less critical tasks are carelessly
discarded when the computation demand of (some of) the high critical tasks
increases. This might nevertheless result in system failure, as these less
critical tasks could be accessing data, the consistency of which should be
preserved. In this paper, we address this problem and propose a method to
cautiously handle task suspension. Furthermore, it is usually assumed that the
less critical tasks will never be re-enabled once discarded. In this paper, we
also address this concern by proposing an approach to re-enable the less
critical tasks, without jeopardizing the timeliness of the high critical ones.
The suggested approaches apply to systems having two or more criticality
levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1020</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1020</id><created>2013-02-05</created><authors><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author><author><keyname>Amjad</keyname><forenames>Rana Ali</forenames></author></authors><title>Block-to-Block Distribution Matching</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, binary block-to-block distribution matching is considered. m
independent and uniformly distributed bits are mapped to n output bits
resembling a target product distribution. A rate R is called achieved by a
sequence of encoder-decoder pairs, if for m,n to infinity, (1) m/n approaches
R, (2) the informational divergence per bit of the output distribution and the
target distribution goes to zero, and (3) the probability of erroneous decoding
goes to zero. It is shown that the maximum achievable rate is equal to the
entropy of the target distribution. A practical encoder-decoder pair is
constructed that provably achieves the maximum rate in the limit. Numerical
results illustrate that the suggested system operates close to the limits with
reasonable complexity. The key idea is to internally use a fixed-to-variable
length matcher and to compensate underflow by random mapping and to cast an
error when overflow occurs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1035</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1035</id><created>2013-02-05</created><authors><author><keyname>Grassl</keyname><forenames>Markus</forenames><affiliation>Centre for Quantum Technologies, Singapore</affiliation></author><author><keyname>Roetteler</keyname><forenames>Martin</forenames><affiliation>NEC Laboratories America, Princeton</affiliation></author></authors><title>Leveraging Automorphisms of Quantum Codes for Fault-Tolerant Quantum
  Computation</title><categories>quant-ph cs.IT math.IT</categories><journal-ref>Proceedings 2013 IEEE International Symposium on Information
  Theory (ISIT 2013), Istanbul, Turkey, pp. 534-538</journal-ref><doi>10.1109/ISIT.2013.6620283</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fault-tolerant quantum computation is a technique that is necessary to build
a scalable quantum computer from noisy physical building blocks. Key for the
implementation of fault-tolerant computations is the ability to perform a
universal set of quantum gates that act on the code space of an underlying
quantum code. To implement such a universal gate set fault-tolerantly is an
expensive task in terms of physical operations, and any possible shortcut to
save operations is potentially beneficial and might lead to a reduction in
overhead for fault-tolerant computations. We show how the automorphism group of
a quantum code can be used to implement some operators on the encoded quantum
states in a fault-tolerant way by merely permuting the physical qubits. We
derive conditions that a code has to satisfy in order to have a large group of
operations that can be implemented transversally when combining transversal
CNOT with automorphisms. We give several examples for quantum codes with large
groups, including codes with parameters [[8,3,3]], [[15,7,3]], [[22,8,4]], and
[[31,11,5]].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1043</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1043</id><created>2013-02-05</created><updated>2013-07-09</updated><authors><author><keyname>Daniely</keyname><forenames>Amit</forenames></author><author><keyname>Helbertal</keyname><forenames>Tom</forenames></author></authors><title>The price of bandit information in multiclass online classification</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two scenarios of multiclass online learning of a hypothesis class
$H\subseteq Y^X$. In the {\em full information} scenario, the learner is
exposed to instances together with their labels. In the {\em bandit} scenario,
the true label is not exposed, but rather an indication whether the learner's
prediction is correct or not. We show that the ratio between the error rates in
the two scenarios is at most $8\cdot|Y|\cdot \log(|Y|)$ in the realizable case,
and $\tilde{O}(\sqrt{|Y|})$ in the agnostic case. The results are tight up to a
logarithmic factor and essentially answer an open question from (Daniely et.
al. - Multiclass learnability and the erm principle).
  We apply these results to the class of $\gamma$-margin multiclass linear
classifiers in $\reals^d$. We show that the bandit error rate of this class is
$\tilde{\Theta}(\frac{|Y|}{\gamma^2})$ in the realizable case and
$\tilde{\Theta}(\frac{1}{\gamma}\sqrt{|Y|T})$ in the agnostic case. This
resolves an open question from (Kakade et. al. - Efficient bandit algorithms
for online multiclass prediction).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1046</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1046</id><created>2013-02-05</created><updated>2013-03-01</updated><authors><author><keyname>Silva</keyname><forenames>Alexandra</forenames><affiliation>Radboud University Nijmegen and Centrum Wiskunde &amp; Informatica</affiliation></author><author><keyname>Bonchi</keyname><forenames>Filippo</forenames><affiliation>ENS Lyon, Universite' de Lyon, LIP</affiliation></author><author><keyname>Bonsangue</keyname><forenames>Marcello</forenames><affiliation>LIACS - Leiden University</affiliation></author><author><keyname>Rutten</keyname><forenames>Jan</forenames><affiliation>Centrum Wiskunde &amp; Informatica and Radboud University Nijmegen</affiliation></author></authors><title>Generalizing determinization from automata to coalgebras</title><categories>cs.LO</categories><comments>23 pages</comments><proxy>LMCS</proxy><acm-class>F.3.2</acm-class><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 1 (March 4,
  2013) lmcs:1087</journal-ref><doi>10.2168/LMCS-9(1:9)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The powerset construction is a standard method for converting a
nondeterministic automaton into a deterministic one recognizing the same
language. In this paper, we lift the powerset construction from automata to the
more general framework of coalgebras with structured state spaces. Coalgebra is
an abstract framework for the uniform study of different kinds of dynamical
systems. An endofunctor F determines both the type of systems (F-coalgebras)
and a notion of behavioural equivalence (~_F) amongst them. Many types of
transition systems and their equivalences can be captured by a functor F. For
example, for deterministic automata the derived equivalence is language
equivalence, while for non-deterministic automata it is ordinary bisimilarity.
We give several examples of applications of our generalized determinization
construction, including partial Mealy machines, (structured) Moore automata,
Rabin probabilistic automata, and, somewhat surprisingly, even pushdown
automata. To further witness the generality of the approach we show how to
characterize coalgebraically several equivalences which have been object of
interest in the concurrency community, such as failure or ready semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1064</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1064</id><created>2013-02-05</created><updated>2013-02-06</updated><authors><author><keyname>K&#xe4;rkk&#xe4;inen</keyname><forenames>Juha</forenames></author><author><keyname>Kempa</keyname><forenames>Dominik</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author></authors><title>Lightweight Lempel-Ziv Parsing</title><categories>cs.DS</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new approach to LZ77 factorization that uses O(n/d) words of
working space and O(dn) time for any d &gt;= 1 (for polylogarithmic alphabet
sizes). We also describe carefully engineered implementations of alternative
approaches to lightweight LZ77 factorization. Extensive experiments show that
the new algorithm is superior in most cases, particularly at the lowest memory
levels and for highly repetitive data. As a part of the algorithm, we describe
new methods for computing matching statistics which may be of independent
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1078</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1078</id><created>2013-02-05</created><authors><author><keyname>Saule</keyname><forenames>Erik</forenames></author><author><keyname>Kaya</keyname><forenames>Kamer</forenames></author><author><keyname>Catalyurek</keyname><forenames>Umit V.</forenames></author></authors><title>Performance Evaluation of Sparse Matrix Multiplication Kernels on Intel
  Xeon Phi</title><categories>cs.PF cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intel Xeon Phi is a recently released high-performance coprocessor which
features 61 cores each supporting 4 hardware threads with 512-bit wide SIMD
registers achieving a peak theoretical performance of 1Tflop/s in double
precision. Many scientific applications involve operations on large sparse
matrices such as linear solvers, eigensolver, and graph mining algorithms. The
core of most of these applications involves the multiplication of a large,
sparse matrix with a dense vector (SpMV). In this paper, we investigate the
performance of the Xeon Phi coprocessor for SpMV. We first provide a
comprehensive introduction to this new architecture and analyze its peak
performance with a number of micro benchmarks. Although the design of a Xeon
Phi core is not much different than those of the cores in modern processors,
its large number of cores and hyperthreading capability allow many application
to saturate the available memory bandwidth, which is not the case for many
cutting-edge processors. Yet, our performance studies show that it is the
memory latency not the bandwidth which creates a bottleneck for SpMV on this
architecture. Finally, our experiments show that Xeon Phi's sparse kernel
performance is very promising and even better than that of cutting-edge general
purpose processors and GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1079</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1079</id><created>2013-01-30</created><authors><author><keyname>Michelusi</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Levorato</keyname><forenames>Marco</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Cognitive Access Policies under a Primary ARQ process via
  Forward-Backward Interference Cancellation</title><categories>cs.IT cs.SY math.IT</categories><comments>16 pages, 11 figures, 2 tables</comments><doi>10.1109/JSAC.2013.131112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel technique for access by a cognitive Secondary
User (SU) using best-effort transmission to a spectrum with an incumbent
Primary User (PU), which uses Type-I Hybrid ARQ. The technique leverages the
primary ARQ protocol to perform Interference Cancellation (IC) at the SU
receiver (SUrx). Two IC mechanisms that work in concert are introduced: Forward
IC, where SUrx, after decoding the PU message, cancels its interference in the
(possible) following PU retransmissions of the same message, to improve the SU
throughput; Backward IC, where SUrx performs IC on previous SU transmissions,
whose decoding failed due to severe PU interference. Secondary access policies
are designed that determine the secondary access probability in each state of
the network so as to maximize the average long-term SU throughput by
opportunistically leveraging IC, while causing bounded average long-term PU
throughput degradation and SU power expenditure. It is proved that the optimal
policy prescribes that the SU prioritizes its access in the states where SUrx
knows the PU message, thus enabling IC. An algorithm is provided to optimally
allocate additional secondary access opportunities in the states where the PU
message is unknown. Numerical results are shown to assess the throughput gain
provided by the proposed techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1089</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1089</id><created>2013-02-05</created><authors><author><keyname>Kostitsyna</keyname><forenames>Irina</forenames></author><author><keyname>Mitchell</keyname><forenames>Joseph</forenames></author></authors><title>Local Redesigning of Airspace Sectors</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the Airspace Sectorization Problem (ASP) where the
goal is to find an optimal partition (sectorization) of the airspace into a
certain number of sectors, each managed by an air traffic controller. The
objective of the ASP is to find a &quot;well-balanced&quot; sectorization that
distributes the workload evenly among the controllers. We formulate the ASP as
a partitioning problem of a set of moving points in a polygonal domain. In
addition to the requirement of balancing the workload, we introduce
restrictions on the geometry of the sectorization which come from the Air
Traffic Management aspects. We investigate several versions of the problem that
arise from different definitions of the notion of the workload and various
choices of geometric restrictions on the sectorization. We conclude that most
of the formulations of the problem, except maybe in some trivial cases, are
NP-hard. Finally, we propose a Local Redesigning Method (LRM), a heuristic
algorithm that rebalances a given sectorization by adjusting the boundaries of
the sectors. We evaluate LRM experimentally on synthetically generated
scenarios as well as on the real historical traffic data. We demonstrate that
the sectorizations produced by our method are superior in comparison with the
current sectorizations of the US airspace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1094</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1094</id><created>2013-02-05</created><updated>2013-03-26</updated><authors><author><keyname>W&#xf6;rmann</keyname><forenames>Julian</forenames></author><author><keyname>Hawe</keyname><forenames>Simon</forenames></author><author><keyname>Kleinsteuber</keyname><forenames>Martin</forenames></author></authors><title>Analysis Based Blind Compressive Sensing</title><categories>cs.IT math.IT</categories><comments>7 pages, 2 figures</comments><doi>10.1109/LSP.2013.2252900</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we address the problem of blindly reconstructing compressively
sensed signals by exploiting the co-sparse analysis model. In the analysis
model it is assumed that a signal multiplied by an analysis operator results in
a sparse vector. We propose an algorithm that learns the operator adaptively
during the reconstruction process. The arising optimization problem is tackled
via a geometric conjugate gradient approach. Different types of sampling noise
are handled by simply exchanging the data fidelity term. Numerical experiments
are performed for measurements corrupted with Gaussian as well as impulsive
noise to show the effectiveness of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1105</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1105</id><created>2013-02-05</created><authors><author><keyname>Odlyzko</keyname><forenames>Andrew</forenames></author></authors><title>Open Access, library and publisher competition, and the evolution of
  general commerce</title><categories>cs.DL cs.CY cs.SI math.HO physics.soc-ph</categories><msc-class>00, 91</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discussions of the economics of scholarly communication are usually devoted
to Open Access, rising journal prices, publisher profits, and boycotts. That
ignores what seems a much more important development in this market.
Publishers, through the oft-reviled &quot;Big Deal&quot; packages, are providing much
greater and more egalitarian access to the journal literature, an approximation
to true Open Access. In the process they are also marginalizing libraries, and
obtaining a greater share of the resources going into scholarly communication.
This is enabling a continuation of publisher profits as well as of what for
decades has been called &quot;unsustainable journal price escalation&quot;. It is also
inhibiting the spread of Open Access, and potentially leading to an oligopoly
of publishers controlling distribution through large-scale licensing.
  The &quot;Big Deal&quot; practices are worth studying for several general reasons. The
degree to which publishers succeed in diminishing the role of libraries may be
an indicator of the degree and speed at which universities transform
themselves. More importantly, these &quot;Big Deals&quot; appear to point the way to the
future of the whole economy, where progress is characterized by declining
privacy, increasing price discrimination, increasing opaqueness in pricing,
increasing reliance on low-paid or upaid work of others for profits, and
business models that depend on customer inertia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1109</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1109</id><created>2013-02-05</created><authors><author><keyname>Zimand</keyname><forenames>Marius</forenames></author></authors><title>Short lists with short programs in short time - a short proof</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bauwens, Mahklin, Vereshchagin and Zimand [ECCC TR13-007] and Teutsch
[arxiv:1212.6104] have shown that given a string x it is possible to construct
in polynomial time a list containing a short description of it. We simplify
their technique and present a shorter proof of this result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1111</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1111</id><created>2013-02-05</created><authors><author><keyname>Y&#xfc;ksel</keyname><forenames>Ender</forenames></author><author><keyname>Nielson</keyname><forenames>Hanne Riis</forenames></author><author><keyname>Nielson</keyname><forenames>Flemming</forenames></author></authors><title>Design-Efficiency in Security</title><categories>cs.CR</categories><report-no>IMM--TECHNICALREPORT?2013-03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this document, we present our applied results on balancing security and
performance using a running example, which is based on sensor networks. These
results are forming a basis for a new approach to balance security and
performance, and therefore provide design-efficiency of key updates.
  We employ probabilistic model checking approach and present our modelling and
analysis study using PRISM model checker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1123</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1123</id><created>2013-02-05</created><authors><author><keyname>Chelba</keyname><forenames>Ciprian</forenames></author><author><keyname>Xu</keyname><forenames>Peng</forenames></author><author><keyname>Pereira</keyname><forenames>Fernando</forenames></author><author><keyname>Richardson</keyname><forenames>Thomas</forenames></author></authors><title>Large Scale Distributed Acoustic Modeling With Back-off N-grams</title><categories>cs.CL</categories><msc-class>68T10</msc-class><acm-class>I.2.7</acm-class><doi>10.1109/TASL.2013.2245649</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper revives an older approach to acoustic modeling that borrows from
n-gram language modeling in an attempt to scale up both the amount of training
data and model size (as measured by the number of parameters in the model), to
approximately 100 times larger than current sizes used in automatic speech
recognition. In such a data-rich setting, we can expand the phonetic context
significantly beyond triphones, as well as increase the number of Gaussian
mixture components for the context-dependent states that allow it. We have
experimented with contexts that span seven or more context-independent phones,
and up to 620 mixture components per state. Dealing with unseen phonetic
contexts is accomplished using the familiar back-off technique used in language
modeling due to implementation simplicity. The back-off acoustic model is
estimated, stored and served using MapReduce distributed computing
infrastructure.
  Speech recognition experiments are carried out in an N-best list rescoring
framework for Google Voice Search. Training big models on large amounts of data
proves to be an effective way to increase the accuracy of a state-of-the-art
automatic speech recognition system. We use 87,000 hours of training data
(speech along with transcription) obtained by filtering utterances in Voice
Search logs on automatic speech recognition confidence. Models ranging in size
between 20--40 million Gaussians are estimated using maximum likelihood
training. They achieve relative reductions in word-error-rate of 11% and 6%
when combined with first-pass models trained using maximum likelihood, and
boosted maximum mutual information, respectively. Increasing the context size
beyond five phones (quinphones) does not help.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1128</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1128</id><created>2013-02-05</created><authors><author><keyname>Karafyllis</keyname><forenames>Iasson</forenames></author><author><keyname>Krstic</keyname><forenames>Miroslav</forenames></author></authors><title>On the Relation of Delay Equations to First-Order Hyperbolic Partial
  Differential Equations</title><categories>math.OC cs.SY math.AP math.DS</categories><comments>32 pages, submitted for possible publication to ESAIM COCV</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper establishes the equivalence between systems described by a single
first-order hyperbolic partial differential equation and systems described by
integral delay equations. System-theoretic results are provided for both
classes of systems (among them converse Lyapunov results). The proposed
framework can allow the study of discontinuous solutions for nonlinear systems
described by a single first-order hyperbolic partial differential equation
under the effect of measurable inputs acting on the boundary and/or on the
differential equation. An illustrative example shows that the conversion of a
system described by a single first-order hyperbolic partial differential
equation to an integral delay system can simplify considerably the solution of
the corresponding robust feedback stabilization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1131</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1131</id><created>2013-02-05</created><authors><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Jayaraman</keyname><forenames>Prem</forenames></author><author><keyname>Zaslavsky</keyname><forenames>Arkady</forenames></author><author><keyname>Christen</keyname><forenames>Peter</forenames></author><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author></authors><title>Dynamic Configuration of Sensors Using Mobile Sensor Hub in Internet of
  Things Paradigm</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of Things (IoT) envisions billions of sensors to be connected to the
Internet. By deploying intelligent low-level computational devices such as
mobile phones in-between sensors and cloud servers, we can reduce data
communication with the use of intelligent processing such as fusing and
filtering sensor data, which saves significant amount of energy. This is also
ideal for real world sensor deployments where connecting sensors directly to a
computer or to the Internet is not practical. Most of the leading IoT
middleware solutions require manual and labour intensive tasks to be completed
in order to connect a mobile phone to them. In this paper we present a mobile
application called Mobile Sensor Hub (MoSHub). It allows variety of different
sensors to be connected to a mobile phone and send the data to the cloud
intelligently reducing network communication. Specifically, we explore
techniques that allow MoSHub to be connected to cloud based IoT middleware
solutions autonomously. For our experiments, we employed Global Sensor Network
(GSN) middleware to implement and evaluate our approach. Such automated
configuration reduces significant amount of manual labour that need to be
performed by technical experts otherwise. We also evaluated different methods
that can be used to automate the configuration process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1134</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1134</id><created>2013-02-05</created><authors><author><keyname>Garrison</keyname><forenames>William C.</forenames><suffix>III</suffix></author><author><keyname>Lee</keyname><forenames>Adam J.</forenames></author><author><keyname>Hinrichs</keyname><forenames>Timothy L.</forenames></author></authors><title>The Design and Demonstration of an Actor-Based, Application-Aware Access
  Control Evaluation Framework</title><categories>cs.CR</categories><comments>27-page extended version of &quot;An Actor-Based, Application-Aware Access
  Control Evaluation Framework&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To date, most work regarding the formal analysis of access control schemes
has focused on quantifying and comparing the expressive power of a set of
schemes. Although expressive power is important, it is a property that exists
in an absolute sense, detached from the application-specific context within
which an access control scheme will ultimately be deployed. In this paper, by
contrast, we formalize the access control suitability analysis problem, which
seeks to evaluate the degree to which a set of candidate access control schemes
can meet the needs of an application-specific workload. This process involves
both reductions to assess whether a scheme is capable of implementing a
workload, as well as cost analysis using ordered measures to quantify the
overheads of using each candidate scheme to service the workload. We develop a
mathematical framework for analyzing instances of the suitability analysis
problem, and evaluate this framework both formally (by quantifying its
efficiency and accuracy properties) and practically (by exploring a group-based
messaging workload from the literature). An ancillary contribution of our work
is the identification of auxiliary machines, which are a useful class of
modifications that can be made to enhance the expressive power of an access
control scheme without negatively impacting the safety properties of the
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1143</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1143</id><created>2013-02-05</created><authors><author><keyname>Lehman</keyname><forenames>Joel</forenames></author><author><keyname>Stanley</keyname><forenames>Kenneth O.</forenames></author></authors><title>Evolvability Is Inevitable: Increasing Evolvability Without the Pressure
  to Adapt</title><categories>cs.NE q-bio.PE</categories><doi>10.1371/journal.pone.0062186</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Why evolvability appears to have increased over evolutionary time is an
important unresolved biological question. Unlike most candidate explanations,
this paper proposes that increasing evolvability can result without any
pressure to adapt. The insight is that if evolvability is heritable, then an
unbiased drifting process across genotypes can still create a distribution of
phenotypes biased towards evolvability, because evolvable organisms diffuse
more quickly through the space of possible phenotypes. Furthermore, because
phenotypic divergence often correlates with founding niches, niche founders may
on average be more evolvable, which through population growth provides a
genotypic bias towards evolvability. Interestingly, the combination of these
two mechanisms can lead to increasing evolvability without any pressure to
out-compete other organisms, as demonstrated through experiments with a series
of simulated models. Thus rather than from pressure to adapt, evolvability may
inevitably result from any drift through genotypic space combined with
evolution's passive tendency to accumulate niches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1153</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1153</id><created>2013-02-05</created><authors><author><keyname>Lopez</keyname><forenames>Moises Homero Sanchez</forenames></author><author><keyname>Fernandez-y-Fernandez</keyname><forenames>Carlos Alberto</forenames></author><author><keyname>Cisneros</keyname><forenames>Jorge Rafael Aguilar</forenames></author></authors><title>On the need for optimization of the software development processes in
  short-term projects</title><categories>cs.SE</categories><comments>8 pages, conference proceedings T\'opicos Selectos de Tecnolog\'ias
  de la Informaci\'on y Comunicaciones in Proceedings of the XXV Congreso
  Nacional y XI Congreso Internacional de Inform\'atica y Computaci\'on ANIEI
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, most of the software development projects in Mexico are short-term
projects (micro and small projects); for this reason, in this paper we are
presenting a research proposal with the goal of identifying the elements
contributing to their success or failure. With this research, we are trying to
identify and propose techniques and tools that would contribute in the
successful outcome of these projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1155</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1155</id><created>2013-02-05</created><authors><author><keyname>Ammon</keyname><forenames>Kurt</forenames></author></authors><title>An Effective Procedure for Computing &quot;Uncomputable&quot; Functions</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an effective procedure that produces a natural number in its output
from any natural number in its input, that is, it computes a total function.
The elementary operations of the procedure are Turing-computable. The procedure
has a second input which can contain the Goedel number of any Turing-computable
total function whose range is a subset of the set of the Goedel numbers of all
Turing-computable total functions. We prove that the second input cannot be set
to the Goedel number of any Turing-computable function that computes the output
from any natural number in its first input. In this sense, there is no Turing
program that computes the output from its first input. The procedure is used to
define creative procedures which compute functions that are not
Turing-computable. We argue that creative procedures model an aspect of
reasoning that cannot be modeled by Turing machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1156</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1156</id><created>2013-02-05</created><updated>2013-02-15</updated><authors><author><keyname>Salavati</keyname><forenames>Amir Hesam</forenames></author><author><keyname>Kumar</keyname><forenames>K. Raj</forenames></author><author><keyname>Shokrollahi</keyname><forenames>Amin</forenames></author></authors><title>A Non-Binary Associative Memory with Exponential Pattern Retrieval
  Capacity and Iterative Learning: Extended Results</title><categories>cs.NE</categories><comments>submitted to IEEE Transactions on Neural Networks and Learning
  Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of neural association for a network of non-binary
neurons. Here, the task is to first memorize a set of patterns using a network
of neurons whose states assume values from a finite number of integer levels.
Later, the same network should be able to recall previously memorized patterns
from their noisy versions. Prior work in this area consider storing a finite
number of purely random patterns, and have shown that the pattern retrieval
capacities (maximum number of patterns that can be memorized) scale only
linearly with the number of neurons in the network.
  In our formulation of the problem, we concentrate on exploiting redundancy
and internal structure of the patterns in order to improve the pattern
retrieval capacity. Our first result shows that if the given patterns have a
suitable linear-algebraic structure, i.e. comprise a sub-space of the set of
all possible patterns, then the pattern retrieval capacity is in fact
exponential in terms of the number of neurons. The second result extends the
previous finding to cases where the patterns have weak minor components, i.e.
the smallest eigenvalues of the correlation matrix tend toward zero. We will
use these minor components (or the basis vectors of the pattern null space) to
both increase the pattern retrieval capacity and error correction capabilities.
  An iterative algorithm is proposed for the learning phase, and two simple
neural update algorithms are presented for the recall phase. Using analytical
results and simulations, we show that the proposed methods can tolerate a fair
amount of errors in the input while being able to memorize an exponentially
large number of patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1157</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1157</id><created>2013-02-05</created><updated>2014-09-20</updated><authors><author><keyname>Towfic</keyname><forenames>Zaid J.</forenames></author><author><keyname>Chen</keyname><forenames>Jianshu</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Excess-Risk of Distributed Stochastic Learners</title><categories>math.OC cs.DC cs.MA cs.SI</categories><comments>62 pages, 4 figures, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the learning ability of consensus and diffusion distributed
learners from continuous streams of data arising from different but related
statistical distributions. Four distinctive features for diffusion learners are
revealed in relation to other decentralized schemes even under left-stochastic
combination policies. First, closed-form expressions for the evolution of their
excess-risk are derived for strongly-convex risk functions under a diminishing
step-size rule. Using these results, it is shown that the diffusion strategy
improves the asymptotic convergence rate of the excess-risk relative to
non-cooperative schemes. It is also shown that when the in-network cooperation
rules are designed optimally, the performance of the diffusion implementation
can outperform that of naive centralized processing. The arguments further show
that diffusion outperforms consensus strategies by reducing the overshoot
during the transient phase of the learning process and asymptotically as well.
The framework adopted in this work studies convergence in the stronger
mean-square-error sense, rather than in distribution, and develops tools that
enable a close examination of the differences between distributed strategies in
terms of asymptotic behavior, as well as in terms of convergence rates. This is
achieved by exploiting properties of Gamma functions and the convergence
properties of products of infinitely many scaling coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1161</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1161</id><created>2013-02-05</created><updated>2013-07-17</updated><authors><author><keyname>Rimoli</keyname><forenames>Julian J.</forenames></author><author><keyname>Rojas</keyname><forenames>Juan J.</forenames></author></authors><title>Meshing strategies for the alleviation of mesh-induced effects in
  cohesive element models</title><categories>physics.comp-ph cs.NA math.NA</categories><doi>10.1007/s10704-015-0013-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main approaches for modeling fracture and crack propagation in
solid materials is adaptive insertion of cohesive elements, in which line-like
(2D) or surface-like (3D) elements are inserted into the finite element mesh to
model the nucleation and propagation of failure surfaces. In this approach,
however, cracks are forced to propagate along element boundaries, following
paths that in general require more energy per unit crack extension (greater
driving forces) than those followed in the original continuum, which in turn
leads to erroneous solutions. In this work we illustrate how the introduction
of a discretization produces two undesired effects, which we term mesh-induced
anisotropy and mesh-induced toughness. Subsequently, we analyze those effects
through polar plots of the path deviation ratio (a measure of the ability of a
mesh to represent straight lines) for commonly adopted meshes. Finally, we
propose to reduce those effects through K-means meshes and through a new type
of mesh, which we term conjugate-directions mesh. The behavior of all meshes
under consideration as the mesh size is reduced is analyzed through a numerical
study of convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1170</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1170</id><created>2013-02-05</created><authors><author><keyname>Jeandel</keyname><forenames>Emmanuel</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Computability of the entropy of one-tape Turing Machines</title><categories>cs.FL cs.CC cs.IT math.DS math.IT</categories><comments>First version (01/08/2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the maximum speed and the entropy of a one-tape Turing machine
are computable, in the sense that we can approximate them to any given
precision $\epsilon$. This is contrary to popular belief, as all dynamical
properties are usually undecidable for Turing machines. The result is quite
specific to one-tape Turing machines, as it is not true anymore for two-tape
Turing machines by the results of Blondel et al., and uses the approach of
crossing sequences introduced by Hennie.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1178</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1178</id><created>2013-02-05</created><authors><author><keyname>Urbano</keyname><forenames>Juli&#xe1;n</forenames></author><author><keyname>Marrero</keyname><forenames>M&#xf3;nica</forenames></author><author><keyname>Mart&#xed;n</keyname><forenames>Diego</forenames></author><author><keyname>Morato</keyname><forenames>Jorge</forenames></author></authors><title>Overview of EIREX 2012: Social Media</title><categories>cs.IR</categories><comments>9 pages, 5 tables, 4 figures. arXiv admin note: substantial text
  overlap with arXiv:1203.0518, arXiv:1201.0274</comments><acm-class>K.3.2; H.3.4</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The third Information Retrieval Education through EXperimentation track
(EIREX 2012) was run at the University Carlos III of Madrid, during the 2012
spring semester. EIREX 2012 is the third in a series of experiments designed to
foster new Information Retrieval (IR) education methodologies and resources,
with the specific goal of teaching undergraduate IR courses from an
experimental perspective. For an introduction to the motivation behind the
EIREX experiments, see the first sections of [Urbano et al., 2011a]. For
information on other editions of EIREX and related data, see the website at
http://ir.kr.inf.uc3m.es/eirex/. The EIREX series have the following goals: a)
to help students get a view of the Information Retrieval process as they would
find it in a real-world scenario, either industrial or academic; b) to make
students realize the importance of laboratory experiments in Computer Science
and have them initiated in their execution and analysis; c) to create a public
repository of resources to teach Information Retrieval courses; d) to seek the
collaboration and active participation of other Universities in this endeavor.
This overview paper summarizes the results of the EIREX 2012 track, focusing on
the creation of the test collection and the analysis to assess its reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1185</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1185</id><created>2013-02-05</created><updated>2013-02-07</updated><authors><author><keyname>Narani</keyname><forenames>Sandeep. R.</forenames></author></authors><title>Social Secret Sharing for Resource Management in Cloud</title><categories>cs.CR</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first explain the notion of secret sharing and also threshold schemes,
which can be implemented with the Shamir's secret sharing. Subsequently, we
review social secret sharing (NSG'10,NS'10) and its trust function. In a secret
sharing scheme, a secret is shared among a group of players who can later
recover the secret. We review the construction of a social secret sharing
scheme and its application for resource management in cloud, as explained in
NS'12. To clarify the social secret sharing scheme, we first review its trust
function according to NL'06. In this scheme, a secret is maintained by
assigning a trust value to each player based on his behavior, i.e.,
availability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1192</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1192</id><created>2013-02-05</created><updated>2013-11-05</updated><authors><author><keyname>Clear</keyname><forenames>Michael</forenames></author><author><keyname>Hughes</keyname><forenames>Arthur</forenames></author><author><keyname>Tewari</keyname><forenames>Hitesh</forenames></author></authors><title>Homomorphic Encryption with Access Policies: Characterization and New
  Constructions</title><categories>cs.CR</categories><comments>Full version of a paper that was published in Africacrypt 2013</comments><journal-ref>Lecture Notes in Computer Science Volume 7918, 2013, pp 61-87</journal-ref><doi>10.1007/978-3-642-38553-7_4</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A characterization of predicate encryption (PE) with support for homomorphic
operations is presented and we describe the homomorphic properties of some
existing PE constructions. Even for the special case of IBE, there are few
known group-homomorphic cryptosystems. Our main construction is an
XOR-homomorphic IBE scheme based on the quadratic residuosity problem (variant
of the Cocks' scheme), which we show to be strongly homomorphic. We were unable
to construct an anonymous variant that preserves this homomorphic property, but
we achieved anonymity for a weaker notion of homomorphic encryption, which we
call \emph{non-universal}. A related security notion for this weaker primitive
is formalized. Finally, some potential applications and open problems are
considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1207</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1207</id><created>2013-02-05</created><authors><author><keyname>Pelayo</keyname><forenames>&#xc1;lvaro</forenames></author><author><keyname>Voevodsky</keyname><forenames>Vladimir</forenames></author><author><keyname>Warren</keyname><forenames>Michael A.</forenames></author></authors><title>A preliminary univalent formalization of the p-adic numbers</title><categories>math.LO cs.LO</categories><comments>57 pages</comments><msc-class>03F65, 68T15, 03B15</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give a preliminary formalization of the p-adic numbers, in
the context of the second author's univalent foundations program. We also
provide the corresponding code verifying the construction in the proof
assistant Coq. Because work in the univalent setting is ongoing, the structure
and organization of the construction of the p-adic numbers we give in this
paper is expected to change as Coq libraries are more suitably rearranged, and
optimized, by the authors and other researchers in the future. So our
construction here should be deemed as a first approximation which is subject to
improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1211</identifier>
 <datestamp>2013-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1211</id><created>2013-02-05</created><updated>2013-05-17</updated><authors><author><keyname>Cong</keyname><forenames>Shuang</forenames></author><author><keyname>Meng</keyname><forenames>Fangfang</forenames></author><author><keyname>Kuang</keyname><forenames>Sen</forenames></author></authors><title>Quantum Lyapunov Control Based on the Average Value of an Imaginary
  Mechanical Quantity</title><categories>cs.SY math-ph math.MP</categories><comments>14 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The convergence of closed quantum systems in the degenerate cases to the
desired target state by using the quantum Lyapunov control based on the average
value of an imaginary mechanical quantity is studied. On the basis of the
existing methods which can only ensure the single-control Hamiltonian systems
converge toward a set, we design the control laws to make the multi-control
Hamiltonian systems converge to the desired target state. The convergence of
the control system is proved, and the convergence to the desired target state
is analyzed. How to make these conditions of convergence to the target state to
be satisfied is proved or analyzed. Finally, numerical simulations for a three
level system in the degenrate case transfering form an initial eigenstate to a
target superposition state are studied to verify the effectiveness of the
proposed control method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1216</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1216</id><created>2013-02-05</created><authors><author><keyname>Huang</keyname><forenames>Jing</forenames></author><author><keyname>Mukherjee</keyname><forenames>Amitav</forenames></author><author><keyname>Swindlehurst</keyname><forenames>A. Lee</forenames></author></authors><title>Secure Communication Via an Untrusted Non-Regenerative Relay in Fading
  Channels</title><categories>cs.IT math.IT</categories><comments>32 pages, 8 figures, to appear in IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2013.2247600</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a relay network where the source can potentially utilize an
untrusted non-regenerative relay to augment its direct transmission of a
confidential message to the destination. Since the relay is untrusted, it is
desirable to protect the confidential data from it while simultaneously making
use of it to increase the reliability of the transmission. We first examine the
secrecy outage probability (SOP) of the network assuming a single antenna
relay, and calculate the exact SOP for three different schemes: direct
transmission without using the relay, conventional non-regenerative relaying,
and cooperative jamming by the destination. Subsequently, we conduct an
asymptotic analysis of the SOPs to determine the optimal policies in different
operating regimes. We then generalize to the multi-antenna relay case and
investigate the impact of the number of relay antennas on the secrecy
performance. Finally, we study a scenario where the relay has only a single RF
chain which necessitates an antenna selection scheme, and we show that unlike
the case where all antennas are used, under certain conditions the cooperative
jamming scheme with antenna selection provides a diversity advantage for the
receiver. Numerical results are presented to verify the theoretical predictions
of the preferred transmission policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1219</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1219</id><created>2013-02-05</created><updated>2015-01-06</updated><authors><author><keyname>Bolotin</keyname><forenames>D. A.</forenames></author><author><keyname>Poslavsky</keyname><forenames>S. V.</forenames></author></authors><title>Introduction to Redberry: a computer algebra system designed for tensor
  manipulation</title><categories>cs.SC hep-ph hep-th</categories><comments>27 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce Redberry --- an open source computer algebra
system with native support of tensorial expressions. It provides basic computer
algebra tools (algebraic manipulations, substitutions, basic simplifications
etc.) which are aware of specific features of indexed expressions: contractions
of indices, permutational symmetries, multiple index types etc. Redberry
supports conventional \LaTeX-style input notation for tensorial expressions.
The high energy physics package includes tools for Feynman diagrams
calculation: Dirac and SU(N) algebra, Levi-Civita simplifications and tools for
one-loop calculations in quantum field theory. In the paper we give detailed
overview of Redberry features: from basic manipulations with tensors to real
Feynman diagrams calculation, accompanied by many examples. Redberry is written
in Java 7 and provides convenient Groovy-based user interface inside the
high-level general purpose programming language environment. Redberry is
available from http://redberry.cc
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1224</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1224</id><created>2013-02-05</created><authors><author><keyname>Baker</keyname><forenames>Thomas</forenames></author><author><keyname>Bechhofer</keyname><forenames>Sean</forenames></author><author><keyname>Isaac</keyname><forenames>Antoine</forenames></author><author><keyname>Miles</keyname><forenames>Alistair</forenames></author><author><keyname>Schreiber</keyname><forenames>Guus</forenames></author><author><keyname>Summers</keyname><forenames>Ed</forenames></author></authors><title>Key Choices in the Design of Simple Knowledge Organization System (SKOS)</title><categories>cs.DL</categories><comments>Submitted to Journal of Web Semantics, 2012-02-05</comments><journal-ref>Web Semantics: Science, Services and Agents on the World Wide Web,
  20, May 2013</journal-ref><doi>10.1016/j.websem.2013.05.001</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Simple Knowledge Organization System (SKOS) provides a data model and
vocabulary for expressing Knowledge Organization Systems (KOSs) such as
thesauri and classi?cation schemes in Semantic Web applications. This paper
presents the main components of SKOS and their formal expression in Web
Ontology Language (OWL), providing an extensive account of the design decisions
taken by the Semantic Web Deployment (SWD) Working Group of the World Wide Web
Consortium (W3C), which between 2006 and 2009 brought SKOS to the status of W3C
Recommendation. The paper explains key design principles such as &quot;minimal
ontological commitment&quot; and systematically cites the requirements and issues
that influenced the design of SKOS components.
  By reconstructing the discussion around alternative features and design
options and presenting the rationale for design decisions, the paper aims at
providing insight into how SKOS turned out as it did, and why. Assuming that
SKOS, like any other successful technology, may eventually be subject to
revision and improvement, the critical account o?ered here may help future
editors approach such a task with deeper understanding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1232</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1232</id><created>2013-02-05</created><authors><author><keyname>Nadakuditi</keyname><forenames>Raj Rao</forenames></author></authors><title>When are the most informative components for inference also the
  principal components?</title><categories>math.ST cs.DS cs.IT cs.LG math.IT math.PR stat.TH</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Which components of the singular value decomposition of a signal-plus-noise
data matrix are most informative for the inferential task of detecting or
estimating an embedded low-rank signal matrix? Principal component analysis
ascribes greater importance to the components that capture the greatest
variation, i.e., the singular vectors associated with the largest singular
values. This choice is often justified by invoking the Eckart-Young theorem
even though that work addresses the problem of how to best represent a
signal-plus-noise matrix using a low-rank approximation and not how to
best_infer_ the underlying low-rank signal component.
  Here we take a first-principles approach in which we start with a
signal-plus-noise data matrix and show how the spectrum of the noise-only
component governs whether the principal or the middle components of the
singular value decomposition of the data matrix will be the informative
components for inference. Simply put, if the noise spectrum is supported on a
connected interval, in a sense we make precise, then the use of the principal
components is justified. When the noise spectrum is supported on multiple
intervals, then the middle components might be more informative than the
principal components.
  The end result is a proper justification of the use of principal components
in the setting where the noise matrix is i.i.d. Gaussian and the identification
of scenarios, generically involving heterogeneous noise models such as mixtures
of Gaussians, where the middle components might be more informative than the
principal components so that they may be exploited to extract additional
processing gain. Our results show how the blind use of principal components can
lead to suboptimal or even faulty inference because of phase transitions that
separate a regime where the principal components are informative from a regime
where they are uninformative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1235</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1235</id><created>2013-02-05</created><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author><author><keyname>Iraids</keyname><forenames>J&#x101;nis</forenames></author><author><keyname>Smotrovs</keyname><forenames>Juris</forenames></author></authors><title>Exact quantum query complexity of EXACT and THRESHOLD</title><categories>quant-ph cs.CC</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A quantum algorithm is exact if it always produces the correct answer, on any
input. Coming up with exact quantum algorithms that substantially outperform
the best classical algorithm has been a quite challenging task. In this paper,
we present two new exact quantum algorithms for natural problems: 1) for the
problem EXACT_k^n in which we have to determine whether the sequence of input
bits x_1, ..., x_n contains exactly k values x_i=1; 2) for the problem
THRESHOLD_k^n in which we have to determine if at least k of n input bits are
equal to 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1236</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1236</id><created>2013-02-05</created><authors><author><keyname>Cai</keyname><forenames>T. Tony</forenames></author><author><keyname>Zhang</keyname><forenames>Anru</forenames></author></authors><title>Sharp RIP Bound for Sparse Signal and Low-Rank Matrix Recovery</title><categories>cs.IT math.IT</categories><comments>to appear in Applied and Computational Harmonic Analysis (2012)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper establishes a sharp condition on the restricted isometry property
(RIP) for both the sparse signal recovery and low-rank matrix recovery. It is
shown that if the measurement matrix $A$ satisfies the RIP condition
$\delta_k^A&lt;1/3$, then all $k$-sparse signals $\beta$ can be recovered exactly
via the constrained $\ell_1$ minimization based on $y=A\beta$. Similarly, if
the linear map $\cal M$ satisfies the RIP condition $\delta_r^{\cal M}&lt;1/3$,
then all matrices $X$ of rank at most $r$ can be recovered exactly via the
constrained nuclear norm minimization based on $b={\cal M}(X)$. Furthermore, in
both cases it is not possible to do so in general when the condition does not
hold. In addition, noisy cases are considered and oracle inequalities are given
under the sharp RIP condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1242</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1242</id><created>2013-02-05</created><authors><author><keyname>Vidick</keyname><forenames>Thomas</forenames></author></authors><title>Three-player entangled XOR games are NP-hard to approximate</title><categories>quant-ph cs.CC</categories><comments>52 pages; comments welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for any eps&gt;0 the problem of finding a factor (2-eps)
approximation to the entangled value of a three-player XOR game is NP-hard.
Equivalently, the problem of approximating the largest possible quantum
violation of a tripartite Bell correlation inequality to within any
multiplicative constant is NP-hard. These results are the first constant-factor
hardness of approximation results for entangled games or quantum violations of
Bell inequalities shown under the sole assumption that P \neq NP. They can be
thought of as an extension of Hastad's optimal hardness of approximation
results for MAX-E3-LIN2 (JACM'01) to the entangled-player setting.
  The key technical component of our work is a soundness analysis of a
point-vs-plane low-degree test against entangled players. This extends and
simplifies the analysis of the multilinearity test by Ito and Vidick (FOCS'12).
Our results demonstrate the possibility for efficient reductions between
entangled-player games and our techniques may lead to further hardness of
approximation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1256</identifier>
 <datestamp>2013-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1256</id><created>2013-02-05</created><updated>2013-05-14</updated><authors><author><keyname>Chen</keyname><forenames>Junyu</forenames></author><author><keyname>Shum</keyname><forenames>Kenneth W.</forenames></author></authors><title>Repairing Multiple Failures in the Suh-Ramchandran Regenerating Codes</title><categories>cs.IT math.IT</categories><comments>IEEE Int. Symp. Inf. Theory, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the idea of interference alignment, Suh and Ramchandran constructed a
class of minimum-storage regenerating codes which can repair one systematic or
one parity-check node with optimal repair bandwidth. With the same code
structure, we show that in addition to single node failure, double node
failures can be repaired collaboratively with optimal repair bandwidth as well.
We give an example of how to repair double failures in the Suh-Ramchandran
regenerating code with six nodes, and give the proof for the general case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1258</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1258</id><created>2013-02-05</created><authors><author><keyname>Wang</keyname><forenames>Lele</forenames></author><author><keyname>Sasoglu</keyname><forenames>Eren</forenames></author><author><keyname>Bandemer</keyname><forenames>Bernd</forenames></author><author><keyname>Kim</keyname><forenames>Young-Han</forenames></author></authors><title>A Comparison of Superposition Coding Schemes</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, 1 table, submitted to IEEE International
  Symposium on Information Theory (ISIT 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are two variants of superposition coding schemes. Cover's original
superposition coding scheme has code clouds of the identical shape, while
Bergmans's superposition coding scheme has code clouds of independently
generated shapes. These two schemes yield identical achievable rate regions in
several scenarios, such as the capacity region for degraded broadcast channels.
This paper shows that under the optimal maximum likelihood decoding, these two
superposition coding schemes can result in different rate regions. In
particular, it is shown that for the two-receiver broadcast channel, Cover's
superposition coding scheme can achieve rates strictly larger than Bergmans's
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1266</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1266</id><created>2013-02-06</created><authors><author><keyname>Lef&#xe8;vre</keyname><forenames>Julien</forenames></author></authors><title>Fiedler Vectors and Elongation of Graphs: A Threshold Phenomenon on a
  Particular Class of Trees</title><categories>cs.DM math.CO</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a graph. Its laplacian matrix $L(G)$ is positive and we consider
eigenvectors of its first non-null eigenvalue that are called Fiedler vector.
They have been intensively used in spectral partitioning problems due to their
good empirical properties. More recently Fiedler vectors have been also
popularized in the computer graphics community to describe elongation of
shapes. In more technical terms, authors have conjectured that extrema of
Fiedler vectors can yield the diameter of a graph. In this work we present
(FED) property for a graph $G$, i.e. the fact that diameter of a graph can be
obtain by Fiedler vectors. We study in detail a parametric family of trees that
gives indeed a counter example for the previous conjecture but reveals a
threshold phenomenon for (FED) property. We end by an exhaustive enumeration of
trees with at most 20 vertices for which (FED) is true and some perspectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1270</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1270</id><created>2013-02-06</created><authors><author><keyname>Mukherjee</keyname><forenames>Amitav</forenames></author></authors><title>Diffusion of Cooperative Behavior in Decentralized Cognitive Radio
  Networks with Selfish Spectrum Sensors</title><categories>cs.IT cs.GT math.IT</categories><doi>10.1109/JSTSP.2013.2246136</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates the diffusion of cooperative behavior over time in a
decentralized cognitive radio network with selfish spectrum-sensing users. The
users can individually choose whether or not to participate in cooperative
spectrum sensing, in order to maximize their individual payoff defined in terms
of the sensing false-alarm rate and transmit energy expenditure. The system is
modeled as a partially connected network with a statistical distribution of the
degree of the users, who play their myopic best responses to the actions of
their neighbors at each iteration. Based on this model, we investigate the
existence and characterization of Bayesian Nash Equilibria for the diffusion
game. The impacts of network topology, channel fading statistics, sensing
protocol, and multiple antennas on the outcome of the diffusion process are
analyzed next. Simulation results that demonstrate how conducive different
network scenarios are to the diffusion of cooperation are presented for further
insight, and we conclude with a discussion on additional refinements and issues
worth pursuing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1294</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1294</id><created>2013-02-06</created><authors><author><keyname>Jassim</keyname><forenames>Firas Ajil</forenames></author><author><keyname>Altaany</keyname><forenames>Fawzi Hasan</forenames></author></authors><title>Image Interpolation Using Kriging Technique for Spatial Data</title><categories>cs.CV</categories><comments>6 pages, 8 figures, 3 tables</comments><journal-ref>Canadian Journal on Image Processing and Computer Vision, Vol. 4
  No. 2, February 2013</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Image interpolation has been used spaciously by customary interpolation
techniques. Recently, Kriging technique has been widely implemented in
simulation area and geostatistics for prediction. In this article, Kriging
technique was used instead of the classical interpolation methods to predict
the unknown points in the digital image array. The efficiency of the proposed
technique was proven using the PSNR and compared with the traditional
interpolation techniques. The results showed that Kriging technique is almost
accurate as cubic interpolation and in some images Kriging has higher accuracy.
A miscellaneous test images have been used to consolidate the proposed
technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1296</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1296</id><created>2013-02-06</created><authors><author><keyname>Jassim</keyname><forenames>Firas Ajil</forenames></author></authors><title>Hybrid Image Segmentation using Discerner Cluster in FCM and Histogram
  Thresholding</title><categories>cs.CV</categories><comments>4 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:1005.4020 by other authors</comments><journal-ref>International Journal of Graphics &amp; Image Processing, Vol 2, issue
  4, November 2012</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Image thresholding has played an important role in image segmentation. This
paper presents a hybrid approach for image segmentation based on the
thresholding by fuzzy c-means (THFCM) algorithm for image segmentation. The
goal of the proposed approach is to find a discerner cluster able to find an
automatic threshold. The algorithm is formulated by applying the standard FCM
clustering algorithm to the frequencies (y-values) on the smoothed histogram.
Hence, the frequencies of an image can be used instead of the conventional
whole data of image. The cluster that has the highest peak which represents the
maximum frequency in the image histogram will play as an excellent role in
determining a discerner cluster to the grey level image. Then, the pixels
belong to the discerner cluster represent an object in the gray level histogram
while the other clusters represent a background. Experimental results with
standard test images have been obtained through the proposed approach (THFCM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1300</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1300</id><created>2013-02-06</created><authors><author><keyname>Jassim</keyname><forenames>Firas Ajil</forenames></author></authors><title>Kriging Interpolation Filter to Reduce High Density Salt and Pepper
  Noise</title><categories>cs.CV</categories><comments>6 pages, 10 figures, 2 tables</comments><journal-ref>World of Computer Science and Information Technology Journal
  (WCSIT), Vol. 3, No. 1, pp.8-14, 2013</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Image denoising is a critical issue in the field of digital image processing.
This paper proposes a novel Salt &amp; Pepper noise suppression by developing a
Kriging Interpolation Filter (KIF) for image denoising. Gray-level images
degraded with Salt &amp; Pepper noise have been considered. A sequential search for
noise detection was made using kXk window size to determine non-noisy pixels
only. The non-noisy pixels are passed into Kriging interpolation method to
predict their absent neighbor pixels that were noisy pixels at the first phase.
The utilization of Kriging interpolation filter proves that it is very
impressive to suppress high noise density. It has been found that Kriging
Interpolation filter achieves noise reduction without loss of edges and
detailed information. Comparisons with existing algorithms are done using
quality metrics like PSNR and MSE to assess the proposed filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1302</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1302</id><created>2013-02-06</created><updated>2013-05-16</updated><authors><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Koch</keyname><forenames>Tobias</forenames></author><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author></authors><title>Quasi-Static SIMO Fading Channels at Finite Blocklength</title><categories>cs.IT math.IT</categories><comments>extended version of a paper submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the maximal achievable rate for a given blocklength and error
probability over quasi-static single-input multiple-output (SIMO) fading
channels. Under mild conditions on the channel gains, it is shown that the
channel dispersion is zero regardless of whether the fading realizations are
available at the transmitter and/or the receiver. The result follows from
computationally and analytically tractable converse and achievability bounds.
Through numerical evaluation, we verify that, in some scenarios, zero
dispersion indeed entails fast convergence to outage capacity as the
blocklength increases. In the example of a particular 1*2 SIMO Rician channel,
the blocklength required to achieve 90% of capacity is about an order of
magnitude smaller compared to the blocklength required for an AWGN channel with
the same capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1306</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1306</id><created>2013-02-06</created><authors><author><keyname>Sun</keyname><forenames>Youcheng</forenames></author><author><keyname>Soulat</keyname><forenames>Romain</forenames></author><author><keyname>Lipari</keyname><forenames>Giuseppe</forenames></author><author><keyname>Andr&#xe9;</keyname><forenames>&#xc9;tienne</forenames></author><author><keyname>Fribourg</keyname><forenames>Laurent</forenames></author></authors><title>Parametric Schedulability Analysis of Fixed Priority Real-Time
  Distributed Systems</title><categories>cs.DC cs.OS</categories><comments>Submitted to ECRTS 2013 (http://ecrts.eit.uni-kl.de/ecrts13)</comments><report-no>LSV-13-03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parametric analysis is a powerful tool for designing modern embedded systems,
because it permits to explore the space of design parameters, and to check the
robustness of the system with respect to variations of some uncontrollable
variable. In this paper, we address the problem of parametric schedulability
analysis of distributed real-time systems scheduled by fixed priority. In
particular, we propose two different approaches to parametric analysis: the
first one is a novel technique based on classical schedulability analysis,
whereas the second approach is based on model checking of Parametric Timed
Automata (PTA).
  The proposed analytic method extends existing sensitivity analysis for single
processors to the case of a distributed system, supporting preemptive and
non-preemptive scheduling, jitters and unconstrained deadlines. Parametric
Timed Automata are used to model all possible behaviours of a distributed
system, and therefore it is a necessary and sufficient analysis. Both
techniques have been implemented in two software tools, and they have been
compared with classical holistic analysis on two meaningful test cases. The
results show that the analytic method provides results similar to classical
holistic analysis in a very efficient way, whereas the PTA approach is slower
but covers the entire space of solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1314</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1314</id><created>2013-02-06</created><updated>2013-10-20</updated><authors><author><keyname>Okayama</keyname><forenames>Tomoaki</forenames></author></authors><title>Error Estimates with Explicit Constants for Sinc Quadrature and Sinc
  Indefinite Integration over Infinite Intervals</title><categories>math.NA cs.NA</categories><comments>Keywords: Sinc numerical methods, verified numerical integration,
  trapezoidal formula, DE formula</comments><msc-class>65D30, 65D32, 65G99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Sinc quadrature and the Sinc indefinite integration are approximation
formulas for definite integration and indefinite integration, respectively,
which can be applied on any interval by using an appropriate variable
transformation. Their convergence rates have been analyzed for typical cases
including finite, semi-infinite, and infinite intervals. In addition, for
verified automatic integration, more explicit error bounds that are computable
have been recently given on a finite interval. In this paper, such explicit
error bounds are given in the remaining cases on semi-infinite and infinite
intervals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1326</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1326</id><created>2013-02-06</created><authors><author><keyname>Zhou</keyname><forenames>Yu</forenames></author></authors><title>Cloud Computing framework for Computer Vision Research:An Introduction</title><categories>cs.CV cs.DC</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing offers the potential to help scientists to process massive
number of computing resources often required in machine learning application
such as computer vision problems. This proposal would like to show that which
benefits can be obtained from cloud in order to help medical image analysis
users (including scientists, clinicians, and research institutes). As security
and privacy of algorithms are important for most of algorithms inventors, these
algorithms can be hidden in a cloud to allow the users to use the algorithms as
a package without any access to see/change their inside. In another word, in
the user part, users send their images to the cloud and configure the algorithm
via an interface. In the cloud part, the algorithms are applied to this image
and the results are returned back to the user. My proposal has two parts: (1)
investigate the potential of cloud computing for computer vision problems and
(2) study the components of a proposed cloud-based framework for medical image
analysis application and develop them (depending on the length of the
internship). The investigation part will involve a study on several aspects of
the problem including security, usability (for medical end users of the
service), appropriate programming abstractions for vision problems, scalability
and resource requirements. In the second part of this proposal I am going to
thoroughly study of the proposed framework components and their relations and
develop them. The proposed cloud-based framework includes an integrated
environment to enable scientists and clinicians to access to the previous and
current medical image analysis algorithms using a handful user interface
without any access to the algorithm codes and procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1334</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1334</id><created>2013-02-06</created><authors><author><keyname>Parzhin</keyname><forenames>Yuri</forenames></author></authors><title>Principles of modal and vector theory of formal intelligence systems</title><categories>cs.AI</categories><comments>34 pages, 8 figures</comments><acm-class>F.4.1; I.2.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The paper considers the class of information systems capable of solving
heuristic problems on basis of formal theory that was termed modal and vector
theory of formal intelligent systems (FIS). The paper justifies the
construction of FIS resolution algorithm, defines the main features of these
systems and proves theorems that underlie the theory. The principle of
representation diversity of FIS construction is formulated. The paper deals
with the main principles of constructing and functioning formal intelligent
system (FIS) on basis of FIS modal and vector theory. The following phenomena
are considered: modular architecture of FIS presentation sub-system, algorithms
of data processing at every step of the stage of creating presentations.
Besides the paper suggests the structure of neural elements, i.e. zone
detectors and processors that are the basis for FIS construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1335</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1335</id><created>2013-02-06</created><authors><author><keyname>Anantharangachar</keyname><forenames>Raghu</forenames></author><author><keyname>Ramani</keyname><forenames>Srinivasan</forenames></author><author><keyname>Rajagopalan</keyname><forenames>S</forenames></author></authors><title>Ontology Guided Information Extraction from Unstructured Text</title><categories>cs.IR</categories><acm-class>I.2.7</acm-class><journal-ref>International Journal of Web &amp; Semantic Technology (IJWesT) Vol.4,
  No.1, January 2013</journal-ref><doi>10.5121/ijwest.2013.4102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe an approach to populate an existing ontology with
instance information present in the natural language text provided as input. An
ontology is defined as an explicit conceptualization of a shared domain. This
approach starts with a list of relevant domain ontologies created by human
experts, and techniques for identifying the most appropriate ontology to be
extended with information from a given text. Then we demonstrate heuristics to
extract information from the unstructured text and for adding it as structured
information to the selected ontology. This identification of the relevant
ontology is critical, as it is used in identifying relevant information in the
text. We extract information in the form of semantic triples from the text,
guided by the concepts in the ontology. We then convert the extracted
information about the semantic class instances into Resource Description
Framework (RDF3) and append it to the existing domain ontology. This enables us
to perform more precise semantic queries over the semantic triple store thus
created. We have achieved 95% accuracy of information extraction in our
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1338</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1338</id><created>2013-02-06</created><authors><author><keyname>Meghanathan</keyname><forenames>Natarajan</forenames></author></authors><title>Source Code Analysis to Remove Security Vulnerabilities in Java Socket
  Programs: A Case Study</title><categories>cs.CR</categories><comments>16 pages, 16 figures</comments><acm-class>C.2.0</acm-class><journal-ref>International Journal of Network Security and its Applications
  (IJNSA), vol. 5, no. 1, pp. 1-16, 2013</journal-ref><doi>10.5121/ijnsa.2013.5101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the source code analysis of a file reader server socket
program (connection-oriented sockets) developed in Java, to illustrate the
identification, impact analysis and solutions to remove five important software
security vulnerabilities, which if left unattended could severely impact the
server running the software and also the network hosting the server. The five
vulnerabilities we study in this paper are: (1) Resource Injection, (2) Path
Manipulation, (3) System Information Leak, (4) Denial of Service and (5)
Unreleased Resource vulnerabilities. We analyze the reason why each of these
vulnerabilities occur in the file reader server socket program, discuss the
impact of leaving them unattended in the program, and propose solutions to
remove each of these vulnerabilities from the program. We also analyze any
potential performance tradeoffs (such as increase in code size and loss of
features) that could arise while incorporating the proposed solutions on the
server program. The proposed solutions are very generic in nature, and can be
suitably modified to correct any such vulnerabilities in software developed in
any other programming language. We use the Fortify Source Code Analyzer to
conduct the source code analysis of the file reader server program, implemented
on a Windows XP virtual machine with the standard J2SE v.7 development kit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1349</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1349</id><created>2013-02-06</created><authors><author><keyname>Feghhi</keyname><forenames>Mahmood Mohassel</forenames></author><author><keyname>Abbasfar</keyname><forenames>Aliazam</forenames></author><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author></authors><title>Optimal Power and Rate Allocation in the Degraded Gaussian Relay Channel
  with Energy Harvesting Nodes</title><categories>cs.IT cs.ET math.IT</categories><comments>6 pages, 2 figures, submitted to IWCIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy Harvesting (EH) is a novel technique to prolong the lifetime of the
wireless networks such as wireless sensor networks or Ad-Hoc networks, by
providing an unlimited source of energy for their nodes. In this sense, it has
emerged as a promising technique for Green Communications, recently. On the
other hand, cooperative communication with the help of relay nodes improves the
performance of wireless communication networks by increasing the system
throughput or the reliability as well as the range and efficient energy
utilization. In order to investigate the cooperation in EH nodes, in this
paper, we consider the problem of optimal power and rate allocation in the
degraded full-duplex Gaussian relay channel in which source and relay can
harvest energy from their environments. We consider the general stochastic
energy arrivals at the source and the relay with known EH times and amounts at
the transmitters before the start of transmission. This problem has a min-max
optimization form that along with the constraints is not easy to solve. We
propose a method based on a mathematical theorem proposed by Terkelsen [1] to
transform it to a solvable convex optimization form. Also, we consider some
special cases for the harvesting profile of the source and the relay nodes and
find their solutions efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1351</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1351</id><created>2013-02-06</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Peng</keyname><forenames>Wei</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>Adaptive Sparse Channel Estimation for Time-Variant MIMO-OFDM Systems</title><categories>cs.IT math.IT</categories><comments>6 cages,10 figures, conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate channel state information (CSI) is required for coherent detection
in time-variant multiple-input multipleoutput (MIMO) communication systems
using orthogonal frequency division multiplexing (OFDM) modulation. One of
low-complexity and stable adaptive channel estimation (ACE) approaches is the
normalized least mean square (NLMS)-based ACE. However, it cannot exploit the
inherent sparsity of MIMO channel which is characterized by a few dominant
channel taps. In this paper, we propose two adaptive sparse channel estimation
(ASCE) methods to take advantage of such sparse structure information for
time-variant MIMO-OFDM systems. Unlike traditional NLMS-based method, two
proposed methods are implemented by introducing sparse penalties to the cost
function of NLMS algorithm. Computer simulations confirm obvious performance
advantages of the proposed ASCEs over the traditional ACE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1353</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1353</id><created>2013-02-06</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Peng</keyname><forenames>Wei</forenames></author><author><keyname>Mehbodniya</keyname><forenames>Abolfazl</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>Adaptive Sparse Channel Estimation for Time-Variant MISO Communication
  Systems</title><categories>cs.IT math.IT</categories><comments>5 pages, 7 figures, 1 table, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Channel estimation problem is one of the key technical issues in time-variant
multiple-input single-output (MSIO) communication systems. To estimate the MISO
channel, least mean square (LMS) algorithm is applied to adaptive channel
estimation (ACE). Since the MISO channel is often described by sparse channel
model, such sparsity can be exploited and then estimation performance can be
improved by adaptive sparse channel estimation (ASCE) methods using sparse LMS
algorithms. However, conventional ASCE methods have two main drawbacks: 1)
sensitive to random scale of training signal and 2) unstable in low
signal-to-noise ratio (SNR) regime. To overcome these two harmful factors, in
this paper, we propose a novel ASCE method using normalized LMS (NLMS)
algorithm (ASCE-NLMS). In addition, we also proposed an improved ASCE method
using normalized least mean fourth (NLMF) algorithm (ASCE-NLMF). Two proposed
methods can exploit the channel sparsity effectively. Also, stability of the
proposed methods is confirmed by mathematical derivation. Computer simulation
results show that the proposed sparse channel estimation methods can achieve
better estimation performance than conventional methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1355</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1355</id><created>2013-02-06</created><authors><author><keyname>Abdeen</keyname><forenames>Hani</forenames></author><author><keyname>Shata</keyname><forenames>Osama</forenames></author></authors><title>Characterizing and Evaluating The Impact of Software Interface Clones</title><categories>cs.SE</categories><comments>11 pages</comments><journal-ref>H. Abdeen, O. Shata, &quot;Characterizing and Evaluating The Impact of
  Software Interface Clones&quot;, International Journal of Software Engineering &amp;
  Applications (IJSEA) 4 (1), Jan. 2013</journal-ref><doi>10.5121/ijsea.2013.4106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software Interfaces are meant to describe contracts governing interactions
between logic modules. Interfaces, if well designed, significantly reduce
software complexity and ease maintainability . However, as software evolves,
the organization and the quality of software interfaces gradually deteriorate.
As a consequence, this often leads to increased development cost, lower code
quality and reduced reusability . Code clones are one of the most known bad
smells in source code. This design defect may occur in interfaces by
duplicating method/API declarations in several interfaces. Such interfaces are
similar from the point of view of public services/APIs they specify, thus they
indicate a bad organization of application services. In this paper, we
characterize the interface clone design defect and illustrate it via examples
taken from real-world open source software applications. We conduct an
empirical study covering nine real-world open source software applications to
quantify the presence of interface clones and evaluate their impact on
interface design quality . The results of the empirical study show that
interface clones are widely present in software interfaces. They also show that
the presence of interface clones may cause a degradation of interface cohesion
and indicate a considerable presence of code clones at implementations level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1358</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1358</id><created>2013-02-06</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Peng</keyname><forenames>Wei</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>Sparse Channel Estimation for MIMO-OFDM Amplify-and-Forward Two-Way
  Relay Networks</title><categories>cs.IT math.IT</categories><comments>6 pages, 7 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate channel impulse response (CIR) is required for coherent detection
and it can also help improve communication quality of service in
next-generation wireless communication systems. One of the advanced systems is
multi-input multi-output orthogonal frequency-division multiplexing (MIMO-OFDM)
amplify and forward two-way relay networks (AF-TWRN). Linear channel estimation
methods, e.g., least square (LS), have been proposed to estimate the CIR.
However, these methods never take advantage of channel sparsity and then cause
performance loss. In this paper, we propose a sparse channel estimation method
to exploit the sparse structure information in the CIR at each end user. Sparse
channel estimation problem is formulated as compressed sensing (CS) using
sparse decomposition theory and the estimation process is implemented by LASSO
algorithm. Computer simulation results are given to confirm the superiority of
proposed method over the LS-based channel estimation method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1369</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1369</id><created>2013-02-06</created><updated>2013-02-11</updated><authors><author><keyname>Br&#xf3;dka</keyname><forenames>Piotr</forenames></author></authors><title>Key User Extraction Based on Telecommunication Data (aka. Key Users in
  Social Network. How to find them?)</title><categories>cs.SI physics.soc-ph</categories><comments>This is my master thesis from 2008, also published in modified
  version as a book 'Key Users in Social Network. How to find them?' LAP
  Lambert Academic Publishing, 2012, ISBN-13: 978-3-659-19597-6, ISBN-10:
  3659195979</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of systems that collect vast amount of data about users rapidly
grow during last few years. Many of these systems contain data not only about
people characteristics but also about their relationships with other system
users. From this kind of data it is possible to extract a social network that
reflects the connections between system's users. Moreover, the analysis of such
social network enables to investigate different characteristics of its members
and their linkages. One of the types of examining such network is key users
extraction. Key users are these who have the biggest impact on other network
members as well as have big influence on network evolution. The obtained about
these users knowledge enables to investigate and predict changes within the
network. So this knowledge is very important for the people or companies who
make a profit from the network like telecommunication company. The second
important thing is the ability to extract these users as quick as possible,
i.e. developed the algorithm that will be time-effective in large social
networks where number of nodes and edges equal few millions. In this master
thesis the method of key user extraction, which is called social position, was
analyzed. Moreover, social position measure was compared with other methods,
which are used to assess the centrality of a node. Furthermore, three
algorithms used to social position calculation was introduced along with
results of comparison between their processing time and others centrality
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1380</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1380</id><created>2013-02-06</created><authors><author><keyname>Moreira</keyname><forenames>Catarina</forenames></author><author><keyname>Mendes</keyname><forenames>Ana Cristina</forenames></author><author><keyname>Coheur</keyname><forenames>Lu&#xed;sa</forenames></author><author><keyname>Martins</keyname><forenames>Bruno</forenames></author></authors><title>Towards the Rapid Development of a Natural Language Understanding Module</title><categories>cs.CL</categories><journal-ref>In Proceedings of the 11th International Conference on Intelligent
  Virtual Agents, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When developing a conversational agent, there is often an urgent need to have
a prototype available in order to test the application with real users. A
Wizard of Oz is a possibility, but sometimes the agent should be simply
deployed in the environment where it will be used. Here, the agent should be
able to capture as many interactions as possible and to understand how people
react to failure. In this paper, we focus on the rapid development of a natural
language understanding module by non experts. Our approach follows the learning
paradigm and sees the process of understanding natural language as a
classification problem. We test our module with a conversational agent that
answers questions in the art domain. Moreover, we show how our approach can be
used by a natural language interface to a cinema database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1390</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1390</id><created>2013-02-06</created><authors><author><keyname>Lankamp</keyname><forenames>Mike</forenames></author><author><keyname>Poss</keyname><forenames>Raphael</forenames></author><author><keyname>Yang</keyname><forenames>Qiang</forenames></author><author><keyname>Fu</keyname><forenames>Jian</forenames></author><author><keyname>Uddin</keyname><forenames>Irfan</forenames></author><author><keyname>Jesshope</keyname><forenames>Chris R.</forenames></author></authors><title>MGSim - Simulation tools for multi-core processor architectures</title><categories>cs.AR cs.DC</categories><comments>33 pages, 22 figures, 4 listings, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MGSim is an open source discrete event simulator for on-chip hardware
components, developed at the University of Amsterdam. It is intended to be a
research and teaching vehicle to study the fine-grained hardware/software
interactions on many-core and hardware multithreaded processors. It includes
support for core models with different instruction sets, a configurable
multi-core interconnect, multiple configurable cache and memory models, a
dedicated I/O subsystem, and comprehensive monitoring and interaction
facilities. The default model configuration shipped with MGSim implements
Microgrids, a many-core architecture with hardware concurrency management.
MGSim is furthermore written mostly in C++ and uses object classes to represent
chip components. It is optimized for architecture models that can be described
as process networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1393</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1393</id><created>2013-02-06</created><authors><author><keyname>Elasri</keyname><forenames>Hicham</forenames></author><author><keyname>Sekkaki</keyname><forenames>Abderrahim</forenames></author></authors><title>Semantic integration process of business components to support
  information system designers</title><categories>cs.SE</categories><comments>International Journal of Web &amp; Semantic Technology (IJWesT). arXiv
  admin note: substantial text overlap with arXiv:1110.4501</comments><journal-ref>International Journal of Web &amp; Semantic Technology (IJWesT) Vol.4,
  No.1, January 2013</journal-ref><doi>10.5121/ijwest.2013.4104</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The present work is inscribed within the intersection of two scientific
thematic: the engineering by reuse of components and ontologies alignment. The
integration of Business Components (BC) is a research problem that has been
identified in the field of engineering by reuse. Our proposal aims to provide
assistance to designers of information systems in the integration phase. It is
a process guided by domain ontology to provide semantic integration of BC. This
process allows the detection and resolution of semantic conflicts naming type
encountered in the process of integration of BC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1396</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1396</id><created>2013-02-06</created><authors><author><keyname>Xu</keyname><forenames>Hao</forenames></author><author><keyname>Jagannathan</keyname><forenames>S.</forenames></author></authors><title>Finite Horizon Adaptive Optimal Distributed Power Allocation for
  Enhanced Cognitive Radio Network in the Presence of Channel Uncertainties</title><categories>cs.ET cs.IT math.IT</categories><journal-ref>International Journal of Computer Networks and Communications
  (IJCNC) 2013</journal-ref><doi>10.5121/ijcnc.2013.5101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, novel enhanced Cognitive Radio Network is considered by using
power control where secondary users are allowed to use wireless resources of
the primary users when primary users are deactivated, but also allow secondary
users to coexist with primary users while primary users are activated by
managing interference caused from secondary users to primary users. Therefore,
a novel finite horizon adaptive optimal distributed power allocation scheme is
proposed by incorporating the effect of channel uncertainties for enhanced
cognitive radio network in the presence of wireless channel uncertainties under
two cases. In Case 1, proposed scheme can force the Signal-to-interference
(SIR) of the secondary users to converge to a higher target value for
increasing network throughput when primary users' are not communicating within
finite horizon. Once primary users are activated as in the Case 2, proposed
scheme can not only force the SIR of primary users to converge to a higher
target SIR, but also force the SIR of secondary users to converge to a lower
value for regulating their interference to Pus during finite time period. In
order to mitigate the attenuation of SIR due to channel uncertainties the
proposed novel finite horizon adaptive optimal distributed power allocation
allows the SIR of both primary users' and secondary users' to converge to a
desired target SIR while minimizing the energy consumption within finite
horizon. Simulation results illustrate that this novel finite horizon adaptive
optimal distributed power allocation scheme can converge much faster and cost
less energy than others by adapting to the channel variations optimally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1400</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1400</id><created>2013-02-06</created><authors><author><keyname>Layeb</keyname><forenames>Abdesslem</forenames></author><author><keyname>Boudra</keyname><forenames>Amira</forenames></author><author><keyname>Korichi</keyname><forenames>Wissem</forenames></author><author><keyname>Chikhi</keyname><forenames>Salim</forenames></author></authors><title>A new greedy randomized adaptive search procedure for multiobjective RNA
  structural alignment</title><categories>cs.DS cs.CE</categories><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST), Vol. 3, No.1,pp. 9-24, January 2013</journal-ref><doi>10.5121/ijfcst.2013.3102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RNA secondary structures prediction is one of the main issues in
bioinformatics. It seeks to elucidate structural conserved regions within a set
of RNA sequences. Unfortunately, finding an accurate conserved structure is a
very hard task to do. Within the present study, the prediction problem is
considered as a multiobjective optimization process in which the structural
conservation and the sensitivity of the multiple alignment are optimized. The
proposed method called GRASPMORSA is based on an aggregate function and GRASP
procedure. The initial solutions are obtained by using a random progressive
local/ global algorithm, and then they are refined by an iterative realignment.
Experiments within a large scale of data have shown the efficacy and
effectiveness of the proposed method and its capacity to reach good quality
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1402</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1402</id><created>2013-02-06</created><updated>2013-02-07</updated><authors><author><keyname>Brandst&#xe4;dt</keyname><forenames>Andreas</forenames></author><author><keyname>Esposito</keyname><forenames>Simone</forenames></author><author><keyname>Nogueira</keyname><forenames>Loana Tito</forenames></author><author><keyname>Protti</keyname><forenames>F&#xe1;bio</forenames></author></authors><title>Clique cycle-transversals in distance-hereditary graphs</title><categories>cs.DM math.CO</categories><msc-class>05C75</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A cycle-transversal of a graph G is a subset T of V(G) such that T intersects
every cycle of G. A clique cycle-transversal, or cct for short, is a
cycle-transversal which is a clique. Recognizing graphs which admit a cct can
be done in polynomial time; however, no structural characterization of such
graphs is known. We characterize distance-hereditary graphs admitting a cct in
terms of forbidden induced subgraphs. This extends similar results for chordal
graphs and cographs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1409</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1409</id><created>2013-02-06</created><authors><author><keyname>Hamodi</keyname><forenames>Jamil M.</forenames></author><author><keyname>Thool</keyname><forenames>Ravindra C.</forenames></author></authors><title>Investigate The Performance Evaluation of IPTV over WiMAX Networks</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with other internet
  sources by other authors</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.5, No.1, January 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deployment Video on Demand (VoD) over the next generation (WiMAX) has become
one of the intense interest subjects in the research these days, and is
expected to be the main revenue generators in the near future and the
efficiency of video streaming over next generation 4G is the key to enabling
this. We are considering video streaming for real time video was coded by
different H.264.x codes (H.264/AVC, and SVC), and we consider an IP-Unicast to
deliver this streaming video over WiMAX. Our approach investigates the
performance evaluation of IPTV (VoD) over WiMAX networks. OPNET is used to
investigate the performance of VoD over WiMAX. Results obtained from simulation
indicate that SVC video codec is an appropriate video codec for video streaming
over WiMAX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1419</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1419</id><created>2013-02-06</created><authors><author><keyname>Shen</keyname><forenames>Lixin</forenames></author><author><keyname>Suter</keyname><forenames>Bruce W.</forenames></author></authors><title>Blind One-Bit Compressive Sampling</title><categories>cs.IT math.IT math.NA</categories><comments>27 pages and 10 figures</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The problem of 1-bit compressive sampling is addressed in this paper. We
introduce an optimization model for reconstruction of sparse signals from 1-bit
measurements. The model targets a solution that has the least l0-norm among all
signals satisfying consistency constraints stemming from the 1-bit
measurements. An algorithm for solving the model is developed. Convergence
analysis of the algorithm is presented. Our approach is to obtain a sequence of
optimization problems by successively approximating the l0-norm and to solve
resulting problems by exploiting the proximity operator. We examine the
performance of our proposed algorithm and compare it with the binary iterative
hard thresholding (BIHT) [10] a state-of-the-art algorithm for 1-bit
compressive sampling reconstruction. Unlike the BIHT, our model and algorithm
does not require a prior knowledge on the sparsity of the signal. This makes
our proposed work a promising practical approach for signal acquisition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1422</identifier>
 <datestamp>2013-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1422</id><created>2013-02-06</created><updated>2013-05-17</updated><authors><author><keyname>Retor&#xe9;</keyname><forenames>Christian</forenames><affiliation>LaBRI, IRIT</affiliation></author></authors><title>S\'emantique des d\'eterminants dans un cadre richement typ\'e</title><categories>cs.CL</categories><proxy>ccsd</proxy><journal-ref>Traitement Automatique du Langage Naturel 2013, Les Sables
  d'Olonnes : France (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The variation of word meaning according to the context leads us to enrich the
type system of our syntactical and semantic analyser of French based on
categorial grammars and Montague semantics (or lambda-DRT). The main advantage
of a deep semantic analyse is too represent meaning by logical formulae that
can be easily used e.g. for inferences. Determiners and quantifiers play a
fundamental role in the construction of those formulae. But in our rich type
system the usual semantic terms do not work. We propose a solution ins- pired
by the tau and epsilon operators of Hilbert, kinds of generic elements and
choice functions. This approach unifies the treatment of the different determi-
ners and quantifiers as well as the dynamic binding of pronouns. Above all,
this fully computational view fits in well within the wide coverage parser
Grail, both from a theoretical and a practical viewpoint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1441</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1441</id><created>2013-02-06</created><updated>2013-10-08</updated><authors><author><keyname>Lebl</keyname><forenames>Jiri</forenames></author></authors><title>Addendum to Uniqueness of certain polynomials constant on a line</title><categories>math.AC cs.CG</categories><comments>Addendum to arXiv:0808.0284, 6 pages. Updated with degree 21 result</comments><msc-class>14P99, 68W30, 11C08, 05E99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computer calculations in arXiv:0808.0284 to classify sharp polynomials
with nonegative coefficients constant on the line $x+y=1$ have been extended to
degrees 19 and 21. In degree 19 a surprisingly large number of 13 sharp
polynomials was found, while in degree 21 only the group invariant polynomial
exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1459</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1459</id><created>2013-02-06</created><updated>2014-07-17</updated><authors><author><keyname>Nomikos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Charalambous</keyname><forenames>Themistoklis</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Skoutas</keyname><forenames>Dimitrios</forenames></author><author><keyname>Vouyioukas</keyname><forenames>Demosthenes</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author></authors><title>A Buffer-aided Successive Opportunistic Relay Selection Scheme with
  Power Adaptation and Inter-Relay Interference Cancellation for Cooperative
  Diversity Systems</title><categories>cs.IT math.IT</categories><comments>Preliminary results of this article have been presented in the IEEE
  International Symposium on Personal Indoor and Mobile Radio Communications,
  8-11 September, 2013, London, United Kingdom</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a simple cooperative network consisting of a
source, a destination and a cluster of decode-and-forward half-duplex relays.
At each time-slot, the source and (possibly) one of the relays transmit a
packet to another relay and the destination, respectively, resulting in
inter-relay interference (IRI). In this work, with the aid of buffers at the
relays, we mitigate the detrimental effect of IRI through interference
cancellation. More specifically, we propose the min-power scheme that minimizes
the total energy expenditure per time slot under an IRI cancellation scheme.
Apart from minimizing the energy expenditure, the min-power selection scheme,
also provides better throughput and lower outage probability than existing
works in the literature. It is the first time that interference cancellation is
combined with buffer-aided relays and power adaptation to mitigate the IRI and
minimize the energy expenditure. The new relay selection policy is analyzed in
terms of outage probability and diversity, by modeling the evolution of the
relay buffers as a Markov Chain (MC). We construct the state transition matrix
of the MC, and hence obtain the steady state with which we can characterize the
outage probability. The proposed scheme outperforms relevant state-of-the-art
relay selection schemes in terms of throughput, diversity and energy
efficiency, as demonstrated via examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1461</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1461</id><created>2013-02-06</created><authors><author><keyname>Wu</keyname><forenames>Jinhong</forenames></author><author><keyname>Vojcic</keyname><forenames>Branimir R.</forenames></author><author><keyname>Sheng</keyname><forenames>Jia</forenames></author></authors><title>Stopping Criteria for Iterative Decoding based on Mutual Information</title><categories>cs.IT math.IT</categories><comments>The Asilomar Conference on Signals, Systems, and Computers, Monterey,
  CA, Nov., 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate stopping criteria for iterative decoding from a
mutual information perspective. We introduce new iteration stopping rules based
on an approximation of the mutual information between encoded bits and decoder
soft output. The first type stopping rule sets a threshold value directly on
the approximated mutual information for terminating decoding. The threshold can
be adjusted according to the expected bit error rate. The second one adopts a
strategy similar to that of the well known cross-entropy stopping rule by
applying a fixed threshold on the ratio of a simple metric obtained after each
iteration over that of the first iteration. Compared with several well known
stopping rules, the new methods achieve higher efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1484</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1484</id><created>2013-02-06</created><authors><author><keyname>Zhang</keyname><forenames>Yuan</forenames></author><author><keyname>Tepedelenlioglu</keyname><forenames>Cihan</forenames></author></authors><title>Analytical and Numerical Characterizations of Shannon Ordering for
  Discrete Memoryless Channels</title><categories>cs.IT math.IT</categories><comments>30 pages, 2 figures, submitted to IEEE Transactions on Information
  Theory</comments><doi>10.1109/TIT.2013.2284771</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies several problems concerning channel inclusion, which is a
partial ordering between discrete memoryless channels (DMCs) proposed by
Shannon. Specifically, majorization-based conditions are derived for channel
inclusion between certain DMCs. Furthermore, under general conditions, channel
equivalence defined through Shannon ordering is shown to be the same as
permutation of input and output symbols. The determination of channel inclusion
is considered as a convex optimization problem, and the sparsity of the weights
related to the representation of the worse DMC in terms of the better one is
revealed when channel inclusion holds between two DMCs. For the exploitation of
this sparsity, an effective iterative algorithm is established based on
modifying the orthogonal matching pursuit algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1489</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1489</id><created>2013-02-06</created><authors><author><keyname>Sun</keyname><forenames>Hongjian</forenames></author><author><keyname>Nallanathan</keyname><forenames>A.</forenames></author><author><keyname>Jiang</keyname><forenames>Jing</forenames></author><author><keyname>Wang</keyname><forenames>Cheng-Xiang</forenames></author></authors><title>Multi-rate Sub-Nyquist Spectrum Sensing in Cognitive Radios</title><categories>cs.IT math.IT</categories><comments>This paper has been submitted to IEEE Transactions on Communications
  on 25 April 2012, revised on 29 September 2012 and 8 January 2013 for
  possible publication. This is the most recent version. This paper has 29
  pages, 7 figures, and 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wideband spectrum sensing is becoming increasingly important to cognitive
radio (CR) systems for exploiting spectral opportunities. This paper introduces
a novel multi-rate sub-Nyquist spectrum sensing (MS3) system that implements
cooperative wideband spectrum sensing in a CR network. MS3 can detect the
wideband spectrum using partial measurements without reconstructing the full
frequency spectrum. Sub-Nyquist sampling rates are adopted in sampling channels
for wrapping the frequency spectrum onto itself. This significantly reduces
sensing requirements of CR. The effects of sub-Nyquist sampling are considered,
and the performance of multi-channel sub-Nyquist samplings is analyzed. To
improve its detection performance, sub-Nyquist sampling rates are chosen to be
different such that the numbers of samples are consecutive prime numbers.
Furthermore, when the received signals at CRs are faded or shadowed, the
performance of MS3 is analytically evaluated. Numerical results show that the
proposed system can significantly enhance the wideband spectrum sensing
performance while requiring low computational and implementation complexities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1493</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1493</id><created>2013-02-06</created><updated>2013-02-07</updated><authors><author><keyname>Chanda</keyname><forenames>Abhishek</forenames></author><author><keyname>Westphal</keyname><forenames>Cedric</forenames></author></authors><title>ContentFlow: Mapping Content to Flows in Software Defined Networks</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1212.3341</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Information-Centric Networks place content as the narrow waist of the network
architecture. This allows to route based upon the content name, and not based
upon the locations of the content consumer and producer. However, current
Internet architecture does not support content routing at the network layer. We
present ContentFlow, an Information-Centric network architecture which supports
content routing by mapping the content name to an IP flow, and thus enables the
use of OpenFlow switches to achieve content routing over a legacy IP
architecture. ContentFlow is viewed as an evolutionary step between the current
IP networking architecture, and a full fledged ICN architecture. It supports
content management, content caching and content routing at the network layer,
while using a legacy OpenFlow infrastructure and a modified controller. In
particular, ContentFlow is transparent from the point of view of the client and
the server, and can be inserted in between with no modification at either end.
We have implemented ContentFlow and describe our implementation choices as well
as the overall architecture specification. We evaluate the performance of
ContentFlow in our testbed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1495</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1495</id><created>2013-02-06</created><authors><author><keyname>Ella</keyname><forenames>Vaignana Spoorthy</forenames></author></authors><title>Generating target probability sequences and events</title><categories>cs.CR</categories><comments>9 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptography and simulation of systems require that events of pre-defined
probability be generated. This paper presents methods to generate target
probability events based on the oblivious transfer protocol and target
probabilistic sequences using probability distribution functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1506</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1506</id><created>2013-02-06</created><authors><author><keyname>Shafiei</keyname><forenames>H.</forenames></author><author><keyname>Khonsari</keyname><forenames>A.</forenames></author><author><keyname>Derakhshi</keyname><forenames>H.</forenames></author><author><keyname>Mousavi</keyname><forenames>P.</forenames></author></authors><title>Rate-Privacy in Wireless Sensor Networks</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper introduces the concept of rate privacy in the context of wireless
sensor networks. Our discussion reveals that the concept indeed is of a great
importance for the privacy preservation of such networks. As a result, we
propose a buffering scheme to protect the rate from adversaries. Simulation
results verify the applicability of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1510</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1510</id><created>2013-02-06</created><authors><author><keyname>Ohashi</keyname><forenames>Ryunosuke</forenames></author><author><keyname>Kasai</keyname><forenames>Kenta</forenames></author><author><keyname>Takeuchi</keyname><forenames>Keigo</forenames></author></authors><title>Multi-Dimensional Spatially-Coupled Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatially-coupled (SC) codes are constructed by coupling many regular
low-density parity-check codes in a chain. The decoding chain of SC codes stops
when facing burst erasures. This problem can not be overcome by increasing
coupling number. In this paper, we introduce multi-dimensional (MD) SC codes.
Numerical results show that 2D-SC codes are more robust to the burst erasures
than 1D-SC codes. Furthermore, we consider designing MD-SC codes with smaller
rateloss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1511</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1511</id><created>2013-02-06</created><authors><author><keyname>Sakata</keyname><forenames>Kosuke</forenames></author><author><keyname>Kasai</keyname><forenames>Kenta</forenames></author><author><keyname>Sakaniwa</keyname><forenames>Kohichi</forenames></author></authors><title>Spatially-Coupled Precoded Rateless Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Raptor codes are rateless codes that achieve the capacity on the binary
erasure channels. However the maximum degree of optimal output degree
distribution is unbounded. This leads to a computational complexity problem
both at encoders and decoders. Aref and Urbanke investigated the potential
advantage of universal achieving-capacity property of proposed
spatially-coupled (SC) low-density generator matrix (LDGM) codes. However the
decoding error probability of SC-LDGM codes is bounded away from 0. In this
paper, we investigate SC-LDGM codes concatenated with SC low-density
parity-check codes. The proposed codes can be regarded as SC Hsu-Anastasopoulos
rateless codes. We derive a lower bound of the asymptotic overhead from
stability analysis for successful decoding by density evolution. The numerical
calculation reveals that the lower bound is tight. We observe that with a
sufficiently large number of information bits, the asymptotic overhead and the
decoding error rate approach 0 with bounded maximum degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1512</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1512</id><created>2013-02-06</created><authors><author><keyname>Tazoe</keyname><forenames>Koji</forenames></author><author><keyname>Kasai</keyname><forenames>Kenta</forenames></author><author><keyname>Sakaniwa</keyname><forenames>Kohichi</forenames></author></authors><title>Efficient Termination of Spatially-Coupled Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatially-coupled low-density parity-check codes attract much attention due
to their capacity-achieving performance and a memory-efficient sliding-window
decoding algorithm. On the other hand, the encoder needs to solve large linear
equations to terminate the encoding process. In this paper, we propose modified
spatially-coupled codes. The modified $(\dl,\dr,L)$ codes have less rate-loss,
i.e., higher coding rate, and have the same threshold as $(\dl,\dr,L)$ codes
and are efficiently terminable by using an accumulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1515</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1515</id><created>2013-02-06</created><updated>2013-07-10</updated><authors><author><keyname>Moitra</keyname><forenames>Ankur</forenames></author><author><keyname>Saks</keyname><forenames>Michael</forenames></author></authors><title>A Polynomial Time Algorithm for Lossy Population Recovery</title><categories>cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a polynomial time algorithm for the lossy population recovery
problem. In this problem, the goal is to approximately learn an unknown
distribution on binary strings of length $n$ from lossy samples: for some
parameter $\mu$ each coordinate of the sample is preserved with probability
$\mu$ and otherwise is replaced by a `?'. The running time and number of
samples needed for our algorithm is polynomial in $n$ and $1/\varepsilon$ for
each fixed $\mu&gt;0$. This improves on algorithm of Wigderson and Yehudayoff that
runs in quasi-polynomial time for any $\mu &gt; 0$ and the polynomial time
algorithm of Dvir et al which was shown to work for $\mu \gtrapprox 0.30$ by
Batman et al. In fact, our algorithm also works in the more general framework
of Batman et al. in which there is no a priori bound on the size of the support
of the distribution. The algorithm we analyze is implicit in previous work; our
main contribution is to analyze the algorithm by showing (via linear
programming duality and connections to complex analysis) that a certain matrix
associated with the problem has a robust local inverse even though its
condition number is exponentially small. A corollary of our result is the first
polynomial time algorithm for learning DNFs in the restriction access model of
Dvir et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1519</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1519</id><created>2013-02-06</created><authors><author><keyname>Bauer</keyname><forenames>Eric</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author><author><keyname>Singer</keyname><forenames>Yoram</forenames></author></authors><title>Update Rules for Parameter Estimation in Bayesian Networks</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-3-13</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper re-examines the problem of parameter estimation in Bayesian
networks with missing values and hidden variables from the perspective of
recent work in on-line learning [Kivinen &amp; Warmuth, 1994]. We provide a unified
framework for parameter estimation that encompasses both on-line learning,
where the model is continuously adapted to new data cases as they arrive, and
the more traditional batch learning, where a pre-accumulated set of samples is
used in a one-time model selection process. In the batch case, our framework
encompasses both the gradient projection algorithm and the EM algorithm for
Bayesian networks. The framework also leads to new on-line and batch parameter
update schemes, including a parameterized version of EM. We provide both
empirical and theoretical results indicating that parameterized EM allows
faster convergence to the maximum likelihood parameters than does standard EM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1520</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1520</id><created>2013-02-06</created><authors><author><keyname>Berler</keyname><forenames>Ami</forenames></author><author><keyname>Shimony</keyname><forenames>Solomon Eyal</forenames></author></authors><title>Bayes Networks for Sonar Sensor Fusion</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-14-21</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wide-angle sonar mapping of the environment by mobile robot is nontrivial due
to several sources of uncertainty: dropouts due to &quot;specular&quot; reflections,
obstacle location uncertainty due to the wide beam, and distance measurement
error. Earlier papers address the latter problems, but dropouts remain a
problem in many environments. We present an approach that lifts the
overoptimistic independence assumption used in earlier work, and use Bayes nets
to represent the dependencies between objects of the model. Objects of the
model consist of readings, and of regions in which &quot;quasi location invariance&quot;
of the (possible) obstacles exists, with respect to the readings. Simulation
supports the method's feasibility. The model is readily extensible to allow for
prior distributions, as well as other types of sensing operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1521</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1521</id><created>2013-02-06</created><authors><author><keyname>Bigham</keyname><forenames>John</forenames></author></authors><title>Exploiting Uncertain and Temporal Information in Correlation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-22-29</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A modelling language is described which is suitable for the correlation of
information when the underlying functional model of the system is incomplete or
uncertain and the temporal dependencies are imprecise. An efficient and
incremental implementation is outlined which depends on cost functions
satisfying certain criteria. Possibilistic logic and probability theory (as it
is used in the applications targetted) satisfy these criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1522</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1522</id><created>2013-02-06</created><authors><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author></authors><title>Correlated Action Effects in Decision Theoretic Regression</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-30-37</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much recent research in decision theoretic planning has adopted Markov
decision processes (MDPs) as the model of choice, and has attempted to make
their solution more tractable by exploiting problem structure. One particular
algorithm, structured policy construction achieves this by means of a decision
theoretic analog of goal regression using action descriptions based on Bayesian
networks with tree-structured conditional probability tables. The algorithm as
presented is not able to deal with actions with correlated effects. We describe
a new decision theoretic regression operator that corrects this weakness. While
conceptually straightforward, this extension requires a somewhat more
complicated technical approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1523</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1523</id><created>2013-02-06</created><authors><author><keyname>Buchner</keyname><forenames>Alex G.</forenames></author><author><keyname>Dubitzky</keyname><forenames>Werner</forenames></author><author><keyname>Schuster</keyname><forenames>Alfons</forenames></author><author><keyname>Lopes</keyname><forenames>Philippe</forenames></author><author><keyname>O'Donoghue</keyname><forenames>Peter G.</forenames></author><author><keyname>Hughes</keyname><forenames>John G.</forenames></author><author><keyname>Bell</keyname><forenames>David A.</forenames></author><author><keyname>Adamson</keyname><forenames>Kenny</forenames></author><author><keyname>White</keyname><forenames>John A.</forenames></author><author><keyname>Anderson</keyname><forenames>John M. C. C.</forenames></author><author><keyname>Mulvenna</keyname><forenames>Maurice D.</forenames></author></authors><title>Corporate Evidential Decision Making in Performance Prediction Domains</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-38-45</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance prediction or forecasting sporting outcomes involves a great deal
of insight into the particular area one is dealing with, and a considerable
amount of intuition about the factors that bear on such outcomes and
performances. The mathematical Theory of Evidence offers representation
formalisms which grant experts a high degree of freedom when expressing their
subjective beliefs in the context of decision-making situations like
performance prediction. Furthermore, this reasoning framework incorporates a
powerful mechanism to systematically pool the decisions made by individual
subject matter experts. The idea behind such a combination of knowledge is to
improve the competence (quality) of the overall decision-making process. This
paper reports on a performance prediction experiment carried out during the
European Football Championship in 1996. Relying on the knowledge of four
predictors, Evidence Theory was used to forecast the final scores of all 31
matches. The results of this empirical study are very encouraging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1524</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1524</id><created>2013-02-06</created><authors><author><keyname>de Campos</keyname><forenames>Luis M.</forenames></author><author><keyname>Huete</keyname><forenames>Juan F.</forenames></author></authors><title>Algorithms for Learning Decomposable Models and Chordal Graphs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-46-53</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decomposable dependency models and their graphical counterparts, i.e.,
chordal graphs, possess a number of interesting and useful properties. On the
basis of two characterizations of decomposable models in terms of independence
relationships, we develop an exact algorithm for recovering the chordal
graphical representation of any given decomposable model. We also propose an
algorithm for learning chordal approximations of dependency models isomorphic
to general undirected graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1525</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1525</id><created>2013-02-06</created><authors><author><keyname>Cassandra</keyname><forenames>Anthony R.</forenames></author><author><keyname>Littman</keyname><forenames>Michael L.</forenames></author><author><keyname>Zhang</keyname><forenames>Nevin Lianwen</forenames></author></authors><title>Incremental Pruning: A Simple, Fast, Exact Method for Partially
  Observable Markov Decision Processes</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-54-61</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most exact algorithms for general partially observable Markov decision
processes (POMDPs) use a form of dynamic programming in which a
piecewise-linear and convex representation of one value function is transformed
into another. We examine variations of the &quot;incremental pruning&quot; method for
solving this problem and compare them to earlier algorithms from theoretical
and empirical perspectives. We find that incremental pruning is presently the
most efficient exact method for solving POMDPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1526</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1526</id><created>2013-02-06</created><authors><author><keyname>Chajewska</keyname><forenames>Urszula</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Defining Explanation in Probabilistic Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-62-71</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As probabilistic systems gain popularity and are coming into wider use, the
need for a mechanism that explains the system's findings and recommendations
becomes more critical. The system will also need a mechanism for ordering
competing explanations. We examine two representative approaches to explanation
in the literature - one due to G\&quot;ardenfors and one due to Pearl - and show
that both suffer from significant problems. We propose an approach to defining
a notion of &quot;better explanation&quot; that combines some of the features of both
together with more recent work by Pearl and others on causality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1527</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1527</id><created>2013-02-06</created><authors><author><keyname>Cheuk</keyname><forenames>Adrian Y. W.</forenames></author><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author></authors><title>Structured Arc Reversal and Simulation of Dynamic Probabilistic Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-72-79</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for arc reversal in Bayesian networks with
tree-structured conditional probability tables, and consider some of its
advantages, especially for the simulation of dynamic probabilistic networks. In
particular, the method allows one to produce CPTs for nodes involved in the
reversal that exploit regularities in the conditional distributions. We argue
that this approach alleviates some of the overhead associated with arc
reversal, plays an important role in evidence integration and can be used to
restrict sampling of variables in DPNs. We also provide an algorithm that
detects the dynamic irrelevance of state variables in forward simulation. This
algorithm exploits the structured CPTs in a reversed network to determine, in a
time-independent fashion, the conditions under which a variable does or does
not need to be sampled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1528</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1528</id><created>2013-02-06</created><updated>2015-05-16</updated><authors><author><keyname>Chickering</keyname><forenames>David Maxwell</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author></authors><title>A Bayesian Approach to Learning Bayesian Networks with Local Structure</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1997-PG-80-89</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently several researchers have investigated techniques for using data to
learn Bayesian networks containing compact representations for the conditional
probability distributions (CPDs) stored at each node. The majority of this work
has concentrated on using decision-tree representations for the CPDs. In
addition, researchers typically apply non-Bayesian (or asymptotically Bayesian)
scoring functions such as MDL to evaluate the goodness-of-fit of networks to
the data. In this paper we investigate a Bayesian approach to learning Bayesian
networks that contain the more general decision-graph representations of the
CPDs. First, we describe how to evaluate the posterior probability that is, the
Bayesian score of such a network, given a database of observed cases. Second,
we describe various search spaces that can be used, in conjunction with a
scoring function and a search procedure, to identify one or more high-scoring
networks. Finally, we present an experimental evaluation of the search spaces,
using a greedy algorithm and a Bayesian scoring function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1529</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1529</id><created>2013-02-06</created><authors><author><keyname>Chu</keyname><forenames>TongSheng</forenames></author><author><keyname>Xiang</keyname><forenames>Yang</forenames></author></authors><title>Exploring Parallelism in Learning Belief Networks</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-90-98</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been shown that a class of probabilistic domain models cannot be
learned correctly by several existing algorithms which employ a single-link
look ahead search. When a multi-link look ahead search is used, the
computational complexity of the learning algorithm increases. We study how to
use parallelism to tackle the increased complexity in learning such models and
to speed up learning in large domains. An algorithm is proposed to decompose
the learning task for parallel processing. A further task decomposition is used
to balance load among processors and to increase the speed-up and efficiency.
For learning from very large datasets, we present a regrouping of the available
processors such that slow data access through file can be replaced by fast
memory access. Our implementation in a parallel computer demonstrates the
effectiveness of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1530</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1530</id><created>2013-02-06</created><authors><author><keyname>Collins</keyname><forenames>Matthew S.</forenames></author><author><keyname>Oliver</keyname><forenames>Jonathan</forenames></author></authors><title>Efficient Induction of Finite State Automata</title><categories>cs.AI cs.FL</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-99-107</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new algorithm for the induction if complex finite
state automata from samples of behavior. The algorithm is based on information
theoretic principles. The algorithm reduces the search space by many orders of
magnitude over what was previously thought possible. We compare the algorithm
with some existing induction techniques for finite state automata and show that
the algorithm is much superior in both run time and quality of inductions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1531</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1531</id><created>2013-02-06</created><authors><author><keyname>Cozman</keyname><forenames>Fabio Gagliardi</forenames></author></authors><title>Robustness Analysis of Bayesian Networks with Local Convex Sets of
  Distributions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-108-115</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust Bayesian inference is the calculation of posterior probability bounds
given perturbations in a probabilistic model. This paper focuses on
perturbations that can be expressed locally in Bayesian networks through convex
sets of distributions. Two approaches for combination of local models are
considered. The first approach takes the largest set of joint distributions
that is compatible with the local sets of distributions; we show how to reduce
this type of robust inference to a linear programming problem. The second
approach takes the convex hull of joint distributions generated from the local
sets of distributions; we demonstrate how to apply interior-point optimization
methods to generate posterior bounds and how to generate approximations that
are guaranteed to converge to correct posterior bounds. We also discuss
calculation of bounds for expected utilities and variances, and global
perturbation models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1532</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1532</id><created>2013-02-06</created><authors><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author><author><keyname>Provan</keyname><forenames>Gregory M.</forenames></author></authors><title>A Standard Approach for Optimizing Belief Network Inference using Query
  DAGs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-116-123</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel, algorithm-independent approach to optimizing
belief network inference. rather than designing optimizations on an algorithm
by algorithm basis, we argue that one should use an unoptimized algorithm to
generate a Q-DAG, a compiled graphical representation of the belief network,
and then optimize the Q-DAG and its evaluator instead. We present a set of
Q-DAG optimizations that supplant optimizations designed for traditional
inference algorithms, including zero compression, network pruning and caching.
We show that our Q-DAG optimizations require time linear in the Q-DAG size, and
significantly simplify the process of designing algorithms for optimizing
belief network inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1533</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1533</id><created>2013-02-06</created><authors><author><keyname>Dean</keyname><forenames>Thomas L.</forenames></author><author><keyname>Givan</keyname><forenames>Robert</forenames></author><author><keyname>Leach</keyname><forenames>Sonia</forenames></author></authors><title>Model Reduction Techniques for Computing Approximately Optimal Solutions
  for Markov Decision Processes</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-124-131</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for solving implicit (factored) Markov decision processes
(MDPs) with very large state spaces. We introduce a property of state space
partitions which we call epsilon-homogeneity. Intuitively, an
epsilon-homogeneous partition groups together states that behave approximately
the same under all or some subset of policies. Borrowing from recent work on
model minimization in computer-aided software verification, we present an
algorithm that takes a factored representation of an MDP and an 0&lt;=epsilon&lt;=1
and computes a factored epsilon-homogeneous partition of the state space. This
partition defines a family of related MDPs - those MDPs with state space equal
to the blocks of the partition, and transition probabilities &quot;approximately&quot;
like those of any (original MDP) state in the source block. To formally study
such families of MDPs, we introduce the new notion of a &quot;bounded parameter MDP&quot;
(BMDP), which is a family of (traditional) MDPs defined by specifying upper and
lower bounds on the transition probabilities and rewards. We describe
algorithms that operate on BMDPs to find policies that are approximately
optimal with respect to the original MDP. In combination, our method for
reducing a large implicit MDP to a possibly much smaller BMDP using an
epsilon-homogeneous partition, and our methods for selecting actions in BMDPs
constitute a new approach for analyzing large implicit MDPs. Among its
advantages, this new approach provides insight into existing algorithms to
solving implicit MDPs, provides useful connections to work in automata theory
and model minimization, and suggests methods, which involve varying epsilon, to
trade time and space (specifically in terms of the size of the corresponding
state space) for solution quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1534</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1534</id><created>2013-02-06</created><authors><author><keyname>Dechter</keyname><forenames>Rina</forenames></author><author><keyname>Rish</keyname><forenames>Irina</forenames></author></authors><title>A Scheme for Approximating Probabilistic Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-132-141</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a class of probabilistic approximation algorithms based
on bucket elimination which offer adjustable levels of accuracy and efficiency.
We analyze the approximation for several tasks: finding the most probable
explanation, belief updating and finding the maximum a posteriori hypothesis.
We identify regions of completeness and provide preliminary empirical
evaluation on randomly generated networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1535</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1535</id><created>2013-02-06</created><authors><author><keyname>Dittmer</keyname><forenames>Soren L.</forenames></author><author><keyname>Jensen</keyname><forenames>Finn Verner</forenames></author></authors><title>Myopic Value of Information in Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-142-149</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for calculation of myopic value of information in
influence diagrams (Howard &amp; Matheson, 1981) based on the strong junction tree
framework (Jensen, Jensen &amp; Dittmer, 1994). The difference in instantiation
order in the influence diagrams is reflected in the corresponding junction
trees by the order in which the chance nodes are marginalized. This order of
marginalization can be changed by table expansion and in effect the same
junction tree with expanded tables may be used for calculating the expected
utility for scenarios with different instantiation order. We also compare our
method to the classic method of modeling different instantiation orders in the
same influence diagram.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1536</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1536</id><created>2013-02-06</created><authors><author><keyname>Doerpmund</keyname><forenames>Jens</forenames></author></authors><title>Limitations of Skeptical Default Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-150-156</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Poole has shown that nonmonotonic logics do not handle the lottery paradox
correctly. In this paper we will show that Pollock's theory of defeasible
reasoning fails for the same reason: defeasible reasoning is incompatible with
the skeptical notion of derivability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1537</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1537</id><created>2013-02-06</created><authors><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Fargier</keyname><forenames>Helene</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>Decision-making Under Ordinal Preferences and Comparative Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-157-164</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of finding a preference relation on a set
of acts from the knowledge of an ordering on events (subsets of states of the
world) describing the decision-maker (DM)s uncertainty and an ordering of
consequences of acts, describing the DMs preferences. However, contrary to
classical approaches to decision theory, we try to do it without resorting to
any numerical representation of utility nor uncertainty, and without even using
any qualitative scale on which both uncertainty and preference could be mapped.
It is shown that although many axioms of Savage theory can be preserved and
despite the intuitive appeal of the method for constructing a preference over
acts, the approach is inconsistent with a probabilistic representation of
uncertainty, but leads to the kind of uncertainty theory encountered in
non-monotonic reasoning (especially preferential and rational inference),
closely related to possibility theory. Moreover the method turns out to be
either very little decisive or to lead to very risky decisions, although its
basic principles look sound. This paper raises the question of the very
possibility of purely symbolic approaches to Savage-like decision-making under
uncertainty and obtains preliminary negative results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1538</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1538</id><created>2013-02-06</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Goldszmidt</keyname><forenames>Moises</forenames></author></authors><title>Sequential Update of Bayesian Network Structure</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-165-174</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is an obvious need for improving the performance and accuracy of a
Bayesian network as new data is observed. Because of errors in model
construction and changes in the dynamics of the domains, we cannot afford to
ignore the information in new data. While sequential update of parameters for a
fixed structure can be accomplished using standard techniques, sequential
update of network structure is still an open problem. In this paper, we
investigate sequential update of Bayesian networks were both parameters and
structure are expected to change. We introduce a new approach that allows for
the flexible manipulation of the tradeoff between the quality of the learned
networks and the amount of information that is maintained about past
observations. We formally describe our approach including the necessary
modifications to the scoring functions for learning Bayesian networks, evaluate
its effectiveness through an empirical study, and extend it to the case of
missing data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1539</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1539</id><created>2013-02-06</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Russell</keyname><forenames>Stuart</forenames></author></authors><title>Image Segmentation in Video Sequences: A Probabilistic Approach</title><categories>cs.CV cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-175-181</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Background subtraction&quot; is an old technique for finding moving objects in a
video sequence for example, cars driving on a freeway. The idea is that
subtracting the current image from a timeaveraged background image will leave
only nonstationary objects. It is, however, a crude approximation to the task
of classifying each pixel of the current image; it fails with slow-moving
objects and does not distinguish shadows from moving objects. The basic idea of
this paper is that we can classify each pixel using a model of how that pixel
looks when it is part of different classes. We learn a mixture-of-Gaussians
classification model for each pixel using an unsupervised technique- an
efficient, incremental version of EM. Unlike the standard image-averaging
approach, this automatically updates the mixture component for each class
according to likelihood of membership; hence slow-moving objects are handled
perfectly. Our approach also identifies and eliminates shadows much more
effectively than other techniques such as thresholding. Application of this
method as part of the Roadwatch traffic surveillance project is expected to
result in significant improvements in vehicle identification and tracking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1540</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1540</id><created>2013-02-06</created><authors><author><keyname>Goldsmith</keyname><forenames>Judy</forenames></author><author><keyname>Littman</keyname><forenames>Michael L.</forenames></author><author><keyname>Mundhenk</keyname><forenames>Martin</forenames></author></authors><title>The Complexity of Plan Existence and Evaluation in Probabilistic Domains</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-182-189</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the computational complexity of testing and finding small plans in
probabilistic planning domains with succinct representations. We find that many
problems of interest are complete for a variety of complexity classes: NP,
co-NP, PP, NP^PP, co-NP^PP, and PSPACE. Of these, the probabilistic classes PP
and NP^PP are likely to be of special interest in the field of uncertainty in
artificial intelligence and are deserving of additional study. These results
suggest a fruitful direction of future algorithmic development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1541</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1541</id><created>2013-02-06</created><authors><author><keyname>Gomes</keyname><forenames>Carla P.</forenames></author><author><keyname>Selman</keyname><forenames>Bart</forenames></author></authors><title>Algorithm Portfolio Design: Theory vs. Practice</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-190-197</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic algorithms are among the best for solving computationally hard
search and reasoning problems. The runtime of such procedures is characterized
by a random variable. Different algorithms give rise to different probability
distributions. One can take advantage of such differences by combining several
algorithms into a portfolio, and running them in parallel or interleaving them
on a single processor. We provide a detailed evaluation of the portfolio
approach on distributions of hard combinatorial search problems. We show under
what conditions the protfolio approach can have a dramatic computational
advantage over the best traditional methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1542</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1542</id><created>2013-02-06</created><authors><author><keyname>Greiner</keyname><forenames>Russell</forenames></author><author><keyname>Grove</keyname><forenames>Adam J.</forenames></author><author><keyname>Schuurmans</keyname><forenames>Dale</forenames></author></authors><title>Learning Bayesian Nets that Perform Well</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-198-207</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Bayesian net (BN) is more than a succinct way to encode a probabilistic
distribution; it also corresponds to a function used to answer queries. A BN
can therefore be evaluated by the accuracy of the answers it returns. Many
algorithms for learning BNs, however, attempt to optimize another criterion
(usually likelihood, possibly augmented with a regularizing term), which is
independent of the distribution of queries that are posed. This paper takes the
&quot;performance criteria&quot; seriously, and considers the challenge of computing the
BN whose performance - read &quot;accuracy over the distribution of queries&quot; - is
optimal. We show that many aspects of this learning task are more difficult
than the corresponding subtasks in the standard model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1543</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1543</id><created>2013-02-06</created><authors><author><keyname>Grove</keyname><forenames>Adam J.</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Probability Update: Conditioning vs. Cross-Entropy</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-208-214</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditioning is the generally agreed-upon method for updating probability
distributions when one learns that an event is certainly true. But it has been
argued that we need other rules, in particular the rule of cross-entropy
minimization, to handle updates that involve uncertain information. In this
paper we re-examine such a case: van Fraassen's Judy Benjamin problem, which in
essence asks how one might update given the value of a conditional probability.
We argue that -- contrary to the suggestions in the literature -- it is
possible to use simple conditionalization in this case, and thereby obtain
answers that agree fully with intuition. This contrasts with proposals such as
cross-entropy, which are easier to apply but can give unsatisfactory answers.
Based on the lessons from this example, we speculate on some general
philosophical issues concerning probability update.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1544</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1544</id><created>2013-02-06</created><authors><author><keyname>Ha</keyname><forenames>Vu A.</forenames></author><author><keyname>Haddawy</keyname><forenames>Peter</forenames></author></authors><title>Problem-Focused Incremental Elicitation of Multi-Attribute Utility
  Models</title><categories>cs.AI cs.GT</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-215-222</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision theory has become widely accepted in the AI community as a useful
framework for planning and decision making. Applying the framework typically
requires elicitation of some form of probability and utility information. While
much work in AI has focused on providing representations and tools for
elicitation of probabilities, relatively little work has addressed the
elicitation of utility models. This imbalance is not particularly justified
considering that probability models are relatively stable across problem
instances, while utility models may be different for each instance. Spending
large amounts of time on elicitation can be undesirable for interactive systems
used in low-stakes decision making and in time-critical decision making. In
this paper we investigate the issues of reasoning with incomplete utility
models. We identify patterns of problem instances where plans can be proved to
be suboptimal if the (unknown) utility function satisfies certain conditions.
We present an approach to planning and decision making that performs the
utility elicitation incrementally and in a way that is informed by the domain
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1545</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1545</id><created>2013-02-06</created><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author></authors><title>Models and Selection Criteria for Regression and Classification</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-223-228</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When performing regression or classification, we are interested in the
conditional probability distribution for an outcome or class variable Y given a
set of explanatoryor input variables X. We consider Bayesian models for this
task. In particular, we examine a special class of models, which we call
Bayesian regression/classification (BRC) models, that can be factored into
independent conditional (y|x) and input (x) models. These models are
convenient, because the conditional model (the portion of the full model that
we care about) can be analyzed by itself. We examine the practice of
transforming arbitrary Bayesian models to BRC models, and argue that this
practice is often inappropriate because it ignores prior knowledge that may be
important for learning. In addition, we examine Bayesian methods for learning
models from data. We discuss two criteria for Bayesian model selection that are
appropriate for repression/classification: one described by Spiegelhalter et
al. (1993), and another by Buntine (1993). We contrast these two criteria using
the prequential framework of Dawid (1984), and give sufficient conditions under
which the criteria agree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1546</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1546</id><created>2013-02-06</created><authors><author><keyname>Hernandez</keyname><forenames>Luis D.</forenames></author><author><keyname>Moral</keyname><forenames>Serafin</forenames></author></authors><title>Inference with Idempotent Valuations</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-229-237</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Valuation based systems verifying an idempotent property are studied. A
partial order is defined between the valuations giving them a lattice
structure. Then, two different strategies are introduced to represent
valuations: as infimum of the most informative valuations or as supremum of the
least informative ones. It is studied how to carry out computations with both
representations in an efficient way. The particular cases of finite sets and
convex polytopes are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1547</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1547</id><created>2013-02-06</created><authors><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author><author><keyname>Lengyel</keyname><forenames>Jed</forenames></author></authors><title>Perception, Attention, and Resources: A Decision-Theoretic Approach to
  Graphics Rendering</title><categories>cs.AI cs.GR</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-238-249</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe work to control graphics rendering under limited computational
resources by taking a decision-theoretic perspective on perceptual costs and
computational savings of approximations. The work extends earlier work on the
control of rendering by introducing methods and models for computing the
expected cost associated with degradations of scene components. The expected
cost is computed by considering the perceptual cost of degradations and a
probability distribution over the attentional focus of viewers. We review the
critical literature describing findings on visual search and attention, discuss
the implications of the findings, and introduce models of expected perceptual
cost. Finally, we discuss policies that harness information about the expected
cost of scene components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1548</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1548</id><created>2013-02-06</created><authors><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author><author><keyname>Seiver</keyname><forenames>Adam</forenames></author></authors><title>Time-Critical Reasoning: Representations and Application</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-250-257</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review the problem of time-critical action and discuss a reformulation
that shifts knowledge acquisition from the assessment of complex temporal
probabilistic dependencies to the direct assessment of time-dependent utilities
over key outcomes of interest. We dwell on a class of decision problems
characterized by the centrality of diagnosing and reacting in a timely manner
to pathological processes. We motivate key ideas in the context of trauma-care
triage and transportation decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1549</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1549</id><created>2013-02-06</created><authors><author><keyname>Hu</keyname><forenames>Jun</forenames></author><author><keyname>Xiang</keyname><forenames>Yang</forenames></author></authors><title>Learning Belief Networks in Domains with Recursively Embedded Pseudo
  Independent Submodels</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-258-265</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pseudo independent (PI) model is a probabilistic domain model (PDM) where
proper subsets of a set of collectively dependent variables display marginal
independence. PI models cannot be learned correctly by many algorithms that
rely on a single link search. Earlier work on learning PI models has suggested
a straightforward multi-link search algorithm. However, when a domain contains
recursively embedded PI submodels, it may escape the detection of such an
algorithm. In this paper, we propose an improved algorithm that ensures the
learning of all embedded PI submodels whose sizes are upper bounded by a
predetermined parameter. We show that this improved learning capability only
increases the complexity slightly beyond that of the previous algorithm. The
performance of the new algorithm is demonstrated through experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1550</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1550</id><created>2013-02-06</created><authors><author><keyname>Jaeger</keyname><forenames>Manfred</forenames></author></authors><title>Relational Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-266-273</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method is developed to represent probabilistic relations on multiple
random events. Where previously knowledge bases containing probabilistic rules
were used for this purpose, here a probability distribution over the relations
is directly represented by a Bayesian network. By using a powerful way of
specifying conditional probability distributions in these networks, the
resulting formalism is more expressive than the previous ones. Particularly, it
provides for constraints on equalities of events, and it allows to define
complex, nested combination functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1551</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1551</id><created>2013-02-06</created><authors><author><keyname>Jirousek</keyname><forenames>Radim</forenames></author></authors><title>Composition of Probability Measures on Finite Spaces</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-274-281</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decomposable models and Bayesian networks can be defined as sequences of
oligo-dimensional probability measures connected with operators of composition.
The preliminary results suggest that the probabilistic models allowing for
effective computational procedures are represented by sequences possessing a
special property; we shall call them perfect sequences. The paper lays down the
elementary foundation necessary for further study of iterative application of
operators of composition. We believe to develop a technique describing several
graph models in a unifying way. We are convinced that practically all
theoretical results and procedures connected with decomposable models and
Bayesian networks can be translated into the terminology introduced in this
paper. For example, complexity of computational procedures in these models is
closely dependent on possibility to change the ordering of oligo-dimensional
measures defining the model. Therefore, in this paper, lot of attention is paid
to possibility to change ordering of the operators of composition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1552</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1552</id><created>2013-02-06</created><authors><author><keyname>Kearns</keyname><forenames>Michael</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Ng</keyname><forenames>Andrew Y.</forenames></author></authors><title>An Information-Theoretic Analysis of Hard and Soft Assignment Methods
  for Clustering</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-282-293</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assignment methods are at the heart of many algorithms for unsupervised
learning and clustering - in particular, the well-known K-means and
Expectation-Maximization (EM) algorithms. In this work, we study several
different methods of assignment, including the &quot;hard&quot; assignments used by
K-means and the ?soft' assignments used by EM. While it is known that K-means
minimizes the distortion on the data and EM maximizes the likelihood, little is
known about the systematic differences of behavior between the two algorithms.
Here we shed light on these differences via an information-theoretic analysis.
The cornerstone of our results is a simple decomposition of the expected
distortion, showing that K-means (and its extension for inferring general
parametric densities from unlabeled sample data) must implicitly manage a
trade-off between how similar the data assigned to each cluster are, and how
the data are balanced among the clusters. How well the data are balanced is
measured by the entropy of the partition defined by the hard assignments. In
addition to letting us predict and verify systematic differences between
K-means and EM on specific examples, the decomposition allows us to give a
rather general argument showing that K ?means will consistently find densities
with less &quot;overlap&quot; than EM. We also study a third natural assignment method
that we call posterior assignment, that is close in spirit to the soft
assignments of EM, but leads to a surprisingly different algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1553</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1553</id><created>2013-02-06</created><authors><author><keyname>Kj&#xe6;rulff</keyname><forenames>Uffe</forenames></author></authors><title>Nested Junction Trees</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-294-301</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The efficiency of inference in both the Hugin and, most notably, the
Shafer-Shenoy architectures can be improved by exploiting the independence
relations induced by the incoming messages of a clique. That is, the message to
be sent from a clique can be computed via a factorization of the clique
potential in the form of a junction tree. In this paper we show that by
exploiting such nested junction trees in the computation of messages both space
and time costs of the conventional propagation methods may be reduced. The
paper presents a structured way of exploiting the nested junction trees
technique to achieve such reductions. The usefulness of the method is
emphasized through a thorough empirical evaluation involving ten large
real-world Bayesian networks and the Hugin inference algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1554</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1554</id><created>2013-02-06</created><authors><author><keyname>Koller</keyname><forenames>Daphne</forenames></author><author><keyname>Pfeffer</keyname><forenames>Avi</forenames></author></authors><title>Object-Oriented Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-302-313</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian networks provide a modeling language and associated inference
algorithm for stochastic domains. They have been successfully applied in a
variety of medium-scale applications. However, when faced with a large complex
domain, the task of modeling using Bayesian networks begins to resemble the
task of programming using logical circuits. In this paper, we describe an
object-oriented Bayesian network (OOBN) language, which allows complex domains
to be described in terms of inter-related objects. We use a Bayesian network
fragment to describe the probabilistic relations between the attributes of an
object. These attributes can themselves be objects, providing a natural
framework for encoding part-of hierarchies. Classes are used to provide a
reusable probabilistic model which can be applied to multiple similar objects.
Classes also support inheritance of model fragments from a class to a subclass,
allowing the common aspects of related classes to be defined only once. Our
language has clear declarative semantics: an OOBN can be interpreted as a
stochastic functional program, so that it uniquely specifies a probabilistic
model. We provide an inference algorithm for OOBNs, and show that much of the
structural information encoded by an OOBN--particularly the encapsulation of
variables within an object and the reuse of model fragments in different
contexts--can also be used to speed up the inference process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1555</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1555</id><created>2013-02-06</created><authors><author><keyname>Kozlov</keyname><forenames>Alexander V.</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Nonuniform Dynamic Discretization in Hybrid Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-314-325</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider probabilistic inference in general hybrid networks, which include
continuous and discrete variables in an arbitrary topology. We reexamine the
question of variable discretization in a hybrid network aiming at minimizing
the information loss induced by the discretization. We show that a nonuniform
partition across all variables as opposed to uniform partition of each variable
separately reduces the size of the data structures needed to represent a
continuous function. We also provide a simple but efficient procedure for
nonuniform partition. To represent a nonuniform discretization in the computer
memory, we introduce a new data structure, which we call a Binary Split
Partition (BSP) tree. We show that BSP trees can be an exponential factor
smaller than the data structures in the standard uniform discretization in
multiple dimensions and show how the BSP trees can be used in the standard join
tree algorithm. We show that the accuracy of the inference process can be
significantly improved by adjusting discretization with evidence. We construct
an iterative anytime algorithm that gradually improves the quality of the
discretization and the accuracy of the answer on a query. We provide empirical
evidence that the algorithm converges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1556</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1556</id><created>2013-02-06</created><authors><author><keyname>Kyburg</keyname><forenames>Henry E.</forenames><suffix>Jr</suffix></author></authors><title>Probabilistic Acceptance</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-326-333</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of fully accepting statements when the evidence has rendered them
probable enough faces a number of difficulties. We leave the interpretation of
probability largely open, but attempt to suggest a contextual approach to full
belief. We show that the difficulties of probabilistic acceptance are not as
severe as they are sometimes painted, and that though there are oddities
associated with probabilistic acceptance they are in some instances less
awkward than the difficulties associated with other nonmonotonic formalisms. We
show that the structure at which we arrive provides a natural home for
statistical inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1557</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1557</id><created>2013-02-06</created><authors><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author><author><keyname>Mahoney</keyname><forenames>Suzanne M.</forenames></author></authors><title>Network Fragments: Representing Knowledge for Constructing Probabilistic
  Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-334-341</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In most current applications of belief networks, domain knowledge is
represented by a single belief network that applies to all problem instances in
the domain. In more complex domains, problem-specific models must be
constructed from a knowledge base encoding probabilistic relationships in the
domain. Most work in knowledge-based model construction takes the rule as the
basic unit of knowledge. We present a knowledge representation framework that
permits the knowledge base designer to specify knowledge in larger semantically
meaningful units which we call network fragments. Our framework provides for
representation of asymmetric independence and canonical intercausal
interaction. We discuss the combination of network fragments to form
problem-specific models to reason about particular problem instances. The
framework is illustrated using examples from the domain of military situation
awareness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1558</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1558</id><created>2013-02-06</created><authors><author><keyname>Lin</keyname><forenames>Yan</forenames></author><author><keyname>Druzdzel</keyname><forenames>Marek J.</forenames></author></authors><title>Computational Advantages of Relevance Reasoning in Bayesian Belief
  Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-342-350</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a computational framework for reasoning in Bayesian
belief networks that derives significant advantages from focused inference and
relevance reasoning. This framework is based on d -separation and other simple
and computationally efficient techniques for pruning irrelevant parts of a
network. Our main contribution is a technique that we call relevance-based
decomposition. Relevance-based decomposition approaches belief updating in
large networks by focusing on their parts and decomposing them into partially
overlapping subnetworks. This makes reasoning in some intractable networks
possible and, in addition, often results in significant speedup, as the total
time taken to update all subnetworks is in practice often considerably less
than the time taken to update the network as a whole. We report results of
empirical tests that demonstrate practical significance of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1559</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1559</id><created>2013-02-06</created><authors><author><keyname>Lopez-Sanchez</keyname><forenames>Maite</forenames></author><author><keyname>de Mantaras</keyname><forenames>Ramon Lopez</forenames></author><author><keyname>Sierra</keyname><forenames>Carles</forenames></author></authors><title>Incremental Map Generation by Low Cost Robots Based on
  Possibility/Necessity Grids</title><categories>cs.RO cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-351-357</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present some results obtained with a troupe of low-cost
robots designed to cooperatively explore and adquire the map of unknown
structured orthogonal environments. In order to improve the covering of the
explored zone, the robots show different behaviours and cooperate by
transferring each other the perceived environment when they meet. The returning
robots deliver to a host computer their partial maps and the host incrementally
generates the map of the environment by means of apossibility/ necessity grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1560</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1560</id><created>2013-02-06</created><authors><author><keyname>Mansell</keyname><forenames>Todd Michael</forenames></author></authors><title>A Target Classification Decision Aid</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-358-365</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A submarine's sonar team is responsible for detecting, localising and
classifying targets using information provided by the platform's sensor suite.
The information used to make these assessments is typically uncertain and/or
incomplete and is likely to require a measure of confidence in its reliability.
Moreover, improvements in sensor and communication technology are resulting in
increased amounts of on-platform and off-platform information available for
evaluation. This proliferation of imprecise information increases the risk of
overwhelming the operator. To assist the task of localisation and
classification a concept demonstration decision aid (Horizon), based on
evidential reasoning, has been developed. Horizon is an information fusion
software package for representing and fusing imprecise information about the
state of the world, expressed across suitable frames of reference. The Horizon
software is currently at prototype stage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1561</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1561</id><created>2013-02-06</created><updated>2015-05-16</updated><authors><author><keyname>Meek</keyname><forenames>Christopher</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Structure and Parameter Learning for Causal Independence and Causal
  Interaction Models</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1997-PG-366-375</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses causal independence models and a generalization of these
models called causal interaction models. Causal interaction models are models
that have independent mechanisms where a mechanism can have several causes. In
addition to introducing several particular types of causal interaction models,
we show how we can apply the Bayesian approach to learning causal interaction
models obtaining approximate posterior distributions for the models and obtain
MAP and ML estimates for the parameters. We illustrate the approach with a
simulation study of learning model posteriors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1562</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1562</id><created>2013-02-06</created><authors><author><keyname>Monney</keyname><forenames>Paul-Andre</forenames></author></authors><title>Support and Plausibility Degrees in Generalized Functional Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-376-383</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By discussing several examples, the theory of generalized functional models
is shown to be very natural for modeling some situations of reasoning under
uncertainty. A generalized functional model is a pair (f, P) where f is a
function describing the interactions between a parameter variable, an
observation variable and a random source, and P is a probability distribution
for the random source. Unlike traditional functional models, generalized
functional models do not require that there is only one value of the parameter
variable that is compatible with an observation and a realization of the random
source. As a consequence, the results of the analysis of a generalized
functional model are not expressed in terms of probability distributions but
rather by support and plausibility functions. The analysis of a generalized
functional model is very logical and is inspired from ideas already put forward
by R.A. Fisher in his theory of fiducial probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1563</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1563</id><created>2013-02-06</created><authors><author><keyname>Morris</keyname><forenames>Scott B.</forenames></author><author><keyname>Cork</keyname><forenames>Doug</forenames></author><author><keyname>Neapolitan</keyname><forenames>Richard E.</forenames></author></authors><title>The Cognitive Processing of Causal Knowledge</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-384-391</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a brief description of the probabilistic causal graph model for
representing, reasoning with, and learning causal structure using Bayesian
networks. It is then argued that this model is closely related to how humans
reason with and learn causal structure. It is shown that studies in psychology
on discounting (reasoning concerning how the presence of one cause of an effect
makes another cause less probable) support the hypothesis that humans reach the
same judgments as algorithms for doing inference in Bayesian networks. Next, it
is shown how studies by Piaget indicate that humans learn causal structure by
observing the same independencies and dependencies as those used by certain
algorithms for learning the structure of a Bayesian network. Based on this
indication, a subjective definition of causality is forwarded. Finally, methods
for further testing the accuracy of these claims are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1564</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1564</id><created>2013-02-06</created><authors><author><keyname>Pennock</keyname><forenames>David M.</forenames></author><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author></authors><title>Representing Aggregate Belief through the Competitive Equilibrium of a
  Securities Market</title><categories>cs.AI cs.GT q-fin.GN</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-392-400</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of belief aggregation: given a group of individual
agents with probabilistic beliefs over a set of uncertain events, formulate a
sensible consensus or aggregate probability distribution over these events.
Researchers have proposed many aggregation methods, although on the question of
which is best the general consensus is that there is no consensus. We develop a
market-based approach to this problem, where agents bet on uncertain events by
buying or selling securities contingent on their outcomes. Each agent acts in
the market so as to maximize expected utility at given securities prices,
limited in its activity only by its own risk aversion. The equilibrium prices
of goods in this market represent aggregate beliefs. For agents with constant
risk aversion, we demonstrate that the aggregate probability exhibits several
desirable properties, and is related to independently motivated techniques. We
argue that the market-based approach provides a plausible mechanism for belief
aggregation in multiagent systems, as it directly addresses self-motivated
agent incentives for participation and for truthfulness, and can provide a
decision-theoretic foundation for the &quot;expert weights&quot; often employed in
centralized pooling techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1565</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1565</id><created>2013-02-06</created><authors><author><keyname>Ramoni</keyname><forenames>Marco</forenames></author><author><keyname>Sebastiani</keyname><forenames>Paola</forenames></author></authors><title>Learning Bayesian Networks from Incomplete Databases</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-401-408</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian approaches to learn the graphical structure of Bayesian Belief
Networks (BBNs) from databases share the assumption that the database is
complete, that is, no entry is reported as unknown. Attempts to relax this
assumption involve the use of expensive iterative methods to discriminate among
different structures. This paper introduces a deterministic method to learn the
graphical structure of a BBN from a possibly incomplete database. Experimental
evaluations show a significant robustness of this method and a remarkable
independence of its execution time from the number of missing data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1567</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1567</id><created>2013-02-06</created><authors><author><keyname>Shimony</keyname><forenames>Solomon Eyal</forenames></author><author><keyname>Domshlak</keyname><forenames>Carmel</forenames></author><author><keyname>Santos</keyname><forenames>Eugene</forenames><suffix>Jr</suffix></author></authors><title>Cost-Sharing in Bayesian Knowledge Bases</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-421-428</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian knowledge bases (BKBs) are a generalization of Bayes networks and
weighted proof graphs (WAODAGs), that allow cycles in the causal graph.
Reasoning in BKBs requires finding the most probable inferences consistent with
the evidence. The cost-sharing heuristic for finding least-cost explanations in
WAODAGs was presented and shown to be effective by Charniak and Husain.
However, the cycles in BKBs would make the definition of cost-sharing cyclic as
well, if applied directly to BKBs. By treating the defining equations of
cost-sharing as a system of equations, one can properly define an admissible
cost-sharing heuristic for BKBs. Empirical evaluation shows that cost-sharing
improves performance significantly when applied to BKBs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1568</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1568</id><created>2013-02-06</created><authors><author><keyname>Shoham</keyname><forenames>Yoav</forenames></author></authors><title>Conditional Utility, Utility Independence, and Utility Networks</title><categories>cs.GT cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-429-436</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new interpretation of two related notions - conditional
utility and utility independence. Unlike the traditional interpretation, the
new interpretation renders the notions the direct analogues of their
probabilistic counterparts. To capture these notions formally, we appeal to the
notion of utility distribution, introduced in previous paper. We show that
utility distributions, which have a structure that is identical to that of
probability distributions, can be viewed as a special case of an additive
multiattribute utility functions, and show how this special case permits us to
capture the novel senses of conditional utility and utility independence.
Finally, we present the notion of utility networks, which do for utilities what
Bayesian networks do for probabilities. Specifically, utility networks exploit
the new interpretation of conditional utility and utility independence to
compactly represent a utility distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1569</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1569</id><created>2013-02-06</created><authors><author><keyname>Teng</keyname><forenames>Choh Man</forenames></author></authors><title>Sequential Thresholds: Context Sensitive Default Extensions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-437-444</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Default logic encounters some conceptual difficulties in representing common
sense reasoning tasks. We argue that we should not try to formulate modular
default rules that are presumed to work in all or most circumstances. We need
to take into account the importance of the context which is continuously
evolving during the reasoning process. Sequential thresholding is a
quantitative counterpart of default logic which makes explicit the role context
plays in the construction of a non-monotonic extension. We present a semantic
characterization of generic non-monotonic reasoning, as well as the
instantiations pertaining to default logic and sequential thresholding. This
provides a link between the two mechanisms as well as a way to integrate the
two that can be beneficial to both.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1570</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1570</id><created>2013-02-06</created><authors><author><keyname>Tennenholtz</keyname><forenames>Moshe</forenames></author></authors><title>On Stable Multi-Agent Behavior in Face of Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-445-452</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A stable joint plan should guarantee the achievement of a designer's goal in
a multi-agent environment, while ensuring that deviations from the prescribed
plan would be detected. We present a computational framework where stable joint
plans can be studied, as well as several basic results about the
representation, verification and synthesis of stable joint plans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1571</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1571</id><created>2013-02-06</created><authors><author><keyname>Thiesson</keyname><forenames>Bo</forenames></author></authors><title>Score and Information for Recursive Exponential Models with Incomplete
  Data</title><categories>stat.ME cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-453-463</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recursive graphical models usually underlie the statistical modelling
concerning probabilistic expert systems based on Bayesian networks. This paper
defines a version of these models, denoted as recursive exponential models,
which have evolved by the desire to impose sophisticated domain knowledge onto
local fragments of a model. Besides the structural knowledge, as specified by a
given model, the statistical modelling may also include expert opinion about
the values of parameters in the model. It is shown how to translate imprecise
expert knowledge into approximately conjugate prior distributions. Based on
possibly incomplete data, the score and the observed information are derived
for these models. This accounts for both the traditional score and observed
information, derived as derivatives of the log-likelihood, and the posterior
score and observed information, derived as derivatives of the log-posterior
distribution. Throughout the paper the specialization into recursive graphical
models is accounted for by a simple example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1572</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1572</id><created>2013-02-06</created><authors><author><keyname>Thomas</keyname><forenames>Ian</forenames></author><author><keyname>Zukerman</keyname><forenames>Ingrid</forenames></author><author><keyname>Oliver</keyname><forenames>Jonathan</forenames></author><author><keyname>Albrecht</keyname><forenames>David</forenames></author><author><keyname>Raskutti</keyname><forenames>Bhavani</forenames></author></authors><title>Lexical Access for Speech Understanding using Minimum Message Length
  Encoding</title><categories>cs.CL</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-464-471</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Lexical Access Problem consists of determining the intended sequence of
words corresponding to an input sequence of phonemes (basic speech sounds) that
come from a low-level phoneme recognizer. In this paper we present an
information-theoretic approach based on the Minimum Message Length Criterion
for solving the Lexical Access Problem. We model sentences using phoneme
realizations seen in training, and word and part-of-speech information obtained
from text corpora. We show results on multiple-speaker, continuous, read speech
and discuss a heuristic using equivalence classes of similar sounding words
which speeds up the recognition process without significant deterioration in
recognition accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1573</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1573</id><created>2013-02-06</created><authors><author><keyname>Zhang</keyname><forenames>Nevin Lianwen</forenames></author><author><keyname>Liu</keyname><forenames>Wenju</forenames></author></authors><title>Region-Based Approximations for Planning in Stochastic Domains</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-472-480</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with planning in stochastic domains by means of
partially observable Markov decision processes (POMDPs). POMDPs are difficult
to solve. This paper identifies a subclass of POMDPs called region observable
POMDPs, which are easier to solve and can be used to approximate general POMDPs
to arbitrary accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1574</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1574</id><created>2013-02-06</created><authors><author><keyname>Zhang</keyname><forenames>Nevin Lianwen</forenames></author><author><keyname>Yan</keyname><forenames>Li</forenames></author></authors><title>Independence of Causal Influence and Clique Tree Propagation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-481-488</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the role of independence of causal influence (ICI) in
Bayesian network inference. ICI allows one to factorize a conditional
probability table into smaller pieces. We describe a method for exploiting the
factorization in clique tree propagation (CTP) - the state-of-the-art exact
inference algorithm for Bayesian networks. We also present empirical results
showing that the resulting algorithm is significantly more efficient than the
combination of CTP and previous techniques for exploiting ICI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1575</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1575</id><created>2013-02-06</created><authors><author><keyname>Zhang</keyname><forenames>Nevin Lianwen</forenames></author><author><keyname>Zhang</keyname><forenames>Weihong</forenames></author></authors><title>Fast Value Iteration for Goal-Directed Markov Decision Processes</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)</comments><proxy>auai</proxy><report-no>UAI-P-1997-PG-489-494</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planning problems where effects of actions are non-deterministic can be
modeled as Markov decision processes. Planning problems are usually
goal-directed. This paper proposes several techniques for exploiting the
goal-directedness to accelerate value iteration, a standard algorithm for
solving Markov decision processes. Empirical studies have shown that the
techniques can bring about significant speedups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1591</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1591</id><created>2013-02-06</created><authors><author><keyname>Lu</keyname><forenames>Charng-Da</forenames></author></authors><title>Automatically Mining Program Build Information via Signature Matching</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Program build information, such as compilers and libraries used, is vitally
important in an auditing and benchmarking framework for HPC systems. We have
developed a tool to automatically extract this information using
signature-based detection, a common strategy employed by anti-virus software to
search for known patterns of data within the program binaries. We formulate the
patterns from various &quot;features&quot; embedded in the program binaries, and the
experiment shows that our tool can successfully identify many different
compilers, libraries, and their versions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1592</identifier>
 <datestamp>2013-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1592</id><created>2013-02-06</created><updated>2013-09-27</updated><authors><author><keyname>Banani</keyname><forenames>S. Alireza</forenames></author><author><keyname>Adve</keyname><forenames>Raviraj S.</forenames></author></authors><title>Required Base Station Density in Coordinated Multi-Point Uplink with
  Rate Constraints</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to ignoring
  interference in the formulations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we obtain the required spatial density of base stations (BSs)
in a coordinated multi-point uplink cellular network to meet a chosen quality
of service metric. Our model assumes cooperation amongst two BSs and the
required density is obtained under shadowing and Rayleigh fading for different
LTE-A path loss models. The proposed approach guarantees that the worst-case
achievable rate in the entire coverage region is above a target rate with
chosen probability. Two models for the position of the BSs are considered: a
hexagonal grid and a Poisson point process (PPP) modified to set a minimum cell
size. First, for each cooperation region, the location with the minimum rate
coverage probability - the worst-case point - is determined. Next, accurate
closed-form approximations are obtained for the worst-case rate coverage
probability. The approximations presented are useful for the quick assessment
of network performance and can be utilized in parametric studies for network
design. Here, they are applied to obtain the required density of BSs to achieve
a target rate coverage probability. As an added benefit, the formulation here
quantifies the penalty in moving from a regular BS deployment (the grid model)
to a random BS deployment (the PPP model).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1596</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1596</id><created>2013-02-06</created><updated>2013-08-06</updated><authors><author><keyname>Y&#x131;lmaz</keyname><forenames>Onur</forenames></author></authors><title>Tag-based Semantic Website Recommendation for Turkish Language</title><categories>cs.IR</categories><comments>7 pages, research and experiment about recommendation system for
  Turkish</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the dramatic increase in the number of websites on the internet, tagging
has become popular for finding related, personal and important documents. When
the potentially increasing internet markets are analyzed, Turkey, in which most
of the people use Turkish language on the internet, found to be exponentially
increasing. In this paper, a tag-based website recommendation method is
presented, where similarity measures are combined with semantic relationships
of tags. In order to evaluate the system, an experiment with 25 people from
Turkey is undertaken and participants are firstly asked to provide websites and
tags in Turkish and then they are asked to evaluate recommended websites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1601</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1601</id><created>2013-02-06</created><updated>2013-06-15</updated><authors><author><keyname>Arbabjolfaei</keyname><forenames>Fatemeh</forenames></author><author><keyname>Bandemer</keyname><forenames>Bernd</forenames></author><author><keyname>Kim</keyname><forenames>Young-Han</forenames></author><author><keyname>Sasoglu</keyname><forenames>Eren</forenames></author><author><keyname>Wang</keyname><forenames>Lele</forenames></author></authors><title>On the Capacity Region for Index Coding</title><categories>cs.IT math.IT</categories><comments>5 pages, 6 figures, accepted to the 2013 IEEE International Symposium
  on Information Theory (ISIT), Istanbul, Turkey, July 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new inner bound on the capacity region of a general index coding problem is
established. Unlike most existing bounds that are based on graph theoretic or
algebraic tools, the bound is built on a random coding scheme and optimal
decoding, and has a simple polymatroidal single-letter expression. The utility
of the inner bound is demonstrated by examples that include the capacity region
for all index coding problems with up to five messages (there are 9846
nonisomorphic ones).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1610</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1610</id><created>2013-02-06</created><updated>2013-05-31</updated><authors><author><keyname>Yang</keyname><forenames>Fei</forenames></author><author><keyname>Jiang</keyname><forenames>Hong</forenames></author><author><keyname>Shen</keyname><forenames>Zuowei</forenames></author><author><keyname>Deng</keyname><forenames>Wei</forenames></author><author><keyname>Metaxas</keyname><forenames>Dimitris</forenames></author></authors><title>Adaptive low rank and sparse decomposition of video using compressive
  sensing</title><categories>cs.IT cs.CV math.IT</categories><comments>Accepted ICIP 2013</comments><journal-ref>IEEE International Conference on Image Processing, ICIP 2013,
  Paper #1870</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of reconstructing and analyzing surveillance videos
using compressive sensing. We develop a new method that performs video
reconstruction by low rank and sparse decomposition adaptively. Background
subtraction becomes part of the reconstruction. In our method, a background
model is used in which the background is learned adaptively as the compressive
measurements are processed. The adaptive method has low latency, and is more
robust than previous methods. We will present experimental results to
demonstrate the advantages of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1611</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1611</id><created>2013-02-06</created><updated>2013-02-12</updated><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Perchet</keyname><forenames>Vianney</forenames></author><author><keyname>Rigollet</keyname><forenames>Philippe</forenames></author></authors><title>Bounded regret in stochastic multi-armed bandits</title><categories>math.ST cs.LG stat.ML stat.TH</categories><msc-class>62L05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the stochastic multi-armed bandit problem when one knows the value
$\mu^{(\star)}$ of an optimal arm, as a well as a positive lower bound on the
smallest positive gap $\Delta$. We propose a new randomized policy that attains
a regret {\em uniformly bounded over time} in this setting. We also prove
several lower bounds, which show in particular that bounded regret is not
possible if one only knows $\Delta$, and bounded regret of order $1/\Delta$ is
not possible if one only knows $\mu^{(\star)}$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1612</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1612</id><created>2013-02-06</created><authors><author><keyname>Froud</keyname><forenames>Hanane</forenames></author><author><keyname>Lachkar</keyname><forenames>Abdelmonaime</forenames></author><author><keyname>Ouatik</keyname><forenames>Said Alaoui</forenames></author></authors><title>Arabic text summarization based on latent semantic analysis to enhance
  arabic documents clustering</title><categories>cs.IR cs.CL</categories><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP)- 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arabic Documents Clustering is an important task for obtaining good results
with the traditional Information Retrieval (IR) systems especially with the
rapid growth of the number of online documents present in Arabic language.
Documents clustering aim to automatically group similar documents in one
cluster using different similarity/distance measures. This task is often
affected by the documents length, useful information on the documents is often
accompanied by a large amount of noise, and therefore it is necessary to
eliminate this noise while keeping useful information to boost the performance
of Documents clustering. In this paper, we propose to evaluate the impact of
text summarization using the Latent Semantic Analysis Model on Arabic Documents
Clustering in order to solve problems cited above, using five
similarity/distance measures: Euclidean Distance, Cosine Similarity, Jaccard
Coefficient, Pearson Correlation Coefficient and Averaged Kullback-Leibler
Divergence, for two times: without and with stemming. Our experimental results
indicate that our proposed approach effectively solves the problems of noisy
information and documents length, and thus significantly improve the clustering
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1625</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1625</id><created>2013-02-06</created><authors><author><keyname>Kahrobaei</keyname><forenames>Delaram</forenames></author><author><keyname>Koupparis</keyname><forenames>Charalambos</forenames></author><author><keyname>Shpilrain</keyname><forenames>Vladimir</forenames></author></authors><title>Public Key Exchange Using Matrices Over Group Rings</title><categories>cs.CR math.GR</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We offer a public key exchange protocol in the spirit of Diffie-Hellman, but
we use (small) matrices over a group ring of a (small) symmetric group as the
platform. This &quot;nested structure&quot; of the platform makes computation very
efficient for legitimate parties. We discuss security of this scheme by
addressing the Decision Diffie-Hellman (DDH) and Computational Diffie-Hellman
(CDH) problems for our platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1626</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1626</id><created>2013-02-06</created><updated>2013-02-19</updated><authors><author><keyname>Chigira</keyname><forenames>Naoki</forenames></author><author><keyname>Harada</keyname><forenames>Masaaki</forenames></author><author><keyname>Kitazume</keyname><forenames>Masaaki</forenames></author></authors><title>On the Classification of Extremal Doubly Even Self-Dual Codes with
  2-Transitive Automorphism Group</title><categories>math.CO cs.IT math.GR math.IT</categories><comments>4 pages, minor revision, Designs, Codes and Cryptography (to appear)</comments><msc-class>94B05, 20B25</msc-class><journal-ref>Des. Codes Cryptogr. (2014) 73:33-35</journal-ref><doi>10.1007/s10623-013-9807-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we complete the classification of extremal doubly even
self-dual codes with 2-transitive automorphism groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1638</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1638</id><created>2013-02-06</created><authors><author><keyname>K.</keyname><forenames>Jnanamurthy H.</forenames></author></authors><title>Discovery of Maximal Frequent Item Sets using Subset Creation</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining is the practice to search large amount of data to discover data
patterns. Data mining uses mathematical algorithms to group the data and
evaluate the future events. Association rule is a research area in the field of
knowledge discovery. Many data mining researchers had improved upon the quality
of association rule for business development by incorporating influential
factors like utility, number of items sold and for the mining of association
data patterns. In this paper, we propose an efficient algorithm to find maximal
frequent itemset first. Most of the association rule algorithms used to find
minimal frequent item first, then with the help of minimal frequent itemsets
derive the maximal frequent itemsets, these methods consume more time to find
maximal frequent itemsets. To overcome this problem, we propose a new approach
to find maximal frequent itemset directly using the concepts of subsets. The
proposed method is found to be efficient in finding maximal frequent itemsets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1642</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1642</id><created>2013-02-06</created><authors><author><keyname>Mohammed</keyname><forenames>Hussein A.</forenames></author><author><keyname>Ali</keyname><forenames>Adnan Hussein</forenames></author><author><keyname>Mohammed</keyname><forenames>Hawraa Jassim</forenames></author></authors><title>The Affects of Different Queuing Algorithms within the Router on QoS
  VoIP application Using OPNET</title><categories>cs.NI</categories><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.5, No.1, January 2013International Journal of Computer Networks</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice over Internet Protocol is a service from the Internet services that
allows users to communicate with each other. Quality of Service is very
sensitive to delay so that Voice over Internet Protocol needs it. The objective
of this research is to study the effect of different queuing algorithms within
the router on Voice over Internet Protocol Quality of Service. In this work,
simulation tool OPNET Modeler is used to implement the proposed network (Voice
over Internet Protocol Network). The proposed network is a private network for
a company that has two locations located at two different countries around the
world in order to simulate the communications within the same location as a
local and the communications between two locations as a long distance and
analyze Voice over Internet Protocol Quality of Service through measuring the
major factors that affect the Quality of Service for Voice over Internet
Protocol according to international telecommunication union standards such as:
delay, jitter and packet loss. In this research, a comparison was carried out
between different queue algorithms like First in First out (FIFO), Priority
queue and Weight Fair Queuing and it was found that Priority queue and Weight
Fair Queuing algorithms are the most appropriate to improve Voice over Internet
Protocol Quality of Service .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1649</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1649</id><created>2013-02-07</created><authors><author><keyname>Anacan</keyname><forenames>Rommel</forenames></author><author><keyname>Alcayde</keyname><forenames>James Greggory</forenames></author><author><keyname>Antegra</keyname><forenames>Retchel</forenames></author><author><keyname>Luna</keyname><forenames>Leah</forenames></author></authors><title>Eye-GUIDE (Eye-Gaze User Interface Design) Messaging for
  Physically-Impaired People</title><categories>cs.HC cs.CV</categories><doi>10.5121/ijdps.2013.4104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eye-GUIDE is an assistive communication tool designed for the paralyzed or
physically impaired people who were unable to move parts of their bodies
especially people whose communications are limited only to eye movements. The
prototype consists of a camera and a computer. Camera captures images then it
will be send to the computer, where the computer will be the one to interpret
the data. Thus, Eye-GUIDE focuses on camera-based gaze tracking. The proponent
designed the prototype to perform simple tasks and provides graphical user
interface in order the paralyzed or physically impaired person can easily use
it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1663</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1663</id><created>2013-02-07</created><updated>2013-02-11</updated><authors><author><keyname>Kim</keyname><forenames>Joongheon</forenames></author><author><keyname>Tian</keyname><forenames>Yafei</forenames></author><author><keyname>Mangold</keyname><forenames>Stefan</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author></authors><title>Joint Scalable Coding and Routing for 60 GHz Real-Time Live HD Video
  Streaming Applications</title><categories>cs.NI</categories><comments>This document will be submitted again after getting co-authors' more
  feedback</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmission of high-definition (HD) video is a promising application for 60
GHz wireless links, since very high transmission rates (up to several Gbit/s)
are possible. In particular we consider a sports stadium broadcasting system
where signals from multiple cameras are transmitted to a central location. Due
to the high pathloss of 60 GHz radiation over the large distances encountered
in this scenario, the use of relays might be required. The current paper
analyzes the joint selection of the routes (relays) and the compression rates
from the various sources for maximization of the overall video quality. We
consider three different scenarios: (i) each source transmits only to one relay
and the relay can receive only one data stream, and (ii) each source can
transmit only to a single relay, but relays can aggregate streams from
different sources and forward to the destination, and (iii) the source can
split its data stream into parallel streams, which can be transmitted via
different relays to the destination. For each scenario, we derive the
mathematical formulations of the optimization problem and re-formulate them as
convex mixed-integer programming, which can guarantee optimal solutions.
Extensive simulations demonstrate that high-quality transmission is possible
for at least ten cameras over distances of 300 m. Furthermore, optimization of
the video quality gives results that can significantly outperform algorithms
that maximize data rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1667</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1667</id><created>2013-02-07</created><updated>2013-02-10</updated><authors><author><keyname>Zerkouk</keyname><forenames>Meriem</forenames></author><author><keyname>Mhamed</keyname><forenames>Abdallah</forenames></author><author><keyname>Messabih</keyname><forenames>Belhadri</forenames></author></authors><title>A user profile based access control model and architecture</title><categories>cs.CR cs.CY</categories><comments>10 pages, 3 figures</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.5, No.1, January 2013</journal-ref><doi>10.5121/ijcnc.2013.5112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personalization and adaptation to the user profile capability are the hottest
issues to ensure ambient assisted living and context awareness in nowadays
environments. With the growing healthcare and wellbeing context aware
applications, modeling security policies becomes an important issue in the
design of future access control models. This requires rich semantics using
ontology modeling for the management of services provided to dependant people.
However, current access control models remain unsuitable due to lack of
personalization, adaptability and smartness to the handicap situation. In this
paper, we propose a novel adaptable access control model and its related
architecture in which the security policy is based on the handicap situation
analyzed from the monitoring of userss behavior in order to grant a service
using any assistive device within intelligent environment. the design of our
model is an ontology-learning and evolving security policy for predicting the
future actions of dependent people. This is reached by reasoning about
historical data, contextual data and user behavior according to the access
rules that are used in the inference engine to provide the right service
according to the users needs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1669</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1669</id><created>2013-02-07</created><authors><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Naroditskiy</keyname><forenames>Victor</forenames></author><author><keyname>Narodytska</keyname><forenames>Nina</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Possible and Necessary Winner Problem in Social Polls</title><categories>cs.GT cs.AI cs.DS cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks are increasingly being used to conduct polls. We introduce a
simple model of such social polling. We suppose agents vote sequentially, but
the order in which agents choose to vote is not necessarily fixed. We also
suppose that an agent's vote is influenced by the votes of their friends who
have already voted. Despite its simplicity, this model provides useful insights
into a number of areas including social polling, sequential voting, and
manipulation. We prove that the number of candidates and the network structure
affect the computational complexity of computing which candidate necessarily or
possibly can win in such a social poll. For social networks with bounded
treewidth and a bounded number of candidates, we provide polynomial algorithms
for both problems. In other cases, we prove that computing which candidates
necessarily or possibly win are computationally intractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1676</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1676</id><created>2013-02-07</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Jain</keyname><forenames>Satbir</forenames></author></authors><title>Comparison of Proposed Data Dissemination Protocols for Sensor Networks
  Using J-Sim</title><categories>cs.NI</categories><comments>8 pages, 9 figures</comments><doi>10.1109/IADCC.2009.4809182</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distinguishing characteristic of wireless sensor networks is the
opportunity to exploit characteristics of the application at lower layers. This
paper reports on the results of a simulation comparison of proposed data
dissemination protocols using the J-Sim simulator for the WSN protocols:
Forwarding Diffusion Data Dissemination(FDDDP), Decentralized Data
Dissemination(DDDP), Credit Broadcast Data Dissemination (CBDDP), Energy Aware
&amp; Geographical Data Dissemination (EAGDDP) .Our performance provides useful
insights for the network designer such as which protocols (and design choices)
scale control traffic well, improve data delivery or reduce overall energy
consumption,improves routing overhead and maximizes the bandwidth utilization.
The static pre configuration of the cell size in DDDP, is one of the reasons
why DDDP exhibits larger routing overhead than FDDDP by 74.2% on average.
Although CBDDP produces approximately 94.6% smaller overhead than DDDP and
90.7% smaller than FDDDP, because of statically configured amount credit CBDDP
delivers on average 7.5 times more of the redundant data packets than DDDP and
FDDDP.EAGDDP improves the delivery by 80% on average and makes a balance of
energy consumption .We suggest that making these protocols truly self-learning
can significantly improve their performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1690</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1690</id><created>2013-02-07</created><authors><author><keyname>Masci</keyname><forenames>Jonathan</forenames></author><author><keyname>Giusti</keyname><forenames>Alessandro</forenames></author><author><keyname>Cire&#x15f;an</keyname><forenames>Dan</forenames></author><author><keyname>Fricout</keyname><forenames>Gabriel</forenames></author><author><keyname>Schmidhuber</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>A Fast Learning Algorithm for Image Segmentation with Max-Pooling
  Convolutional Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fast algorithm for training MaxPooling Convolutional Networks to
segment images. This type of network yields record-breaking performance in a
variety of tasks, but is normally trained on a computationally expensive
patch-by-patch basis. Our new method processes each training image in a single
pass, which is vastly more efficient.
  We validate the approach in different scenarios and report a 1500-fold
speed-up. In an application to automated steel defect detection and
segmentation, we obtain excellent performance with short training times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1697</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1697</id><created>2013-02-07</created><authors><author><keyname>Sanabria-Russo</keyname><forenames>Luis</forenames></author><author><keyname>Barcelo</keyname><forenames>Jaume</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author></authors><title>Fairness in Collision-Free WLANs</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CSMA/ECA is a contention protocol that makes it possible to construct a
collision-free schedule by using a deterministic backoff after successful
transmissions. In this paper, we further enhance the CSMA/ECA protocol with two
properties that allows to fairly accommodate a large number of contenders in a
collision-free schedule. The first property, called hysteresis, instructs the
contenders not to reset their contention window after successful transmissions.
Thanks to hysteresis, the protocol sustains a high throughput regardless of the
number of contenders. The second property, called fair-share, preserves
fairness when different nodes use different contention windows. We present
simulations results that evidence how these properties account for performance
gains that go even further beyond CSMA/CA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1700</identifier>
 <datestamp>2013-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1700</id><created>2013-02-07</created><authors><author><keyname>Giusti</keyname><forenames>Alessandro</forenames></author><author><keyname>Cire&#x15f;an</keyname><forenames>Dan C.</forenames></author><author><keyname>Masci</keyname><forenames>Jonathan</forenames></author><author><keyname>Gambardella</keyname><forenames>Luca M.</forenames></author><author><keyname>Schmidhuber</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks</title><categories>cs.CV cs.AI</categories><comments>11 pages, 2 figures, 3 tables, 21 references, submitted to ICIP 2013</comments><report-no>IDSIA-01-13</report-no><journal-ref>International Conference on Image Processing (ICIP) 2013,
  Melbourne</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Neural Networks now excel at image classification, detection and
segmentation. When used to scan images by means of a sliding window, however,
their high computational complexity can bring even the most powerful hardware
to its knees. We show how dynamic programming can speedup the process by orders
of magnitude, even when max-pooling layers are present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1726</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1726</id><created>2013-02-07</created><updated>2013-05-16</updated><authors><author><keyname>O'Callaghan</keyname><forenames>Derek</forenames></author><author><keyname>Greene</keyname><forenames>Derek</forenames></author><author><keyname>Conway</keyname><forenames>Maura</forenames></author><author><keyname>Carthy</keyname><forenames>Joe</forenames></author><author><keyname>Cunningham</keyname><forenames>P&#xe1;draig</forenames></author></authors><title>Uncovering the Wider Structure of Extreme Right Communities Spanning
  Popular Online Networks</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 11 figures. Due to use of &quot;sigchi&quot; template, minor changes
  were made to ensure 10 page limit was not exceeded. Minor clarifications in
  Introduction, Data and Methodology sections</comments><acm-class>J.4; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen increased interest in the online presence of extreme
right groups. Although originally composed of dedicated websites, the online
extreme right milieu now spans multiple networks, including popular social
media platforms such as Twitter, Facebook and YouTube. Ideally therefore, any
contemporary analysis of online extreme right activity requires the
consideration of multiple data sources, rather than being restricted to a
single platform. We investigate the potential for Twitter to act as a gateway
to communities within the wider online network of the extreme right, given its
facility for the dissemination of content. A strategy for representing
heterogeneous network data with a single homogeneous network for the purpose of
community detection is presented, where these inherently dynamic communities
are tracked over time. We use this strategy to discover and analyze persistent
English and German language extreme right communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1727</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1727</id><created>2013-02-07</created><authors><author><keyname>Nasution</keyname><forenames>Mahyuddin K. M.</forenames></author><author><keyname>Elfida</keyname><forenames>Maria</forenames></author></authors><title>Terrorist Network: Towards An Analysis</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages, 1 figure, Seminar International/Nasional Matematika dan
  Terapan (SiManTap2011), Medan, 28-29 November 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Terrorist network is a paradigms to understand the terrorism. The terrorist
involves a lot of people, and among them are involved as perpetrators, but on
the contrary it is very difficult to know who they are caused by lack of
information. Network structure is used to reveal other things about the
terrorist beyond the ability of social sciences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1733</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1733</id><created>2013-02-07</created><authors><author><keyname>Gonz&#xe1;lez</keyname><forenames>Fernando</forenames></author><author><keyname>Belanche</keyname><forenames>Llu&#xed;s A.</forenames></author></authors><title>Feature Selection for Microarray Gene Expression Data using Simulated
  Annealing guided by the Multivariate Joint Entropy</title><categories>q-bio.QM cs.CE cs.LG stat.ML</categories><comments>12 pages, 6 Tables, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work a new way to calculate the multivariate joint entropy is
presented. This measure is the basis for a fast information-theoretic based
evaluation of gene relevance in a Microarray Gene Expression data context. Its
low complexity is based on the reuse of previous computations to calculate
current feature relevance. The mu-TAFS algorithm --named as such to
differentiate it from previous TAFS algorithms-- implements a simulated
annealing technique specially designed for feature subset selection. The
algorithm is applied to the maximization of gene subset relevance in several
public-domain microarray data sets. The experimental results show a notoriously
high classification performance and low size subsets formed by biologically
meaningful genes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1737</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1737</id><created>2013-02-07</created><authors><author><keyname>Pous</keyname><forenames>Damien</forenames><affiliation>LIP</affiliation></author></authors><title>Kleene Algebra with Tests and Coq Tools for While Programs</title><categories>cs.LO cs.MS cs.PL</categories><comments>16+3 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Coq library about Kleene algebra with tests, including a proof
of their completeness over the appropriate notion of languages, a decision
procedure for their equational theory, and tools for exploiting hypotheses of a
particular shape in such a theory. Kleene algebra with tests make it possible
to represent if-then-else statements and while loops in most imperative
programming languages. They were actually introduced by Kozen as an alternative
to propositional Hoare logic. We show how to exploit the corresponding Coq
tools in the context of program verification by proving equivalences of while
programs, correctness of some standard compiler optimisations, Hoare rules for
partial correctness, and a particularly challenging equivalence of flowchart
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1741</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1741</id><created>2013-02-07</created><updated>2013-04-29</updated><authors><author><keyname>Laarhoven</keyname><forenames>Thijs</forenames></author><author><keyname>de Weger</keyname><forenames>Benne</forenames></author></authors><title>Discrete Distributions in the Tardos Scheme, Revisited</title><categories>cs.CR</categories><comments>5 pages, 2 figures</comments><acm-class>E.4; G.1.4</acm-class><journal-ref>ACM Workshop on Information Hiding and Multimedia Security
  (IH&amp;MMSec), pp. 13-18, 2013</journal-ref><doi>10.1145/2482513.2482533</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Tardos scheme is a well-known traitor tracing scheme to protect
copyrighted content against collusion attacks. The original scheme contained
some suboptimal design choices, such as the score function and the distribution
function used for generating the biases. Skoric et al. previously showed that a
symbol-symmetric score function leads to shorter codes, while Nuida et al.
obtained the optimal distribution functions for arbitrary coalition sizes.
Later, Nuida et al. showed that combining these results leads to even shorter
codes when the coalition size is small. We extend their analysis to the case of
large coalitions and prove that these optimal distributions converge to the
arcsine distribution, thus showing that the arcsine distribution is
asymptotically optimal in the symmetric Tardos scheme. We also present a new,
practical alternative to the discrete distributions of Nuida et al. and give a
comparison of the estimated lengths of the fingerprinting codes for each of
these distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1747</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1747</id><created>2013-02-07</created><authors><author><keyname>Fisher</keyname><forenames>Nathan</forenames></author><author><keyname>Goossens</keyname><forenames>Jo&#xeb;l</forenames></author><author><keyname>Hettiarachchi</keyname><forenames>Pradeep M.</forenames></author><author><keyname>Paolillo</keyname><forenames>Antonio</forenames></author></authors><title>Energy Minimization for Parallel Real-Time Systems with Malleable Jobs
  and Homogeneous Frequencies</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we investigate the potential utility of parallelization for
meeting real-time constraints and minimizing energy. We consider malleable Gang
scheduling of implicit-deadline sporadic tasks upon multiprocessors. We first
show the non-necessity of dynamic voltage/frequency regarding optimality of our
scheduling problem. We adapt the canonical schedule for DVFS multiprocessor
platforms and propose a polynomial-time optimal processor/frequency-selection
algorithm. We evaluate the performance of our algorithm via simulations using
parameters obtained from a hardware testbed implementation. Our algorithm has
up to a 60 watt decrease in power consumption over the optimal non-parallel
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1756</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1756</id><created>2013-02-07</created><authors><author><keyname>Kavoukis</keyname><forenames>Anastasios</forenames></author><author><keyname>Aljareh</keyname><forenames>Salem</forenames></author></authors><title>Efficient time synchronized one-time password scheme to provide secure
  wake-up authentication on wireless sensor networks</title><categories>cs.NI</categories><comments>International Journal Of Advanced Smart Sensor Network Systems
  (IJASSN), Vol 3, No.1, January 2013
  http://airccse.org/journal/ijassn/papers/3113ijassn01.pdf</comments><doi>10.5121/ijassn.2013.3101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose Time Synchronized One-Time-Password scheme to
provide secure wake up authentication. The main constraint of wireless sensor
networks is their limited power resource that prevents us from using radio
transmission over the network to transfer the passwords. On the other hand
computation power consumption is insignificant when compared to the costs
associated with the power needed for transmitting the right set of keys. In
addition to prevent adversaries from reading and following the timeline of the
network, we propose to encrypt the tokens using symmetric encryption to prevent
replay attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1772</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1772</id><created>2013-02-07</created><authors><author><keyname>Majidnezhad</keyname><forenames>Vahid</forenames></author><author><keyname>Kheidorov</keyname><forenames>Igor</forenames></author></authors><title>An ANN-based Method for Detecting Vocal Fold Pathology</title><categories>cs.LG cs.CV cs.SD</categories><comments>4 pages, 3 figures, Published with International Journal of Computer
  Applications (IJCA)</comments><journal-ref>International Journal of Computer Applications 62(7):1-4, January
  2013</journal-ref><doi>10.5120/10089-4722</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are different algorithms for vocal fold pathology diagnosis. These
algorithms usually have three stages which are Feature Extraction, Feature
Reduction and Classification. While the third stage implies a choice of a
variety of machine learning methods, the first and second stages play a
critical role in performance and accuracy of the classification system. In this
paper we present initial study of feature extraction and feature reduction in
the task of vocal fold pathology diagnosis. A new type of feature vector, based
on wavelet packet decomposition and Mel-Frequency-Cepstral-Coefficients
(MFCCs), is proposed. Also Principal Component Analysis (PCA) is used for
feature reduction. An Artificial Neural Network is used as a classifier for
evaluating the performance of our proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1777</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1777</id><created>2013-02-07</created><updated>2013-03-04</updated><authors><author><keyname>Sun</keyname><forenames>Hongjian</forenames></author><author><keyname>Nallanathan</keyname><forenames>Arumugam</forenames></author><author><keyname>Wang</keyname><forenames>Cheng-Xiang</forenames></author><author><keyname>Chen</keyname><forenames>Yunfei</forenames></author></authors><title>Wideband Spectrum Sensing for Cognitive Radio Networks: A Survey</title><categories>cs.IT math.IT</categories><comments>17 pages, 4 figures, 2 tables. This paper has been accepted to be
  published in IEEE Wireless Communications, to appear April 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radio has emerged as one of the most promising candidate solutions
to improve spectrum utilization in next generation cellular networks. A crucial
requirement for future cognitive radio networks is wideband spectrum sensing:
secondary users reliably detect spectral opportunities across a wide frequency
range. In this article, various wideband spectrum sensing algorithms are
presented, together with a discussion of the pros and cons of each algorithm
and the challenging issues. Special attention is paid to the use of sub-Nyquist
techniques, including compressive sensing and multi-channel sub-Nyquist
sampling techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1789</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1789</id><created>2013-02-07</created><authors><author><keyname>Huang</keyname><forenames>Gang</forenames></author><author><keyname>Jiang</keyname><forenames>Hong</forenames></author><author><keyname>Matthews</keyname><forenames>Kim</forenames></author><author><keyname>Wilford</keyname><forenames>Paul</forenames></author></authors><title>Lensless Compressive Sensing Imaging</title><categories>cs.CV cs.IT math.IT</categories><comments>12 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a lensless compressive sensing imaging
architecture. The architecture consists of two components, an aperture assembly
and a sensor. No lens is used. The aperture assembly consists of a two
dimensional array of aperture elements. The transmittance of each aperture
element is independently controllable. The sensor is a single detection
element, such as a single photo-conductive cell. Each aperture element together
with the sensor defines a cone of a bundle of rays, and the cones of the
aperture assembly define the pixels of an image. Each pixel value of an image
is the integration of the bundle of rays in a cone. The sensor is used for
taking compressive measurements. Each measurement is the integration of rays in
the cones modulated by the transmittance of the aperture elements. A
compressive sensing matrix is implemented by adjusting the transmittance of the
individual aperture elements according to the values of the sensing matrix. The
proposed architecture is simple and reliable because no lens is used.
Furthermore, the sharpness of an image from our device is only limited by the
resolution of the aperture assembly, but not affected by blurring due to
defocus. The architecture can be used for capturing images of visible lights,
and other spectra such as infrared, or millimeter waves. Such devices may be
used in surveillance applications for detecting anomalies or extracting
features such as speed of moving objects. Multiple sensors may be used with a
single aperture assembly to capture multi-view images simultaneously. A
prototype was built by using a LCD panel and a photoelectric sensor for
capturing images of visible spectrum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1823</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1823</id><created>2013-02-07</created><authors><author><keyname>Chitikela</keyname><forenames>Sindhu</forenames></author></authors><title>Intensity and State Estimation in Quantum Cryptography</title><categories>quant-ph cs.CR</categories><comments>11 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes how the communicating parties can employ intensity and
state estimation to detect if the eavesdropper has siphoned off and injected
photons in the received communication. This is of relevance in quantum
cryptography based on random rotations of photon polarizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1836</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1836</id><created>2013-02-07</created><updated>2013-02-14</updated><authors><author><keyname>Farsani</keyname><forenames>Reza K.</forenames></author></authors><title>The Capacity Region of the Wireless Ergodic Fading Interference Channel
  with Partial CSIT to Within One Bit</title><categories>cs.IT math.IT</categories><comments>21 pages. A shorter version was also submitted for conference
  publication. In this second version, only the reference [29] was edited</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fundamental capacity limits are studied for the two-user wireless ergodic
fading IC with partial Channel State Information at the Transmitters (CSIT)
where each transmitter is equipped with an arbitrary deterministic function of
the channel state (this model yields a full control over how much state
information is available). One of the main challenges in the analysis of fading
networks, specifically multi-receiver networks including fading ICs, is to
obtain efficient capacity outer bounds. In this paper, a novel capacity outer
bound is established for the two-user ergodic fading IC. For this purpose, by a
subtle combination of broadcast channel techniques (i.e., manipulating mutual
information functions composed of vector random variables by Csiszar-Korner
identity) and genie-aided techniques, first a single-letter outer bound
characterized by mutual information functions including some auxiliary random
variables is derived. Then, by novel arguments the derived bound is optimized
over its auxiliaries only using the entropy power inequality. Besides being
well-described, our outer bound is efficient from several aspects.
Specifically, it is optimal for the fading IC with uniformly strong
interference. Also, it is sum-rate optimal for the channel with uniformly mixed
interference. More importantly, it is proved that when each transmitter has
access to any amount of CSIT that includes the interference to noise ratio of
its non-corresponding receiver, the outer bound differs by no more than one bit
from the achievable rate region given by Han-Kobayashi scheme. This result is
viewed as a natural generalization of the ETW to within one bit capacity result
for the static channel to the wireless ergodic fading case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1837</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1837</id><created>2013-02-07</created><authors><author><keyname>Farsani</keyname><forenames>Reza K.</forenames></author></authors><title>On the Capacity Region of the Two-User Interference Channel</title><categories>cs.IT math.IT</categories><comments>12 pages. In this paper a noisy interference regime is identified for
  any two-user interference channel (potentially non-Gaussian). For conference
  publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the key open problems in network information theory is to obtain the
capacity region for the two-user Interference Channel (IC). In this paper, new
results are derived for this channel. As a first result, a noisy interference
regime is given for the general IC where the sum-rate capacity is achieved by
treating interference as noise at the receivers. To obtain this result, a
single-letter outer bound in terms of some auxiliary random variables is first
established for the sum-rate capacity of the general IC and then those
conditions under which this outer bound is reduced to the achievable sum-rate
given by the simple treating interference as noise strategy are specified. The
main benefit of this approach is that it is applicable for any two-user IC
(potentially non-Gaussian). For the special case of Gaussian channel, our
result is reduced to the noisy interference regime that was previously
obtained. Next, some results are given on the Han-Kobayashi (HK) achievable
rate region. The evaluation of this rate region is in general difficult. In
this paper, a simple characterization of the HK rate region is derived for some
special cases, specifically, for a novel very weak interference regime. As a
remarkable characteristic, it is shown that for this very weak interference
regime, the achievable sum-rate due to the HK region is identical to the one
given by the simple treating interference as noise strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1842</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1842</id><created>2013-02-07</created><authors><author><keyname>Sun</keyname><forenames>Hongjian</forenames></author><author><keyname>Chiu</keyname><forenames>Wei-Yu</forenames></author><author><keyname>Nallanathan</keyname><forenames>A.</forenames></author></authors><title>Adaptive Compressive Spectrum Sensing for Wideband Cognitive Radios</title><categories>cs.IT math.IT</categories><comments>11 pages, 4 figures. This paper has been accepted to be published in
  IEEE Communications Letters. The associate editor coordinating the review of
  this letter and approving it for publication was O. Dobre</comments><journal-ref>IEEE Communications Letters, Nov 2012, vol. 16(11), 1812-1815</journal-ref><doi>10.1109/LCOMM.2012.092812.121648</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter presents an adaptive spectrum sensing algorithm that detects
wideband spectrum using sub-Nyquist sampling rates. By taking advantage of
compressed sensing (CS), the proposed algorithm reconstructs the wideband
spectrum from compressed samples. Furthermore, an l2 norm validation approach
is proposed that enables cognitive radios (CRs) to automatically terminate the
signal acquisition once the current spectral recovery is satisfactory, leading
to enhanced CR throughput. Numerical results show that the proposed algorithm
can not only shorten the spectrum sensing interval, but also improve the
throughput of wideband CRs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1845</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1845</id><created>2013-02-07</created><authors><author><keyname>Kovalev</keyname><forenames>Alexey A.</forenames></author><author><keyname>Dumer</keyname><forenames>Ilya</forenames></author><author><keyname>Pryadko</keyname><forenames>Leonid P.</forenames></author></authors><title>Linked-Cluster Technique for Finding the Distance of a Quantum LDPC Code</title><categories>quant-ph cs.IT math.IT</categories><comments>5.5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a linked-cluster technique for calculating the distance of a
quantum LDPC code. It offers an advantage over existing deterministic
techniques for codes with small relative distances (which includes all known
families of quantum LDPC codes), and over the probabilistic technique for codes
with sufficiently high rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1847</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1847</id><created>2013-02-07</created><authors><author><keyname>Sun</keyname><forenames>Hongjian</forenames></author><author><keyname>Chiu</keyname><forenames>Wei-Yu</forenames></author><author><keyname>Jiang</keyname><forenames>Jing</forenames></author><author><keyname>Nallanathan</keyname><forenames>Arumugam</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Wideband Spectrum Sensing with Sub-Nyquist Sampling in Cognitive Radios</title><categories>cs.IT math.IT</categories><comments>14 pages, 4 figures and 1 table. This manuscript has been accepted to
  be published in IEEE Transactions on Signal Processing</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 60(11), 2012, pp.
  6068-6073</journal-ref><doi>10.1109/TSP.2012.2212892</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-rate asynchronous sub-Nyquist sampling (MASS) is proposed for wideband
spectrum sensing. Corresponding spectral recovery conditions are derived and
the probability of successful recovery is given. Compared to previous
approaches, MASS offers lower sampling rate, and is an attractive approach for
cognitive radio networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1848</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1848</id><created>2013-02-07</created><updated>2013-02-20</updated><authors><author><keyname>Lopez-Cozar</keyname><forenames>Emilio Delgado</forenames></author><author><keyname>Sanchez</keyname><forenames>Manuel Ramirez</forenames></author></authors><title>H Index of History journals published in Spain according to Google
  Scholar Metrics (2007-2011)</title><categories>cs.DL</categories><comments>7 pages, 2 tables</comments><report-no>EC3 Working Papers 10</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Google Scholar Metrics (GSM), which was recently launched in April 2012,
features new bibliometric systems for gauging scientific journals by counting
the number of citations obtained in Google Scholar. This way, it opens new
possibilities for measuring journal impacts in the field of Humanities. The
present article intends to evaluate the scope of this tool through analysing
GSM searches, from the 5th through 6th of December 2012, of History journals
published in Spain. In sum, 69 journals were identified, accounting for only
24% of the History journals published in Spain. The ranges of H index values
for this field are so small that the ranking can no longer be said to show a
discriminating potential. In the light of this, we would like to propose a
change in the way Google Scholar Metrics is designed so that it could also
accommodate production and citation patterns in the particular field of
History, and, in a broader scope, in the area of Humanities as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1857</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1857</id><created>2013-02-07</created><authors><author><keyname>Sun</keyname><forenames>Hongjian</forenames></author><author><keyname>Tan</keyname><forenames>Bo</forenames></author><author><keyname>Jiang</keyname><forenames>Jing</forenames></author><author><keyname>Thompson</keyname><forenames>John S.</forenames></author><author><keyname>Nallanathan</keyname><forenames>Arumugam</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Relaying Technologies for Smart Grid Communications</title><categories>cs.IT cs.NI math.IT</categories><comments>17 pages, 6 figures. This paper has been accepted to be published in
  IEEE Wireless Communications</comments><journal-ref>IEEE Wireless Communications, Dec 2012, 52-59</journal-ref><doi>10.1109/MWC.2012.6393518</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless technologies can support a broad range of smart grid applications
including advanced metering infrastructure (AMI) and demand response (DR).
However, there are many formidable challenges when wireless technologies are
applied to the smart gird, e.g., the tradeoffs between wireless coverage and
capacity, the high reliability requirement for communication, and limited
spectral resources. Relaying has emerged as one of the most promising candidate
solutions for addressing these issues. In this article, an introduction to
various relaying strategies is presented, together with a discussion of how to
improve spectral efficiency and coverage in relay-based information and
communications technology (ICT) infrastructure for smart grid applications.
Special attention is paid to the use of unidirectional relaying, collaborative
beamforming, and bidirectional relaying strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1859</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1859</id><created>2013-02-05</created><updated>2013-06-10</updated><authors><author><keyname>Peternell</keyname><forenames>Martin</forenames></author><author><keyname>Gotthart</keyname><forenames>Lukas</forenames></author><author><keyname>Sendra</keyname><forenames>J. Rafael</forenames></author><author><keyname>Sendra</keyname><forenames>Juana</forenames></author></authors><title>The Relation Between Offset and Conchoid Constructions</title><categories>math.AG cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The one-sided offset surface Fd of a given surface F is, roughly speaking,
obtained by shifting the tangent planes of F in direction of its oriented
normal vector. The conchoid surface Gd of a given surface G is roughly speaking
obtained by increasing the distance of G to a fixed reference point O by d.
Whereas the offset operation is well known and implemented in most CAD-software
systems, the conchoid operation is less known, although already mentioned by
the ancient Greeks, and recently studied by some authors. These two operations
are algebraic and create new objects from given input objects. There is a
surprisingly simple relation between the o?set and the conchoid operation. As
derived there exists a rational bijective quadratic map which transforms a
given surface F and its offset surfaces Fd to a surface G and its conchoidal
surface Gd, and vice versa. Geometric properties of this map are studied and
illustrated at hand of some complete examples. Furthermore rational universal
parameterizations for offsets and conchoid surfaces are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1882</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1882</id><created>2013-02-07</created><authors><author><keyname>Vidakovic</keyname><forenames>Dragan</forenames></author><author><keyname>Nikolic</keyname><forenames>Olivera</forenames></author><author><keyname>Parezanovic</keyname><forenames>Dusko</forenames></author></authors><title>Acceleration detection of large (probably) prime numbers</title><categories>cs.CR</categories><comments>8 pages, 6 figures</comments><msc-class>D.4.6</msc-class><acm-class>D.3.2</acm-class><journal-ref>International Journal of UbiComp (IJU), Vol.4, No.1, January 2013</journal-ref><doi>10.5121/iju.2013.4101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to avoid unnecessary applications of Miller-Rabin algorithm to the
number in question, we resort to trial division by a few initial prime numbers,
since such a division take less time.
  How far we should go with such a division is the that we are trying to answer
in this paper?For the theory of the matter is fully resolved. However, that in
practice we do not have much use.
  Therefore, we present a solution that is probably irrelevant to theorists,
but it is very useful to people who have spent many nights to produce large
(probably) prime numbers using its own software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1886</identifier>
 <datestamp>2013-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1886</id><created>2013-02-07</created><authors><author><keyname>Silverberg</keyname><forenames>Jesse L.</forenames></author><author><keyname>Bierbaum</keyname><forenames>Matthew</forenames></author><author><keyname>Sethna</keyname><forenames>James P.</forenames></author><author><keyname>Cohen</keyname><forenames>Itai</forenames></author></authors><title>Collective Motion of Moshers at Heavy Metal Concerts</title><categories>physics.soc-ph cs.SI physics.bio-ph</categories><comments>4 pages, 2 figures</comments><doi>10.1103/PhysRevLett.110.228701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human collective behavior can vary from calm to panicked depending on social
context. Using videos publicly available online, we study the highly energized
collective motion of attendees at heavy metal concerts. We find these extreme
social gatherings generate similarly extreme behaviors: a disordered gas-like
state called a mosh pit and an ordered vortex-like state called a circle pit.
Both phenomena are reproduced in flocking simulations demonstrating that human
collective behavior is consistent with the predictions of simplified models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1901</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1901</id><created>2013-02-07</created><authors><author><keyname>Davies</keyname><forenames>Todd</forenames></author><author><keyname>Mintz</keyname><forenames>Mike D.</forenames></author></authors><title>Relational Access Control with Bivalent Permissions in a Social
  Web/Collaboration Architecture</title><categories>cs.SI cs.CR</categories><comments>in Waleed W. Smari and William MacQuay (Editors), Proceedings of the
  2010 International Symposium on Collaborative Technologies and Systems (CTS
  2010), IEEE, May 2010, pp. 57-66; 10 pages, 2 figures, 3 tables</comments><acm-class>K.4.3; D.4.6</acm-class><doi>10.1109/CTS.2010.5478523</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an access control model that has been implemented in the web
content management framework &quot;Deme&quot; (which rhymes with &quot;team&quot;). Access control
in Deme is an example of what we call &quot;bivalent relation object access
control&quot;(BROAC). This model builds on recent work by Giunchiglia et al. on
relation-based access control (RelBAC), as well as other work on relational,
flexible, fine-grained, and XML access control models. We describe Deme's
architecture and review access control models, motivating our approach. BROAC
allows for both positive and negative permissions, which may conflict with each
other. We argue for the usefulness of defining access control rules as objects
in the target database, and for the necessity of resolving permission conflicts
in a social Web/collaboration architecture. After describing how Deme access
control works, including the precedence relations between different permission
types in Deme, we provide several examples of realistic scenarios in which
permission conflicts arise, and show how Deme resolves them. Initial
performance tests indicate that permission checking scales linearly in time on
a practical Deme website.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1902</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1902</id><created>2013-02-07</created><authors><author><keyname>Jiang</keyname><forenames>Jing</forenames></author><author><keyname>Thompson</keyname><forenames>John S.</forenames></author><author><keyname>Sun</keyname><forenames>Hongjian</forenames></author><author><keyname>Grant</keyname><forenames>Peter M.</forenames></author></authors><title>Practical Analysis of Codebook Design and Frequency Offset Estimation
  for Virtual-MIMO Systems</title><categories>cs.IT math.IT</categories><comments>27 pages, 8 figures, 1 table. This paper has been submitted to IET
  Communications and accepted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A virtual multiple-input multiple-output (MIMO) wireless system using the
receiver-side cooperation with the compress-and-forward (CF) protocol, is an
alternative to a point-to-point MIMO system, when a single receiver is not
equipped with multiple antennas. It is evident that the practicality of CF
cooperation will be greatly enhanced if an efficient source coding technique
can be used at the relay. It is even more desirable that CF cooperation should
not be unduly sensitive to carrier frequency offsets (CFOs). This paper
presents a practical study of these two issues. Firstly, codebook designs of
the Voronoi vector quantization (VQ) and the tree-structure vector quantization
(TSVQ) to enable CF cooperation at the relay are described. A comparison in
terms of the codebook design and encoding complexity is analyzed. It is shown
that the TSVQ is much simpler to design and operate, and can achieve a
favorable performance-complexity tradeoff. Furthermore, this paper demonstrates
that CFO can lead to significant performance degradation for the virtual MIMO
system. To overcome this, it is proposed to maintain clock synchronization and
jointly estimate the CFO between the relay and the destination. This approach
is shown to provide a significant performance improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1912</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1912</id><created>2013-02-07</created><authors><author><keyname>Seth</keyname><forenames>Ashish</forenames></author><author><keyname>Agarwal</keyname><forenames>Himanshu</forenames></author><author><keyname>Singla</keyname><forenames>Ashim Raj</forenames></author></authors><title>Testing and Evaluation of Service Oriented Systems</title><categories>cs.SE</categories><comments>13 pages 3 figures 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluation of service oriented system has been a challenge, though there are
large number of evaluation metrics exist but none of them is efficient to
evaluate these systems effectively.This paper discusses the different testing
tools and evaluation methods available for SOA and summarizes their limitation
and support in context of service oriented architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1913</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1913</id><created>2013-02-07</created><authors><author><keyname>Banaei</keyname><forenames>Armin</forenames></author><author><keyname>Georghiades</keyname><forenames>Costas N.</forenames></author></authors><title>On Randomized Sensing and Access Schemes in Wireless Ad-Hoc Cognitive
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade we have witnessed a rapid growth and development in
wireless communication systems, to the point that conventional spectrum
allocation policies may not be able to fulfill them all. Federal Communications
Commission (FCC) licenses certain frequency segments to a particular user in a
particular geographic area. Industrial, Scientific and Medical (ISM) radio
bands have also been envisioned for all other unlicensed user to share, as long
as they follow certain power regulations. But with the recent boom in the
wireless technologies, these open channels have become overcrowded with
everything from wireless networks to wireless controllers.
  Therefore, the regulatory and standardization agencies have been working on
new spectrum regulation policies for wireless communication systems. The
underlying idea is to let unlicensed users to use the licensed band as long as
they can guarantee low interference to the licensed users. Though seemingly
simple, sophisticated interference management protocols are needed to meet the
expected level of transparency accepted by licensed users. In this report we
adopt the dynamic spectrum access approach to limit the interference to primary
users and analyze the performance of the cognitive MAC protocols based on
randomized sensing and access schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1919</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1919</id><created>2013-02-07</created><authors><author><keyname>Aistleitner</keyname><forenames>Christoph</forenames></author></authors><title>Normal numbers and normality measure</title><categories>math.CO cs.DM math.NT</categories><msc-class>68R15, 11K45, 11K16</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The normality measure $\mathcal{N}$ has been introduced by Mauduit and
S{\'a}rk{\&quot;o}zy in order to describe the pseudorandomness properties of finite
binary sequences. Alon, Kohayakawa, Mauduit, Moreira and R{\&quot;o}dl proved that
the minimal possible value of the normality measure of an $N$-element binary
sequence satisfies $$ (1/2 + o(1)) \log_2 N \leq \min_{E_N \in \{0,1\}^N}
\mathcal{N}(E_N) \leq 3 N^{1/3} (\log N)^{2/3} $$ for sufficiently large $N$.
In the present paper we improve the upper bound to $c (\log N)^2$ for some
constant $c$, by this means solving the problem of the asymptotic order of the
minimal value of the normality measure up to a logarithmic factor, and
disproving a conjecture of Alon \emph{et al.}. The proof is based on relating
the normality measure of binary sequences to the discrepancy of normal numbers
in base 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1920</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1920</id><created>2013-02-07</created><authors><author><keyname>Jha</keyname><forenames>Susmit</forenames></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames></author></authors><title>SWATI: Synthesizing Wordlengths Automatically Using Testing and
  Induction</title><categories>cs.SY</categories><acm-class>F.2.1; D.2.4; I.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an automated technique SWATI: Synthesizing
Wordlengths Automatically Using Testing and Induction, which uses a combination
of Nelder-Mead optimization based testing, and induction from examples to
automatically synthesize optimal fixedpoint implementation of numerical
routines. The design of numerical software is commonly done using
floating-point arithmetic in design-environments such as Matlab. However, these
designs are often implemented using fixed-point arithmetic for speed and
efficiency reasons especially in embedded systems. The fixed-point
implementation reduces implementation cost, provides better performance, and
reduces power consumption. The conversion from floating-point designs to
fixed-point code is subject to two opposing constraints: (i) the word-width of
fixed-point types must be minimized, and (ii) the outputs of the fixed-point
program must be accurate. In this paper, we propose a new solution to this
problem. Our technique takes the floating-point program, specified accuracy and
an implementation cost model and provides the fixed-point program with
specified accuracy and optimal implementation cost. We demonstrate the
effectiveness of our approach on a set of examples from the domain of automated
control, robotics and digital signal processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1921</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1921</id><created>2013-02-07</created><authors><author><keyname>Kuribayashi</keyname><forenames>Shin-ichi</forenames></author></authors><title>Improving Quality of Service and Reducing Power Consumption with WAN
  accelerator in Cloud Computing Environments</title><categories>cs.NI</categories><comments>12 pages, International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.5, No.1, January 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The widespread use of cloud computing services is expected to deteriorate a
Quality of Service and toincrease the power consumption of ICT devices, since
the distance to a server becomes longer than before. Migration of virtual
machines over a wide area can solve many problems such as load balancing and
power saving in cloud computing environments.
  This paper proposes to dynamically apply WAN accelerator within the network
when a virtual machine is moved to a distant center, in order to prevent the
degradation in performance after live migration of virtual machines over a wide
area. mSCTP-based data transfer using different TCP connections before and
after migration is proposed in order to use a currently available WAN
accelerator. This paper does not consider the performance degradation of live
migration itself. Then, this paper proposes to reduce the power consumption of
ICT devices, which consists of installing WAN accelerators as part of cloud
resources actively and increasing the packet transfer rate of communication
link temporarily. It is demonstrated that the power consumption with WAN
accelerator could be reduced to one-tenth of that without WAN accelerator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1923</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1923</id><created>2013-02-07</created><authors><author><keyname>Liu</keyname><forenames>Jixue</forenames></author><author><keyname>Liu</keyname><forenames>Chengfei</forenames></author><author><keyname>Haerder</keyname><forenames>Theo</forenames></author><author><keyname>Yu</keyname><forenames>Jeffery Xu</forenames></author></authors><title>Update XML Views</title><categories>cs.DB</categories><acm-class>H.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  View update is the problem of translating an update to a view to some updates
to the source data of the view. In this paper, we show the factors determining
XML view update translation, propose a translation procedure, and propose
translated updates to the source document for different types of views. We
further show that the translated updates are precise. The proposed solution
makes it possible for users who do not have access privileges to the source
data to update the source data via a view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1931</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1931</id><created>2013-02-07</created><updated>2014-02-27</updated><authors><author><keyname>Roth</keyname><forenames>Ron M.</forenames></author><author><keyname>Vontobel</keyname><forenames>Pascal O.</forenames></author></authors><title>Coding for Combined Block-Symbol Error Correction</title><categories>cs.IT math.CO math.IT</categories><comments>To appear in IEEE Transactions on Information Theory, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design low-complexity error correction coding schemes for channels that
introduce different types of errors and erasures: on the one hand, the proposed
schemes can successfully deal with symbol errors and erasures, and, on the
other hand, they can also successfully handle phased burst errors and erasures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1937</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1937</id><created>2013-02-07</created><authors><author><keyname>Cranefield</keyname><forenames>Stephen</forenames></author><author><keyname>Ranathunga</keyname><forenames>Surangika</forenames></author></authors><title>Embedding agents in business applications using enterprise integration
  patterns</title><categories>cs.MA</categories><doi>10.1007/978-3-642-45343-4_6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the issue of integrating agents with a variety of
external resources and services, as found in enterprise computing environments.
We propose an approach for interfacing agents and existing message routing and
mediation engines based on the endpoint concept from the enterprise integration
patterns of Hohpe and Woolf. A design for agent endpoints is presented, and an
architecture for connecting the Jason agent platform to the Apache Camel
enterprise integration framework using this type of endpoint is described. The
approach is illustrated by means of a business process use case, and a number
of Camel routes are presented. These demonstrate the benefits of interfacing
agents to external services via a specialised message routing tool that
supports enterprise integration patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1939</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1939</id><created>2013-02-07</created><authors><author><keyname>Sobie</keyname><forenames>R.</forenames></author><author><keyname>Agarwal</keyname><forenames>A.</forenames></author><author><keyname>Gable</keyname><forenames>I.</forenames></author><author><keyname>Leavett-Brown</keyname><forenames>C.</forenames></author><author><keyname>Paterson</keyname><forenames>M.</forenames></author><author><keyname>Taylor</keyname><forenames>R.</forenames></author><author><keyname>Charbonneau</keyname><forenames>A.</forenames></author><author><keyname>Impey</keyname><forenames>R.</forenames></author><author><keyname>Podiama</keyname><forenames>W.</forenames></author></authors><title>HTC Scientific Computing in a Distributed Cloud Environment</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the use of a distributed cloud computing system for
high-throughput computing (HTC) scientific applications. The distributed cloud
computing system is composed of a number of separate
Infrastructure-as-a-Service (IaaS) clouds that are utilized in a unified
infrastructure. The distributed cloud has been in production-quality operation
for two years with approximately 500,000 completed jobs where a typical
workload has 500 simultaneous embarrassingly-parallel jobs that run for
approximately 12 hours. We review the design and implementation of the system
which is based on pre-existing components and a number of custom components. We
discuss the operation of the system, and describe our plans for the expansion
to more sites and increased computing capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1942</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1942</id><created>2013-02-07</created><authors><author><keyname>Jiang</keyname><forenames>Hong</forenames></author><author><keyname>Deng</keyname><forenames>Wei</forenames></author><author><keyname>Shen</keyname><forenames>Zuowei</forenames></author></authors><title>Surveillance Video Processing Using Compressive Sensing</title><categories>cs.CV cs.IT math.IT</categories><comments>14 pages, 5 figures</comments><msc-class>Primary: 00A69, 41-02, Secondary: 46N10</msc-class><journal-ref>Inverse Problems and Imaging, Volume 6, No. 2, 2012, 201-214</journal-ref><doi>10.3934/ipi.2012.6.201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A compressive sensing method combined with decomposition of a matrix formed
with image frames of a surveillance video into low rank and sparse matrices is
proposed to segment the background and extract moving objects in a surveillance
video. The video is acquired by compressive measurements, and the measurements
are used to reconstruct the video by a low rank and sparse decomposition of
matrix. The low rank component represents the background, and the sparse
component is used to identify moving objects in the surveillance video. The
decomposition is performed by an augmented Lagrangian alternating direction
method. Experiments are carried out to demonstrate that moving objects can be
reliably extracted with a small amount of measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1947</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1947</id><created>2013-02-08</created><authors><author><keyname>Li</keyname><forenames>Chengbo</forenames></author><author><keyname>Jiang</keyname><forenames>Hong</forenames></author><author><keyname>Wilford</keyname><forenames>Paul</forenames></author><author><keyname>Zhang</keyname><forenames>Yin</forenames></author><author><keyname>Scheutzow</keyname><forenames>Mike</forenames></author></authors><title>A new compressive video sensing framework for mobile broadcast</title><categories>cs.MM cs.CV cs.IT math.IT</categories><comments>9 pages, 12 figures</comments><journal-ref>IEEE Transactions on Broadcasting VOL 59 NO 1 MARCH 2013 pp. 197 -
  205</journal-ref><doi>10.1109/TBC.2012.2226509</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new video coding method based on compressive sampling is proposed. In this
method, a video is coded using compressive measurements on video cubes. Video
reconstruction is performed by minimization of total variation (TV) of the
pixelwise DCT coefficients along the temporal direction. A new reconstruction
algorithm is developed from TVAL3, an efficient TV minimization algorithm based
on the alternating minimization and augmented Lagrangian methods. Video coding
with this method is inherently scalable, and has applications in mobile
broadcast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1948</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1948</id><created>2013-02-08</created><authors><author><keyname>Dasgupta</keyname><forenames>Sanjoy</forenames></author><author><keyname>Sinha</keyname><forenames>Kaushik</forenames></author></authors><title>Randomized partition trees for exact nearest neighbor search</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The k-d tree was one of the first spatial data structures proposed for
nearest neighbor search. Its efficacy is diminished in high-dimensional spaces,
but several variants, with randomization and overlapping cells, have proved to
be successful in practice. We analyze three such schemes. We show that the
probability that they fail to find the nearest neighbor, for any data set and
any query point, is directly related to a simple potential function that
captures the difficulty of the point configuration. We then bound this
potential function in two situations of interest: the first, when data come
from a doubling measure, and the second, when the data are documents from a
topic model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1951</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1951</id><created>2013-02-08</created><authors><author><keyname>Jiang</keyname><forenames>Bin</forenames></author></authors><title>Editorial: Making GIScience Research More Open Access</title><categories>cs.DL nlin.AO physics.soc-ph</categories><comments>3 pages</comments><journal-ref>International Journal of Geographical Information Science, 2011,
  25(8), 1217 - 1220</journal-ref><doi>10.1080/13658816.2011.585613</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the editorial for the special issue on &quot;data-intensive geospatial
computing&quot;, which I guest edited with the International Journal of Geographical
Information Science (Taylor &amp; Francis). As remarked in the editorial, the
special issue is particularly special in the sense that all source and data are
published together with the published papers. This editorial elaborates on
scholarly communication, with particular attention to publishing data alongside
papers and the emergence of open access journals, in order to make our research
more open access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1954</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1954</id><created>2013-02-08</created><authors><author><keyname>Li</keyname><forenames>Zheng</forenames></author><author><keyname>O'Brien</keyname><forenames>Liam</forenames></author><author><keyname>Zhang</keyname><forenames>He</forenames></author><author><keyname>Cai</keyname><forenames>Rainbow</forenames></author></authors><title>On a Catalogue of Metrics for Evaluating Commercial Cloud Services</title><categories>cs.DC cs.PF</categories><comments>10 pages, Proceedings of the 13th ACM/IEEE International Conference
  on Grid Computing (Grid 2012), pp. 164-173, Beijing, China, September 20-23,
  2012</comments><doi>10.1109/Grid.2012.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the continually increasing amount of commercial Cloud services in the
market, evaluation of different services plays a significant role in
cost-benefit analysis or decision making for choosing Cloud Computing. In
particular, employing suitable metrics is essential in evaluation
implementations. However, to the best of our knowledge, there is not any
systematic discussion about metrics for evaluating Cloud services. By using the
method of Systematic Literature Review (SLR), we have collected the de facto
metrics adopted in the existing Cloud services evaluation work. The collected
metrics were arranged following different Cloud service features to be
evaluated, which essentially constructed an evaluation metrics catalogue, as
shown in this paper. This metrics catalogue can be used to facilitate the
future practice and research in the area of Cloud services evaluation.
Moreover, considering metrics selection is a prerequisite of benchmark
selection in evaluation implementations, this work also supplements the
existing research in benchmarking the commercial Cloud services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1957</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1957</id><created>2013-02-08</created><authors><author><keyname>Li</keyname><forenames>Zheng</forenames></author><author><keyname>O'Brien</keyname><forenames>Liam</forenames></author><author><keyname>Cai</keyname><forenames>Rainbow</forenames></author><author><keyname>Zhang</keyname><forenames>He</forenames></author></authors><title>Towards a Taxonomy of Performance Evaluation of Commercial Cloud
  Services</title><categories>cs.DC cs.PF</categories><comments>8 pages, Proceedings of the 5th International Conference on Cloud
  Computing (IEEE CLOUD 2012), pp. 344-351, Honolulu, Hawaii, USA, June 24-29,
  2012</comments><doi>10.1109/CLOUD.2012.74</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Computing, as one of the most promising computing paradigms, has become
increasingly accepted in industry. Numerous commercial providers have started
to supply public Cloud services, and corresponding performance evaluation is
then inevitably required for Cloud provider selection or cost-benefit analysis.
Unfortunately, inaccurate and confusing evaluation implementations can be often
seen in the context of commercial Cloud Computing, which could severely
interfere and spoil evaluation-related comprehension and communication. This
paper introduces a taxonomy to help profile and standardize the details of
performance evaluation of commercial Cloud services. Through a systematic
literature review, we constructed the taxonomy along two dimensions by
arranging the atomic elements of Cloud-related performance evaluation. As such,
this proposed taxonomy can be employed both to analyze existing evaluation
practices through decomposition into elements and to design new experiments
through composing elements for evaluating performance of commercial Cloud
services. Moreover, through smooth expansion, we can continually adapt this
taxonomy to the more general area of evaluation of Cloud Computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.1971</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.1971</id><created>2013-02-08</created><authors><author><keyname>Senthilnayaki</keyname><forenames>B.</forenames></author><author><keyname>Venkatalakshmi</keyname><forenames>K.</forenames></author><author><keyname>Kannan</keyname><forenames>A.</forenames></author></authors><title>A fuzzy similarity based approach for intelligent web based e-learning</title><categories>cs.CY</categories><comments>6 pages, 7 figures and 1 table</comments><msc-class>14J60 (Primary), 14F05, 14J26 (Secondary)</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an intelligent system for web based e-Learning is proposed
which analyzes students knowledge capacity by applying clustering technique.
This system uses fuzzy logic and k-means clustering algorithm to arrange the
documents according to the level of their performance. Moreover, a new domain
ontology alignment technique is proposed that uses contextual information of
the knowledge sources from the e-Learning domain for effective decision making.
The proposed ontology alignment method has been empirically tested in an
e-Learning environment and the experimental results show that the proposed
method performs better than the existing methods in terms of precision and
recall. The salient contributions of this paper are the use of Jaccard
Similarity, fuzzy approach for ontology alignment and K-Means clustering
algorithm for decision making using decision rules for providing intelligent
e-Learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2015</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2015</id><created>2013-02-08</created><updated>2013-02-15</updated><authors><author><keyname>Skraba</keyname><forenames>Primoz</forenames></author><author><keyname>Vejdemo-Johansson</keyname><forenames>Mikael</forenames></author></authors><title>Persistence modules: Algebra and algorithms</title><categories>cs.CG math.AT math.RT</categories><comments>28 pages, submitted to Mathematics of Computation</comments><msc-class>13P10, 55N35</msc-class><acm-class>I.1.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Persistent homology was shown by Carlsson and Zomorodian to be homology of
graded chain complexes with coefficients in the graded ring $\kk[t]$. As such,
the behavior of persistence modules -- graded modules over $\kk[t]$ is an
important part in the analysis and computation of persistent homology.
  In this paper we present a number of facts about persistence modules; ranging
from the well-known but under-utilized to the reconstruction of techniques to
work in a purely algebraic approach to persistent homology. In particular, the
results we present give concrete algorithms to compute the persistent homology
of a simplicial complex with torsion in the chain complex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2017</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2017</id><created>2013-02-08</created><authors><author><keyname>Hatanaka</keyname><forenames>Takeshi</forenames></author><author><keyname>Wasa</keyname><forenames>Yasuaki</forenames></author><author><keyname>Fujita</keyname><forenames>Masayuki</forenames></author></authors><title>Cooperative Environmental Monitoring for PTZ Visual Sensor Networks: A
  Payoff-based Learning Approach</title><categories>cs.SY</categories><comments>34 pages, 27 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates cooperative environmental monitoring for
Pan-Tilt-Zoom (PTZ) visual sensor networks. We first present a novel
formulation of the optimal environmental monitoring problem, whose objective
function is intertwined with the uncertain state of the environment. In
addition, due to the large volume of vision data, it is desired for each sensor
to execute processing through local computation and communication. To address
the issues, we present a distributed solution to the problem based on game
theoretic cooperative control and payoff-based learning. At the first stage, a
utility function is designed so that the resulting game constitutes a potential
game with potential function equal to the group objective function, where the
designed utility is shown to be computable through local image processing and
communication. Then, we present a payoff-based learning algorithm so that the
sensors are led to the global objective function maximizers without using any
prior information on the environmental state. Finally, we run experiments to
demonstrate the effectiveness of the present approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2024</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2024</id><created>2013-02-08</created><authors><author><keyname>Klein</keyname><forenames>Jonathan</forenames></author><author><keyname>Reuling</keyname><forenames>Dennis</forenames></author><author><keyname>Grimm</keyname><forenames>Jan</forenames></author><author><keyname>Pfau</keyname><forenames>Andreas</forenames></author><author><keyname>Lefloch</keyname><forenames>Damien</forenames></author><author><keyname>Lambers</keyname><forenames>Martin</forenames></author><author><keyname>Kolb</keyname><forenames>Andreas</forenames></author></authors><title>User Interface for Volume Rendering in Virtual Reality Environments</title><categories>cs.GR cs.HC</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Volume Rendering applications require sophisticated user interaction for the
definition and refinement of transfer functions. Traditional 2D desktop user
interface elements have been developed to solve this task, but such concepts do
not map well to the interaction devices available in Virtual Reality
environments.
  In this paper, we propose an intuitive user interface for Volume Rendering
specifically designed for Virtual Reality environments. The proposed interface
allows transfer function design and refinement based on intuitive two-handed
operation of Wand-like controllers. Additional interaction modes such as
navigation and clip plane manipulation are supported as well.
  The system is implemented using the Sony PlayStation Move controller system.
This choice is based on controller device capabilities as well as application
and environment constraints.
  Initial results document the potential of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2028</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2028</id><created>2013-02-08</created><authors><author><keyname>Shapira</keyname><forenames>Yuri</forenames></author><author><keyname>Shapira</keyname><forenames>Bracha</forenames></author><author><keyname>Shabtai</keyname><forenames>Asaf</forenames></author></authors><title>Content-based data leakage detection using extended fingerprinting</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protecting sensitive information from unauthorized disclosure is a major
concern of every organization. As an organizations employees need to access
such information in order to carry out their daily work, data leakage detection
is both an essential and challenging task. Whether caused by malicious intent
or an inadvertent mistake, data loss can result in significant damage to the
organization. Fingerprinting is a content-based method used for detecting data
leakage. In fingerprinting, signatures of known confidential content are
extracted and matched with outgoing content in order to detect leakage of
sensitive content. Existing fingerprinting methods, however, suffer from two
major limitations. First, fingerprinting can be bypassed by rephrasing (or
minor modification) of the confidential content, and second, usually the whole
content of document is fingerprinted (including non-confidential parts),
resulting in false alarms. In this paper we propose an extension to the
fingerprinting approach that is based on sorted k-skip-n-grams. The proposed
method is able to produce a fingerprint of the core confidential content which
ignores non-relevant (non-confidential) sections. In addition, the proposed
fingerprint method is more robust to rephrasing and can also be used to detect
a previously unseen confidential document and therefore provide better
detection of intentional leakage incidents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2048</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2048</id><created>2013-02-08</created><authors><author><keyname>Barbier</keyname><forenames>Morgan</forenames><affiliation>GREYC</affiliation></author><author><keyname>Munuera</keyname><forenames>Carlos</forenames></author></authors><title>Improving success probability and embedding efficiency in code based
  steganography</title><categories>cs.IT math.IT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For stegoschemes arising from error correcting codes, embedding depends on a
decoding map for the corresponding code. As decoding maps are usually not
complete, embedding can fail. We propose a method to ensure or increase the
probability of embedding success for these stegoschemes. This method is based
on puncturing codes. We show how the use of punctured codes may also increase
the embedding efficiency of the obtained stegoschemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2056</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2056</id><created>2013-02-08</created><authors><author><keyname>Hernandez-Orallo</keyname><forenames>Jose</forenames></author></authors><title>Complexity distribution of agent policies</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse the complexity of environments according to the policies that need
to be used to achieve high performance. The performance results for a
population of policies leads to a distribution that is examined in terms of
policy complexity and analysed through several diagrams and indicators. The
notion of environment response curve is also introduced, by inverting the
performance results into an ability scale. We apply all these concepts,
diagrams and indicators to a minimalistic environment class, agent-populated
elementary cellular automata, showing how the difficulty, discriminating power
and ranges (previous to normalisation) may vary for several environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2073</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2073</id><created>2013-02-08</created><updated>2013-03-28</updated><authors><author><keyname>Seidel</keyname><forenames>Florian</forenames></author><author><keyname>Hage</keyname><forenames>Clemens</forenames></author><author><keyname>Kleinsteuber</keyname><forenames>Martin</forenames></author></authors><title>pROST : A Smoothed Lp-norm Robust Online Subspace Tracking Method for
  Realtime Background Subtraction in Video</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing number of methods for background subtraction use Robust PCA to
identify sparse foreground objects. While many algorithms use the L1-norm as a
convex relaxation of the ideal sparsifying function, we approach the problem
with a smoothed Lp-norm and present pROST, a method for robust online subspace
tracking. The algorithm is based on alternating minimization on manifolds.
Implemented on a graphics processing unit it achieves realtime performance.
Experimental results on a state-of-the-art benchmark for background subtraction
on real-world video data indicate that the method succeeds at a broad variety
of background subtraction scenarios, and it outperforms competing approaches
when video quality is deteriorated by camera jitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2082</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2082</id><created>2013-02-08</created><updated>2014-01-16</updated><authors><author><keyname>Parhizkar</keyname><forenames>Reza</forenames></author><author><keyname>Barbotin</keyname><forenames>Yann</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>Sequences with Minimal Time-Frequency Uncertainty</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central problem in signal processing and communications is to design
signals that are compact both in time and frequency. Heisenberg's uncertainty
principle states that a given function cannot be arbitrarily compact both in
time and frequency, defining an &quot;uncertainty&quot; lower bound. Taking the variance
as a measure of localization in time and frequency, Gaussian functions reach
this bound for continuous-time signals. For sequences, however, this is not
true; it is known that Heisenberg's bound is generally unachievable. For a
chosen frequency variance, we formulate the search for &quot;maximally compact
sequences&quot; as an exactly and efficiently solved convex optimization problem,
thus providing a sharp uncertainty principle for sequences. Interestingly, the
optimization formulation also reveals that maximally compact sequences are
derived from Mathieu's harmonic cosine function of order zero. We further
provide rational asymptotic expansions of this sharp uncertainty bound. We use
the derived bounds as a benchmark to compare the compactness of well-known
window functions with that of the optimal Mathieu's functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2093</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2093</id><created>2013-02-08</created><authors><author><keyname>Doan</keyname><forenames>Minh Dang</forenames></author><author><keyname>Giselsson</keyname><forenames>Pontus</forenames></author><author><keyname>Keviczky</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>De Schutter</keyname><forenames>Bart</forenames></author><author><keyname>Rantzer</keyname><forenames>Anders</forenames></author></authors><title>A distributed accelerated gradient algorithm for distributed model
  predictive control of a hydro power valley</title><categories>math.OC cs.MA cs.SY math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distributed model predictive control (DMPC) approach based on distributed
optimization is applied to the power reference tracking problem of a hydro
power valley (HPV) system. The applied optimization algorithm is based on
accelerated gradient methods and achieves a convergence rate of O(1/k^2), where
k is the iteration number. Major challenges in the control of the HPV include a
nonlinear and large-scale model, nonsmoothness in the power-production
functions, and a globally coupled cost function that prevents distributed
schemes to be applied directly. We propose a linearization and approximation
approach that accommodates the proposed the DMPC framework and provides very
similar performance compared to a centralized solution in simulations. The
provided numerical studies also suggest that for the sparsely interconnected
system at hand, the distributed algorithm we propose is faster than a
centralized state-of-the-art solver such as CPLEX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2112</identifier>
 <datestamp>2013-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2112</id><created>2013-02-08</created><updated>2013-04-25</updated><authors><author><keyname>Rastaghi</keyname><forenames>Roohallah</forenames></author></authors><title>Cryptanalysis and Improvement of Akleylek et al.'s cryptosystem</title><categories>cs.CR cs.IT math.IT</categories><comments>This article is an extended/revised version of an ISCISC'12 paper,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Akleylek et al. [S. Akleylek, L. Emmungil and U. Nuriyev, A mod ified
algorithm for peer-to-peer security, journal of Appl. Comput. Math., vol. 6(2),
pp.258-264, 2007.], introduced a modified public-key encryption scheme with
steganographic approach for security in peer-to-peer (P2P) networks. In this
cryptosystem, Akleylek et al. attempt to increase security of the P2P networks
by mixing ElGamal cryptosystem with knapsack problem. In this paper, we present
a ciphertext-only attack against their system to recover message. In addition,
we show that for their scheme completeness property is not holds, and
therefore, the receiver cannot uniquely decrypts messages. Furthermore, we also
show that this system is not chosen-ciphertext secure, thus the proposed scheme
is vulnerable to man-in-the-middle-attack, one of the most pernicious attacks
against P2P networks. Therefore, this scheme is not suitable to implement in
the P2P networks.
  We modify this cryptosystem in order to increase its security and efficiency.
Our construction is the efficient CCA2-secure variant of the Akleylek et al.'s
encryption scheme in the standard model, the de facto security notion for
public-key encryption schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2123</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2123</id><created>2013-02-08</created><updated>2013-08-03</updated><authors><author><keyname>Hirsch</keyname><forenames>Andrew K.</forenames></author><author><keyname>Clarkson</keyname><forenames>Michael R.</forenames></author></authors><title>Belief Semantics of Authorization Logic</title><categories>cs.LO cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Authorization logics have been used in the theory of computer security to
reason about access control decisions. In this work, a formal belief semantics
for authorization logics is given. The belief semantics is proved to subsume a
standard Kripke semantics. The belief semantics yields a direct representation
of principals' beliefs, without resorting to the technical machinery used in
Kripke semantics. A proof system is given for the logic; that system is proved
sound with respect to the belief and Kripke semantics. The soundness proof for
the belief semantics, and for a variant of the Kripke semantics, is mechanized
in Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2127</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2127</id><created>2013-02-08</created><updated>2013-04-10</updated><authors><author><keyname>K&#xf6;nemann</keyname><forenames>Jochen</forenames></author><author><keyname>Sadeghian</keyname><forenames>Sina</forenames></author><author><keyname>Sanit&#xe0;</keyname><forenames>Laura</forenames></author></authors><title>An LMP O(log n)-Approximation Algorithm for Node Weighted Prize
  Collecting Steiner Tree</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the node-weighted prize-collecting Steiner tree problem (NW-PCST) we are
given an undirected graph $G=(V,E)$, non-negative costs $c(v)$ and penalties
$\pi(v)$ for each $v \in V$. The goal is to find a tree $T$ that minimizes the
total cost of the vertices spanned by $T$ plus the total penalty of vertices
not in $T$. This problem is well-known to be set-cover hard to approximate.
Moss and Rabani (STOC'01) presented a primal-dual
Lagrangean-multiplier-preserving $O(\ln |V|)$-approximation algorithm for this
problem. We show a serious problem with the algorithm, and present a new,
fundamentally different primal-dual method achieving the same performance
guarantee. Our algorithm introduces several novel features to the primal-dual
method that may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2128</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2128</id><created>2013-02-08</created><updated>2013-11-25</updated><authors><author><keyname>Skorski</keyname><forenames>Maciej</forenames></author></authors><title>Modulus Computational Entropy</title><categories>cs.IT cs.CR math.IT</categories><comments>Accepted at ICTS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The so-called {\em leakage-chain rule} is a very important tool used in many
security proofs. It gives an upper bound on the entropy loss of a random
variable $X$ in case the adversary who having already learned some random
variables $Z_{1},\ldots,Z_{\ell}$ correlated with $X$, obtains some further
information $Z_{\ell+1}$ about $X$. Analogously to the information-theoretic
case, one might expect that also for the \emph{computational} variants of
entropy the loss depends only on the actual leakage, i.e. on $Z_{\ell+1}$.
Surprisingly, Krenn et al.\ have shown recently that for the most commonly used
definitions of computational entropy this holds only if the computational
quality of the entropy deteriorates exponentially in
$|(Z_{1},\ldots,Z_{\ell})|$. This means that the current standard definitions
of computational entropy do not allow to fully capture leakage that occurred
&quot;in the past&quot;, which severely limits the applicability of this notion.
  As a remedy for this problem we propose a slightly stronger definition of the
computational entropy, which we call the \emph{modulus computational entropy},
and use it as a technical tool that allows us to prove a desired chain rule
that depends only on the actual leakage and not on its history. Moreover, we
show that the modulus computational entropy unifies other,sometimes seemingly
unrelated, notions already studied in the literature in the context of
information leakage and chain rules. Our results indicate that the modulus
entropy is, up to now, the weakest restriction that guarantees that the chain
rule for the computational entropy works. As an example of application we
demonstrate a few interesting cases where our restricted definition is
fulfilled and the chain rule holds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2129</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2129</id><created>2013-02-08</created><authors><author><keyname>Noorshams</keyname><forenames>Nima</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin</forenames></author></authors><title>Non-Asymptotic Analysis of an Optimal Algorithm for Network-Constrained
  Averaging with Noisy Links</title><categories>cs.DC stat.CO</categories><journal-ref>N. Noorshams, M. J. Wainwright, &quot;Non-Asymptotic Analysis of an
  Optimal Algorithm for Network-Constrained Averaging with Noisy Links&quot;, IEEE
  journal of selected topics in signal processing, vol. 5, no. 4, pp. 833-844,
  Aug. 2011</journal-ref><doi>10.1109/JSTSP.2011.2122241</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of network-constrained averaging is to compute the average of a
set of values distributed throughout a graph G using an algorithm that can pass
messages only along graph edges. We study this problem in the noisy setting, in
which the communication along each link is modeled by an additive white
Gaussian noise channel. We propose a two-phase decentralized algorithm, and we
use stochastic approximation methods in conjunction with the spectral graph
theory to provide concrete (non-asymptotic) bounds on the mean-squared error.
Having found such bounds, we analyze how the number of iterations T_G(n;
\delta) required to achieve mean-squared error \delta\ scales as a function of
the graph topology and the number of nodes n. Previous work provided guarantees
with the number of iterations scaling inversely with the second smallest
eigenvalue of the Laplacian. This paper gives an algorithm that reduces this
graph dependence to the graph diameter, which is the best scaling possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2131</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2131</id><created>2013-02-08</created><authors><author><keyname>Pavlyshenko</keyname><forenames>Bohdan</forenames></author></authors><title>Data Mining of the Concept &quot;End of the World&quot; in Twitter Microblogs</title><categories>cs.SI cs.CL cs.IR physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the analysis of quantitative characteristics of frequent
sets and association rules in the posts of Twitter microblogs, related to the
discussion of &quot;end of the world&quot;, which was allegedly predicted on December 21,
2012 due to the Mayan calendar. Discovered frequent sets and association rules
characterize semantic relations between the concepts of analyzed subjects.The
support for some fequent sets reaches the global maximum before the expected
event with some time delay. Such frequent sets may be considered as predictive
markers that characterize the significance of expected events for blogosphere
users. It was shown that time dynamics of confidence of some revealed
association rules can also have predictive characteristics. Exceeding a certain
threshold, it may be a signal for the corresponding reaction in the society
during the time interval between the maximum and probable coming of an event.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2137</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2137</id><created>2013-02-08</created><authors><author><keyname>Cohen</keyname><forenames>Edith</forenames></author><author><keyname>Cormode</keyname><forenames>Graham</forenames></author><author><keyname>Duffield</keyname><forenames>Nick</forenames></author><author><keyname>Lund</keyname><forenames>Carsten</forenames></author></authors><title>On the Tradeoff between Stability and Fit</title><categories>cs.DS</categories><comments>24 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computing, as in many aspects of life, changes incur cost. Many
optimization problems are formulated as a one-time instance starting from
scratch. However, a common case that arises is when we already have a set of
prior assignments, and must decide how to respond to a new set of constraints,
given that each change from the current assignment comes at a price. That is,
we would like to maximize the fitness or efficiency of our system, but we need
to balance it with the changeout cost from the previous state.
  We provide a precise formulation for this tradeoff and analyze the resulting
{\em stable extensions} of some fundamental problems in measurement and
analytics. Our main technical contribution is a stable extension of PPS
(probability proportional to size) weighted random sampling, with applications
to monitoring and anomaly detection problems. We also provide a general
framework that applies to top-$k$, minimum spanning tree, and assignment. In
both cases, we are able to provide exact solutions, and discuss efficient
incremental algorithms that can find new solutions as the input changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2157</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2157</id><created>2013-02-08</created><updated>2013-05-18</updated><authors><author><keyname>Mahdavi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author></authors><title>Passive Learning with Target Risk</title><categories>cs.LG</categories><journal-ref>Conference on Learning Theory (COLT 2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider learning in passive setting but with a slight
modification. We assume that the target expected loss, also referred to as
target risk, is provided in advance for learner as prior knowledge. Unlike most
studies in the learning theory that only incorporate the prior knowledge into
the generalization bounds, we are able to explicitly utilize the target risk in
the learning process. Our analysis reveals a surprising result on the sample
complexity of learning: by exploiting the target risk in the learning
algorithm, we show that when the loss function is both strongly convex and
smooth, the sample complexity reduces to $\O(\log (\frac{1}{\epsilon}))$, an
exponential improvement compared to the sample complexity
$\O(\frac{1}{\epsilon})$ for learning with strongly convex loss functions.
Furthermore, our proof is constructive and is based on a computationally
efficient stochastic optimization algorithm for such settings which demonstrate
that the proposed algorithm is practically useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2158</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2158</id><created>2013-02-08</created><updated>2016-01-06</updated><authors><author><keyname>Dvorak</keyname><forenames>Zdenek</forenames></author><author><keyname>Kral</keyname><forenames>Daniel</forenames></author><author><keyname>Thomas</keyname><forenames>Robin</forenames></author></authors><title>Three-coloring triangle-free graphs on surfaces II. 4-critical graphs in
  a disk</title><categories>math.CO cs.DM</categories><comments>45 pages, 2 figures This version: Minor fix to the statement of one
  lemma, bibliography update</comments><msc-class>05C15 (Primary) 05C10 (Secondary)</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be a plane graph of girth at least five. We show that if there exists a
3-coloring phi of a cycle C of G that does not extend to a 3-coloring of G,
then G has a subgraph H on O(|C|) vertices that also has no 3-coloring
extending phi. This is asymptotically best possible and improves a previous
bound of Thomassen. In the next paper of the series we will use this result and
the attendant theory to prove a generalization to graphs on surfaces with
several precolored cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2167</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2167</id><created>2013-02-08</created><authors><author><keyname>Venkat</keyname><forenames>Kartik</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author><author><keyname>Carmon</keyname><forenames>Yair</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author></authors><title>Information, Estimation, and Lookahead in the Gaussian channel</title><categories>cs.IT math.IT</categories><comments>30 pages, 10 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider mean squared estimation with lookahead of a continuous-time
signal corrupted by additive white Gaussian noise. We show that the mutual
information rate function, i.e., the mutual information rate as function of the
signal-to-noise ratio (SNR), does not, in general, determine the minimum mean
squared error (MMSE) with fixed finite lookahead, in contrast to the special
cases with 0 and infinite lookahead (filtering and smoothing errors),
respectively, which were previously established in the literature. We also
establish a new expectation identity under a generalized observation model
where the Gaussian channel has an SNR jump at $t=0$, capturing the tradeoff
between lookahead and SNR.
  Further, we study the class of continuous-time stationary Gauss-Markov
processes (Ornstein-Uhlenbeck processes) as channel inputs, and explicitly
characterize the behavior of the minimum mean squared error (MMSE) with finite
lookahead and signal-to-noise ratio (SNR). The MMSE with lookahead is shown to
converge exponentially rapidly to the non-causal error, with the exponent being
the reciprocal of the non-causal error. We extend our results to mixtures of
Ornstein-Uhlenbeck processes, and use the insight gained to present lower and
upper bounds on the MMSE with lookahead for a class of stationary Gaussian
input processes, whose spectrum can be expressed as a mixture of
Ornstein-Uhlenbeck spectra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2168</identifier>
 <datestamp>2013-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2168</id><created>2013-02-08</created><updated>2013-05-22</updated><authors><author><keyname>Ji</keyname><forenames>Mingyue</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author></authors><title>Optimal Throughput-Outage Trade-off in Wireless One-Hop Caching Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>5 pages, 3 figures, to appear in ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wireless device-to-device (D2D) network where the nodes have
cached information from a library of possible files. Inspired by the current
trend in the standardization of the D2D mode for 4th generation wireless
networks, we restrict to one-hop communication: each node place a request to a
file in the library, and downloads from some other node which has the requested
file in its cache through a direct communication link, without going through a
base station. We describe the physical layer communication through a simple
&quot;protocol-model&quot;, based on interference avoidance (independent set scheduling).
For this network we define the outage-throughput tradeoff problem and
characterize the optimal scaling laws for various regimes where both the number
of nodes and the files in the library grow to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2169</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2169</id><created>2013-02-08</created><authors><author><keyname>Rich</keyname><forenames>Albert D.</forenames></author><author><keyname>Stoutemyer</keyname><forenames>David R.</forenames></author></authors><title>Representation, simplification and display of fractional powers of
  rational numbers in computer algebra</title><categories>cs.SC</categories><comments>23 pages, 1 figure, 4 tables</comments><msc-class>11 Number Theory, Algebraic Number Theory Computations</msc-class><acm-class>I.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simplification of fractional powers of positive rational numbers and of sums,
products and powers of such numbers is taught in beginning algebra. Such
numbers can often be expressed in many ways, as this article discusses in some
detail. Since they are such a restricted subset of algebraic numbers, it might
seem that good simplification of them must already be implemented in all widely
used computer algebra systems. However, the algorithm taught in beginning
algebra uses integer factorization, which can consume unacceptable time for the
large numbers that often arise within computer algebra. Therefore some systems
apparently use various ad hoc techniques that can return an incorrect result
because of not simplifying to 0 the difference between two equivalent such
expressions. Even systems that avoid this flaw often do not return the same
result for all equivalent such input forms, or return an unnecessarily bulky
result that does not have any other compensating useful property. This article
identifies some of these deficiencies, then describes the advantages and
disadvantages of various alternative forms and how to overcome the deficiencies
without costly integer factorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2176</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2176</id><created>2013-02-08</created><authors><author><keyname>McMahan</keyname><forenames>H. Brendan</forenames></author></authors><title>Minimax Optimal Algorithms for Unconstrained Linear Optimization</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design and analyze minimax-optimal algorithms for online linear
optimization games where the player's choice is unconstrained. The player
strives to minimize regret, the difference between his loss and the loss of a
post-hoc benchmark strategy. The standard benchmark is the loss of the best
strategy chosen from a bounded comparator set. When the the comparison set and
the adversary's gradients satisfy L_infinity bounds, we give the value of the
game in closed form and prove it approaches sqrt(2T/pi) as T -&gt; infinity.
  Interesting algorithms result when we consider soft constraints on the
comparator, rather than restricting it to a bounded set. As a warmup, we
analyze the game with a quadratic penalty. The value of this game is exactly
T/2, and this value is achieved by perhaps the simplest online algorithm of
all: unprojected gradient descent with a constant learning rate.
  We then derive a minimax-optimal algorithm for a much softer penalty
function. This algorithm achieves good bounds under the standard notion of
regret for any comparator point, without needing to specify the comparator set
in advance. The value of this game converges to sqrt{e} as T -&gt;infinity; we
give a closed-form for the exact value as a function of T. The resulting
algorithm is natural in unconstrained investment or betting scenarios, since it
guarantees at worst constant loss, while allowing for exponential reward
against an &quot;easy&quot; adversary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2178</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2178</id><created>2013-02-08</created><authors><author><keyname>Bandemer</keyname><forenames>Bernd</forenames><affiliation>Shitz</affiliation></author><author><keyname>Tian</keyname><forenames>Chao</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Gaussian State Amplification with Noisy State Observations</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures; submitted to IEEE International Symposium on
  Information Theory (ISIT 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of simultaneous message transmission and state amplification in a
Gaussian channel with additive Gaussian state is studied when the sender has
imperfect noncausal knowledge of the state sequence. Inner and outer bounds to
the rate--state-distortion region are provided. The coding scheme underlying
the inner bound combines analog signaling and Gelfand-Pinsker coding, where the
latter deviates from the operating point of Costa's dirty paper coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2183</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2183</id><created>2013-02-08</created><authors><author><keyname>Haim</keyname><forenames>Eli</forenames></author><author><keyname>Kochman</keyname><forenames>Yuval</forenames></author><author><keyname>Erez</keyname><forenames>Uri</forenames></author></authors><title>The Importance of Tie-Breaking in Finite-Blocklength Bounds</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider upper bounds on the error probability in channel coding. We
derive an improved maximum-likelihood union bound, which takes into account
events where the likelihood of the correct codeword is tied with that of some
competitors. We compare this bound to various previous results, both
qualitatively and quantitatively. With respect to maximal error probability of
linear codes, we observe that when the channel is additive, the derivation of
bounds, as well as the assumptions on the admissible encoder and decoder,
simplify considerably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2184</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2184</id><created>2013-02-08</created><updated>2015-09-30</updated><authors><author><keyname>Borradaile</keyname><forenames>Glencora</forenames></author><author><keyname>Klein</keyname><forenames>Philip</forenames></author></authors><title>The two-edge connectivity survivable-network design problem in planar
  graphs</title><categories>cs.DS</categories><comments>Updated from original conference version (ICALP '08). To appear:
  Transactions on Algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the following problem: given a graph with edge costs and a subset Q
of vertices, find a minimum-cost subgraph in which there are two edge-disjoint
paths connecting every pair of vertices in Q. The problem is a
failure-resilient analog of the Steiner tree problem arising, for example, in
telecommunications applications. We study a more general mixed-connectivity
formulation, also employed in telecommunications optimization. Given a number
(or requirement) r(v) in {0, 1, 2} for each vertex v in the graph, find a
minimum-cost subgraph in which there are min{r(u), r(v)} edge-disjoint u-to-v
paths for every pair u, v of vertices.
  We address the problem in planar graphs, considering a popular relaxation in
which the solution is allowed to use multiple copies of the input-graph edges
(paying separately for each copy). The problem is max SNP-hard in general
graphs and strongly NP-hard in planar graphs. We give the first polynomial-time
approximation scheme in planar graphs. The running time is O(n log n).
  Under the additional restriction that the requirements are only non-zero for
vertices on the boundary of a single face of a planar graph, we give a
polynomial-time algorithm to find the optimal solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2185</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2185</id><created>2013-02-08</created><updated>2013-12-16</updated><authors><author><keyname>Everett</keyname><forenames>Evan</forenames></author><author><keyname>Sahai</keyname><forenames>Achaleshwar</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Passive Self-Interference Suppression for Full-Duplex Infrastructure
  Nodes</title><categories>cs.IT cs.NI math.IT</categories><comments>16 pages, Accepted by IEEE Transactions on Wireless Communication,
  October 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research results have demonstrated the feasibility of full-duplex
wireless communication for short-range links. Although the focus of the
previous works has been active cancellation of the self-interference signal, a
majority of the overall self-interference suppression is often due to passive
suppression, i.e., isolation of the transmit and receive antennas. We present a
measurement-based study of the capabilities and limitations of three key
mechanisms for passive self-interference suppression: directional isolation,
absorptive shielding, and cross-polarization. The study demonstrates that more
than 70 dB of passive suppression can be achieved in certain environments, but
also establishes two results on the limitations of passive suppression: (1)
environmental reflections limit the amount of passive suppression that can be
achieved, and (2) passive suppression, in general, increases the frequency
selectivity of the residual self-interference signal. These results suggest two
design implications: (1) deployments of full-duplex infrastructure nodes should
minimize near-antenna reflectors, and (2) active cancellation in concatenation
with passive suppression should employ higher-order filters or per-subcarrier
cancellation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2187</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2187</id><created>2013-02-08</created><authors><author><keyname>Kaviani</keyname><forenames>Saeed</forenames><affiliation>Shitz</affiliation></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Krzymie&#x144;</keyname><forenames>Witold A</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Linear Precoding and Equalization for Network MIMO with Partial
  Cooperation</title><categories>cs.IT math.IT math.OC</categories><comments>13 pages, 5 figures, published in IEEE Transactions on Vehicular
  Technology, June 2012</comments><journal-ref>Kaviani, S.; Simeone, O.; Krzymien, W.A.; Shamai, S.; , &quot;Linear
  Precoding and Equalization for Network MIMO With Partial Cooperation,&quot;
  Vehicular Technology, IEEE Transactions on , vol.61, no.5, pp.2083-2096, Jun
  2012</journal-ref><doi>10.1109/TVT.2012.2187710</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A cellular multiple-input multiple-output (MIMO) downlink system is studied
in which each base station (BS) transmits to some of the users, so that each
user receives its intended signal from a subset of the BSs. This scenario is
referred to as network MIMO with partial cooperation, since only a subset of
the BSs are able to coordinate their transmission towards any user. The focus
of this paper is on the optimization of linear beamforming strategies at the
BSs and at the users for network MIMO with partial cooperation. Individual
power constraints at the BSs are enforced, along with constraints on the number
of streams per user. It is first shown that the system is equivalent to a MIMO
interference channel with generalized linear constraints (MIMO-IFC-GC). The
problems of maximizing the sum-rate(SR) and minimizing the weighted sum mean
square error (WSMSE) of the data estimates are non-convex, and suboptimal
solutions with reasonable complexity need to be devised. Based on this,
suboptimal techniques that aim at maximizing the sum-rate for the MIMO-IFC-GC
are reviewed from recent literature and extended to the MIMO-IFC-GC where
necessary. Novel designs that aim at minimizing the WSMSE are then proposed.
Extensive numerical simulations are provided to compare the performance of the
considered schemes for realistic cellular systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2191</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2191</id><created>2013-02-08</created><authors><author><keyname>Kavehei</keyname><forenames>Omid</forenames></author><author><keyname>Hosung</keyname><forenames>Chun</forenames></author><author><keyname>Ranasinghe</keyname><forenames>Damith</forenames></author><author><keyname>Skafidas</keyname><forenames>Stan</forenames></author></authors><title>mrPUF: A Memristive Device based Physical Unclonable Function</title><categories>cond-mat.other cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical unclonable functions (PUFs) exploit the intrinsic complexity and
irreproducibility of physical systems to generate secret information. PUFs have
the potential to provide fundamentally higher security than traditional
cryptographic methods by preventing the cloning of identities and the
extraction of secret keys. One unique and exciting opportunity is that of using
the super-high information content (SHIC) capability of nanocrossbar
architecture as well as the high resistance programming variation of resistive
memories to develop a highly secure on-chip PUFs for extremely resource
constrained devices characterized by limited power and area budgets such as
passive Radio Frequency Identification (RFID) devices. We show how to implement
PUF based on nano-scale memristive (resistive memory) devices (mrPUF). Our
proposed architecture significantly increased the number of possible
challenge-response pairs (CRPs), while also consuming relatively lesser power
(around 70 uW). The presented approach can be used in other silicon-based PUFs
as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2193</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2193</id><created>2013-02-08</created><authors><author><keyname>Li</keyname><forenames>Zheng</forenames></author><author><keyname>O'Brien</keyname><forenames>Liam</forenames></author><author><keyname>Zhang</keyname><forenames>He</forenames></author></authors><title>Circumstantial-Evidence-Based Judgment for Software Effort Estimation</title><categories>cs.SE</categories><comments>10 pages, Proceedings of the 1st International Workshop on Evidential
  Assessment of Software Technologies (EAST 2011), pp. 18-27, Beijing, China,
  June 08-11, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expert judgment for software effort estimation is oriented toward direct
evidences that refer to actual effort of similar projects or activities through
experts' experiences. However, the availability of direct evidences implies the
requirement of suitable experts together with past data. The
circumstantial-evidence-based judgment proposed in this paper focuses on the
development experiences deposited in human knowledge, and can then be used to
qualitatively estimate implementation effort of different proposals of a new
project by rational inference. To demonstrate the process of
circumstantial-evidence-based judgment, this paper adopts propositional
learning theory based diagnostic reasoning to infer and compare different
effort estimates when implementing a Web service composition project with some
different techniques and contexts. The exemplar shows our proposed work can
help determine effort tradeoff before project implementation. Overall,
circumstantial-evidence-based judgment is not an alternative but complementary
to expert judgment so as to facilitate and improve software effort estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2197</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2197</id><created>2013-02-09</created><authors><author><keyname>Li</keyname><forenames>Zheng</forenames></author><author><keyname>Zhang</keyname><forenames>He</forenames></author><author><keyname>O'Brien</keyname><forenames>Liam</forenames></author></authors><title>Towards Technology Independent Strategies for SOA Implementations</title><categories>cs.SE cs.DC</categories><comments>12 pages, Proceedings of the 6th International Conference on
  Evaluation of Novel Approaches to Software Engineering (ENASE 2011), pp.
  143-154, Beijing, China, June 08-11, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Benefiting from the technology based strategies, Service-Oriented
Architecture (SOA) has been able to achieve the general goals such as agility,
flexibility, reusability and efficiency. Nevertheless, technical conditions
alone cannot guarantee successful SOA implementations. As a valuable and
necessary supplement, the space of technology independent strategies should
also be explored. Through treating SOA system as an instance of organization
and identifying the common ground on the similar process of SOA implementation
and organization design, this paper uses existing work in organization theory
area to inspire the research into technology independent strategies of SOA
implementation. As a result, four preliminary strategies that can be applied to
organizational area we identify to support SOA implementations. Furthermore,
the novel methodology of investigating technology independent strategies for
implementing SOA is revealed, which encourages interdisciplinary research
across service-oriented computing and organization theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2199</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2199</id><created>2013-02-09</created><authors><author><keyname>Li</keyname><forenames>Zheng</forenames></author><author><keyname>Keung</keyname><forenames>Jacky</forenames></author></authors><title>Software Cost Estimation Framework for Service-Oriented Architecture
  Systems using Divide-and-Conquer Approach</title><categories>cs.SE cs.DC</categories><comments>8 pages, Proceedings of the 5th International Symposium on
  Service-Oriented System Engineering (SOSE 2010), pp. 47-54, Nanjing, China,
  June 4-5, 2010. arXiv admin note: text overlap with arXiv:1302.1912</comments><doi>10.1109/SOSE.2010.29</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the complexity of Service-Oriented Architecture (SOA), cost and effort
estimation for SOA-based software development is more difficult than that for
traditional software development. Unfortunately, there is a lack of published
work about cost and effort estimation for SOA-based software. Existing cost
estimation approaches are inadequate to address the complex service-oriented
systems. This paper proposes a novel framework based on Divide-and-Conquer
(D&amp;C) for cost estimation for building SOA-based software. By dealing with
separately development parts, the D&amp;C framework can help organizations simplify
and regulate SOA implementation cost estimation. Furthermore, both cost
estimation modeling and software sizing work can be satisfied respectively by
switching the corresponding metrics within this framework. Given the
requirement of developing these metrics, this framework also defines the future
research in four different directions according to the separate cost estimation
sub-problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2201</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2201</id><created>2013-02-09</created><authors><author><keyname>Li</keyname><forenames>Zheng</forenames></author><author><keyname>O'Brien</keyname><forenames>Liam</forenames></author><author><keyname>Keung</keyname><forenames>Jacky</forenames></author><author><keyname>Xu</keyname><forenames>Xiwei</forenames></author></authors><title>Effort-Oriented Classification Matrix of Web Service Composition</title><categories>cs.SE cs.DC</categories><comments>6 pages, Proceedings of the 5th International Conference on Internet
  and Web Applications and Services (ICIW 2010), pp. 357-362, Barcelona, Spain,
  May 9-15, 2010</comments><doi>10.1109/ICIW.2010.59</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the service-oriented computing domain, Web service composition is an
effective realization to satisfy the rapidly changing requirements of business.
Therefore, the research into Web service composition has unfolded broadly.
Since examining all of the related work in this area becomes a mission next to
impossible, the classification of composition approaches can be used to
facilitate multiple research tasks. However, the current attempts to classify
Web service composition do not have clear objectives. Furthermore, the contexts
and technologies of composition approaches are confused in the existing
classifications. This paper proposes an effort-oriented classification matrix
for Web service composition, which distinguishes between the context and
technology dimension. The context dimension is aimed at analyzing the
environment influence on the effort of Web service composition, while the
technology dimension focuses on the technique influence on the effort.
Consequently, besides the traditional classification benefits, this matrix can
be used to build the basis of cost estimation for Web service composition in
future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2202</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2202</id><created>2013-02-09</created><authors><author><keyname>Li</keyname><forenames>Zheng</forenames></author><author><keyname>O'Brien</keyname><forenames>Liam</forenames></author><author><keyname>Cai</keyname><forenames>Rainbow</forenames></author><author><keyname>Zhang</keyname><forenames>He</forenames></author></authors><title>Building an Expert System for Evaluation of Commercial Cloud Services</title><categories>cs.DC cs.CY</categories><comments>8 page, Proceedings of the 2012 International Conference on Cloud and
  Service Computing (CSC 2012), pp. 168-175, Shanghai, China, November 22-24,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Commercial Cloud services have been increasingly supplied to customers in
industry. To facilitate customers' decision makings like cost-benefit analysis
or Cloud provider selection, evaluation of those Cloud services are becoming
more and more crucial. However, compared with evaluation of traditional
computing systems, more challenges will inevitably appear when evaluating
rapidly-changing and user-uncontrollable commercial Cloud services. This paper
proposes an expert system for Cloud evaluation that addresses emerging
evaluation challenges in the context of Cloud Computing. Based on the knowledge
and data accumulated by exploring the existing evaluation work, this expert
system has been conceptually validated to be able to give suggestions and
guidelines for implementing new evaluation experiments. As such, users can
conveniently obtain evaluation experiences by using this expert system, which
is essentially able to make existing efforts in Cloud services evaluation
reusable and sustainable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2203</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2203</id><created>2013-02-09</created><authors><author><keyname>Li</keyname><forenames>Zheng</forenames></author><author><keyname>O'Brien</keyname><forenames>Liam</forenames></author><author><keyname>Zhang</keyname><forenames>He</forenames></author><author><keyname>Cai</keyname><forenames>Rainbow</forenames></author></authors><title>A Factor Framework for Experimental Design for Performance Evaluation of
  Commercial Cloud Services</title><categories>cs.DC</categories><comments>8 pages, Proceedings of the 4th International Conference on Cloud
  Computing Technology and Science (CloudCom 2012), pp. 169-176, Taipei,
  Taiwan, December 03-06, 2012</comments><doi>10.1109/CloudCom.2012.6427525</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the diversity of commercial Cloud services, performance evaluations of
candidate services would be crucial and beneficial for both service customers
(e.g. cost-benefit analysis) and providers (e.g. direction of service
improvement). Before an evaluation implementation, the selection of suitable
factors (also called parameters or variables) plays a prerequisite role in
designing evaluation experiments. However, there seems a lack of systematic
approaches to factor selection for Cloud services performance evaluation. In
other words, evaluators randomly and intuitively concerned experimental factors
in most of the existing evaluation studies. Based on our previous taxonomy and
modeling work, this paper proposes a factor framework for experimental design
for performance evaluation of commercial Cloud services. This framework
capsules the state-of-the-practice of performance evaluation factors that
people currently take into account in the Cloud Computing domain, and in turn
can help facilitate designing new experiments for evaluating Cloud services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2206</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2206</id><created>2013-02-09</created><authors><author><keyname>Hamodi</keyname><forenames>Jamil M.</forenames></author><author><keyname>Thool</keyname><forenames>Ravindra C.</forenames></author></authors><title>Performance Evaluation of IPTV over WiMAX Networks Under Different
  Terrain Environments</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1302.1409, and
  substantial text overlap with other internet sources by other authors</comments><journal-ref>International Journal of Engineering Inventions, Volume 2, Issue 2
  (January 2013) PP: 21-25</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deployment Video on Demand (VoD) over the next generation (WiMAX) has become
one of the intense interest subjects in the research these days, and is
expected to be the main revenue generators in the near future. In this paper,
the performance of Quilty of Service of video streaming (IPTV) over fixed
mobile WiMax network is investigated under different terrain environments,
namely Free Space, Outdoor to Indoor and Pedestrian. OPNET is used to
investigate the performance of VoD over WiMAX. Our findings analyzing different
network statistics such as packet lost, path loss, delay, network throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2217</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2217</id><created>2013-02-09</created><authors><author><keyname>Dubois</keyname><forenames>Swan</forenames><affiliation>LPD, EPFL</affiliation></author><author><keyname>Guerraoui</keyname><forenames>Rachid</forenames><affiliation>LPD, EPFL</affiliation></author></authors><title>Introducing Speculation in Self-Stabilization - An Application to Mutual
  Exclusion</title><categories>cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-stabilization ensures that, after any transient fault, the system
recovers in a finite time and eventually exhibits. Speculation consists in
guaranteeing that the system satisfies its requirements for any execution but
exhibits significantly better performances for a subset of executions that are
more probable. A speculative protocol is in this sense supposed to be both
robust and efficient in practice. We introduce the notion of speculative
stabilization which we illustrate through the mutual exclusion problem. We then
present a novel speculatively stabilizing mutual exclusion protocol. Our
protocol is self-stabilizing for any asynchronous execution. We prove that its
stabilization time for synchronous executions is diam(g)/2 steps (where diam(g)
denotes the diameter of the system). This complexity result is of independent
interest. The celebrated mutual exclusion protocol of Dijkstra stabilizes in n
steps (where n is the number of processes) in synchronous executions and the
question whether the stabilization time could be strictly smaller than the
diameter has been open since then (almost 40 years). We show that this is
indeed possible for any underlying topology. We also provide a lower bound
proof that shows that our new stabilization time of diam(g)/2 steps is optimal
for synchronous executions, even if asynchronous stabilization is not required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2222</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2222</id><created>2013-02-09</created><authors><author><keyname>Horvat</keyname><forenames>Marko</forenames></author><author><keyname>Gledec</keyname><forenames>Gordan</forenames></author><author><keyname>Bogunovi&#x107;</keyname><forenames>Nikola</forenames></author></authors><title>Ontology-Based Administration of Web Directories</title><categories>cs.IR cs.DL</categories><comments>22 pages, 9 figures</comments><journal-ref>Lecture Notes in Computer Science, Transactions on Computational
  Collective Intelligence I., 6220, pp. 101-120 (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Administration of a Web directory and maintenance of its content and the
associated structure is a delicate and labor intensive task performed
exclusively by human domain experts. Subsequently there is an imminent risk of
a directory structures becoming unbalanced, uneven and difficult to use to all
except for a few users proficient with the particular Web directory and its
domain. These problems emphasize the need to establish two important issues: i)
generic and objective measures of Web directories structure quality, and ii)
mechanism for fully automated development of a Web directory's structure. In
this paper we demonstrate how to formally and fully integrate Web directories
with the Semantic Web vision. We propose a set of criteria for evaluation of a
Web directory's structure quality. Some criterion functions are based on
heuristics while others require the application of ontologies. We also suggest
an ontology-based algorithm for construction of Web directories. By using
ontologies to describe the semantics of Web resources and Web directories'
categories it is possible to define algorithms that can build or rearrange the
structure of a Web directory. Assessment procedures can provide feedback and
help steer the ontology-based construction process. The issues raised in the
article can be equally applied to new and existing Web directories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2223</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2223</id><created>2013-02-09</created><authors><author><keyname>Horvat</keyname><forenames>Marko</forenames></author><author><keyname>Grbin</keyname><forenames>Anton</forenames></author><author><keyname>Gledec</keyname><forenames>Gordan</forenames></author></authors><title>WNtags: A Web-Based Tool For Image Labeling And Retrieval With Lexical
  Ontologies</title><categories>cs.IR cs.AI cs.MM</categories><comments>10 pages, 3 figures, published in 16th International Conference on
  Knowledge-Based and Intelligent Information &amp; Engineering Systems, 10-12 Sep
  2012, San Sebastian, Spain</comments><journal-ref>Frontiers in artificial intelligence and applications, 243, pp.
  585-594 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ever growing number of image documents available on the Internet continuously
motivates research in better annotation models and more efficient retrieval
methods. Formal knowledge representation of objects and events in pictures,
their interaction as well as context complexity becomes no longer an option for
a quality image repository, but a necessity. We present an ontology-based
online image annotation tool WNtags and demonstrate its usefulness in several
typical multimedia retrieval tasks using International Affective Picture System
emotionally annotated image database. WNtags is built around WordNet lexical
ontology but considers Suggested Upper Merged Ontology as the preferred
labeling formalism. WNtags uses sets of weighted WordNet synsets as high-level
image semantic descriptors and query matching is performed with word stemming
and node distance metrics. We also elaborate our near future plans to expand
image content description with induced affect as in stimuli for research of
human emotion and attention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2224</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2224</id><created>2013-02-09</created><authors><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Belkhir</keyname><forenames>Walid</forenames></author><author><keyname>Lenczner</keyname><forenames>Michel</forenames></author></authors><title>Computer-Aided Derivation of Multi-scale Models: A Rewriting Framework</title><categories>cs.SC</categories><comments>26 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We introduce a framework for computer-aided derivation of multi-scale models.
It relies on a combination of an asymptotic method used in the field of partial
differential equations with term rewriting techniques coming from computer
science.
  In our approach, a multi-scale model derivation is characterized by the
features taken into account in the asymptotic analysis. Its formulation
consists in a derivation of a reference model associated to an elementary
nominal model, and in a set of transformations to apply to this proof until it
takes into account the wanted features. In addition to the reference model
proof, the framework includes first order rewriting principles designed for
asymptotic model derivations, and second order rewriting principles dedicated
to transformations of model derivations. We apply the method to generate a
family of homogenized models for second order elliptic equations with periodic
coefficients that could be posed in multi-dimensional domains, with possibly
multi-domains and/or thin domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2227</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2227</id><created>2013-02-09</created><authors><author><keyname>Esfandiarpoor</keyname><forenames>Sina</forenames></author><author><keyname>Pahlavan</keyname><forenames>Ali</forenames></author><author><keyname>Goudarzi</keyname><forenames>Maziar</forenames></author></authors><title>Virtual Machine Consolidation for Datacenter Energy Improvement</title><categories>cs.DC</categories><comments>This is draft version. The finally version will be published</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid growth and proliferation of cloud computing services around the world
has increased the necessity and significance of improving the energy efficiency
of could implementations. Virtual machines (VM) comprise the backend of most,
if not all, cloud computing services. Several VMs are often consolidated on a
physical machine to better utilize its resources. We take into account the
cooling and network structure of the datacenter hosting the physical machines
when consolidating the VMs so that fewer racks and routers are employed,
without compromising the service-level agreements, so that unused routing and
cooling equipment can be turned off to reduce energy consumption. Our
experimental results on four benchmarks shows that our technique improves
energy consumption of servers, network equipment, and cooling systems by 2.5%,
18.8%, and 28.2% respectively, resulting in a total of 14.7% improvement on
average in the entire datacenter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2244</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2244</id><created>2013-02-09</created><authors><author><keyname>Xiong</keyname><forenames>Jiping</forenames></author><author><keyname>Zhao</keyname><forenames>Jian</forenames></author><author><keyname>Chen</keyname><forenames>Lei</forenames></author></authors><title>Efficient Data Gathering in Wireless Sensor Networks Based on Matrix
  Completion and Compressive Sensing</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gathering data in an energy efficient manner in wireless sensor networks is
an important design challenge. In wireless sensor networks, the readings of
sensors always exhibit intra-temporal and inter-spatial correlations.
Therefore, in this letter, we use low rank matrix completion theory to explore
the inter-spatial correlation and use compressive sensing theory to take
advantage of intra-temporal correlation. Our method, dubbed MCCS, can
significantly reduce the amount of data that each sensor must send through
network and to the sink, thus prolong the lifetime of the whole networks.
Experiments using real datasets demonstrate the feasibility and efficacy of our
MCCS method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2246</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2246</id><created>2013-02-09</created><authors><author><keyname>Randriam</keyname><forenames>Hugues</forenames></author><author><keyname>Sok</keyname><forenames>Lin</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>Patrick</forenames></author></authors><title>Lower bounds on the minimum distance of long codes in the Lee metric</title><categories>cs.IT math.IT</categories><report-no>13 pages, 7 figures</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Gilbert type bound for codes in the title is reviewed, both for small and
large alphabets. Constructive lower bounds better than these existential bounds
are derived from geometric codes, either over Fp or Fp2 ; or over even degree
extensions of Fp: In the latter case the approach is concatena- tion with a
good code for the Hamming metric as outer code and a short code for the Lee
metric as an inner code. In the former case lower bounds on the minimum Lee
distance are derived by algebraic geometric arguments inspired by results of
Wu, Kuijper, Udaya (2007).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2247</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2247</id><created>2013-02-09</created><updated>2014-03-09</updated><authors><author><keyname>Banerji</keyname><forenames>Sourangsu</forenames></author><author><keyname>Chowdhury</keyname><forenames>Rahul Singha</forenames></author></authors><title>Wi-Fi &amp; WiMAX: A Comparative Study</title><categories>cs.NI</categories><comments>5 pages, 4 figures, 1 table</comments><journal-ref>Indian Journal of Engineering, Volume 2,Issue 5, March 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Usually broadband wireless access networks are considered to be enterprise
level networks providing us with more capacity as well as coverage. We have
seen that in remote inaccessible areas wired networks are not at all cost
effective. Wireless networking has offered us an alternative solution for such
problem of information access. They have definitely changed the way people
communicate and share information among themselves by overcoming problems
nowadays associated with distance and location. This paper provides a
comparison and technical analysis of alternatives for implementing last-mile
wireless broadband services. It provides detailed technical differences between
802.11 (Wi-FI) wireless networks with 802.16 (WiMAX), a new technology that
solves many of the difficulties in last-mile implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2253</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2253</id><created>2013-02-09</created><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author></authors><title>Security and Privacy Challenges in Cognitive Wireless Sensor Networks</title><categories>cs.CR cs.NI</categories><comments>36 pages, 4 figures, 2 tables. The book chapter is accepted for
  publication in 2013</comments><journal-ref>Book Chapter in Cognitive Radio Technology Applications for
  Wireless and Mobile Ad hoc Networks, Natarajan Meghanathan and Y. B. Reddy
  (Eds.), IGI-Global, USA, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks (WSNs) have attracted a lot of interest in the
research community due to their potential applicability in a wide range of
real-world practical applications. However, due to the distributed nature and
their deployments in critical applications without human interventions and
sensitivity and criticality of data communicated, these networks are vulnerable
to numerous security and privacy threats that can adversely affect their
performance. These issues become even more critical in cognitive wireless
sensor networks (CWSNs) in which the sensor nodes have the capabilities of
changing their transmission and reception parameters according to the radio
environment under which they operate in order to achieve reliable and efficient
communication and optimum utilization of the network resources. This chapter
presents a comprehensive discussion on the security and privacy issues in CWSNs
by identifying various security threats in these networks and various defense
mechanisms to counter these vulnerabilities. Various types of attacks on CWSNs
are categorized under different classes based on their natures and targets, and
corresponding to each attack class, appropriate security mechanisms are also
discussed. Some critical research issues on security and privacy in CWSNs are
also identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2261</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2261</id><created>2013-02-09</created><updated>2013-07-09</updated><authors><author><keyname>Wootters</keyname><forenames>Mary</forenames></author></authors><title>On the list decodability of random linear codes with large error rates</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that a random q-ary code of rate \Omega(\epsilon^2) is list
decodable up to radius (1 - 1/q - \epsilon) with list sizes on the order of
1/\epsilon^2, with probability 1 - o(1). However, until recently, a similar
statement about random linear codes has until remained elusive. In a recent
paper, Cheraghchi, Guruswami, and Velingker show a connection between list
decodability of random linear codes and the Restricted Isometry Property from
compressed sensing, and use this connection to prove that a random linear code
of rate \Omega(\epsilon^2 / log^3(1/\epsilon)) achieves the list decoding
properties above, with constant probability. We improve on their result to show
that in fact we may take the rate to be \Omega(\epsilon^2), which is optimal,
and further that the success probability is 1 - o(1), rather than constant. As
an added benefit, our proof is relatively simple. Finally, we extend our
methods to more general ensembles of linear codes. As an example, we show that
randomly punctured Reed-Muller codes have the same list decoding properties as
the original codes, even when the rate is improved to a constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2271</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2271</id><created>2013-02-09</created><updated>2015-04-30</updated><authors><author><keyname>Barequet</keyname><forenames>Gill</forenames></author><author><keyname>Cannon</keyname><forenames>Sarah M.</forenames></author><author><keyname>Fox-Epstein</keyname><forenames>Eli</forenames></author><author><keyname>Hescott</keyname><forenames>Benjamin</forenames></author><author><keyname>Souvaine</keyname><forenames>Diane L.</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>Csaba D.</forenames></author><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author></authors><title>Diffuse Reflection Diameter in Simple Polygons</title><categories>cs.CG</categories><comments>To appear in Discrete Applied Mathematics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a conjecture of Aanjaneya, Bishnu, and Pal that the minimum number
of diffuse reflections sufficient to illuminate the interior of any simple
polygon with $n$ walls from any interior point light source is $\lfloor n/2
\rfloor - 1$. Light reflecting diffusely leaves a surface in all directions,
rather than at an identical angle as with specular reflections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2273</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2273</id><created>2013-02-09</created><authors><author><keyname>Garg</keyname><forenames>Pranav</forenames></author><author><keyname>Loding</keyname><forenames>Christof</forenames></author><author><keyname>Madhusudan</keyname><forenames>P.</forenames></author><author><keyname>Neider</keyname><forenames>Daniel</forenames></author></authors><title>Learning Universally Quantified Invariants of Linear Data Structures</title><categories>cs.PL cs.FL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new automaton model, called quantified data automata over words,
that can model quantified invariants over linear data structures, and build
poly-time active learning algorithms for them, where the learner is allowed to
query the teacher with membership and equivalence queries. In order to express
invariants in decidable logics, we invent a decidable subclass of QDAs, called
elastic QDAs, and prove that every QDA has a unique
minimally-over-approximating elastic QDA. We then give an application of these
theoretically sound and efficient active learning algorithms in a passive
learning framework and show that we can efficiently learn quantified linear
data structure invariants from samples obtained from dynamic runs for a large
class of programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2277</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2277</id><created>2013-02-09</created><updated>2013-02-17</updated><authors><author><keyname>Deng</keyname><forenames>Houtao</forenames></author><author><keyname>Runger</keyname><forenames>George</forenames></author><author><keyname>Tuv</keyname><forenames>Eugene</forenames></author><author><keyname>Vladimir</keyname><forenames>Martyanov</forenames></author></authors><title>A Time Series Forest for Classification and Feature Extraction</title><categories>cs.LG</categories><journal-ref>Information Sciences 239: 142-153 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a tree ensemble method, referred to as time series forest (TSF),
for time series classification. TSF employs a combination of the entropy gain
and a distance measure, referred to as the Entrance (entropy and distance)
gain, for evaluating the splits. Experimental studies show that the Entrance
gain criterion improves the accuracy of TSF. TSF randomly samples features at
each tree node and has a computational complexity linear in the length of a
time series and can be built using parallel computing techniques such as
multi-core computing used here. The temporal importance curve is also proposed
to capture the important temporal characteristics useful for classification.
Experimental studies show that TSF using simple features such as mean,
deviation and slope outperforms strong competitors such as one-nearest-neighbor
classifiers with dynamic time warping, is computationally efficient, and can
provide insights into the temporal characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2279</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2279</id><created>2013-02-09</created><authors><author><keyname>Yang</keyname><forenames>Fan</forenames></author></authors><title>Expressing Second-order Sentences in Intuitionistic Dependence Logic</title><categories>math.LO cs.LO</categories><comments>18 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Intuitionistic dependence logic was introduced by Abramsky and Vaananen
(2009) as a variant of dependence logic under a general construction of Hodges'
(trump) team semantics. It was proven that there is a translation from
intuitionistic dependence logic sentences into second order logic sentences. In
this paper, we prove that the other direction is also true, therefore
intuitionistic dependence logic is equivalent to second order logic on the
level of sentences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2308</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2308</id><created>2013-02-10</created><authors><author><keyname>James</keyname><forenames>Joshua I.</forenames></author><author><keyname>Gladyshev</keyname><forenames>Pavel</forenames></author><author><keyname>Abdullah</keyname><forenames>Mohd Taufik</forenames></author><author><keyname>Zhu</keyname><forenames>Yuandong</forenames></author></authors><title>Analysis of Evidence Using Formal Event Reconstruction</title><categories>cs.FL cs.CR</categories><comments>10 pages, 11 figures, Presented at the 1st International Conference
  on Digital Forensics &amp; Cyber Crime</comments><journal-ref>James, J.I., P. Gladyshev, M. Abdullah, Y. Zhu (2010) &quot;Analysis of
  Evidence Using Formal Event Reconstruction&quot;. Digital Forensics and Cyber
  Crime. Vol 31. pp 85-98. Springer</journal-ref><doi>10.1007/978-3-642-11534-9_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper expands upon the finite state machine approach for the formal
analysis of digital evidence. The proposed method may be used to support the
feasibility of a given statement by testing it against a relevant system model.
To achieve this, a novel method for modeling the system and evidential
statements is given. The method is then examined in a case study example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2315</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2315</id><created>2013-02-10</created><authors><author><keyname>Kang</keyname><forenames>Sinuk</forenames></author></authors><title>A sampling theorem on shift-invariant spaces associated with the
  fractional Fourier transform domain</title><categories>math.FA cs.IT math.IT</categories><msc-class>94A20, 42C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a generalization of the Fourier transform, the fractional Fourier
transform was introduced and has been further investigated both in theory and
in applications of signal processing. We obtain a sampling theorem on
shift-invariant spaces associated with the fractional Fourier transform domain.
The resulting sampling theorem extends not only the classical
Whittaker-Shannon-Kotelnikov sampling theorem associated with the fractional
Fourier transform domain, but also extends the prior sampling theorems on
shift-invariant spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2318</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2318</id><created>2013-02-10</created><authors><author><keyname>Sirotkin</keyname><forenames>Pavel</forenames></author></authors><title>On Search Engine Evaluation Metrics</title><categories>cs.IR</categories><comments>Doctoral thesis. 192 pages</comments><acm-class>H.3.3; H.3.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The search engine evaluation research has quite a lot metrics available to
it. Only recently, the question of the significance of individual metrics
started being raised, as these metrics' correlations to real-world user
experiences or performance have generally not been well-studied. The first part
of this thesis provides an overview of previous literature on the evaluation of
search engine evaluation metrics themselves, as well as critiques of and
comments on individual studies and approaches. The second part introduces a
meta-evaluation metric, the Preference Identification Ratio (PIR), that
quantifies the capacity of an evaluation metric to capture users' preferences.
Also, a framework for simultaneously evaluating many metrics while varying
their parameters and evaluation standards is introduced. Both PIR and the
meta-evaluation framework are tested in a study which shows some interesting
preliminary results; in particular, the unquestioning adherence to metrics or
their ad hoc parameters seems to be disadvantageous. Instead, evaluation
methods should themselves be rigorously evaluated with regard to goals set for
a particular study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2330</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2330</id><created>2013-02-10</created><updated>2013-04-26</updated><authors><author><keyname>Qin</keyname><forenames>Haohao</forenames></author><author><keyname>Sun</keyname><forenames>Yin</forenames></author><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author><author><keyname>Chen</keyname><forenames>Xiang</forenames></author><author><keyname>Chi</keyname><forenames>Chong-Yung</forenames></author><author><keyname>Zhao</keyname><forenames>Ming</forenames></author><author><keyname>Wang</keyname><forenames>Jing</forenames></author></authors><title>Power Allocation and Time-Domain Artificial Noise Design for Wiretap
  OFDM with Discrete Inputs</title><categories>cs.IT math.IT</categories><comments>12 pages, 7 figures, accepted by IEEE Transactions on Wireless
  Communications, Jan. 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal power allocation for orthogonal frequency division multiplexing
(OFDM) wiretap channels with Gaussian channel inputs has already been studied
in some previous works from an information theoretical viewpoint. However,
these results are not sufficient for practical system design. One reason is
that discrete channel inputs, such as quadrature amplitude modulation (QAM)
signals, instead of Gaussian channel inputs, are deployed in current practical
wireless systems to maintain moderate peak transmission power and receiver
complexity. In this paper, we investigate the power allocation and artificial
noise design for OFDM wiretap channels with discrete channel inputs. We first
prove that the secrecy rate function for discrete channel inputs is nonconcave
with respect to the transmission power. To resolve the corresponding nonconvex
secrecy rate maximization problem, we develop a low-complexity power allocation
algorithm, which yields a duality gap diminishing in the order of
O(1/\sqrt{N}), where N is the number of subcarriers of OFDM. We then show that
independent frequency-domain artificial noise cannot improve the secrecy rate
of single-antenna wiretap channels. Towards this end, we propose a novel
time-domain artificial noise design which exploits temporal degrees of freedom
provided by the cyclic prefix of OFDM systems {to jam the eavesdropper and
boost the secrecy rate even with a single antenna at the transmitter}.
Numerical results are provided to illustrate the performance of the proposed
design schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2331</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2331</id><created>2013-02-10</created><authors><author><keyname>Donoho</keyname><forenames>David L.</forenames></author><author><keyname>Gavish</keyname><forenames>Matan</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>The Phase Transition of Matrix Recovery from Gaussian Measurements
  Matches the Minimax MSE of Matrix Denoising</title><categories>cs.IT math.IT math.ST stat.TH</categories><doi>10.1073/pnas.1306110110</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Let $X_0$ be an unknown $M$ by $N$ matrix. In matrix recovery, one takes $n &lt;
MN$ linear measurements $y_1,..., y_n$ of $X_0$, where $y_i = \Tr(a_i^T X_0)$
and each $a_i$ is a $M$ by $N$ matrix. For measurement matrices with Gaussian
i.i.d entries, it known that if $X_0$ is of low rank, it is recoverable from
just a few measurements. A popular approach for matrix recovery is Nuclear Norm
Minimization (NNM). Empirical work reveals a \emph{phase transition} curve,
stated in terms of the undersampling fraction $\delta(n,M,N) = n/(MN)$, rank
fraction $\rho=r/N$ and aspect ratio $\beta=M/N$. Specifically, a curve
$\delta^* = \delta^*(\rho;\beta)$ exists such that, if $\delta &gt;
\delta^*(\rho;\beta)$, NNM typically succeeds, while if $\delta &lt;
\delta^*(\rho;\beta)$, it typically fails. An apparently quite different
problem is matrix denoising in Gaussian noise, where an unknown $M$ by $N$
matrix $X_0$ is to be estimated based on direct noisy measurements $Y = X_0 +
Z$, where the matrix $Z$ has iid Gaussian entries. It has been empirically
observed that, if $X_0$ has low rank, it may be recovered quite accurately from
the noisy measurement $Y$. A popular matrix denoising scheme solves the
unconstrained optimization problem $\text{min} \| Y - X \|_F^2/2 + \lambda
\|X\|_* $. When optimally tuned, this scheme achieves the asymptotic minimax
MSE $\cM(\rho) = \lim_{N \goto \infty} \inf_\lambda \sup_{\rank(X) \leq \rho
\cdot N} MSE(X,\hat{X}_\lambda)$. We report extensive experiments showing that
the phase transition $\delta^*(\rho)$ in the first problem coincides with the
minimax risk curve $\cM(\rho)$ in the second problem, for {\em any} rank
fraction $0 &lt; \rho &lt; 1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2339</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2339</id><created>2013-02-10</created><authors><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Robust Low-Rank LCMV Beamforming Algorithms Based on Joint Iterative
  Optimization Strategies</title><categories>cs.IT math.IT</categories><comments>7 figures. arXiv admin note: substantial text overlap with
  arXiv:1205.4391</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter presents reduced-rank linearly constrained minimum variance
(LCMV) algorithms based on the concept of joint iterative optimization of
parameters. The proposed reduced-rank scheme is based on a constrained robust
joint iterative optimization (RJIO) of parameters according to the minimum
variance criterion. The robust optimization procedure adjusts the parameters of
a rank-reduction matrix, a reduced-rank beamformer and the diagonal loading in
an alternating manner. LCMV expressions are developed for the design of the
rank-reduction matrix and the reduced-rank beamformer. Stochastic gradient and
recursive least-squares adaptive algorithms are then devised for an efficient
implementation of the RJIO robust beamforming technique. Simulations for a
application in the presence of uncertainties show that the RJIO scheme and
algorithms outperform in convergence and tracking performances existing
algorithms while requiring a comparable complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2340</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2340</id><created>2013-02-10</created><updated>2013-04-28</updated><authors><author><keyname>Avis</keyname><forenames>David</forenames></author><author><keyname>Tiwary</keyname><forenames>Hans Raj</forenames></author></authors><title>On the extension complexity of combinatorial polytopes</title><categories>math.CO cs.CC</categories><comments>15 pages, 3 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we extend recent results of Fiorini et al. on the extension
complexity of the cut polytope and related polyhedra. We first describe a
lifting argument to show exponential extension complexity for a number of
NP-complete problems including subset-sum and three dimensional matching. We
then obtain a relationship between the extension complexity of the cut polytope
of a graph and that of its graph minors. Using this we are able to show
exponential extension complexity for the cut polytope of a large number of
graphs, including those used in quantum information and suspensions of cubic
planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2343</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2343</id><created>2013-02-10</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Adaptive Space-Time Beamforming in Radar Systems</title><categories>cs.IT math.IT</categories><comments>6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this chapter is to review the recent work and advances in the
area of space-time beamforming algorithms and their application to radar
systems. These systems include phased-array \cite{melvin} and multi-input
multi-output (MIMO) radar systems \cite{haimo_08}, mono-static and bi-static
radar systems and other configurations \cite{melvin}. Furthermore, this chapter
also describes in detail some of the most successful space-time beamforming
algorithms that exploit low-rank and sparsity properties as well as the use of
prior-knowledge to improve the performance of STAP algorithms in radar systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2370</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2370</id><created>2013-02-10</created><authors><author><keyname>Cadek</keyname><forenames>Martin</forenames></author><author><keyname>Krcal</keyname><forenames>Marek</forenames></author><author><keyname>Matousek</keyname><forenames>Jiri</forenames></author><author><keyname>Vokrinek</keyname><forenames>Lukas</forenames></author><author><keyname>Wagner</keyname><forenames>Uli</forenames></author></authors><title>Extendability of continuous maps is undecidable</title><categories>cs.CG math.AT</categories><comments>38 pages</comments><msc-class>68U05, 68W99, 68Q17, 55S35, 55S36, 55P99, 55Q05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two basic problems of algebraic topology, the extension problem
and the computation of higher homotopy groups, from the point of view of
computability and computational complexity. The extension problem is the
following: Given topological spaces X and Y, a subspace A\subseteq X, and a
(continuous) map f:A-&gt;Y, decide whether f can be extended to a continuous map
\bar{f}:X-&gt;Y. All spaces are given as finite simplicial complexes and the map f
is simplicial. Recent positive algorithmic results, proved in a series of
companion papers, show that for (k-1)-connected Y, k&gt;=2, the extension problem
is algorithmically solvable if the dimension of X is at most 2k-1, and even in
polynomial time when k is fixed. Here we show that the condition \dim X&lt;=2k-1
cannot be relaxed: for \dim X=2k, the extension problem with (k-1)-connected Y
becomes undecidable. Moreover, either the target space Y or the pair (X,A) can
be fixed in such a way that the problem remains undecidable. Our second result,
a strengthening of a result of Anick, says that the computation of \pi_k(Y) of
a 1-connected simplicial complex Y is #P-hard when k is considered as a part of
the input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2376</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2376</id><created>2013-02-10</created><authors><author><keyname>Shafiq</keyname><forenames>M. Zubair</forenames></author><author><keyname>Liu</keyname><forenames>Alex X.</forenames></author></authors><title>Modeling Morphology of Social Network Cascades</title><categories>cs.SI physics.soc-ph</categories><comments>12 pages, technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cascades represent an important phenomenon across various disciplines such as
sociology, economy, psychology, political science, marketing, and epidemiology.
An important property of cascades is their morphology, which encompasses the
structure, shape, and size. However, cascade morphology has not been rigorously
characterized and modeled in prior literature. In this paper, we propose a
Multi-order Markov Model for the Morphology of Cascades ($M^4C$) that can
represent and quantitatively characterize the morphology of cascades with
arbitrary structures, shapes, and sizes. $M^4C$ can be used in a variety of
applications to classify different types of cascades. To demonstrate this, we
apply it to an unexplored but important problem in online social networks --
cascade size prediction. Our evaluations using real-world Twitter data show
that $M^4C$ based cascade size prediction scheme outperforms the baseline
scheme based on cascade graph features such as edge growth rate, degree
distribution, clustering, and diameter. $M^4C$ based cascade size prediction
scheme consistently achieves more than 90% classification accuracy under
different experimental scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2384</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2384</id><created>2013-02-10</created><updated>2013-02-12</updated><authors><author><keyname>Bendtsen</keyname><forenames>Jan</forenames></author><author><keyname>Sridharan</keyname><forenames>Srinivas</forenames></author></authors><title>Efficient Desynchronization of Thermostatically Controlled Loads</title><categories>math.OC cs.SY</categories><comments>6 pages, 8 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers demand side management in smart power grid systems
containing significant numbers of thermostatically controlled loads such as air
conditioning systems, heat pumps, etc. Recent studies have shown that the
overall power consumption of such systems can be regulated up and down
centrally by broadcasting small setpoint change commands without significantly
impacting consumer comfort. However, sudden simultaneous setpoint changes
induce undesirable power consumption oscillations due to sudden synchronization
of the on/off cycles of the individual units. In this paper, we present a novel
algorithm for counter-acting these unwanted oscillations, which requires
neither central management of the individual units nor communication between
units. We present a formal proof of convergence of homogeneous populations to
desynchronized status, as well as simulations that indicate that the algorithm
is able to effectively dampen power consumption oscillations for both
homogeneous and heterogeneous populations of thermostatically controlled loads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2395</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2395</id><created>2013-02-10</created><authors><author><keyname>James</keyname><forenames>Joshua I.</forenames></author><author><keyname>Gladyshev</keyname><forenames>Pavel</forenames></author><author><keyname>Zhu</keyname><forenames>Yuandong</forenames></author></authors><title>Signature Based Detection of User Events for Post-Mortem Forensic
  Analysis</title><categories>cs.CR</categories><comments>15 pages, 4 figures, 5 tables, 1 appendix, 2nd International
  Conference on Digital Forensics and Cyber Crime</comments><journal-ref>James, J.I., P. Gladyshev, Y. Zhu. (2011) &quot;Signature Based
  Detection of User Events for Post-Mortem Forensic Analysis&quot;. Digital
  Forensics and Cyber Crime. Vol 53. pp 96-109. Springer</journal-ref><doi>10.1007/978-3-642-19513-6_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel approach to user event reconstruction by
showing the practicality of generating and implementing signature-based
analysis methods to reconstruct high-level user actions from a collection of
low-level traces found during a post-mortem forensic analysis of a system.
Traditional forensic analysis and the inferences an investigator normally makes
when given digital evidence, are examined. It is then demonstrated that this
natural process of inferring high-level events from low-level traces may be
encoded using signature-matching techniques. Simple signatures using the
defined method are created and applied for three popular Windows-based programs
as a proof of concept.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2405</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2405</id><created>2013-02-11</created><updated>2014-04-04</updated><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Zhang</keyname><forenames>Yaqiong</forenames></author></authors><title>Acyclic edge coloring of graphs</title><categories>math.CO cs.DM</categories><comments>19 pages</comments><journal-ref>Discrete Appl. Math., 167 (2014) 290--303</journal-ref><doi>10.1016/j.dam.2013.12.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An {\em acyclic edge coloring} of a graph $G$ is a proper edge coloring such
that the subgraph induced by any two color classes is a linear forest (an
acyclic graph with maximum degree at most two). The {\em acyclic chromatic
index} $\chiup_{a}'(G)$ of a graph $G$ is the least number of colors needed in
an acyclic edge coloring of $G$. Fiam\v{c}\'{i}k (1978) conjectured that
$\chiup_{a}'(G) \leq \Delta(G) + 2$, where $\Delta(G)$ is the maximum degree of
$G$. This conjecture is well known as Acyclic Edge Coloring Conjecture (AECC).
A graph $G$ with maximum degree at most $\kappa$ is {\em
$\kappa$-deletion-minimal} if $\chiup_{a}'(G) &gt; \kappa$ and $\chiup_{a}'(H)
\leq \kappa$ for every proper subgraph $H$ of $G$. The purpose of this paper is
to provide many structural lemmas on $\kappa$-deletion-minimal graphs. By using
the structural lemmas, we firstly prove that AECC is true for the graphs with
maximum average degree less than four (\autoref{NMAD4}). We secondly prove that
AECC is true for the planar graphs without triangles adjacent to cycles of
length at most four, with an additional condition that every $5$-cycle has at
most three edges contained in triangles (\autoref{NoAdjacent}), from which we
can conclude some known results as corollaries. We thirdly prove that every
planar graph $G$ without intersecting triangles satisfies $\chiup_{a}'(G) \leq
\Delta(G) + 3$ (\autoref{NoIntersect}). Finally, we consider one extreme case
and prove it: if $G$ is a graph with $\Delta(G) \geq 3$ and all the
$3^{+}$-vertices are independent, then $\chiup_{a}'(G) = \Delta(G)$. We hope
the structural lemmas will shed some light on the acyclic edge coloring
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2420</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2420</id><created>2013-02-11</created><authors><author><keyname>Wu</keyname><forenames>Xiaofu</forenames></author><author><keyname>Yang</keyname><forenames>Zhen</forenames></author><author><keyname>Gan</keyname><forenames>Lu</forenames></author></authors><title>Compressed Sensing with Incremental Sparse Measurements</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures, submitted to SampTA2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a verification-based decoding approach for reconstruction
of a sparse signal with incremental sparse measurements. In its first step, the
verification-based decoding algorithm is employed to reconstruct the signal
with a fixed number of sparse measurements. Often, it may fail as the number of
sparse measurements may be not enough, possibly due to an underestimate of the
signal sparsity. However, we observe that even if this first recovery fails,
many component samples of the sparse signal have been identified. Hence, it is
natural to further employ incremental measurements tuned to the unidentified
samples with known locations. This approach has been proven very efficiently by
extensive simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2426</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2426</id><created>2013-02-11</created><authors><author><keyname>Asinowski</keyname><forenames>Andrei</forenames></author><author><keyname>Cardinal</keyname><forenames>Jean</forenames></author><author><keyname>Cohen</keyname><forenames>Nathann</forenames></author><author><keyname>Collette</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Hackl</keyname><forenames>Thomas</forenames></author><author><keyname>Hoffmann</keyname><forenames>Michael</forenames></author><author><keyname>Knauer</keyname><forenames>Kolja</forenames></author><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author><author><keyname>Laso&#x144;</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Micek</keyname><forenames>Piotr</forenames></author><author><keyname>Rote</keyname><forenames>G&#xfc;nter</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author></authors><title>Coloring Hypergraphs Induced by Dynamic Point Sets and Bottomless
  Rectangles</title><categories>cs.CG cs.DS</categories><comments>A preliminary version was presented by a subset of the authors to the
  European Workshop on Computational Geometry, held in Assisi (Italy) on March
  19-21, 2012</comments><journal-ref>In: Algorithms and Data Structures Symposium-WADS 2013, August
  2013, Editors: Frank Dehne, Roberto Solis-Oba, and J\&quot;org-R\&quot;udiger Sack,
  Lecture Notes in Computer Science, 8037, Springer-Verlag, 2013, pp. 73-84</journal-ref><doi>10.1007/978-3-642-40104-6_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a coloring problem on dynamic, one-dimensional point sets: points
appearing and disappearing on a line at given times. We wish to color them with
k colors so that at any time, any sequence of p(k) consecutive points, for some
function p, contains at least one point of each color.
  We prove that no such function p(k) exists in general. However, in the
restricted case in which points appear gradually, but never disappear, we give
a coloring algorithm guaranteeing the property at any time with p(k)=3k-2. This
can be interpreted as coloring point sets in R^2 with k colors such that any
bottomless rectangle containing at least 3k-2 points contains at least one
point of each color. Here a bottomless rectangle is an axis-aligned rectangle
whose bottom edge is below the lowest point of the set. For this problem, we
also prove a lower bound p(k)&gt;ck, where c&gt;1.67. Hence for every k there exists
a point set, every k-coloring of which is such that there exists a bottomless
rectangle containing ck points and missing at least one of the k colors.
  Chen et al. (2009) proved that no such function $p(k)$ exists in the case of
general axis-aligned rectangles. Our result also complements recent results
from Keszegh and Palvolgyi on cover-decomposability of octants (2011, 2012).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2427</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2427</id><created>2013-02-11</created><authors><author><keyname>Zeng</keyname><forenames>Weijun</forenames></author><author><keyname>Wu</keyname><forenames>Xiaofu</forenames></author><author><keyname>Yang</keyname><forenames>Zhen</forenames></author></authors><title>Turbo DPSK in Bi-directional Relaying</title><categories>cs.IT math.IT</categories><comments>5 pages, 7 figures, accepted for presentation in WCNC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, iterative differential phase-shift keying (DPSK) demodulation
and channel decoding scheme is investigated for the Joint Channel decoding and
physical layer Network Coding (JCNC) approach in two-way relaying systems. The
Bahl, Cocke, Jelinek, and Raviv (BCJR) algorithm for both coherent and
noncoherent detection is derived for soft-in soft-out decoding of DPSK
signalling over the two-user multiple-access channel with Rayleigh fading.
Then, we propose a pragmatic approach with the JCNC scheme for iteratively
exploiting the extrinsic information of the outer code. With coherent
detection, we show that DPSK can be well concatenated with simple convolutional
codes to achieve excellent coding gain just like in traditional point-to-point
communication scenarios. The proposed noncoherent detection, which essentially
requires that the channel keeps constant over two consecutive symbols, can work
without explicit channel estimation. Simulation results show that the iterative
processing converges very fast and most of the coding gain is obtained within
two iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2436</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2436</id><created>2013-02-11</created><authors><author><keyname>Ali</keyname><forenames>Mohd Mahmood</forenames></author><author><keyname>Qaseem</keyname><forenames>Mohd S</forenames></author><author><keyname>Rajamani</keyname><forenames>Lakshmi</forenames></author><author><keyname>Govardhan</keyname><forenames>A</forenames></author></authors><title>Extracting useful rules through improved decision tree induction using
  information entropy</title><categories>cs.LG</categories><comments>15 pages, 7 figures, 4 tables, International Journal of Information
  Sciences and Techniques (IJIST) Vol.3, No.1, January 2013</comments><doi>10.5121/ijist.2013.3103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classification is widely used technique in the data mining domain, where
scalability and efficiency are the immediate problems in classification
algorithms for large databases. We suggest improvements to the existing C4.5
decision tree algorithm. In this paper attribute oriented induction (AOI) and
relevance analysis are incorporated with concept hierarchys knowledge and
HeightBalancePriority algorithm for construction of decision tree along with
Multi level mining. The assignment of priorities to attributes is done by
evaluating information entropy, at different levels of abstraction for building
decision tree using HeightBalancePriority algorithm. Modified DMQL queries are
used to understand and explore the shortcomings of the decision trees generated
by C4.5 classifier for education dataset and the results are compared with the
proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2455</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2455</id><created>2013-02-11</created><authors><author><keyname>Lohrey</keyname><forenames>Markus</forenames></author><author><keyname>Steinberg</keyname><forenames>Benjamin</forenames></author><author><keyname>Zetzsche</keyname><forenames>Georg</forenames></author></authors><title>Rational Subsets and Submonoids of Wreath Products</title><categories>math.GR cs.FL</categories><msc-class>20F10, 20E22, 68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that membership in rational subsets of wreath products H \wr V
with H a finite group and V a virtually free group is decidable. On the other
hand, it is shown that there exists a fixed finitely generated submonoid in the
wreath product Z \wr Z with an undecidable membership problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2465</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2465</id><created>2013-02-11</created><updated>2013-03-06</updated><authors><author><keyname>Rodler</keyname><forenames>Patrick</forenames></author><author><keyname>Shchekotykhin</keyname><forenames>Kostyantyn</forenames></author><author><keyname>Fleiss</keyname><forenames>Philipp</forenames></author><author><keyname>Friedrich</keyname><forenames>Gerhard</forenames></author></authors><title>RIO: Minimizing User Interaction in Debugging of Knowledge Bases</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1209.3734</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The best currently known interactive debugging systems rely upon some
meta-information in terms of fault probabilities in order to improve their
efficiency. However, misleading meta information might result in a dramatic
decrease of the performance and its assessment is only possible a-posteriori.
Consequently, as long as the actual fault is unknown, there is always some risk
of suboptimal interactions. In this work we present a reinforcement learning
strategy that continuously adapts its behavior depending on the performance
achieved and minimizes the risk of using low-quality meta information.
Therefore, this method is suitable for application scenarios where reliable
prior fault estimates are difficult to obtain. Using diverse real-world
knowledge bases, we show that the proposed interactive query strategy is
scalable, features decent reaction time, and outperforms both entropy-based and
no-risk strategies on average w.r.t. required amount of user interaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2472</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2472</id><created>2013-02-11</created><authors><author><keyname>Mavrodiev</keyname><forenames>Pavlin</forenames></author><author><keyname>Tessone</keyname><forenames>Claudio J.</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Quantifying the effects of social influence</title><categories>physics.soc-ph cs.SI</categories><comments>3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How do humans respond to indirect social influence when making decisions? We
analysed an experiment where subjects had to repeatedly guess the correct
answer to factual questions, while having only aggregated information about the
answers of others. While the response of humans to aggregated information is a
widely observed phenomenon, it has not been investigated quantitatively, in a
controlled setting. We found that the adjustment of individual guesses depends
linearly on the distance to the mean of all guesses. This is a remarkable, and
yet surprisingly simple, statistical regularity. It holds across all questions
analysed, even though the correct answers differ in several orders of
magnitude. Our finding supports the assumption that individual diversity does
not affect the response to indirect social influence. It also complements
previous results on the nonlinear response in information-rich scenarios. We
argue that the nature of the response to social influence crucially changes
with the level of information aggregation. This insight contributes to the
empirical foundation of models for collective decisions under social influence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2481</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2481</id><created>2013-02-11</created><authors><author><keyname>Koliander</keyname><forenames>G&#xfc;nther</forenames></author><author><keyname>Riegler</keyname><forenames>Erwin</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Morgenshtern</keyname><forenames>Veniamin I.</forenames></author><author><keyname>Hlawatsch</keyname><forenames>Franz</forenames></author></authors><title>A Lower Bound on the Noncoherent Capacity Pre-log for the MIMO Channel
  with Temporally Correlated Fading</title><categories>cs.IT math.IT</categories><comments>8 pages, presented at the 50th Annual Allerton Conference on
  Communication, Control, and Computing - 2012</comments><msc-class>94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a lower bound on the capacity pre-log of a temporally correlated
Rayleigh block-fading multiple-input multiple-output (MIMO) channel with T
transmit antennas and R receive antennas in the noncoherent setting (no a
priori channel knowledge at the transmitter and the receiver). In this model,
the fading process changes independently across blocks of length L and is
temporally correlated within each block for each transmit-receive antenna pair,
with a given rank Q of the corresponding correlation matrix. Our result implies
that for almost all choices of the coloring matrix that models the temporal
correlation, the pre-log can be lower-bounded by T(1-1/L) for T &lt;= (L-1)/Q
provided that R is sufficiently large. The widely used constant block-fading
model is equivalent to the temporally correlated block-fading model with Q = 1
for the special case when the temporal correlation for each transmit-receive
antenna pair is the same, which is unlikely to be observed in practice. For the
constant block-fading model, the capacity pre-log is given by T(1-T/L), which
is smaller than our lower bound for the case Q = 1. Thus, our result suggests
that the assumptions underlying the constant block- fading model lead to a
pessimistic result for the capacity pre-log.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2501</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2501</id><created>2013-02-11</created><updated>2013-04-07</updated><authors><author><keyname>Parra-Arnau</keyname><forenames>Javier</forenames></author><author><keyname>Rebollo-Monedero</keyname><forenames>David</forenames></author><author><keyname>Forn&#xe9;</keyname><forenames>Jordi</forenames></author></authors><title>Optimal Forgery and Suppression of Ratings for Privacy Enhancement in
  Recommendation Systems</title><categories>cs.IT math.IT math.OC</categories><comments>25 pages and 24 figures</comments><doi>10.3390/e16031586</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommendation systems are information-filtering systems that tailor
information to users on the basis of knowledge about their preferences. The
ability of these systems to profile users is what enables such intelligent
functionality, but at the same time, it is the source of serious privacy
concerns. In this paper we investigate a privacy-enhancing technology that aims
at hindering an attacker in its efforts to accurately profile users based on
the items they rate. Our approach capitalizes on the combination of two
perturbative mechanisms---the forgery and the suppression of ratings. While
this technique enhances user privacy to a certain extent, it inevitably comes
at the cost of a loss in data utility, namely a degradation of the
recommendation's accuracy. In short, it poses a trade-off between privacy and
utility.
  The theoretical analysis of said trade-off is the object of this work. We
measure privacy as the Kullback-Leibler divergence between the user's and the
population's item distributions, and quantify utility as the proportion of
ratings users consent to forge and eliminate. Equipped with these quantitative
measures, we find a closed-form solution to the problem of optimal forgery and
suppression of ratings, and characterize the trade-off among privacy, forgery
rate and suppression rate. Experimental results on a popular recommendation
system show how our approach may contribute to privacy enhancement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2512</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2512</id><created>2013-02-11</created><updated>2013-07-15</updated><authors><author><keyname>Kumar</keyname><forenames>Gowtham R.</forenames></author><author><keyname>Courtade</keyname><forenames>Thomas A.</forenames></author></authors><title>Which Boolean Functions are Most Informative?</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure. Presented at ISIT 2013 in Istanbul, Turkey. (v2
  corrects minor typos present in v1)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a simply stated conjecture regarding the maximum mutual
information a Boolean function can reveal about noisy inputs. Specifically, let
$X^n$ be i.i.d. Bernoulli(1/2), and let $Y^n$ be the result of passing $X^n$
through a memoryless binary symmetric channel with crossover probability
$\alpha$. For any Boolean function $b:\{0,1\}^n\rightarrow \{0,1\}$, we
conjecture that $I(b(X^n);Y^n)\leq 1-H(\alpha)$. While the conjecture remains
open, we provide substantial evidence supporting its validity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2518</identifier>
 <datestamp>2013-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2518</id><created>2013-02-11</created><updated>2013-09-12</updated><authors><author><keyname>Molnar</keyname><forenames>F.</forenames><suffix>Jr.</suffix></author><author><keyname>Sreenivasan</keyname><forenames>S.</forenames></author><author><keyname>Szymanski</keyname><forenames>B. K.</forenames></author><author><keyname>Korniss</keyname><forenames>G.</forenames></author></authors><title>Minimum Dominating Sets in Scale-Free Network Ensembles</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><msc-class>05C69</msc-class><acm-class>G.2.2</acm-class><journal-ref>Scientific Reports 3, 1736 (2013)</journal-ref><doi>10.1038/srep01736</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the scaling behavior of the size of minimum dominating set (MDS) in
scale-free networks, with respect to network size $N$ and power-law exponent
$\gamma$, while keeping the average degree fixed. We study ensembles generated
by three different network construction methods, and we use a greedy algorithm
to approximate the MDS. With a structural cutoff imposed on the maximal degree
($k_{\max}=\sqrt{N}$) we find linear scaling of the MDS size with respect to
$N$ in all three network classes. Without any cutoff ($k_{\max}=N-1$) two of
the network classes display a transition at $\gamma \approx 1.9$, with linear
scaling above, and vanishingly weak dependence below, but in the third network
class we find linear scaling irrespective of $\gamma$. We find that the partial
MDS, which dominates a given $z&lt;1$ fraction of nodes, displays essentially the
same scaling behavior as the MDS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2529</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2529</id><created>2013-02-11</created><updated>2013-03-26</updated><authors><author><keyname>Aleksiev</keyname><forenames>Tyanko</forenames></author><author><keyname>Barkow</keyname><forenames>Simon</forenames></author><author><keyname>Kunszt</keyname><forenames>Peter</forenames></author><author><keyname>Maffioletti</keyname><forenames>Sergio</forenames></author><author><keyname>Murri</keyname><forenames>Riccardo</forenames></author><author><keyname>Panse</keyname><forenames>Christian</forenames></author></authors><title>VM-MAD: a cloud/cluster software for service-oriented academic
  environments</title><categories>cs.DC</categories><comments>16 pages, 5 figures. Accepted at the International Supercomputing
  Conference ISC13, June 17--20 Leipzig, Germany</comments><acm-class>C.1.4; C.5.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of powerful computing hardware in IaaS clouds makes cloud
computing attractive also for computational workloads that were up to now
almost exclusively run on HPC clusters.
  In this paper we present the VM-MAD Orchestrator software: an open source
framework for cloudbursting Linux-based HPC clusters into IaaS clouds but also
computational grids. The Orchestrator is completely modular, allowing flexible
configurations of cloudbursting policies. It can be used with any batch system
or cloud infrastructure, dynamically extending the cluster when needed. A
distinctive feature of our framework is that the policies can be tested and
tuned in a simulation mode based on historical or synthetic cluster accounting
data.
  In the paper we also describe how the VM-MAD Orchestrator was used in a
production environment at the FGCZ to speed up the analysis of mass
spectrometry-based protein data by cloudbursting to the Amazon EC2. The
advantages of this hybrid system are shown with a large evaluation run using
about hundred large EC2 nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2543</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2543</id><created>2013-02-11</created><authors><author><keyname>Vaidya</keyname><forenames>Nitin H.</forenames></author><author><keyname>Garg</keyname><forenames>Vijay K.</forenames></author></authors><title>Byzantine Vector Consensus in Complete Graphs</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a network of n processes each of which has a d-dimensional vector of
reals as its input. Each process can communicate directly with all the
processes in the system; thus the communication network is a complete graph.
All the communication channels are reliable and FIFO (first-in-first-out). The
problem of Byzantine vector consensus (BVC) requires agreement on a
d-dimensional vector that is in the convex hull of the d-dimensional input
vectors at the non-faulty processes. We obtain the following results for
Byzantine vector consensus in complete graphs while tolerating up to f
Byzantine failures:
  * We prove that in a synchronous system, n &gt;= max(3f+1, (d+1)f+1) is
necessary and sufficient for achieving Byzantine vector consensus.
  * In an asynchronous system, it is known that exact consensus is impossible
in presence of faulty processes. For an asynchronous system, we prove that n &gt;=
(d+2)f+1 is necessary and sufficient to achieve approximate Byzantine vector
consensus.
  Our sufficiency proofs are constructive. We show sufficiency by providing
explicit algorithms that solve exact BVC in synchronous systems, and
approximate BVC in asynchronous systems.
  We also obtain tight bounds on the number of processes for achieving BVC
using algorithms that are restricted to a simpler communication pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2550</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2550</id><created>2013-02-11</created><authors><author><keyname>Ortner</keyname><forenames>Ronald</forenames></author><author><keyname>Ryabko</keyname><forenames>Daniil</forenames></author></authors><title>Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</title><categories>cs.LG</categories><journal-ref>in proceedings of NIPS 2012, pp. 1772--1780</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive sublinear regret bounds for undiscounted reinforcement learning in
continuous state space. The proposed algorithm combines state aggregation with
the use of upper confidence bounds for implementing optimism in the face of
uncertainty. Beside the existence of an optimal policy which satisfies the
Poisson equation, the only assumptions made are Holder continuity of rewards
and transition probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2551</identifier>
 <datestamp>2013-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2551</id><created>2013-02-11</created><updated>2013-04-25</updated><authors><author><keyname>Mucha</keyname><forenames>Marcin</forenames></author><author><keyname>Sviridenko</keyname><forenames>Maxim</forenames></author></authors><title>No-Wait Flowshop Scheduling is as Hard as Asymmetric Traveling Salesman
  Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the classical no-wait flowshop scheduling problem with
makespan objective (F|no-wait|C_max in the standard three-field notation). This
problem is well-known to be a special case of the asymmetric traveling salesman
problem (ATSP) and as such has an approximation algorithm with logarithmic
performance guarantee. In this work we show a reverse connection, we show that
any polynomial time \alpha-approximation algorithm for the no-wait flowshop
scheduling problem with makespan objective implies the existence of a
polynomial-time \alpha(1+\epsilon)-approximation algorithm for the ATSP, for
any \epsilon&gt;0. This in turn implies that all non-approximability results for
the ATSP (current or future) will carry over to its special case. In
particular, it follows that no-wait flowshop problem is APX-hard, which is the
first non-approximability result for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2552</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2552</id><created>2013-02-11</created><authors><author><keyname>Maillard</keyname><forenames>Odalric-Ambrym</forenames></author><author><keyname>Munos</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Ryabko</keyname><forenames>Daniil</forenames></author></authors><title>Selecting the State-Representation in Reinforcement Learning</title><categories>cs.LG</categories><journal-ref>NIPS 2011, pp. 2627-2635</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of selecting the right state-representation in a reinforcement
learning problem is considered. Several models (functions mapping past
observations to a finite set) of the observations are given, and it is known
that for at least one of these models the resulting state dynamics are indeed
Markovian. Without knowing neither which of the models is the correct one, nor
what are the probabilistic characteristics of the resulting MDP, it is required
to obtain as much reward as the optimal policy for the correct model (or for
the best of the correct models, if there are several). We propose an algorithm
that achieves that, with a regret of order T^{2/3} where T is the horizon time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2553</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2553</id><created>2013-02-11</created><updated>2013-03-18</updated><authors><author><keyname>Maillard</keyname><forenames>Odalric-Ambrym</forenames></author><author><keyname>Nguyen</keyname><forenames>Phuong</forenames></author><author><keyname>Ortner</keyname><forenames>Ronald</forenames></author><author><keyname>Ryabko</keyname><forenames>Daniil</forenames></author></authors><title>Optimal Regret Bounds for Selecting the State Representation in
  Reinforcement Learning</title><categories>cs.LG</categories><journal-ref>In Proceedings of ICML, JMLR W&amp;CP 28(1):543-551, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an agent interacting with an environment in a single stream of
actions, observations, and rewards, with no reset. This process is not assumed
to be a Markov Decision Process (MDP). Rather, the agent has several
representations (mapping histories of past interactions to a discrete state
space) of the environment with unknown dynamics, only some of which result in
an MDP. The goal is to minimize the average regret criterion against an agent
who knows an MDP representation giving the highest optimal reward, and acts
optimally in it. Recent regret bounds for this setting are of order
$O(T^{2/3})$ with an additive term constant yet exponential in some
characteristics of the optimal MDP. We propose an algorithm whose regret after
$T$ time steps is $O(\sqrt{T})$, with all constants reasonably small. This is
optimal in $T$ since $O(\sqrt{T})$ is the optimal regret in the setting of
learning in a (single discrete) MDP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2563</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2563</id><created>2013-02-11</created><authors><author><keyname>Kovanen</keyname><forenames>Lauri</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author><author><keyname>Kert&#xe9;sz</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>Saram&#xe4;ki</keyname><forenames>Jari</forenames></author></authors><title>Temporal motifs reveal homophily, gender-specific patterns and group
  talk in mobile communication networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>8 pages, 3 figures (SI: 20 pages, 18 figures)</comments><doi>10.1073/pnas.1307941110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electronic communication records provide detailed information about temporal
aspects of human interaction. Previous studies have shown that individuals'
communication patterns have complex temporal structure, and that this structure
has system-wide effects. In this paper we use mobile phone records to show that
interaction patterns involving multiple individuals have non-trivial temporal
structure that cannot be deduced from a network presentation where only
interaction frequencies are taken into account. We apply a recently introduced
method, temporal motifs, to identify interaction patterns in a temporal network
where nodes have additional attributes such as gender and age. We then develop
a null model that allows identifying differences between various types of nodes
so that these differences are independent of the network based on interaction
frequencies. We find gender-related differences in communication patters, and
show the existence of temporal homophily, the tendency of similar individuals
to participate in interaction patterns beyond what would be expected on the
basis of the network structure alone. We also show that temporal patterns
differ between dense and sparse parts of the network. Because this result is
independent of edge weights, it can be considered as an extension of
Granovetter's hypothesis to temporal networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2569</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2569</id><created>2013-02-11</created><authors><author><keyname>Catoni</keyname><forenames>Olivier</forenames></author><author><keyname>Mainguy</keyname><forenames>Thomas</forenames></author></authors><title>Toric grammars: a new statistical approach to natural language modeling</title><categories>stat.ML cs.CL math.PR</categories><msc-class>62M09, 62P99, 68T50, 91F20, 03B65, 91E40, 60J20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new statistical model for computational linguistics. Rather than
trying to estimate directly the probability distribution of a random sentence
of the language, we define a Markov chain on finite sets of sentences with many
finite recurrent communicating classes and define our language model as the
invariant probability measures of the chain on each recurrent communicating
class. This Markov chain, that we call a communication model, recombines at
each step randomly the set of sentences forming its current state, using some
grammar rules. When the grammar rules are fixed and known in advance instead of
being estimated on the fly, we can prove supplementary mathematical properties.
In particular, we can prove in this case that all states are recurrent states,
so that the chain defines a partition of its state space into finite recurrent
communicating classes. We show that our approach is a decisive departure from
Markov models at the sentence level and discuss its relationships with Context
Free Grammars. Although the toric grammars we use are closely related to
Context Free Grammars, the way we generate the language from the grammar is
qualitatively different. Our communication model has two purposes. On the one
hand, it is used to define indirectly the probability distribution of a random
sentence of the language. On the other hand it can serve as a (crude) model of
language transmission from one speaker to another speaker through the
communication of a (large) set of sentences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2570</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2570</id><created>2013-02-11</created><authors><author><keyname>Fraigniaud</keyname><forenames>Pierre</forenames></author><author><keyname>G&#xf6;&#xf6;s</keyname><forenames>Mika</forenames></author><author><keyname>Korman</keyname><forenames>Amos</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author></authors><title>What can be decided locally without identifiers?</title><categories>cs.DC cs.CC</categories><comments>1 + 15 pages, 3 figures</comments><doi>10.1145/2484239.2484264</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Do unique node identifiers help in deciding whether a network $G$ has a
prescribed property $P$? We study this question in the context of distributed
local decision, where the objective is to decide whether $G \in P$ by having
each node run a constant-time distributed decision algorithm. If $G \in P$, all
the nodes should output yes; if $G \notin P$, at least one node should output
no.
  A recent work (Fraigniaud et al., OPODIS 2012) studied the role of
identifiers in local decision and gave several conditions under which
identifiers are not needed. In this article, we answer their original question.
More than that, we do so under all combinations of the following two critical
variations on the underlying model of distributed computing:
  ($B$): the size of the identifiers is bounded by a function of the size of
the input network; as opposed to ($\neg B$): the identifiers are unbounded.
  ($C$): the nodes run a computable algorithm; as opposed to ($\neg C$): the
nodes can compute any, possibly uncomputable function.
  While it is easy to see that under ($\neg B, \neg C$) identifiers are not
needed, we show that under all other combinations there are properties that can
be decided locally if and only if identifiers are present. Our constructions
use ideas from classical computability theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2575</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2575</id><created>2013-02-04</created><authors><author><keyname>Llull</keyname><forenames>Patrick</forenames></author><author><keyname>Liao</keyname><forenames>Xuejun</forenames></author><author><keyname>Yuan</keyname><forenames>Xin</forenames></author><author><keyname>Yang</keyname><forenames>Jianbo</forenames></author><author><keyname>Kittle</keyname><forenames>David</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author><author><keyname>Brady</keyname><forenames>David J.</forenames></author></authors><title>Coded aperture compressive temporal imaging</title><categories>cs.CV cs.IT math.IT</categories><comments>19 pages (when compiled with Optics Express' TEX template), 15
  figures</comments><doi>10.1364/OE.21.010526</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use mechanical translation of a coded aperture for code division multiple
access compression of video. We present experimental results for reconstruction
at 148 frames per coded snapshot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2576</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2576</id><created>2013-02-11</created><authors><author><keyname>Koyejo</keyname><forenames>Oluwasanmi</forenames></author><author><keyname>Lee</keyname><forenames>Cheng</forenames></author><author><keyname>Ghosh</keyname><forenames>Joydeep</forenames></author></authors><title>The trace norm constrained matrix-variate Gaussian process for multitask
  bipartite ranking</title><categories>cs.LG stat.ML</categories><comments>14 pages, 9 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel hierarchical model for multitask bipartite ranking. The
proposed approach combines a matrix-variate Gaussian process with a generative
model for task-wise bipartite ranking. In addition, we employ a novel trace
constrained variational inference approach to impose low rank structure on the
posterior matrix-variate Gaussian process. The resulting posterior covariance
function is derived in closed form, and the posterior mean function is the
solution to a matrix-variate regression with a novel spectral elastic net
regularizer. Further, we show that variational inference for the trace
constrained matrix-variate Gaussian process combined with maximum likelihood
parameter estimation for the bipartite ranking model is jointly convex. Our
motivating application is the prioritization of candidate disease genes. The
goal of this task is to aid the identification of unobserved associations
between human genes and diseases using a small set of observed associations as
well as kernels induced by gene-gene interaction networks and disease
ontologies. Our experimental results illustrate the performance of the proposed
model on real world datasets. Moreover, we find that the resulting low rank
solution improves the computational scalability of training and testing as
compared to baseline models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2577</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2577</id><created>2013-02-11</created><authors><author><keyname>Foukalas</keyname><forenames>Fotis</forenames></author><author><keyname>Merakos</keyname><forenames>Lazaros</forenames></author></authors><title>A Study on Spectral Efficiency of Physical Layer over Cognitive Radio</title><categories>cs.ET cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the conclusive potential of cognitive radio for provisioning the
dynamic and flexible spectrum/channel allocation, the research community should
study the performance gain of physical layer over such a radio with cognition
capabilities. To this end, several mechanisms of physical layers such as
adaptive modulation, multiple-input multiple output antennas; channel coding
and/or combination of them should be studied. These studies should be
accomplished in terms of spectral efficiency. Therefore, the gain of cognitive
radio in wireless networks available into the market will be identified
practically. Another issue under consideration should be the performance
evaluation of cognitive radio assuming a cross-layer combination between the
cognitive physical and the upper layers. To this direction, this paper presents
a study on spectral efficiency at the physical layer with cognitive
capabilities. In sequel, we study a cross-layer combination of physical layer
with upper layers in the same cognitive context. The performance gain of
cognitive radio in such a physical layer is realized practically as well as a
few cross-layer design issues have been raised.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2584</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2584</id><created>2013-02-11</created><updated>2013-08-05</updated><authors><author><keyname>Wang</keyname><forenames>Yuting</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Chaudhuri</keyname><forenames>Kaustuv</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Gacek</keyname><forenames>Andrew</forenames></author><author><keyname>Nadathur</keyname><forenames>Gopalan</forenames></author></authors><title>Reasoning About Higher-Order Relational Specifications</title><categories>cs.LO</categories><comments>Principles and Practice of Declarative Programming (2013)</comments><proxy>ccsd</proxy><doi>10.1145/2505879.2505889</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The logic of hereditary Harrop formulas (HH) has proven useful for specifying
a wide range of formal systems. This logic includes a form of hypothetical
judgment that leads to dynamically changing sets of assumptions and that is key
to encoding side conditions and contexts that occur frequently in structural
operational semantics (SOS) style presentations. Specifications are often
useful in reasoning about the systems they describe. The Abella theorem prover
supports such reasoning by explicitly embedding the specification logic within
a rich reasoning logic; specifications are then reasoned about through this
embedding. However, realizing an induction principle in the face of dynamically
changing assumption sets is nontrivial and the original Abella system uses only
a subset of the HH specification logic for this reason. We develop a method
here for supporting inductive reasoning over all of HH. Our approach takes
advantage of a focusing property of HH to isolate the use of an assumption and
the ability to finitely characterize the structure of any such assumption in
the reasoning logic. We demonstrate the effectiveness of these ideas via
several specification and meta-theoretic reasoning examples that have been
implemented in an extended version of Abella.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2606</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2606</id><created>2013-02-11</created><updated>2013-11-16</updated><authors><author><keyname>Teldja</keyname><forenames>Amghar Yasmina</forenames></author><author><keyname>Hadria</keyname><forenames>Fizazi</forenames></author></authors><title>A new bio-inspired method for remote sensing imagery classification</title><categories>cs.NE cs.CV</categories><comments>13 pages, 17 figures. Updated author's affiliation and corrected
  co-author's name</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of supervised classification of the satellite image is considered
to be the task of grouping pixels into a number of homogeneous regions in space
intensity. This paper proposes a novel approach that combines a radial basic
function clustering network with a growing neural gas include utility factor
classifier to yield improved solutions, obtained with previous networks. The
double objective technique is first used to the development of a method to
perform the satellite images classification, and finally, the implementation to
address the issue of the number of nodes in the hidden layer of the classic
Radial Basis functions network. Results demonstrating the effectiveness of the
proposed technique are provided for numeric remote sensing imagery. Moreover,
the remotely sensed image of Oran city in Algeria has been classified using the
proposed technique to establish its utility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2615</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2615</id><created>2013-02-09</created><authors><author><keyname>Horvat</keyname><forenames>Marko</forenames></author><author><keyname>Gledec</keyname><forenames>Gordan</forenames></author><author><keyname>Bogunovi&#x107;</keyname><forenames>Nikola</forenames></author></authors><title>Assessing Semantic Quality of Web Directory Structure</title><categories>cs.IR cs.DL</categories><comments>12 pages, 6 figures. arXiv admin note: substantial text overlap with
  arXiv:1302.2222</comments><journal-ref>Lecture Notes in Computer Science, Lecture Notes in Artificial
  Intelligence, 1, 5796, pp. 377-388 (2009)</journal-ref><doi>10.1007/978-3-642-15034-0_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The administration of a Web directory content and associated structure is a
labor intensive task performed by human domain experts. Because of that there
always exists a realistic risk of the structure becoming unbalanced, uneven and
difficult to use to all except for a few users proficient in a particular Web
directory. These problems emphasize the importance of generic and objective
measures of Web directories structure quality. In this paper we demonstrate how
to formally merge Web directories into the Semantic Web vision. We introduce a
set of objective criterions for evaluation of a Web directory's structure
quality. Some criteria functions are based on heuristics while others require
the application of ontologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2645</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2645</id><created>2013-02-11</created><updated>2013-05-03</updated><authors><author><keyname>Mirkes</keyname><forenames>E. M.</forenames></author><author><keyname>Zinovyev</keyname><forenames>A.</forenames></author><author><keyname>Gorban</keyname><forenames>A. N.</forenames></author></authors><title>Geometrical complexity of data approximators</title><categories>stat.ML cs.LG</categories><comments>10 pages, 3 figures, minor correction and extension</comments><journal-ref>Advances in Computation Intelligence, Springer LNCS 7902, pp.
  500-509, 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  There are many methods developed to approximate a cloud of vectors embedded
in high-dimensional space by simpler objects: starting from principal points
and linear manifolds to self-organizing maps, neural gas, elastic maps, various
types of principal curves and principal trees, and so on. For each type of
approximators the measure of the approximator complexity was developed too.
These measures are necessary to find the balance between accuracy and
complexity and to define the optimal approximations of a given type. We propose
a measure of complexity (geometrical complexity) which is applicable to
approximators of several types and which allows comparing data approximations
of different types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2654</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2654</id><created>2013-02-11</created><authors><author><keyname>Mani</keyname><forenames>Murali</forenames></author><author><keyname>Shah</keyname><forenames>Kinnari</forenames></author><author><keyname>Gunda</keyname><forenames>Manikanta</forenames></author></authors><title>Enabling Secure Database as a Service using Fully Homomorphic
  Encryption: Challenges and Opportunities</title><categories>cs.DB cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The database community, at least for the last decade, has been grappling with
querying encrypted data, which would enable secure database as a service
solutions. A recent breakthrough in the cryptographic community (in 2009)
related to fully homomorphic encryption (FHE) showed that arbitrary computation
on encrypted data is possible. Successful adoption of FHE for query processing
is, however, still a distant dream, and numerous challenges have to be
addressed. One challenge is how to perform algebraic query processing of
encrypted data, where we produce encrypted intermediate results and operations
on encrypted data can be composed. In this paper, we describe our solution for
algebraic query processing of encrypted data, and also outline several other
challenges that need to be addressed, while also describing the lessons that
can be learnt from a decade of work by the database community in querying
encrypted data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2656</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2656</id><created>2013-02-11</created><authors><author><keyname>Stanekov&#xe1;</keyname><forenames>Lubica</forenames></author><author><keyname>Stanek</keyname><forenames>Martin</forenames></author></authors><title>How to choose a PIN - assessment of dictionary methods</title><categories>cs.CR</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personal Identification Numbers (PINs) are commonly used as an authentication
mechanism. An important security requirement is that PINs should be hard to
guess for an attacker. On the other hand, remembering several random PINs can
be difficult task for an individual. We evaluate several dictionary-based
methods of choosing a PIN. We experimentally show that these methods are far
from ideal with respect to expected covering of the PIN space and the entropy
of PINs. We also discuss two methods for constructing easy to memorize PIN
words for randomly chosen PINs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2657</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2657</id><created>2013-02-11</created><authors><author><keyname>Abdeen</keyname><forenames>Hani</forenames></author><author><keyname>Shata</keyname><forenames>Osama</forenames></author></authors><title>Metrics for Assessing The Design of Software Interfaces</title><categories>cs.SE</categories><comments>8 pages</comments><journal-ref>Hani Abdeen and Osama Shata. &quot;Metrics for Assessing the Design of
  Software Interfaces.&quot; International Journal of Advanced Research in Computer
  and Communication Engineering (IJARCCE) 1.10 (2012): 737-745</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recent studies have largely investigated the detection of class design
anomalies. They proposed a large set of metrics that help in detecting those
anomalies and in predicting the quality of class design. While those studies
and the proposed metrics are valuable, they do not address the particularities
of software interfaces. Interfaces define the contracts that spell out how
software modules and logic units interact with each other. This paper proposes
a list of design defects related to interfaces: shared similarity between
interfaces, interface clones and redundancy in interface hierarchy. We identify
and describe those design defects through real examples, taken from well-known
Java applications. Then we define three metrics that help in automatically
estimating the interface design quality, regarding the proposed design
anomalies, and identify refactoring candidates. We investigate our metrics and
show their usefulness through an empirical study conducted on three large Java
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2671</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2671</id><created>2013-02-11</created><updated>2014-04-30</updated><authors><author><keyname>Cho</keyname><forenames>Yoon-Sik</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author><author><keyname>Brantingham</keyname><forenames>P. Jeffrey</forenames></author><author><keyname>Tita</keyname><forenames>George</forenames></author></authors><title>Latent Self-Exciting Point Process Model for Spatial-Temporal Networks</title><categories>cs.SI cs.LG stat.ML</categories><comments>20 pages, 6 figures (v3); 11 pages, 6 figures (v2); previous version
  appeared in the 9th Bayesian Modeling Applications Workshop, UAI'12</comments><journal-ref>DISCRETE AND CONTINUOUS DYNAMICAL SYSTEMS SERIES B, Vol. 19, pp.
  1335-1354, 2014</journal-ref><doi>10.3934/dcdsb.2014.19.1335</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a latent self-exciting point process model that describes
geographically distributed interactions between pairs of entities. In contrast
to most existing approaches that assume fully observable interactions, here we
consider a scenario where certain interaction events lack information about
participants. Instead, this information needs to be inferred from the available
observations. We develop an efficient approximate algorithm based on
variational expectation-maximization to infer unknown participants in an event
given the location and the time of the event. We validate the model on
synthetic as well as real-world data, and obtain very promising results on the
identity-inference task. We also use our model to predict the timing and
participants of future events, and demonstrate that it compares favorably with
baseline approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2672</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2672</id><created>2013-02-11</created><authors><author><keyname>Han</keyname><forenames>Wei</forenames></author><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>Competing With Strategies</title><categories>stat.ML cs.GT cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of online learning with a notion of regret defined with
respect to a set of strategies. We develop tools for analyzing the minimax
rates and for deriving regret-minimization algorithms in this scenario. While
the standard methods for minimizing the usual notion of regret fail, through
our analysis we demonstrate existence of regret-minimization methods that
compete with such sets of strategies as: autoregressive algorithms, strategies
based on statistical models, regularized least squares, and follow the
regularized leader strategies. In several cases we also derive efficient
learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2675</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2675</id><created>2013-02-11</created><updated>2013-03-26</updated><authors><author><keyname>Fogarty</keyname><forenames>Seth J.</forenames><affiliation>Rice University</affiliation></author><author><keyname>Kupferman</keyname><forenames>Orna</forenames><affiliation>School of Computer Science and Engineering, Hebrew University of Jerusalem, Isra</affiliation></author><author><keyname>Wilke</keyname><forenames>Thomas</forenames><affiliation>Institut f&#xfc;r Informatik, Christian-Albrechts-Universit&#xe4;t zu Kiel,</affiliation></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames><affiliation>Department of Computer Science, Rice University, Houston, TX</affiliation></author></authors><title>Unifying B\&quot;uchi Complementation Constructions</title><categories>cs.FL</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 1 (March 27,
  2013) lmcs:1179</journal-ref><doi>10.2168/LMCS-9(1:13)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complementation of B\&quot;uchi automata, required for checking automata
containment, is of major theoretical and practical interest in formal
verification. We consider two recent approaches to complementation. The first
is the rank-based approach of Kupferman and Vardi, which operates over a DAG
that embodies all runs of the automaton. This approach is based on the
observation that the vertices of this DAG can be ranked in a certain way,
termed an odd ranking, iff all runs are rejecting. The second is the
slice-based approach of K\&quot;ahler and Wilke. This approach tracks levels of
&quot;split trees&quot; - run trees in which only essential information about the history
of each run is maintained. While the slice-based construction is conceptually
simple, the complementing automata it generates are exponentially larger than
those of the recent rank-based construction of Schewe, and it suffers from the
difficulty of symbolically encoding levels of split trees. In this work we
reformulate the slice-based approach in terms of run DAGs and preorders over
states. In doing so, we begin to draw parallels between the rank-based and
slice-based approaches. Through deeper analysis of the slice-based approach, we
strongly restrict the nondeterminism it generates. We are then able to employ
the slice-based approach to provide a new odd ranking, called a retrospective
ranking, that is different from the one provided by Kupferman and Vardi. This
new ranking allows us to construct a deterministic-in-the-limit rank-based
automaton with a highly restricted transition function. Further, by phrasing
the slice-based approach in terms of ranks, our approach affords a simple
symbolic encoding and achieves the tight bound of Schewe's construction
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2684</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2684</id><created>2013-02-11</created><updated>2013-10-24</updated><authors><author><keyname>Anandkumar</keyname><forenames>Anima</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author></authors><title>A Tensor Approach to Learning Mixed Membership Community Models</title><categories>cs.LG cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection is the task of detecting hidden communities from observed
interactions. Guaranteed community detection has so far been mostly limited to
models with non-overlapping communities such as the stochastic block model. In
this paper, we remove this restriction, and provide guaranteed community
detection for a family of probabilistic network models with overlapping
communities, termed as the mixed membership Dirichlet model, first introduced
by Airoldi et al. This model allows for nodes to have fractional memberships in
multiple communities and assumes that the community memberships are drawn from
a Dirichlet distribution. Moreover, it contains the stochastic block model as a
special case. We propose a unified approach to learning these models via a
tensor spectral decomposition method. Our estimator is based on low-order
moment tensor of the observed network, consisting of 3-star counts. Our
learning method is fast and is based on simple linear algebraic operations,
e.g. singular value decomposition and tensor power iterations. We provide
guaranteed recovery of community memberships and model parameters and present a
careful finite sample analysis of our learning method. As an important special
case, our results match the best known scaling requirements for the
(homogeneous) stochastic block model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2689</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2689</id><created>2013-02-11</created><updated>2013-03-25</updated><authors><author><keyname>Zhang</keyname><forenames>Hong</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>Partitioned and implicit-explicit general linear methods for ordinary
  differential equations</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implicit-explicit (IMEX) time stepping methods can efficiently solve
differential equa- tions with both stiff and nonstiff components. IMEX
Runge-Kutta methods and IMEX linear multistep methods have been studied in the
literature. In this pa- per we study new implicit-explicit methods of general
linear type (IMEX-GLMs). We develop an order conditions theory for high stage
order partitioned GLMs that share the same abscissae, and show that no
additional coupling order conditions are needed. Consequently, GLMs offer an
excellent framework for the construction of multi-method integration
algorithms. Next, we propose a family of IMEX schemes based on
diagonally-implicit multi-stage integration methods and construct practical
schemes of order three. Numerical results confirm the theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2692</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2692</id><created>2013-02-11</created><authors><author><keyname>Liang</keyname><forenames>Shuying</forenames></author><author><keyname>Might</keyname><forenames>Matthew</forenames></author><author><keyname>Gilray</keyname><forenames>Thomas</forenames></author><author><keyname>Van Horn</keyname><forenames>David</forenames></author></authors><title>Pushdown Exception-Flow Analysis of Object-Oriented Programs</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statically reasoning in the presence of and about exceptions is challenging:
exceptions worsen the well-known mutual recursion between data-flow and
control-flow analysis. The recent development of pushdown control-flow analysis
for the {\lambda}-calculus hints at a way to improve analysis of exceptions: a
pushdown stack can precisely match catches to throws in the same way it matches
returns to calls. This work generalizes pushdown control-flow analysis to
object-oriented programs and to exceptions. Pushdown analysis of exceptions
improves precision over the next best analysis, Bravenboer and Smaragdakis's
Doop, by orders of magnitude. By then generalizing abstract garbage collection
to object-oriented programs, we reduce analysis time by half over pure pushdown
analysis. We evaluate our implementation for Dalvik bytecode on standard
benchmarks as well as several Android applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2698</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2698</id><created>2013-02-11</created><updated>2015-12-06</updated><authors><author><keyname>Jooyandeh</keyname><forenames>Mohammadreza</forenames></author><author><keyname>McKay</keyname><forenames>Brendan D.</forenames></author><author><keyname>&#xd6;sterg&#xe5;rd</keyname><forenames>Patric R. J.</forenames></author><author><keyname>Pettersson</keyname><forenames>Ville H.</forenames></author><author><keyname>Zamfirescu</keyname><forenames>Carol T.</forenames></author></authors><title>Planar Hypohamiltonian Graphs on 40 Vertices</title><categories>math.CO cs.DS</categories><msc-class>05C10, 05C30, 05C38, 05C45, 05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is hypohamiltonian if it is not Hamiltonian, but the deletion of any
single vertex gives a Hamiltonian graph. Until now, the smallest known planar
hypohamiltonian graph had 42 vertices, a result due to Araya and Wiener. That
result is here improved upon by 25 planar hypohamiltonian graphs of order 40,
which are found through computer-aided generation of certain families of planar
graphs with girth 4 and a fixed number of 4-faces. It is further shown that
planar hypohamiltonian graphs exist for all orders greater than or equal to 42.
If Hamiltonian cycles are replaced by Hamiltonian paths throughout the
definition of hypohamiltonian graphs, we get the definition of hypotraceable
graphs. It is shown that there is a planar hypotraceable graph of order 154 and
of all orders greater than or equal to 156. We also show that the smallest
hypohamiltonian planar graph of girth 5 has 45 vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2702</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2702</id><created>2013-02-12</created><updated>2015-10-02</updated><authors><author><keyname>Iyengar</keyname><forenames>Aravind R.</forenames></author><author><keyname>Siegel</keyname><forenames>Paul H.</forenames></author><author><keyname>Wolf</keyname><forenames>Jack K.</forenames></author></authors><title>On the Capacity of Channels with Timing Synchronization Errors</title><categories>cs.IT math.IT</categories><comments>23 pages, 5 figures, submitted to the IEEE Transactions on
  Information Theory, Feb 2013. Summary of some results presented at ISIT 2011
  (http://arxiv.org/abs/1106.0070) Revised Feb 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a new formulation of a class of synchronization error channels
and derive analytical bounds and numerical estimates for the capacity of these
channels. For the binary channel with only deletions, we obtain an expression
for the symmetric information rate in terms of subsequence weights which
reduces to a tight lower bound for small deletion probabilities. We are also
able to exactly characterize the Markov-1 rate for the binary channel with only
replications. For a channel that introduces deletions as well as replications
of input symbols, we design approximating channels that parameterize the state
space and show that the information rates of these approximate channels
approach that of the deletion-replication channel as the state space grows. For
the case of the channel where deletions and replications occur with the same
probabilities, a stronger result in the convergence of mutual information rates
is shown. The numerous advantages this new formulation presents are explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2712</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2712</id><created>2013-02-12</created><updated>2014-07-26</updated><authors><author><keyname>Huang</keyname><forenames>Yue</forenames></author><author><keyname>Paisley</keyname><forenames>John</forenames></author><author><keyname>Lin</keyname><forenames>Qin</forenames></author><author><keyname>Ding</keyname><forenames>Xinghao</forenames></author><author><keyname>Fu</keyname><forenames>Xueyang</forenames></author><author><keyname>Zhang</keyname><forenames>Xiao-ping</forenames></author></authors><title>Bayesian Nonparametric Dictionary Learning for Compressed Sensing MRI</title><categories>cs.CV physics.med-ph stat.AP</categories><doi>10.1109/TIP.2014.2360122</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a Bayesian nonparametric model for reconstructing magnetic
resonance images (MRI) from highly undersampled k-space data. We perform
dictionary learning as part of the image reconstruction process. To this end,
we use the beta process as a nonparametric dictionary learning prior for
representing an image patch as a sparse combination of dictionary elements. The
size of the dictionary and the patch-specific sparsity pattern are inferred
from the data, in addition to other dictionary learning variables. Dictionary
learning is performed directly on the compressed image, and so is tailored to
the MRI being considered. In addition, we investigate a total variation penalty
term in combination with the dictionary learning model, and show how the
denoising property of dictionary learning removes dependence on regularization
parameters in the noisy setting. We derive a stochastic optimization algorithm
based on Markov Chain Monte Carlo (MCMC) for the Bayesian model, and use the
alternating direction method of multipliers (ADMM) for efficiently performing
total variation minimization. We present empirical results on several MRI,
which show that the proposed regularization framework can improve
reconstruction accuracy over other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2718</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2718</id><created>2013-02-12</created><updated>2013-02-13</updated><authors><author><keyname>Agarwal</keyname><forenames>Monika</forenames></author></authors><title>Text Steganographic Approaches: A Comparison</title><categories>cs.CR</categories><comments>16 pages, 6 figures, 5 tables</comments><journal-ref>Monika Agarwal, &quot;Text Steganographic Approaches: A Comparison&quot;,
  International Journal of Network Security &amp; Its Applications (IJNSA), Vol.5,
  No.1, January 2013, pp.91-106</journal-ref><doi>10.5121/ijnsa.2013.5107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents three novel approaches of text steganography. The first
approach uses the theme of missing letter puzzle where each character of
message is hidden by missing one or more letters in a word of cover. The
average Jaro score was found to be 0.95 indicating closer similarity between
cover and stego file. The second approach hides a message in a wordlist where
ASCII value of embedded character determines length and starting letter of a
word. The third approach conceals a message, without degrading cover, by using
start and end letter of words of the cover. For enhancing the security of
secret message, the message is scrambled using one-time pad scheme before being
concealed and cipher text is then concealed in cover. We also present an
empirical comparison of the proposed approaches with some of the popular text
steganographic approaches and show that our approaches outperform the existing
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2738</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2738</id><created>2013-02-12</created><updated>2015-03-15</updated><authors><author><keyname>Miszczak</keyname><forenames>J. A.</forenames></author><author><keyname>Wahl</keyname><forenames>M.</forenames></author></authors><title>RandFile package for Mathematica for accessing file-based sources of
  randomness</title><categories>physics.comp-ph cs.MS physics.data-an quant-ph</categories><comments>16 pages, 4 figures, 3 tables, improved version of the software
  available from http://www.iitis.pl/~miszczak/rand_file/</comments><acm-class>G.3; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a package for Mathematica computer algebra system which allows the
exploitation of local files as sources of random data. We provide the
description of the package and illustrate its usage by showing some examples.
We also compare the provided functionality with alternative sources of
randomness, namely a built-in pseudo-random generator and the package for
accessing hardware true random number generators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2747</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2747</id><created>2013-02-12</created><authors><author><keyname>Gandomani</keyname><forenames>Taghi Javdani</forenames></author><author><keyname>Zulzalil</keyname><forenames>Hazura</forenames></author><author><keyname>Ghani</keyname><forenames>Abdul Azim Abdul</forenames></author><author><keyname>Sultan</keyname><forenames>Abu Bakar Md.</forenames></author></authors><title>Effective factors in agile transformation process from change management
  perspective</title><categories>cs.SE</categories><journal-ref>Taghi Javdani Gandomani, et al., Effective factors in agile
  transformation process from change management perspective, 2nd Int. Conf. on
  Advance Information System, E-Education &amp; Development (CAISED 2013), Jan.
  2013, Kuala Lumpur, Malaysia</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After introducing agile approach in 2001, several agile methods were founded
over the last decade. Agile values such as customer collaboration, embracing
changes, iteration and frequent delivery, continuous integration, etc. motivate
all software stakeholders to use these methods in their projects. The main
issue is that for using these methods instead of traditional methods in
software development, companies should change their approach from traditional
to agile. This change is a fundamental and critical mutation. Several studies
have been done for investigating of barriers, challenges and issues in agile
movement process and also in how to use agile methods in companies. The main
issue is altering attitude from traditional to agile approach. We believe that
before managing agile transformation process, its related factors should be
studied in deep. This study focuses on different dimensions of changing
approach to agile from change management perspective. These factors are how to
being agile, method selection and awareness of challenges and issues. These
fundamental factors encompass many items for agile movement and adoption
process. However these factors may change in different organization, but they
should be studied in deep before any action plan for designing a change
strategy. The main contribution of this paper is introducing and these factors
and discuss on them deeply.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2748</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2748</id><created>2013-02-12</created><authors><author><keyname>Gandomani</keyname><forenames>Taghi Javdani</forenames></author><author><keyname>Zulzalil</keyname><forenames>Hazura</forenames></author><author><keyname>Ghani</keyname><forenames>Abdul Azim Abdul</forenames></author><author><keyname>Sultan</keyname><forenames>Abu Bakar Md</forenames></author></authors><title>A Systematic Literature Review on relationship between agile methods and
  Open Source Software Development methodology</title><categories>cs.SE</categories><comments>6 pages, 5 tables</comments><journal-ref>Taghi Javdani Gandomani, at al., A systematic literature review on
  relationship between agile SD and open source SD, International review on
  computers and software (IRECOS), 2012, Vol. 7, Issue 4, pp. 1602-1607</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agile software development methods (ASD) and open source software development
methods (OSSD) are two different approaches which were introduced in last
decade and both of them have their fanatical advocators. Yet, it seems that
relation and interface between ASD and OSSD is a fertile area and few rigorous
studies have been done in this matter. Major goal of this study was assessment
of the relation and integration of ASD and OSSD. Analyzing of collected data
shows that ASD and OSSD are able to support each other. Some practices in one
of them are useful in the other. Another finding is that however there are some
case studies using ASD and OSSD simultaneously, but there is not enough
evidence about comprehensive integration of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2749</identifier>
 <datestamp>2013-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2749</id><created>2013-02-12</created><updated>2013-05-03</updated><authors><author><keyname>Pastorelli</keyname><forenames>Mario</forenames></author><author><keyname>Barbuzzi</keyname><forenames>Antonio</forenames></author><author><keyname>Carra</keyname><forenames>Damiano</forenames></author><author><keyname>Dell'Amico</keyname><forenames>Matteo</forenames></author><author><keyname>Michiardi</keyname><forenames>Pietro</forenames></author></authors><title>Practical Size-based Scheduling for MapReduce Workloads</title><categories>cs.DC</categories><comments>12 pages, 8 figures</comments><acm-class>C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Hadoop Fair Sojourn Protocol (HFSP) scheduler, which
implements a size-based scheduling discipline for Hadoop. The benefits of
size-based scheduling disciplines are well recognized in a variety of contexts
(computer networks, operating systems, etc...), yet, their practical
implementation for a system such as Hadoop raises a number of important
challenges. With HFSP, which is available as an open-source project, we address
issues related to job size estimation, resource management and study the
effects of a variety of preemption strategies. Although the architecture
underlying HFSP is suitable for any size-based scheduling discipline, in this
work we revisit and extend the Fair Sojourn Protocol, which solves problems
related to job starvation that affect FIFO, Processor Sharing and a range of
size-based disciplines. Our experiments, in which we compare HFSP to standard
Hadoop schedulers, pinpoint at a significant decrease in average job sojourn
times - a metric that accounts for the total time a job spends in the system,
including waiting and serving times - for realistic workloads that we generate
according to production traces available in literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2752</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2752</id><created>2013-02-12</created><updated>2015-03-25</updated><authors><author><keyname>Gottlieb</keyname><forenames>Lee-Ad</forenames></author><author><keyname>Kontorovich</keyname><forenames>Aryeh</forenames></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author></authors><title>Adaptive Metric Dimensionality Reduction</title><categories>cs.LG cs.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study adaptive data-dependent dimensionality reduction in the context of
supervised learning in general metric spaces. Our main statistical contribution
is a generalization bound for Lipschitz functions in metric spaces that are
doubling, or nearly doubling. On the algorithmic front, we describe an analogue
of PCA for metric spaces: namely an efficient procedure that approximates the
data's intrinsic dimension, which is often much lower than the ambient
dimension. Our approach thus leverages the dual benefits of low dimensionality:
(1) more efficient algorithms, e.g., for proximity search, and (2) more
optimistic generalization bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2757</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2757</id><created>2013-02-12</created><authors><author><keyname>Cederman</keyname><forenames>Daniel</forenames></author><author><keyname>Gidenstam</keyname><forenames>Anders</forenames></author><author><keyname>Ha</keyname><forenames>Phuong</forenames></author><author><keyname>Sundell</keyname><forenames>H&#xe5;kan</forenames></author><author><keyname>Papatriantafilou</keyname><forenames>Marina</forenames></author><author><keyname>Tsigas</keyname><forenames>Philippas</forenames></author></authors><title>Lock-free Concurrent Data Structures</title><categories>cs.DC cs.DS cs.PL</categories><comments>To appear in &quot;Programming Multi-core and Many-core Computing
  Systems&quot;, eds. S. Pllana and F. Xhafa, Wiley Series on Parallel and
  Distributed Computing</comments><acm-class>E.1; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrent data structures are the data sharing side of parallel programming.
Data structures give the means to the program to store data, but also provide
operations to the program to access and manipulate these data. These operations
are implemented through algorithms that have to be efficient. In the sequential
setting, data structures are crucially important for the performance of the
respective computation. In the parallel programming setting, their importance
becomes more crucial because of the increased use of data and resource sharing
for utilizing parallelism.
  The first and main goal of this chapter is to provide a sufficient background
and intuition to help the interested reader to navigate in the complex research
area of lock-free data structures. The second goal is to offer the programmer
familiarity to the subject that will allow her to use truly concurrent methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2759</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2759</id><created>2013-02-12</created><authors><author><keyname>Rajyalakshmi</keyname><forenames>M.</forenames></author><author><keyname>Rao</keyname><forenames>T. Kameswara</forenames></author><author><keyname>Prasad</keyname><forenames>T. V.</forenames></author></authors><title>Exploration of Recent Advances in the Field of Brain Computer Interfaces</title><categories>cs.HC cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach for implementing number of expressions, emotions and, actions
to operate objects through the thoughts of brain using a Non-Invasive Brain
Computing Interface (BCI) technique has been proposed. In this paper a survey
on brain and its operations are presented. The steps involved in the brain
signal processing are discussed. The current systems are able to present few
expressions and emotions on a single device. The proposed system provides the
extended number of expressions on multiple numbers of objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2760</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2760</id><created>2013-02-12</created><authors><author><keyname>Golosovsky</keyname><forenames>Michael</forenames></author><author><keyname>Solomon</keyname><forenames>Sorin</forenames></author></authors><title>The transition towards immortality: non-linear autocatalytic growth of
  citations to scientific papers</title><categories>physics.soc-ph cond-mat.stat-mech cs.DL</categories><comments>to appear soon in the Special issue of the Journal of Statistical
  Physics</comments><doi>10.1007/s10955-013-0714-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss microscopic mechanisms of complex network growth, with the special
emphasis of how these mechanisms can be evaluated from the measurements on real
networks. As an example we consider the network of citations to scientific
papers. Contrary to common belief that its growth is determined by the linear
preferential attachment, our microscopic measurements show that it is driven by
the nonlinear autocatalytic growth. This invalidates the scale-free hypothesis
for the citation network. The nonlinearity is responsible for a dramatic
dynamical phase transition: while the citation lifetime of majority of papers
is 6-10 years, the highly-cited papers have practically infinite lifetime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2762</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2762</id><created>2013-02-12</created><updated>2014-08-20</updated><authors><author><keyname>Iosif</keyname><forenames>Radu</forenames><affiliation>Verimag/CNRS</affiliation></author><author><keyname>Konecny</keyname><forenames>Filip</forenames><affiliation>Verimag/CNRS and FIT/BUT</affiliation></author><author><keyname>Bozga</keyname><forenames>Marius</forenames><affiliation>Verimag/CNRS</affiliation></author></authors><title>Deciding Conditional Termination</title><categories>cs.LO cs.FL</categories><comments>61 pages, 6 figures, 2 tables</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (August
  21, 2014) lmcs:737</journal-ref><doi>10.2168/LMCS-10(3:8)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of conditional termination, which is that of defining
the set of initial configurations from which a given program always terminates.
First we define the dual set, of initial configurations from which a
non-terminating execution exists, as the greatest fixpoint of the function that
maps a set of states into its pre-image with respect to the transition
relation. This definition allows to compute the weakest non-termination
precondition if at least one of the following holds: (i) the transition
relation is deterministic, (ii) the descending Kleene sequence
overapproximating the greatest fixpoint converges in finitely many steps, or
(iii) the transition relation is well founded. We show that this is the case
for two classes of relations, namely octagonal and finite monoid affine
relations. Moreover, since the closed forms of these relations can be defined
in Presburger arithmetic, we obtain the decidability of the termination problem
for such loops.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2767</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2767</id><created>2013-02-12</created><updated>2013-11-02</updated><authors><author><keyname>Kir&#xe1;ly</keyname><forenames>Franz J.</forenames></author><author><keyname>Theran</keyname><forenames>Louis</forenames></author></authors><title>Coherence and sufficient sampling densities for reconstruction in
  compressed sensing</title><categories>cs.LG cs.IT math.AG math.IT stat.ML</categories><comments>16 pages, 1 figure. v2 streamlines the exposition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new, very general, formulation of the compressed sensing problem in
terms of coordinate projections of an analytic variety, and derive sufficient
sampling rates for signal reconstruction. Our bounds are linear in the
coherence of the signal space, a geometric parameter independent of the
specific signal and measurement, and logarithmic in the ambient dimension where
the signal is presented. We exemplify our approach by deriving sufficient
sampling densities for low-rank matrix completion and distance matrix
completion which are independent of the true matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2787</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2787</id><created>2013-02-12</created><updated>2014-03-13</updated><authors><author><keyname>Benjamini</keyname><forenames>Itai</forenames></author><author><keyname>Shinkar</keyname><forenames>Igor</forenames></author><author><keyname>Tsur</keyname><forenames>Gilad</forenames></author></authors><title>Acquaintance Time of a Graph</title><categories>cs.CC cs.DS cs.SI math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define the following parameter of connected graphs. For a given graph $G$
we place one agent in each vertex of $G$. Every pair of agents sharing a common
edge is declared to be acquainted. In each round we choose some matching of $G$
(not necessarily a maximal matching), and for each edge in the matching the
agents on this edge swap places. After the swap, again, every pair of agents
sharing a common edge become acquainted, and the process continues. We define
the \emph{acquaintance time} of a graph $G$, denoted by $AC(G)$, to be the
minimal number of rounds required until every two agents are acquainted.
  We first study the acquaintance time for some natural families of graphs
including the path, expanders, the binary tree, and the complete bipartite
graph. We also show that for all positive integers $n$ and $k \leq n^{1.5}$
there exists an $n$-vertex graph $G$ such that $AC(G) =\Theta(k)$. We also
prove that for all $n$-vertex connected graphs $G$ we have $AC(G) =
O\left(\frac{n^2}{\log(n)/\log\log(n)}\right)$, improving the $O(n^2)$ trivial
upper bound achieved by sequentially letting each agent perform depth-first
search along a spanning tree of $G$.
  Studying the computational complexity of this problem, we prove that for any
constant $t \geq 1$ the problem of deciding that a given graph $G$ has $AC(G)
\leq t$ or $AC(G) \geq 2t$ is $\mathcal{NP}$-complete. That is, $AC(G)$ is
$\mathcal{NP}$-hard to approximate within multiplicative factor of 2, as well
as within any additive constant factor.
  On the algorithmic side, we give a deterministic algorithm that given a graph
$G$ with $AC(G)=1$ finds a ${\lceil n/c\rceil}$-rounds strategy for
acquaintance in time $n^{c+O(1)}$. We also design a randomized polynomial time
algorithm that given a graph $G$ with $AC(G)=1$ finds with high probability an
$O(\log(n))$-rounds strategy for acquaintance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2788</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2788</id><created>2013-02-12</created><authors><author><keyname>Dereniowski</keyname><forenames>Dariusz</forenames></author><author><keyname>Kubiak</keyname><forenames>Wieslaw</forenames></author><author><keyname>Zwols</keyname><forenames>Yori</forenames></author></authors><title>Minimum length path decompositions</title><categories>cs.DS math.CO</categories><comments>Work presented at the 5th Workshop on GRAph Searching, Theory and
  Applications (GRASTA 2012), Banff International Research Station, Banff, AB,
  Canada</comments><report-no>Technical Report 19/2012, Gdansk University of Technology, Faculty
  of Electronics, Telecommunications and Informatics</report-no><msc-class>68Q25, 05C85, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a bi-criteria generalization of the pathwidth problem, where, for
given integers $k,l$ and a graph $G$, we ask whether there exists a path
decomposition $\cP$ of $G$ such that the width of $\cP$ is at most $k$ and the
number of bags in $\cP$, i.e., the \emph{length} of $\cP$, is at most $l$.
  We provide a complete complexity classification of the problem in terms of
$k$ and $l$ for general graphs. Contrary to the original pathwidth problem,
which is fixed-parameter tractable with respect to $k$, we prove that the
generalized problem is NP-complete for any fixed $k\geq 4$, and is also
NP-complete for any fixed $l\geq 2$. On the other hand, we give a
polynomial-time algorithm that, for any (possibly disconnected) graph $G$ and
integers $k\leq 3$ and $l&gt;0$, constructs a path decomposition of width at most
$k$ and length at most $l$, if any exists.
  As a by-product, we obtain an almost complete classification of the problem
in terms of $k$ and $l$ for connected graphs. Namely, the problem is
NP-complete for any fixed $k\geq 5$ and it is polynomial-time for any $k\leq
3$. This leaves open the case $k=4$ for connected graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2805</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2805</id><created>2013-02-12</created><authors><author><keyname>Komm</keyname><forenames>Dennis</forenames></author><author><keyname>Kr&#xe1;lovi&#x10d;</keyname><forenames>Rastislav</forenames></author><author><keyname>Kr&#xe1;lovi&#x10d;</keyname><forenames>Richard</forenames></author><author><keyname>M&#xf6;mke</keyname><forenames>Tobias</forenames></author></authors><title>Randomized online computation with high probability guarantees</title><categories>cs.DS</categories><comments>20 pages, 2 figures</comments><acm-class>F.1.2; F.2.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the relationship between the competitive ratio and the tail
distribution of randomized online minimization problems. To this end, we define
a broad class of online problems that includes some of the well-studied
problems like paging, k-server and metrical task systems on finite metrics, and
show that for these problems it is possible to obtain, given an algorithm with
constant expected competitive ratio, another algorithm that achieves the same
solution quality up to an arbitrarily small constant error a with high
probability; the &quot;high probability&quot; statement is in terms of the optimal cost.
Furthermore, we show that our assumptions are tight in the sense that removing
any of them allows for a counterexample to the theorem. In addition, there are
examples of other problems not covered by our definition, where similar high
probability results can be obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2814</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2814</id><created>2013-02-12</created><authors><author><keyname>Infante</keyname><forenames>Jorge</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author></authors><title>Voice over IP in the WiFi Network business models: Will voice be a
  killer application for WiFi Public Networks?</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stunning growth of WiFi networks, together with the spreading of mobile
telephony and the increasing use of Voice over IP (VoIP) on top of Internet,
pose relevant questions on the application of WiFi networks to support VoIP
services that could be used as a complement and/or competition to the 2G/3G
cellular networks. The paper explores the state of the art on the capability of
WiFi networks to support voice services in a user itinerant context,
identifying actual and future strengths, weakness, opportunities and threats.
First of all, the paper reviews the key aspects on the changing structure of
the voice market, dominated by a clear evolution to mobility, itinerancy and
increased use of VoIP-based services. The evolution of business models,
coverage for WiFi networks (hotspots, municipal networks, cooperative networks
and) and user voice service use patterns are also analysed in order to asses
the potentialities for complementing or -in some scenarios- substitute the
cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2818</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2818</id><created>2013-02-12</created><updated>2013-03-01</updated><authors><author><keyname>Kiefer</keyname><forenames>Stefan</forenames><affiliation>Oxford University</affiliation></author><author><keyname>Murawski</keyname><forenames>Andrzej</forenames><affiliation>University of Leicester</affiliation></author><author><keyname>Ouaknine</keyname><forenames>Joel</forenames><affiliation>Oxford University</affiliation></author><author><keyname>Wachter</keyname><forenames>Bjoern</forenames><affiliation>Oxford University</affiliation></author><author><keyname>Worrell</keyname><forenames>James</forenames><affiliation>Oxford University</affiliation></author></authors><title>On the Complexity of Equivalence and Minimisation for Q-weighted
  Automata</title><categories>cs.FL</categories><comments>arXiv admin note: text overlap with arXiv:1112.4644</comments><proxy>LMCS</proxy><acm-class>F.2.1, F.3.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 1 (March 4,
  2013) lmcs:908</journal-ref><doi>10.2168/LMCS-9(1:8)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the computational complexity of equivalence and
minimisation for automata with transition weights in the field Q of rational
numbers. We use polynomial identity testing and the Isolation Lemma to obtain
complexity bounds, focussing on the class NC of problems within P solvable in
polylogarithmic parallel time. For finite Q-weighted automata, we give a
randomised NC procedure that either outputs that two automata are equivalent or
returns a word on which they differ. We also give an NC procedure for deciding
whether a given automaton is minimal, as well as a randomised NC procedure that
minimises an automaton. We consider probabilistic automata with rewards,
similar to Markov Decision Processes. For these automata we consider two
notions of equivalence: expectation equivalence and distribution equivalence.
The former requires that two automata have the same expected reward on each
input word, while the latter requires that each input word induce the same
distribution on rewards in each automaton. For both notions we give algorithms
for deciding equivalence by reduction to equivalence of Q-weighted automata.
Finally we show that the equivalence problem for Q-weighted visibly pushdown
automata is logspace equivalent to the polynomial identity testing problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2820</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2820</id><created>2013-02-12</created><authors><author><keyname>Mattern</keyname><forenames>Christopher</forenames></author></authors><title>Linear and Geometric Mixtures - Analysis</title><categories>cs.IT math.IT</categories><comments>Data Compression Conference (DCC) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear and geometric mixtures are two methods to combine arbitrary models in
data compression. Geometric mixtures generalize the empirically well-performing
PAQ7 mixture. Both mixture schemes rely on weight vectors, which heavily
determine their performance. Typically weight vectors are identified via Online
Gradient Descent. In this work we show that one can obtain strong code length
bounds for such a weight estimation scheme. These bounds hold for arbitrary
input sequences. For this purpose we introduce the class of nice mixtures and
analyze how Online Gradient Descent with a fixed step size combined with a nice
mixture performs. These results translate to linear and geometric mixtures,
which are nice, as we show. The results hold for PAQ7 mixtures as well, thus we
provide the first theoretical analysis of PAQ7.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2824</identifier>
 <datestamp>2013-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2824</id><created>2013-02-12</created><updated>2013-05-23</updated><authors><author><keyname>Simatos</keyname><forenames>Florian</forenames></author><author><keyname>Bouman</keyname><forenames>Niek</forenames></author><author><keyname>Borst</keyname><forenames>Sem</forenames></author></authors><title>Lingering Issues in Distributed Scheduling</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances have resulted in queue-based algorithms for medium access
control which operate in a distributed fashion, and yet achieve the optimal
throughput performance of centralized scheduling algorithms. However,
fundamental performance bounds reveal that the &quot;cautious&quot; activation rules
involved in establishing throughput optimality tend to produce extremely large
delays, typically growing exponentially in 1/(1-r), with r the load of the
system, in contrast to the usual linear growth.
  Motivated by that issue, we explore to what extent more &quot;aggressive&quot; schemes
can improve the delay performance. Our main finding is that aggressive
activation rules induce a lingering effect, where individual nodes retain
possession of a shared resource for excessive lengths of time even while a
majority of other nodes idle. Using central limit theorem type arguments, we
prove that the idleness induced by the lingering effect may cause the delays to
grow with 1/(1-r) at a quadratic rate. To the best of our knowledge, these are
the first mathematical results illuminating the lingering effect and
quantifying the performance impact.
  In addition extensive simulation experiments are conducted to illustrate and
validate the various analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2826</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2826</id><created>2013-02-12</created><updated>2013-04-02</updated><authors><author><keyname>Chamon</keyname><forenames>Claudio</forenames></author><author><keyname>Mucciolo</keyname><forenames>Eduardo R.</forenames></author></authors><title>Renyi entropies as a measure of the complexity of counting problems</title><categories>cond-mat.stat-mech cs.CC quant-ph</categories><comments>13 pages, 4 figures</comments><journal-ref>J. Stat. Mech. (2013) P04008</journal-ref><doi>10.1088/1742-5468/2013/04/P04008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counting problems such as determining how many bit strings satisfy a given
Boolean logic formula are notoriously hard. In many cases, even getting an
approximate count is difficult. Here we propose that entanglement, a common
concept in quantum information theory, may serve as a telltale of the
difficulty of counting exactly or approximately. We quantify entanglement by
using Renyi entropies S(q), which we define by bipartitioning the logic
variables of a generic satisfiability problem. We conjecture that
S(q\rightarrow 0) provides information about the difficulty of counting
solutions exactly, while S(q&gt;0) indicates the possibility of doing an efficient
approximate counting. We test this conjecture by employing a matrix computing
scheme to numerically solve #2SAT problems for a large number of uniformly
distributed instances. We find that all Renyi entropies scale linearly with the
number of variables in the case of the #2SAT problem; this is consistent with
the fact that neither exact nor approximate efficient algorithms are known for
this problem. However, for the negated (disjunctive) form of the problem,
S(q\rightarrow 0) scales linearly while S(q&gt;0) tends to zero when the number of
variables is large. These results are consistent with the existence of fully
polynomial-time randomized approximate algorithms for counting solutions of
disjunctive normal forms and suggests that efficient algorithms for the
conjunctive normal form may not exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2828</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2828</id><created>2013-02-12</created><authors><author><keyname>&#x10c;&#xe1;p</keyname><forenames>Michal</forenames></author><author><keyname>Nov&#xe1;k</keyname><forenames>Peter</forenames></author><author><keyname>Vok&#x159;&#xed;nek</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>P&#x11b;chou&#x10d;ek</keyname><forenames>Michal</forenames></author></authors><title>Multi-agent RRT*: Sampling-based Cooperative Pathfinding (Extended
  Abstract)</title><categories>cs.RO cs.AI cs.MA</categories><comments>To appear at AAMAS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative pathfinding is a problem of finding a set of non-conflicting
trajectories for a number of mobile agents. Its applications include planning
for teams of mobile robots, such as autonomous aircrafts, cars, or underwater
vehicles. The state-of-the-art algorithms for cooperative pathfinding typically
rely on some heuristic forward-search pathfinding technique, where A* is often
the algorithm of choice. Here, we propose MA-RRT*, a novel algorithm for
multi-agent path planning that builds upon a recently proposed
asymptotically-optimal sampling-based algorithm for finding single-agent
shortest path called RRT*. We experimentally evaluate the performance of the
algorithm and show that the sampling-based approach offers better scalability
than the classical forward-search approach in relatively large, but sparse
environments, which are typical in real-world applications such as
multi-aircraft collision avoidance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2837</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2837</id><created>2013-02-12</created><updated>2014-10-23</updated><authors><author><keyname>Nanz</keyname><forenames>Sebastian</forenames></author><author><keyname>West</keyname><forenames>Scott</forenames></author><author><keyname>da Silveira</keyname><forenames>Kaue Soares</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author></authors><title>Benchmarking Usability and Performance of Multicore Languages</title><categories>cs.DC cs.PL</categories><journal-ref>Proceedings of the 7th ACM-IEEE International Symposium Empirical
  Software Engineering and Measurement (ESEM'13), pages 183-192. IEEE, 2013</journal-ref><doi>10.1109/ESEM.2013.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developers face a wide choice of programming languages and libraries
supporting multicore computing. Ever more diverse paradigms for expressing
parallelism and synchronization become available while their influence on
usability and performance remains largely unclear. This paper describes an
experiment comparing four markedly different approaches to parallel
programming: Chapel, Cilk, Go, and Threading Building Blocks (TBB). Each
language is used to implement sequential and parallel versions of six benchmark
programs. The implementations are then reviewed by notable experts in the
language, thereby obtaining reference versions for each language and benchmark.
The resulting pool of 96 implementations is used to compare the languages with
respect to source code size, coding time, execution time, and speedup. The
experiment uncovers strengths and weaknesses in all approaches, facilitating an
informed selection of a language under a particular set of requirements. The
expert review step furthermore highlights the importance of expert knowledge
when using modern parallel programming approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2839</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2839</id><created>2013-02-12</created><authors><author><keyname>Mattern</keyname><forenames>Christopher</forenames></author></authors><title>Mixing Strategies in Data Compression</title><categories>cs.IT math.IT</categories><comments>Data Compression Conference (DCC) 2012</comments><doi>10.1109/DCC.2012.40</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose geometric weighting as a novel method to combine multiple models
in data compression. Our results reveal the rationale behind PAQ-weighting and
generalize it to a non-binary alphabet. Based on a similar technique we present
a new, generic linear mixture technique. All novel mixture techniques rely on
given weight vectors. We consider the problem of finding optimal weights and
show that the weight optimization leads to a strictly convex (and thus,
good-natured) optimization problem. Finally, an experimental evaluation
compares the two presented mixture techniques for a binary alphabet. The
results indicate that geometric weighting is superior to linear weighting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2840</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2840</id><created>2013-02-12</created><authors><author><keyname>Kari</keyname><forenames>Lila</forenames></author><author><keyname>Kopecki</keyname><forenames>Steffen</forenames></author><author><keyname>Simjour</keyname><forenames>Amirhossein</forenames></author></authors><title>Hypergraph Automata: A Theoretical Model for Patterned Self-assembly</title><categories>cs.DM cs.FL math.CO</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Patterned self-assembly is a process whereby coloured tiles self-assemble to
build a rectangular coloured pattern. We propose self-assembly (SA) hypergraph
automata as an automata-theoretic model for patterned self-assembly. We
investigate the computational power of SA-hypergraph automata and show that for
every recognizable picture language, there exists an SA-hypergraph automaton
that accepts this language. Conversely, we prove that for any restricted
SA-hypergraph automaton, there exists a Wang Tile System, a model for
recognizable picture languages, that accepts the same language. The advantage
of SA-hypergraph automata over Wang automata, acceptors for the class of
recognizable picture languages, is that they do not rely on an a priori defined
scanning strategy
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2855</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2855</id><created>2013-02-12</created><authors><author><keyname>Seidl</keyname><forenames>Mathis</forenames></author><author><keyname>Schenk</keyname><forenames>Andreas</forenames></author><author><keyname>Stierstorfer</keyname><forenames>Clemens</forenames></author><author><keyname>Huber</keyname><forenames>Johannes B.</forenames></author></authors><title>Polar-Coded Modulaton</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A framework is proposed that allows for a joint description and optimization
of both binary polar coding and $2^m$-ary digital pulse-amplitude modulation
(PAM) schemes such as multilevel coding (MLC) and bit-interleaved coded
modulation (BICM). The conceptual equivalence of polar coding and multilevel
coding is pointed out in detail. Based on a novel characterization of the
channel polarization phenomenon, rules for the optimal choice of the labeling
in coded modulation schemes employing polar codes are developed. Simulation
results regarding the error performance of the proposed schemes on the AWGN
channel are included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2856</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2856</id><created>2013-02-12</created><authors><author><keyname>Mattern</keyname><forenames>Christopher</forenames></author></authors><title>Combining non-stationary prediction, optimization and mixing for data
  compression</title><categories>cs.IT math.IT</categories><comments>International Conference on Data Compression, Communication and
  Processing (CCP) 2011</comments><doi>10.1109/CCP.2011.22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper an approach to modelling nonstationary binary sequences, i.e.,
predicting the probability of upcoming symbols, is presented. After studying
the prediction model we evaluate its performance in two non-artificial test
cases. First the model is compared to the Laplace and Krichevsky-Trofimov
estimators. Secondly a statistical ensemble model for compressing
Burrows-Wheeler-Transform output is worked out and evaluated. A systematic
approach to the parameter optimization of an individual model and the ensemble
model is stated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2869</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2869</id><created>2013-02-12</created><updated>2013-07-07</updated><authors><author><keyname>Nguyen</keyname><forenames>Thanh</forenames></author><author><keyname>Subramanian</keyname><forenames>Vijay G.</forenames></author><author><keyname>Berry</keyname><forenames>Randall A.</forenames></author></authors><title>Searching and Bargaining with Middlemen</title><categories>cs.GT math.PR</categories><comments>39 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study decentralized markets with the presence of middlemen, modeled by a
non-cooperative bargaining game in trading networks. Our goal is to investigate
how the network structure of the market and the role of middlemen influence the
market's efficiency and fairness. We introduce the concept of limit stationary
equilibrium in a general trading network and use it to analyze how competition
among middlemen is influenced by the network structure, how endogenous delay
emerges in trade and how surplus is shared between producers and consumers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2875</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2875</id><created>2013-02-12</created><updated>2014-10-07</updated><authors><author><keyname>Yousefi</keyname><forenames>Mansoor I.</forenames></author><author><keyname>Kschischang</keyname><forenames>Frank R.</forenames></author></authors><title>Information Transmission using the Nonlinear Fourier Transform, Part
  III: Spectrum Modulation</title><categories>cs.IT math.IT</categories><comments>Updated version of IEEE Transactions on Information Theory, vol. 60,
  no. 7, pp. 4346--4369, July, 2014</comments><journal-ref>IEEE Transactions on Information Theory, vol. 60, no. 7, pp.
  4346--4369, July, 2014</journal-ref><doi>10.1109/TIT.2014.2321155</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the looming &quot;capacity crunch&quot; in fiber-optic networks,
information transmission over such systems is revisited. Among numerous
distortions, inter-channel interference in multiuser wavelength-division
multiplexing (WDM) is identified as the seemingly intractable factor limiting
the achievable rate at high launch power. However, this distortion and similar
ones arising from nonlinearity are primarily due to the use of methods suited
for linear systems, namely WDM and linear pulse-train transmission, for the
nonlinear optical channel. Exploiting the integrability of the nonlinear
Schr\&quot;odinger (NLS) equation, a nonlinear frequency-division multiplexing
(NFDM) scheme is presented, which directly modulates non-interacting signal
degrees-of-freedom under NLS propagation. The main distinction between this and
previous methods is that NFDM is able to cope with the nonlinearity, and thus,
as the the signal power or transmission distance is increased, the new method
does not suffer from the deterministic cross-talk between signal components
which has degraded the performance of previous approaches. In this paper,
emphasis is placed on modulation of the discrete component of the nonlinear
Fourier transform of the signal and some simple examples of achievable spectral
efficiencies are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2937</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2937</id><created>2013-02-12</created><authors><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author><author><keyname>Chater</keyname><forenames>Nick</forenames></author><author><keyname>Pastor-Satorras</keyname><forenames>Romualdo</forenames></author><author><keyname>Christiansen</keyname><forenames>Morten H.</forenames></author></authors><title>The Biological Origin of Linguistic Diversity</title><categories>physics.soc-ph cs.MA q-bio.PE</categories><journal-ref>PLoS ONE 7(10): e48029 (2012)</journal-ref><doi>10.1371/journal.pone.0048029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In contrast with animal communication systems, diversity is characteristic of
almost every aspect of human language. Languages variously employ tones,
clicks, or manual signs to signal differences in meaning; some languages lack
the noun-verb distinction (e.g., Straits Salish), whereas others have a
proliferation of fine-grained syntactic categories (e.g., Tzeltal); and some
languages do without morphology (e.g., Mandarin), while others pack a whole
sentence into a single word (e.g., Cayuga). A challenge for evolutionary
biology is to reconcile the diversity of languages with the high degree of
biological uniformity of their speakers. Here, we model processes of language
change and geographical dispersion and find a consistent pressure for flexible
learning, irrespective of the language being spoken. This pressure arises
because flexible learners can best cope with the observed high rates of
linguistic change associated with divergent cultural evolution following human
migration. Thus, rather than genetic adaptations for specific aspects of
language, such as recursion, the coevolution of genes and fast-changing
linguistic structure provides the biological basis for linguistic diversity.
Only biological adaptations for flexible learning combined with cultural
evolution can explain how each child has the potential to learn any human
language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2966</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2966</id><created>2013-02-12</created><authors><author><keyname>Sakr</keyname><forenames>Sherif</forenames></author><author><keyname>Liu</keyname><forenames>Anna</forenames></author><author><keyname>Fayoumi</keyname><forenames>Ayman G.</forenames></author></authors><title>The Family of MapReduce and Large Scale Data Processing Systems</title><categories>cs.DB</categories><comments>arXiv admin note: text overlap with arXiv:1105.4252 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last two decades, the continuous increase of computational power has
produced an overwhelming flow of data which has called for a paradigm shift in
the computing architecture and large scale data processing mechanisms.
MapReduce is a simple and powerful programming model that enables easy
development of scalable parallel applications to process vast amounts of data
on large clusters of commodity machines. It isolates the application from the
details of running a distributed program such as issues on data distribution,
scheduling and fault tolerance. However, the original implementation of the
MapReduce framework had some limitations that have been tackled by many
research efforts in several followup works after its introduction. This article
provides a comprehensive survey for a family of approaches and mechanisms of
large scale data processing mechanisms that have been implemented based on the
original idea of the MapReduce framework and are currently gaining a lot of
momentum in both research and industrial communities. We also cover a set of
introduced systems that have been implemented to provide declarative
programming interfaces on top of the MapReduce framework. In addition, we
review several large scale data processing systems that resemble some of the
ideas of the MapReduce framework for different purposes and application
scenarios. Finally, we discuss some of the future research directions for
implementing the next generation of MapReduce-like solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.2994</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.2994</id><created>2013-02-13</created><authors><author><keyname>Kaced</keyname><forenames>Tarik</forenames></author></authors><title>Equivalence of Two Proof Techniques for Non-Shannon-type Inequalities</title><categories>cs.IT math.IT math.PR</categories><comments>5 pages paper submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare two different techniques for proving non-Shannon-type information
inequalities. The first one is the original Zhang-Yeung's method, commonly
referred to as the copy/pasting lemma/trick. The copy lemma was used to derive
the first conditional and unconditional non-Shannon-type inequalities. The
second technique first appeared in Makarychev et al paper [7] and is based on a
coding lemma from Ahlswede and K\&quot;orner works. We first emphasize the
importance of balanced inequalities and provide a simpler proof of a theorem of
Chan's for the case of Shannon-type inequalities. We compare the power of
various proof systems based on a single technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3020</identifier>
 <datestamp>2013-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3020</id><created>2013-02-13</created><updated>2013-09-24</updated><authors><author><keyname>Callegari</keyname><forenames>Sergio</forenames></author><author><keyname>Bizzarri</keyname><forenames>Federico</forenames></author></authors><title>Output Filter Aware Optimization of the Noise Shaping Properties of
  {\Delta}{\Sigma} Modulators via Semi-Definite Programming</title><categories>cs.IT math.IT</categories><comments>14 pages, 18 figures, journal. Code accompanying the paper is
  available at http://pydsm.googlecode.com</comments><journal-ref>IEEE Transactions on Circuits and Systems - Part I, Regular
  Papers. Vol. 60, N. 9, pp 2352 - 2365, Sept. 2013</journal-ref><doi>10.1109/TCSI.2013.2239091</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Noise Transfer Function (NTF) of {\Delta}{\Sigma} modulators is typically
designed after the features of the input signal. We suggest that in many
applications, and notably those involving D/D and D/A conversion or actuation,
the NTF should instead be shaped after the properties of the
output/reconstruction filter. To this aim, we propose a framework for optimal
design based on the Kalman-Yakubovich-Popov (KYP) lemma and semi-definite
programming. Some examples illustrate how in practical cases the proposed
strategy can outperform more standard approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3033</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3033</id><created>2013-02-13</created><authors><author><keyname>Tai</keyname><forenames>Chih-Hua</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author><author><keyname>Yang</keyname><forenames>De-Nian</forenames></author><author><keyname>Chen</keyname><forenames>Ming-Syan</forenames></author></authors><title>Structural Diversity for Resisting Community Identification in Published
  Social Networks</title><categories>cs.SI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an increasing number of social networking data is published and shared for
commercial and research purposes, privacy issues about the individuals in
social networks have become serious concerns. Vertex identification, which
identifies a particular user from a network based on background knowledge such
as vertex degree, is one of the most important problems that has been
addressed. In reality, however, each individual in a social network is inclined
to be associated with not only a vertex identity but also a community identity,
which can represent the personal privacy information sensitive to the public,
such as political party affiliation. This paper first addresses the new privacy
issue, referred to as community identification, by showing that the community
identity of a victim can still be inferred even though the social network is
protected by existing anonymity schemes. For this problem, we then propose the
concept of \textit{structural diversity} to provide the anonymity of the
community identities. The $k$-Structural Diversity Anonymization ($k$-SDA) is
to ensure sufficient vertices with the same vertex degree in at least $k$
communities in a social network. We propose an Integer Programming formulation
to find optimal solutions to $k$-SDA and also devise scalable heuristics to
solve large-scale instances of $k$-SDA from different perspectives. The
performance studies on real data sets from various perspectives demonstrate the
practical utility of the proposed privacy scheme and our anonymization
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3035</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3035</id><created>2013-02-13</created><authors><author><keyname>Hlava</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Yet another approach to the Maximum Flow</title><categories>cs.DS</categories><comments>preprint. uploaded to make sure, that this idea will not be stolen</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I introduce a new approach to the maximum flow problem by a simple algorithm
with a slightly better runtime. This approach is based on sorting arcs insight
of vertices on a residual graph. This new approach leads to an O(mn^0.5) time
bound for a network with n vertices and m arcs.
  Category: Algorithms, Graph Theory and maximum flows
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3045</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3045</id><created>2013-02-13</created><updated>2013-07-20</updated><authors><author><keyname>Nath</keyname><forenames>Swaprava</forenames></author><author><keyname>Narayanaswamy</keyname><forenames>Balakrishnan</forenames></author></authors><title>Improving Productive Output in Influencer-Influencee Networks</title><categories>cs.GT</categories><comments>29 pages, 6 figures</comments><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In any organization, a primary goal of the designer is to maximize the net
productive output. Every organization has a network (either hierarchical or
other) that is used to divide the tasks. It is observed that the individuals in
these networks trade off between their productive and managing efforts to do
these tasks and the trade-off is caused by their positions and share of rewards
in the network. Efforts of the agents here are substitutable, e.g., the
increase in the productive effort by an individual in effect reduces the same
of some other individual in the network, who now puts their efforts into
management. The management effort of an agent improves the productivity of
certain other agents in the network. In this paper, we provide a detailed
analysis of the Nash equilibrium efforts of the individuals connected over a
network and a design recipe of the reward sharing scheme that maximizes the net
productive output. Our results show that under the strategic behavior of the
agents, it may not always be possible to achieve the optimal output and we
provide bounds on the achievability in such scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3051</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3051</id><created>2013-02-13</created><updated>2014-06-30</updated><authors><author><keyname>Kim</keyname><forenames>Ryul</forenames></author><author><keyname>Song</keyname><forenames>Ok-Hyon</forenames></author><author><keyname>Ri</keyname><forenames>Hyon-Chol</forenames></author></authors><title>Some Properties of Generalized Self-reciprocal Polynomials over Finite
  Fields</title><categories>math.NT cs.IT math.IT math.RA</categories><comments>8 pages</comments><report-no>KISU-MATH-2011-E-R-003</report-no><msc-class>11T06</msc-class><journal-ref>Romanian Journal of Mathematics and Computer Science, Vol.4, No.2,
  2014, pp.131-138</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous results on self-reciprocal polynomials over finite fields have been
studied. In this paper we generalize some of these to a-self reciprocal
polynomials defined in [4]. We consider some properties of the divisibility of
a-reciprocal polynomials and characterize the parity of the number of
irreducible factors for a-self reciprocal polynomials over finite fields of odd
characteristic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3057</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3057</id><created>2013-02-13</created><authors><author><keyname>Dlougach</keyname><forenames>Jacob</forenames></author><author><keyname>Galinskaya</keyname><forenames>Irina</forenames></author></authors><title>Building a reordering system using tree-to-string hierarchical model</title><categories>cs.CL</categories><comments>8 pages + references, published in Proceedings of COLING 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes our submission to the First Workshop on Reordering for
Statistical Machine Translation. We have decided to build a reordering system
based on tree-to-string model, using only publicly available tools to
accomplish this task. With the provided training data we have built a
translation model using Moses toolkit, and then we applied a chart decoder,
implemented in Moses, to reorder the sentences. Even though our submission only
covered English-Farsi language pair, we believe that the approach itself should
work regardless of the choice of the languages, so we have also carried out the
experiments for English-Italian and English-Urdu. For these language pairs we
have noticed a significant improvement over the baseline in BLEU, Kendall-Tau
and Hamming metrics. A detailed description is given, so that everyone can
reproduce our results. Also, some possible directions for further improvements
are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3081</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3081</id><created>2013-02-13</created><updated>2013-06-02</updated><authors><author><keyname>Sharir</keyname><forenames>Micha</forenames></author><author><keyname>Sheffer</keyname><forenames>Adam</forenames></author><author><keyname>Solymosi</keyname><forenames>J&#xf3;zsef</forenames></author></authors><title>Distinct distances on two lines</title><categories>math.CO cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let P_1 and P_2 be two sets of points in the plane, so that P_1 is contained
in a line L_1, P_2 is contained in a line L_2, and L_1 and L_2 are neither
parallel nor orthogonal. Then the number of distinct distances determined by
the pairs of P_1xP_2 is \Omega(\min{|P_1|^{2/3}|P_2|^{2/3},|P_1|^2, |P_2|^2}).
  In particular, if |P_1|=|P_2|=m, then the number of these distinct distances
is \Omega(m^{4/3}), improving upon the previous bound \Omega(m^{5/4}) of
Elekes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3085</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3085</id><created>2013-02-13</created><authors><author><keyname>Hou</keyname><forenames>I-Hong</forenames><affiliation>LINCS</affiliation></author><author><keyname>Chen</keyname><forenames>Chung Shue</forenames><affiliation>LINCS</affiliation></author></authors><title>An Energy-Aware Protocol for Self-Organizing Heterogeneous LTE Systems</title><categories>cs.NI</categories><proxy>ccsd</proxy><doi>10.1109/JSAC.2013.130512</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of self-organizing heterogeneous LTE systems.
We propose a model that jointly considers several important characteristics of
heterogeneous LTE system, including the usage of orthogonal frequency division
multiple access (OFDMA), the frequency-selective fading for each link, the
interference among different links, and the different transmission capabilities
of different types of base stations. We also consider the cost of energy by
taking into account the power consumption, including that for wireless
transmission and that for operation, of base stations and the price of energy.
Based on this model, we aim to propose a distributed protocol that improves the
spectrum efficiency of the system, which is measured in terms of the weighted
proportional fairness among the throughputs of clients, and reduces the cost of
energy. We identify that there are several important components involved in
this problem. We propose distributed strategies for each of these components.
Each of the proposed strategies requires small computational and
communicational overheads. Moreover, the interactions between components are
also considered in the proposed strategies. Hence, these strategies result in a
solution that jointly considers all factors of heterogeneous LTE systems.
Simulation results also show that our proposed strategies achieve much better
performance than existing ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3086</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3086</id><created>2013-02-13</created><authors><author><keyname>Jarynowski</keyname><forenames>Andrzej</forenames></author><author><keyname>Jankowski</keyname><forenames>Jaroslaw</forenames></author><author><keyname>Zbieg</keyname><forenames>Anita</forenames></author></authors><title>Viral spread with or without emotions in online community</title><categories>cs.SI nlin.AO physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion of information and viral content, social contagion and influence
are still topics of broad evaluation. We have studied the information epidemic
in a social networking platform in order compare different campaign setups. The
goal of this work is to present the new knowledge obtained from studying two
artificial (experimental) and one natural (where people act emotionally) viral
spread that took place in a closed virtual world. We propose an approach to
modeling the behavior of online community exposed on external impulses as an
epidemic process. The presented results base on online multilayer system
observation, and show characteristic difference between setups, moreover, some
important aspects of branching processes are presented. We run experiments,
where we introduced viral to system and agents were able to propagate it. There
were two modes of experiment: with or without award. Dynamic of spreading both
of virals were described by epidemiological model and diffusion. Results of
experiments were compared with real propagation process - spontaneous
organization against ACTA. During general-national protest against new
antypiracy multinational agreement - ACTA, criticized for its adverse effect on
e.g. freedom of expression and privacy of communication, members of chosen
community could send a viral such as Stop-ACTA transparent. In this scenario,
we are able to capture behavior of society, when real emotions play a role, and
compare results with artificiality conditioned experiments. Moreover, we could
measure effect of emotions in viral propagation. As theory explaining the role
of emotions in spreading behaviour as an factor of message targeting and
individuals spread emotional-oriented content in a more carefully and more
influential way, the experiments show that probabilities of secondary
infections are four times bigger if emotions play a role.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3091</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3091</id><created>2013-02-13</created><authors><author><keyname>Mitchell</keyname><forenames>Joseph S. B.</forenames></author><author><keyname>Polishchuk</keyname><forenames>Valentin</forenames></author><author><keyname>Sysikaski</keyname><forenames>Mikko</forenames></author></authors><title>Minimum-Link Paths Revisited</title><categories>cs.CG</categories><comments>29 pages, 22 figures</comments><msc-class>68W01</msc-class><acm-class>F.2.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A path or a polygonal domain is C-oriented if the orientations of its edges
belong to a set of C given orientations; this is a generalization of the
notable rectilinear case (C = 2). We study exact and approximation algorithms
for minimum-link C-oriented paths and paths with unrestricted orientations,
both in C-oriented and in general domains. Our two main algorithms are as
follows:
  A subquadratic-time algorithm with a non-trivial approximation guarantee for
general (unrestricted-orientation) minimum-link paths in general domains.
  An algorithm to find a minimum-link C-oriented path in a C-oriented domain.
Our algorithm is simpler and more time-space efficient than the prior
algorithm.
  We also obtain several related results:
  - 3SUM-hardness of determining the link distance with unrestricted
orientations (even in a rectilinear domain).
  - An optimal algorithm for finding a minimum-link rectilinear path in a
rectilinear domain. The algorithm and its analysis are simpler than the
existing ones.
  - An extension of our methods to find a C-oriented minimum-link path in a
general (not necessarily C-oriented) domain.
  - A more efficient algorithm to compute a 2-approximate C-oriented
minimum-link path.
  - A notion of &quot;robust&quot; paths. We show how minimum-link C-oriented paths
approximate the robust paths with unrestricted orientations to within an
additive error of 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3101</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3101</id><created>2013-02-13</created><authors><author><keyname>Zeng</keyname><forenames>An</forenames></author><author><keyname>Gualdi</keyname><forenames>Stanislao</forenames></author><author><keyname>Medo</keyname><forenames>Matus</forenames></author><author><keyname>Zhang</keyname><forenames>Yi-Cheng</forenames></author></authors><title>Trend prediction in temporal bipartite networks: the case of Movielens,
  Netflix, and Digg</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, 1 table, 5 figures</comments><journal-ref>Advances in Complex Systems 16, 1350024, 2013</journal-ref><doi>10.1142/S0219525913500240</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online systems where users purchase or collect items of some kind can be
effectively represented by temporal bipartite networks where both nodes and
links are added with time. We use this representation to predict which items
might become popular in the near future. Various prediction methods are
evaluated on three distinct datasets originating from popular online services
(Movielens, Netflix, and Digg). We show that the prediction performance can be
further enhanced if the user social network is known and centrality of
individual users in this network is used to weight their actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3105</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3105</id><created>2013-02-09</created><authors><author><keyname>Atig</keyname><forenames>Mohamed Faouzi</forenames><affiliation>Uppsala University</affiliation></author><author><keyname>Rezine</keyname><forenames>Ahmed</forenames><affiliation>Linkoping University</affiliation></author></authors><title>Proceedings 14th International Workshop on Verification of
  Infinite-State Systems</title><categories>cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 107, 2013</journal-ref><doi>10.4204/EPTCS.107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of Infinity'12, the 14th International
Workshop on Verification of Infinite-State Systems, which was held in Paris,
France on the 27th of August 2012 as a satellite event of FM'12. The aim of the
INFINITY workshop is to provide a forum for researchers interested in the
development of formal methods and algorithmic techniques for the analysis of
systems with infinitely many states, and their application in automated
verification of complex software and hardware systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3109</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3109</id><created>2013-02-13</created><updated>2013-02-20</updated><authors><author><keyname>Fearnley</keyname><forenames>John</forenames></author><author><keyname>Jurdzi&#x144;ski</keyname><forenames>Marcin</forenames></author></authors><title>Reachability in Two-Clock Timed Automata is PSPACE-complete</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent result of Haase et al. has shown that reachability in two-clock
timed automata is log-space equivalent to reachability in bounded one-counter
automata. We show that reachability in bounded one-counter automata is
PSPACE-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3110</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3110</id><created>2013-02-13</created><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author><author><keyname>Imre</keyname><forenames>Sandor</forenames></author></authors><title>Concatenated Capacity-Achieving Polar Codes for Optical Quantum Channels</title><categories>quant-ph cs.IT math.IT</categories><comments>4 pages, 2 figures, summary of the full paper. Journal-ref: Frontiers
  in Optics 2012 (OSA FiO 2012), Section on Quantum Computation and
  Communications, Rochester, New York, USA. The Optical Society of America</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct concatenated capacity-achieving quantum codes for noisy optical
quantum channels. We demonstrate that the error-probability of
capacity-achieving quantum polar encoding can be reduced by the proposed
low-complexity concatenation scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3114</identifier>
 <datestamp>2013-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3114</id><created>2013-02-13</created><updated>2013-05-26</updated><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>Polaractivation of Hidden Private Classical Capacity Region of Quantum
  Channels</title><categories>quant-ph cs.IT math.IT</categories><comments>49 pages, 13 figures (with supplemental material), minor changes,
  Journal-ref: IEEE Symposium on Quantum Computing and Computational
  Intelligence 2013 (IEEE QCCI 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a new phenomenon for communication over noisy quantum channels. The
investigated solution is called polaractivation and based on quantum polar
encoding. Polaractivation is a natural consequence of the channel polarization
effect in quantum systems and makes possible to open the hidden capacity
regions of a noisy quantum channel by using the idea of rate increment. While
in case of a classical channel only the rate of classical communication can be
increased, in case of a quantum channel the channel polarization and the rate
improvement can be exploited to open unreachable capacity regions. We
demonstrate the results for the opening of private classical capacity-domain.
We prove that the method works for arbitrary quantum channels if a given
criteria in the symmetric classical capacity is satisfied. We also derived a
necessary lower bound on the rate of classical communication for the
polaractivation of private classical capacity-domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3116</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3116</id><created>2013-02-13</created><updated>2014-02-12</updated><authors><author><keyname>Fearnley</keyname><forenames>John</forenames></author><author><keyname>Gairing</keyname><forenames>Martin</forenames></author><author><keyname>Goldberg</keyname><forenames>Paul</forenames></author><author><keyname>Savani</keyname><forenames>Rahul</forenames></author></authors><title>Learning Equilibria of Games via Payoff Queries</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent body of experimental literature has studied empirical
game-theoretical analysis, in which we have partial knowledge of a game,
consisting of observations of a subset of the pure-strategy profiles and their
associated payoffs to players. The aim is to find an exact or approximate Nash
equilibrium of the game, based on these observations. It is usually assumed
that the strategy profiles may be chosen in an on-line manner by the algorithm.
We study a corresponding computational learning model, and the query complexity
of learning equilibria for various classes of games. We give basic results for
bimatrix and graphical games. Our focus is on symmetric network congestion
games. For directed acyclic networks, we can learn the cost functions (and
hence compute an equilibrium) while querying just a small fraction of
pure-strategy profiles. For the special case of parallel links, we have the
stronger result that an equilibrium can be identified while only learning a
small fraction of the cost values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3118</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3118</id><created>2013-02-13</created><updated>2013-10-04</updated><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>The Correlation Conversion Property of Quantum Channels</title><categories>quant-ph cs.IT math.IT</categories><comments>v3: 37 pages, Journal-ref: Quant. Inf. Proc. (2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmission of quantum entanglement will play a crucial role in future
networks and long-distance quantum communications. Quantum Key Distribution,
the working mechanism of quantum repeaters and the various quantum
communication protocols are all based on quantum entanglement. On the other
hand, quantum entanglement is extremely fragile and sensitive to the noise of
the communication channel over which it has been transmitted. To share
entanglement between distant points, high fidelity quantum channels are needed.
In practice, these communication links are noisy, which makes it impossible or
extremely difficult and expensive to distribute entanglement. In this work we
first show that quantum entanglement can be generated by a new idea, exploiting
the most natural effect of the communication channels: the noise itself of the
link. We prove that the noise transformation of quantum channels that are not
able to transmit quantum entanglement can be used to generate distillable
(useable) entanglement from classically correlated input. We call this new
phenomenon the Correlation Conversion property (CC-property) of quantum
channels. The proposed solution does not require any non-local operation or
local measurement by the parties, only the use of standard quantum channels.
Our results have implications and consequences for the future of quantum
communications, and for global-scale quantum communication networks. The
discovery also revealed that entanglement generation by local operations is
possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3119</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3119</id><created>2013-01-10</created><authors><author><keyname>Murali</keyname><forenames>S.</forenames></author><author><keyname>Chittapur</keyname><forenames>Govindraj B.</forenames></author><author><keyname>S</keyname><forenames>Prabhakara H.</forenames></author><author><keyname>Anami</keyname><forenames>Basavaraj S.</forenames></author></authors><title>Comparision and analysis of photo image forgery detection techniques</title><categories>cs.CV cs.CR cs.MM</categories><comments>12 pages, International Journal on Computational Sciences &amp;
  Applications (IJCSA) Vo2, No.6, December 2012</comments><doi>10.5121/ijcsa.2012.2605</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital Photo images are everywhere, on the covers of magazines, in
newspapers, in courtrooms, and all over the Internet. We are exposed to them
throughout the day and most of the time. Ease with which images can be
manipulated; we need to be aware that seeing does not always imply believing.
We propose methodologies to identify such unbelievable photo images and
succeeded to identify forged region by given only the forged image. Formats are
additive tag for every file system and contents are relatively expressed with
extension based on most popular digital camera uses JPEG and Other image
formats like png, bmp etc. We have designed algorithm running behind with the
concept of abnormal anomalies and identify the forgery regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3120</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3120</id><created>2013-01-09</created><authors><author><keyname>Fang</keyname><forenames>Jian</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author><author><keyname>Zhang</keyname><forenames>Bingchen</forenames></author><author><keyname>Hong</keyname><forenames>Wen</forenames></author><author><keyname>Wu</keyname><forenames>Yirong</forenames></author></authors><title>Fast Compressed Sensing SAR Imaging based on Approximated Observation</title><categories>cs.IT math.IT</categories><comments>Submitted To IEEE-JSTAR</comments><journal-ref>Fang J, Xu Z, Zhang B, et al, IEEE Journal of Selected Topics in
  Applied Earth Observations and Remote Sensing, vol.7 no. 1, pp: 352-363, 2014</journal-ref><doi>10.1109/JSTARS.2013.2263309</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, compressed sensing (CS) has been applied in the field of
synthetic aperture radar (SAR) imaging and shows great potential. The existing
models are, however, based on application of the sensing matrix acquired by the
exact observation functions. As a result, the corresponding reconstruction
algorithms are much more time consuming than traditional matched filter (MF)
based focusing methods, especially in high resolution and wide swath systems.
In this paper, we formulate a new CS-SAR imaging model based on the use of the
approximated SAR observation deducted from the inverse of focusing procedures.
We incorporate CS and MF within an sparse regularization framework that is then
solved by a fast iterative thresholding algorithm. The proposed model forms a
new CS-SAR imaging method that can be applied to high-quality and
high-resolution imaging under sub-Nyquist rate sampling, while saving the
computational cost substantially both in time and memory. Simulations and real
SAR data applications support that the proposed method can perform SAR imaging
effectively and efficiently under Nyquist rate, especially for large scale
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3123</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3123</id><created>2013-01-08</created><authors><author><keyname>Banu</keyname><forenames>P. K. Nizar</forenames></author><author><keyname>Inbarani</keyname><forenames>H. Hannah</forenames></author></authors><title>An Analysis of Gene Expression Data using Penalized Fuzzy C-Means
  Approach</title><categories>cs.CV cs.CE</categories><comments>14; IJCCI, Vol. 1, Issue 2,(January-July)2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid advances of microarray technologies, large amounts of
high-dimensional gene expression data are being generated, which poses
significant computational challenges. A first step towards addressing this
challenge is the use of clustering techniques, which is essential in the data
mining process to reveal natural structures and identify interesting patterns
in the underlying data. A robust gene expression clustering approach to
minimize undesirable clustering is proposed. In this paper, Penalized Fuzzy
C-Means (PFCM) Clustering algorithm is described and compared with the most
representative off-line clustering techniques: K-Means Clustering, Rough
K-Means Clustering and Fuzzy C-Means clustering. These techniques are
implemented and tested for a Brain Tumor gene expression Dataset. Analysis of
the performance of the proposed approach is presented through qualitative
validation experiments. From experimental results, it can be observed that
Penalized Fuzzy C-Means algorithm shows a much higher usability than the other
projected clustering algorithms used in our comparison study. Significant and
promising clustering results are presented using Brain Tumor Gene expression
dataset. Thus patterns seen in genome-wide expression experiments can be
interpreted as indications of the status of cellular processes. In these
clustering results, we find that Penalized Fuzzy C-Means algorithm provides
useful information as an aid to diagnosis in oncology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3126</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3126</id><created>2013-02-13</created><authors><author><keyname>Chessa</keyname><forenames>Alessandro</forenames></author><author><keyname>Morescalchi</keyname><forenames>Andrea</forenames></author><author><keyname>Pammolli</keyname><forenames>Fabio</forenames></author><author><keyname>Penner</keyname><forenames>Orion</forenames></author><author><keyname>Petersen</keyname><forenames>Alexander M.</forenames></author><author><keyname>Riccaboni</keyname><forenames>Massimo</forenames></author></authors><title>Is Europe Evolving Toward an Integrated Research Area?</title><categories>physics.soc-ph cs.DL cs.SI physics.data-an</categories><comments>24 pages, 4 figures, 2 tables</comments><journal-ref>Science 339, 650-651 (2013)</journal-ref><doi>10.1126/science.1227970</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An integrated European Research Area (ERA) is a critical component for a more
competitive and open European R&amp;D system. However, the impact of EU-specific
integration policies aimed at overcoming innovation barriers associated with
national borders is not well understood. Here we analyze 2.4 x 10^6 patent
applications filed with the European Patent Office (EPO) over the 25-year
period 1986-2010 along with a sample of 2.6 x 10^5 records from the ISI Web of
Science to quantitatively measure the role of borders in international R&amp;D
collaboration and mobility. From these data we construct five different
networks for each year analyzed: (i) the patent co-inventor network, (ii) the
publication co-author network, (iii) the co-applicant patent network, (iv) the
patent citation network, and (v) the patent mobility network. We use methods
from network science and econometrics to perform a comparative analysis across
time and between EU and non-EU countries to determine the &quot;treatment effect&quot;
resulting from EU integration policies. Using non-EU countries as a control
set, we provide quantitative evidence that, despite decades of efforts to build
a European Research Area, there has been little integration above global trends
in patenting and publication. This analysis provides concrete evidence that
Europe remains a collection of national innovation systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3145</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3145</id><created>2013-02-13</created><updated>2015-01-06</updated><authors><author><keyname>Friggstad</keyname><forenames>Zachary</forenames></author><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Singh</keyname><forenames>Mohit</forenames></author></authors><title>An Improved Integrality Gap for Asymmetric TSP Paths</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Asymmetric Traveling Salesperson Path Problem (ATSPP) is one where, given
an asymmetric metric space $(V,d)$ with specified vertices s and t, the goal is
to find an s-t path of minimum length that passes through all the vertices in
V.
  This problem is closely related to the Asymmetric TSP (ATSP), which seeks to
find a tour (instead of an $s-t$ path) visiting all the nodes: for ATSP, a
$\rho$-approximation guarantee implies an $O(\rho)$-approximation for ATSPP.
However, no such connection is known for the integrality gaps of the linear
programming relaxations for these problems: the current-best approximation
algorithm for ATSPP is $O(\log n/\log\log n)$, whereas the best bound on the
integrality gap of the natural LP relaxation (the subtour elimination LP) for
ATSPP is $O(\log n)$.
  In this paper, we close this gap, and improve the current best bound on the
integrality gap from $O(\log n)$ to $O(\log n/\log\log n)$. The resulting
algorithm uses the structure of narrow $s$-$t$ cuts in the LP solution to
construct a (random) tree spanning tree that can be cheaply augmented to
contain an Eulerian $s$-$t$ walk.
  We also build on a result of Oveis Gharan and Saberi and show a strong form
of Goddyn's conjecture about thin spanning trees implies the integrality gap of
the subtour elimination LP relaxation for ATSPP is bounded by a constant.
Finally, we give a simpler family of instances showing the integrality gap of
this LP is at least 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3155</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3155</id><created>2013-02-13</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Anirban</forenames></author><author><keyname>Qian</keyname><forenames>Zhen</forenames></author><author><keyname>Bhandarkar</keyname><forenames>Suchendra M.</forenames></author><author><keyname>Liu</keyname><forenames>Tianming</forenames></author><author><keyname>Rinehart</keyname><forenames>Sarah</forenames></author><author><keyname>Voros</keyname><forenames>Szilard</forenames></author></authors><title>Morphological Analusis Of The Left Ventricular Eendocardial Surface
  Using A Bag-Of-Features Descriptor</title><categories>cs.CV</categories><comments>Submitted to Medical Image Analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The limitations of conventional imaging techniques have hitherto precluded a
thorough and formal investigation of the complex morphology of the left
ventricular (LV) endocardial surface and its relation to the severity of
Coronary Artery Disease (CAD). Recent developments in high-resolution
Multirow-Detector Computed Tomography (MDCT) scanner technology have enabled
the imaging of LV endocardial surface morphology in a single heart beat.
Analysis of high-resolution Computed Tomography (CT) images from a 320-MDCT
scanner allows the study of the relationship between percent Diameter Stenosis
(DS) of the major coronary arteries and localization of the cardiac segments
affected by coronary arterial stenosis. In this paper a novel approach for the
analysis using a combination of rigid transformation-invariant shape
descriptors and a more generalized isometry-invariant Bag-of-Features (BoF)
descriptor, is proposed and implemented. The proposed approach is shown to be
successful in identifying, localizing and quantifying the incidence and extent
of CAD and thus, is seen to have a potentially significant clinical impact.
Specifically, the association between the incidence and extent of CAD,
determined via the percent DS measurements of the major coronary arteries, and
the alterations in the endocardial surface morphology is formally quantified. A
multivariate regression test performed on a strict leave-one-out basis are
shown to exhibit a distinct pattern in terms of the correlation coefficient
within the cardiac segments where the incidence of coronary arterial stenosis
is localized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3160</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3160</id><created>2013-02-13</created><updated>2013-02-18</updated><authors><author><keyname>Wang</keyname><forenames>Xiuli</forenames></author></authors><title>A New Construction of Multi-receiver Authentication Codes from
  Pseudo-Symplectic Geometry over Finite Fields</title><categories>cs.IT math.IT</categories><comments>9 pages</comments><msc-class>15A03, 94A60, 94A62</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-receiver authentication codes allow one sender to construct an
authenticated message for a group of receivers such that each receiver can
verify authenticity of the received message. In this paper, we constructed one
multi-receiver authentication codes from pseudo-symplectic geometry over finite
fields. The parameters and the probabilities of deceptions of this codes are
also computed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3166</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3166</id><created>2013-02-13</created><authors><author><keyname>de Kerret</keyname><forenames>Paul</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author></authors><title>CSI Sharing Strategies for Transmitter Cooperation in Wireless Networks</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Wireless Communications Magazine,
  Feb. 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple-antenna &quot;based&quot; transmitter (TX) cooperation has been established as
a promising tool towards avoiding, aligning, or shaping the interference
resulting from aggressive spectral reuse. The price paid in the form of
feedback and exchanging channel state information (CSI) between cooperating
devices in most existing methods is often underestimated however. In reality,
feedback and information overhead threatens the practicality and scalability of
TX cooperation approaches in dense networks. Hereby we addresses a &quot;Who needs
to know what?&quot; problem, when it comes to CSI at cooperating transmitters. A
comprehensive answer to this question remains beyond our reach and the scope of
this paper. Nevertheless, recent results in this area suggest that CSI overhead
can be contained for even large networks provided the allocation of feedback to
TXs is made non-uniform and to properly depend on the network's topology. This
paper provides a few hints toward solving the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3167</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3167</id><created>2013-02-13</created><authors><author><keyname>Min</keyname><forenames>Chol-Rim</forenames></author><author><keyname>Ri</keyname><forenames>Won-Hak</forenames></author><author><keyname>O</keyname><forenames>Hyong-Chol</forenames></author></authors><title>Equiaffine Structure and Conjugate Ricci-symmetry of a Statistical
  Manifold</title><categories>math.DS cs.IT math-ph math.DG math.IT math.MP</categories><comments>7 pages</comments><report-no>KISU-MATH-2013-E-R-002</report-no><msc-class>53A15(Primary), 53B05, 53C44(secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A condition for a statistical manifold to have an equiaffine structure is
studied. The facts that dual flatness and conjugate symmetry of a statistical
manifold are sufficient conditions for a statistical manifold to have an
equiaffine structure were obtained in [2] and [3]. In this paper, a fact that a
statistical manifold, which is conjugate Ricci-symmetric, has an equiaffine
structure is given, where conjugate Ricci-symmetry is weaker condition than
conjugate symmetry. A condition for conjugate symmetry and conjugate
Ricci-symmetry to coincide is also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3178</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3178</id><created>2013-02-13</created><authors><author><keyname>Lester</keyname><forenames>Martin</forenames></author><author><keyname>Ong</keyname><forenames>Luke</forenames></author><author><keyname>Schaefer</keyname><forenames>Max</forenames></author></authors><title>Information Flow Analysis for a Dynamically Typed Functional Language
  with Staged Metaprogramming</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web applications written in JavaScript are regularly used for dealing with
sensitive or personal data. Consequently, reasoning about their security
properties has become an important problem, which is made very difficult by the
highly dynamic nature of the language, particularly its support for runtime
code generation. As a first step towards dealing with this, we propose to
investigate security analyses for languages with more principled forms of
dynamic code generation. To this end, we present a static information flow
analysis for a dynamically typed functional language with prototype-based
inheritance and staged metaprogramming. We prove its soundness, implement it
and test it on various examples designed to show its relevance to proving
security properties, such as noninterference, in JavaScript. To our knowledge,
this is the first fully static information flow analysis for a language with
staged metaprogramming, and the first formal soundness proof of a CFA-based
information flow analysis for a functional programming language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3200</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3200</id><created>2013-02-13</created><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Lidick&#xfd;</keyname><forenames>Bernard</forenames></author></authors><title>Peeling the Grid</title><categories>cs.DM cs.CG math.CO</categories><comments>8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the set of points formed by the integer $n \times n$ grid, and the
process that in each iteration removes from the point set the vertices of its
convex-hull. Here, we prove that the number of iterations of this process is
O(n^{4/3}); that is, the number of convex layers of the $n\times n$ grid is
\Theta(n^{4/3}).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3203</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3203</id><created>2013-02-13</created><updated>2014-08-27</updated><authors><author><keyname>Duchi</keyname><forenames>John C.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Local Privacy, Data Processing Inequalities, and Statistical Minimax
  Rates</title><categories>math.ST cs.CR cs.IT math.IT stat.TH</categories><comments>59 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Working under a model of privacy in which data remains private even from the
statistician, we study the tradeoff between privacy guarantees and the utility
of the resulting statistical estimators. We prove bounds on
information-theoretic quantities, including mutual information and
Kullback-Leibler divergence, that depend on the privacy guarantees. When
combined with standard minimax techniques, including the Le Cam, Fano, and
Assouad methods, these inequalities allow for a precise characterization of
statistical rates under local privacy constraints. We provide a treatment of
several canonical families of problems: mean estimation, parameter estimation
in fixed-design regression, multinomial probability estimation, and
nonparametric density estimation. For all of these families, we provide lower
and upper bounds that match up to constant factors, and exhibit new (optimal)
privacy-preserving mechanisms and computationally efficient estimators that
achieve the bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3209</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3209</id><created>2013-02-13</created><authors><author><keyname>Davies</keyname><forenames>Todd</forenames></author><author><keyname>O'Connor</keyname><forenames>Brendan</forenames></author><author><keyname>Cochran</keyname><forenames>Alex</forenames></author><author><keyname>Parker</keyname><forenames>Andrew</forenames></author></authors><title>&quot;Groupware for Groups&quot;: Problem-Driven Design in Deme</title><categories>cs.HC cs.SI</categories><comments>Position paper from the Beyond Threaded Conversation Workshop at CHI
  2005, Portland, Oregon, April 3, 2005; 3 pages, 2 figures</comments><acm-class>H.4.1; H.5.3; I.5.2; H.5.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Design choices can be clarified when group interaction software is directed
at solving the interaction needs of particular groups that pre-date the
groupware. We describe an example: the Deme platform for online deliberation.
Traditional threaded conversation systems are insufficient for solving the
problem at which Deme is aimed, namely, that the democratic process in
grassroots community groups is undermined both by the limited availability of
group members for face-to-face meetings and by constraints on the use of
information in real-time interactions. We describe and motivate design
elements, either implemented or planned for Deme, that addresses this problem.
We believe that &quot;problem focused&quot; design of software for preexisting groups
provides a useful framework for evaluating the appropriateness of design
elements in groupware generally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3219</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3219</id><created>2013-02-13</created><authors><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Kim</keyname><forenames>Junae</forenames></author><author><keyname>Liu</keyname><forenames>Fayao</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>An Efficient Dual Approach to Distance Metric Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distance metric learning is of fundamental interest in machine learning
because the distance metric employed can significantly affect the performance
of many learning methods. Quadratic Mahalanobis metric learning is a popular
approach to the problem, but typically requires solving a semidefinite
programming (SDP) problem, which is computationally expensive. Standard
interior-point SDP solvers typically have a complexity of $O(D^{6.5})$ (with
$D$ the dimension of input data), and can thus only practically solve problems
exhibiting less than a few thousand variables. Since the number of variables is
$D (D+1) / 2 $, this implies a limit upon the size of problem that can
practically be solved of around a few hundred dimensions. The complexity of the
popular quadratic Mahalanobis metric learning approach thus limits the size of
problem to which metric learning can be applied. Here we propose a
significantly more efficient approach to the metric learning problem based on
the Lagrange dual formulation of the problem. The proposed formulation is much
simpler to implement, and therefore allows much larger Mahalanobis metric
learning problems to be solved. The time complexity of the proposed method is
$O (D ^ 3) $, which is significantly lower than that of the SDP approach.
Experiments on a variety of datasets demonstrate that the proposed method
achieves an accuracy comparable to the state-of-the-art, but is applicable to
significantly larger problems. We also show that the proposed method can be
applied to solve more general Frobenius-norm regularized SDP problems
approximately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3250</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3250</id><created>2013-02-13</created><updated>2013-07-31</updated><authors><author><keyname>Kwak</keyname><forenames>Jaewook</forenames></author><author><keyname>Lee</keyname><forenames>Chul-Ho</forenames></author><author><keyname>Eun</keyname><forenames>Do Young</forenames></author></authors><title>Exploiting the Past to Reduce Delay in CSMA Scheduling: A High-order
  Markov Chain Approach</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently several CSMA algorithms based on the Glauber dynamics model have
been proposed for multihop wireless scheduling, as viable solutions to achieve
the throughput optimality, yet are simple to implement. However, their delay
performances still remain unsatisfactory, mainly due to the nature of the
underlying Markov chains that imposes a fundamental constraint on how the link
state can evolve over time. In this paper, we propose a new approach toward
better queueing and delay performance, based on our observation that the
algorithm needs not be Markovian, as long as it can be implemented in a
distributed manner, achieve the same throughput optimality, while offering far
better delay performance for general network topologies. Our approach hinges
upon utilizing past state information observed by local link and then
constructing a high-order Markov chain for the evolution of the feasible link
schedules. We show in theory and simulation that our proposed algorithm, named
delayed CSMA, adds virtually no additional overhead onto the existing
CSMA-based algorithms, achieves the throughput optimality under the usual
choice of link weight as a function of local queue length, and also provides
much better delay performance by effectively `de-correlating' the link state
process (thus removing link starvation) under any arbitrary network topology.
From our extensive simulations we observe that the delay under our algorithm
can be often reduced by a factor of 20 over a wide range of scenarios, compared
to the standard Glauber-dynamics-based CSMA algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3261</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3261</id><created>2013-02-13</created><authors><author><keyname>Bichler</keyname><forenames>O.</forenames></author><author><keyname>Zhao</keyname><forenames>W.</forenames></author><author><keyname>Alibart</keyname><forenames>F.</forenames></author><author><keyname>Pleutin</keyname><forenames>S.</forenames></author><author><keyname>Lenfant</keyname><forenames>S.</forenames></author><author><keyname>Vuillaume</keyname><forenames>D.</forenames></author><author><keyname>Gamrat</keyname><forenames>C.</forenames></author></authors><title>Pavlov's dog associative learning demonstrated on synaptic-like organic
  transistors</title><categories>q-bio.NC cond-mat.dis-nn cs.ET cs.NE</categories><journal-ref>Neural Computation 25(2), 549-566 (2013)</journal-ref><doi>10.1162/NECO_a_00377</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we present an original demonstration of an associative
learning neural network inspired by the famous Pavlov's dogs experiment. A
single nanoparticle organic memory field effect transistor (NOMFET) is used to
implement each synapse. We show how the physical properties of this dynamic
memristive device can be used to perform low power write operations for the
learning and implement short-term association using temporal coding and spike
timing dependent plasticity based learning. An electronic circuit was built to
validate the proposed learning scheme with packaged devices, with good
reproducibility despite the complex synaptic-like dynamic of the NOMFET in
pulse regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3268</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3268</id><created>2013-02-13</created><updated>2013-05-20</updated><authors><author><keyname>Abraham</keyname><forenames>Ittai</forenames></author><author><keyname>Alonso</keyname><forenames>Omar</forenames></author><author><keyname>Kandylas</keyname><forenames>Vasilis</forenames></author><author><keyname>Slivkins</keyname><forenames>Aleksandrs</forenames></author></authors><title>Adaptive Crowdsourcing Algorithms for the Bandit Survey Problem</title><categories>cs.LG</categories><comments>Full version of a paper in COLT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Very recently crowdsourcing has become the de facto platform for distributing
and collecting human computation for a wide range of tasks and applications
such as information retrieval, natural language processing and machine
learning. Current crowdsourcing platforms have some limitations in the area of
quality control. Most of the effort to ensure good quality has to be done by
the experimenter who has to manage the number of workers needed to reach good
results.
  We propose a simple model for adaptive quality control in crowdsourced
multiple-choice tasks which we call the \emph{bandit survey problem}. This
model is related to, but technically different from the well-known multi-armed
bandit problem. We present several algorithms for this problem, and support
them with analysis and simulations. Our approach is based in our experience
conducting relevance evaluation for a large commercial search engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3283</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3283</id><created>2013-02-13</created><updated>2014-02-08</updated><authors><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Lin</keyname><forenames>Guosheng</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>StructBoost: Boosting Methods for Predicting Structured Output Variables</title><categories>cs.LG</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boosting is a method for learning a single accurate predictor by linearly
combining a set of less accurate weak learners. Recently, structured learning
has found many applications in computer vision. Inspired by structured support
vector machines (SSVM), here we propose a new boosting algorithm for structured
output prediction, which we refer to as StructBoost. StructBoost supports
nonlinear structured learning by combining a set of weak structured learners.
As SSVM generalizes SVM, our StructBoost generalizes standard boosting
approaches such as AdaBoost, or LPBoost to structured learning. The resulting
optimization problem of StructBoost is more challenging than SSVM in the sense
that it may involve exponentially many variables and constraints. In contrast,
for SSVM one usually has an exponential number of constraints and a
cutting-plane method is used. In order to efficiently solve StructBoost, we
formulate an equivalent $ 1 $-slack formulation and solve it using a
combination of cutting planes and column generation. We show the versatility
and usefulness of StructBoost on a range of problems such as optimizing the
tree loss for hierarchical multi-class classification, optimizing the Pascal
overlap criterion for robust visual tracking and learning conditional random
field parameters for image segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3290</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3290</id><created>2013-02-13</created><authors><author><keyname>Gotlieb</keyname><forenames>Arnaud</forenames><affiliation>Certus Software V &amp; V Center, SIMULA Research Laboratory, Norway</affiliation></author><author><keyname>Denmat</keyname><forenames>Tristan</forenames><affiliation>INRIA Rennes Bretagne-Atlantique, France</affiliation></author><author><keyname>Lazaar</keyname><forenames>Nadjib</forenames><affiliation>LIRMM, Montpellier, France</affiliation></author></authors><title>Constraint-based reachability</title><categories>cs.LO</categories><comments>In Proceedings Infinity 2012, arXiv:1302.3105</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 107, 2013, pp. 25-43</journal-ref><doi>10.4204/EPTCS.107.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iterative imperative programs can be considered as infinite-state systems
computing over possibly unbounded domains. Studying reachability in these
systems is challenging as it requires to deal with an infinite number of states
with standard backward or forward exploration strategies. An approach that we
call Constraint-based reachability, is proposed to address reachability
problems by exploring program states using a constraint model of the whole
program. The keypoint of the approach is to interpret imperative constructions
such as conditionals, loops, array and memory manipulations with the
fundamental notion of constraint over a computational domain. By combining
constraint filtering and abstraction techniques, Constraint-based reachability
is able to solve reachability problems which are usually outside the scope of
backward or forward exploration strategies. This paper proposes an
interpretation of classical filtering consistencies used in Constraint
Programming as abstract domain computations, and shows how this approach can be
used to produce a constraint solver that efficiently generates solutions for
reachability problems that are unsolvable by other approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3291</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3291</id><created>2013-02-13</created><authors><author><keyname>Abdulla</keyname><forenames>Parosh Aziz</forenames><affiliation>Uppsala University</affiliation></author><author><keyname>Mayr</keyname><forenames>Richard</forenames><affiliation>University of Edinburgh</affiliation></author></authors><title>Petri Nets with Time and Cost</title><categories>cs.LO</categories><comments>In Proceedings Infinity 2012, arXiv:1302.3105</comments><proxy>EPTCS</proxy><acm-class>D.2.4</acm-class><journal-ref>EPTCS 107, 2013, pp. 9-24</journal-ref><doi>10.4204/EPTCS.107.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider timed Petri nets, i.e., unbounded Petri nets where each token
carries a real-valued clock. Transition arcs are labeled with time intervals,
which specify constraints on the ages of tokens. Our cost model assigns token
storage costs per time unit to places, and firing costs to transitions. We
study the cost to reach a given control-state. In general, a cost-optimal run
may not exist. However,we show that the infimum of the costs is computable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3292</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3292</id><created>2013-02-13</created><authors><author><keyname>Randolph</keyname><forenames>Aurel</forenames><affiliation>&#xc9;cole Polytechnique de Montr&#xe9;al</affiliation></author><author><keyname>Boucheneb</keyname><forenames>Hanifa</forenames><affiliation>&#xc9;cole Polytechnique de Montr&#xe9;al</affiliation></author><author><keyname>Imine</keyname><forenames>Abdessamad</forenames><affiliation>INRIA Grand-Est and Nancy-Universit&#xe9;</affiliation></author><author><keyname>Quintero</keyname><forenames>Alejandro</forenames><affiliation>&#xc9;cole Polytechnique de Montr&#xe9;al</affiliation></author></authors><title>On Consistency of Operational Transformation Approach</title><categories>cs.DC cs.SY</categories><comments>In Proceedings Infinity 2012, arXiv:1302.3105</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 107, 2013, pp. 45-59</journal-ref><doi>10.4204/EPTCS.107.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Operational Transformation (OT) approach, used in many collaborative
editors, allows a group of users to concurrently update replicas of a shared
object and exchange their updates in any order. The basic idea of this approach
is to transform any received update operation before its execution on a replica
of the object. This transformation aims to ensure the convergence of the
different replicas of the object, even though the operations are executed in
different orders. However, designing transformation functions for achieving
convergence is a critical and challenging issue. Indeed, the transformation
functions proposed in the literature are all revealed incorrect.
  In this paper, we investigate the existence of transformation functions for a
shared string altered by insert and delete operations. From the theoretical
point of view, two properties - named TP1 and TP2 - are necessary and
sufficient to ensure convergence. Using controller synthesis technique, we show
that there are some transformation functions which satisfy only TP1 for the
basic signatures of insert and delete operations. As a matter of fact, it is
impossible to meet both properties TP1 and TP2 with these simple signatures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3293</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3293</id><created>2013-02-13</created><authors><author><keyname>Fronc</keyname><forenames>&#x141;ukasz</forenames><affiliation>IBISC, Universit&#xe9; d'Evry-Val d'Essonne</affiliation></author></authors><title>Effective Marking Equivalence Checking in Systems with Dynamic Process
  Creation</title><categories>cs.LO</categories><comments>In Proceedings Infinity 2012, arXiv:1302.3105</comments><proxy>EPTCS</proxy><acm-class>D.2.4</acm-class><journal-ref>EPTCS 107, 2013, pp. 61-75</journal-ref><doi>10.4204/EPTCS.107.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The starting point of this work is a framework allowing to model systems with
dynamic process creation, equipped with a procedure to detect symmetric
executions (ie., which differ only by the identities of processes). This allows
to reduce the state space, potentially to an exponentially smaller size, and,
because process identifiers are never reused, this also allows to reduce to
finite size some infinite state spaces. However, in this approach, the
procedure to detect symmetries does not allow for computationally efficient
algorithms, mainly because each newly computed state has to be compared with
every already reached state.
  In this paper, we propose a new approach to detect symmetries in this
framework that will solve this problem, thus enabling for efficient algorithms.
We formalise a canonical representation of states and identify a sufficient
condition on the analysed model that guarantees that every symmetry can be
detected. For the models that do not fall into this category, our approach is
still correct but does not guarantee a maximal reduction of state space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3299</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3299</id><created>2013-02-13</created><updated>2013-05-18</updated><authors><author><keyname>Mitchell</keyname><forenames>Lewis</forenames></author><author><keyname>Harris</keyname><forenames>Kameron Decker</forenames></author><author><keyname>Frank</keyname><forenames>Morgan R.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author></authors><title>The Geography of Happiness: Connecting Twitter sentiment and expression,
  demographics, and objective characteristics of place</title><categories>physics.soc-ph cs.SI</categories><journal-ref>PLoS ONE 8(5): e64417, 2013</journal-ref><doi>10.1371/journal.pone.0064417</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We conduct a detailed investigation of correlations between real-time
expressions of individuals made across the United States and a wide range of
emotional, geographic, demographic, and health characteristics. We do so by
combining (1) a massive, geo-tagged data set comprising over 80 million words
generated over the course of several recent years on the social network service
Twitter and (2) annually-surveyed characteristics of all 50 states and close to
400 urban populations. Among many results, we generate taxonomies of states and
cities based on their similarities in word use; estimate the happiness levels
of states and cities; correlate highly-resolved demographic characteristics
with happiness levels; and connect word choice and message length with urban
characteristics such as education levels and obesity rates. Our results show
how social media may potentially be used to estimate real-time levels and
changes in population-level measures such as obesity rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3308</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3308</id><created>2013-02-13</created><authors><author><keyname>Kumar</keyname><forenames>Mrinal</forenames></author><author><keyname>Maheshwari</keyname><forenames>Gaurav</forenames></author><author><keyname>N</keyname><forenames>Jayalal Sarma M.</forenames></author></authors><title>Arithmetic Circuit Lower Bounds via MaxRank</title><categories>cs.CC</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the polynomial coefficient matrix and identify maximum rank of
this matrix under variable substitution as a complexity measure for
multivariate polynomials. We use our techniques to prove super-polynomial lower
bounds against several classes of non-multilinear arithmetic circuits. In
particular, we obtain the following results :
  As our main result, we prove that any homogeneous depth-3 circuit for
computing the product of $d$ matrices of dimension $n \times n$ requires
$\Omega(n^{d-1}/2^d)$ size. This improves the lower bounds by Nisan and
Wigderson(1995) when $d=\omega(1)$.
  There is an explicit polynomial on $n$ variables and degree at most
$\frac{n}{2}$ for which any depth-3 circuit $C$ of product dimension at most
$\frac{n}{10}$ (dimension of the space of affine forms feeding into each
product gate) requires size $2^{\Omega(n)}$. This generalizes the lower bounds
against diagonal circuits proved by Saxena(2007). Diagonal circuits are of
product dimension 1.
  We prove a $n^{\Omega(\log n)}$ lower bound on the size of product-sparse
formulas. By definition, any multilinear formula is a product-sparse formula.
Thus, our result extends the known super-polynomial lower bounds on the size of
multilinear formulas by Raz(2006).
  We prove a $2^{\Omega(n)}$ lower bound on the size of partitioned arithmetic
branching programs. This result extends the known exponential lower bound on
the size of ordered arithmetic branching programs given by Jansen(2008).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3309</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3309</id><created>2013-02-13</created><updated>2013-02-23</updated><authors><author><keyname>Askalidis</keyname><forenames>Georgios</forenames></author><author><keyname>Immorlica</keyname><forenames>Nicole</forenames></author><author><keyname>Pountourakis</keyname><forenames>Emmanouil</forenames></author></authors><title>Socially Stable Matchings</title><categories>cs.GT</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In two-sided matching markets, the agents are partitioned into two sets. Each
agent wishes to be matched to an agent in the other set and has a strict
preference over these potential matches. A matching is stable if there are no
blocking pairs, i.e., no pair of agents that prefer each other to their
assigned matches. In this paper we study a variant of stable matching motivated
by the fact that, in most centralized markets, many agents do not have direct
communication with each other. Hence even if some blocking pairs exist, the
agents involved in those pairs may not be able to coordinate a deviation. We
model communication channels with a bipartite graph between the two sets of
agents which we call the social graph, and we study socially stable matchings.
A matching is socially stable if there are no blocking pairs that are connected
by an edge in the social graph. Socially stable matchings vary in size and so
we look for a maximum socially stable matching. We prove that this problem is
NP-hard and, assuming the unique games conjecture, hard to approximate within a
factor of 3/2-{\epsilon}, for any constant {\epsilon}&gt;0. We complement the
hardness results with a 3/2-approximation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3344</identifier>
 <datestamp>2013-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3344</id><created>2013-02-14</created><updated>2013-06-05</updated><authors><author><keyname>Li</keyname><forenames>Runhui</forenames></author><author><keyname>Lin</keyname><forenames>Jian</forenames></author><author><keyname>Lee</keyname><forenames>Patrick P. C.</forenames></author></authors><title>CORE: Augmenting Regenerating-Coding-Based Recovery for Single and
  Concurrent Failures in Distributed Storage Systems</title><categories>cs.DC</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data availability is critical in distributed storage systems, especially when
node failures are prevalent in real life. A key requirement is to minimize the
amount of data transferred among nodes when recovering the lost or unavailable
data of failed nodes. This paper explores recovery solutions based on
regenerating codes, which are shown to provide fault-tolerant storage and
minimum recovery bandwidth. Existing optimal regenerating codes are designed
for single node failures. We build a system called CORE, which augments
existing optimal regenerating codes to support a general number of failures
including single and concurrent failures. We theoretically show that CORE
achieves the minimum possible recovery bandwidth for most cases. We implement
CORE and evaluate our prototype atop a Hadoop HDFS cluster testbed with up to
20 storage nodes. We demonstrate that our CORE prototype conforms to our
theoretical findings and achieves recovery bandwidth saving when compared to
the conventional recovery approach based on erasure codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3347</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3347</id><created>2013-02-14</created><authors><author><keyname>Fischer</keyname><forenames>Johannes</forenames></author><author><keyname>Gawrychowski</keyname><forenames>Pawel</forenames></author></authors><title>Alphabet-Dependent String Searching with Wexponential Search Trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is widely assumed that $O(m+\lg \sigma)$ is the best one can do for
finding a pattern of length $m$ in a compacted trie storing strings over an
alphabet of size $\sigma$, if one insists on linear-size data structures and
deterministic worst-case running times [Cole et al., ICALP'06]. In this
article, we first show that a rather straightforward combination of well-known
ideas yields $O(m+\lg\lg \sigma)$ deterministic worst-case searching time for
static tries.
  Then we move on to dynamic tries, where we achieve a worst-case bound of
$O(m+\frac{\lg^{2}\lg\sigma}{\lg\lg\lg\sigma})$ per query or update, which
should again be compared to the previously known $O(m+\lg\sigma)$ deterministic
worst-case bounds [Cole et al., ICALP'06], and to the alphabet
\emph{in}dependent $O(m+\sqrt{\lg n/\lg\lg n})$ deterministic worst-case bounds
[Andersson and Thorup, SODA'01], where $n$ is the number of nodes in the trie.
The basis of our update procedure is a weighted variant of exponential search
trees which, while simple, might be of independent interest.
  As one particular application, the above bounds (static and dynamic) apply to
suffix trees. There, an update corresponds to pre- or appending a letter to the
text, and an additional goal is to do the updates quicker than rematching
entire suffixes. We show how to do this in $O(\lg\lg n +
\frac{\lg^{2}\lg\sigma}{\lg\lg\lg\sigma})$ time, which improves the previously
known $O(\lg n)$ bound [Amir et al., SPIRE'05].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3360</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3360</id><created>2013-02-14</created><updated>2015-03-16</updated><authors><author><keyname>L&#xea;</keyname><forenames>H&#xf4;ng V&#xe2;n</forenames></author></authors><title>Lower bounds for the circuit size of partially homogeneous polynomials</title><categories>cs.CC math.AC math.LO</categories><comments>25 pages, final version, accepted to Journal of Fundamental and
  Applied Mathematics. Some missing definitions and coefficients in the last
  version have been added and corrected now</comments><msc-class>03D15, 68Q17, 13P25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we associate to each multivariate polynomial $f$ that is
homogeneous relative to a subset of its variables a series of polynomial
families $P_\lambda (f)$ of $m$-tuples of homogeneous polynomials of equal
degree such that the circuit size of any member in $P_\lambda (f)$ is bounded
from above by the circuit size of $f$. This provides a method for obtaining
lower bounds for the circuit size of $f$ by proving $(s,r)$-(weak) elusiveness
of the polynomial mapping associated with $P_\lambda (f)$. We discuss some
algebraic methods for proving the $(s,r)$-(weak) elusiveness. We also improve
estimates in the normal homogeneous-form of an arithmetic circuit obtained by
Raz in \cite{Raz2009} which results in better lower bounds for circuit size
(Lemma \ref{lem:cor38}, Remark \ref{rem:cor38}).
  Our methods yield non-trivial lower bound for the circuit size of several
classes of multivariate homogeneous polynomials (Corollary \ref{cor:412},
Example \ref{ex:bi}).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3363</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3363</id><created>2013-02-14</created><authors><author><keyname>Paulev&#xe9;</keyname><forenames>Lo&#xef;c</forenames></author><author><keyname>Craciun</keyname><forenames>Gheorghe</forenames></author><author><keyname>Koeppl</keyname><forenames>Heinz</forenames></author></authors><title>Dynamical Properties of Discrete Reaction Networks</title><categories>cs.DM math.DS</categories><doi>10.1007/s00285-013-0686-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reaction networks are commonly used to model the evolution of populations of
species subject to transformations following an imposed stoichiometry. This
paper focuses on the efficient characterisation of dynamical properties of
Discrete Reaction Networks (DRNs). DRNs can be seen as modelling the underlying
discrete nondeterministic transitions of stochastic models of reactions
networks. In that sense, any proof of non-reachability in DRNs directly applies
to any concrete stochastic models, independently of kinetics laws and
constants. Moreover, if stochastic kinetic rates never vanish, reachability
properties are equivalent in the two settings. The analysis of two global
dynamical properties of DRNs is addressed: irreducibility, i.e., the ability to
reach any discrete state from any other state; and recurrence, i.e., the
ability to return to any initial state. Our results consider both the
verification of such properties when species are present in a large copy
number, and in the general case. The obtained necessary and sufficient
conditions involve algebraic conditions on the network reactions which in most
cases can be verified using linear programming. Finally, the relationship of
DRN irreducibility and recurrence with dynamical properties of stochastic and
continuous models of reaction networks is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3365</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3365</id><created>2013-02-14</created><authors><author><keyname>Paulev&#xe9;</keyname><forenames>Lo&#xef;c</forenames></author><author><keyname>Andrieux</keyname><forenames>Geoffroy</forenames></author><author><keyname>Koeppl</keyname><forenames>Heinz</forenames></author></authors><title>Under-approximating Cut Sets for Reachability in Large Scale Automata
  Networks</title><categories>cs.SY</categories><doi>10.1007/978-3-642-39799-8_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the scope of discrete finite-state models of interacting components, we
present a novel algorithm for identifying sets of local states of components
whose activity is necessary for the reachability of a given local state. If all
the local states from such a set are disabled in the model, the concerned
reachability is impossible. Those sets are referred to as cut sets and are
computed from a particular abstract causality structure, so-called Graph of
Local Causality, inspired from previous work and generalised here to finite
automata networks. The extracted sets of local states form an
under-approximation of the complete minimal cut sets of the dynamics: there may
exist smaller or additional cut sets for the given reachability. Applied to
qualitative models of biological systems, such cut sets provide potential
therapeutic targets that are proven to prevent molecules of interest to become
active, up to the correctness of the model. Our new method makes tractable the
formal analysis of very large scale networks, as illustrated by the computation
of cut sets within a Boolean model of biological pathways interactions
gathering more than 9000 components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3379</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3379</id><created>2013-02-14</created><authors><author><keyname>South</keyname><forenames>David M.</forenames></author></authors><title>The DPHEP Study Group: Data Preservation in High Energy Physics</title><categories>hep-ex cs.DL</categories><comments>6 pages, 2 figures; proceedings of talk given at ICHEP 2012,
  Melbourne, July 2012</comments><report-no>ICHEP 2012</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An inter-experimental study group, DPHEP, was formed in 2009 to
systematically investigate the technical and organisational aspects of data
preservation and long-term analysis in high-energy physics, a subject which had
hitherto lacked clarity in the field. The study group includes representation
from all major high-energy physics collider-based experiments and laboratories,
as well as computing centres and funding agencies. A major report was released
in May 2012, greatly expanding on the ideas contained in a preliminary
publication three years earlier, and providing a more solid set of
recommendations, not only concerning data preservation and its implementation
in high-energy physics, but also the future direction and organisational model
of the study group. A brief description of the DPHEP Study Group and some of
the key messages from the major report are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3404</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3404</id><created>2013-02-14</created><authors><author><keyname>Larjomaa</keyname><forenames>Tommi</forenames></author><author><keyname>Popa</keyname><forenames>Alexandru</forenames></author></authors><title>The min-max edge q-coloring problem</title><categories>cs.DS</categories><comments>16 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce and study a new problem named \emph{min-max edge
$q$-coloring} which is motivated by applications in wireless mesh networks. The
input of the problem consists of an undirected graph and an integer $q$. The
goal is to color the edges of the graph with as many colors as possible such
that: (a) any vertex is incident to at most $q$ different colors, and (b) the
maximum size of a color group (i.e. set of edges identically colored) is
minimized. We show the following results: 1. Min-max edge $q$-coloring is
NP-hard, for any $q \ge 2$. 2. A polynomial time exact algorithm for min-max
edge $q$-coloring on trees. 3. Exact formulas of the optimal solution for
cliques and almost tight bounds for bicliques and hypergraphs. 4. A non-trivial
lower bound of the optimal solution with respect to the average degree of the
graph. 5. An approximation algorithm for planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3407</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3407</id><created>2013-02-14</created><authors><author><keyname>Khaleghi</keyname><forenames>Azaden</forenames></author><author><keyname>Ryabko</keyname><forenames>Daniil</forenames></author></authors><title>A consistent clustering-based approach to estimating the number of
  change-points in highly dependent time-series</title><categories>stat.ML cs.IT cs.LG math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of change-point estimation is considered under a general
framework where the data are generated by unknown stationary ergodic process
distributions. In this context, the consistent estimation of the number of
change-points is provably impossible. However, it is shown that a consistent
clustering method may be used to estimate the number of change points, under
the additional constraint that the correct number of process distributions that
generate the data is provided. This additional parameter has a natural
interpretation in many real-world applications. An algorithm is proposed that
estimates the number of change-points and locates the changes. The proposed
algorithm is shown to be asymptotically consistent; its empirical evaluations
are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3412</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3412</id><created>2013-02-14</created><updated>2014-06-03</updated><authors><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Cai</keyname><forenames>Minglai</forenames></author><author><keyname>Cai</keyname><forenames>Ning</forenames></author><author><keyname>Deppe</keyname><forenames>Christian</forenames></author></authors><title>Secrecy capacities of compound quantum wiretap channels and applications</title><categories>cs.IT math.IT quant-ph</categories><comments>We revised and improved our paper arXiv:1208.1151. We added a new
  result for capacity and a new section for applications. arXiv admin note:
  text overlap with arXiv:1202.0773. Published in the 1 May 2014 issue of
  Physical Review A, Vol.89, No.5</comments><doi>10.1103/PhysRevA.89.052320</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We determine the secrecy capacity of the compound channel with quantum
wiretapper and channel state information at the transmitter. Moreover, we
derive a lower bound on the secrecy capacity of this channel without channel
state information and determine the secrecy capacity of the compound
classical-quantum wiretap channel with channel state information at the
transmitter. We use this result to derive a new proof for a lower bound on the
entanglement generating capacity of compound quantum channel. We also derive a
new proof for the entanglement generating capacity of compound quantum channel
with channel state information at the encoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3416</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3416</id><created>2013-02-14</created><authors><author><keyname>Charalambous</keyname><forenames>Charalambos D.</forenames></author><author><keyname>Ahmed</keyname><forenames>Nasir U.</forenames></author></authors><title>Centralized Versus Decentralized Team Games of Distributed Stochastic
  Differential Decision Systems with Noiseless Information Structures-Part II:
  Applications</title><categories>math.OC cs.IT cs.SY math.IT</categories><comments>39 pages Submitted to IEEE Transaction on Automatic Control</comments><msc-class>49J55, 49K45, 93E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this second part of our two-part paper, we invoke the stochastic maximum
principle, conditional Hamiltonian and the coupled backward-forward stochastic
differential equations of the first part [1] to derive team optimal
decentralized strategies for distributed stochastic differential systems with
noiseless information structures. We present examples of such team games of
nonlinear as well as linear quadratic forms. In some cases we obtain closed
form expressions of the optimal decentralized strategies.
  Through the examples, we illustrate the effect of information signaling among
the decision makers in reducing the computational complexity of optimal
decentralized decision strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3417</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3417</id><created>2013-02-14</created><authors><author><keyname>Levi</keyname><forenames>Reut</forenames></author><author><keyname>Ron</keyname><forenames>Dana</forenames></author></authors><title>A Quasi-Polynomial Time Partition Oracle for Graphs with an Excluded
  Minor</title><categories>cs.DS</categories><comments>13 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the problem of testing planarity and related properties, we
study the problem of designing efficient {\em partition oracles}. A {\em
partition oracle} is a procedure that, given access to the incidence lists
representation of a bounded-degree graph $G= (V,E)$ and a parameter $\eps$,
when queried on a vertex $v\in V$, returns the part (subset of vertices) which
$v$ belongs to in a partition of all graph vertices. The partition should be
such that all parts are small, each part is connected, and if the graph has
certain properties, the total number of edges between parts is at most $\eps
|V|$. In this work we give a partition oracle for graphs with excluded minors
whose query complexity is quasi-polynomial in $1/\eps$, thus improving on the
result of Hassidim et al. ({\em Proceedings of FOCS 2009}) who gave a partition
oracle with query complexity exponential in $1/\eps$. This improvement implies
corresponding improvements in the complexity of testing planarity and other
properties that are characterized by excluded minors as well as sublinear-time
approximation algorithms that work under the promise that the graph has an
excluded minor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3422</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3422</id><created>2013-02-14</created><updated>2015-06-26</updated><authors><author><keyname>Hu</keyname><forenames>Kai</forenames></author><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Yin</keyname><forenames>Baolin</forenames></author></authors><title>Baselining Network-Wide Traffic by Time-Frequency Constrained Stable
  Principal Component Pursuit</title><categories>cs.NI</categories><comments>Accepted to AEU-International Journal of Electronics and
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet traffic analysis is important to network management,and
extracting the baseline traffic patterns is especially helpful for some
significant network applications.In this paper, we study on the baseline
problem of the traffic matrix satisfying a refined traffic matrix decomposition
model,since this model extends the assumption of the baseline traffic component
to characterize its smoothness, and is more realistic than the existing traffic
matrix models. We develop a novel baseline scheme, named Stable Principal
Component Pursuit with Time-Frequency Constraints (SPCP-TFC), which extends the
Stable Principal Component Pursuit (SPCP) by applying new time-frequency
constraints. Then we design an efficient numerical algorithm for SPCP-TFC. At
last, we evaluate this baseline scheme through simulations, and show it has
superior performance than the existing baseline schemes RBL and PCA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3433</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3433</id><created>2013-02-14</created><authors><author><keyname>Wilson</keyname><forenames>Richard C.</forenames></author><author><keyname>Aziz</keyname><forenames>Furqan</forenames></author><author><keyname>Hancock</keyname><forenames>Edwin R.</forenames></author></authors><title>Eigenfunctions of the Edge-Based Laplacian on a Graph</title><categories>cs.DM math.CO math.SP</categories><comments>10 pages, 1 figure</comments><msc-class>05C50, 05C81</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the eigenfunctions of the edge-based Laplacian on a
graph and the relationship of these functions to random walks on the graph. We
commence by discussing the set of eigenfunctions supported at the vertices, and
demonstrate the relationship of these eigenfunctions to the classical random
walk on the graph. Then, from an analysis of functions supported only on the
interior of edges, we develop a method for explicitly calculating the
edge-interior eigenfunctions of the edge-based Laplacian. This reveals a
connection between the edge-based Laplacian and the adjacency matrix of
backtrackless random walk on the graph. The edge-based eigenfunctions therefore
correspond to some eigenfunctions of the normalised Hashimoto matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3437</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3437</id><created>2013-02-14</created><updated>2013-07-25</updated><authors><author><keyname>Apostolico</keyname><forenames>Alberto</forenames></author><author><keyname>Erd&#x151;s</keyname><forenames>P&#xe9;ter L.</forenames></author><author><keyname>Mikl&#xf3;s</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>Siemons</keyname><forenames>Johannes</forenames></author></authors><title>Modulated String Searching</title><categories>cs.DS cs.DM math.CO</categories><comments>10 pages</comments><msc-class>68W32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In his 1987 paper entitled &quot;Generalized String Matching&quot;, Abrahamson
introduced {\em pattern matching with character classes} and provided the first
efficient algorithm to solve it. The best known solution to date is due to
Linhart and Shamir (2009).
  Another broad yet comparatively less studied class of string matching
problems is that of numerical string searching, such as, e.g., the `less-than'
or $L_1$-norm string searching. The best known solutions for problems in this
class are based on FFT convolution after some suitable re-encoding.
  The present paper introduces {\em modulated string searching} as a unified
framework for string matching problems where the numerical conditions can be
combined with some Boolean/numerical decision conditions on the character
classes. One example problem in this class is the {\em locally bounded
$L_1$-norm} matching problem on character classes: here the &quot;match&quot; between a
character at some position in the text and a set of characters at some position
in the pattern is assessed based on the smallest $L_1$ distance between the
text character and one of those pattern characters. The two positions &quot;match&quot;
if the (absolute value of the) difference between the two characters does not
exceed a predefined constant. The pattern has an occurrence in an alignment
with the text if the sum of all such differences does not exceed a second
predefined constant value. This problem requires a pointwise evaluation of the
quality of each match and has no known solution based on the previously
mentioned algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3440</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3440</id><created>2013-02-14</created><authors><author><keyname>Rachadi</keyname><forenames>Abdeljalil</forenames></author><author><keyname>Jedra</keyname><forenames>Mohamed</forenames></author><author><keyname>Zahid</keyname><forenames>Noureddine</forenames></author></authors><title>Self Avoiding Paths Routing Algorithm in Scale-Free Networks</title><categories>cs.NI physics.comp-ph</categories><comments>Published in Chaos: An Interdisciplinary Journal of Nonlinear Science</comments><journal-ref>Chaos 23, 013114 (2013)</journal-ref><doi>10.1063/1.4790864</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new routing algorithm called &quot;the Self Avoiding
Paths Routing Algorithm&quot;. Its application to traffic flow in scale-free
networks shows a great improvement over the so called &quot;efficient routing&quot;
protocol while at the same time maintaining a relatively low average packet
travel time. It has the advantage of minimizing path overlapping throughout the
network in a self consistent manner with a relatively small number of
iterations by maintaining an equilibrated path distribution especially among
the hubs. This results in a significant shifting of the critical packet
generation rate over which traffic congestion occurs, thus permitting the
network to sustain more information packets in the free flow state. The
performance of the algorithm is discussed both on a Bar\'abasi-Albert (BA)
network and real autonomous system (AS) network data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3446</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3446</id><created>2013-02-14</created><updated>2013-10-15</updated><authors><author><keyname>Yuan</keyname><forenames>Xin</forenames></author><author><keyname>Yang</keyname><forenames>Jianbo</forenames></author><author><keyname>Llull</keyname><forenames>Patrick</forenames></author><author><keyname>Liao</keyname><forenames>Xuejun</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author><author><keyname>Brady</keyname><forenames>David J.</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author></authors><title>Adaptive Temporal Compressive Sensing for Video</title><categories>stat.AP cs.CV cs.MM</categories><comments>IEEE Interonal International Conference on Image Processing
  (ICIP),2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the concept of adaptive temporal compressive sensing
(CS) for video. We propose a CS algorithm to adapt the compression ratio based
on the scene's temporal complexity, computed from the compressed data, without
compromising the quality of the reconstructed video. The temporal adaptivity is
manifested by manipulating the integration time of the camera, opening the
possibility to real-time implementation. The proposed algorithm is a
generalized temporal CS approach that can be incorporated with a diverse set of
existing hardware systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3447</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3447</id><created>2013-02-13</created><authors><author><keyname>Chen</keyname><forenames>Zhengjia</forenames></author><author><keyname>Chen</keyname><forenames>Xinjia</forenames></author></authors><title>Exact Methods for Multistage Estimation of a Binomial Proportion</title><categories>math.ST cs.LG cs.NA math.PR stat.TH</categories><comments>38 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first review existing sequential methods for estimating a binomial
proportion. Afterward, we propose a new family of group sequential sampling
schemes for estimating a binomial proportion with prescribed margin of error
and confidence level. In particular, we establish the uniform controllability
of coverage probability and the asymptotic optimality for such a family of
sampling schemes. Our theoretical results establish the possibility that the
parameters of this family of sampling schemes can be determined so that the
prescribed level of confidence is guaranteed with little waste of samples.
Analytic bounds for the cumulative distribution functions and expectations of
sample numbers are derived. Moreover, we discuss the inherent connection of
various sampling schemes. Numerical issues are addressed for improving the
accuracy and efficiency of computation. Computational experiments are conducted
for comparing sampling schemes. Illustrative examples are given for
applications in clinical trials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3452</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3452</id><created>2013-02-14</created><authors><author><keyname>Charalambous</keyname><forenames>Charalambos D.</forenames></author><author><keyname>Ahmed</keyname><forenames>Nasir U.</forenames></author></authors><title>Centralized Versus Decentralized Team Games of Distributed Stochastic
  Differential Decision Systems with Noiseless Information Structures-Part I:
  General Theory</title><categories>math.OC cs.IT math.IT math.ST stat.TH</categories><comments>39 pages, 1 figure Submitted to IEEE Transactions on Automatic
  Control</comments><msc-class>49J55, 49K45, 93E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decentralized optimization of distributed stochastic differential systems has
been an active area of research for over half a century. Its formulation
utilizing static team and person-by-person optimality criteria is well
investigated. However, the results have not been generalized to nonlinear
distributed stochastic differential systems possibly due to technical
difficulties inherent with decentralized decision strategies.
  In this first part of the two-part paper, we derive team optimality and
person-by-person optimality conditions for distributed stochastic differential
systems with different information structures. The optimality conditions are
given in terms of a Hamiltonian system of equations described by a system of
coupled backward and forward stochastic differential equations and a
conditional Hamiltonian, under both regular and relaxed strategies. Our
methodology is based on the semi martingale representation theorem and
variational methods. Throughout the presentation we discuss similarities to
optimality conditions of centralized decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3462</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3462</id><created>2013-02-14</created><updated>2013-02-15</updated><authors><author><keyname>Stowell</keyname><forenames>Dan</forenames></author><author><keyname>Mu&#x161;evi&#x10d;</keyname><forenames>Sa&#x161;o</forenames></author><author><keyname>Bonada</keyname><forenames>Jordi</forenames></author><author><keyname>Plumbley</keyname><forenames>Mark D.</forenames></author></authors><title>Improved multiple birdsong tracking with distribution derivative method
  and Markov renewal process clustering</title><categories>cs.SD</categories><comments>Submitted to ICASSP 2013</comments><doi>10.1109/ICASSP.2013.6637691</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segregating an audio mixture containing multiple simultaneous bird sounds is
a challenging task. However, birdsong often contains rapid pitch modulations,
and these modulations carry information which may be of use in automatic
recognition. In this paper we demonstrate that an improved spectrogram
representation, based on the distribution derivative method, leads to improved
performance of a segregation algorithm which uses a Markov renewal process
model to track vocalisation patterns consisting of singing and silences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3465</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3465</id><created>2013-02-14</created><authors><author><keyname>Dunn</keyname><forenames>J. Michael</forenames></author><author><keyname>Moss</keyname><forenames>Lawrence S.</forenames></author><author><keyname>Wang</keyname><forenames>Zhenghan</forenames></author></authors><title>The Third Life of Quantum Logic: Quantum Logic Inspired by Quantum
  Computing</title><categories>math.LO cs.LO quant-ph</categories><comments>To appear in a special issue of Journal of Philosophical Logic</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We begin by discussing the history of quantum logic, dividing it into three
eras or lives. The first life has to do with Birkhoff and von Neumann's
algebraic approach in the 1930's. The second life has to do with attempt to
understand quantum logic as logic that began in the late 1950's and blossomed
in the 1970's. And the third life has to do with recent developments in quantum
logic coming from its connections to quantum computation. We discuss our own
work connecting quantum logic to quantum computation (viewing quantum logic as
the logic of quantum registers storing qubits), and make some speculations
about mathematics based on quantum principles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3481</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3481</id><created>2013-02-14</created><updated>2014-01-22</updated><authors><author><keyname>Je&#x17c;</keyname><forenames>Artur</forenames></author></authors><title>One-variable word equations in linear time</title><categories>cs.FL cs.DS cs.LO</categories><comments>submitted to a journal, general overhaul over the previous version</comments><acm-class>F.2.2; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider word equations with one variable (and arbitrary
many appearances of it). A recent technique of recompression, which is
applicable to general word equations, is shown to be suitable also in this
case. While in general case it is non-deterministic, it determinises in case of
one variable and the obtained running time is O(n + #_X log n), where #_X is
the number of appearances of the variable in the equation. This matches the
previously-best algorithm due to D\k{a}browski and Plandowski. Then, using a
couple of heuristics as well as more detailed time analysis the running time is
lowered to O(n) in RAM model. Unfortunately no new properties of solutions are
shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3486</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3486</id><created>2013-02-14</created><authors><author><keyname>Bonamy</keyname><forenames>Marthe</forenames></author><author><keyname>Bousquet</keyname><forenames>Nicolas</forenames></author></authors><title>Recoloring bounded treewidth graphs</title><categories>cs.DM math.CO</categories><comments>11 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $k$ be an integer. Two vertex $k$-colorings of a graph are
\emph{adjacent} if they differ on exactly one vertex. A graph is
\emph{$k$-mixing} if any proper $k$-coloring can be transformed into any other
through a sequence of adjacent proper $k$-colorings. Any graph is
$(tw+2)$-mixing, where $tw$ is the treewidth of the graph (Cereceda 2006). We
prove that the shortest sequence between any two $(tw+2)$-colorings is at most
quadratic, a problem left open in Bonamy et al. (2012).
  Jerrum proved that any graph is $k$-mixing if $k$ is at least the maximum
degree plus two. We improve Jerrum's bound using the grundy number, which is
the worst number of colors in a greedy coloring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3489</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3489</id><created>2013-02-14</created><authors><author><keyname>Cece</keyname><forenames>G&#xe9;rard</forenames><affiliation>FEMTO-ST/DISC</affiliation></author></authors><title>Bisimulations over DLTS in O(m.log n)-time</title><categories>cs.FL</categories><comments>Submitted to DLT'13</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The well known Hopcroft's algorithm to minimize deterministic complete
automata runs in $O(kn\log n)$-time, where $k$ is the size of the alphabet and
$n$ the number of states. The main part of this algorithm corresponds to the
computation of a coarsest bisimulation over a finite Deterministic Labelled
Transition System (DLTS). By applying techniques we have developed in the case
of simulations, we design a new algorithm which computes the coarsest
bisimulation over a finite DLTS in $O(m\log n)$-time and $O(k+m+n)$-space, with
$m$ the number of transitions. The underlying DLTS does not need to be complete
and thus: $m\leq kn$. This new algorithm is much simpler than the two others
found in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3492</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3492</id><created>2013-02-14</created><updated>2013-07-15</updated><authors><author><keyname>Courtade</keyname><forenames>Thomas A.</forenames></author></authors><title>Outer Bounds for Multiterminal Source Coding via a Strong Data
  Processing Inequality</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures. Presented at ISIT 2013 in Istanbul, Turkey.
  (NOTE: v2 corrects an error in v1 due to its use of the Erkip-Cover strong
  data processing inequality. This inequality has recently been corrected by
  Anantharam et al. in arxiv/1304.6133v1 [cs.IT])</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An intuitive outer bound for the multiterminal source coding problem is
given. The proposed bound explicitly couples the rate distortion functions for
each source and correlation measures which derive from a &quot;strong&quot; data
processing inequality. Unlike many standard outer bounds, the proposed bound is
not parameterized by a continuous family of auxiliary random variables, but
instead only requires maximizing two ratios of divergences which do not depend
on the distortion functions under consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3494</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3494</id><created>2013-02-14</created><updated>2013-02-15</updated><authors><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author></authors><title>On Polynomial Kernels for Sparse Integer Linear Programs</title><categories>cs.CC cs.DS</categories><comments>To appear in STACS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integer linear programs (ILPs) are a widely applied framework for dealing
with combinatorial problems that arise in practice. It is known, e.g., by the
success of CPLEX, that preprocessing and simplification can greatly speed up
the process of optimizing an ILP. The present work seeks to further the
theoretical understanding of preprocessing for ILPs by initiating a rigorous
study within the framework of parameterized complexity and kernelization.
  A famous result of Lenstra (Mathematics of Operations Research, 1983) shows
that feasibility of any ILP with n variables and m constraints can be decided
in time O(c^{n^3} m^c'). Thus, by a folklore argument, any such ILP admits a
kernelization to an equivalent instance of size O(c^{n^3}). It is known, that
unless NP \subseteq coNP/poly and the polynomial hierarchy collapses, no
kernelization with size bound polynomial in n is possible. However, this lower
bound only applies for the case when constraints may include an arbitrary
number of variables since it follows from lower bounds for Satisfiability and
Hitting Set, whose bounded arity variants admit polynomial kernelizations.
  We consider the feasibility problem for ILPs Ax&lt;= b where A is an
r-row-sparse matrix parameterized by the number of variables. We show that the
kernelizability of this problem depends strongly on the range of the variables.
If the range is unbounded then this problem does not admit a polynomial
kernelization unless NP \subseteq coNP/poly. If, on the other hand, the range
of each variable is polynomially bounded in n then we do get a polynomial
kernelization. Additionally, this holds also for the more general case when the
maximum range d is an additional parameter, i.e., the size obtained is
polynomial in n+d.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3496</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3496</id><created>2013-02-14</created><updated>2013-02-15</updated><authors><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author></authors><title>On Polynomial Kernels for Integer Linear Programs: Covering, Packing and
  Feasibility</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the existence of polynomial kernels for the problem of deciding
feasibility of integer linear programs (ILPs), and for finding good solutions
for covering and packing ILPs. Our main results are as follows: First, we show
that the ILP Feasibility problem admits no polynomial kernelization when
parameterized by both the number of variables and the number of constraints,
unless NP \subseteq coNP/poly. This extends to the restricted cases of bounded
variable degree and bounded number of variables per constraint, and to covering
and packing ILPs. Second, we give a polynomial kernelization for the Cover ILP
problem, asking for a solution to Ax &gt;= b with c^Tx &lt;= k, parameterized by k,
when A is row-sparse; this generalizes a known polynomial kernelization for the
special case with 0/1-variables and coefficients (d-Hitting Set).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3498</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3498</id><created>2013-02-14</created><authors><author><keyname>Boros</keyname><forenames>Endre</forenames></author><author><keyname>Gurvich</keyname><forenames>Vladimir</forenames></author><author><keyname>Milanic</keyname><forenames>Martin</forenames></author></authors><title>On CIS Circulants</title><categories>math.CO cs.DM</categories><comments>27 pages</comments><msc-class>05C25, 05C69</msc-class><journal-ref>Discrete Mathematics 318 (2014) 78--95</journal-ref><doi>10.1016/j.disc.2013.11.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A circulant is a Cayley graph over a cyclic group. A well-covered graph is a
graph in which all maximal stable sets are of the same size, or in other words,
they are all maximum. A CIS graph is a graph in which every maximal stable set
and every maximal clique intersect. It is not difficult to show that a
circulant G is a CIS graph if and only if G and its complement are both
well-covered and the product of the independence and the clique numbers of G is
equal to the number of vertices. It is also easy to demonstrate that both
families, the circulants and the CIS graphs, are closed with respect to the
operations of taking the complement and lexicographic product. We study the
structure of the CIS circulants. It is well-known that all P_4-free graphs are
CIS. In this paper, in addition to the simple family of the P_4-free
circulants, we construct a non-trivial sparse but infinite family of CIS
circulants. We are not aware of any CIS circulant that could not be obtained
from graphs in this family by the operations of taking the complement and
lexicographic product.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3518</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3518</id><created>2013-02-14</created><updated>2013-07-14</updated><authors><author><keyname>Even</keyname><forenames>Guy</forenames></author><author><keyname>Halabi</keyname><forenames>Nissim</forenames></author></authors><title>Analysis of the Min-Sum Algorithm for Packing and Covering Problems via
  Linear Programming</title><categories>cs.IT cs.DM cs.DS math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Message-passing algorithms based on belief-propagation (BP) are successfully
used in many applications including decoding error correcting codes and solving
constraint satisfaction and inference problems. BP-based algorithms operate
over graph representations, called factor graphs, that are used to model the
input. Although in many cases BP-based algorithms exhibit impressive empirical
results, not much has been proved when the factor graphs have cycles.
  This work deals with packing and covering integer programs in which the
constraint matrix is zero-one, the constraint vector is integral, and the
variables are subject to box constraints. We study the performance of the
min-sum algorithm when applied to the corresponding factor graph models of
packing and covering LPs.
  We compare the solutions computed by the min-sum algorithm for packing and
covering problems to the optimal solutions of the corresponding linear
programming (LP) relaxations. In particular, we prove that if the LP has an
optimal fractional solution, then for each fractional component, the min-sum
algorithm either computes multiple solutions or the solution oscillates below
and above the fraction. This implies that the min-sum algorithm computes the
optimal integral solution only if the LP has a unique optimal solution that is
integral.
  The converse is not true in general. For a special case of packing and
covering problems, we prove that if the LP has a unique optimal solution that
is integral and on the boundary of the box constraints, then the min-sum
algorithm computes the optimal solution in pseudo-polynomial time.
  Our results unify and extend recent results for the maximum weight matching
problem by [Sanghavi et al.,'2011] and [Bayati et al., 2011] and for the
maximum weight independent set problem [Sanghavi et al.'2009].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3530</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3530</id><created>2013-02-14</created><updated>2013-08-09</updated><authors><author><keyname>Krioukov</keyname><forenames>Dmitri</forenames></author><author><keyname>Ostilli</keyname><forenames>Massimo</forenames></author></authors><title>Duality between equilibrium and growing networks</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><journal-ref>Phys. Rev. E 88, 022808 (2013)</journal-ref><doi>10.1103/PhysRevE.88.022808</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In statistical physics any given system can be either at an equilibrium or
away from it. Networks are not an exception. Most network models can be
classified as either equilibrium or growing. Here we show that under certain
conditions there exists an equilibrium formulation for any growing network
model, and vice versa. The equivalence between the equilibrium and
nonequilibrium formulations is exact not only asymptotically, but even for any
finite system size. The required conditions are satisfied in random geometric
graphs in general and causal sets in particular, and to a large extent in some
real networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3541</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3541</id><created>2013-02-14</created><authors><author><keyname>Buzas</keyname><forenames>Jeffrey S.</forenames></author><author><keyname>Dinitz</keyname><forenames>Jeffrey</forenames></author></authors><title>An analysis of NK and generalized NK landscapes</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulated landscapes have been used for decades to evaluate search strategies
whose goal is to find the landscape location with maximum fitness. Applications
include modeling the capacity of enzymes to catalyze reactions and the clinical
effectiveness of medical treatments. Understanding properties of landscapes is
important for understanding search difficulty. This paper presents a novel and
transparent characterization of NK landscapes.
  We prove that NK landscapes can be represented by parametric linear
interaction models where model coefficients have meaningful interpretations. We
derive the statistical properties of the model coefficients, providing insight
into how the NK algorithm parses importance to main effects and interactions.
An important insight derived from the linear model representation is that the
rank of the linear model defined by the NK algorithm is correlated with the
number of local optima, a strong determinant of landscape complexity and search
difficulty. We show that the maximal rank for an NK landscape is achieved
through epistatic interactions that form partially balanced incomplete block
designs. Finally, an analytic expression representing the expected number of
local optima on the landscape is derived, providing a way to quickly compute
the expected number of local optima for very large landscapes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3545</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3545</id><created>2013-02-14</created><authors><author><keyname>Davies</keyname><forenames>Todd</forenames></author><author><keyname>Newman</keyname><forenames>Benjamin</forenames></author><author><keyname>O'Connor</keyname><forenames>Brendan</forenames></author><author><keyname>Tam</keyname><forenames>Aaron</forenames></author><author><keyname>Perry</keyname><forenames>Leo</forenames></author></authors><title>Displaying Asynchronous Reactions to a Document: Two Goals and a Design</title><categories>cs.HC</categories><comments>Appeared as a Poster Paper, Conference on Computer Supported
  Cooperative Work, 20th Anniversary - Conference Supplement (CSCW 2006, Banff,
  November 4-8, 2006), pp. 169-170; Modified as &quot;Document Centered Discussion:
  A Design Pattern for Online Deliberation&quot;, in D. Schuler, Liberating Voices:
  A Pattern Language for Communication Revolution, MIT Press, 2008, pp.
  384-386; 2 pages, 1 figure, 1 table</comments><acm-class>H.5.3; I.7.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We describe and motivate three goals for the screen display of asynchronous
text deliberation pertaining to a document: (1) visibility of relationships
between comments and the text they reference, between different comments, and
between group members and the document and discussion, and (2)
distinguishability of boundaries between contextually related and unrelated
text and comments and between individual authors of documents and comments.
Interfaces for document-centered discussion generally fail to fulfill one or
both of these goals as well as they could. We describe the design of the new
version of Deme, a Web-based platform for online deliberation, and argue that
it achieves the two goals better than other recent designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3548</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3548</id><created>2013-02-14</created><updated>2013-02-15</updated><authors><author><keyname>Czabarka</keyname><forenames>&#xc9;va</forenames></author><author><keyname>Dutle</keyname><forenames>Aaron</forenames></author><author><keyname>Erd&#xf6;s</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>Mikl&#xf3;s</keyname><forenames>Istv&#xe1;n</forenames></author></authors><title>On Realizations of a Joint Degree Matrix</title><categories>math.CO cs.DM cs.SI</categories><comments>12 pages, 2 figures</comments><msc-class>05C82, 90B10, 90C40</msc-class><journal-ref>Disc. Appl. Math 181 (2015), 283-288</journal-ref><doi>10.1016/j.dam.2014.10.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The joint degree matrix of a graph gives the number of edges between vertices
of degree i and degree j for every pair (i,j). One can perform restricted swap
operations to transform a graph into another with the same joint degree matrix.
We prove that the space of all realizations of a given joint degree matrix over
a fixed vertex set is connected via these restricted swap operations. This was
claimed before, but there is an error in the previous proof, which we
illustrate by example. We also give a simplified proof of the necessary and
sufficient conditions for a matrix to be a joint degree matrix. Finally, we
address some of the issues concerning the mixing time of the corresponding MCMC
method to sample uniformly from these realizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3549</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3549</id><created>2013-02-13</created><authors><author><keyname>Acid</keyname><forenames>Silvia</forenames></author><author><keyname>de Campos</keyname><forenames>Luis M.</forenames></author></authors><title>An Algorithm for Finding Minimum d-Separating Sets in Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-3-10</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The criterion commonly used in directed acyclic graphs (dags) for testing
graphical independence is the well-known d-separation criterion. It allows us
to build graphical representations of dependency models (usually probabilistic
dependency models) in the form of belief networks, which make easy
interpretation and management of independence relationships possible, without
reference to numerical parameters (conditional probabilities). In this paper,
we study the following combinatorial problem: finding the minimum d-separating
set for two nodes in a dag. This set would represent the minimum information
(in the sense of minimum number of variables) necessary to prevent these two
nodes from influencing each other. The solution to this basic problem and some
of its extensions can be useful in several ways, as we shall see later. Our
solution is based on a two-step process: first, we reduce the original problem
to the simpler one of finding a minimum separating set in an undirected graph,
and second, we develop an algorithm for solving it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3550</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3550</id><created>2013-02-13</created><authors><author><keyname>Agosta</keyname><forenames>John Mark</forenames></author></authors><title>Constraining Influence Diagram Structure by Generative Planning: An
  Application to the Optimization of Oil Spill Response</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-11-19</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper works through the optimization of a real world planning problem,
with a combination of a generative planning tool and an influence diagram
solver. The problem is taken from an existing application in the domain of oil
spill emergency response. The planning agent manages constraints that order
sets of feasible equipment employment actions. This is mapped at an
intermediate level of abstraction onto an influence diagram. In addition, the
planner can apply a surveillance operator that determines observability of the
state---the unknown trajectory of the oil. The uncertain world state and the
objective function properties are part of the influence diagram structure, but
not represented in the planning agent domain. By exploiting this structure
under the constraints generated by the planning agent, the influence diagram
solution complexity simplifies considerably, and an optimum solution to the
employment problem based on the objective function is found. Finding this
optimum is equivalent to the simultaneous evaluation of a range of plans. This
result is an example of bounded optimality, within the limitations of this
hybrid generative planner and influence diagram architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3551</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3551</id><created>2013-02-13</created><authors><author><keyname>Alag</keyname><forenames>Satnam</forenames></author><author><keyname>Agogino</keyname><forenames>Alice M.</forenames></author></authors><title>Inference Using Message Propagation and Topology Transformation in
  Vector Gaussian Continuous Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-20-27</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend Gaussian networks - directed acyclic graphs that encode
probabilistic relationships between variables - to its vector form. Vector
Gaussian continuous networks consist of composite nodes representing
multivariates, that take continuous values. These vector or composite nodes can
represent correlations between parents, as opposed to conventional univariate
nodes. We derive rules for inference in these networks based on two methods:
message propagation and topology transformation. These two approaches lead to
the development of algorithms, that can be implemented in either a centralized
or a decentralized manner. The domain of application of these networks are
monitoring and estimation problems. This new representation along with the
rules for inference developed here can be used to derive current Bayesian
algorithms such as the Kalman filter, and provide a rich foundation to develop
new algorithms. We illustrate this process by deriving the decentralized form
of the Kalman filter. This work unifies concepts from artificial intelligence
and modern control theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3552</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3552</id><created>2013-02-13</created><authors><author><keyname>Aliferis</keyname><forenames>Constantin F.</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>A Structurally and Temporally Extended Bayesian Belief Network Model:
  Definitions, Properties, and Modeling Techniques</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-28-39</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We developed the language of Modifiable Temporal Belief Networks (MTBNs) as a
structural and temporal extension of Bayesian Belief Networks (BNs) to
facilitate normative temporal and causal modeling under uncertainty. In this
paper we present definitions of the model, its components, and its fundamental
properties. We also discuss how to represent various types of temporal
knowledge, with an emphasis on hybrid temporal-explicit time modeling, dynamic
structures, avoiding causal temporal inconsistencies, and dealing with models
that involve simultaneously actions (decisions) and causal and non-causal
associations. We examine the relationships among BNs, Modifiable Belief
Networks, and MTBNs with a single temporal granularity, and suggest areas of
application suitable to each one of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3553</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3553</id><created>2013-02-13</created><authors><author><keyname>Andersson</keyname><forenames>Steen A.</forenames></author><author><keyname>Madigan</keyname><forenames>David</forenames></author><author><keyname>Perlman</keyname><forenames>Michael D.</forenames></author></authors><title>An Alternative Markov Property for Chain Graphs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-40-48</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphical Markov models use graphs, either undirected, directed, or mixed, to
represent possible dependences among statistical variables. Applications of
undirected graphs (UDGs) include models for spatial dependence and image
analysis, while acyclic directed graphs (ADGs), which are especially convenient
for statistical analysis, arise in such fields as genetics and psychometrics
and as models for expert systems and Bayesian belief networks. Lauritzen,
Wermuth and Frydenberg (LWF) introduced a Markov property for chain graphs,
which are mixed graphs that can be used to represent simultaneously both causal
and associative dependencies and which include both UDGs and ADGs as special
cases. In this paper an alternative Markov property (AMP) for chain graphs is
introduced, which in some ways is a more direct extension of the ADG Markov
property than is the LWF property for chain graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3554</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3554</id><created>2013-02-13</created><authors><author><keyname>Atkins</keyname><forenames>Ella M.</forenames></author><author><keyname>Durfee</keyname><forenames>Edmund H.</forenames></author><author><keyname>Shin</keyname><forenames>Kang G.</forenames></author></authors><title>Plan Development using Local Probabilistic Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-49-56</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate models of world state transitions are necessary when building
plans for complex systems operating in dynamic environments. External event
probabilities can depend on state feature values as well as time spent in that
particular state. We assign temporally -dependent probability functions to
state transitions. These functions are used to locally compute state
probabilities, which are then used to select highly probable goal paths and
eliminate improbable states. This probabilistic model has been implemented in
the Cooperative Intelligent Real-time Control Architecture (CIRCA), which
combines an AI planner with a separate real-time system such that plans are
developed, scheduled, and executed with real-time guarantees. We present flight
simulation tests that demonstrate how our probabilistic model may improve CIRCA
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3555</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3555</id><created>2013-02-13</created><authors><author><keyname>Bamber</keyname><forenames>Donald</forenames></author></authors><title>Entailment in Probability of Thresholded Generalizations</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-57-64</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A nonmonotonic logic of thresholded generalizations is presented. Given
propositions A and B from a language L and a positive integer k, the
thresholded generalization A=&gt;B{k} means that the conditional probability
P(B|A) falls short of one by no more than c*d^k. A two-level probability
structure is defined. At the lower level, a model is defined to be a
probability function on L. At the upper level, there is a probability
distribution over models. A definition is given of what it means for a
collection of thresholded generalizations to entail another thresholded
generalization. This nonmonotonic entailment relation, called &quot;entailment in
probability&quot;, has the feature that its conclusions are &quot;probabilistically
trustworthy&quot; meaning that, given true premises, it is improbable that an
entailed conclusion would be false. A procedure is presented for ascertaining
whether any given collection of premises entails any given conclusion. It is
shown that entailment in probability is closely related to Goldszmidt and
Pearl's System-Z^+, thereby demonstrating that the conclusions of System-Z^+
are probabilistically trustworthy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3556</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3556</id><created>2013-02-13</created><authors><author><keyname>Barrouil</keyname><forenames>Claude</forenames></author><author><keyname>Lemaire</keyname><forenames>Jerome</forenames></author></authors><title>Object Recognition with Imperfect Perception and Redundant Description</title><categories>cs.CV cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-65-72</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with a scene recognition system in a robotics contex. The
general problem is to match images with &lt;I&gt;a priori&lt;/I&gt; descriptions. A typical
mission would consist in identifying an object in an installation with a vision
system situated at the end of a manipulator and with a human operator provided
description, formulated in a pseudo-natural language, and possibly redundant.
The originality of this work comes from the nature of the description, from the
special attention given to the management of imprecision and uncertainty in the
interpretation process and from the way to assess the description redundancy so
as to reinforce the overall matching likelihood.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3557</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3557</id><created>2013-02-13</created><authors><author><keyname>Bauer</keyname><forenames>Mathias</forenames></author></authors><title>Approximations for Decision Making in the Dempster-Shafer Theory of
  Evidence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-73-80</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computational complexity of reasoning within the Dempster-Shafer theory
of evidence is one of the main points of criticism this formalism has to face.
To overcome this difficulty various approximation algorithms have been
suggested that aim at reducing the number of focal elements in the belief
functions involved. Besides introducing a new algorithm using this method, this
paper describes an empirical study that examines the appropriateness of these
approximation procedures in decision making situations. It presents the
empirical findings and discusses the various tradeoffs that have to be taken
into account when actually applying one of these methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3558</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3558</id><created>2013-02-13</created><authors><author><keyname>Becker</keyname><forenames>Ann</forenames></author><author><keyname>Geiger</keyname><forenames>Dan</forenames></author></authors><title>A Sufficiently Fast Algorithm for Finding Close to Optimal Junction
  Trees</title><categories>cs.DS cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-81-89</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm is developed for finding a close to optimal junction tree of a
given graph G. The algorithm has a worst case complexity O(c^k n^a) where a and
c are constants, n is the number of vertices, and k is the size of the largest
clique in a junction tree of G in which this size is minimized. The algorithm
guarantees that the logarithm of the size of the state space of the heaviest
clique in the junction tree produced is less than a constant factor off the
optimal value. When k = O(log n), our algorithm yields a polynomial inference
algorithm for Bayesian networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3559</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3559</id><created>2013-02-13</created><authors><author><keyname>Benferhat</keyname><forenames>Salem</forenames></author><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>Coping with the Limitations of Rational Inference in the Framework of
  Possibility Theory</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-90-97</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Possibility theory offers a framework where both Lehmann's &quot;preferential
inference&quot; and the more productive (but less cautious) &quot;rational closure
inference&quot; can be represented. However, there are situations where the second
inference does not provide expected results either because it cannot produce
them, or even provide counter-intuitive conclusions. This state of facts is not
due to the principle of selecting a unique ordering of interpretations (which
can be encoded by one possibility distribution), but rather to the absence of
constraints expressing pieces of knowledge we have implicitly in mind. It is
advocated in this paper that constraints induced by independence information
can help finding the right ordering of interpretations. In particular,
independence constraints can be systematically assumed with respect to formulas
composed of literals which do not appear in the conditional knowledge base, or
for default rules with respect to situations which are &quot;normal&quot; according to
the other default rules in the base. The notion of independence which is used
can be easily expressed in the qualitative setting of possibility theory.
Moreover, when a counter-intuitive plausible conclusion of a set of defaults,
is in its rational closure, but not in its preferential closure, it is always
possible to repair the set of defaults so as to produce the desired conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3560</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3560</id><created>2013-02-13</created><authors><author><keyname>Bonet</keyname><forenames>Blai</forenames></author><author><keyname>Geffner</keyname><forenames>Hector</forenames></author></authors><title>Arguing for Decisions: A Qualitative Model of Decision Making</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-98-105</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a qualitative model of decision making with two aims: to describe
how people make simple decisions and to enable computer programs to do the
same. Current approaches based on Planning or Decisions Theory either ignore
uncertainty and tradeoffs, or provide languages and algorithms that are too
complex for this task. The proposed model provides a language based on rules, a
semantics based on high probabilities and lexicographical preferences, and a
transparent decision procedure where reasons for and against decisions
interact. The model is no substitude for Decision Theory, yet for decisions
that people find easy to explain it may provide an appealing alternative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3561</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3561</id><created>2013-02-13</created><authors><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author></authors><title>Learning Conventions in Multiagent Stochastic Domains using Likelihood
  Estimates</title><categories>cs.GT cs.MA</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-106-114</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fully cooperative multiagent systems - those in which agents share a joint
utility model- is of special interest in AI. A key problem is that of ensuring
that the actions of individual agents are coordinated, especially in settings
where the agents are autonomous decision makers. We investigate approaches to
learning coordinated strategies in stochastic domains where an agent's actions
are not directly observable by others. Much recent work in game theory has
adopted a Bayesian learning perspective to the more general problem of
equilibrium selection, but tends to assume that actions can be observed. We
discuss the special problems that arise when actions are not observable,
including effects on rates of convergence, and the effect of action failure
probabilities and asymmetries. We also use likelihood estimates as a means of
generalizing fictitious play learning models in our setting. Finally, we
propose the use of maximum likelihood as a means of removing strategies from
consideration, with the aim of convergence to a conventional equilibrium, at
which point learning and deliberation can cease.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3562</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3562</id><created>2013-02-13</created><authors><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Goldszmidt</keyname><forenames>Moises</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Context-Specific Independence in Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-115-123</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian networks provide a language for qualitatively representing the
conditional independence properties of a distribution. This allows a natural
and compact representation of the distribution, eases knowledge acquisition,
and supports effective inference algorithms. It is well-known, however, that
there are certain independencies that we cannot capture qualitatively within
the Bayesian network structure: independencies that hold only in certain
contexts, i.e., given a specific assignment of values to certain variables. In
this paper, we propose a formal notion of context-specific independence (CSI),
based on regularities in the conditional probability tables (CPTs) at a node.
We present a technique, analogous to (and based on) d-separation, for
determining when such independence holds in a given network. We then focus on a
particular qualitative representation scheme - tree-structured CPTs - for
capturing CSI. We suggest ways in which this representation can be used to
support effective inference algorithms. In particular, we present a structural
decomposition of the resulting network which can improve the performance of
clustering algorithms, and an alternative algorithm based on cutset
conditioning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3563</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3563</id><created>2013-02-13</created><updated>2015-05-17</updated><authors><author><keyname>Breese</keyname><forenames>John S.</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Decision-Theoretic Troubleshooting: A Framework for Repair and
  Experiment</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1996-PG-124-132</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop and extend existing decision-theoretic methods for troubleshooting
a nonfunctioning device. Traditionally, diagnosis with Bayesian networks has
focused on belief updating---determining the probabilities of various faults
given current observations. In this paper, we extend this paradigm to include
taking actions. In particular, we consider three classes of actions: (1) we can
make observations regarding the behavior of a device and infer likely faults as
in traditional diagnosis, (2) we can repair a component and then observe the
behavior of the device to infer likely faults, and (3) we can change the
configuration of the device, observe its new behavior, and infer the likelihood
of faults. Analysis of latter two classes of troubleshooting actions requires
incorporating notions of persistence into the belief-network formalism used for
probabilistic inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3564</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3564</id><created>2013-02-13</created><authors><author><keyname>Castillo</keyname><forenames>Enrique F.</forenames></author><author><keyname>Solares</keyname><forenames>Cristina</forenames></author><author><keyname>Gomez</keyname><forenames>Patricia</forenames></author></authors><title>Tail Sensitivity Analysis in Bayesian Networks</title><categories>cs.AI stat.AP</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-133-140</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an efficient method for simulating the tails of a target
variable Z=h(X) which depends on a set of basic variables X=(X_1, ..., X_n). To
this aim, variables X_i, i=1, ..., n are sequentially simulated in such a
manner that Z=h(x_1, ..., x_i-1, X_i, ..., X_n) is guaranteed to be in the tail
of Z. When this method is difficult to apply, an alternative method is
proposed, which leads to a low rejection proportion of sample values, when
compared with the Monte Carlo method. Both methods are shown to be very useful
to perform a sensitivity analysis of Bayesian networks, when very large
confidence intervals for the marginal/conditional probabilities are required,
as in reliability or risk analysis. The methods are shown to behave best when
all scores coincide. The required modifications for this to occur are
discussed. The methods are illustrated with several examples and one example of
application to a real case is used to illustrate the whole process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3565</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3565</id><created>2013-02-13</created><authors><author><keyname>Chavez</keyname><forenames>Tom</forenames></author></authors><title>Decision-Analytic Approaches to Operational Decision Making: Application
  and Observation</title><categories>cs.AI cs.CY</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-141-149</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision analysis (DA) and the rich set of tools developed by researchers in
decision making under uncertainty show great potential to penetrate the
technological content of the products and services delivered by firms in a
variety of industries as well as the business processes used to deliver those
products and services to market. In this paper I describe work in progress at
Sun Microsystems in the application of decision-analytic methods to Operational
Decision Making (ODM) in its World-Wide Operations (WWOPS) Business Management
Group. Working with membersof product engineering, marketing, and sales,
operations planners from WWOPS have begun to use a decision-analytic framework
called SCRAM (Supply Communication/Risk Assessment and Management) to structure
and solve problems in product planning, tracking, and transition. Concepts such
as information value provide a powerful method of managing huge information
sets and thereby enable managers to focus attention on factors that matter most
for their business. Finally, our process-oriented introduction of
decision-analytic methods to Sun managers has led to a focused effort to
develop decision support software based on methods from decision making under
uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3566</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3566</id><created>2013-02-13</created><authors><author><keyname>Chickering</keyname><forenames>David Maxwell</forenames></author></authors><title>Learning Equivalence Classes of Bayesian Networks Structures</title><categories>cs.AI cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-150-157</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approaches to learning Bayesian networks from data typically combine a
scoring function with a heuristic search procedure. Given a Bayesian network
structure, many of the scoring functions derived in the literature return a
score for the entire equivalence class to which the structure belongs. When
using such a scoring function, it is appropriate for the heuristic search
algorithm to search over equivalence classes of Bayesian networks as opposed to
individual structures. We present the general formulation of a search space for
which the states of the search correspond to equivalence classes of structures.
Using this space, any one of a number of heuristic search algorithms can easily
be applied. We compare greedy search performance in the proposed search space
to greedy search performance in a search space for which the states correspond
to individual Bayesian network structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3567</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3567</id><created>2013-02-13</created><updated>2015-05-16</updated><authors><author><keyname>Chickering</keyname><forenames>David Maxwell</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Efficient Approximations for the Marginal Likelihood of Incomplete Data
  Given a Bayesian Network</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1996-PG-158-168</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss Bayesian methods for learning Bayesian networks when data sets are
incomplete. In particular, we examine asymptotic approximations for the
marginal likelihood of incomplete data given a Bayesian network. We consider
the Laplace approximation and the less accurate but more efficient BIC/MDL
approximation. We also consider approximations proposed by Draper (1993) and
Cheeseman and Stutz (1995). These approximations are as efficient as BIC/MDL,
but their accuracy has not been studied in any depth. We compare the accuracy
of these approximations under the assumption that the Laplace approximation is
the most accurate. In experiments using synthetic data generated from discrete
naive-Bayes models having a hidden root node, we find that the CS measure is
the most accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3568</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3568</id><created>2013-02-13</created><authors><author><keyname>Chrisman</keyname><forenames>Lonnie</forenames></author></authors><title>Independence with Lower and Upper Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-169-177</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that the ability of the interval probability representation to
capture epistemological independence is severely limited. Two events are
epistemologically independent if knowledge of the first event does not alter
belief (i.e., probability bounds) about the second. However, independence in
this form can only exist in a 2-monotone probability function in degenerate
cases i.e., if the prior bounds are either point probabilities or entirely
vacuous. Additional limitations are characterized for other classes of lower
probabilities as well. It is argued that these phenomena are simply a matter of
interpretation. They appear to be limitations when one interprets probability
bounds as a measure of epistemological indeterminacy (i.e., uncertainty arising
from a lack of knowledge), but are exactly as one would expect when probability
intervals are interpreted as representations of ontological indeterminacy
(indeterminacy introduced by structural approximations). The ontological
interpretation is introduced and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3569</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3569</id><created>2013-02-13</created><authors><author><keyname>Chrisman</keyname><forenames>Lonnie</forenames></author></authors><title>Propagation of 2-Monotone Lower Probabilities on an Undirected Graph</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-178-185</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lower and upper probabilities, also known as Choquet capacities, are widely
used as a convenient representation for sets of probability distributions. This
paper presents a graphical decomposition and exact propagation algorithm for
computing marginal posteriors of 2-monotone lower probabilities (equivalently,
2-alternating upper probabilities).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3570</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3570</id><created>2013-02-13</created><authors><author><keyname>Cozman</keyname><forenames>Fabio Gagliardi</forenames></author><author><keyname>Krotkov</keyname><forenames>Eric</forenames></author></authors><title>Quasi-Bayesian Strategies for Efficient Plan Generation: Application to
  the Planning to Observe Problem</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-186-193</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quasi-Bayesian theory uses convex sets of probability distributions and
expected loss to represent preferences about plans. The theory focuses on
decision robustness, i.e., the extent to which plans are affected by deviations
in subjective assessments of probability. The present work presents solutions
for plan generation when robustness of probability assessments must be
included: plans contain information about the robustness of certain actions.
The surprising result is that some problems can be solved faster in the
Quasi-Bayesian framework than within usual Bayesian theory. We investigate this
on the planning to observe problem, i.e., an agent must decide whether to take
new observations or not. The fundamental question is: How, and how much, to
search for a ?best? plan, based on the robustness of probability assessments?
Plan generation algorithms are derived in the context of material
classification with an acoustic robotic probe. A package that constructs
Quasi-Bayesian plans is available through anonymous ftp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3571</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3571</id><created>2013-02-13</created><authors><author><keyname>D'Ambrosio</keyname><forenames>Bruce</forenames></author><author><keyname>Burgess</keyname><forenames>Scott</forenames></author></authors><title>Some Experiments with Real-Time Decision Algorithms</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-194-202</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-time Decision algorithms are a class of incremental resource-bounded
[Horvitz, 89] or anytime [Dean, 93] algorithms for evaluating influence
diagrams. We present a test domain for real-time decision algorithms, and the
results of experiments with several Real-time Decision Algorithms in this
domain. The results demonstrate high performance for two algorithms, a
decision-evaluation variant of Incremental Probabilisitic Inference [D'Ambrosio
93] and a variant of an algorithm suggested by Goldszmidt, [Goldszmidt, 95],
PK-reduced. We discuss the implications of these experimental results and
explore the broader applicability of these algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3572</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3572</id><created>2013-02-13</created><authors><author><keyname>Dechter</keyname><forenames>Rina</forenames></author></authors><title>Bucket Elimination: A Unifying Framework for Several Probabilistic
  Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-211-219</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic inference algorithms for finding the most probable explanation,
the maximum aposteriori hypothesis, and the maximum expected utility and for
updating belief are reformulated as an elimination--type algorithm called
bucket elimination. This emphasizes the principle common to many of the
algorithms appearing in that literature and clarifies their relationship to
nonserial dynamic programming algorithms. We also present a general way of
combining conditioning and elimination within this framework. Bounds on
complexity are given for all the algorithms as a function of the problem's
structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3573</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3573</id><created>2013-02-13</created><authors><author><keyname>Dechter</keyname><forenames>Rina</forenames></author></authors><title>Topological Parameters for Time-Space Tradeoff</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-220-227</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a family of algorithms combining tree-clustering
with conditioning that trade space for time. Such algorithms are useful for
reasoning in probabilistic and deterministic networks as well as for
accomplishing optimization tasks. By analyzing the problem structure it will be
possible to select from a spectrum the algorithm that best meets a given
time-space specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3574</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3574</id><created>2013-02-13</created><authors><author><keyname>Doan</keyname><forenames>AnHai</forenames></author><author><keyname>Haddawy</keyname><forenames>Peter</forenames></author></authors><title>Sound Abstraction of Probabilistic Actions in The Constraint Mass
  Assignment Framework</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-228-235</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a formal and practical framework for sound abstraction of
probabilistic actions. We start by precisely defining the concept of sound
abstraction within the context of finite-horizon planning (where each plan is a
finite sequence of actions). Next we show that such abstraction cannot be
performed within the traditional probabilistic action representation, which
models a world with a single probability distribution over the state space. We
then present the constraint mass assignment representation, which models the
world with a set of probability distributions and is a generalization of mass
assignment representations. Within this framework, we present sound abstraction
procedures for three types of action abstraction. We end the paper with
discussions and related work on sound and approximate abstraction. We give
pointers to papers in which we discuss other sound abstraction-related issues,
including applications, estimating loss due to abstraction, and automatically
generating abstraction hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3575</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3575</id><created>2013-02-13</created><authors><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>Belief Revision with Uncertain Inputs in the Possibilistic Setting</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-236-243</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses belief revision under uncertain inputs in the framework
of possibility theory. Revision can be based on two possible definitions of the
conditioning operation, one based on min operator which requires a purely
ordinal scale only, and another based on product, for which a richer structure
is needed, and which is a particular case of Dempster's rule of conditioning.
Besides, revision under uncertain inputs can be understood in two different
ways depending on whether the input is viewed, or not, as a constraint to
enforce. Moreover, it is shown that M.A. Williams' transmutations, originally
defined in the setting of Spohn's functions, can be captured in this framework,
as well as Boutilier's natural revision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3576</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3576</id><created>2013-02-13</created><authors><author><keyname>Fattah</keyname><forenames>Yousri El</forenames></author><author><keyname>Dechter</keyname><forenames>Rina</forenames></author></authors><title>An Evaluation of Structural Parameters for Probabilistic Reasoning:
  Results on Benchmark Circuits</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-244-251</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many algorithms for processing probabilistic networks are dependent on the
topological properties of the problem's structure. Such algorithms (e.g.,
clustering, conditioning) are effective only if the problem has a sparse graph
captured by parameters such as tree width and cycle-cut set size. In this paper
we initiate a study to determine the potential of structure-based algorithms in
real-life applications. We analyze empirically the structural properties of
problems coming from the circuit diagnosis domain. Specifically, we locate
those properties that capture the effectiveness of clustering and conditioning
as well as of a family of conditioning+clustering algorithms designed to
gradually trade space for time. We perform our analysis on 11 benchmark
circuits widely used in the testing community. We also report on the effect of
ordering heuristics on tree-clustering and show that, on our benchmarks, the
well-known max-cardinality ordering is substantially inferior to an ordering
called min-degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3577</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3577</id><created>2013-02-13</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Goldszmidt</keyname><forenames>Moises</forenames></author></authors><title>Learning Bayesian Networks with Local Structure</title><categories>cs.AI cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-252-262</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we examine a novel addition to the known methods for learning
Bayesian networks from data that improves the quality of the learned networks.
Our approach explicitly represents and learns the local structure in the
conditional probability tables (CPTs), that quantify these networks. This
increases the space of possible models, enabling the representation of CPTs
with a variable number of parameters that depends on the learned local
structures. The resulting learning procedure is capable of inducing models that
better emulate the real complexity of the interactions present in the data. We
describe the theoretical foundations and practical aspects of learning local
structures, as well as an empirical evaluation of the proposed method. This
evaluation indicates that learning curves characterizing the procedure that
exploits the local structure converge faster than these of the standard
procedure. Our results also show that networks learned with local structure
tend to be more complex (in terms of arcs), yet require less parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3578</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3578</id><created>2013-02-13</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>A Qualitative Markov Assumption and its Implications for Belief Change</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-263-273</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of belief change has been an active area in philosophy and AI. In
recent years two special cases of belief change, belief revision and belief
update, have been studied in detail. Roughly, revision treats a surprising
observation as a sign that previous beliefs were wrong, while update treats a
surprising observation as an indication that the world has changed. In general,
we would expect that an agent making an observation may both want to revise
some earlier beliefs and assume that some change has occurred in the world. We
define a novel approach to belief change that allows us to do this, by applying
ideas from probability theory in a qualitative setting. The key idea is to use
a qualitative Markov assumption, which says that state transitions are
independent. We show that a recent approach to modeling qualitative uncertainty
using plausibility measures allows us to make such a qualitative Markov
assumption in a relatively straightforward way, and show how the Markov
assumption can be used to provide an attractive belief-change model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3579</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3579</id><created>2013-02-13</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Yakhini</keyname><forenames>Zohar</forenames></author></authors><title>On the Sample Complexity of Learning Bayesian Networks</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-274-282</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years there has been an increasing interest in learning Bayesian
networks from data. One of the most effective methods for learning such
networks is based on the minimum description length (MDL) principle. Previous
work has shown that this learning procedure is asymptotically successful: with
probability one, it will converge to the target distribution, given a
sufficient number of samples. However, the rate of this convergence has been
hitherto unknown. In this work we examine the sample complexity of MDL based
learning procedures for Bayesian networks. We show that the number of samples
needed to learn an epsilon-close approximation (in terms of entropy distance)
with confidence delta is O((1/epsilon)^(4/3)log(1/epsilon)log(1/delta)loglog
(1/delta)). This means that the sample complexity is a low-order polynomial in
the error threshold and sub-linear in the confidence bound. We also discuss how
the constants in this term depend on the complexity of the target distribution.
Finally, we address questions of asymptotic minimality and propose a method for
using the sample complexity results to speed up the learning process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3580</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3580</id><created>2013-02-13</created><updated>2015-05-16</updated><authors><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author></authors><title>Asymptotic Model Selection for Directed Networks with Hidden Variables</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1996-PG-283-290</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the Bayesian Information Criterion (BIC), an asymptotic
approximation for the marginal likelihood, to Bayesian networks with hidden
variables. This approximation can be used to select models given large samples
of data. The standard BIC as well as our extension punishes the complexity of a
model according to the dimension of its parameters. We argue that the dimension
of a Bayesian network with hidden variables is the rank of the Jacobian matrix
of the transformation between the parameters of the network and the parameters
of the observable variables. We compute the dimensions of several networks
including the naive Bayes model with a hidden root node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3581</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3581</id><created>2013-02-13</created><authors><author><keyname>Ha</keyname><forenames>Vu A.</forenames></author><author><keyname>Haddawy</keyname><forenames>Peter</forenames></author></authors><title>Theoretical Foundations for Abstraction-Based Probabilistic Planning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-291-298</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling worlds and actions under uncertainty is one of the central problems
in the framework of decision-theoretic planning. The representation must be
general enough to capture real-world problems but at the same time it must
provide a basis upon which theoretical results can be derived. The central
notion in the framework we propose here is that of the affine-operator, which
serves as a tool for constructing (convex) sets of probability distributions,
and which can be considered as a generalization of belief functions and
interval mass assignments. Uncertainty in the state of the worlds is modeled
with sets of probability distributions, represented by affine-trees while
actions are defined as tree-manipulators. A small set of key properties of the
affine-operator is presented, forming the basis for most existing
operator-based definitions of probabilistic action projection and action
abstraction. We derive and prove correct three projection rules, which vividly
illustrate the precision-complexity tradeoff in plan projection. Finally, we
show how the three types of action abstraction identified by Haddawy and Doan
are manifested in the present framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3582</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3582</id><created>2013-02-13</created><authors><author><keyname>Henrion</keyname><forenames>Max</forenames></author><author><keyname>Pradhan</keyname><forenames>Malcolm</forenames></author><author><keyname>del Favero</keyname><forenames>Brendan</forenames></author><author><keyname>Huang</keyname><forenames>Kurt</forenames></author><author><keyname>Provan</keyname><forenames>Gregory M.</forenames></author><author><keyname>O'Rorke</keyname><forenames>Paul</forenames></author></authors><title>Why Is Diagnosis Using Belief Networks Insensitive to Imprecision In
  Probabilities?</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-307-314</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has found that diagnostic performance with Bayesian belief
networks is often surprisingly insensitive to imprecision in the numerical
probabilities. For example, the authors have recently completed an extensive
study in which they applied random noise to the numerical probabilities in a
set of belief networks for medical diagnosis, subsets of the CPCS network, a
subset of the QMR (Quick Medical Reference) focused on liver and bile diseases.
The diagnostic performance in terms of the average probabilities assigned to
the actual diseases showed small sensitivity even to large amounts of noise. In
this paper, we summarize the findings of this study and discuss possible
explanations of this low sensitivity. One reason is that the criterion for
performance is average probability of the true hypotheses, rather than average
error in probability, which is insensitive to symmetric noise distributions.
But, we show that even asymmetric, logodds-normal noise has modest effects. A
second reason is that the gold-standard posterior probabilities are often near
zero or one, and are little disturbed by noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3583</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3583</id><created>2013-02-13</created><authors><author><keyname>Horsch</keyname><forenames>Michael C.</forenames></author><author><keyname>Poole</keyname><forenames>David L.</forenames></author></authors><title>Flexible Policy Construction by Information Refinement</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-315-324</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on work towards flexible algorithms for solving decision problems
represented as influence diagrams. An algorithm is given to construct a tree
structure for each decision node in an influence diagram. Each tree represents
a decision function and is constructed incrementally. The improvements to the
tree converge to the optimal decision function (neglecting computational costs)
and the asymptotic behaviour is only a constant factor worse than dynamic
programming techniques, counting the number of Bayesian network queries.
Empirical results show how expected utility increases with the size of the tree
and the number of Bayesian net calculations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3584</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3584</id><created>2013-02-13</created><authors><author><keyname>Huang</keyname><forenames>Kurt</forenames></author><author><keyname>Henrion</keyname><forenames>Max</forenames></author></authors><title>Efficient Search-Based Inference for Noisy-OR Belief Networks:
  TopEpsilon</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-325-331</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference algorithms for arbitrary belief networks are impractical for large,
complex belief networks. Inference algorithms for specialized classes of belief
networks have been shown to be more efficient. In this paper, we present a
search-based algorithm for approximate inference on arbitrary, noisy-OR belief
networks, generalizing earlier work on search-based inference for two-level,
noisy-OR belief networks. Initial experimental results appear promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3585</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3585</id><created>2013-02-13</created><authors><author><keyname>Ibarguengoytia</keyname><forenames>Pablo H.</forenames></author><author><keyname>Sucar</keyname><forenames>Luis Enrique</forenames></author><author><keyname>Vadera</keyname><forenames>Sunil</forenames></author></authors><title>A Probabilistic Model For Sensor Validation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-332-339</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The validation of data from sensors has become an important issue in the
operation and control of modern industrial plants. One approach is to use
knowledge based techniques to detect inconsistencies in measured data. This
article presents a probabilistic model for the detection of such
inconsistencies. Based on probability propagation, this method is able to find
the existence of a possible fault among the set of sensors. That is, if an
error exists, many sensors present an apparent fault due to the propagation
from the sensor(s) with a real fault. So the fault detection mechanism can only
tell if a sensor has a potential fault, but it can not tell if the fault is
real or apparent. So the central problem is to develop a theory, and then an
algorithm, for distinguishing real and apparent faults, given that one or more
sensors can fail at the same time. This article then, presents an approach
based on two levels: (i) probabilistic reasoning, to detect a potential fault,
and (ii) constraint management, to distinguish the real fault from the apparent
ones. The proposed approach is exemplified by applying it to a power plant
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3586</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3586</id><created>2013-02-13</created><authors><author><keyname>Jaakkola</keyname><forenames>Tommi S.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Computing Upper and Lower Bounds on Likelihoods in Intractable Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-340-348</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present deterministic techniques for computing upper and lower bounds on
marginal probabilities in sigmoid and noisy-OR networks. These techniques
become useful when the size of the network (or clique size) precludes exact
computations. We illustrate the tightness of the bounds by numerical
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3587</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3587</id><created>2013-02-13</created><authors><author><keyname>Jensen</keyname><forenames>Allan Leck</forenames></author><author><keyname>Jensen</keyname><forenames>Finn Verner</forenames></author></authors><title>MIDAS - An Influence Diagram for Management of Mildew in Winter Wheat</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-349-356</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a prototype of a decision support system for management of the
fungal disease mildew in winter wheat. The prototype is based on an influence
diagram which is used to determine the optimal time and dose of mildew
treatments. This involves multiple decision opportunities over time,
stochasticity, inaccurate information and incomplete knowledge. The paper
describes the practical and theoretical problems encountered during the
construction of the influence diagram, and also the experience with the
prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3588</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3588</id><created>2013-02-13</created><authors><author><keyname>Kozlov</keyname><forenames>Alexander V.</forenames></author><author><keyname>Singh</keyname><forenames>Jaswinder Pal</forenames></author></authors><title>Computational Complexity Reduction for BN2O Networks Using Similarity of
  States</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-357-364</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although probabilistic inference in a general Bayesian belief network is an
NP-hard problem, computation time for inference can be reduced in most
practical cases by exploiting domain knowledge and by making approximations in
the knowledge representation. In this paper we introduce the property of
similarity of states and a new method for approximate knowledge representation
and inference which is based on this property. We define two or more states of
a node to be similar when the ratio of their probabilities, the likelihood
ratio, does not depend on the instantiations of the other nodes in the network.
We show that the similarity of states exposes redundancies in the joint
probability distribution which can be exploited to reduce the computation time
of probabilistic inference in networks with multiple similar states, and that
the computational complexity in the networks with exponentially many similar
states might be polynomial. We demonstrate our ideas on the example of a BN2O
network?a two layer network often used in diagnostic problems?by reducing it to
a very close network with multiple similar states. We show that the answers to
practical queries converge very fast to the answers obtained with the original
network. The maximum error is as low as 5% for models that require only 10% of
the computation time needed by the original BN2O model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3589</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3589</id><created>2013-02-13</created><authors><author><keyname>Kyburg</keyname><forenames>Henry E.</forenames><suffix>Jr</suffix></author></authors><title>Uncertain Inferences and Uncertain Conclusions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-365-372</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uncertainty may be taken to characterize inferences, their conclusions, their
premises or all three. Under some treatments of uncertainty, the inferences
itself is never characterized by uncertainty. We explore both the significance
of uncertainty in the premises and in the conclusion of an argument that
involves uncertainty. We argue that for uncertainty to characterize the
conclusion of an inference is natural, but that there is an interplay between
uncertainty in the premises and uncertainty in the procedure of argument
itself. We show that it is possible in principle to incorporate all uncertainty
in the premises, rendering uncertainty arguments deductively valid. But we then
argue (1) that this does not reflect human argument, (2) that it is
computationally costly, and (3) that the gain in simplicity obtained by
allowing uncertainty inference can sometimes outweigh the loss of flexibility
it entails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3590</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3590</id><created>2013-02-13</created><authors><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author><author><keyname>Martignon</keyname><forenames>Laura</forenames></author></authors><title>Bayesian Learning of Loglinear Models for Neural Connectivity</title><categories>cs.LG q-bio.NC stat.AP stat.ML</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-373-380</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a Bayesian approach to learning the connectivity
structure of a group of neurons from data on configuration frequencies. A major
objective of the research is to provide statistical tools for detecting changes
in firing patterns with changing stimuli. Our framework is not restricted to
the well-understood case of pair interactions, but generalizes the Boltzmann
machine model to allow for higher order interactions. The paper applies a
Markov Chain Monte Carlo Model Composition (MC3) algorithm to search over
connectivity structures and uses Laplace's method to approximate posterior
probabilities of structures. Performance of the methods was tested on synthetic
data. The models were also applied to data obtained by Vaadia on multi-unit
recordings of several neurons in the visual cortex of a rhesus monkey in two
different attentional states. Results confirmed the experimenters' conjecture
that different attentional states were associated with different interaction
structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3591</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3591</id><created>2013-02-13</created><authors><author><keyname>Mahoney</keyname><forenames>Suzanne M.</forenames></author><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author></authors><title>Network Engineering for Complex Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-389-396</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Like any large system development effort, the construction of a complex
belief network model requires systems engineering to manage the design and
construction process. We propose a rapid prototyping approach to network
engineering. We describe criteria for identifying network modules and the use
of &quot;stubs&quot; to represent not-yet-constructed modules. We propose an object
oriented representation for belief networks which captures the semantics of the
problem in addition to conditional independencies and probabilities. Methods
for evaluating complex belief network models are discussed. The ideas are
illustrated with examples from a large belief network construction problem in
the military intelligence domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3592</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3592</id><created>2013-02-13</created><authors><author><keyname>Ngo</keyname><forenames>Liem</forenames></author></authors><title>Probabilistic Disjunctive Logic Programming</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-397-404</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a framework for combining Disjunctive Logic
Programming and Poole's Probabilistic Horn Abduction. We use the concept of
hypothesis to specify the probability structure. We consider the case in which
probabilistic information is not available. Instead of using probability
intervals, we allow for the specification of the probabilities of disjunctions.
Because minimal models are used as characteristic models in disjunctive logic
programming, we apply the principle of indifference on the set of minimal
models to derive default probability values. We define the concepts of
explanation and partial explanation of a formula, and use them to determine the
default probability distribution(s) induced by a program. An algorithm for
calculating the default probability of a goal is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3593</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3593</id><created>2013-02-13</created><authors><author><keyname>Pennock</keyname><forenames>David M.</forenames></author><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author></authors><title>Toward a Market Model for Bayesian Inference</title><categories>cs.GT cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-405-413</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a methodology for representing probabilistic relationships in a
general-equilibrium economic model. Specifically, we define a precise mapping
from a Bayesian network with binary nodes to a market price system where
consumers and producers trade in uncertain propositions. We demonstrate the
correspondence between the equilibrium prices of goods in this economy and the
probabilities represented by the Bayesian network. A computational market model
such as this may provide a useful framework for investigations of belief
aggregation, distributed probabilistic inference, resource allocation under
uncertainty, and other problems of decentralized uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3594</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3594</id><created>2013-02-13</created><authors><author><keyname>Peot</keyname><forenames>Mark Alan</forenames></author></authors><title>Geometric Implications of the Naive Bayes Assumption</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-414-419</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A naive (or Idiot) Bayes network is a network with a single hypothesis node
and several observations that are conditionally independent given the
hypothesis. We recently surveyed a number of members of the UAI community and
discovered a general lack of understanding of the implications of the Naive
Bayes assumption on the kinds of problems that can be solved by these networks.
It has long been recognized [Minsky 61] that if observations are binary, the
decision surfaces in these networks are hyperplanes. We extend this result
(hyperplane separability) to Naive Bayes networks with m-ary observations. In
addition, we illustrate the effect of observation-observation dependencies on
decision surfaces. Finally, we discuss the implications of these results on
knowledge acquisition and research in learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3595</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3595</id><created>2013-02-13</created><authors><author><keyname>Pearl</keyname><forenames>Judea</forenames></author><author><keyname>Dechter</keyname><forenames>Rina</forenames></author></authors><title>Identifying Independencies in Causal Graphs with Feedback</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-420-426</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the d -separation criterion constitutes a valid test for
conditional independence relationships that are induced by feedback systems
involving discrete variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3596</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3596</id><created>2013-02-13</created><authors><author><keyname>Poh</keyname><forenames>Kim-Leng</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author></authors><title>A Graph-Theoretic Analysis of Information Value</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-427-435</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive qualitative relationships about the informational relevance of
variables in graphical decision models based on a consideration of the topology
of the models. Specifically, we identify dominance relations for the expected
value of information on chance variables in terms of their position and
relationships in influence diagrams. The qualitative relationships can be
harnessed to generate nonnumerical procedures for ordering uncertain variables
in a decision model by their informational relevance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3597</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3597</id><created>2013-02-13</created><authors><author><keyname>Poole</keyname><forenames>David L.</forenames></author></authors><title>A Framework for Decision-Theoretic Planning I: Combining the Situation
  Calculus, Conditional Plans, Probability and Utility</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-436-445</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows how we can combine logical representations of actions and
decision theory in such a manner that seems natural for both. In particular we
assume an axiomatization of the domain in terms of situation calculus, using
what is essentially Reiter's solution to the frame problem, in terms of the
completion of the axioms defining the state change. Uncertainty is handled in
terms of the independent choice logic, which allows for independent choices and
a logic program that gives the consequences of the choices. As part of the
consequences are a specification of the utility of (final) states. The robot
adopts robot plans, similar to the GOLOG programming language. Within this
logic, we can define the expected utility of a conditional plan, based on the
axiomatization of the actions, the uncertainty and the utility. The ?planning'
problem is to find the plan with the highest expected utility. This is related
to recent structured representations for POMDPs; here we use stochastic
situation calculus rules to specify the state transition function and the
reward/value function. Finally we show that with stochastic frame axioms,
actions representations in probabilistic STRIPS are exponentially larger than
using the representation proposed here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3598</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3598</id><created>2013-02-13</created><authors><author><keyname>Pradhan</keyname><forenames>Malcolm</forenames></author><author><keyname>Dagum</keyname><forenames>Paul</forenames></author></authors><title>Optimal Monte Carlo Estimation of Belief Network Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-446-453</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two Monte Carlo sampling algorithms for probabilistic inference
that guarantee polynomial-time convergence for a larger class of network than
current sampling algorithms provide. These new methods are variants of the
known likelihood weighting algorithm. We use of recent advances in the theory
of optimal stopping rules for Monte Carlo simulation to obtain an inference
approximation with relative error epsilon and a small failure probability
delta. We present an empirical evaluation of the algorithms which demonstrates
their improved performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3599</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3599</id><created>2013-02-13</created><authors><author><keyname>Richardson</keyname><forenames>Thomas S.</forenames></author></authors><title>A Discovery Algorithm for Directed Cyclis Graphs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-454-461</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Directed acyclic graphs have been used fruitfully to represent causal
strucures (Pearl 1988). However, in the social sciences and elsewhere models
are often used which correspond both causally and statistically to directed
graphs with directed cycles (Spirtes 1995). Pearl (1993) discussed predicting
the effects of intervention in models of this kind, so-called linear
non-recursive structural equation models. This raises the question of whether
it is possible to make inferences about causal structure with cycles, form
sample data. In particular do there exist general, informative, feasible and
reliable precedures for inferring causal structure from conditional
independence relations among variables in a sample generated by an unknown
causal structure? In this paper I present a discovery algorithm that is correct
in the large sample limit, given commonly (but often implicitly) made plausible
assumptions, and which provides information about the existence or
non-existence of causal pathways from one variable to another. The algorithm is
polynomial on sparse graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3600</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3600</id><created>2013-02-13</created><authors><author><keyname>Richardson</keyname><forenames>Thomas S.</forenames></author></authors><title>A Polynomial-Time Algorithm for Deciding Markov Equivalence of Directed
  Cyclic Graphical Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-462-469</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the concept of d-separation was originally defined for directed
acyclic graphs (see Pearl 1988), there is a natural extension of he concept to
directed cyclic graphs. When exactly the same set of d-separation relations
hold in two directed graphs, no matter whether respectively cyclic or acyclic,
we say that they are Markov equivalent. In other words, when two directed
cyclic graphs are Markov equivalent, the set of distributions that satisfy a
natural extension of the Global Directed Markov condition (Lauritzen et al.
1990) is exactly the same for each graph. There is an obvious exponential (in
the number of vertices) time algorithm for deciding Markov equivalence of two
directed cyclic graphs; simply chech all of the d-separation relations in each
graph. In this paper I state a theorem that gives necessary and sufficient
conditions for the Markov equivalence of two directed cyclic graphs, where each
of the conditions can be checked in polynomial time. Hence, the theorem can be
easily adapted into a polynomial time algorithm for deciding the Markov
equivalence of two directed cyclic graphs. Although space prohibits inclusion
of correctness proofs, they are fully described in Richardson (1994b).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3601</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3601</id><created>2013-02-13</created><authors><author><keyname>Roedder</keyname><forenames>Wilhelm</forenames></author><author><keyname>Meyer</keyname><forenames>Carl-Heinz</forenames></author></authors><title>Coherent Knowledge Processing at Maximum Entropy by SPIRIT</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-470-476</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SPIRIT is an expert system shell for probabilistic knowledge bases. Knowledge
acquisition is performed by processing facts and rules on discrete variables in
a rich syntax. The shell generates a probability distribution which respects
all acquired facts and rules and which maximizes entropy. The user-friendly
devices of SPIRIT to define variables, formulate rules and create the knowledge
base are revealed in detail. Inductive learning is possible. Medium sized
applications show the power of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3602</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3602</id><created>2013-02-13</created><authors><author><keyname>Santos</keyname><forenames>Eugene</forenames><suffix>Jr.</suffix></author><author><keyname>Shimony</keyname><forenames>Solomon Eyal</forenames></author><author><keyname>Williams</keyname><forenames>Edward</forenames></author></authors><title>Sample-and-Accumulate Algorithms for Belief Updating in Bayes Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-477-484</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Belief updating in Bayes nets, a well known computationally hard problem, has
recently been approximated by several deterministic algorithms, and by various
randomized approximation algorithms. Deterministic algorithms usually provide
probability bounds, but have an exponential runtime. Some randomized schemes
have a polynomial runtime, but provide only probability estimates. We present
randomized algorithms that enumerate high-probability partial instantiations,
resulting in probability bounds. Some of these algorithms are also sampling
algorithms. Specifically, we introduce and evaluate a variant of backward
sampling, both as a sampling algorithm and as a randomized enumeration
algorithm. We also relax the implicit assumption used by both sampling and
accumulation algorithms, that query nodes must be instantiated in all the
samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3603</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3603</id><created>2013-02-13</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author><author><keyname>Mandelbaum</keyname><forenames>Marvin</forenames></author></authors><title>A Measure of Decision Flexibility</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-485-491</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a decision-analytical approach to comparing the flexibility of
decision situations from the perspective of a decision-maker who exhibits
constant risk-aversion over a monetary value model. Our approach is simple yet
seems to be consistent with a variety of flexibility concepts, including robust
and adaptive alternatives. We try to compensate within the model for
uncertainty that was not anticipated or not modeled. This approach not only
allows one to compare the flexibility of plans, but also guides the search for
new, more flexible alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3604</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3604</id><created>2013-02-13</created><authors><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author></authors><title>Binary Join Trees</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-492-499</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main goal of this paper is to describe a data structure called binary
join trees that are useful in computing multiple marginals efficiently using
the Shenoy-Shafer architecture. We define binary join trees, describe their
utility, and sketch a procedure for constructing them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3605</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3605</id><created>2013-02-13</created><authors><author><keyname>Srinivas</keyname><forenames>Sampath</forenames></author><author><keyname>Nayak</keyname><forenames>Pandurang</forenames></author></authors><title>Efficient Enumeration of Instantiations in Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-500-508</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past several years Bayesian networks have been applied to a wide
variety of problems. A central problem in applying Bayesian networks is that of
finding one or more of the most probable instantiations of a network. In this
paper we develop an efficient algorithm that incrementally enumerates the
instantiations of a Bayesian network in decreasing order of probability. Such
enumeration algorithms are applicable in a variety of applications ranging from
medical expert systems to model-based diagnosis. Fundamentally, our algorithm
is simply performing a lazy enumeration of the sorted list of all
instantiations of the network. This insight leads to a very concise algorithm
statement which is both easily understood and implemented. We show that for
singly connected networks, our algorithm generates the next instantiation in
time polynomial in the size of the network. The algorithm extends to arbitrary
Bayesian networks using standard conditioning techniques. We empirically
evaluate the enumeration algorithm and demonstrate its practicality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3606</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3606</id><created>2013-02-13</created><authors><author><keyname>Studeny</keyname><forenames>Milan</forenames></author></authors><title>On Separation Criterion and Recovery Algorithm for Chain Graphs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-509-516</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chain graphs give a natural unifying point of view on Markov and Bayesian
networks and enlarge the potential of graphical models for description of
conditional independence structures. In the paper a direct graphical separation
criterion for chain graphs, called c-separation, which generalizes the
d-separation criterion for Bayesian networks is introduced (recalled). It is
equivalent to the classic moralization criterion for chain graphs and complete
in sense that for every chain graph there exists a probability distribution
satisfying exactly conditional independencies derivable from the chain graph by
the c-separation criterion. Every class of Markov equivalent chain graphs can
be uniquely described by a natural representative, called the largest chain
graph. A recovery algorithm, which on basis of the (conditional) dependency
model induced by an unknown chain graph finds the corresponding largest chain
graph, is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3607</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3607</id><created>2013-02-13</created><authors><author><keyname>Teng</keyname><forenames>Choh Man</forenames></author></authors><title>Possible World Partition Sequences: A Unifying Framework for Uncertain
  Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-517-524</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When we work with information from multiple sources, the formalism each
employs to handle uncertainty may not be uniform. In order to be able to
combine these knowledge bases of different formats, we need to first establish
a common basis for characterizing and evaluating the different formalisms, and
provide a semantics for the combined mechanism. A common framework can provide
an infrastructure for building an integrated system, and is essential if we are
to understand its behavior. We present a unifying framework based on an ordered
partition of possible worlds called partition sequences, which corresponds to
our intuitive notion of biasing towards certain possible scenarios when we are
uncertain of the actual situation. We show that some of the existing
formalisms, namely, default logic, autoepistemic logic, probabilistic
conditioning and thresholding (generalized conditioning), and possibility
theory can be incorporated into this general framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3608</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3608</id><created>2013-02-13</created><authors><author><keyname>Thiebaux</keyname><forenames>Sylvie</forenames></author><author><keyname>Cordier</keyname><forenames>Marie-Odile</forenames></author><author><keyname>Jehl</keyname><forenames>Olivier</forenames></author><author><keyname>Krivine</keyname><forenames>Jean-Paul</forenames></author></authors><title>Supply Restoration in Power Distribution Systems - A Case Study in
  Integrating Model-Based Diagnosis and Repair Planning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-525-532</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrating diagnosis and repair is particularly crucial when gaining
sufficient information to discriminate between several candidate diagnoses
requires carrying out some repair actions. A typical case is supply restoration
in a faulty power distribution system. This problem, which is a major concern
for electricity distributors, features partial observability, and stochastic
repair actions which are more elaborate than simple replacement of components.
This paper analyses the difficulties in applying existing work on integrating
model-based diagnosis and repair and on planning in partially observable
stochastic domains to this real-world problem, and describes the pragmatic
approach we have retained so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3609</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3609</id><created>2013-02-13</created><authors><author><keyname>Welch</keyname><forenames>Robert L.</forenames></author></authors><title>Real Time Estimation of Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-533-544</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For real time evaluation of a Bayesian network when there is not sufficient
time to obtain an exact solution, a guaranteed response time, approximate
solution is required. It is shown that nontraditional methods utilizing
estimators based on an archive of trial solutions and genetic search can
provide an approximate solution that is considerably superior to the
traditional Monte Carlo simulation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3610</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3610</id><created>2013-02-13</created><authors><author><keyname>Wong</keyname><forenames>Michael S. K. M.</forenames></author></authors><title>Testing Implication of Probabilistic Dependencies</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-545-553</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Axiomatization has been widely used for testing logical implications. This
paper suggests a non-axiomatic method, the chase, to test if a new dependency
follows from a given set of probabilistic dependencies. Although the chase
computation may require exponential time in some cases, this technique is a
powerful tool for establishing nontrivial theoretical results. More
importantly, this approach provides valuable insight into the intriguing
connection between relational databases and probabilistic reasoning systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3611</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3611</id><created>2013-02-13</created><authors><author><keyname>Wurman</keyname><forenames>Peter R.</forenames></author><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author></authors><title>Optimal Factory Scheduling using Stochastic Dominance A*</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-554-563</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine a standard factory scheduling problem with stochastic processing
and setup times, minimizing the expectation of the weighted number of tardy
jobs. Because the costs of operators in the schedule are stochastic and
sequence dependent, standard dynamic programming algorithms such as A* may fail
to find the optimal schedule. The SDA* (Stochastic Dominance A*) algorithm
remedies this difficulty by relaxing the pruning condition. We present an
improved state-space search formulation for these problems and discuss the
conditions under which stochastic scheduling problems can be solved optimally
using SDA*. In empirical testing on randomly generated problems, we found that
in 70%, the expected cost of the optimal stochastic solution is lower than that
of the solution derived using a deterministic approximation, with comparable
search effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3612</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3612</id><created>2013-02-13</created><authors><author><keyname>Xiang</keyname><forenames>Yang</forenames></author><author><keyname>Wong</keyname><forenames>Michael S. K. M.</forenames></author><author><keyname>Cercone</keyname><forenames>N.</forenames></author></authors><title>Critical Remarks on Single Link Search in Learning Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-564-571</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In learning belief networks, the single link lookahead search is widely
adopted to reduce the search space. We show that there exists a class of
probabilistic domain models which displays a special pattern of dependency. We
analyze the behavior of several learning algorithms using different scoring
metrics such as the entropy, conditional independence, minimal description
length and Bayesian metrics. We demonstrate that single link lookahead search
procedures (employed in these algorithms) cannot learn these models correctly.
Thus, when the underlying domain model actually belongs to this class, the use
of a single link search procedure will result in learning of an incorrect
model. This may lead to inference errors when the model is used. Our analysis
suggests that if the prior knowledge about a domain does not rule out the
possible existence of these models, a multi-link lookahead search or other
heuristics should be used for the learning process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3636</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3636</id><created>2013-02-14</created><authors><author><keyname>Hartke</keyname><forenames>Stephen G.</forenames></author><author><keyname>Stolee</keyname><forenames>Derrick</forenames></author></authors><title>A Branch-and-Cut Strategy for the Manickam-Miklos-Singhi Conjecture</title><categories>math.CO cs.DM</categories><comments>23 pages, 1 figure, 4 tables</comments><msc-class>05D05, 68R05, 90C27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Manickam-Miklos-Singhi Conjecture states that when n is at least 4k,
every multiset of n real numbers with nonnegative total sum has at least (n-1
choose k-1) k-subsets with nonnegative sum. We develop a branch-and-cut
strategy using a linear programming formulation to show that verifying the
conjecture for fixed values of k is a finite problem. To improve our search, we
develop a zero-error randomized propagation algorithm. Using implementations of
these algorithms, we verify a stronger form of the conjecture for all k at most
seven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3639</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3639</id><created>2013-02-14</created><updated>2013-12-12</updated><authors><author><keyname>Chen</keyname><forenames>George H.</forenames></author><author><keyname>Nikolov</keyname><forenames>Stanislav</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author></authors><title>A Latent Source Model for Nonparametric Time Series Classification</title><categories>stat.ML cs.LG cs.SI</categories><comments>Advances in Neural Information Processing Systems (NIPS 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For classifying time series, a nearest-neighbor approach is widely used in
practice with performance often competitive with or better than more elaborate
methods such as neural networks, decision trees, and support vector machines.
We develop theoretical justification for the effectiveness of
nearest-neighbor-like classification of time series. Our guiding hypothesis is
that in many applications, such as forecasting which topics will become trends
on Twitter, there aren't actually that many prototypical time series to begin
with, relative to the number of time series we have access to, e.g., topics
become trends on Twitter only in a few distinct manners whereas we can collect
massive amounts of Twitter data. To operationalize this hypothesis, we propose
a latent source model for time series, which naturally leads to a &quot;weighted
majority voting&quot; classification rule that can be approximated by a
nearest-neighbor classifier. We establish nonasymptotic performance guarantees
of both weighted majority voting and nearest-neighbor classification under our
model accounting for how much of the time series we observe and the model
complexity. Experimental results on synthetic data show weighted majority
voting achieving the same misclassification rate as nearest-neighbor
classification while observing less of the time series. We then use weighted
majority to forecast which news topics on Twitter become trends, where we are
able to detect such &quot;trending topics&quot; in advance of Twitter 79% of the time,
with a mean early advantage of 1 hour and 26 minutes, a true positive rate of
95%, and a false positive rate of 4%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3642</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3642</id><created>2013-02-14</created><authors><author><keyname>Flyvbjerg</keyname><forenames>Bent</forenames></author></authors><title>From Nobel Prize to Project Management: Getting Risks Right</title><categories>q-fin.GN cs.CY</categories><comments>arXiv admin note: text overlap with arXiv:1302.2544</comments><journal-ref>Bent Flyvbjerg, &quot;From Nobel Prize to Project Management: Getting
  Risks Right,&quot; Project Management Journal, vol. 37, no. 3, August 2006, pp.
  5-15</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major source of risk in project management is inaccurate forecasts of
project costs, demand, and other impacts. The paper presents a promising new
approach to mitigating such risk, based on theories of decision making under
uncertainty which won the 2002 Nobel prize in economics. First, the paper
documents inaccuracy and risk in project management. Second, it explains
inaccuracy in terms of optimism bias and strategic misrepresentation. Third,
the theoretical basis is presented for a promising new method called &quot;reference
class forecasting,&quot; which achieves accuracy by basing forecasts on actual
performance in a reference class of comparable projects and thereby bypassing
both optimism bias and strategic misrepresentation. Fourth, the paper presents
the first instance of practical reference class forecasting, which concerns
cost forecasts for large transportation infrastructure projects. Finally,
potentials for and barriers to reference class forecasting are assessed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3660</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3660</id><created>2013-02-14</created><authors><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Viswanatha</keyname><forenames>Kumar</forenames></author><author><keyname>Rose</keyname><forenames>Kenneth</forenames></author><author><keyname>Ramstad</keyname><forenames>Tor</forenames></author></authors><title>On Zero Delay Source-Channel Coding</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, 18 pages, 10
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the zero-delay source-channel coding problem, and
specifically the problem of obtaining the vector transformations that optimally
map between the m-dimensional source space and the k-dimensional channel space,
under a given transmission power constraint and for the mean square error
distortion. We first study the functional properties of this problem and show
that the objective is concave in the source and noise densities and convex in
the density of the input to the channel. We then derive the necessary
conditions for optimality of the encoder and decoder mappings. A well known
result in information theory pertains to the linearity of optimal encoding and
decoding mappings in the scalar Gaussian source and channel setting, at all
channel signal-to-noise ratios (CSNRs). In this paper, we study this result
more generally, beyond the Gaussian source and channel, and derive the
necessary and sufficient condition for linearity of optimal mappings, given a
noise (or source) distribution, and a specified power constraint. We also prove
that the Gaussian source-channel pair is unique in the sense that it is the
only source-channel pair for which the optimal mappings are linear at more than
one CSNR values. Moreover, we show the asymptotic linearity of optimal mappings
for low CSNR if the channel is Gaussian regardless of the source and, at the
other extreme, for high CSNR if the source is Gaussian, regardless of the
channel. Our numerical results show strict improvement over prior methods. The
numerical approach is extended to the scenario of source-channel coding with
decoder side information. The resulting encoding mappings are shown to be
continuous relatives of, and in fact subsume as special case, the Wyner-Ziv
mappings encountered in digital distributed source coding systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3663</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3663</id><created>2013-02-14</created><authors><author><keyname>Hammond</keyname><forenames>Jason F.</forenames></author><author><keyname>Stewart</keyname><forenames>Elizabeth J.</forenames></author><author><keyname>Younger</keyname><forenames>John G.</forenames></author><author><keyname>Solomon</keyname><forenames>Michael J.</forenames></author><author><keyname>Bortz</keyname><forenames>David M.</forenames></author></authors><title>Spatially Heterogeneous Biofilm Simulations using an Immersed Boundary
  Method with Lagrangian Nodes Defined by Bacterial Locations</title><categories>math.NA cs.CE physics.flu-dyn</categories><comments>43 pages, 18 figures, 9 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider how surface-adherent bacterial biofilm communities
respond in flowing systems. We simulate the fluid-structure interaction and
separation process using the immersed boundary method. In these simulations we
model and simulate different density and viscosity values of the biofilm than
that of the surrounding fluid. The simulation also includes breakable springs
connecting the bacteria in the biofilm. This allows the inclusion of erosion
and detachment into the simulation. We use the incompressible Navier-Stokes
(N-S) equations to describe the motion of the flowing fluid. We discretize the
fluid equations using finite differences and use a geometric multigrid method
to solve the resulting equations at each time step. The use of multigrid is
necessary because of the dramatically different densities and viscosities
between the biofilm and the surrounding fluid. We investigate and simulate the
model in both two and three dimensions.
  Our method differs from previous attempts of using IBM for modeling
biofilm/flow interactions in the following ways: the density and viscosity of
the biofilm can differ from the surrounding fluid, and the Lagrangian node
locations correspond to experimentally measured bacterial cell locations from
3D images taken of Staphylococcus epidermidis in a biofilm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3668</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3668</id><created>2013-02-14</created><authors><author><keyname>Narayanan</keyname><forenames>Ajit</forenames></author><author><keyname>Chen</keyname><forenames>Yi</forenames></author></authors><title>Bio-inspired data mining: Treating malware signatures as biosequences</title><categories>cs.LG q-bio.QM stat.ML</categories><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The application of machine learning to bioinformatics problems is well
established. Less well understood is the application of bioinformatics
techniques to machine learning and, in particular, the representation of
non-biological data as biosequences. The aim of this paper is to explore the
effects of giving amino acid representation to problematic machine learning
data and to evaluate the benefits of supplementing traditional machine learning
with bioinformatics tools and techniques. The signatures of 60 computer viruses
and 60 computer worms were converted into amino acid representations and first
multiply aligned separately to identify conserved regions across different
families within each class (virus and worm). This was followed by a second
alignment of all 120 aligned signatures together so that non-conserved regions
were identified prior to input to a number of machine learning techniques.
Differences in length between virus and worm signatures after the first
alignment were resolved by the second alignment. Our first set of experiments
indicates that representing computer malware signatures as amino acid sequences
followed by alignment leads to greater classification and prediction accuracy.
Our second set of experiments indicates that checking the results of data
mining from artificial virus and worm data against known proteins can lead to
generalizations being made from the domain of naturally occurring proteins to
malware signatures. However, further work is needed to determine the advantages
and disadvantages of different representations and sequence alignment methods
for handling problematic machine learning data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3669</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3669</id><created>2013-02-14</created><authors><author><keyname>Bazaikin</keyname><forenames>Ya. V.</forenames></author><author><keyname>Taimanov</keyname><forenames>I. A.</forenames></author></authors><title>On a numerical algorithm for computing topological characteristics of
  three-dimensional bodies</title><categories>cs.CG math.AT</categories><comments>13 pages</comments><journal-ref>Zh. Vychisl. Mat. Mat. Fiz. 53:4 (2013), 523-530</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for computing the main topological characteristics of
three-dimensional bodies. The algorithm is based on a discretization of Morse
theory and uses discrete analogs of smooth functions with only nondegenerate
(Morse) and the simplest degenerate critical points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3672</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3672</id><created>2013-02-14</created><updated>2015-12-30</updated><authors><author><keyname>Wang</keyname><forenames>Jiun-Jie</forenames></author></authors><title>A Polynomial Time Algorithm for Finding Area-Universal Rectangular
  Layouts</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A rectangular layout $\mathcal{L}$ is a rectangle partitioned into disjoint
smaller rectangles so that no four smaller rectangles meet at the same point.
Rectangular layouts were originally used as floorplans in VLSI design to
represent VLSI chip layouts. More recently, they are used in graph drawing as
rectangular cartograms. In these applications, an area $a(r)$ is assigned to
each rectangle $r$, and the actual area of $r$ in $\mathcal{L}$ is required to
be $a(r)$. Moreover, some applications require that we use combinatorially
equivalent rectangular layouts to represent multiple area assignment functions.
$\mathcal{L}$ is called {\em area-universal} if any area assignment to its
rectangles can be realized by a layout that is combinatorially equivalent to
$\mathcal{L}$.
  A basic question in this area is to determine if a given plane graph $G$ has
an area-universal rectangular layout or not. A fixed-parameter-tractable
algorithm for solving this problem was obtained in \cite{EMSV12}. Their
algorithm takes $O(2^{O(K^2)}n^{O(1)})$ time (where $K$ is the maximum number
of degree 4 vertices in any minimal separation component), which is exponential
time in general case. It is an open problem to find a true polynomial time
algorithm for solving this problem. In this paper, we describe such a
polynomial time algorithm.
  This paper has been revised for many versions. For previous versions,
referrers who are familiar with area-universal rectangular layouts always have
the same doubt for the correctness of our algorithm. They doubt that our
algorithm will give a wrong output which combine two \emph{conflicting} REL
together.
  In the current version, we realize this critical issue for the previous
algorithm and we will provide two subsections 5.3 and 5.4 to solve this issue.
(A backtracking algorithm to detect wrong outputs.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3673</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3673</id><created>2013-02-14</created><updated>2013-11-20</updated><authors><author><keyname>Ruan</keyname><forenames>Ning</forenames></author><author><keyname>Gao</keyname><forenames>David Y</forenames></author></authors><title>Global Optimal Solutions to General Sensor Network Localization Problem</title><categories>cs.CC</categories><comments>29 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensor network localization problem is to determine the position of the
sensor nodes in a network given pairwise distance measurements. Such problem
can be formulated as a polynomial minimization via the least squares method.
This paper presents a canonical duality theory for solving this challenging
problem. It is shown that the nonconvex minimization problem can be
reformulated as a concave maximization dual problem over a convex set in a
symmetrical matrix space, and hence can be solved efficiently by combining a
general (linear or quadratic) perturbation technique with existing optimization
techniques. Applications are illustrated by solving some relatively large-scale
problems. Our results show that the general sensor network localization problem
is not NP-hard unless its canonical dual problem has no solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3681</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3681</id><created>2013-02-15</created><authors><author><keyname>Gupta</keyname><forenames>Manish K</forenames></author><author><keyname>Agrawal</keyname><forenames>Anupam</forenames></author><author><keyname>Yadav</keyname><forenames>Deepak</forenames></author></authors><title>On Weak Dress Codes for Cloud Storage</title><categories>cs.IT math.IT</categories><comments>6 pages, 7 figures, submitted to NetCod 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a distributed storage network, reliability and bandwidth optimization can
be provided by regenerating codes. Recently table based regenerating codes viz.
DRESS (Distributed Replication-based Exact Simple Storage) codes has been
proposed which also optimizes the disk I/O. Dress codes consists of an outer
MDS code with an inner fractional repetition (FR) code with replication degree
$\rho$. Several constructions of FR codes based on regular graphs, resolvable
designs and bipartite graphs are known. This paper presents a simple modular
construction of FR codes. We also generalize the concept of FR codes to weak
fractional repetition (WFR) codes where each node has different number of
packets. We present a construction of WFR codes based on partial regular graph.
Finally we present a simple generalized ring construction of both strong and
weak fractional repetition codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3697</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3697</id><created>2013-02-15</created><updated>2013-10-14</updated><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Marx</keyname><forenames>Werner</forenames></author></authors><title>How to evaluate individual researchers working in the natural and life
  sciences meaningfully? A proposal of methods based on percentiles of
  citations</title><categories>cs.DL physics.soc-ph stat.AP</categories><comments>Accepted for publication in Scientometrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although bibliometrics has been a separate research field for many years,
there is still no uniformity in the way bibliometric analyses are applied to
individual researchers. Therefore, this study aims to set up proposals how to
evaluate individual researchers working in the natural and life sciences. 2005
saw the introduction of the h index, which gives information about a
researcher's productivity and the impact of his or her publications in a single
number (h is the number of publications with at least h citations); however, it
is not possible to cover the multidimensional complexity of research
performance and to undertake inter-personal comparisons with this number. This
study therefore includes recommendations for a set of indicators to be used for
evaluating researchers. Our proposals relate to the selection of data on which
an evaluation is based, the analysis of the data and the presentation of the
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3700</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3700</id><created>2013-02-15</created><authors><author><keyname>Quinn</keyname><forenames>John A.</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Density Ratio Hidden Markov Models</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hidden Markov models and their variants are the predominant sequential
classification method in such domains as speech recognition, bioinformatics and
natural language processing. Being generative rather than discriminative
models, however, their classification performance is a drawback. In this paper
we apply ideas from the field of density ratio estimation to bypass the
difficult step of learning likelihood functions in HMMs. By reformulating
inference and model fitting in terms of density ratios and applying a fast
kernel-based estimation method, we show that it is possible to obtain a
striking increase in discriminative performance while retaining the
probabilistic qualities of the HMM. We demonstrate experimentally that this
formulation makes more efficient use of training data than alternative
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3702</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3702</id><created>2013-02-15</created><authors><author><keyname>Nazeer</keyname><forenames>Muhammad</forenames></author><author><keyname>Nargis</keyname><forenames>Bibi</forenames></author><author><keyname>Malik</keyname><forenames>Yasir Mehmood</forenames></author><author><keyname>Kim</keyname><forenames>Dai-Gyoung</forenames></author></authors><title>A Fresnelet-Based Encryption of Medical Images using Arnold Transform</title><categories>cs.CR cs.CV</categories><comments>16 pages, 11 figures, Journal Paper</comments><msc-class>68U10, 68M11</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Medical images are commonly stored in digital media and transmitted via
Internet for certain uses. If a medical information image alters, this can lead
to a wrong diagnosis which may create a serious health problem. Moreover,
medical images in digital form can easily be modified by wiping off or adding
small pieces of information intentionally for certain illegal purposes. Hence,
the reliability of medical images is an important criterion in a hospital
information system. In this paper, Fresnelet transform is employed along with
appropriate handling of the Arnold transform and the discrete cosine transform
to provide secure distribution of medical images. This method presents a new
data hiding system in which steganography and cryptography are used to prevent
unauthorized data access. The experimental results exhibit high
imperceptibility for embedded images and significant encryption of information
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3705</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3705</id><created>2013-02-15</created><authors><author><keyname>Wang</keyname><forenames>Xiumin</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Partial Third-Party Information Exchange with Network Coding</title><categories>cs.IT math.IT</categories><comments>network coding</comments><journal-ref>IEEE Communications Letters, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of exchanging channel state
information in a wireless network such that a subset of the clients can obtain
the complete channel state information of all the links in the network. We
first derive the minimum number of required transmissions for such partial
third-party information exchange problem. We then design an optimal
transmission scheme by determining the number of packets that each client
should send, and designing a deterministic encoding strategy such that the
subset of clients can acquire complete channel state information of the network
with minimal number of transmissions. Numerical results show that network
coding can efficiently reduce the number of transmissions, even with only
pairwise encoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3720</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3720</id><created>2013-02-15</created><authors><author><keyname>Aupy</keyname><forenames>Guillaume</forenames></author><author><keyname>Benoit</keyname><forenames>Anne</forenames></author><author><keyname>Melhem</keyname><forenames>Rami</forenames></author><author><keyname>Renaud-Goud</keyname><forenames>Paul</forenames></author><author><keyname>Robert</keyname><forenames>Yves</forenames></author></authors><title>Energy-aware checkpointing of divisible tasks with soft or hard
  deadlines</title><categories>cs.DS</categories><comments>This work was supported by ANR Rescue</comments><report-no>INRIA RR-8238</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we aim at minimizing the energy consumption when executing a
divisible workload under a bound on the total execution time, while resilience
is provided through checkpointing. We discuss several variants of this
multi-criteria problem. Given the workload, we need to decide how many chunks
to use, what are the sizes of these chunks, and at which speed each chunk is
executed. Furthermore, since a failure may occur during the execution of a
chunk, we also need to decide at which speed a chunk should be re-executed in
the event of a failure. The goal is to minimize the expectation of the total
energy consumption, while enforcing a deadline on the execution time, that
should be met either in expectation (soft deadline), or in the worst case (hard
deadline). For each problem instance, we propose either an exact solution, or a
function that can be optimized numerically. The different models are then
compared through an extensive set of experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3721</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3721</id><created>2013-02-15</created><authors><author><keyname>Mellor</keyname><forenames>Joseph</forenames></author><author><keyname>Shapiro</keyname><forenames>Jonathan</forenames></author></authors><title>Thompson Sampling in Switching Environments with Bayesian Online Change
  Point Detection</title><categories>cs.LG</categories><comments>A version will appear in the Sixteenth international conference on
  Artificial Intelligence and Statistics (AIStats 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thompson Sampling has recently been shown to be optimal in the Bernoulli
Multi-Armed Bandit setting[Kaufmann et al., 2012]. This bandit problem assumes
stationary distributions for the rewards. It is often unrealistic to model the
real world as a stationary distribution. In this paper we derive and evaluate
algorithms using Thompson Sampling for a Switching Multi-Armed Bandit Problem.
We propose a Thompson Sampling strategy equipped with a Bayesian change point
mechanism to tackle this problem. We develop algorithms for a variety of cases
with constant switching rate: when switching occurs all arms change (Global
Switching), switching occurs independently for each arm (Per-Arm Switching),
when the switching rate is known and when it must be inferred from data. This
leads to a family of algorithms we collectively term Change-Point Thompson
Sampling (CTS). We show empirical results of the algorithm in 4 artificial
environments, and 2 derived from real world data; news click-through[Yahoo!,
2011] and foreign exchange data[Dukascopy, 2012], comparing them to some other
bandit algorithms. In real world data CTS is the most effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3722</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3722</id><created>2013-02-15</created><authors><author><keyname>Frid</keyname><forenames>Anna E.</forenames></author><author><keyname>Jamet</keyname><forenames>Damien</forenames></author></authors><title>The number of binary rotation words</title><categories>math.CO cs.DM math.DS</categories><comments>Submitted to RAIRO ITA</comments><msc-class>68R15, 37B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider binary rotation words generated by partitions of the unit circle
to two intervals and give a precise formula for the number of such words of
length n. We also give the precise asymptotics for it, which happens to be
O(n^4). The result continues the line initiated by the formula for the number
of all Sturmian words obtained by Lipatov in 1982, then independently by
Berenstein, Kanal, Lavine and Olson in 1987, Mignosi in 1991, and then with
another technique by Berstel and Pocchiola in 1993.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3723</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3723</id><created>2013-02-15</created><authors><author><keyname>Klotz</keyname><forenames>Johannes Georg</forenames></author><author><keyname>Bossert</keyname><forenames>Martin</forenames></author><author><keyname>Schober</keyname><forenames>Steffen</forenames></author></authors><title>Computing preimages of Boolean Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to WCSB 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an algorithm to address the predecessor problem of
feed-forward Boolean networks. We propose an probabilistic algorithm, which
solves this problem in linear time with respect to the number of nodes in the
network. Finally, we evaluate our algorithm for random Boolean networks and the
regulatory network of Escherichia coli.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3726</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3726</id><created>2013-02-15</created><updated>2013-12-15</updated><authors><author><keyname>Potechin</keyname><forenames>Aaron</forenames></author></authors><title>Improved upper and lower bound techniques for monotone switching
  networks for directed connectivity</title><categories>cs.DS cs.NI</categories><comments>48 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the monotone space of complexity of directed
connectivity for a large class of input graphs $G$ using the switching network
model. The upper and lower bounds we obtain are a significant generalization of
previous results and the proofs involve several completely new techniques and
ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3741</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3741</id><created>2013-02-15</created><updated>2013-04-26</updated><authors><author><keyname>Stewart</keyname><forenames>Alistair</forenames></author><author><keyname>Etessami</keyname><forenames>Kousha</forenames></author><author><keyname>Yannakakis</keyname><forenames>Mihalis</forenames></author></authors><title>Upper bounds for Newton's method on monotone polynomial systems, and
  P-time model checking of probabilistic one-counter automata</title><categories>cs.LO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central computational problem for analyzing and model checking various
classes of infinite-state recursive probabilistic systems (including
quasi-birth-death processes, multi-type branching processes, stochastic
context-free grammars, probabilistic pushdown automata and recursive Markov
chains) is the computation of {\em termination probabilities}, and computing
these probabilities in turn boils down to computing the {\em least fixed point}
(LFP) solution of a corresponding {\em monotone polynomial system} (MPS) of
equations, denoted x=P(x).
  It was shown by Etessami &amp; Yannakakis that a decomposed variant of Newton's
method converges monotonically to the LFP solution for any MPS that has a
non-negative solution. Subsequently, Esparza, Kiefer, &amp; Luttenberger obtained
upper bounds on the convergence rate of Newton's method for certain classes of
MPSs. More recently, better upper bounds have been obtained for special classes
of MPSs. However, prior to this paper, for arbitrary (not necessarily
strongly-connected) MPSs, no upper bounds at all were known on the convergence
rate of Newton's method as a function of the encoding size |P| of the input
MPS, x=P(x).
  In this paper we provide worst-case upper bounds, as a function of both the
input encoding size |P|, and epsilon &gt; 0, on the number of iterations required
for decomposed Newton's method (even with rounding) to converge within additive
error epsilon &gt; 0 of q^*, for any MPS with LFP solution q^*. Our upper bounds
are essentially optimal in terms of several important parameters.
  Using our upper bounds, and building on prior work, we obtain the first
P-time algorithm (in the standard Turing model of computation) for quantitative
model checking, to within desired precision, of discrete-time QBDs and
(equivalently) probabilistic 1-counter automata, with respect to any (fixed)
omega-regular or LTL property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3747</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3747</id><created>2013-02-15</created><updated>2014-01-09</updated><authors><author><keyname>Olteanu</keyname><forenames>Gabriela</forenames></author><author><keyname>Van Gelder</keyname><forenames>Inneke</forenames></author></authors><title>Construction of minimal non-abelian left group codes</title><categories>math.RT cs.IT math.GR math.IT math.RA</categories><msc-class>94B05, 16S34, 20C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms to construct minimal left group codes are provided. These are
based on results describing a complete set of orthogonal primitive idempotents
in each Wedderburn component of a semisimple finite group algebra FG for a
large class of groups G.
  As an illustration of our methods, alternative constructions to some best
linear codes over F_2 and F_3 are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3749</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3749</id><created>2013-02-15</created><authors><author><keyname>Ismaeel</keyname><forenames>Ayad Ghany</forenames></author><author><keyname>Jabar</keyname><forenames>Emad Khadhm</forenames></author></authors><title>Effective System for Pregnant Women using Mobile GIS</title><categories>cs.CY</categories><comments>7 pages,10 Figures, 4 Tables</comments><journal-ref>http://www.ijcaonline.org/archives/volume64/number11/10675-5547
  (2013)</journal-ref><doi>10.5120/10675-5547</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  World Health Organization showed at one year about 287 000 women died most of
them during and following pregnancy and childbirth in Africa and south Asia.
This paper suggests mHealth system for serving pregnant women, that proposed
system is first an effective mHealth system works base on mobile GIS to select
adjacent care centre or hospital maternity on Google map at online registration
for woman pregnant, that is done when the pregnant woman will send SMS via GPRS
network contains her ID and coordinates (Longitude and Latitude) the server
when receive it will search database support that system and using the same
infrastructure for help the pregnant women at her location (home, market, etc)
in emergency cases when the woman send SMS contains her coordinates for
succoring. Implement the proposed pregnant women system shows more effective
from view of cost than other systems because it works in economic (SMS) mode
and from view of serve the system can easy and rapidly manage when achieving
locally registration, succoring in emergency cases, change the review date of
pregnant woman, as well as different types of advising.
  Keywords Pregnant Women, mHealth System, General Packet Radio Service (GPRS);
Mobile GIS; Short Message Service (SMS), Global Position System (GPS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3752</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3752</id><created>2013-02-15</created><updated>2013-12-03</updated><authors><author><keyname>Aupy</keyname><forenames>Guillaume</forenames></author><author><keyname>Robert</keyname><forenames>Yves</forenames></author><author><keyname>Vivien</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Zaidouni</keyname><forenames>Dounia</forenames></author></authors><title>Checkpointing algorithms and fault prediction</title><categories>cs.DC</categories><comments>Supported in part by ANR Rescue. Published in Journal of Parallel and
  Distributed Computing. arXiv admin note: text overlap with arXiv:1207.6936</comments><report-no>INRIA RR-8237</report-no><journal-ref>Journal of Parallel and Distributed Computing, Available online 7
  November 2013, ISSN 0743-7315</journal-ref><doi>10.1016/j.jpdc.2013.10.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the impact of fault prediction techniques on
checkpointing strategies. We extend the classical first-order analysis of Young
and Daly in the presence of a fault prediction system, characterized by its
recall and its precision. In this framework, we provide an optimal algorithm to
decide when to take predictions into account, and we derive the optimal value
of the checkpointing period. These results allow to analytically assess the key
parameters that impact the performance of fault predictors at very large scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3763</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3763</id><created>2013-02-15</created><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author></authors><title>Faster exponential-time algorithms in graphs of bounded average degree</title><categories>cs.DS</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first show that the Traveling Salesman Problem in an n-vertex graph with
average degree bounded by d can be solved in O*(2^{(1-\eps_d)n}) time and
exponential space for a constant \eps_d depending only on d, where the
O*-notation suppresses factors polynomial in the input size. Thus, we
generalize the recent results of Bjorklund et al. [TALG 2012] on graphs of
bounded degree.
  Then, we move to the problem of counting perfect matchings in a graph. We
first present a simple algorithm for counting perfect matchings in an n-vertex
graph in O*(2^{n/2}) time and polynomial space; our algorithm matches the
complexity bounds of the algorithm of Bjorklund [SODA 2012], but relies on
inclusion-exclusion principle instead of algebraic transformations. Building
upon this result, we show that the number of perfect matchings in an n-vertex
graph with average degree bounded by d can be computed in
O*(2^{(1-\eps_{2d})n/2}) time and exponential space, where \eps_{2d} is the
constant obtained by us for the Traveling Salesman Problem in graphs of average
degree at most 2d.
  Moreover we obtain a simple algorithm that counts the number of perfect
matchings in an n-vertex bipartite graph of average degree at most d in
O*(2^{(1-1/(3.55d))n/2}) time, improving and simplifying the recent result of
Izumi and Wadayama [FOCS 2012].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3777</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3777</id><created>2013-02-15</created><updated>2014-02-23</updated><authors><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Capacity of the State-Dependent Half-Duplex Relay Channel Without
  Source-Destination Link</title><categories>cs.IT math.IT</categories><comments>Withdrawn by the author due to errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive the capacity of the state-dependent half-duplex relay channel
without source-destination link. The output of the state-dependent half-duplex
relay channel depends on the randomly varying channel states of the
source-relay and relay-destination links, which are known causally at all three
nodes. For this channel, we prove a converse and show the achievability of the
capacity based on a buffer-aided relaying protocol with adaptive link
selection. This protocol chooses in each times slot one codeword to be
transmitted over either the source-relay or the relay-destination channel
depending on the channel states. Our proof of the converse reveals that
state-dependent half-duplex relay networks offer one additional degree of
freedom which has been previously overlooked. Namely, the freedom of the
half-duplex relay to choose when to receive and when to transmit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3784</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3784</id><created>2013-02-15</created><authors><author><keyname>Deb</keyname><forenames>Supratim</forenames></author><author><keyname>Monogioudis</keyname><forenames>Pantelis</forenames></author><author><keyname>Miernik</keyname><forenames>Jerzy</forenames></author><author><keyname>Seymour</keyname><forenames>James P.</forenames></author></authors><title>Algorithms for Enhanced Inter Cell Interference Coordination (eICIC) in
  LTE HetNets</title><categories>cs.NI</categories><comments>Accepted to appear in IEEE/ACM Transactions on Networking</comments><doi>10.1109/TNET.2013.2246820</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of LTE Heterogeneous Networks (HetNets) with macro cells and pico
cells critically depends on efficient spectrum sharing between high-power
macros and low-power picos. Two important challenges in this context are, {(i)}
determining the amount of radio resources that macro cells should {\em offer}
to pico cells, and {(ii)} determining the association rules that decide which
UEs should associate with picos. In this paper, we develop a novel algorithm to
solve these two coupled problems in a joint manner. Our algorithm has provable
guarantee, and furthermore, it accounts for network topology, traffic load, and
macro-pico interference map. Our solution is standard compliant and can be
implemented using the notion of Almost Blank Subframes (ABS) and Cell Selection
Bias (CSB) proposed by LTE standards. We also show extensive evaluations using
RF plan from a real network and discuss SON based eICIC implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3785</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3785</id><created>2013-02-15</created><updated>2013-08-02</updated><authors><author><keyname>Vural</keyname><forenames>Elif</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Analysis of Descent-Based Image Registration</title><categories>cs.CV</categories><journal-ref>SIAM Journal on Imaging Sciences, Vol. 6, No. 4, pp 2310-2349,
  2013</journal-ref><doi>10.1137/130909858</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a performance analysis for image registration with gradient
descent methods. We consider a typical multiscale registration setting where
the global 2-D translation between a pair of images is estimated by smoothing
the images and minimizing the distance between them with gradient descent. Our
study particularly concentrates on the effect of noise and low-pass filtering
on the alignment accuracy. We adopt an analytic representation for images and
analyze the well-behavedness of the image distance function by estimating the
neighborhood of translations for which it is free of undesired local minima.
This corresponds to the neighborhood of translation vectors that are correctly
computable with a simple gradient descent minimization. We show that the area
of this neighborhood increases at least quadratically with the smoothing filter
size, which justifies the use of a smoothing step in image registration with
local optimizers such as gradient descent. We then examine the effect of noise
on the alignment accuracy and derive an upper bound for the alignment error in
terms of the noise properties and filter size. Our main finding is that the
error increases at a rate that is at least linear with respect to the filter
size. Therefore, smoothing improves the well-behavedness of the distance
function; however, this comes at the cost of amplifying the alignment error in
noisy settings. Our results provide a mathematical insight about why
hierarchical techniques are effective in image registration, suggesting that
the multiscale coarse-to-fine alignment strategy of these techniques is very
suitable from the perspective of the trade-off between the well-behavedness of
the objective function and the registration accuracy. To the best of our
knowledge, this is the first such study for descent-based image registration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3793</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3793</id><created>2013-02-15</created><authors><author><keyname>Goldberg</keyname><forenames>Paul</forenames></author><author><keyname>Pastink</keyname><forenames>Arnoud</forenames></author></authors><title>On the Communication Complexity of Approximate Nash Equilibria</title><categories>cs.GT</categories><comments>18 pages; preliminary version appeared at 2012 SAGT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of computing approximate Nash equilibria of bimatrix
games, in a setting where players initially know their own payoffs but not the
payoffs of the other player. In order for a solution of reasonable quality to
be found, some amount of communication needs to take place between the players.
We are interested in algorithms where the communication is substantially less
than the contents of a payoff matrix, for example logarithmic in the size of
the matrix. When the communication is polylogarithmic in the number of
strategies n, we show how to obtain epsilon-approximate Nash equilibria for
epsilon approximately 0.438, and for well-supported approximate equilibria we
obtain epsilon approximately 0.732. For one-way communication we show that
epsilon=1/2 is achievable, but no constant improvement over 1/2 is possible,
even with unlimited one-way communication. For well-supported equilibria, no
value of epsilon less than 1 is achievable with one-way communication. When the
players do not communicate at all, epsilon-Nash equilibria can be obtained for
epsilon=3/4, and we also give a lower bound of slightly more than 1/2 on the
lowest constant epsilon achievable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3798</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3798</id><created>2013-02-15</created><authors><author><keyname>Zetzsche</keyname><forenames>Georg</forenames></author></authors><title>Silent Transitions in Automata with Storage</title><categories>cs.FL</categories><comments>32 pages, submitted</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We consider the computational power of silent transitions in one-way automata
with storage. Specifically, we ask which storage mechanisms admit a
transformation of a given automaton into one that accepts the same language and
reads at least one input symbol in each step.
  We study this question using the model of valence automata. Here, a finite
automaton is equipped with a storage mechanism that is given by a monoid.
  This work presents generalizations of known results on silent transitions.
For two classes of monoids, it provides characterizations of those monoids that
allow the removal of \lambda-transitions. Both classes are defined by graph
products of copies of the bicyclic monoid and the group of integers. The first
class contains pushdown storages as well as the blind counters while the second
class contains the blind and the partially blind counters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3800</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3800</id><created>2013-02-15</created><authors><author><keyname>Kaddoum</keyname><forenames>Georges</forenames></author><author><keyname>Gagnon</keyname><forenames>Francois</forenames></author><author><keyname>Couillard</keyname><forenames>Denis</forenames></author></authors><title>An Enhanced Spectral Efficiency Chaos-Based Symbolic Dynamics
  Transceiver Design</title><categories>cs.IT math.IT</categories><comments>6 pages, 7 figues, accepted in International Conference on. Signal
  Processing and Communication Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chaotic synchronization performs poorly in noisy environments, with the main
drawback being that the coherent receiver cannot be implemented in realistic
communication channels. In this paper, we focus our study on a promising
communication system based on chaotic symbolic dynamics. Such modulation shows
a high synchronization quality, without the need for a complex chaotic
synchronization mechanism. Our study mainly concerns an improvement of the
bandwidth efficiency of the chaotic modulator. A new chaotic map is proposed to
achieve this goal, and a receiver based on the maximum likelihood algorithm is
designed to estimate the transmitted symbols. The performance of the proposed
system is analyzed and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3809</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3809</id><created>2013-02-15</created><authors><author><keyname>Evako</keyname><forenames>Alexander</forenames></author></authors><title>Variable density preserving topology grids and the digital models for
  the plane</title><categories>cs.DM</categories><comments>7 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define LCL decompositions of the plane and investigate the advantages of
using such decompositions in the context of digital topology. We show that
discretization schemes based on such decompositions associate, to each LCL
tiling of the plane, the digital model preserving the local topological
structure of the object. We prove that for any LCL tiling of the plane, the
digital model is necessarily a digital 2-manifold. We show that elements of an
LCL tiling can be of an arbitrary shape and size. This feature generates a
variable density grid with a required resolution in any region of interest,
which is extremely important in medicine. Finally, we describe a simple
algorithm, which allows transforming regions of interest produced by the image
acquisition process into digital spaces with topological features of the
regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3820</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3820</id><created>2013-02-15</created><authors><author><keyname>Patwari</keyname><forenames>Neal</forenames></author><author><keyname>Brewer</keyname><forenames>Lara</forenames></author><author><keyname>Tate</keyname><forenames>Quinn</forenames></author><author><keyname>Kaltiokallio</keyname><forenames>Ossi</forenames></author><author><keyname>Bocca</keyname><forenames>Maurizio</forenames></author></authors><title>Breathfinding: A Wireless Network that Monitors and Locates Breathing in
  a Home</title><categories>cs.HC</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper explores using RSS measurements on many links in a wireless
network to estimate the breathing rate of a person, and the location where the
breathing is occurring, in a home, while the person is sitting, laying down,
standing, or sleeping. The main challenge in breathing rate estimation is that
&quot;motion interference&quot;, i.e., movements other than a person's breathing,
generally cause larger changes in RSS than inhalation and exhalation. We
develop a method to estimate breathing rate despite motion interference, and
demonstrate its performance during multiple short (3-7 minute) tests and during
a longer 66 minute test. Further, for the same experiments, we show the
location of the breathing person can be estimated, to within about 2 m average
error in a 56 square meter apartment. Being able to locate a breathing person
who is not otherwise moving, without calibration, is important for applications
in search and rescue, health care, and security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3826</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3826</id><created>2013-02-15</created><authors><author><keyname>Geng</keyname><forenames>Jun</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author><author><keyname>Lai</keyname><forenames>Lifeng</forenames></author></authors><title>Quickest Search Over Multiple Sequences with Mixed Observations</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of sequentially finding an independent and identically
distributed (i.i.d.) sequence that is drawn from a probability distribution
$F_1$ by searching over multiple sequences, some of which are drawn from $F_1$
and the others of which are drawn from a different distribution $F_0$, is
considered. The sensor is allowed to take one observation at a time. It has
been shown in a recent work that if each observation comes from one sequence,
Cumulative Sum (CUSUM) test is optimal. In this paper, we propose a new
approach in which each observation can be a linear combination of samples from
multiple sequences. The test has two stages. In the first stage, namely
scanning stage, one takes a linear combination of a pair of sequences with the
hope of scanning through sequences that are unlikely to be generated from $F_1$
and quickly identifying a pair of sequences such that at least one of them is
highly likely to be generated by $F_1$. In the second stage, namely refinement
stage, one examines the pair identified from the first stage more closely and
picks one sequence to be the final sequence. The problem under this setup
belongs to a class of multiple stopping time problems. In particular, it is an
ordered two concatenated Markov stopping time problem. We obtain the optimal
solution using the tools from the multiple stopping time theory. Numerical
simulation results show that this search strategy can significantly reduce the
searching time, especially when $F_{1}$ is rare.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3828</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3828</id><created>2013-02-15</created><authors><author><keyname>Clementi</keyname><forenames>Andrea</forenames></author><author><keyname>Crescenzi</keyname><forenames>Pierluigi</forenames></author><author><keyname>Doerr</keyname><forenames>Carola</forenames></author><author><keyname>Fraigniaud</keyname><forenames>Pierre</forenames></author><author><keyname>Isopi</keyname><forenames>Marco</forenames></author><author><keyname>Panconesi</keyname><forenames>Alessandro</forenames></author><author><keyname>Pasquale</keyname><forenames>Francesco</forenames></author><author><keyname>Silvestri</keyname><forenames>Riccardo</forenames></author></authors><title>Rumor Spreading in Random Evolving Graphs</title><categories>cs.DM cs.DC cs.SI math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Randomized gossip is one of the most popular way of disseminating information
in large scale networks. This method is appreciated for its simplicity,
robustness, and efficiency. In the &quot;push&quot; protocol, every informed node
selects, at every time step (a.k.a. round), one of its neighboring node
uniformly at random and forwards the information to this node. This protocol is
known to complete information spreading in $O(\log n)$ time steps with high
probability (w.h.p.) in several families of $n$-node &quot;static&quot; networks. The
Push protocol has also been empirically shown to perform well in practice, and,
specifically, to be robust against dynamic topological changes.
  In this paper, we aim at analyzing the Push protocol in &quot;dynamic&quot; networks.
We consider the &quot;edge-Markovian&quot; evolving graph model which captures natural
temporal dependencies between the structure of the network at time $t$, and the
one at time $t+1$. Precisely, a non-edge appears with probability $p$, while an
existing edge dies with probability $q$. In order to fit with real-world
traces, we mostly concentrate our study on the case where $p=\Omega(1/n)$ and
$q$ is constant. We prove that, in this realistic scenario, the Push protocol
does perform well, completing information spreading in $O(\log n)$ time steps
w.h.p. Note that this performance holds even when the network is, w.h.p.,
disconnected at every time step (e.g., when $p &lt;&lt; (\log n) / n$). Our result
provides the first formal argument demonstrating the robustness of the Push
protocol against network changes. We also address other ranges of parameters
$p$ and $q$ (e.g., $p+q=1$ with arbitrary $p$ and $q$, and $p=1/n$ with
arbitrary $q$). Although they do not precisely fit with the measures performed
on real-world traces, they can be of independent interest for other settings.
The results in these cases confirm the positive impact of dynamism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3831</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3831</id><created>2013-02-15</created><updated>2013-08-15</updated><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>Sozzo</keyname><forenames>Sandro</forenames></author></authors><title>Quantum Entanglement in Concept Combinations</title><categories>cs.AI cs.CL quant-ph</categories><comments>16 pages, no figures</comments><journal-ref>International Journal of Theoretical Physics, 53, pp. 3587-3603,
  2014</journal-ref><doi>10.1007/s10773-013-1946-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research in the application of quantum structures to cognitive science
confirms that these structures quite systematically appear in the dynamics of
concepts and their combinations and quantum-based models faithfully represent
experimental data of situations where classical approaches are problematical.
In this paper, we analyze the data we collected in an experiment on a specific
conceptual combination, showing that Bell's inequalities are violated in the
experiment. We present a new refined entanglement scheme to model these data
within standard quantum theory rules, where 'entangled measurements and
entangled evolutions' occur, in addition to the expected 'entangled states',
and present a full quantum representation in complex Hilbert space of the data.
This stronger form of entanglement in measurements and evolutions might have
relevant applications in the foundations of quantum theory, as well as in the
interpretation of nonlocality tests. It could indeed explain some
non-negligible 'anomalies' identified in EPR-Bell experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3834</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3834</id><created>2013-02-15</created><authors><author><keyname>Geng</keyname><forenames>Jun</forenames></author><author><keyname>Lai</keyname><forenames>Lifeng</forenames></author></authors><title>Non-Bayesian Quickest Detection with Stochastic Sample Right Constraints</title><categories>cs.IT math.IT</categories><comments>30 pages, 5 figures</comments><doi>10.1109/TSP.2013.2273442</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the design and analysis of optimal detection scheme
for sensors that are deployed to monitor the change in the environment and are
powered by the energy harvested from the environment. In this type of
applications, detection delay is of paramount importance. We model this problem
as quickest change detection problem with a stochastic energy constraint. In
particular, a wireless sensor powered by renewable energy takes observations
from a random sequence, whose distribution will change at a certain unknown
time. Such a change implies events of interest. The energy in the sensor is
consumed by taking observations and is replenished randomly. The sensor cannot
take observations if there is no energy left in the battery. Our goal is to
design a power allocation scheme and a detection strategy to minimize the worst
case detection delay, which is the difference between the time when an alarm is
raised and the time when the change occurs. Two types of average run length
(ARL) constraint, namely an algorithm level ARL constraint and an system level
ARL constraint, are considered. We propose a low complexity scheme in which the
energy allocation rule is to spend energy to take observations as long as the
battery is not empty and the detection scheme is the Cumulative Sum test. We
show that this scheme is optimal for the formulation with the algorithm level
ARL constraint and is asymptotically optimal for the formulations with the
system level ARL constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3844</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3844</id><created>2013-02-15</created><updated>2014-11-14</updated><authors><author><keyname>Charlier</keyname><forenames>&#xc9;milie</forenames></author><author><keyname>Kamae</keyname><forenames>Teturo</forenames></author><author><keyname>Puzynina</keyname><forenames>Svetlana</forenames></author><author><keyname>Zamboni</keyname><forenames>Luca Q.</forenames></author></authors><title>Infinite Self-Shuffling Words</title><categories>math.CO cs.DM</categories><msc-class>68R15</msc-class><journal-ref>J. Comb. Theory, Ser. A 128: 1-40 (2014)</journal-ref><doi>10.1016/j.jcta.2014.07.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce and study a new property of infinite words: An
infinite word $x\in A^\mathbb{N}$, with values in a finite set $A$, is said to
be $k$-self-shuffling $(k\geq 2)$ if $x$ admits factorizations:
$x=\prod_{i=0}^\infty U_i^{(1)}\cdots U_i^{(k)}=\prod_{i=0}^\infty
U_i^{(1)}=\cdots =\prod_{i=0}^\infty U_i^{(k)}$. In other words, there exists a
shuffle of $k$-copies of $x$ which produces $x$. We are particularly interested
in the case $k=2$, in which case we say $x$ is self-shuffling. This property of
infinite words is shown to be an intrinsic property of the word and not of its
language (set of factors). For instance, every aperiodic word contains a non
self-shuffling word in its shift orbit closure. While the property of being
self-shuffling is a relatively strong condition, many important words arising
in the area of symbolic dynamics are verified to be self-shuffling. They
include for instance the Thue-Morse word and all Sturmian words of intercept
$0&lt;\rho &lt;1$ (while those of intercept $\rho=0$ are not self-shuffling). Our
characterization of self-shuffling Sturmian words can be interpreted
arithmetically in terms of a dynamical embedding and defines an arithmetic
process we call the {\it stepping stone model}. One important feature of
self-shuffling words stems from its morphic invariance, which provides a useful
tool for showing that one word is not the morphic image of another. The notion
of self-shuffling has other unexpected applications particularly in the area of
substitutive dynamical systems. For example, as a consequence of our
characterization of self-shuffling Sturmian words, we recover a number
theoretic result, originally due to Yasutomi, on a classification of pure
morphic Sturmian words in the orbit of the characteristic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3857</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3857</id><created>2013-02-15</created><authors><author><keyname>Dames</keyname><forenames>Philip</forenames></author><author><keyname>Kumar</keyname><forenames>Vijay</forenames></author></authors><title>Technical Report: Cooperative Multi-Target Localization With Noisy
  Sensors</title><categories>cs.RO cs.MA</categories><comments>Extended version of paper accepted to 2013 IEEE International
  Conference on Robotics and Automation (ICRA)</comments><doi>10.1109/ICRA.2013.6630825</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical report is an extended version of the paper 'Cooperative
Multi-Target Localization With Noisy Sensors' accepted to the 2013 IEEE
International Conference on Robotics and Automation (ICRA).
  This paper addresses the task of searching for an unknown number of static
targets within a known obstacle map using a team of mobile robots equipped with
noisy, limited field-of-view sensors. Such sensors may fail to detect a subset
of the visible targets or return false positive detections. These measurement
sets are used to localize the targets using the Probability Hypothesis Density,
or PHD, filter. Robots communicate with each other on a local peer-to-peer
basis and with a server or the cloud via access points, exchanging measurements
and poses to update their belief about the targets and plan future actions. The
server provides a mechanism to collect and synthesize information from all
robots and to share the global, albeit time-delayed, belief state to robots
near access points. We design a decentralized control scheme that exploits this
communication architecture and the PHD representation of the belief state.
Specifically, robots move to maximize mutual information between the target set
and measurements, both self-collected and those available by accessing the
server, balancing local exploration with sharing knowledge across the team.
Furthermore, robots coordinate their actions with other robots exploring the
same local region of the environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3860</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3860</id><created>2013-02-15</created><authors><author><keyname>Trencs&#xe9;ni</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>Gazs&#xf3;</keyname><forenames>Attila</forenames></author></authors><title>ScalienDB: Designing and Implementing a Distributed Database using Paxos</title><categories>cs.DB cs.DC</categories><comments>18 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ScalienDB is a scalable, replicated database built on top of the Paxos
algorithm. It was developed from 2010 to 2012, when the startup backing it
failed. This paper discusses the design decisions of the distributed database,
describes interesting parts of the C++ codebase and enumerates lessons learned
putting ScalienDB into production at a handful of clients. The source code is
available on Github under the AGPL license, but it is no longer developed or
maintained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3862</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3862</id><created>2013-02-15</created><authors><author><keyname>Te&#xf3;filo</keyname><forenames>Lu&#xed;s Filipe</forenames></author><author><keyname>Nogueira</keyname><forenames>Pedro Alves</forenames></author><author><keyname>Silva</keyname><forenames>Pedro Brand&#xe3;o</forenames></author></authors><title>GEMINI: A Generic Multi-Modal Natural Interface Framework for Videogames</title><categories>cs.HC</categories><comments>WorldCIST'13 Internacional Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years videogame companies have recognized the role of player
engagement as a major factor in user experience and enjoyment. This encouraged
a greater investment in new types of game controllers such as the WiiMote, Rock
Band instruments and the Kinect. However, the native software of these
controllers was not originally designed to be used in other game applications.
This work addresses this issue by building a middleware framework, which maps
body poses or voice commands to actions in any game. This not only warrants a
more natural and customized user-experience but it also defines an
interoperable virtual controller. In this version of the framework, body poses
and voice commands are respectively recognized through the Kinect's built-in
cameras and microphones. The acquired data is then translated into the native
interaction scheme in real time using a lightweight method based on spatial
restrictions. The system is also prepared to use Nintendo's Wiimote as an
auxiliary and unobtrusive gamepad for physically or verbally impractical
commands. System validation was performed by analyzing the performance of
certain tasks and examining user reports. Both confirmed this approach as a
practical and alluring alternative to the game's native interaction scheme. In
sum, this framework provides a game-controlling tool that is totally
customizable and very flexible, thus expanding the market of game consumers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3868</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3868</id><created>2013-02-15</created><authors><author><keyname>Zamani</keyname><forenames>Majid</forenames></author><author><keyname>Esfahani</keyname><forenames>Peyman Mohajerin</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author><author><keyname>Abate</keyname><forenames>Alessandro</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>Symbolic control of stochastic systems via approximately bisimilar
  finite abstractions</title><categories>math.OC cs.SY</categories><comments>27 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symbolic approaches to the control design over complex systems employ the
construction of finite-state models that are related to the original control
systems, then use techniques from finite-state synthesis to compute controllers
satisfying specifications given in a temporal logic, and finally translate the
synthesized schemes back as controllers for the concrete complex systems. Such
approaches have been successfully developed and implemented for the synthesis
of controllers over non-probabilistic control systems. In this paper, we extend
the technique to probabilistic control systems modeled by controlled stochastic
differential equations. We show that for every stochastic control system
satisfying a probabilistic variant of incremental input-to-state stability, and
for every given precision $\varepsilon&gt;0$, a finite-state transition system can
be constructed, which is $\varepsilon$-approximately bisimilar (in the sense of
moments) to the original stochastic control system. Moreover, we provide
results relating stochastic control systems to their corresponding finite-state
transition systems in terms of probabilistic bisimulation relations known in
the literature. We demonstrate the effectiveness of the construction by
synthesizing controllers for stochastic control systems over rich
specifications expressed in linear temporal logic. The discussed technique
enables a new, automated, correct-by-construction controller synthesis approach
for stochastic control systems, which are common mathematical models employed
in many safety critical systems subject to structured uncertainty and are thus
relevant for cyber-physical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3876</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3876</id><created>2013-02-15</created><updated>2015-02-01</updated><authors><author><keyname>Nino-Ruiz</keyname><forenames>Elias D.</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author><author><keyname>Anderson</keyname><forenames>Jeffrey</forenames></author></authors><title>An Efficient Implementation of the Ensemble Kalman Filter Based on an
  Iterative Sherman-Morrison Formula</title><categories>cs.NA</categories><msc-class>15A09, 65C05, 68R01</msc-class><acm-class>F.2.1; I.6.8; G.3</acm-class><journal-ref>Statistics and Computing, ISSN:0960-3174, PP: 1-17, Feb 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a practical implementation of the ensemble Kalman (EnKF) filter
based on an iterative Sherman-Morrison formula. The new direct method exploits
the special structure of the ensemble-estimated error covariance matrices in
order to efficiently solve the linear systems involved in the analysis step of
the EnKF. The computational complexity of the proposed implementation is
equivalent to that of the best EnKF implementations available in the literature
when the number of observations is much larger than the number of ensemble
members. Even when this conditions is not fulfilled, the proposed method is
expected to perform well since it does not employ matrix decompositions.
Computational experiments using the Lorenz 96 and the oceanic quasi-geostrophic
models are performed in order to compare the proposed algorithm with EnKF
implementations that use matrix decompositions. In terms of accuracy, the
results of all implementations are similar. The proposed method is considerably
faster than other EnKF variants, even when the number of observations is large
relative to the number of ensemble members.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3889</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3889</id><created>2013-02-15</created><authors><author><keyname>Karbasioun</keyname><forenames>Mohammad M.</forenames></author><author><keyname>Shaikhet</keyname><forenames>Gennady</forenames></author><author><keyname>Kranakis</keyname><forenames>Evangelos</forenames></author><author><keyname>Lambadaris</keyname><forenames>Ioannis</forenames></author></authors><title>Power Strip Packing of Malleable Demands in Smart Grid</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a problem of supplying electricity to a set of $\mathcal{N}$
customers in a smart-grid framework. Each customer requires a certain amount of
electrical energy which has to be supplied during the time interval $[0,1]$. We
assume that each demand has to be supplied without interruption, with possible
duration between $\ell$ and $r$, which are given system parameters ($\ell\le
r$). At each moment of time, the power of the grid is the sum of all the
consumption rates for the demands being supplied at that moment. Our goal is to
find an assignment that minimizes the {\it power peak} - maximal power over
$[0,1]$ - while satisfying all the demands. To do this first we find the lower
bound of optimal power peak. We show that the problem depends on whether or not
the pair $\ell, r$ belongs to a &quot;good&quot; region $\mathcal{G}$. If it does - then
an optimal assignment almost perfectly &quot;fills&quot; the rectangle $time \times power
= [0,1] \times [0, A]$ with $A$ being the sum of all the energy demands - thus
achieving an optimal power peak $A$. Conversely, if $\ell, r$ do not belong to
$\mathcal{G}$, we identify the lower bound $\bar{A} &gt;A$ on the optimal value of
power peak and introduce a simple linear time algorithm that almost perfectly
arranges all the demands in a rectangle $[0, A /\bar{A}] \times [0, \bar{A}]$
and show that it is asymptotically optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3892</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3892</id><created>2013-02-15</created><authors><author><keyname>Altmann</keyname><forenames>Eduardo G.</forenames></author><author><keyname>Whichard</keyname><forenames>Zakary L.</forenames></author><author><keyname>Motter</keyname><forenames>Adilson E.</forenames></author></authors><title>Identifying trends in word frequency dynamics</title><categories>physics.soc-ph cond-mat.dis-nn cs.CL q-bio.PE</categories><journal-ref>J. Stat. Phys. 151, p. 277 (2013)</journal-ref><doi>10.1007/s10955-013-0699-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The word-stock of a language is a complex dynamical system in which words can
be created, evolve, and become extinct. Even more dynamic are the short-term
fluctuations in word usage by individuals in a population. Building on the
recent demonstration that word niche is a strong determinant of future rise or
fall in word frequency, here we introduce a model that allows us to distinguish
persistent from temporary increases in frequency. Our model is illustrated
using a 10^8-word database from an online discussion group and a 10^11-word
collection of digitized books. The model reveals a strong relation between
changes in word dissemination and changes in frequency. Aside from their
implications for short-term word frequency dynamics, these observations are
potentially important for language evolution as new words must survive in the
short term in order to survive in the long term.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3894</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3894</id><created>2013-02-15</created><authors><author><keyname>Funke</keyname><forenames>S. W.</forenames></author><author><keyname>Farrell</keyname><forenames>P. E.</forenames></author></authors><title>A framework for automated PDE-constrained optimisation</title><categories>cs.MS</categories><acm-class>G.4; G.1.8; G.1.6; I.6.5; J.2; J.6; D.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A generic framework for the solution of PDE-constrained optimisation problems
based on the FEniCS system is presented. Its main features are an intuitive
mathematical interface, a high degree of automation, and an efficient
implementation of the generated adjoint model. The framework is based upon the
extension of a domain-specific language for variational problems to cleanly
express complex optimisation problems in a compact, high-level syntax. For
example, optimisation problems constrained by the time-dependent Navier-Stokes
equations can be written in tens of lines of code. Based on this high-level
representation, the framework derives the associated adjoint equations in the
same domain-specific language, and uses the FEniCS code generation technology
to emit parallel optimised low-level C++ code for the solution of the forward
and adjoint systems. The functional and gradient information so computed is
then passed to the optimisation algorithm to update the parameter values. This
approach works both for steady-state as well as transient, and for linear as
well as nonlinear governing PDEs and a wide range of functionals and control
parameters. We demonstrate the applicability and efficiency of this approach on
classical textbook optimisation problems and advanced examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3900</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3900</id><created>2013-02-15</created><authors><author><keyname>Graf</keyname><forenames>Franz</forenames></author><author><keyname>Kriegel</keyname><forenames>Hans-Peter</forenames></author><author><keyname>Weiler</keyname><forenames>Michael</forenames></author></authors><title>Robust Image Segmentation in Low Depth Of Field Images</title><categories>cs.CV</categories><comments>Extended Version of the short paper published in &quot;Robust Image
  Segmentation in Low Depth Of Field Images&quot;, IEEE International Conference on
  Image Processing 2011 (ICIP). The paper contains a lot more details about the
  algorithm and more evaluation</comments><journal-ref>Extended Version of &quot;Robust Image Segmentation in Low Depth Of
  Field Images&quot;, IEEE International Conference on Image Processing 2011 (ICIP)</journal-ref><doi>10.1109/TIP.2005.846030</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In photography, low depth of field (DOF) is an important technique to
emphasize the object of interest (OOI) within an image. Thus, low DOF images
are widely used in the application area of macro, portrait or sports
photography. When viewing a low DOF image, the viewer implicitly concentrates
on the regions that are sharper regions of the image and thus segments the
image into regions of interest and non regions of interest which has a major
impact on the perception of the image. Thus, a robust algorithm for the fully
automatic detection of the OOI in low DOF images provides valuable information
for subsequent image processing and image retrieval. In this paper we propose a
robust and parameterless algorithm for the fully automatic segmentation of low
DOF images. We compare our method with three similar methods and show the
superior robustness even though our algorithm does not require any parameters
to be set by hand. The experiments are conducted on a real world data set with
high and low DOF images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3901</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3901</id><created>2013-02-15</created><updated>2013-04-12</updated><authors><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author></authors><title>Enhanced secure key exchange systems based on the Johnson-noise scheme</title><categories>cs.CR cs.ET</categories><comments>This version is accepted for publication</comments><journal-ref>Metrology and Measurement Systems 20 (2013) 191-204 (open access)</journal-ref><doi>10.2478/mms-2013-0017</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We introduce seven new versions of the Kirchhoff-Law-Johnson-(like)-Noise
(KLJN) classical physical secure key exchange scheme and a new transient
protocol for practically-perfect security. While these practical improvements
offer progressively enhanced security and/or speed for the non-ideal
conditions, the fundamental physical laws providing the security remain the
same.
  In the &quot;intelligent&quot; KLJN (iKLJN) scheme, Alice and Bob utilize the fact that
they exactly know not only their own resistor value but also the stochastic
time function of their own noise, which they generate before feeding it into
the loop.
  In the &quot;multiple&quot; KLJN (MKLJN) system, Alice and Bob have publicly known
identical sets of different resistors with a proper, publicly known truth table
about the bit-interpretation of their combination. In the &quot;keyed&quot; KLJN (KKLJN)
system, by using secure communication with a formerly shared key, Alice and Bob
share a proper time-dependent truth table for the bit-interpretation of the
resistor situation for each secure bit exchange step during generating the next
key.
  The remaining four KLJN schemes are the combinations of the above protocols
to synergically enhance the security properties. These are: the
&quot;intelligent-multiple&quot; (iMKLJN), the &quot;intelligent-keyed&quot; (iKKLJN), the
&quot;keyed-multiple&quot; (KMKLJN) and the &quot;intelligent-keyed-multiple&quot; (iKMKLJN) KLJN
key exchange systems.
  Finally, we introduce a new transient-protocol offering practically-perfect
security without privacy amplification, which is not needed at practical
applications but it is shown for the sake of ongoing discussions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3906</identifier>
 <datestamp>2013-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3906</id><created>2013-02-15</created><updated>2013-05-22</updated><authors><author><keyname>Brzozowski</keyname><forenames>Janusz</forenames></author><author><keyname>Davies</keyname><forenames>Gareth</forenames></author></authors><title>Maximal Syntactic Complexity of Regular Languages Implies Maximal
  Quotient Complexities of Atoms</title><categories>cs.FL</categories><comments>12 pages, 2 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We relate two measures of complexity of regular languages. The first is
syntactic complexity, that is, the cardinality of the syntactic semigroup of
the language. That semigroup is isomorphic to the semigroup of transformations
of states induced by non-empty words in the minimal deterministic finite
automaton accepting the language. If the language has n left quotients (its
minimal automaton has n states), then its syntactic complexity is at most n^n
and this bound is tight. The second measure consists of the quotient (state)
complexities of the atoms of the language, where atoms are non-empty
intersections of complemented and uncomplemented quotients. A regular language
has at most 2^n atoms and this bound is tight. The maximal quotient complexity
of any atom with r complemented quotients is 2^n-1, if r=0 or r=n, and
1+\sum_{k=1}^{r} \sum_{h=k+1}^{k+n-r} \binom{h}{n} \binom{k}{h}, otherwise. We
prove that if a language has maximal syntactic complexity, then it has 2^n
atoms and each atom has maximal quotient complexity, but the converse is false.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3912</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3912</id><created>2013-02-15</created><authors><author><keyname>Davies</keyname><forenames>Todd</forenames></author><author><keyname>O'Connor</keyname><forenames>Brendan</forenames></author><author><keyname>Cochran</keyname><forenames>Alex</forenames></author><author><keyname>Effrat</keyname><forenames>Jonathan J.</forenames></author><author><keyname>Parker</keyname><forenames>Andrew</forenames></author><author><keyname>Newman</keyname><forenames>Benjamin</forenames></author><author><keyname>Tam</keyname><forenames>Aaron</forenames></author></authors><title>An Online Environment for Democratic Deliberation: Motivations,
  Principles, and Design</title><categories>cs.HC cs.CY cs.SI</categories><comments>Appeared in Todd Davies and Seeta Pe\~na Gangadharan (Editors),
  Online Deliberation: Design, Research, and Practice, CSLI
  Publications/University of Chicago Press, October 2009, pp. 275-292; 18
  pages, 3 figures</comments><acm-class>H.5.3; K.4.1; K.4.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We have created a platform for online deliberation called Deme (which rhymes
with 'team'). Deme is designed to allow groups of people to engage in
collaborative drafting, focused discussion, and decision making using the
Internet. The Deme project has evolved greatly from its beginning in 2003. This
chapter outlines the thinking behind Deme's initial design: our motivations for
creating it, the principles that guided its construction, and its most
important design features. The version of Deme described here was written in
PHP and was deployed in 2004 and used by several groups (including organizers
of the 2005 Online Deliberation Conference). Other papers describe later
developments in the Deme project (see Davies et al. 2005, 2008; Davies and
Mintz 2009).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3917</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3917</id><created>2013-02-15</created><authors><author><keyname>Ebeida</keyname><forenames>Mohamed S.</forenames></author><author><keyname>Patney</keyname><forenames>Anjul</forenames></author><author><keyname>Mitchell</keyname><forenames>Scott A.</forenames></author><author><keyname>Dalbey</keyname><forenames>Keith R.</forenames></author><author><keyname>Davidson</keyname><forenames>Andrew A.</forenames></author><author><keyname>Owens</keyname><forenames>John D.</forenames></author></authors><title>k-d Darts: Sampling by k-Dimensional Flat Searches</title><categories>cs.GR</categories><comments>19 pages 16 figures</comments><acm-class>I.3.5</acm-class><journal-ref>Transactions on Graphics, vol. 33, no. 1 (Jan 2014) pp. 3:1--3:16</journal-ref><doi>10.1145/2522528</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formalize the notion of sampling a function using k-d darts. A k-d dart is
a set of independent, mutually orthogonal, k-dimensional subspaces called k-d
flats. Each dart has d choose k flats, aligned with the coordinate axes for
efficiency. We show that k-d darts are useful for exploring a function's
properties, such as estimating its integral, or finding an exemplar above a
threshold. We describe a recipe for converting an algorithm from point sampling
to k-d dart sampling, assuming the function can be evaluated along a k-d flat.
  We demonstrate that k-d darts are more efficient than point-wise samples in
high dimensions, depending on the characteristics of the sampling domain: e.g.
the subregion of interest has small volume and evaluating the function along a
flat is not too expensive. We present three concrete applications using line
darts (1-d darts): relaxed maximal Poisson-disk sampling, high-quality
rasterization of depth-of-field blur, and estimation of the probability of
failure from a response surface for uncertainty quantification. In these
applications, line darts achieve the same fidelity output as point darts in
less time. We also demonstrate the accuracy of higher dimensional darts for a
volume estimation problem. For Poisson-disk sampling, we use significantly less
memory, enabling the generation of larger point clouds in higher dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3918</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3918</id><created>2013-02-15</created><updated>2013-06-10</updated><authors><author><keyname>Divekar</keyname><forenames>Atul</forenames></author><author><keyname>Needell</keyname><forenames>Deanna</forenames></author></authors><title>Using Correlated Subset Structure for Compressive Sensing Recovery</title><categories>cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing is a methodology for the reconstruction of sparse or
compressible signals using far fewer samples than required by the Nyquist
criterion. However, many of the results in compressive sensing concern random
sampling matrices such as Gaussian and Bernoulli matrices. In common physically
feasible signal acquisition and reconstruction scenarios such as
super-resolution of images, the sensing matrix has a non-random structure with
highly correlated columns. Here we present a compressive sensing recovery
algorithm that exploits this correlation structure. We provide algorithmic
justification as well as empirical comparisons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3921</identifier>
 <datestamp>2013-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3921</id><created>2013-02-15</created><updated>2013-05-31</updated><authors><author><keyname>Fernandez-Granda</keyname><forenames>Carlos</forenames></author></authors><title>Support detection in super-resolution</title><categories>cs.IT math.IT math.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of super-resolving a superposition of point sources from
noisy low-pass data with a cut-off frequency f. Solving a tractable convex
program is shown to locate the elements of the support with high precision as
long as they are separated by 2/f and the noise level is small with respect to
the amplitude of the signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3931</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3931</id><created>2013-02-16</created><updated>2013-10-09</updated><authors><author><keyname>Zhao</keyname><forenames>Xiaozhao</forenames></author><author><keyname>Hou</keyname><forenames>Yuexian</forenames></author><author><keyname>Yu</keyname><forenames>Qian</forenames></author><author><keyname>Song</keyname><forenames>Dawei</forenames></author><author><keyname>Li</keyname><forenames>Wenjie</forenames></author></authors><title>Understanding Boltzmann Machine and Deep Learning via A Confident
  Information First Principle</title><categories>cs.NE cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Typical dimensionality reduction methods focus on directly reducing the
number of random variables while retaining maximal variations in the data. In
this paper, we consider the dimensionality reduction in parameter spaces of
binary multivariate distributions. We propose a general
Confident-Information-First (CIF) principle to maximally preserve parameters
with confident estimates and rule out unreliable or noisy parameters. Formally,
the confidence of a parameter can be assessed by its Fisher information, which
establishes a connection with the inverse variance of any unbiased estimate for
the parameter via the Cram\'{e}r-Rao bound. We then revisit Boltzmann machines
(BM) and theoretically show that both single-layer BM without hidden units
(SBM) and restricted BM (RBM) can be solidly derived using the CIF principle.
This can not only help us uncover and formalize the essential parts of the
target density that SBM and RBM capture, but also suggest that the deep neural
network consisting of several layers of RBM can be seen as the layer-wise
application of CIF. Guided by the theoretical analysis, we develop a
sample-specific CIF-based contrastive divergence (CD-CIF) algorithm for SBM and
a CIF-based iterative projection procedure (IP) for RBM. Both CD-CIF and IP are
studied in a series of density estimation experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3932</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3932</id><created>2013-02-16</created><authors><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author><author><keyname>Alizadeh</keyname><forenames>Mahnoosh</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author></authors><title>Real-Time Power Balancing via Decentralized Coordinated Home Energy
  Scheduling</title><categories>cs.SY cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Smart Grid</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  It is anticipated that an uncoordinated operation of individual home energy
management (HEM) systems in a neighborhood would have a rebound effect on the
aggregate demand profile. To address this issue, this paper proposes a
coordinated home energy management (CoHEM) architecture in which distributed
HEM units collaborate with each other in order to keep the demand and supply
balanced in their neighborhood. Assuming the energy requests by customers are
random in time, we formulate the proposed CoHEM design as a multi-stage
stochastic optimization problem. We propose novel models to describe the
deferrable appliance load (e.g., Plug-in (Hybrid) Electric Vehicles (PHEV)),
and apply approximation and decomposition techniques to handle the considered
design problem in a decentralized fashion. The developed decentralized CoHEM
algorithm allow the customers to locally compute their scheduling solutions
using domestic user information and with message exchange between their
neighbors only. Extensive simulation results demonstrate that the proposed
CoHEM architecture can effectively improve real-time power balancing.
Extensions to joint power procurement and real-time CoHEM scheduling are also
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3946</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3946</id><created>2013-02-16</created><authors><author><keyname>Chen</keyname><forenames>Lin</forenames></author><author><keyname>Ye</keyname><forenames>Deshi</forenames></author><author><keyname>Zhang</keyname><forenames>Guochuan</forenames></author></authors><title>Approximating the optimal competitive ratio for an ancient online
  scheduling problem</title><categories>cs.DS</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classical online scheduling problem P||C_{max} in which jobs
are released over list and provide a nearly optimal online algorithm. More
precisely, an online algorithm whose competitive ratio is at most (1+\epsilon)
times that of an optimal online algorithm could be achieved in polynomial time,
where m, the number of machines, is a part of the input. It substantially
improves upon the previous results by almost closing the gap between the
currently best known lower bound of 1.88 (Rudin, Ph.D thesis, 2001) and the
best known upper bound of 1.92 (Fleischer, Wahl, Journal of Scheduling, 2000).
It has been known by folklore that an online problem could be viewed as a game
between an adversary and the online player. Our approach extensively explores
such a structure and builds up a completely new framework to show that, for the
online over list scheduling problem, given any \epsilon&gt;0, there exists a
uniform threshold K which is polynomial in m such that if the competitive ratio
of an online algorithm is \rho&lt;=2, then there exists a list of at most K jobs
to enforce the online algorithm to achieve a competitive ratio of at least
\rho-O(\epsilon). Our approach is substantially different from that of Gunther
et al. (Gunther et al., SODA 2013), in which an approximation scheme for online
over time scheduling problems is given, where the number of machines is fixed.
Our method could also be extended to several related online over list
scheduling models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3949</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3949</id><created>2013-02-16</created><updated>2013-06-21</updated><authors><author><keyname>Nishi</keyname><forenames>Ryosuke</forenames></author><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>A collective opinion formation model under Bayesian updating and
  confirmation bias</title><categories>physics.soc-ph cs.SI</categories><comments>23 pages, 7 figures</comments><journal-ref>Physical Review E 87, 062123 (2013)</journal-ref><doi>10.1103/PhysRevE.87.062123</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a collective opinion formation model with a so-called confirmation
bias. The confirmation bias is a psychological effect with which, in the
context of opinion formation, an individual in favor of an opinion is prone to
misperceive new incoming information as supporting the current belief of the
individual. Our model modifies a Bayesian decision-making model for single
individuals [M. Rabin and J. L. Schrag, Q. J. Econ. 114, 37 (1999)] for the
case of a well-mixed population of interacting individuals in the absence of
the external input. We numerically simulate the model to show that all the
agents eventually agree on one of the two opinions only when the confirmation
bias is weak. Otherwise, the stochastic population dynamics ends up creating a
disagreement configuration (also called polarization), particularly for large
system sizes. A strong confirmation bias allows various final disagreement
configurations with different fractions of the individuals in favor of the
opposite opinions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3956</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3956</id><created>2013-02-16</created><authors><author><keyname>Namayandeh</keyname><forenames>Raheleh</forenames></author><author><keyname>Didehvar</keyname><forenames>Farzad</forenames></author><author><keyname>Shojaei</keyname><forenames>Zahra</forenames></author></authors><title>Clustering validity based on the most similarity</title><categories>cs.LG stat.ML</categories><comments>4 pages,2 figueres, 6 tables</comments><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One basic requirement of many studies is the necessity of classifying data.
Clustering is a proposed method for summarizing networks. Clustering methods
can be divided into two categories named model-based approaches and algorithmic
approaches. Since the most of clustering methods depend on their input
parameters, it is important to evaluate the result of a clustering algorithm
with its different input parameters, to choose the most appropriate one. There
are several clustering validity techniques based on inner density and outer
density of clusters that represent different metrics to choose the most
appropriate clustering independent of the input parameters. According to
dependency of previous methods on the input parameters, one challenge in facing
with large systems, is to complete data incrementally that effects on the final
choice of the most appropriate clustering. Those methods define the existence
of high intensity in a cluster, and low intensity among different clusters as
the measure of choosing the optimal clustering. This measure has a tremendous
problem, not availing all data at the first stage. In this paper, we introduce
an efficient measure in which maximum number of repetitions for various initial
values occurs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3969</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3969</id><created>2013-02-16</created><authors><author><keyname>Yang</keyname><forenames>Hong-yong</forenames></author><author><keyname>Guo</keyname><forenames>Lei</forenames></author><author><keyname>Zhu</keyname><forenames>Xun-lin</forenames></author><author><keyname>Cao</keyname><forenames>Ke-cai</forenames></author></authors><title>Coordination Control of Heterogeneous Compounded-Order Multi-Agent
  Systems with Communication Delays</title><categories>cs.SY</categories><comments>15pages, 4figures</comments><msc-class>93C85(Primary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the complexity of the practical environment, many distributed networked
systems can not be illustrated with the integer-order dynamics and only be
described as the fractional-order dynamics. Suppose multi-agent systems will
show the individual diversity with difference agents, where the heterogeneous
(integer-order and fractional-order) dynamics are used to illustrate the agent
systems and compose integer-fractional compounded-order systems. Applying
Laplace transform and frequency domain theory of the fractional-order operator,
consensus of delayed multi-agent systems with directed weighted topologies is
studied. Since integer-order model is the special case of fractional-order
model, the results in this paper can be extend to the systems with
integer-order models. Finally, numerical examples are used to verify our
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3971</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3971</id><created>2013-02-16</created><updated>2015-12-22</updated><authors><author><keyname>Charalambous</keyname><forenames>Charalambos D.</forenames></author><author><keyname>Stavrou</keyname><forenames>Photios A.</forenames></author></authors><title>Directed Information on Abstract Spaces: Properties and Variational
  Equalities</title><categories>cs.IT math.FA math.IT math.OC math.PR</categories><comments>49 pages, 2 figures, Submitted for publication to IEEE Transactions
  on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Directed information or its variants are utilized extensively in the
characterization of the capacity of channels with memory and feedback,
nonanticipative lossy data compression, and their generalizations to networks.
In this paper, we derive several functional and topological properties of
directed information for general abstract alphabets (complete separable metric
spaces) using the topology of weak convergence of probability measures. These
include convexity of the set of consistent distributions, which uniquely define
causally conditioned distributions, convexity and concavity of directed
information with respect to the sets of consistent distributions, weak
compactness of these sets of distributions, their joint distributions and their
marginals. Furthermore, we show lower semicontinuity of directed information,
and under certain conditions we also establish continuity of directed
information. Finally, we derive variational equalities for directed
information, including sequential versions. These may be viewed as the analogue
of the variational equalities of mutual information (utilized in Blahut-Arimoto
algorithm).
  In summary, we extend the basic functional and topological properties of
mutual information to directed information. These properties are discussed in
the context of extremum problems of directed information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3982</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3982</id><created>2013-02-16</created><authors><author><keyname>Chintakunta</keyname><forenames>Harish</forenames></author><author><keyname>Krim</keyname><forenames>Hamid</forenames></author></authors><title>Distributed boundary tracking using alpha and Delaunay-Cech shapes</title><categories>cs.CG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  For a given point set $S$ in a plane, we develop a distributed algorithm to
compute the $\alpha-$shape of $S$. $\alpha-$shapes are well known geometric
objects which generalize the idea of a convex hull, and provide a good
definition for the shape of $S$. We assume that the distances between pairs of
points which are closer than a certain distance $r&gt;0$ are provided, and we show
constructively that this information is sufficient to compute the alpha shapes
for a range of parameters, where the range depends on $r$.
  Such distributed algorithms are very useful in domains such as sensor
networks, where each point represents a sensing node, the location of which is
not necessarily known.
  We also introduce a new geometric object called the Delaunay-\v{C}ech shape,
which is geometrically more appropriate than an $\alpha-$shape for some cases,
and show that it is topologically equivalent to $\alpha-$shapes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.3988</identifier>
 <datestamp>2013-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.3988</id><created>2013-02-16</created><updated>2013-09-09</updated><authors><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author></authors><title>A solution concept for games with altruism and cooperation</title><categories>cs.GT cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the years, numerous experiments have been accumulated to show that
cooperation is not casual and depends on the payoffs of the game. These
findings suggest that humans have attitude to cooperation by nature and the
same person may act more or less cooperatively depending on the particular
payoffs. In other words, people do not act a priori as single agents, but they
forecast how the game would be played if they formed coalitions and then they
play according to their best forecast. In this paper we formalize this idea and
we define a new solution concept for one-shot normal form games. We prove that
this \emph{cooperative equilibrium} exists for all finite games and it explains
a number of different experimental findings, such as (1) the rate of
cooperation in the Prisoner's dilemma depends on the cost-benefit ratio; (2)
the rate of cooperation in the Traveler's dilemma depends on the bonus/penalty;
(3) the rate of cooperation in the Publig Goods game depends on the pro-capite
marginal return and on the numbers of players; (4) the rate of cooperation in
the Bertrand competition depends on the number of players; (5) players tend to
be fair in the bargaining problem; (6) players tend to be fair in the Ultimatum
game; (7) players tend to be altruist in the Dictator game; (8) offers in the
Ultimatum game are larger than offers in the Dictator game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4000</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4000</id><created>2013-02-16</created><updated>2013-02-19</updated><authors><author><keyname>Micha&#x142;</keyname><forenames>Jamr&#xf3;z</forenames></author><author><keyname>Andrzej</keyname><forenames>Koli&#x144;ski</forenames></author></authors><title>ClusCo: clustering and comparison of protein models</title><categories>q-bio.BM cs.CE q-bio.QM</categories><doi>10.1186/1471-2105-14-62</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: The development, optimization and validation of protein modeling
methods require efficient tools for structural comparison. Frequently, a large
number of models need to be compared with the target native structure. The main
reason for the development of Clusco software was to create a high-throughput
tool for all-versus-all comparison, because calculating similarity matrix is
the one of the bottlenecks in the protein modeling pipeline. Results: Clusco is
fast and easy-to-use software for high-throughput comparison of protein models
with different similarity measures (cRMSD, dRMSD, GDT_TS, TM-Score, MaxSub,
Contact Map Overlap) and clustering of the comparison results with standard
methods: K-means Clustering or Hierarchical Agglomerative Clustering.
Conclusions: The application was highly optimized and written in C/C++,
including the code for parallel execution on CPU and GPU version of cRMSD,
which resulted in a significant speedup over similar clustering and scoring
computation programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4006</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4006</id><created>2013-02-16</created><updated>2014-04-15</updated><authors><author><keyname>Andersen</keyname><forenames>Jakob L.</forenames></author><author><keyname>Flamm</keyname><forenames>Christoph</forenames></author><author><keyname>Merkle</keyname><forenames>Daniel</forenames></author><author><keyname>Stadler</keyname><forenames>Peter F.</forenames></author></authors><title>Generic Strategies for Chemical Space Exploration</title><categories>cs.FL q-bio.BM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational approaches to exploring &quot;chemical universes&quot;, i.e., very large
sets, potentially infinite sets of compounds that can be constructed by a
prescribed collection of reaction mechanisms, in practice suffer from a
combinatorial explosion. It quickly becomes impossible to test, for all pairs
of compounds in a rapidly growing network, whether they can react with each
other. More sophisticated and efficient strategies are therefore required to
construct very large chemical reaction networks.
  Undirected labeled graphs and graph rewriting are natural models of chemical
compounds and chemical reactions. Borrowing the idea of partial evaluation from
functional programming, we introduce partial applications of rewrite rules.
Binding substrate to rules increases the number of rules but drastically prunes
the substrate sets to which it might match, resulting in dramatically reduced
resource requirements. At the same time, exploration strategies can be guided,
e.g. based on restrictions on the product molecules to avoid the explicit
enumeration of very unlikely compounds. To this end we introduce here a generic
framework for the specification of exploration strategies in graph-rewriting
systems. Using key examples of complex chemical networks from sugar chemistry
and the realm of metabolic networks we demonstrate the feasibility of a
high-level strategy framework.
  The ideas presented here can not only be used for a strategy-based chemical
space exploration that has close correspondence of experimental results, but
are much more general. In particular, the framework can be used to emulate
higher-level transformation models such as illustrated in a small puzzle game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4009</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4009</id><created>2013-02-16</created><authors><author><keyname>Bjorndahl</keyname><forenames>Adam</forenames></author></authors><title>Subset Space Public Announcement Logic Revisited</title><categories>math.LO cs.LO</categories><comments>13 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By a modification of the central definition given in [W\'ang and {\AA}gotnes,
2013], we provide semantics for public annoucements in subset spaces. We argue
that these revised semantics improve on the original, and we provide a simple
sound and complete axiomatization of the resulting logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4010</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4010</id><created>2013-02-16</created><authors><author><keyname>Li</keyname><forenames>Haoyu</forenames></author><author><keyname>Ma</keyname><forenames>Di</forenames></author><author><keyname>Saxena</keyname><forenames>Nitesh</forenames></author><author><keyname>Shrestha</keyname><forenames>Babins</forenames></author><author><keyname>Zhu</keyname><forenames>Yan</forenames></author></authors><title>Tap-Wave-Rub: Lightweight Malware Prevention for Smartphones Using
  Intuitive Human Gestures</title><categories>cs.CR</categories><comments>13 pages, 4 figures, 2 tables, partial of this extended technical
  report appears as a short paper in ACM WiSec'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a lightweight permission enforcement approach -
Tap-Wave-Rub (TWR) - for smartphone malware prevention. TWR is based on simple
human gestures that are very quick and intuitive but less likely to be
exhibited in users' daily activities. Presence or absence of such gestures,
prior to accessing an application, can effectively inform the OS whether the
access request is benign or malicious. Specifically, we present the design of
two mechanisms: (1) accelerometer based phone tapping detection; and (2)
proximity sensor based finger tapping, rubbing or hand waving detection. The
first mechanism is geared for NFC applications, which usually require the user
to tap her phone with another device. The second mechanism involves very simple
gestures, i.e., tapping or rubbing a finger near the top of phone's screen or
waving a hand close to the phone, and broadly appeals to many applications
(e.g., SMS). In addition, we present the TWR-enhanced Android permission model,
the prototypes implementing the underlying gesture recognition mechanisms, and
a variety of novel experiments to evaluate these mechanisms. Our results
suggest the proposed approach could be very effective for malware detection and
prevention, with quite low false positives and false negatives, while imposing
little to no additional burden on the users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4014</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4014</id><created>2013-02-16</created><updated>2015-08-11</updated><authors><author><keyname>Barmpalias</keyname><forenames>George</forenames></author><author><keyname>Elwes</keyname><forenames>Richard</forenames></author><author><keyname>Lewis-Pye</keyname><forenames>Andy</forenames></author></authors><title>Digital morphogenesis via Schelling segregation</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Schelling's model of segregation looks to explain the way in which particles
or agents of two types may come to arrange themselves spatially into
configurations consisting of large homogeneous clusters, i.e.\ connected
regions consisting of only one type. As one of the earliest agent based models
studied by economists and perhaps the most famous model of self-organising
behaviour, it also has direct links to areas at the interface between computer
science and statistical mechanics, such as the Ising model and the study of
contagion and cascading phenomena in networks.
  While the model has been extensively studied it has largely resisted rigorous
analysis, prior results from the literature generally pertaining to variants of
the model which are tweaked so as to be amenable to standard techniques from
statistical mechanics or stochastic evolutionary game theory. In \cite{BK},
Brandt, Immorlica, Kamath and Kleinberg provided the first rigorous analysis of
the unperturbed model, for a specific set of input parameters. Here we provide
a rigorous analysis of the model's behaviour much more generally and establish
some surprising forms of threshold behaviour, notably the existence of
situations where an \emph{increased} level of intolerance for neighbouring
agents of opposite type leads almost certainly to \emph{decreased} segregation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4016</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4016</id><created>2013-02-16</created><updated>2013-07-06</updated><authors><author><keyname>Kucherov</keyname><forenames>Gregory</forenames></author><author><keyname>Nekrich</keyname><forenames>Yakov</forenames></author></authors><title>Full-fledged Real-Time Indexing for Constant Size Alphabets</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a data structure that supports pattern matching
queries on a dynamically arriving text over an alphabet ofconstant size. Each
new symbol can be prepended to $T$ in O(1) worst-case time. At any moment, we
can report all occurrences of a pattern $P$ in the current text in $O(|P|+k)$
time, where $|P|$ is the length of $P$ and $k$ is the number of occurrences.
This resolves, under assumption of constant-size alphabet, a long-standing open
problem of existence of a real-time indexing method for string matching (see
\cite{AmirN08}).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4019</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4019</id><created>2013-02-16</created><updated>2014-06-13</updated><authors><author><keyname>Tallapragada</keyname><forenames>Pavankumar</forenames></author><author><keyname>Chopra</keyname><forenames>Nikhil</forenames></author></authors><title>Decentralized Event-Triggering for Control of Nonlinear Systems</title><categories>cs.SY math.OC</categories><journal-ref>IEEE Transactions on Automatic Control, vol.59, no.12,
  pp.3312,3324, 2014</journal-ref><doi>10.1109/TAC.2014.2351931</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers nonlinear systems with full state feedback, a central
controller and distributed sensors not co-located with the central controller.
We present a methodology for designing decentralized asynchronous
event-triggers, which utilize only locally available information, for
determining the time instants of transmission from the sensors to the central
controller. The proposed design guarantees a positive lower bound for the
inter-transmission times of each sensor, while ensuring asymptotic stability of
the origin of the system with an arbitrary, but priorly fixed, compact region
of attraction. In the special case of Linear Time Invariant (LTI) systems,
global asymptotic stability is guaranteed and scale invariance of
inter-transmission times is preserved. A modified design method is also
proposed for nonlinear systems, with the addition of event-triggered
communication from the controller to the sensors, that promises to
significantly increase the average sensor inter-transmission times compared to
the case where the controller does not transmit data to the sensors. The
proposed designs are illustrated through simulations of a linear and a
nonlinear example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4020</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4020</id><created>2013-02-16</created><authors><author><keyname>Sun</keyname><forenames>Hua</forenames></author><author><keyname>Geng</keyname><forenames>Chunhua</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>Topological Interference Management with Alternating Connectivity</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The topological interference management problem refers to the study of the
capacity of partially connected linear (wired and wireless) communication
networks with no channel state information at the transmitters (no CSIT) beyond
the network topology, i.e., a knowledge of which channel coefficients are zero
(weaker than the noise floor in the wireless case). While the problem is
originally studied with fixed topology, in this work we explore the
implications of varying connectivity, through a series of simple and
conceptually representative examples. Specifically, we highlight the
synergistic benefits of coding across alternating topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4024</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4024</id><created>2013-02-16</created><updated>2013-02-23</updated><authors><author><keyname>Kim</keyname><forenames>James</forenames></author></authors><title>Note on the Complex Networks and Epidemiology Part I: Complex Networks</title><categories>physics.soc-ph cs.SI nlin.AO q-bio.MN q-bio.PE</categories><comments>arXiv admin note: text overlap with arXiv:cond-mat/0106096 by other
  authors</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Complex networks describe a wide range of systems in nature and society.
Frequently cited examples include Internet, WWW, a network of chemicals linked
by chemical reactions, social relationship networks, citation networks, etc.
The research of complex networks has attracted many scientists' attention.
Physicists have shown that these networks exhibit some surprising characters,
such as high clustering coefficient, small diameter, and the absence of the
thresholds of percolation. Scientists in mathematical epidemiology discovered
that the threshold of infectious disease disappears on contact networks that
following Scale-Free distribution. Researchers in economics and public health
also find that the imitation behavior could lead to cluster phenomena of
vaccination and un-vaccination. In this note, we will review the basic concepts
of complex networks; Basic epidemic models; the development of complex networks
and epidemiology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4030</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4030</id><created>2013-02-16</created><authors><author><keyname>Zhang</keyname><forenames>Jianwei</forenames></author><author><keyname>Xing</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Yongchao</forenames></author><author><keyname>Lu</keyname><forenames>Dongming</forenames></author></authors><title>Modeling and Performance Analysis of Pull-Based Live Streaming Schemes
  in Peer-to-Peer Network</title><categories>cs.NI</categories><journal-ref>Computer Communications, vol. 40, pp. 22-32, Mar. 2014</journal-ref><doi>10.1016/j.comcom.2013.12.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years mesh-based Peer-to-Peer live streaming has become a promising
way for service providers to offer high-quality live video streaming service to
Internet users. In this paper, we make a detailed study on modeling and
performance analysis of the pull-based P2P streaming systems. We establish the
analytical framework for the pull-based streaming schemes in P2P network, give
accurate models of the chunk selection and peer selection strategies, and
organize them into three categories, i.e., the chunk first scheme, the peer
first scheme and the epidemic scheme. Through numerical performance evaluation,
the impacts of some important parameters, such as size of neighbor set, reply
number, buffer size and so on are investigated. For the peer first and chunk
first scheme, we show that the pull-based schemes do not perform as well as the
push-based schemes when peers are limited to reply only one request in each
time slot. When the reply number increases, the pull-based streaming schemes
will reach close to optimal playout probability. As to the pull-based epidemic
scheme, we find it has unexpected poor performance, which is significantly
different from the push-based epidemic scheme. Therefore we propose a simple,
efficient and easily deployed push-pull scheme which can significantly improve
the playout probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4043</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4043</id><created>2013-02-17</created><authors><author><keyname>Akrout</keyname><forenames>Belhassen</forenames></author><author><keyname>Kallel</keyname><forenames>Imen Khanfir</forenames></author><author><keyname>Amar</keyname><forenames>Chokri Ben</forenames></author></authors><title>A new scheme of signature extraction for iris authentication</title><categories>cs.CV</categories><comments>7 pages, 13 figures,International Multi-Conference on Systems Signals
  and Devices</comments><report-no>REGIM-2009-03</report-no><journal-ref>IEEE 6th International Multi-Conference on Systems, Signals and
  Devices. (2009) 1-8</journal-ref><doi>10.1109/SSD.2009.4956749</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iris recognition, a relatively new biometric technology, has great
advantages, such as variability, stability and security, thus is the most
promising for high security environment. Iris recognition is proposed in this
report. We describe some methods, the first one is based on grey level
histogram to extract the pupil, the second is based on elliptic and parabolic
HOUGH transformation to determinate the edge of iris, upper and lower eyelids,
the third we used 2D Gabor Wavelets to encode the iris and finally we used the
Hamming distance for authentication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4046</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4046</id><created>2013-02-17</created><authors><author><keyname>Agarwal</keyname><forenames>Tejaswi</forenames></author><author><keyname>Leonetti</keyname><forenames>Mike A.</forenames></author></authors><title>Design and Implementation of an IP based authentication mechanism for
  Open Source Proxy Servers in Interception Mode</title><categories>cs.NI cs.CR</categories><comments>11 pages, Authenticating Clients in Transparent/ Interception Mode,
  Squid Proxy Server</comments><journal-ref>Advanced Computing: An International Journal, Vol 4, Number 1,
  January 2013</journal-ref><doi>10.5121/acij.2013.4103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proxy servers are being increasingly deployed at organizations for
performance benefits; however, there still exists drawbacks in ease of client
authentication in interception proxy mode mainly for Open Source Proxy Servers.
  Technically, an interception mode is not designed for client authentication,
but implementation in certain organizations does require this feature. In this
paper, we focus on the World Wide Web, highlight the existing transparent proxy
authentication mechanisms, its drawbacks and propose an authentication scheme
for transparent proxy users by using external scripts based on the clients
Internet Protocol Address. This authentication mechanism has been implemented
and verified on Squid-one of the most widely used HTTP Open Source Proxy
Server.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4053</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4053</id><created>2013-02-17</created><authors><author><keyname>Garc&#xed;a</keyname><forenames>J. A.</forenames></author><author><keyname>Rodr&#xed;guez-S&#xe1;nchez</keyname><forenames>Rosa</forenames></author><author><keyname>Fdez-Valdivia</keyname><forenames>J.</forenames></author><author><keyname>Robinson-Garc&#xed;a</keyname><forenames>Nicolas</forenames></author><author><keyname>Torres-Salinas</keyname><forenames>Daniel</forenames></author></authors><title>Mapping Academic Institutions According to Their Journal Publication
  Profile: Spanish Universities as a Case Study</title><categories>cs.DL</categories><comments>Published in Journal of the American Society of Information Science
  and Technology, 2012: 63(11), 2328-2340</comments><journal-ref>Garcia, J. et al. (2012). Mapping Academic Institutions According
  to Their Journal Publication Profile: Spanish Universities as a Case Study.
  Journal of the American Society for Information Science and Technology,
  63(11): 2328-2340</journal-ref><doi>10.1002/asi.22735</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel methodology for mapping academic institutions based on
their journal publication profiles. We believe that journals in which
researchers from academic institutions publish their works can be considered as
useful identifiers for representing the relationships between these
institutions and establishing comparisons. However, when academic journals are
used for research output representation, distinctions must be introduced
between them, based on their value as institution descriptors. This leads us to
the use of journal weights attached to the institution identifiers. Since a
journal in which researchers from a large proportion of institutions published
their papers may be a bad indicator of similarity between two academic
institutions, it seems reasonable to weight it in accordance with how
frequently researchers from different institutions published their papers in
this journal. Cluster analysis can then be applied to group the academic
institutions, and dendrograms can be provided to illustrate groups of
institutions following agglomerative hierarchical clustering. In order to test
this methodology, we use a sample of Spanish universities as a case study. We
first map the study sample according to an institution's overall research
output, then we use it for two scientific fields (Information and Communication
Technologies, as well as Medicine and Pharmacology) as a means to demonstrate
how our methodology can be applied, not only for analyzing institutions as a
whole, but also in different disciplinary contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4059</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4059</id><created>2013-02-17</created><authors><author><keyname>Jurdzinski</keyname><forenames>Tomasz</forenames></author><author><keyname>Kowalski</keyname><forenames>Dariusz R.</forenames></author><author><keyname>Stachowiak</keyname><forenames>Grzegorz</forenames></author></authors><title>Distributed Deterministic Broadcasting in Uniform-Power Ad Hoc Wireless
  Networks</title><categories>cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1207.6732</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Development of many futuristic technologies, such as MANET, VANET, iThings,
nano-devices, depend on efficient distributed communication protocols in
multi-hop ad hoc networks. A vast majority of research in this area focus on
design heuristic protocols, and analyze their performance by simulations on
networks generated randomly or obtained in practical measurements of some
(usually small-size) wireless networks. %some library. Moreover, they often
assume access to truly random sources, which is often not reasonable in case of
wireless devices. In this work we use a formal framework to study the problem
of broadcasting and its time complexity in any two dimensional Euclidean
wireless network with uniform transmission powers. For the analysis, we
consider two popular models of ad hoc networks based on the
Signal-to-Interference-and-Noise Ratio (SINR): one with opportunistic links,
and the other with randomly disturbed SINR. In the former model, we show that
one of our algorithms accomplishes broadcasting in $O(D\log^2 n)$ rounds, where
$n$ is the number of nodes and $D$ is the diameter of the network. If nodes
know a priori the granularity $g$ of the network, i.e., the inverse of the
maximum transmission range over the minimum distance between any two stations,
a modification of this algorithm accomplishes broadcasting in $O(D\log g)$
rounds.
  Finally, we modify both algorithms to make them efficient in the latter model
with randomly disturbed SINR, with only logarithmic growth of performance.
  Ours are the first provably efficient and well-scalable, under the two
models, distributed deterministic solutions for the broadcast task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4061</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4061</id><created>2013-02-17</created><authors><author><keyname>Ralph</keyname><forenames>Paul</forenames></author></authors><title>The Sensemaking-Coevolution-Implementation Theory of Software Design</title><categories>cs.SE</categories><comments>7 tables, 7 Figures, 157 references</comments><doi>10.1016/j.scico.2014.11.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding software design practice is critical to understanding modern
information systems development. New developments in empirical software
engineering, information systems design science and the interdisciplinary
design literature combined with recent advances in process theory and
testability have created a situation ripe for innovation. Consequently, this
paper utilizes these breakthroughs to formulate a process theory of software
design practice: Sensemaking-Coevolution-Implementation Theory explains how
complex software systems are created by collocated software development teams
in organizations. It posits that an independent agent (design team) creates a
software system by alternating between three activities: organizing their
perceptions about the context, mutually refining their understandings of the
context and design space, and manifesting their understanding of the design
space in a technological artifact. This theory development paper defines and
illustrates Sensemaking-Coevolution-Implementation Theory, grounds its concepts
and relationships in existing literature, conceptually evaluates the theory and
situates it in the broader context of information systems development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4062</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4062</id><created>2013-02-17</created><updated>2014-04-27</updated><authors><author><keyname>Usatyuk</keyname><forenames>Vasiliy</forenames></author></authors><title>Upper and lower bound on the cardinality containing shortest vectors in
  a lattice reduced by block Korkin-Zolotarev method (Russian)</title><categories>cs.DM</categories><comments>in Russian, 14 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article present a concise estimate of upper and lower bound on the
cardinality containing shortest vector in a lattice reduced by block
Korkin-Zolotarev method (BKZ) for different value of the block size. Paper show
how density affect to this cardinality, in form of the ratio of shortest vector
size and sucessive minimal. Moreover we give upper estimate of cardinality for
critical and Goldstein-Mayer lattices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4064</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4064</id><created>2013-02-17</created><authors><author><keyname>Kim</keyname><forenames>Jinil</forenames></author><author><keyname>Eades</keyname><forenames>Peter</forenames></author><author><keyname>Fleischer</keyname><forenames>Rudolf</forenames></author><author><keyname>Hong</keyname><forenames>Seok-Hee</forenames></author><author><keyname>Iliopoulos</keyname><forenames>Costas S.</forenames></author><author><keyname>Park</keyname><forenames>Kunsoo</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author><author><keyname>Tokuyama</keyname><forenames>Takeshi</forenames></author></authors><title>Order Preserving Matching</title><categories>cs.DS</categories><comments>15 pages; submitted to Theoretical Computer Science, 5 Dec 2012;
  presented at Theo Murphy International Scientific Meeting of the Royal
  Society on Storage and Indexing of Massive Data, 7 Feb 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new string matching problem called order-preserving matching
on numeric strings where a pattern matches a text if the text contains a
substring whose relative orders coincide with those of the pattern.
Order-preserving matching is applicable to many scenarios such as stock price
analysis and musical melody matching in which the order relations should be
matched instead of the strings themselves. Solving order-preserving matching
has to do with representations of order relations of a numeric string. We
define prefix representation and nearest neighbor representation, which lead to
efficient algorithms for order-preserving matching. We present efficient
algorithms for single and multiple pattern cases. For the single pattern case,
we give an O(n log m) time algorithm and optimize it further to obtain O(n + m
log m) time. For the multiple pattern case, we give an O(n log m) time
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4077</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4077</id><created>2013-02-17</created><authors><author><keyname>Abdel-Rehim</keyname><forenames>A. M.</forenames></author><author><keyname>Stathopoulos</keyname><forenames>Andreas</forenames></author><author><keyname>Orginos</keyname><forenames>Kostas</forenames></author></authors><title>Extending the eigCG algorithm to nonsymmetric Lanczos for linear systems
  with multiple right-hand sides</title><categories>hep-lat cs.NA math.NA</categories><comments>25 pages, 18 figures</comments><journal-ref>Numer. Linear Algebra Appl., 21: 473-493 (2014)</journal-ref><doi>10.1002/nla.1893</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The technique that was used to build the EigCG algorithm for sparse symmetric
linear systems is extended to the nonsymmetric case using the BiCG algorithm.
We show that, similarly to the symmetric case, we can build an algorithm that
is capable of computing a few smallest magnitude eigenvalues and their
corresponding left and right eigenvectors of a nonsymmetric matrix using only a
small window of the BiCG residuals while simultaneously solving a linear system
with that matrix. For a system with multiple right-hand sides, we give an
algorithm that computes incrementally more eigenvalues while solving the first
few systems and then uses the computed eigenvectors to deflate BiCGStab for the
remaining systems. Our experiments on various test problems, including Lattice
QCD, show the remarkable ability of EigBiCG to compute spectral approximations
with accuracy comparable to that of the unrestarted, nonsymmetric Lanczos.
Furthermore, our incremental EigBiCG followed by appropriately restarted and
deflated BiCGStab provides a competitive method for systems with multiple
right-hand sides.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4085</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4085</id><created>2013-02-17</created><authors><author><keyname>Lu</keyname><forenames>Charng-Da</forenames></author></authors><title>Comprehensive Resource Measurement and Analysis for HPC Systems with
  TACC_Stats</title><categories>cs.DC cs.PF</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  High-performance computing (HPC) systems are a complex combination of
software, processors, memory, networks, and storage systems characterized by
frequent disruptive technological advances. Anomalous behavior has to be
manually diagnosed and remedied with incomplete and sparse data. It also has
been effort-intensive for users to assess the effectiveness with which they are
using the available resources. The data available for system level analyses
appear from multiple sources and in disparate formats (from Linux &quot;sysstat&quot; and
accounting to scheduler/kernel logs). Sysstat does not resolve its measurements
by job so that job-oriented analyses require individual measurements. There are
many user-oriented performance instrumentation and profiling tools but they
require extensive system knowledge, code changes and recompilation, and thus
are not widely used. To address this issue, we develop TACC_Stats, a
job-oriented and logically structured version of the conventional Linux
&quot;sysstat/sar&quot; system-wide performance monitor. We use TACC_Stats-collected data
from a supercomputer &quot;Ranger&quot; to demonstrate its effectiveness in two case
studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4092</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4092</id><created>2013-02-17</created><authors><author><keyname>Viana</keyname><forenames>Matheus P.</forenames></author><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author></authors><title>On time-varying collaboration networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><journal-ref>Journal of Informetrics 7 (2) 371-378 (2013)</journal-ref><doi>10.1016/j.joi.2012.12.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The patterns of scientific collaboration have been frequently investigated in
terms of complex networks without reference to time evolution. In the present
work, we derive collaborative networks (from the arXiv repository)
parameterized along time. By defining the concept of affine group, we identify
several interesting trends in scientific collaboration, including the fact that
the average size of the affine groups grows exponentially, while the number of
authors increases as a power law. We were therefore able to identify, through
extrapolation, the possible date when a single affine group is expected to
emerge. Characteristic collaboration patterns were identified for each
researcher, and their analysis revealed that larger affine groups tend to be
less stable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4095</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4095</id><created>2013-02-17</created><authors><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author><author><keyname>Oliveira</keyname><forenames>Osvaldo N.</forenames><suffix>Jr.</suffix></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author></authors><title>Three-feature model to reproduce the topology of citation networks and
  the effects from authors' visibility on their h-index</title><categories>physics.soc-ph cs.DL cs.SI physics.data-an</categories><journal-ref>Journal of Informetrics Volume 6, Issue 3, July 2012, Pages
  427-434</journal-ref><doi>10.1016/j.joi.2012.02.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various factors are believed to govern the selection of references in
citation networks, but a precise, quantitative determination of their
importance has remained elusive. In this paper, we show that three factors can
account for the referencing pattern of citation networks for two topics, namely
&quot;graphenes&quot; and &quot;complex networks&quot;, thus allowing one to reproduce the
topological features of the networks built with papers being the nodes and the
edges established by citations. The most relevant factor was content
similarity, while the other two - in-degree (i.e. citation counts) and {age of
publication} had varying importance depending on the topic studied. This
dependence indicates that additional factors could play a role. Indeed, by
intuition one should expect the reputation (or visibility) of authors and/or
institutions to affect the referencing pattern, and this is only indirectly
considered via the in-degree that should correlate with such reputation.
Because information on reputation is not readily available, we simulated its
effect on artificial citation networks considering two communities with
distinct fitness (visibility) parameters. One community was assumed to have
twice the fitness value of the other, which amounts to a double probability for
a paper being cited. While the h-index for authors in the community with larger
fitness evolved with time with slightly higher values than for the control
network (no fitness considered), a drastic effect was noted for the community
with smaller fitness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4099</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4099</id><created>2013-02-17</created><authors><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author><author><keyname>Oliveira</keyname><forenames>Osvaldo N.</forenames><suffix>Jr.</suffix></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author></authors><title>Identification of Literary Movements Using Complex Networks to Represent
  Texts</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>The Supplementary Information (SI) is available from
  http://iopscience.iop.org/1367-2630/14/4/043029/media/njp043029suppdata.pdf</comments><journal-ref>New J. Phys. 14 043029 (2012)</journal-ref><doi>10.1088/1367-2630/14/4/043029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of statistical methods to analyze large databases of text has been
useful to unveil patterns of human behavior and establish historical links
between cultures and languages. In this study, we identify literary movements
by treating books published from 1590 to 1922 as complex networks, whose
metrics were analyzed with multivariate techniques to generate six clusters of
books. The latter correspond to time periods coinciding with relevant literary
movements over the last 5 centuries. The most important factor contributing to
the distinction between different literary styles was {the average shortest
path length (particularly, the asymmetry of the distribution)}. Furthermore,
over time there has been a trend toward larger average shortest path lengths,
which is correlated with increased syntactic complexity, and a more uniform use
of the words reflected in a smaller power-law coefficient for the distribution
of word frequency. Changes in literary style were also found to be driven by
opposition to earlier writing styles, as revealed by the analysis performed
with geometrical concepts. The approaches adopted here are generic and may be
extended to analyze a number of features of languages and cultures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4107</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4107</id><created>2013-02-17</created><authors><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author><author><keyname>Oliveira</keyname><forenames>Osvaldo N.</forenames><suffix>Jr.</suffix></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author></authors><title>Using Complex Networks to Quantify Consistency in the Use of Words</title><categories>physics.soc-ph cs.SI physics.data-an</categories><journal-ref>J. Stat. Mech. (2012) P01004</journal-ref><doi>10.1088/1742-5468/2012/01/P01004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we quantify the consistency of word usage in written texts
represented by complex networks, where words were taken as nodes, by measuring
the degree of preservation of the node neighborhood.} Words were considered
highly consistent if the authors used them with the same neighborhood. When
ranked according to the consistency of use, the words obeyed a log-normal
distribution, in contrast to the Zipf's law that applies to the frequency of
use. Consistency correlated positively with the familiarity and frequency of
use, and negatively with ambiguity and age of acquisition. An inspection of
some highly consistent words confirmed that they are used in very limited
semantic contexts. A comparison of consistency indices for 8 authors indicated
that these indices may be employed for author recognition. Indeed, as expected
authors of novels could be distinguished from those who wrote scientific texts.
Our analysis demonstrated the suitability of the consistency indices, which can
now be applied in other tasks, such as emotion recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4118</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4118</id><created>2013-02-17</created><updated>2013-03-25</updated><authors><author><keyname>Sun</keyname><forenames>Shunqiao</forenames></author><author><keyname>Petropulu</keyname><forenames>Athina P.</forenames></author><author><keyname>Bajwa</keyname><forenames>Waheed U.</forenames></author></authors><title>Target Estimation in Colocated MIMO Radar via Matrix Completion</title><categories>cs.IT math.IT stat.AP</categories><comments>5 pages, ICASSP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a colocated MIMO radar scenario, in which the receive antennas
forward their measurements to a fusion center. Based on the received data, the
fusion center formulates a matrix which is then used for target parameter
estimation. When the receive antennas sample the target returns at Nyquist
rate, and assuming that there are more receive antennas than targets, the data
matrix at the fusion center is low-rank. When each receive antenna sends to the
fusion center only a small number of samples, along with the sample index, the
receive data matrix has missing elements, corresponding to the samples that
were not forwarded. Under certain conditions, matrix completion techniques can
be applied to recover the full receive data matrix, which can then be used in
conjunction with array processing techniques, e.g., MUSIC, to obtain target
information. Numerical results indicate that good target recovery can be
achieved with occupancy of the receive data matrix as low as 50%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4127</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4127</id><created>2013-02-17</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Adaptive Set-Membership Reduced-Rank Least Squares Beamforming
  Algorithms</title><categories>cs.IT math.IT</categories><comments>2 figures</comments><journal-ref>Proceedings of Asilomar Conference on Signals Systems and
  Computers, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new adaptive algorithm for the linearly constrained
minimum variance (LCMV) beamformer design. We incorporate the set-membership
filtering (SMF) mechanism into the reduced-rank joint iterative optimization
(JIO) scheme to develop a constrained recursive least squares (RLS) based
algorithm called JIO-SM-RLS. The proposed algorithm inherits the positive
features of reduced-rank signal processing techniques to enhance the output
performance, and utilizes the data-selective updates (around 10-15%) of the SMF
methodology to save the computational cost significantly. An effective
time-varying bound is imposed on the array output as a constraint to circumvent
the risk of overbounding or underbounding, and to update the parameters for
beamforming. The updated parameters construct a set of solutions (a membership
set) that satisfy the constraints of the LCMV beamformer. Simulations are
performed to show the superior performance of the proposed algorithm in terms
of the convergence rate and the reduced computational complexity in comparison
with the existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4128</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4128</id><created>2013-02-17</created><authors><author><keyname>Chatterjee</keyname><forenames>Avhishek</forenames></author><author><keyname>Deb</keyname><forenames>Supratim</forenames></author><author><keyname>Nagaraj</keyname><forenames>Kanthi</forenames></author><author><keyname>Srinivasan</keyname><forenames>Vikram</forenames></author></authors><title>Low Delay MAC Scheduling for Frequency-agile Multi-radio Wireless
  Networks</title><categories>cs.NI</categories><comments>Accepted to appear in IEEE Journal on Selected Areas in Communication
  (JSAC) - Cognitive Radio Series, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent trends suggest that cognitive radio based wireless networks will be
frequency agile and the nodes will be equipped with multiple radios capable of
tuning across large swaths of spectrum. The MAC scheduling problem in such
networks refers to making intelligent decisions on which communication links to
activate at which time instant and over which frequency band. The challenge in
designing a low-complexity distributed MAC, that achieves low delay, is posed
by two additional dimensions of cognitive radio networks: interference graphs
and data rates that are frequency-band dependent, and explosion in number of
feasible schedules due to large number of available frequency-bands. In this
paper, we propose MAXIMALGAIN MAC, a distributed MAC scheduler for frequency
agile multi-band networks that simultaneously achieves the following: (i)
optimal network-delay scaling with respect to the number of communicating
pairs, (ii) low computational complexity of O(log2(maximum degree of the
interference graphs)) which is independent of the number of frequency bands,
number of radios per node, and overall size of the network, and (iii)
robustness, i.e., it can be adapted to a scenario where nodes are not
synchronized and control packets could be lost. Our proposed MAC also achieves
a throughput provably within a constant fraction (under isotropic propagation)
of the maximum throughput. Due to a recent impossibility result, optimal
delay-scaling could only be achieved with some amount of throughput loss [30].
Extensive simulations using OMNeT++ network simulator shows that, compared to a
multi-band extension of a state-of-art CSMA algorithm (namely, Q-CSMA), our
asynchronous algorithm achieves a 2.5x? reduction in delay while achieving at
least 85% of the maximum achievable throughput. Our MAC algorithms are derived
from a novel local search based technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4129</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4129</id><created>2013-02-17</created><authors><author><keyname>Gad</keyname><forenames>Eyal En</forenames></author><author><keyname>Mateescu</keyname><forenames>Robert</forenames></author><author><keyname>Blagojevic</keyname><forenames>Filip</forenames></author><author><keyname>Guyot</keyname><forenames>Cyril</forenames></author><author><keyname>Bandic</keyname><forenames>Zvonimir</forenames></author></authors><title>Repair-Optimal MDS Array Codes over GF(2)</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum-distance separable (MDS) array codes with high rate and an optimal
repair property were introduced recently. These codes could be applied in
distributed storage systems, where they minimize the communication and disk
access required for the recovery of failed nodes. However, the encoding and
decoding algorithms of the proposed codes use arithmetic over finite fields of
order greater than 2, which could result in a complex implementation.
  In this work, we present a construction of 2-parity MDS array codes, that
allow for optimal repair of a failed information node using XOR operations
only. The reduction of the field order is achieved by allowing more parity bits
to be updated when a single information bit is being changed by the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4130</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4130</id><created>2013-02-17</created><authors><author><keyname>Cai</keyname><forenames>Yunlong</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Adaptive Minimum BER Reduced-Rank Interference Suppression Algorithms
  Based on Joint and Iterative Optimization of Parameters</title><categories>cs.IT math.IT</categories><comments>3 figures</comments><journal-ref>IEEE Communications Letters, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we propose a novel adaptive reduced-rank strategy based on
joint iterative optimization (JIO) of filters according to the minimization of
the bit error rate (BER) cost function. The proposed optimization technique
adjusts the weights of a subspace projection matrix and a reduced-rank filter
jointly. We develop stochastic gradient (SG) algorithms for their adaptive
implementation and introduce a novel automatic rank selection method based on
the BER criterion. Simulation results for direct-sequence
code-division-multiple-access (DS-CDMA) systems show that the proposed adaptive
algorithms significantly outperform the existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4133</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4133</id><created>2013-02-17</created><authors><author><keyname>Nguyen</keyname><forenames>Viet Hung</forenames></author><author><keyname>Massacci</keyname><forenames>Fabio</forenames></author></authors><title>The (Un)Reliability of NVD Vulnerable Versions Data: an Empirical
  Experiment on Google Chrome Vulnerabilities</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NVD is one of the most popular databases used by researchers to conduct
empirical research on data sets of vulnerabilities. Our recent analysis on
Chrome vulnerability data reported by NVD has revealed an abnormally phenomenon
in the data where almost vulnerabilities were originated from the first
versions. This inspires our experiment to validate the reliability of the NVD
vulnerable version data. In this experiment, we verify for each version of
Chrome that NVD claims vulnerable is actually vulnerable. The experiment
revealed several errors in the vulnerability data of Chrome. Furthermore, we
have also analyzed how these errors might impact the conclusions of an
empirical study on foundational vulnerability. Our results show that different
conclusions could be obtained due to the data errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4136</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4136</id><created>2013-02-17</created><authors><author><keyname>Cai</keyname><forenames>Kun</forenames></author><author><keyname>Gao</keyname><forenames>David Y.</forenames></author><author><keyname>Qin</keyname><forenames>Qing H.</forenames></author></authors><title>Post-buckling Solutions of Hyper-elastic Beam by Canonical Dual Finite
  Element Method</title><categories>cs.CE math.NA</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Post buckling problem of a large deformed beam is analyzed using canonical
dual finite element method (CD-FEM). The feature of this method is to choose
correctly the canonical dual stress so that the original non-convex potential
energy functional is reformulated in a mixed complementary energy form with
both displacement and stress fields, and a pure complementary energy is
explicitly formulated in finite dimensional space. Based on the canonical
duality theory and the associated triality theorem, a primal-dual algorithm is
proposed, which can be used to find all possible solutions of this nonconvex
post-buckling problem. Numerical results show that the global maximum of the
pure-complementary energy leads to a stable buckled configuration of the beam.
While the local extrema of the pure-complementary energy present unstable
deformation states, especially. We discovered that the unstable buckled state
is very sensitive to the number of total elements and the external loads.
Theoretical results are verified through numerical examples and some
interesting phenomena in post-bifurcation of this large deformed beam are
observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4138</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4138</id><created>2013-02-17</created><updated>2013-05-12</updated><authors><author><keyname>Babaioff</keyname><forenames>Moshe</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author><author><keyname>Slivkins</keyname><forenames>Aleksandrs</forenames></author></authors><title>Multi-parameter Mechanisms with Implicit Payment Computation</title><categories>cs.GT</categories><comments>This is a full version of a paper in ACM EC 2013</comments><acm-class>J.4; K.4.4; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that payment computation essentially does not present
any obstacle in designing truthful mechanisms, even for multi-parameter
domains, and even when we can only call the allocation rule once. We present a
general reduction that takes any allocation rule which satisfies &quot;cyclic
monotonicity&quot; (a known necessary and sufficient condition for truthfulness) and
converts it to a truthful mechanism using a single call to the allocation rule,
with arbitrarily small loss to the expected social welfare.
  A prominent example for a multi-parameter setting in which an allocation rule
can only be called once arises in sponsored search auctions. These are
multi-parameter domains when each advertiser has multiple possible ads he may
display, each with a different value per click. Moreover, the mechanism
typically does not have complete knowledge of the click-realization or the
click-through rates (CTRs); it can only call the allocation rule a single time
and observe the click information for ads that were presented. % are not known.
On the negative side, we show that an allocation that is truthful for any
realization essentially cannot depend on the bids, and hence cannot do better
than random selection for one agent. We then consider a relaxed requirement of
truthfulness, only in expectation over the CTRs. Even for that relaxed version,
making any progress is challenging as standard techniques for construction of
truthful mechanisms (as using VCG or an MIDR allocation rule) cannot be used in
this setting. We design an allocation rule with non-trivial performance and
directly prove it is cyclic-monotone, and thus it can be used to create a
truthful mechanism using our general reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4141</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4141</id><created>2013-02-17</created><authors><author><keyname>Latorre</keyname><forenames>Vittorio</forenames></author><author><keyname>Gao</keyname><forenames>David Yang</forenames></author></authors><title>Canonical dual solutions to nonconvex radial basis neural network
  optimization problem</title><categories>cs.NE cs.LG stat.ML</categories><comments>10 pages, 7 figures, one table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radial Basis Functions Neural Networks (RBFNNs) are tools widely used in
regression problems. One of their principal drawbacks is that the formulation
corresponding to the training with the supervision of both the centers and the
weights is a highly non-convex optimization problem, which leads to some
fundamentally difficulties for traditional optimization theory and methods.
  This paper presents a generalized canonical duality theory for solving this
challenging problem. We demonstrate that by sequential canonical dual
transformations, the nonconvex optimization problem of the RBFNN can be
reformulated as a canonical dual problem (without duality gap). Both global
optimal solution and local extrema can be classified. Several applications to
one of the most used Radial Basis Functions, the Gaussian function, are
illustrated. Our results show that even for one-dimensional case, the global
minimizer of the nonconvex problem may not be the best solution to the RBFNNs,
and the canonical dual theory is a promising tool for solving general neural
networks training problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4146</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4146</id><created>2013-02-17</created><authors><author><keyname>Guang</keyname><forenames>Xuan</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author></authors><title>Linear Network Error Correction Multicast/Broadcast/Dispersion Codes</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, for the purposes of information transmission and network error
correction simultaneously, three classes of important linear network codes in
network coding, linear multicast/broadcast/dispersion codes are generalized to
linear network error correction coding, i.e., linear network error correction
multicast/broadcast/dispersion codes. We further propose the (weakly, strongly)
extended Singleton bounds for these new classes of codes, and define the
optimal codes satisfying the corresponding Singleton bounds with equality,
which are called multicast/broadcast/dispersion MDS codes respectively. The
existence of such codes are proved by an algebraic method and one kind of
constructive algorithm is also proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4147</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4147</id><created>2013-02-17</created><updated>2013-02-23</updated><authors><author><keyname>Guang</keyname><forenames>Xuan</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author></authors><title>The Failure Probability of Random Linear Network Coding for Networks</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to ISIT 2013. And add the interpretation of the
  notation $CUT_{t,k}^{out}$ to original manuscript (in column 2, page 2)
  submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In practice, since many communication networks are huge in scale, or
complicated in structure, or even dynamic, the predesigned linear network codes
based on the network topology is impossible even if the topological structure
is known. Therefore, random linear network coding has been proposed as an
acceptable coding technique for the case that the network topology cannot be
utilized completely. Motivated by the fact that different network topological
information can be obtained for different practical applications, we study the
performance analysis of random linear network coding by analyzing some failure
probabilities depending on these different topological information of networks.
We obtain some tight or asymptotically tight upper bounds on these failure
probabilities and indicate the worst cases for these bounds, i.e., the networks
meeting the upper bounds with equality. In addition, if the more topological
information of the network is utilized, the better upper bounds are obtained.
On the other hand, we also discuss the lower bounds on the failure
probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4150</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4150</id><created>2013-02-17</created><authors><author><keyname>Lai</keyname><forenames>Ching-Yi</forenames></author><author><keyname>Brun</keyname><forenames>Todd A.</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Duality in Entanglement-Assisted Quantum Error Correction</title><categories>quant-ph cs.IT math.IT</categories><comments>This paper is a compact version of arXiv:1010.5506</comments><journal-ref>IEEE Transactions on Information Theory vol. 59, no. 6, pages
  4020-4024 (June 2013)</journal-ref><doi>10.1109/TIT.2013.2246274</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dual of an entanglement-assisted quantum error-correcting (EAQEC) code is
defined from the orthogonal group of a simplified stabilizer group. From the
Poisson summation formula, this duality leads to the MacWilliams identities and
linear programming bounds for EAQEC codes. We establish a table of upper and
lower bounds on the minimum distance of any maximal-entanglement EAQEC code
with length up to 15 channel qubits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4168</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4168</id><created>2013-02-18</created><authors><author><keyname>Kumar</keyname><forenames>K. Ashwin</forenames></author><author><keyname>Deshpande</keyname><forenames>Amol</forenames></author><author><keyname>Khuller</keyname><forenames>Samir</forenames></author></authors><title>Data Placement and Replica Selection for Improving Co-location in
  Distributed Environments</title><categories>cs.DB cs.DC</categories><comments>12 pages, 22 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing need for large-scale data analytics in a number of application
domains has led to a dramatic rise in the number of distributed data management
systems, both parallel relational databases, and systems that support
alternative frameworks like MapReduce. There is thus an increasing contention
on scarce data center resources like network bandwidth; further, the energy
requirements for powering the computing equipment are also growing
dramatically. As we show empirically, increasing the execution parallelism by
spreading out data across a large number of machines may achieve the intended
goal of decreasing query latencies, but in most cases, may increase the total
resource and energy consumption significantly. For many analytical workloads,
however, minimizing query latencies is often not critical; in such scenarios,
we argue that we should instead focus on minimizing the average query span,
i.e., the average number of machines that are involved in processing of a
query, through colocation of data items that are frequently accessed together.
In this work, we exploit the fact that most distributed environments need to
use replication for fault tolerance, and we devise workload-driven replica
selection and placement algorithms that attempt to minimize the average query
span. We model a historical query workload trace as a hypergraph over a set of
data items, and formulate and analyze the problem of replica placement by
drawing connections to several well-studied graph theoretic concepts. We
develop a series of algorithms to decide which data items to replicate, and
where to place the replicas. We show effectiveness of our proposed approach by
presenting results on a collection of synthetic and real workloads. Our
experiments show that careful data placement and replication can dramatically
reduce the average query spans resulting in significant reductions in the
resource consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4172</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4172</id><created>2013-02-18</created><authors><author><keyname>Mohota</keyname><forenames>Nilesh A.</forenames></author><author><keyname>Badjate</keyname><forenames>Sanjay L.</forenames></author></authors><title>Reduction in Packet Delay Through the use of Common Buffer over
  Distributed Buffer in the Routing Node of NOC Architecture</title><categories>cs.AR</categories><comments>8 Pages, 10 Figures, 2 Tables</comments><journal-ref>International Journal of Innovative Technology and Creative
  Engineering, ISSN : 2045-8711, December 2012 Issue, Volume2, No. 12</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance evaluation of the routing node in terms of latency is the
characteristics of an efficient design of Buffer in input module. It is
intended to study and quantify the behavior of the single packet array design
in relation to the multiple packet array design. The utilization efficiency of
the packet buffer array improves when a common buffer is used instead of
individual buffers in each input port. First Poissons Queuing model was
prepared to manifest the differences in packet delays. The queuing model can be
classified as (M/M/1), (32/FIFO). Arrival rate has been assumed to be Poisson
distributed with a mean arrival rate of 10 x 1000000. The service rate is
assumed to be exponentially distributed with a mean service rate of 10.05 x
1000000. It has been observed that latency in Common Buffer improved by 46
percent over its distributed buffer. A Simulink model later simulated on MATLAB
to calculate the improvement in packet delay. It has been observed that the
delay improved by approximately 40 percent through the use of a common buffer.
A verilog RTL for both common and shared buffer has been prepared and later
synthesized using Design Compiler of SYNOPSYS. In distributed buffer, arrival
of data packet could be delayed by 2 or 4 clock cycles which lead to latency
improvement either by 17 percent or 34 percent in a common buffer
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4187</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4187</id><created>2013-02-18</created><authors><author><keyname>R&#xfc;mmer</keyname><forenames>Philipp</forenames><affiliation>Uppsala University</affiliation></author><author><keyname>Hojjat</keyname><forenames>Hossein</forenames><affiliation>EPFL Lausanne</affiliation></author><author><keyname>Kuncak</keyname><forenames>Viktor</forenames><affiliation>EPFL Lausanne</affiliation></author></authors><title>The Relationship between Craig Interpolation and Recursion-Free Horn
  Clauses</title><categories>cs.LO</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite decades of research, there are still a number of concepts commonly
found in software programs that are considered challenging for verification:
among others, such concepts include concurrency, and the compositional analysis
of programs with procedures. As a promising direction to overcome such
difficulties, recently the use of Horn constraints as intermediate
representation of software programs has been proposed. Horn constraints are
related to Craig interpolation, which is one of the main techniques used to
construct and refine abstractions in verification, and to synthesise inductive
loop invariants. We give a survey of the different forms of Craig interpolation
found in literature, and show that all of them correspond to natural fragments
of (recursion-free) Horn constraints. We also discuss techniques for solving
systems of recursion-free Horn constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4201</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4201</id><created>2013-02-18</created><authors><author><keyname>Certic</keyname><forenames>Stefan</forenames></author></authors><title>The Future of Mobile Security</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Mobile devices are more than just phones, they are a lifeline to the outdoor
world, entertainment platform, GPS system, a little black book and a shopping
and banking tool. What is not well known is that these devices are also
gateways. Mobile devices can be used by a hacker as an access point into many
other aspects of your digital life as well the lives of others in your network,
making mobile security about more than just protecting your phone. This is an
overview of technologies supporting mobile data cryptography and two-step
protection mechanisms that may be used to secure online transactions and user
authentication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4205</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4205</id><created>2013-02-18</created><updated>2013-04-03</updated><authors><author><keyname>Belkhir</keyname><forenames>Walid</forenames></author><author><keyname>Chevalier</keyname><forenames>Yannick</forenames></author><author><keyname>Rusinowitch</keyname><forenames>Michael</forenames></author></authors><title>Fresh-Variable Automata for Service Composition</title><categories>cs.FL</categories><comments>28 pages. 4 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To model Web services handling data from an infinite domain, or with multiple
sessions, we introduce fresh-variable automata, a simple extension of
finite-state automata in which some transitions are labeled with variables that
can be refreshed in some specified states. We prove several closure properties
for this class of automata and study their decision problems. We then introduce
a notion of simulation that enables us to reduce the Web service composition
problem to the construction of a simulation of a target service by the
asynchronous product of existing services, and prove that this construction is
computable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4207</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4207</id><created>2013-02-18</created><authors><author><keyname>Montanaro</keyname><forenames>Ashley</forenames></author></authors><title>A composition theorem for decision tree complexity</title><categories>cs.CC</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We completely characterise the complexity in the decision tree model of
computing composite relations of the form h = g(f^1,...,f^n), where each
relation f^i is boolean-valued. Immediate corollaries include a direct sum
theorem for decision tree complexity and a tight characterisation of the
decision tree complexity of iterated boolean functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4213</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4213</id><created>2013-02-18</created><authors><author><keyname>Jansen</keyname><forenames>Klaus</forenames></author><author><keyname>Klein</keyname><forenames>Kim-Manuel</forenames></author></authors><title>A Robust AFPTAS for Online Bin Packing with Polynomial Migration</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop general LP and ILP techniques to find an approximate
solution with improved objective value close to an existing solution. The task
of improving an approximate solution is closely related to a classical theorem
of Cook et al. in the sensitivity analysis for LPs and ILPs. This result is
often applied in designing robust algorithms for online problems. We apply our
new techniques to the online bin packing problem, where it is allowed to
reassign a certain number of items, measured by the migration factor. The
migration factor is defined by the total size of reassigned items divided by
the size of the arriving item. We obtain a robust asymptotic fully polynomial
time approximation scheme (AFPTAS) for the online bin packing problem with
migration factor bounded by a polynomial in $\frac{1}{\epsilon}$. This answers
an open question stated by Epstein and Levin in the affirmative. As a byproduct
we prove an approximate variant of the sensitivity theorem by Cook at el. for
linear programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4216</identifier>
 <datestamp>2013-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4216</id><created>2013-02-18</created><updated>2013-04-30</updated><authors><author><keyname>Bringmann</keyname><forenames>Karl</forenames></author><author><keyname>Doerr</keyname><forenames>Benjamin</forenames></author><author><keyname>Neumann</keyname><forenames>Adrian</forenames></author><author><keyname>Sliacan</keyname><forenames>Jakub</forenames></author></authors><title>Online Checkpointing with Improved Worst-Case Guarantees</title><categories>cs.DS</categories><comments>25 pages, 5 figures. ICALP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the online checkpointing problem, the task is to continuously maintain a
set of k checkpoints that allow to rewind an ongoing computation faster than by
a full restart. The only operation allowed is to replace an old checkpoint by
the current state. Our aim are checkpoint placement strategies that minimize
rewinding cost, i.e., such that at all times T when requested to rewind to some
time t &lt;= T the number of computation steps that need to be redone to get to t
from a checkpoint before t is as small as possible. In particular, we want that
the closest checkpoint earlier than t is not further away from t than q_k times
the ideal distance T / (k+1), where q_k is a small constant.
  Improving over earlier work showing 1 + 1/k &lt;= q_k &lt;= 2, we show that q_k can
be chosen asymptotically less than 2. We present algorithms with asymptotic
discrepancy q_k &lt;= 1.59 + o(1) valid for all k and q_k &lt;= ln(4) + o(1) &lt;= 1.39
+ o(1) valid for k being a power of two. Experiments indicate the uniform bound
p_k &lt;= 1.7 for all k. For small k, we show how to use a linear programming
approach to compute good checkpointing algorithms. This gives discrepancies of
less than 1.55 for all k &lt; 60.
  We prove the first lower bound that is asymptotically more than one, namely
q_k &gt;= 1.30 - o(1). We also show that optimal algorithms (yielding the infimum
discrepancy) exist for all k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4225</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4225</id><created>2013-02-18</created><authors><author><keyname>Ansari</keyname><forenames>Imran Shafique</forenames></author><author><keyname>Yilmaz</keyname><forenames>Ferkan</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Impact of Pointing Errors on the Performance of Mixed RF/FSO Dual-Hop
  Transmission Systems</title><categories>cs.IT cs.PF math.IT math.PR</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, the performance analysis of a dual-hop relay transmission
system composed of asymmetric radio-frequency (RF)/free-space optical (FSO)
links with pointing errors is presented. More specifically, we build on the
system model presented in [1] to derive new exact closed-form expressions for
the cumulative distribution function, probability density function, moment
generating function, and moments of the end-to-end signal-to-noise ratio in
terms of the Meijer's G function. We then capitalize on these results to offer
new exact closed-form expressions for the higher-order amount of fading,
average error rate for binary and M-ary modulation schemes, and the ergodic
capacity, all in terms of Meijer's G functions. Our new analytical results were
also verified via computer-based Monte-Carlo simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4233</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4233</id><created>2013-02-18</created><authors><author><keyname>Ramamurthy</keyname><forenames>Nallagarla</forenames></author><author><keyname>Varadarajan</keyname><forenames>S.</forenames></author></authors><title>The Robust Digital Image Watermarking using Quantization and Fuzzy Logic
  Approach in DWT Domain</title><categories>cs.MM cs.CR</categories><comments>7 pages, 11 figures, IJCSN Journal</comments><report-no>IJCSN-2012-1-5-13</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a novel approach to embed watermark into the host image using
quantization with the help of Dynamic Fuzzy Inference System (DFIS) is
proposed. The cover image is decomposed up to 3- levels using quantization and
Discrete Wavelet Transform (DWT). A bitmap of size 64x64 pixels is embedded
into the host image using DFIS rule base. The DFIS is utilized to generate the
watermark weighting function to embed the imperceptible watermark. The
implemented watermarking algorithm is imperceptible and robust to some normal
attacks such as JPEG Compression, salt&amp;pepper noise, median filtering, rotation
and cropping.
  Keywords: Watermark, Quantization, Dynamic Fuzzy Inference System,
Imperceptible, Robust, JPEG Compression, Cropping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4242</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4242</id><created>2013-02-18</created><updated>2013-02-26</updated><authors><author><keyname>Chevallier</keyname><forenames>Sylvain</forenames></author><author><keyname>Barth&#xe9;lemy</keyname><forenames>Quentin</forenames></author><author><keyname>Atif</keyname><forenames>Jamal</forenames></author></authors><title>Metrics for Multivariate Dictionaries</title><categories>cs.LG stat.ML</categories><msc-class>53A20, 53A45, 42C15, 28C15</msc-class><acm-class>K.3.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Overcomplete representations and dictionary learning algorithms kept
attracting a growing interest in the machine learning community. This paper
addresses the emerging problem of comparing multivariate overcomplete
representations. Despite a recurrent need to rely on a distance for learning or
assessing multivariate overcomplete representations, no metrics in their
underlying spaces have yet been proposed. Henceforth we propose to study
overcomplete representations from the perspective of frame theory and matrix
manifolds. We consider distances between multivariate dictionaries as distances
between their spans which reveal to be elements of a Grassmannian manifold. We
introduce Wasserstein-like set-metrics defined on Grassmannian spaces and study
their properties both theoretically and numerically. Indeed a deep experimental
study based on tailored synthetic datasetsand real EEG signals for
Brain-Computer Interfaces (BCI) have been conducted. In particular, the
introduced metrics have been embedded in clustering algorithm and applied to
BCI Competition IV-2a for dataset quality assessment. Besides, a principled
connection is made between three close but still disjoint research fields,
namely, Grassmannian packing, dictionary learning and compressed sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4245</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4245</id><created>2013-02-18</created><updated>2013-12-31</updated><authors><author><keyname>Wilson</keyname><forenames>Andrew Gordon</forenames></author><author><keyname>Adams</keyname><forenames>Ryan Prescott</forenames></author></authors><title>Gaussian Process Kernels for Pattern Discovery and Extrapolation</title><categories>stat.ML cs.AI stat.ME</categories><comments>10 pages, 5 figures, 1 table. Minor edits and titled changed from
  &quot;Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation&quot;
  to &quot;Gaussian Process Kernels for Pattern Discovery and Extrapolation&quot;.
  Appears at the International Conference on Machine Learning (ICML), JMLR W&amp;CP
  28(3):1067-1075, 2013</comments><journal-ref>International Conference on Machine Learning (ICML), JMLR W&amp;CP
  28(3):1067-1075, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian processes are rich distributions over functions, which provide a
Bayesian nonparametric approach to smoothing and interpolation. We introduce
simple closed form kernels that can be used with Gaussian processes to discover
patterns and enable extrapolation. These kernels are derived by modelling a
spectral density -- the Fourier transform of a kernel -- with a Gaussian
mixture. The proposed kernels support a broad class of stationary covariances,
but Gaussian process inference remains simple and analytic. We demonstrate the
proposed kernels by discovering patterns and performing long range
extrapolation on synthetic examples, as well as atmospheric CO2 trends and
airline passenger data. We also show that we can reconstruct standard
covariances within our framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4248</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4248</id><created>2013-02-18</created><updated>2014-11-03</updated><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Doyen</keyname><forenames>Laurent</forenames></author><author><keyname>Randour</keyname><forenames>Mickael</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Looking at Mean-Payoff and Total-Payoff through Windows</title><categories>cs.GT cs.LO</categories><comments>Extended version of ATVA 2013 version. Full version to appear in
  Information and Computation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two-player games played on weighted directed graphs with
mean-payoff and total-payoff objectives, two classical quantitative objectives.
While for single-dimensional games the complexity and memory bounds for both
objectives coincide, we show that in contrast to multi-dimensional mean-payoff
games that are known to be coNP-complete, multi-dimensional total-payoff games
are undecidable. We introduce conservative approximations of these objectives,
where the payoff is considered over a local finite window sliding along a play,
instead of the whole play. For single dimension, we show that (i) if the window
size is polynomial, deciding the winner takes polynomial time, and (ii) the
existence of a bounded window can be decided in NP $\cap$ coNP, and is at least
as hard as solving mean-payoff games. For multiple dimensions, we show that (i)
the problem with fixed window size is EXPTIME-complete, and (ii) there is no
primitive-recursive algorithm to decide the existence of a bounded window.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4258</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4258</id><created>2013-02-18</created><authors><author><keyname>Yang</keyname><forenames>Fanny</forenames></author><author><keyname>Pohl</keyname><forenames>Volker</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author></authors><title>Phase Retrieval via Structured Modulations in Paley-Wiener Spaces</title><categories>cs.IT math.IT</categories><comments>Submitted to SAMPTA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the recovery of continuous time signals from the
magnitude of its samples. It uses a combination of structured modulation and
oversampling and provides sufficient conditions on the signal and the sampling
system such that signal recovery is possible. In particular, it is shown that
an average sampling rate of four times the Nyquist rate is sufficient to
reconstruct a signal from its magnitude measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4266</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4266</id><created>2013-02-18</created><updated>2014-03-24</updated><authors><author><keyname>Lampis</keyname><forenames>Michael</forenames><affiliation>Research Institute for Mathematical Sciences</affiliation></author></authors><title>Model Checking Lower Bounds for Simple Graphs</title><categories>cs.CC cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (March 25,
  2014) lmcs:976</journal-ref><doi>10.2168/LMCS-10(1:18)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A well-known result by Frick and Grohe shows that deciding FO logic on trees
involves a parameter dependence that is a tower of exponentials. Though this
lower bound is tight for Courcelle's theorem, it has been evaded by a series of
recent meta-theorems for other graph classes. Here we provide some additional
non-elementary lower bound results, which are in some senses stronger. Our goal
is to explain common traits in these recent meta-theorems and identify barriers
to further progress. More specifically, first, we show that on the class of
threshold graphs, and therefore also on any union and complement-closed class,
there is no model-checking algorithm with elementary parameter dependence even
for FO logic. Second, we show that there is no model-checking algorithm with
elementary parameter dependence for MSO logic even restricted to paths (or
equivalently to unary strings), unless E=NE. As a corollary, we resolve an open
problem on the complexity of MSO model-checking on graphs of bounded max-leaf
number. Finally, we look at MSO on the class of colored trees of depth d. We
show that, assuming the ETH, for every fixed d&gt;=1 at least d+1 levels of
exponentiation are necessary for this problem, thus showing that the (d+1)-fold
exponential algorithm recently given by Gajarsk\`{y} and Hlin\u{e}n\`{y} is
essentially optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4268</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4268</id><created>2013-02-18</created><updated>2013-09-30</updated><authors><author><keyname>Senger</keyname><forenames>Christian</forenames></author></authors><title>Re-Encoding Techniques for Interpolation-Based Decoding of Reed-Solomon
  Codes</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider interpolation-based decoding of Reed-Solomon codes using the
Guruswami-Sudan algorithm (GSA) and investigate the effects of two modification
techniques for received vectors, i.e., the re-encoding map and the newly
introduced periodicity projection. After an analysis of the latter, we track
the benefits (that is low Hamming weight and regular structure) of modified
received vectors through the interpolation step of the GSA and show how the
involved homogeneous linear system of equations can be compressed. We show that
this compression as well as the recovery of the interpolated bivariate
polynomial is particularly simple when the periodicity projection was applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4280</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4280</id><created>2013-02-18</created><authors><author><keyname>Wittmann</keyname><forenames>Markus</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Zeiser</keyname><forenames>Thomas</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>Asynchronous MPI for the Masses</title><categories>cs.DC cs.PF</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple library which equips MPI implementations with truly
asynchronous non-blocking point-to-point operations, and which is independent
of the underlying communication infrastructure. It utilizes the MPI profiling
interface (PMPI) and the MPI_THREAD_MULTIPLE thread compatibility level, and
works with current versions of Intel MPI, Open MPI, MPICH2, MVAPICH2, Cray MPI,
and IBM MPI. We show performance comparisons on a commodity InfiniBand cluster
and two tier-1 systems in Germany, using low-level and application benchmarks.
Issues of thread/process placement and the peculiarities of different MPI
implementations are discussed in detail. We also identify the MPI libraries
that already support asynchronous operations. Finally we show how our ideas can
be extended to MPI-IO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4283</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4283</id><created>2013-02-18</created><updated>2013-02-21</updated><authors><author><keyname>Alves</keyname><forenames>Hirley</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>On the Fly Self-Organized Base Station Placement</title><categories>cs.NI cs.IT math.IT</categories><comments>This paper has been withdrawn because this isnt the final version of
  the manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the deployment of base stations (BSs) in a
one-dimensional network in which the users are randomly distributed.In order to
take into account the users' distribution to optimally place the BSs we
optimize the uplink MMSE sum rate. Moreover, given a massive number of antennas
at the BSs we propose a novel random matrix theory-based technique so as to
obtain tight approximations for the MMSE sum rate in the uplink. We investigate
a cooperative (CP) scenario where the BSs jointly decode the messages and a
non-cooperative (NCP) scheme in which the BS can only decode its own users. Our
results show that the CP strategy considerably outperforms the NCP case.
Moreover, we show that there exists a trade off in the BS deployment regarding
the position of each BS. Thus, through location games we can optimize the
position of each BS in order to maximize the system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4295</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4295</id><created>2013-02-18</created><updated>2013-10-17</updated><authors><author><keyname>Kuperberg</keyname><forenames>Greg</forenames></author><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author><author><keyname>Peled</keyname><forenames>Ron</forenames></author></authors><title>Probabilistic existence of regular combinatorial structures</title><categories>math.CO cs.CC math.PR</categories><comments>An extended abstract of this work appeared in STOC 2012.
  arXiv:1111.0492. Version 2 has a slightly improved presentation with more
  accurate literature references</comments><msc-class>05B30, 60C05</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show the existence of regular combinatorial objects which previously were
not known to exist. Specifically, for a wide range of the underlying
parameters, we show the existence of non-trivial orthogonal arrays, t-designs,
and t-wise permutations. In all cases, the sizes of the objects are optimal up
to polynomial overhead. The proof of existence is probabilistic. We show that a
randomly chosen structure has the required properties with positive yet tiny
probability. Our method allows also to give rather precise estimates on the
number of objects of a given size and this is applied to count the number of
orthogonal arrays, t-designs and regular hypergraphs. The main technical
ingredient is a special local central limit theorem for suitable lattice random
walks with finitely many steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4297</identifier>
 <datestamp>2013-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4297</id><created>2013-02-18</created><updated>2013-05-14</updated><authors><author><keyname>Sabato</keyname><forenames>Sivan</forenames></author><author><keyname>Kalai</keyname><forenames>Adam</forenames></author></authors><title>Feature Multi-Selection among Subjective Features</title><categories>cs.LG stat.ML</categories><journal-ref>S. Sabato and A. Kalai, &quot;Feature Multi-Selection among Subjective
  Features&quot;, Proceedings of the 30th International Conference on Machine
  Learning (ICML), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When dealing with subjective, noisy, or otherwise nebulous features, the
&quot;wisdom of crowds&quot; suggests that one may benefit from multiple judgments of the
same feature on the same object. We give theoretically-motivated `feature
multi-selection' algorithms that choose, among a large set of candidate
features, not only which features to judge but how many times to judge each
one. We demonstrate the effectiveness of this approach for linear regression on
a crowdsourced learning task of predicting people's height and weight from
photos, using features such as 'gender' and 'estimated weight' as well as
culturally fraught ones such as 'attractive'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4317</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4317</id><created>2013-02-18</created><authors><author><keyname>Hong</keyname><forenames>Dohy</forenames></author></authors><title>Introducing One Step Back Iterative Approach to Solve Linear and Non
  Linear Fixed Point Problem</title><categories>cs.NA math.NA</categories><comments>2 pages</comments><acm-class>G.1.0; G.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a new iterative method which we call one step
back approach: the main idea is to anticipate the consequence of the iterative
computation per coordinate and to optimize on the choice of the sequence of the
coordinates on which the iterative update computations are done. The method
requires the increase of the size of the state vectors and one iteration step
loss from the initial vector. We illustrate the approach in linear and non
linear iterative equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4332</identifier>
 <datestamp>2013-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4332</id><created>2013-02-18</created><authors><author><keyname>Beyer</keyname><forenames>Lucas</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>Streaming Data from HDD to GPUs for Sustained Peak Performance</title><categories>cs.DC cs.CE cs.MS q-bio.GN</categories><report-no>AICES-2013/02-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of the genome-wide association studies (GWAS), one has to
solve long sequences of generalized least-squares problems; such a task has two
limiting factors: execution time --often in the range of days or weeks-- and
data management --data sets in the order of Terabytes. We present an algorithm
that obviates both issues. By pipelining the computation, and thanks to a
sophisticated transfer strategy, we stream data from hard disk to main memory
to GPUs and achieve sustained peak performance; with respect to a
highly-optimized CPU implementation, our algorithm shows a speedup of 2.6x.
Moreover, the approach lends itself to multiple GPUs and attains almost perfect
scalability. When using 4 GPUs, we observe speedups of 9x over the
aforementioned implementation, and 488x over a widespread biology library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4343</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4343</id><created>2013-02-18</created><authors><author><keyname>Kar</keyname><forenames>Purushottam</forenames></author><author><keyname>Karnick</keyname><forenames>Harish</forenames></author></authors><title>On Translation Invariant Kernels and Screw Functions</title><categories>math.FA cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the connection between Hilbertian metrics and positive definite
kernels on the real line. In particular, we look at a well-known
characterization of translation invariant Hilbertian metrics on the real line
by von Neumann and Schoenberg (1941). Using this result we are able to give an
alternate proof of Bochner's theorem for translation invariant positive
definite kernels on the real line (Rudin, 1962).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4347</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4347</id><created>2013-02-18</created><authors><author><keyname>Sviridenko</keyname><forenames>Maxim</forenames></author><author><keyname>Ward</keyname><forenames>Justin</forenames></author></authors><title>Large Neighborhood Local Search for the Maximum Set Packing Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the classical maximum set packing problem where set
cardinality is upper bounded by $k$. We show how to design a variant of a
polynomial-time local search algorithm with performance guarantee $(k+2)/3$.
This local search algorithm is a special case of a more general procedure that
allows to swap up to $\Theta(\log n)$ elements per iteration. We also design
problem instances with locality gap $k/3$ even for a wide class of exponential
time local search procedures, which can swap up to $cn$ elements for a constant
$c$. This shows that our analysis of this class of algorithms is almost tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4350</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4350</id><created>2013-02-18</created><updated>2013-06-17</updated><authors><author><keyname>Sankaran</keyname><forenames>Abhisekh</forenames></author><author><keyname>Adsul</keyname><forenames>Bharat</forenames></author><author><keyname>Chakraborty</keyname><forenames>Supratik</forenames></author></authors><title>Generalizations of the Los-Tarski Preservation Theorem</title><categories>cs.LO</categories><comments>Added 2 new results: (a) A preservation theorem providing a semantic
  characterization of \Sigma^0_n theories for each natural number n (which
  builds on our generalization of the existential amalgamation theorem) (b)
  Theories in PSC(k) and PSC_f are equivalent to \Sigma^0_2 theories and that
  the latter are strictly more general than the former. These results are in
  Sections 8 and 9</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new preservation theorems that semantically characterize the
$\exists^k \forall^*$ and $\forall^k \exists^*$ prefix classes of first order
logic, for each natural number $k$. Unlike preservation theorems in the
literature that characterize the $\exists^* \forall^*$ and $\forall^*
\exists^*$ prefix classes, our theorems relate the count of quantifiers in the
leading block of the quantifier prefix to natural quantitative properties of
the models. As special cases of our results, we obtain the classical Los-Tarski
preservation theorem for sentences in both its extensional and substructural
versions. For arbitrary finite vocabularies, we also generalize the extensional
version of the Los-Tarski preservation theorem for theories. We also present an
interpolant-based approach towards these results. Finally, we present partial
results towards generalizing to theories, the substructural version of the
Los-Tarski theorem and in the process, we give a preservation theorem that
provides a semantic characterization of $\Sigma^0_n$ theories for each natural
number $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4359</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4359</id><created>2013-02-18</created><authors><author><keyname>Avgustinovich</keyname><forenames>Sergey</forenames></author><author><keyname>Puzynina</keyname><forenames>Svetlana</forenames></author></authors><title>Weak abelian periodicity of infinite words</title><categories>math.CO cs.DM</categories><comments>This is a preliminary version an extended abstract accepted for CSR
  2013</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We say that an infinite word w is weak abelian periodic if it can be
factorized into finite words with the same frequencies of letters. In the paper
we study properties of weak abelian periodicity, its relations with balance and
frequency. We establish necessary and sufficient conditions for weak abelian
periodicity of fixed points of uniform binary morphisms. Also, we discuss weak
abelian periodicity in minimal subshifts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4381</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4381</id><created>2013-02-18</created><updated>2014-01-06</updated><authors><author><keyname>Maier</keyname><forenames>Marc</forenames></author><author><keyname>Marazopoulou</keyname><forenames>Katerina</forenames></author><author><keyname>Jensen</keyname><forenames>David</forenames></author></authors><title>Reasoning about Independence in Probabilistic Models of Relational Data</title><categories>cs.AI</categories><comments>61 pages, substantial revisions to formalisms, theory, and related
  work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the theory of d-separation to cases in which data instances are not
independent and identically distributed. We show that applying the rules of
d-separation directly to the structure of probabilistic models of relational
data inaccurately infers conditional independence. We introduce relational
d-separation, a theory for deriving conditional independence facts from
relational models. We provide a new representation, the abstract ground graph,
that enables a sound, complete, and computationally efficient method for
answering d-separation queries about relational models, and we present
empirical results that demonstrate effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4382</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4382</id><created>2013-02-15</created><authors><author><keyname>Arnela</keyname><forenames>Marc</forenames></author><author><keyname>Guasch</keyname><forenames>Oriol</forenames></author></authors><title>Finite element computation of elliptical vocal tract impedances using
  the two-microphone transfer function method</title><categories>cs.SD physics.class-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The experimental two-microphone transfer function method (TMTF) is adapted to
the numerical framework to compute the radiation and input impedances of
three-dimensional vocal tracts of elliptical cross section. In its simplest
version, the TMTF method only requires measuring the acoustic pressure at two
points in an impedance duct and the postprocessing of the corresponding
transfer function. However, some considerations are to be taken into account
when using the TMTF method in the numerical context, which constitute the main
objective of this paper. In particular, the importance of including absorption
at the impedance duct walls to avoid lengthy numerical simulations is discussed
and analytical complex axial wave numbers for elliptical ducts are derived for
this purpose. It is also shown how the plane wave restriction of the TMTF
method can be circumvented to some extent by appropriate location of the
virtual microphones, thus extending the method frequency range of validity.
Virtual microphone spacing is also discussed on the basis of the so called
singularity factor. Numerical examples include the computation of the radiation
impedance of vowels /a/, /i/ and /u/ and the input impedance of vowel /a/, for
simplified vocal tracts of circular and elliptical cross sections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4383</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4383</id><created>2013-02-18</created><authors><author><keyname>Allahverdyan</keyname><forenames>Armen E.</forenames></author><author><keyname>Deng</keyname><forenames>Weibing</forenames></author><author><keyname>Wang</keyname><forenames>Q. A.</forenames></author></authors><title>Explaining Zipf's Law via Mental Lexicon</title><categories>physics.data-an cond-mat.stat-mech cs.CL</categories><doi>10.1103/PhysRevE.88.062804</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Zipf's law is the major regularity of statistical linguistics that served
as a prototype for rank-frequency relations and scaling laws in natural
sciences. Here we show that the Zipf's law -- together with its applicability
for a single text and its generalizations to high and low frequencies including
hapax legomena -- can be derived from assuming that the words are drawn into
the text with random probabilities. Their apriori density relates, via the
Bayesian statistics, to general features of the mental lexicon of the author
who produced the text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4384</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4384</id><created>2013-02-18</created><updated>2013-02-19</updated><authors><author><keyname>Zitt</keyname><forenames>Michel</forenames></author><author><keyname>Cointet</keyname><forenames>Jean-Philippe</forenames></author></authors><title>Citation impacts revisited: how novel impact measures reflect
  interdisciplinarity and structural change at the local and global level</title><categories>cs.DL physics.soc-ph</categories><comments>submitted to ISSI 2013, Vienna</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citation networks have fed numerous works in scientific evaluation, science
mapping (and more recently large-scale network studies) for decades. The
variety of citation behavior across scientific fields is both a research topic
in sociology of science, and a problem in scientific evaluation. Normalization,
tantamount to a particular weighting of links in the citation network, is
necessary for allowing across-field comparisons of citation scores and
interdisciplinary studies. In addition to classical normalization which
drastically reduces all variability factors altogether, two tracks of research
have emerged in the recent years. One is the revival of iterative &quot;influence
measures&quot;. The second is the &quot;citing-side&quot; normalization, whose only purpose is
to control for the main factor of variability, the inequality in citing
propensity, letting other aspects play: knowledge export/imports and growth.
When all variables are defined at the same field-level, two propositions are
established: (a) the gross impact measure identifies with the product of
relative growth rate, gross balance of citation exchanges, and relative number
of references (b) the normalized impact identifies with the product of relative
growth rate and normalized balance. At the science level, the variance of
growth rate over domains is a proxy for change in the system, and the variance
of balance a measure of inter-disciplinary dependences. This opens a new
perspective, where the resulting variance of normalized impact, and a related
measure, the sum of these variances proposed as a Change-Exchange Indicator,
summarize important aspects of science structure and dynamism. Results based on
a decade's data are discussed. The behavior of normalized impact according to
scale changes is also briefly discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4387</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4387</id><created>2013-02-18</created><updated>2013-06-01</updated><authors><author><keyname>Cesa-Bianchi</keyname><forenames>Nicolo</forenames></author><author><keyname>Dekel</keyname><forenames>Ofer</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>Online Learning with Switching Costs and Other Adaptive Adversaries</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the power of different types of adaptive (nonoblivious) adversaries
in the setting of prediction with expert advice, under both full-information
and bandit feedback. We measure the player's performance using a new notion of
regret, also known as policy regret, which better captures the adversary's
adaptiveness to the player's behavior. In a setting where losses are allowed to
drift, we characterize ---in a nearly complete manner--- the power of adaptive
adversaries with bounded memories and switching costs. In particular, we show
that with switching costs, the attainable rate with bandit feedback is
$\widetilde{\Theta}(T^{2/3})$. Interestingly, this rate is significantly worse
than the $\Theta(\sqrt{T})$ rate attainable with switching costs in the
full-information case. Via a novel reduction from experts to bandits, we also
show that a bounded memory adversary can force $\widetilde{\Theta}(T^{2/3})$
regret even in the full information case, proving that switching costs are
easier to control than bounded memory adversaries. Our lower bounds rely on a
new stochastic adversary strategy that generates loss processes with strong
dependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4389</identifier>
 <datestamp>2013-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4389</id><created>2013-02-18</created><updated>2013-09-20</updated><authors><author><keyname>Goodfellow</keyname><forenames>Ian J.</forenames></author><author><keyname>Warde-Farley</keyname><forenames>David</forenames></author><author><keyname>Mirza</keyname><forenames>Mehdi</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Maxout Networks</title><categories>stat.ML cs.LG</categories><comments>This is the version of the paper that appears in ICML 2013</comments><journal-ref>JMLR WCP 28 (3): 1319-1327, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of designing models to leverage a recently introduced
approximate model averaging technique called dropout. We define a simple new
model called maxout (so named because its output is the max of a set of inputs,
and because it is a natural companion to dropout) designed to both facilitate
optimization by dropout and improve the accuracy of dropout's fast approximate
model averaging technique. We empirically verify that the model successfully
accomplishes both of these tasks. We use maxout and dropout to demonstrate
state of the art classification performance on four benchmark datasets: MNIST,
CIFAR-10, CIFAR-100, and SVHN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4391</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4391</id><created>2013-02-18</created><updated>2013-03-21</updated><authors><author><keyname>Ghodsi</keyname><forenames>Mohammadreza</forenames></author></authors><title>Constructing a genome assembly that has the maximum likelihood</title><categories>cs.CE cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate genome assembly problem as an optimization problem in which the
objective function is the likelihood of the assembly given the reads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4400</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4400</id><created>2013-02-18</created><authors><author><keyname>Asinowski</keyname><forenames>Andrei</forenames></author><author><keyname>Miltzow</keyname><forenames>Tillmann</forenames></author><author><keyname>Rote</keyname><forenames>G&#xfc;nter</forenames></author></authors><title>Quasi-Parallel Segments and Characterization of Unique Bichromatic
  Matchings</title><categories>cs.CG</categories><comments>31 pages, 24 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given n red and n blue points in general position in the plane, it is
well-known that there is a perfect matching formed by non-crossing line
segments. We characterize the bichromatic point sets which admit exactly one
non-crossing matching. We give several geometric descriptions of such sets, and
find an O(nlogn) algorithm that checks whether a given bichromatic set has this
property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4405</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4405</id><created>2013-02-18</created><authors><author><keyname>Zhu</keyname><forenames>Junan</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author></authors><title>Performance Regions in Compressed Sensing from Noisy Measurements</title><categories>cs.IT math.IT</categories><comments>Accepted by CISS2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, compressed sensing with noisy measurements is addressed. The
theoretically optimal reconstruction error is studied by evaluating Tanaka's
equation. The main contribution is to show that in several regions, which have
different measurement rates and noise levels, the reconstruction error behaves
differently. This paper also evaluates the performance of the belief
propagation (BP) signal reconstruction method in the regions discovered. When
the measurement rate and the noise level lie in a certain region, BP is
suboptimal with respect to Tanaka's equation, and it may be possible to develop
reconstruction algorithms with lower error in that region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4406</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4406</id><created>2013-02-18</created><authors><author><keyname>Wojtczak</keyname><forenames>Dominik</forenames></author></authors><title>Optimal Scheduling for Linear-Rate Multi-Mode Systems</title><categories>cs.FL cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear-Rate Multi-Mode Systems is a model that can be seen both as a subclass
of switched linear systems with imposed global safety constraints and as hybrid
automata with no guards on transitions. We study the existence and design of a
controller for this model that keeps the state of the system within a given
safe set for the whole time. A sufficient and necessary condition is given for
such a controller to exist as well as an algorithm that finds one in polynomial
time. We further generalise the model by adding costs on modes and present an
algorithm that constructs a safe controller which minimises the peak cost, the
average-cost or any cost expressed as a weighted sum of these two. Finally, we
present numerical simulation results based on our implementation of these
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4412</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4412</id><created>2013-02-18</created><updated>2013-02-19</updated><authors><author><keyname>Mitzlaff</keyname><forenames>Folke</forenames></author><author><keyname>Stumme</keyname><forenames>Gerd</forenames></author></authors><title>Recommending Given Names</title><categories>cs.IR cs.SI physics.soc-ph</categories><comments>Baseline results for the ECML PKDD Discovery Challenge 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All over the world, future parents are facing the task of finding a suitable
given name for their child. This choice is influenced by different factors,
such as the social context, language, cultural background and especially
personal taste. Although this task is omnipresent, little research has been
conducted on the analysis and application of interrelations among given names
from a data mining perspective.
  The present work tackles the problem of recommending given names, by firstly
mining for inter-name relatedness in data from the Social Web. Based on these
results, the name search engine &quot;Nameling&quot; was built, which attracted more than
35,000 users within less than six months, underpinning the relevance of the
underlying recommendation task. The accruing usage data is then used for
evaluating different state-of-the-art recommendation systems, as well our new
NameRank algorithm which we adopted from our previous work on folksonomies and
which yields the best results, considering the trade-off between prediction
accuracy and runtime performance as well as its ability to generate
personalized recommendations. We also show, how the gathered inter-name
relationships can be used for meaningful result diversification of
PageRank-based recommendation systems.
  As all of the considered usage data is made publicly available, the present
work establishes baseline results, encouraging other researchers to implement
advanced recommendation systems for given names.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4414</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4414</id><created>2013-02-18</created><updated>2013-05-23</updated><authors><author><keyname>Beaumont</keyname><forenames>Olivier</forenames><affiliation>LaBRI, INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Duchon</keyname><forenames>Philippe</forenames><affiliation>LaBRI, INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Renaud-Goud</keyname><forenames>Paul</forenames><affiliation>LaBRI</affiliation></author></authors><title>Approximation Algorithms for Energy Minimization in Cloud Service
  Allocation under Reliability Constraints</title><categories>cs.DC</categories><proxy>ccsd</proxy><report-no>RR-8241</report-no><journal-ref>N&amp;deg; RR-8241 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider allocation problems that arise in the context of service
allocation in Clouds. More specifically, we assume on the one part that each
computing resource is associated to a capacity constraint, that can be chosen
using Dynamic Voltage and Frequency Scaling (DVFS) method, and to a probability
of failure. On the other hand, we assume that the service runs as a set of
independent instances of identical Virtual Machines. Moreover, there exists a
Service Level Agreement (SLA) between the Cloud provider and the client that
can be expressed as follows: the client comes with a minimal number of service
instances which must be alive at the end of the day, and the Cloud provider
offers a list of pairs (price,compensation), this compensation being paid by
the Cloud provider if it fails to keep alive the required number of services.
On the Cloud provider side, each pair corresponds actually to a guaranteed
success probability of fulfilling the constraint on the minimal number of
instances. In this context, given a minimal number of instances and a
probability of success, the question for the Cloud provider is to find the
number of necessary resources, their clock frequency and an allocation of the
instances (possibly using replication) onto machines. This solution should
satisfy all types of constraints during a given time period while minimizing
the energy consumption of used resources. We consider two energy consumption
models based on DVFS techniques, where the clock frequency of physical
resources can be changed. For each allocation problem and each energy model, we
prove deterministic approximation ratios on the consumed energy for algorithms
that provide guaranteed probability failures, as well as an efficient
heuristic, whose energy ratio is not guaranteed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4421</identifier>
 <datestamp>2013-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4421</id><created>2013-02-18</created><updated>2013-05-10</updated><authors><author><keyname>Gwynne</keyname><forenames>Matthew</forenames></author><author><keyname>Kullmann</keyname><forenames>Oliver</forenames></author></authors><title>Towards a theory of good SAT representations</title><categories>cs.AI cs.LO</categories><comments>59 pages; second version with some extended discussions and editorial
  corrections, third version with extended introduction, more examples and
  explanations, and some editorial improvements, fourth version with further
  examples, explanations and discussions, and with added computational
  experiments</comments><msc-class>68T15, 68T30</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We aim at providing a foundation of a theory of &quot;good&quot; SAT representations F
of boolean functions f. We argue that the hierarchy UC_k of unit-refutation
complete clause-sets of level k, introduced by the authors, provides the most
basic target classes, that is, F in UC_k is to be achieved for k as small as
feasible. If F does not contain new variables, i.e., F is equivalent (as a CNF)
to f, then F in UC_1 is similar to &quot;achieving (generalised) arc consistency&quot;
known from the literature (it is somewhat weaker, but theoretically much nicer
to handle). We show that for polysize representations of boolean functions in
this sense, the hierarchy UC_k is strict. The boolean functions for these
separations are &quot;doped&quot; minimally unsatisfiable clause-sets of deficiency 1;
these functions have been introduced in [Sloan, Soerenyi, Turan, 2007], and we
generalise their construction and show a correspondence to a strengthened
notion of irredundant sub-clause-sets. Turning from lower bounds to upper
bounds, we believe that many common CNF representations fit into the UC_k
scheme, and we give some basic tools to construct representations in UC_1 with
new variables, based on the Tseitin translation. Note that regarding new
variables the UC_1-representations are stronger than mere &quot;arc consistency&quot;,
since the new variables are not excluded from consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4426</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4426</id><created>2013-02-18</created><authors><author><keyname>Rajabi-Alni</keyname><forenames>Fatemeh</forenames></author></authors><title>A new algorithm for Many to Many Matching with Demands and Capacities</title><categories>cs.DS</categories><comments>9 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let A={a_1,a_2,...,a_s} and {b_1,b_2,...,b_t} with s+r=n, the many to many
point matching with demands and capacities matches each point a_i in A to at
least alpha_i and at most alpha_i points in B, and each point b_j in B to at
least beta_j and at most beta_j points in A for all 1 &lt;= i &lt;= s and 1 &lt;= j &lt;=
t. In this paper, we present an O(n^4) time and O(n) space algorithm for this
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4433</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4433</id><created>2013-02-17</created><authors><author><keyname>Cai</keyname><forenames>Yunlong</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Adaptive Minimum BER Reduced-Rank Linear Detection for Massive MIMO
  Systems</title><categories>cs.IT math.IT</categories><comments>6 figures. arXiv admin note: substantial text overlap with
  arXiv:1302.4130</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel adaptive reduced-rank strategy for very
large multiuser multi-input multi-output (MIMO) systems. The proposed
reduced-rank scheme is based on the concept of joint iterative optimization
(JIO) of filters according to the minimization of the bit error rate (BER) cost
function. The proposed optimization technique adjusts the weights of a
projection matrix and a reduced-rank filter jointly. We develop stochastic
gradient (SG) algorithms for their adaptive implementation and introduce a
novel automatic rank selection method based on the BER criterion. Simulation
results for multiuser MIMO systems show that the proposed adaptive algorithms
significantly outperform existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4462</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4462</id><created>2013-02-18</created><authors><author><keyname>Martinez-Rubi</keyname><forenames>O.</forenames></author><author><keyname>Veligatla</keyname><forenames>V. K.</forenames></author><author><keyname>de Bruyn</keyname><forenames>A. G.</forenames></author><author><keyname>Lampropoulos</keyname><forenames>P.</forenames></author><author><keyname>Offringa</keyname><forenames>A. R.</forenames></author><author><keyname>Jelic</keyname><forenames>V.</forenames></author><author><keyname>Yatawatta</keyname><forenames>S.</forenames></author><author><keyname>Koopmans</keyname><forenames>L. V. E.</forenames></author><author><keyname>Zaroubi</keyname><forenames>S.</forenames></author></authors><title>LEDDB: LOFAR Epoch of Reionization Diagnostic Database</title><categories>astro-ph.IM cs.DB</categories><comments>ADASS XXII proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the key science projects of the Low-Frequency Array (LOFAR) is the
detection of the cosmological signal coming from the Epoch of Reionization
(EoR). Here we present the LOFAR EoR Diagnostic Database (LEDDB) that is used
in the storage, management, processing and analysis of the LOFAR EoR
observations. It stores referencing information of the observations and
diagnostic parameters extracted from their calibration. This stored data is
used to ease the pipeline processing, monitor the performance of the telescope
and visualize the diagnostic parameters which facilitates the analysis of the
several contamination effects on the signals. It is implemented with PostgreSQL
and accessed through the psycopg2 python module. We have developed a very
flexible query engine, which is used by a web user interface to access the
database, and a very extensive set of tools for the visualization of the
diagnostic parameters through all their multiple dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4463</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4463</id><created>2013-02-18</created><authors><author><keyname>Jarollahi</keyname><forenames>Hooman</forenames></author><author><keyname>Gripon</keyname><forenames>Vincent</forenames></author><author><keyname>Onizawa</keyname><forenames>Naoya</forenames></author><author><keyname>Gross</keyname><forenames>Warren J.</forenames></author></authors><title>A Low-Power Content-Addressable-Memory Based on
  Clustered-Sparse-Networks</title><categories>cs.AR</categories><comments>Submitted to IEEE ASAP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A low-power Content-Addressable-Memory (CAM) is introduced employing a new
mechanism for associativity between the input tags and the corresponding
address of the output data. The proposed architecture is based on a recently
developed clustered-sparse-network using binary-weighted connections that
on-average will eliminate most of the parallel comparisons performed during a
search. Therefore, the dynamic energy consumption of the proposed design is
significantly lower compared to that of a conventional low-power CAM design.
Given an input tag, the proposed architecture computes a few possibilities for
the location of the matched tag and performs the comparisons on them to locate
a single valid match. A 0.13 um CMOS technology was used for simulation
purposes. The energy consumption and the search delay of the proposed design
are 9.5%, and 30.4% of that of the conventional NAND architecture respectively
with a 3.4% higher number of transistors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4464</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4464</id><created>2013-02-18</created><authors><author><keyname>Jarollahi</keyname><forenames>Hooman</forenames><affiliation>EIT</affiliation></author><author><keyname>Hobson</keyname><forenames>Richard F.</forenames></author></authors><title>Dynamic Power Reduction in a Novel CMOS 5T-SRAM for Low-Power SoC</title><categories>cs.AR cs.ET</categories><comments>7 pages, CDES'10 - The 2010 International Conference on Computer
  Design</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses a novel five-transistor (5T) CMOS SRAM design with high
performance and reliability in 65nm CMOS, and illustrates how it reduces the
dynamic power consumption in comparison with the conventional and low-power 6T
SRAM counterparts. This design can be used as cache memory in processors and
low-power portable devices. The proposed SRAM cell features ~13% area reduction
compared to a conventional 6T cell, and features a unique bit-line and negative
supply voltage biasing methodology and ground control architecture to enhance
performance, and suppress standby leakage power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4465</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4465</id><created>2013-02-18</created><authors><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author><author><keyname>Oliveira</keyname><forenames>Osvaldo N.</forenames><suffix>Jr.</suffix></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author></authors><title>Unveiling the relationship between complex networks metrics and word
  senses</title><categories>physics.soc-ph cs.CL cs.SI physics.data-an</categories><comments>The Supplementary Information (SI) is available from
  http://dl.dropbox.com/u/2740286/epl_SI.pdf</comments><journal-ref>Europhysics Letters (2012) 98 18002</journal-ref><doi>10.1209/0295-5075/98/18002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The automatic disambiguation of word senses (i.e., the identification of
which of the meanings is used in a given context for a word that has multiple
meanings) is essential for such applications as machine translation and
information retrieval, and represents a key step for developing the so-called
Semantic Web. Humans disambiguate words in a straightforward fashion, but this
does not apply to computers. In this paper we address the problem of Word Sense
Disambiguation (WSD) by treating texts as complex networks, and show that word
senses can be distinguished upon characterizing the local structure around
ambiguous words. Our goal was not to obtain the best possible disambiguation
system, but we nevertheless found that in half of the cases our approach
outperforms traditional shallow methods. We show that the hierarchical
connectivity and clustering of words are usually the most relevant features for
WSD. The results reported here shine light on the relationship between semantic
and structural parameters of complex networks. They also indicate that when
combined with traditional techniques the complex network approach may be useful
to enhance the discrimination of senses in large texts
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4471</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4471</id><created>2013-02-18</created><authors><author><keyname>Silva</keyname><forenames>Thiago C.</forenames></author><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author></authors><title>Word sense disambiguation via high order of learning in complex networks</title><categories>physics.soc-ph cs.CL cs.SI physics.data-an</categories><comments>The Supplementary Information (SI) is hosted at
  http://dl.dropbox.com/u/2740286/epl_SI_9apr.pdf</comments><journal-ref>Europhysics Letters (2012) 98 58001</journal-ref><doi>10.1209/0295-5075/98/58001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks have been employed to model many real systems and as a
modeling tool in a myriad of applications. In this paper, we use the framework
of complex networks to the problem of supervised classification in the word
disambiguation task, which consists in deriving a function from the supervised
(or labeled) training data of ambiguous words. Traditional supervised data
classification takes into account only topological or physical features of the
input data. On the other hand, the human (animal) brain performs both low- and
high-level orders of learning and it has facility to identify patterns
according to the semantic meaning of the input data. In this paper, we apply a
hybrid technique which encompasses both types of learning in the field of word
sense disambiguation and show that the high-level order of learning can really
improve the accuracy rate of the model. This evidence serves to demonstrate
that the internal structures formed by the words do present patterns that,
generally, cannot be correctly unveiled by only traditional techniques.
Finally, we exhibit the behavior of the model for different weights of the low-
and high-level classifiers by plotting decision boundaries. This study helps
one to better understand the effectiveness of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4474</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4474</id><created>2013-02-18</created><authors><author><keyname>Huang</keyname><forenames>Shurui</forenames></author><author><keyname>Ramamoorthy</keyname><forenames>Aditya</forenames></author></authors><title>On the multiple unicast capacity of 3-source, 3-terminal directed
  acyclic networks</title><categories>cs.IT cs.NI math.IT</categories><comments>To appear in the IEEE/ACM Transactions on Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the multiple unicast problem with three source-terminal pairs
over directed acyclic networks with unit-capacity edges. The three $s_i-t_i$
pairs wish to communicate at unit-rate via network coding. The connectivity
between the $s_i - t_i$ pairs is quantified by means of a connectivity level
vector, $[k_1 k_2 k_3]$ such that there exist $k_i$ edge-disjoint paths between
$s_i$ and $t_i$. In this work we attempt to classify networks based on the
connectivity level. It can be observed that unit-rate transmission can be
supported by routing if $k_i \geq 3$, for all $i = 1, \dots, 3$. In this work,
we consider, connectivity level vectors such that $\min_{i = 1, \dots, 3} k_i &lt;
3$. We present either a constructive linear network coding scheme or an
instance of a network that cannot support the desired unit-rate requirement,
for all such connectivity level vectors except the vector $[1~2~4]$ (and its
permutations). The benefits of our schemes extend to networks with higher and
potentially different edge capacities. Specifically, our experimental results
indicate that for networks where the different source-terminal paths have a
significant overlap, our constructive unit-rate schemes can be packed along
with routing to provide higher throughput as compared to a pure routing
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4475</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4475</id><created>2013-02-18</created><updated>2013-02-20</updated><authors><author><keyname>Kotomin</keyname><forenames>Emil</forenames></author></authors><title>In Love With a Robot: the Dawn of Machine-To-Machine Marketing</title><categories>cs.AI cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article looks at mass market artificial intelligence tools in the context
of their ever-growing sophistication, availability and market penetration. The
subject is especially relevant today for these exact reasons - if a few years
ago AI was the subject of high tech research and science fiction novels, today,
we increasingly rely on cloud robotics to cater to our daily needs - to trade
stock, predict weather, manage diaries, find friends and buy presents online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4489</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4489</id><created>2013-02-18</created><authors><author><keyname>Liu</keyname><forenames>Sa</forenames></author><author><keyname>Zhang</keyname><forenames>Chengzhi</forenames></author></authors><title>Termhood-based Comparability Metrics of Comparable Corpus in Special
  Domain</title><categories>cs.CL</categories><journal-ref>Lecture Notes in Computer Science Volume 7717, 2013, pp 134-144</journal-ref><doi>10.1007/978-3-642-36337-5_15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross-Language Information Retrieval (CLIR) and machine translation (MT)
resources, such as dictionaries and parallel corpora, are scarce and hard to
come by for special domains. Besides, these resources are just limited to a few
languages, such as English, French, and Spanish and so on. So, obtaining
comparable corpora automatically for such domains could be an answer to this
problem effectively. Comparable corpora, that the subcorpora are not
translations of each other, can be easily obtained from web. Therefore,
building and using comparable corpora is often a more feasible option in
multilingual information processing. Comparability metrics is one of key issues
in the field of building and using comparable corpus. Currently, there is no
widely accepted definition or metrics method of corpus comparability. In fact,
Different definitions or metrics methods of comparability might be given to
suit various tasks about natural language processing. A new comparability,
namely, termhood-based metrics, oriented to the task of bilingual terminology
extraction, is proposed in this paper. In this method, words are ranked by
termhood not frequency, and then the cosine similarities, calculated based on
the ranking lists of word termhood, is used as comparability. Experiments
results show that termhood-based metrics performs better than traditional
frequency-based metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4490</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4490</id><created>2013-02-18</created><authors><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author><author><keyname>Aluisio</keyname><forenames>Sandra M.</forenames></author><author><keyname>Oliveira</keyname><forenames>Osvaldo N.</forenames><suffix>Jr.</suffix></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author></authors><title>Complex networks analysis of language complexity</title><categories>physics.soc-ph cs.CL cs.SI physics.data-an</categories><comments>The Supplementary Information (SI) is available from
  https://dl.dropbox.com/u/2740286/supplementary.pdf</comments><journal-ref>Europhysics Letters (2012) 100 58002</journal-ref><doi>10.1209/0295-5075/100/58002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methods from statistical physics, such as those involving complex networks,
have been increasingly used in quantitative analysis of linguistic phenomena.
In this paper, we represented pieces of text with different levels of
simplification in co-occurrence networks and found that topological regularity
correlated negatively with textual complexity. Furthermore, in less complex
texts the distance between concepts, represented as nodes, tended to decrease.
The complex networks metrics were treated with multivariate pattern recognition
techniques, which allowed us to distinguish between original texts and their
simplified versions. For each original text, two simplified versions were
generated manually with increasing number of simplification operations. As
expected, distinction was easier for the strongly simplified versions, where
the most relevant metrics were node strength, shortest paths and diversity.
Also, the discrimination of complex texts was improved with higher hierarchical
network metrics, thus pointing to the usefulness of considering wider contexts
around the concepts. Though the accuracy rate in the distinction was not as
high as in methods using deep linguistic knowledge, the complex network
approach is still useful for a rapid screening of texts whenever assessing
complexity is essential to guarantee accessibility to readers with limited
reading ability
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4492</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4492</id><created>2013-02-18</created><authors><author><keyname>Zhang</keyname><forenames>Chengzhi</forenames></author><author><keyname>Wu</keyname><forenames>Dan</forenames></author></authors><title>Bilingual Terminology Extraction Using Multi-level Termhood</title><categories>cs.CL</categories><journal-ref>Electronic Library, The, Vol. 30 Iss: 2, 2012, pp.295 - 309</journal-ref><doi>10.1108/02640471211221395</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: Terminology is the set of technical words or expressions used in
specific contexts, which denotes the core concept in a formal discipline and is
usually applied in the fields of machine translation, information retrieval,
information extraction and text categorization, etc. Bilingual terminology
extraction plays an important role in the application of bilingual dictionary
compilation, bilingual Ontology construction, machine translation and
cross-language information retrieval etc. This paper addresses the issues of
monolingual terminology extraction and bilingual term alignment based on
multi-level termhood.
  Design/methodology/approach: A method based on multi-level termhood is
proposed. The new method computes the termhood of the terminology candidate as
well as the sentence that includes the terminology by the comparison of the
corpus. Since terminologies and general words usually have differently
distribution in the corpus, termhood can also be used to constrain and enhance
the performance of term alignment when aligning bilingual terms on the parallel
corpus. In this paper, bilingual term alignment based on termhood constraints
is presented.
  Findings: Experiment results show multi-level termhood can get better
performance than existing method for terminology extraction. If termhood is
used as constrain factor, the performance of bilingual term alignment can be
improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4504</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4504</id><created>2013-02-18</created><authors><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author><author><keyname>Oliveira</keyname><forenames>Osvaldo N.</forenames><suffix>Jr.</suffix></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author></authors><title>On the use of topological features and hierarchical characterization for
  disambiguating names in collaborative networks</title><categories>physics.soc-ph cs.DL cs.IR cs.SI</categories><journal-ref>Europhysics Letters (2012) 99 48002</journal-ref><doi>10.1209/0295-5075/99/48002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many features of complex systems can now be unveiled by applying statistical
physics methods to treat them as social networks. The power of the analysis may
be limited, however, by the presence of ambiguity in names, e.g., caused by
homonymy in collaborative networks. In this paper we show that the ability to
distinguish between homonymous authors is enhanced when longer-distance
connections are considered, rather than looking at only the immediate neighbors
of a node in the collaborative network. Optimized results were obtained upon
using the 3rd hierarchy in connections. Furthermore, reasonable distinction
among authors could also be achieved upon using pattern recognition strategies
for the data generated from the topology of the collaborative network. These
results were obtained with a network from papers in the arXiv repository, into
which homonymy was deliberately introduced to test the methods with a
controlled, reliable dataset. In all cases, several methods of supervised and
unsupervised machine learning were used, leading to the same overall results.
The suitability of using deeper hierarchies and network topology was confirmed
with a real database of movie actors, with the additional finding that the
distinguishing ability can be further enhanced by combining topology features
and long-range connections in the collaborative network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4510</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4510</id><created>2013-02-18</created><authors><author><keyname>Sharma</keyname><forenames>Avinash</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Anurag</forenames></author><author><keyname>Tak</keyname><forenames>Nikhar</forenames></author><author><keyname>Sharma</keyname><forenames>Anuradha</forenames></author><author><keyname>Avasthi</keyname><forenames>Jitendra</forenames></author><author><keyname>sharma</keyname><forenames>Prerna</forenames></author></authors><title>An Approach Of Substitution Method Based On ASCII Codes In Encryption
  Technique</title><categories>cs.CR</categories><comments>7 pages</comments><journal-ref>International Journal of advanced studies in Computers, Science &amp;
  Engineering, vol 1, Issue 3, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In poly alphabetic substitution the plain texts letters are enciphered
differently according to their position. The name poly alphabetic suggests that
there are more than one key so we have used two keys combination instead of
just one, in order to produce the cipher text. We can also use three or more
keys to make the enciphering process more complicated. In this paper have
produced ASCII Codes of the plain text and then we have reversed it said
reverse ASCII Codes and then we have generated two keys K1 is generated by
addition of reverse ASCII Codes and K2 is generated by addition of ASCII Codes.
Then these K1 and K2 Keys are alternatively applied on Reverse ASCII codes in
order to produce cipher text. On the Destination hand Deciphering is used to
produce the plain text again. Our technique generates random cipher text for
the same plain text and this is the major advantage of our technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4516</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4516</id><created>2013-02-18</created><authors><author><keyname>Van Nguyen</keyname><forenames>Thuy</forenames></author><author><keyname>Nosratinia</keyname><forenames>Aria</forenames></author><author><keyname>Divsalar</keyname><forenames>Dariush</forenames></author></authors><title>Bilayer Protograph Codes for Half-Duplex Relay Channels</title><categories>cs.IT math.IT</categories><comments>Accepted in IEEE Trans. Wireless Comm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite encouraging advances in the design of relay codes, several important
challenges remain. Many of the existing LDPC relay codes are tightly optimized
for fixed channel conditions and not easily adapted without extensive
re-optimization of the code. Some have high encoding complexity and some need
long block lengths to approach capacity. This paper presents a high-performance
protograph-based LDPC coding scheme for the half-duplex relay channel that
addresses simultaneously several important issues: structured coding that
permits easy design, low encoding complexity, embedded structure for convenient
adaptation to various channel conditions, and performance close to capacity
with a reasonable block length. The application of the coding structure to
multi-relay networks is demonstrated. Finally, a simple new methodology for
evaluating the end-to-end error performance of relay coding systems is
developed and used to highlight the performance of the proposed codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4519</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4519</id><created>2013-02-19</created><authors><author><keyname>Quang-Hung</keyname><forenames>Nguyen</forenames></author><author><keyname>Nien</keyname><forenames>Pham Dac</forenames></author><author><keyname>Nam</keyname><forenames>Nguyen Hoai</forenames></author><author><keyname>Tuong</keyname><forenames>Nguyen Huynh</forenames></author><author><keyname>Thoai</keyname><forenames>Nam</forenames></author></authors><title>A Genetic Algorithm for Power-Aware Virtual Machine Allocation in
  Private Cloud</title><categories>cs.NE cs.DC</categories><comments>10 pages</comments><journal-ref>Information and Communication Technology, Lecture Notes in
  Computer Science, Vol. 7804, Information Systems and Applications, incl.
  Internet/Web, and HCI, IFIP-LNCS Volumes, ISBN 978-3-642-36817-2, 2013, XVI,
  552 p. 170 illus</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Energy efficiency has become an important measurement of scheduling algorithm
for private cloud. The challenge is trade-off between minimizing of energy
consumption and satisfying Quality of Service (QoS) (e.g. performance or
resource availability on time for reservation request). We consider resource
needs in context of a private cloud system to provide resources for
applications in teaching and researching. In which users request computing
resources for laboratory classes at start times and non-interrupted duration in
some hours in prior. Many previous works are based on migrating techniques to
move online virtual machines (VMs) from low utilization hosts and turn these
hosts off to reduce energy consumption. However, the techniques for migration
of VMs could not use in our case. In this paper, a genetic algorithm for
power-aware in scheduling of resource allocation (GAPA) has been proposed to
solve the static virtual machine allocation problem (SVMAP). Due to limited
resources (i.e. memory) for executing simulation, we created a workload that
contains a sample of one-day timetable of lab hours in our university. We
evaluate the GAPA and a baseline scheduling algorithm (BFD), which sorts list
of virtual machines in start time (i.e. earliest start time first) and using
best-fit decreasing (i.e. least increased power consumption) algorithm, for
solving the same SVMAP. As a result, the GAPA algorithm obtains total energy
consumption is lower than the baseline algorithm on simulated experimentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4536</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4536</id><created>2013-02-19</created><updated>2014-01-10</updated><authors><author><keyname>Chakrabarty</keyname><forenames>Deeparnab</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>A o(n) monotonicity tester for Boolean functions over the hypercube</title><categories>cs.DM cs.DS math.CO</categories><comments>Journal version, with discussion on directed isoperimetry</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Boolean function $f:\{0,1\}^n \mapsto \{0,1\}$ is said to be $\eps$-far
from monotone if $f$ needs to be modified in at least $\eps$-fraction of the
points to make it monotone. We design a randomized tester that is given oracle
access to $f$ and an input parameter $\eps&gt;0$, and has the following guarantee:
It outputs {\sf Yes} if the function is monotonically non-decreasing, and
outputs {\sf No} with probability $&gt;2/3$, if the function is $\eps$-far from
monotone. This non-adaptive, one-sided tester makes
$O(n^{7/8}\eps^{-3/2}\ln(1/\eps))$ queries to the oracle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4539</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4539</id><created>2013-02-19</created><authors><author><keyname>Ganty</keyname><forenames>Pierre</forenames></author><author><keyname>Genaim</keyname><forenames>Samir</forenames></author></authors><title>Proving Termination Starting from the End</title><categories>cs.LO</categories><comments>16 pages</comments><doi>10.1007/978-3-642-39799-8_27</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel technique for proving program termination which introduces
a new dimension of modularity. Existing techniques use the program to
incrementally construct a termination proof. While the proof keeps changing,
the program remains the same. Our technique goes a step further. We show how to
use the current partial proof to partition the transition relation into those
behaviors known to be terminating from the current proof, and those whose
status (terminating or not) is not known yet. This partition enables a new and
unexplored dimension of incremental reasoning on the program side. In addition,
we show that our approach naturally applies to conditional termination which
searches for a precondition ensuring termination. We further report on a
prototype implementation that advances the state-of-the-art on the grounds of
termination and conditional termination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4544</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4544</id><created>2013-02-19</created><authors><author><keyname>Sarma</keyname><forenames>Atish Das</forenames></author><author><keyname>Nanongkai</keyname><forenames>Danupon</forenames></author><author><keyname>Pandurangan</keyname><forenames>Gopal</forenames></author><author><keyname>Tetali</keyname><forenames>Prasad</forenames></author></authors><title>Distributed Random Walks</title><categories>cs.DC cs.DS</categories><comments>Preprint of an article to appear in Journal of the ACM in February
  2013. The official journal version has several gramatical corrections.
  Preliminary versions of this paper appeared in PODC 2009 and PODC 2010. arXiv
  admin note: substantial text overlap with arXiv:0911.3195, arXiv:1205.5525</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performing random walks in networks is a fundamental primitive that has found
applications in many areas of computer science, including distributed
computing. In this paper, we focus on the problem of sampling random walks
efficiently in a distributed network and its applications. Given bandwidth
constraints, the goal is to minimize the number of rounds required to obtain
random walk samples.
  All previous algorithms that compute a random walk sample of length $\ell$ as
a subroutine always do so naively, i.e., in $O(\ell)$ rounds. The main
contribution of this paper is a fast distributed algorithm for performing
random walks. We present a sublinear time distributed algorithm for performing
random walks whose time complexity is sublinear in the length of the walk. Our
algorithm performs a random walk of length $\ell$ in $\tilde{O}(\sqrt{\ell D})$
rounds ($\tilde{O}$ hides $\polylog{n}$ factors where $n$ is the number of
nodes in the network) with high probability on an undirected network, where $D$
is the diameter of the network. For small diameter graphs, this is a
significant improvement over the naive $O(\ell)$ bound. Furthermore, our
algorithm is optimal within a poly-logarithmic factor as there exists a
matching lower bound [Nanongkai et al. PODC 2011]. We further extend our
algorithms to efficiently perform $k$ independent random walks in
$\tilde{O}(\sqrt{k\ell D} + k)$ rounds. We also show that our algorithm can be
applied to speedup the more general Metropolis-Hastings sampling.
  Our random walk algorithms can be used to speed up distributed algorithms in
applications that use random walks as a subroutine, such as computing a random
spanning tree and estimating mixing time and related parameters. Our algorithm
is fully decentralized and can serve as a building block in the design of
topologically-aware networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4545</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4545</id><created>2013-02-19</created><authors><author><keyname>Schipper</keyname><forenames>Burkhard C.</forenames></author></authors><title>Preference-Based Unawareness</title><categories>cs.GT cs.AI cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Morris (1996, 1997) introduced preference-based definitions of knowledge and
belief in standard state-space structures. This paper extends this
preference-based approach to unawareness structures (Heifetz, Meier, and
Schipper, 2006, 2008). By defining unawareness and knowledge in terms of
preferences over acts in unawareness structures and showing their equivalence
to the epistemic notions of unawareness and knowledge, we try to build a bridge
between decision theory and epistemic logic. Unawareness of an event is
characterized behaviorally as the event being null and its negation being null.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4546</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4546</id><created>2013-02-19</created><authors><author><keyname>Li</keyname><forenames>Rong-Hua</forenames></author><author><keyname>Yu</keyname><forenames>Jeffrey Xu</forenames></author><author><keyname>Huang</keyname><forenames>Xin</forenames></author><author><keyname>Cheng</keyname><forenames>Hong</forenames></author></authors><title>Random-walk domination in large graphs: problem definitions and fast
  solutions</title><categories>cs.SI cs.DS physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and formulate two types of random-walk domination problems in
graphs motivated by a number of applications in practice (e.g., item-placement
problem in online social network, Ads-placement problem in advertisement
networks, and resource-placement problem in P2P networks). Specifically, given
a graph $G$, the goal of the first type of random-walk domination problem is to
target $k$ nodes such that the total hitting time of an $L$-length random walk
starting from the remaining nodes to the targeted nodes is minimal. The second
type of random-walk domination problem is to find $k$ nodes to maximize the
expected number of nodes that hit any one targeted node through an $L$-length
random walk. We prove that these problems are two special instances of the
submodular set function maximization with cardinality constraint problem. To
solve them effectively, we propose a dynamic-programming (DP) based greedy
algorithm which is with near-optimal performance guarantee. The DP-based greedy
algorithm, however, is not very efficient due to the expensive marginal gain
evaluation. To further speed up the algorithm, we propose an approximate greedy
algorithm with linear time complexity w.r.t.\ the graph size and also with
near-optimal performance guarantee. The approximate greedy algorithm is based
on a carefully designed random-walk sampling and sample-materialization
techniques. Extensive experiments demonstrate the effectiveness, efficiency and
scalability of the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4549</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4549</id><created>2013-02-19</created><updated>2013-02-20</updated><authors><author><keyname>Ailon</keyname><forenames>Nir</forenames></author><author><keyname>Chen</keyname><forenames>Yudong</forenames></author><author><keyname>Huan</keyname><forenames>Xu</forenames></author></authors><title>Breaking the Small Cluster Barrier of Graph Clustering</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates graph clustering in the planted cluster model in the
presence of {\em small clusters}. Traditional results dictate that for an
algorithm to provably correctly recover the clusters, {\em all} clusters must
be sufficiently large (in particular, $\tilde{\Omega}(\sqrt{n})$ where $n$ is
the number of nodes of the graph). We show that this is not really a
restriction: by a more refined analysis of the trace-norm based recovery
approach proposed in Jalali et al. (2011) and Chen et al. (2012), we prove that
small clusters, under certain mild assumptions, do not hinder recovery of large
ones.
  Based on this result, we further devise an iterative algorithm to recover
{\em almost all clusters} via a &quot;peeling strategy&quot;, i.e., recover large
clusters first, leading to a reduced problem, and repeat this procedure. These
results are extended to the {\em partial observation} setting, in which only a
(chosen) part of the graph is observed.The peeling strategy gives rise to an
active learning algorithm, in which edges adjacent to smaller clusters are
queried more often as large clusters are learned (and removed).
  From a high level, this paper sheds novel insights on high-dimensional
statistics and learning structured data, by presenting a structured matrix
learning problem for which a one shot convex relaxation approach necessarily
fails, but a carefully constructed sequence of convex relaxationsdoes the job.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4557</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4557</id><created>2013-02-19</created><authors><author><keyname>P</keyname><forenames>Kirana Kumara</forenames></author></authors><title>Extracting Three Dimensional Surface Model of Human Kidney from the
  Visible Human Data Set using Free Software</title><categories>physics.med-ph cs.CE</categories><comments>14 pages, 8 figures, accepted version</comments><journal-ref>Leonardo Electronic Journal of Practices and Technologies (LEJPT),
  Issue 20 (January-June), 2012 (11), p. 115-126 [ISSN: 1583-1078]</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three dimensional digital model of a representative human kidney is needed
for a surgical simulator that is capable of simulating a laparoscopic surgery
involving kidney. Buying a three dimensional computer model of a representative
human kidney, or reconstructing a human kidney from an image sequence using
commercial software, both involve (sometimes significant amount of) money. In
this paper, author has shown that one can obtain a three dimensional surface
model of human kidney by making use of images from the Visible Human Data Set
and a few free software packages (ImageJ, ITK-SNAP, and MeshLab in particular).
Images from the Visible Human Data Set, and the software packages used here,
both do not cost anything. Hence, the practice of extracting the geometry of a
representative human kidney for free, as illustrated in the present work, could
be a free alternative to the use of expensive commercial software or to the
purchase of a digital model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4558</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4558</id><created>2013-02-19</created><authors><author><keyname>Aupy</keyname><forenames>Guillaume</forenames></author><author><keyname>Robert</keyname><forenames>Yves</forenames></author><author><keyname>Vivien</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Zaidouni</keyname><forenames>Dounia</forenames></author></authors><title>Checkpointing strategies with prediction windows</title><categories>cs.DC</categories><comments>35 pages, work supported by ANR Rescue. arXiv admin note: substantial
  text overlap with arXiv:1207.6936, arXiv:1302.3752</comments><report-no>INRIA RR-8239</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the impact of fault prediction techniques on
checkpointing strategies. We suppose that the fault-prediction system provides
prediction windows instead of exact predictions, which dramatically complicates
the analysis of the checkpointing strategies. We propose a new approach based
upon two periodic modes, a regular mode outside prediction windows, and a
proactive mode inside prediction windows, whenever the size of these windows is
large enough. We are able to compute the best period for any size of the
prediction windows, thereby deriving the scheduling strategy that minimizes
platform waste. In addition, the results of this analytical evaluation are
nicely corroborated by a comprehensive set of simulations, which demonstrate
the validity of the model and the accuracy of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4572</identifier>
 <datestamp>2013-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4572</id><created>2013-02-19</created><updated>2013-06-11</updated><authors><author><keyname>Klemm</keyname><forenames>Konstantin</forenames></author></authors><title>Searchability of central nodes in networks</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages, 3 figures, 1 table, v2: minor corrections</comments><journal-ref>Journal of Statistical Physics 151, 707-719 (2013)</journal-ref><doi>10.1007/s10955-013-0727-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks are discrete systems with a large amount of heterogeneity
among nodes (individuals). Measures of centrality aim at a quantification of
nodes' importance for structure and function. Here we ask to which extent the
most central nodes can be found by purely local search. We find that many
networks have close-to-optimal searchability under eigenvector centrality,
outperforming searches for degree and betweenness. Searchability of the
strongest spreaders in epidemic dynamics tends to be substantially larger for
supercritical than for subcritical spreading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4587</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4587</id><created>2013-02-19</created><updated>2014-02-01</updated><authors><author><keyname>Birn</keyname><forenames>Marcel</forenames></author><author><keyname>Osipov</keyname><forenames>Vitaly</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author><author><keyname>Sitchinava</keyname><forenames>Nodari</forenames></author></authors><title>Efficient Parallel and External Matching</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a simple algorithm for computing a matching on a graph runs in a
logarithmic number of phases incurring work linear in the input size. The
algorithm can be adapted to provide efficient algorithms in several models of
computation, such as PRAM, External Memory, MapReduce and distributed memory
models. Our CREW PRAM algorithm is the first O(log^2 n) time, linear work
algorithm. Our experimental results indicate the algorithm's high speed and
efficiency combined with good solution quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4591</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4591</id><created>2013-02-19</created><updated>2013-06-19</updated><authors><author><keyname>Summers</keyname><forenames>Ed</forenames></author><author><keyname>Salo</keyname><forenames>Dorothea</forenames></author></authors><title>Linking Things on the Web: A Pragmatic Examination of Linked Data for
  Libraries, Archives and Museums</title><categories>cs.DL</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The Web publishing paradigm of Linked Data has been gaining traction in the
cultural heritage sector: libraries, archives and museums. At first glance, the
principles of Linked Data seem simple enough. However experienced Web
developers, designers and architects who attempt to put these ideas into
practice often find themselves having to digest and understand debates about
Web architecture, the semantic web, artificial intelligence and the
philosophical nature of identity. In this paper I will discuss some of the
reasons why Linked Data is of interest to the cultural heritage community, what
some of the pain points are for deploying it, and characterize some pragmatic
ways for cultural heritage organizations to realize the goals of Linked Data
with examples from the Web we have today.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4609</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4609</id><created>2013-02-19</created><authors><author><keyname>Csirmaz</keyname><forenames>L.</forenames></author><author><keyname>Tardos</keyname><forenames>G.</forenames></author></authors><title>Optimal information rate of secret sharing schemes on trees</title><categories>cs.CR cs.DS</categories><comments>4 pages, 2 figures</comments><msc-class>05B40, 05C85, 94A62, 94A60</msc-class><acm-class>E.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The information rate for an access structure is the reciprocal of the load of
the optimal secret sharing scheme for this structure. We determine this value
for all trees: it is 1/(2-1/c), where c is the size of the largest core of the
tree. A subset of the vertices of a tree is a core if it induces a connected
subgraph and for each vertex in the subset one finds a neighbor outside the
subset. Our result follows from a lower and an upper bound on the information
rate that applies for any graph and happen to coincide for trees because of a
correspondence between the size of the largest core and a quantity related to a
fractional cover of the tree with stars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4617</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4617</id><created>2013-02-19</created><authors><author><keyname>Coulaud</keyname><forenames>Olivier</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Bordat</keyname><forenames>Patrice</forenames><affiliation>IPREM</affiliation></author><author><keyname>Fayon</keyname><forenames>Pierre</forenames><affiliation>IPREM</affiliation></author><author><keyname>Lebris</keyname><forenames>Vincent</forenames><affiliation>IPREM</affiliation></author><author><keyname>Baraille</keyname><forenames>Isabelle</forenames><affiliation>IPREM</affiliation></author><author><keyname>Brown</keyname><forenames>Ross</forenames><affiliation>IPREM</affiliation></author></authors><title>Extensions of the siesta dft code for simulation of molecules</title><categories>physics.comp-ph cond-mat.mtrl-sci cs.DC</categories><proxy>ccsd</proxy><report-no>RR-8221</report-no><journal-ref>N&amp;deg; RR-8221 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe extensions to the siesta density functional theory (dft) code
[30], for the simulation of isolated molecules and their absorption spectra.
The extensions allow for: - Use of a multi-grid solver for the Poisson equation
on a finite dft mesh. Non-periodic, Dirichlet boundary conditions are computed
by expansion of the electric multipoles over spherical harmonics. - Truncation
of a molecular system by the method of design atom pseudo- potentials of Xiao
and Zhang[32]. - Electrostatic potential fitting to determine effective atomic
charges. - Derivation of electronic absorption transition energies and
oscillator stren- gths from the raw spectra produced by a recently described,
order O(N3), time-dependent dft code[21]. The code is furthermore integrated
within siesta as a post-processing option.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4619</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4619</id><created>2013-02-19</created><authors><author><keyname>Lande</keyname><forenames>D. V.</forenames></author><author><keyname>Snarskii</keyname><forenames>A. A.</forenames></author></authors><title>Compactified Horizontal Visibility Graph for the Language Network</title><categories>cs.CL cs.DS</categories><comments>9 pages, 3 figures, 2 appendix tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A compactified horizontal visibility graph for the language network is
proposed. It was found that the networks constructed in such way are scale
free, and have a property that among the nodes with largest degrees there are
words that determine not only a text structure communication, but also its
informational structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4625</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4625</id><created>2013-02-19</created><updated>2014-04-12</updated><authors><author><keyname>Ba&#x10d;kurs</keyname><forenames>Art&#x16b;rs</forenames></author><author><keyname>Bavarian</keyname><forenames>Mohammad</forenames></author></authors><title>On the sum of $L1$ influences</title><categories>cs.CC</categories><comments>Proceedings of CCC (2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a function $f$ over the discrete cube, the total $L_1$ influence of $f$
is defined as $\sum_{i=1}^n \|\partial_i f\|_1$, where $\partial_i f$ denotes
the discrete derivative of $f$ in the direction $i$. In this work, we show that
the total $L_1$ influence of a $[-1,1]$-valued function $f$ can be upper
bounded by a polynomial in the degree of $f$, resolving affirmatively an open
problem of Aaronson and Ambainis (ITCS 2011). The main challenge here is that
the $L_1$ influences do not admit an easy Fourier analytic representation. In
our proof, we overcome this problem by introducing a new analytic quantity
$\mathcal I_p(f)$, relating this new quantity to the total $L_1$ influence of
$f$. This new quantity, which roughly corresponds to an average of the total
$L_1$ influences of some ensemble of functions related to $f$, has the benefit
of being much easier to analyze, allowing us to resolve the problem of Aaronson
and Ambainis. We also give an application of the theorem to graph theory, and
discuss the connection between the study of bounded functions over the cube and
the quantum query complexity of partial functions where Aaronson and Ambainis
encountered this question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4656</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4656</id><created>2013-02-19</created><authors><author><keyname>Kai</keyname><forenames>Caihong</forenames></author><author><keyname>Zhang</keyname><forenames>Shengli</forenames></author></authors><title>Throughput Analysis of CSMA Wireless Networks with Finite Offered-load</title><categories>cs.NI</categories><comments>6 pages. arXiv admin note: text overlap with arXiv:1007.5255 by other
  authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an approximate method, equivalent access intensity (EAI),
for the throughput analysis of CSMA wireless networks in which links have
finite offered-load and their MAC-layer transmit buffers may be empty from time
to time. Different from prior works that mainly considered the saturated
network, we take into account in our analysis the impacts of empty transmit
buffers on the interactions and dependencies among links in the network that is
more common in practice. It is known that the empty transmit buffer incurs
extra waiting time for a link to compete for the channel airtime usage, since
when it has no packet waiting for transmission, the link will not perform
channel competition. The basic idea behind EAI is that this extra waiting time
can be mapped to an equivalent &quot;longer&quot; backoff countdown time for the
unsaturated link, yielding a lower link access intensity that is defined as the
mean packet transmission time divided by the mean backoff countdown time. That
is, we can compute the &quot;equivalent access intensity&quot; of an unsaturated link to
incorporate the effects of the empty transmit buffer on its behavior of channel
competition. Then, prior saturated ideal CSMA network (ICN) model can be
adopted for link throughput computation. Specifically, we propose an iterative
algorithm, &quot;Compute-and-Compare&quot;, to identify which links are unsaturated under
current offered-load and protocol settings, compute their &quot;equivalent access
intensities&quot; and calculate link throughputs. Simulation shows that our
algorithm has high accuracy under various offered-load and protocol settings.
We believe the ability to identify unsaturated links and compute links
throughputs as established in this paper will serve an important first step
toward the design and optimization of general CSMA wireless networks with
offered-load control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4660</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4660</id><created>2013-02-19</created><authors><author><keyname>Reboredo</keyname><forenames>Hugo</forenames><affiliation>Instituto de Telecomunica&#xe7;&#xf5;es, Universidade do Porto, Portugal</affiliation></author><author><keyname>Renna</keyname><forenames>Francesco</forenames><affiliation>Instituto de Telecomunica&#xe7;&#xf5;es, Universidade do Porto, Portugal</affiliation></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames><affiliation>Department of ECE, Duke University, NC, USA</affiliation></author><author><keyname>Rodrigues</keyname><forenames>Miguel R. D.</forenames><affiliation>Department of E&amp;EE, University College London, UK</affiliation></author></authors><title>Compressive Classification</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, submitted to the 2013 IEEE International
  Symposium on Information Theory (ISIT 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper derives fundamental limits associated with compressive
classification of Gaussian mixture source models. In particular, we offer an
asymptotic characterization of the behavior of the (upper bound to the)
misclassification probability associated with the optimal Maximum-A-Posteriori
(MAP) classifier that depends on quantities that are dual to the concepts of
diversity gain and coding gain in multi-antenna communications. The diversity,
which is shown to determine the rate at which the probability of
misclassification decays in the low noise regime, is shown to depend on the
geometry of the source, the geometry of the measurement system and their
interplay. The measurement gain, which represents the counterpart of the coding
gain, is also shown to depend on geometrical quantities. It is argued that the
diversity order and the measurement gain also offer an optimization criterion
to perform dictionary learning for compressive classification applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4670</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4670</id><created>2013-02-19</created><authors><author><keyname>Tian</keyname><forenames>Chao</forenames></author><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Vaishampayan</keyname><forenames>Vinay A.</forenames></author></authors><title>Exact-Repair Regenerating Codes Via Layered Erasure Correction and Block
  Designs</title><categories>cs.IT math.IT</categories><comments>7 figures, 30 pages. Part of the result has been submitted to ISIT
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new class of exact-repair regenerating codes is constructed by combining
two layers of erasure correction codes together with combinatorial block
designs, e.g., Steiner systems, balanced incomplete block designs and
t-designs. The proposed codes have the &quot;uncoded repair&quot; property where the
nodes participating in the repair simply transfer part of the stored data
directly, without performing any computation. The layered error correction
structure makes the decoding process rather straightforward, and in general the
complexity is low. We show that this construction is able to achieve
performance better than time-sharing between the minimum storage regenerating
codes and the minimum repair-bandwidth regenerating codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4673</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4673</id><created>2013-02-19</created><authors><author><keyname>Scheirer</keyname><forenames>Walter J.</forenames></author><author><keyname>Wilber</keyname><forenames>Michael J.</forenames></author><author><keyname>Eckmann</keyname><forenames>Michael</forenames></author><author><keyname>Boult</keyname><forenames>Terrance E.</forenames></author></authors><title>Good Recognition is Non-Metric</title><categories>cs.CV</categories><comments>9 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognition is the fundamental task of visual cognition, yet how to formalize
the general recognition problem for computer vision remains an open issue. The
problem is sometimes reduced to the simplest case of recognizing matching
pairs, often structured to allow for metric constraints. However, visual
recognition is broader than just pair matching -- especially when we consider
multi-class training data and large sets of features in a learning context.
What we learn and how we learn it has important implications for effective
algorithms. In this paper, we reconsider the assumption of recognition as a
pair matching test, and introduce a new formal definition that captures the
broader context of the problem. Through a meta-analysis and an experimental
assessment of the top algorithms on popular data sets, we gain a sense of how
often metric properties are violated by good recognition algorithms. By
studying these violations, useful insights come to light: we make the case that
locally metric algorithms should leverage outside information to solve the
general recognition problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4677</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4677</id><created>2013-02-19</created><updated>2014-02-28</updated><authors><author><keyname>P&#xe1;lv&#xf6;lgyi</keyname><forenames>D&#xf6;m&#xf6;t&#xf6;r</forenames></author><author><keyname>Gy&#xe1;rf&#xe1;s</keyname><forenames>Andr&#xe1;s</forenames></author></authors><title>Domination in transitive colorings of tournaments</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An edge coloring of a tournament $T$ with colors $1,2,\dots,k$ is called \it
$k$-transitive \rm if the digraph $T(i)$ defined by the edges of color $i$ is
transitively oriented for each $1\le i \le k$. We explore a conjecture of the
second author: For each positive integer $k$ there exists a (least) $p(k)$ such
that every $k$-transitive tournament has a dominating set of at most $p(k)$
vertices.
  We show how this conjecture relates to other conjectures and results. For
example, it is a special case of a well-known conjecture of Erd\H os, Sands,
Sauer and Woodrow (so the conjecture is interesting even if false). We show
that the conjecture implies a stronger conjecture, a possible extension of a
result of B\'ar\'any and Lehel on covering point sets by boxes. The principle
used leads also to an upper bound $O(2^{2^{d-1}}d\log d)$ on the
$d$-dimensional box-cover number that is better than all previous bounds, in a
sense close to best possible. We also improve the best bound known in
3-dimensions from $3^{14}$ to 64 and propose possible further improvements
through finding the maximum domination number over parity tournaments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4680</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4680</id><created>2013-02-19</created><authors><author><keyname>Newstadt</keyname><forenames>Gregory E.</forenames></author><author><keyname>Zelnio</keyname><forenames>Edmund G.</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Moving target inference with hierarchical Bayesian models in synthetic
  aperture radar imagery</title><categories>cs.IT math.IT</categories><comments>35 pages, 8 figures, 1 algorithm, 11 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In synthetic aperture radar (SAR), images are formed by focusing the response
of stationary objects to a single spatial location. On the other hand, moving
targets cause phase errors in the standard formation of SAR images that cause
displacement and defocusing effects. SAR imagery also contains significant
sources of non-stationary spatially-varying noises, including antenna gain
discrepancies, angular scintillation (glints) and complex speckle. In order to
account for this intricate phenomenology, this work combines the knowledge of
the physical, kinematic, and statistical properties of SAR imaging into a
single unified Bayesian structure that simultaneously (a) estimates the
nuisance parameters such as clutter distributions and antenna miscalibrations
and (b) estimates the target signature required for detection/inference of the
target state. Moreover, we provide a Monte Carlo estimate of the posterior
distribution for the target state and nuisance parameters that infers the
parameters of the model directly from the data, largely eliminating tuning of
algorithm parameters. We demonstrate that our algorithm competes at least as
well on a synthetic dataset as state-of-the-art algorithms for estimating
sparse signals. Finally, performance analysis on a measured dataset
demonstrates that the proposed algorithm is robust at detecting/estimating
targets over a wide area and performs at least as well as popular algorithms
for SAR moving target detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4688</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4688</id><created>2013-02-19</created><authors><author><keyname>Alvandi</keyname><forenames>Parisa</forenames></author><author><keyname>Chen</keyname><forenames>Changbo</forenames></author><author><keyname>Maza</keyname><forenames>Marc Moreno</forenames></author></authors><title>An Algorithm for Computing the Limit Points of the Quasi-component of a
  Regular Chain</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a regular chain $R$, we propose an algorithm which computes the
(non-trivial) limit points of the quasi-component of $R$, that is, the set
$\bar{W(R)} \setminus W(R)$. Our procedure relies on Puiseux series expansions
and does not require to compute a system of generators of the saturated ideal
of $R$. We focus on the case where this saturated ideal has dimension one and
we discuss extensions of this work in higher dimensions. We provide
experimental results illustrating the benefits of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4689</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4689</id><created>2013-02-19</created><updated>2013-03-07</updated><authors><author><keyname>Tran</keyname><forenames>Le Minh Sang</forenames></author><author><keyname>Solhaug</keyname><forenames>Bj&#xf8;rnar</forenames></author><author><keyname>St&#xf8;len</keyname><forenames>Ketil</forenames></author></authors><title>An Approach to Select Cost-Effective Risk Countermeasures Exemplified in
  CORAS</title><categories>cs.OH</categories><comments>33 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Risk is unavoidable in business and risk management is needed amongst others
to set up good security policies. Once the risks are evaluated, the next step
is to decide how they should be treated. This involves managers making
decisions on proper countermeasures to be implemented to mitigate the risks.
The countermeasure expenditure, together with its ability to mitigate risks, is
factors that affect the selection. While many approaches have been proposed to
perform risk analysis, there has been less focus on delivering the prescriptive
and specific information that managers require to select cost-effective
countermeasures. This paper proposes a generic approach to integrate the cost
assessment into risk analysis to aid such decision making. The approach makes
use of a risk model which has been annotated with potential countermeasures,
estimates for their cost and effect. A calculus is then employed to reason
about this model in order to support decision in terms of decision diagrams. We
exemplify the instantiation of the generic approach in the CORAS method for
security risk analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4701</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4701</id><created>2013-02-19</created><authors><author><keyname>Miridakis</keyname><forenames>Nikolaos I.</forenames></author><author><keyname>Vergados</keyname><forenames>Dimitrios D.</forenames></author><author><keyname>Papadakis</keyname><forenames>Emmanouil</forenames></author></authors><title>A Receiver-Centric OFCDM Approach with Subcarrier Grouping</title><categories>cs.IT cs.SY math.IT</categories><comments>4 pages, 3 figures</comments><doi>10.1109/LCOMM.2012.032612.112222</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, following a cross-layer design concept, we propose a novel
subcarrier grouping technique for Orthogonal Frequency and Code Division
Multiplexing (OFCDM) multiuser systems. We adopt a two dimensional (2D)
spreading, so as to achieve both frequency- and time-domain channel gain.
Furthermore, we enable a receiver-centric approach, where the receiver rather
than a potential sender controls the admission decision of the communication
establishment. We study the robustness of the proposed scheme in terms of the
Bit-Error-Rate (BER) and the outage probability. The derived results indicate
that the proposed scheme outperforms the classical OFCDM approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4705</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4705</id><created>2013-02-19</created><authors><author><keyname>Miridakis</keyname><forenames>Nikolaos I.</forenames></author><author><keyname>Vergados</keyname><forenames>Dimitrios D.</forenames></author></authors><title>Performance Analysis of the Ordered V-BLAST Approach over Nakagami-m
  Fading Channels</title><categories>cs.IT cs.SY math.IT</categories><doi>10.1109/WCL.2012.092612.120554</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of the V-BLAST approach, which utilizes successive
interference cancellation (SIC) with optimal ordering, over independent
Nakagami-m fading channels is studied. Systems with two transmit and n receive
antennas are employed whereas the potential erroneous decision of SIC is also
considered. In particular, tight closed-form bound expressions are derived in
terms of the average symbol error rate (ASER) and the outage probability, in
case of binary and rectangular M-ary constellation alphabets. The mathematical
analysis is accompanied with selected performance evaluation and numerical
results, which demonstrate the usefulness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4706</identifier>
 <datestamp>2013-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4706</id><created>2013-02-19</created><updated>2013-06-12</updated><authors><author><keyname>Campello</keyname><forenames>Antonio</forenames></author><author><keyname>Torezzan</keyname><forenames>Cristiano</forenames></author><author><keyname>Costa</keyname><forenames>Sueli I. R.</forenames></author></authors><title>Curves on Flat Tori and Analog Source-Channel Codes</title><categories>cs.IT math.IT</categories><comments>15 pages, 6 figures. Accepted for publication IEEE Trans. Inf.
  Theory. arXiv admin note: substantial text overlap with arXiv:1202.2111</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of transmitting a continuous alphabet
discrete-time source over an AWGN channel. We propose a constructive scheme
based on a set of curves on the surface of a N-dimensional sphere. Our approach
shows that the design of good codes for this communication problem is related
to geometrical properties of spherical codes and projections of N-dimensional
rectangular lattices. Theoretical comparisons with some previous works in terms
of the mean square error as a function of the channel SNR as well as
simulations are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4707</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4707</id><created>2013-02-19</created><updated>2015-10-08</updated><authors><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>L&#xf6;ffler</keyname><forenames>Maarten</forenames></author><author><keyname>Silveira</keyname><forenames>Rodrigo I.</forenames></author><author><keyname>Strash</keyname><forenames>Darren</forenames></author></authors><title>On the Complexity of Barrier Resilience for Fat Regions</title><categories>cs.CC cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the \emph {barrier resilience} problem (introduced by Kumar {\em et al.},
Wireless Networks 2007), we are given a collection of regions of the plane,
acting as obstacles, and we would like to remove the minimum number of regions
so that two fixed points can be connected without crossing any region. In this
paper, we show that the problem is NP-hard when the collection only contains
fat regions with bounded ply $\ply$ (even when they are axis-aligned rectangles
of aspect ratio $1 : (1 + \eps)$). We also show that the problem is
fixed-parameter tractable (FPT) for unit disks and for similarly-sized
$\beta$-fat regions with bounded ply $\ply$ and $O(1)$ pairwise boundary
intersections.
  We then use our FPT algorithm to construct an $(1+\eps)$-approximation
algorithm that runs in $O(2^{f(\Delta, \eps,\beta)}n^7)$ time, where $f\in
O(\frac{\ply^4\beta^8}{\eps^4}\log(\beta\ply/\eps))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4713</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4713</id><created>2013-02-19</created><updated>2014-01-06</updated><authors><author><keyname>Dughmi</keyname><forenames>Shaddin</forenames></author><author><keyname>Immorlica</keyname><forenames>Nicole</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author></authors><title>Constrained Signaling in Auction Design</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of an auctioneer who faces the task of selling a good
(drawn from a known distribution) to a set of buyers, when the auctioneer does
not have the capacity to describe to the buyers the exact identity of the good
that he is selling. Instead, he must come up with a constrained signalling
scheme: a (non injective) mapping from goods to signals, that satisfies the
constraints of his setting. For example, the auctioneer may be able to
communicate only a bounded length message for each good, or he might be legally
constrained in how he can advertise the item being sold. Each candidate
signaling scheme induces an incomplete-information game among the buyers, and
the goal of the auctioneer is to choose the signaling scheme and accompanying
auction format that optimizes welfare. In this paper, we use techniques from
submodular function maximization and no-regret learning to give algorithms for
computing constrained signaling schemes for a variety of constrained signaling
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4717</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4717</id><created>2013-02-19</created><authors><author><keyname>Yu</keyname><forenames>Zhenhua</forenames></author><author><keyname>Baxley</keyname><forenames>Robert J.</forenames></author><author><keyname>Walkenhorst</keyname><forenames>Brett T.</forenames></author><author><keyname>Zhou</keyname><forenames>G. Tong</forenames></author></authors><title>Channel Sounding Waveforms Design for Asynchronous Multiuser MIMO
  Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide three contributions to the field of channel sounding
waveform design in asynchronous Multi-user (MU) MIMO systems. The first
contribution is a derivation of the asynchronous MU-MIMO model and the
conditions that the sounding waveform must meet to independently resolve all of
the spatial channel responses. Next we propose a chirp waveform that meets the
constraints and we show that the MSE of our system meets the Cramer-Rao Bound
(CRB) when the time offset is an integer multiple of the sampling interval.
Finally we demonstrate that the channel capacity region of the asynchronous
system and synchronous system is equivalent under certain conditions.
Simulation results are provided to illustrate the findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4720</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4720</id><created>2013-02-11</created><authors><author><keyname>Bocca</keyname><forenames>Maurizio</forenames></author><author><keyname>Kaltiokallio</keyname><forenames>Ossi</forenames></author><author><keyname>Patwari</keyname><forenames>Neal</forenames></author><author><keyname>Venkatasubramanian</keyname><forenames>Suresh</forenames></author></authors><title>Multiple Target Tracking with RF Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RF sensor networks are wireless networks that can localize and track people
(or targets) without needing them to carry or wear any electronic device. They
use the change in the received signal strength (RSS) of the links due to the
movements of people to infer their locations. In this paper, we consider
real-time multiple target tracking with RF sensor networks. We perform radio
tomographic imaging (RTI), which generates images of the change in the
propagation field, as if they were frames of a video. Our RTI method uses RSS
measurements on multiple frequency channels on each link, combining them with a
fade level-based weighted average. We describe methods to adapt machine vision
methods to the peculiarities of RTI to enable real time multiple target
tracking. Several tests are performed in an open environment, a one-bedroom
apartment, and a cluttered office environment. The results demonstrate that the
system is capable of accurately tracking in real-time up to 4 targets in
cluttered indoor environments, even when their trajectories intersect multiple
times, without mis-estimating the number of targets found in the monitored
area. The highest average tracking error measured in the tests is 0.45 m with
two targets, 0.46 m with three targets, and 0.55 m with four targets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4721</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4721</id><created>2013-02-19</created><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Lo</keyname><forenames>Ernest S.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Energy-Efficient Resource Allocation in OFDMA Systems with Hybrid Energy
  Harvesting Base Station</title><categories>cs.IT math.IT</categories><comments>32 pages, 7 figures, and 1 table. Submitted for possible journal
  publication in 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study resource allocation algorithm design for energy-efficient
communication in an OFDMA downlink network with hybrid energy harvesting base
station. Specifically, an energy harvester and a constant energy source driven
by a non-renewable resource are used for supplying the energy required for
system operation. We first consider a deterministic offline system setting. In
particular, assuming availability of non-causal knowledge about energy arrivals
and channel gains, an offline resource allocation problem is formulated as a
non-convex optimization problem taking into account the circuit energy
consumption, a finite energy storage capacity, and a minimum required data
rate. We transform this non-convex optimization problem into a convex
optimization problem by applying time-sharing and fractional programming which
results in an efficient asymptotically optimal offline iterative resource
allocation algorithm. In each iteration, the transformed problem is solved by
using Lagrange dual decomposition. The obtained resource allocation policy
maximizes the weighted energy efficiency of data transmission. Subsequently, we
focus on online algorithm design. A stochastic dynamic programming approach is
employed to obtain the optimal online resource allocation algorithm which
requires a prohibitively high complexity. To strike a balance between system
performance and computational complexity, we propose a low complexity
suboptimal online iterative algorithm which is motivated by the offline
optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4726</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4726</id><created>2013-02-19</created><authors><author><keyname>Bouzidi</keyname><forenames>Khalil Riad</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Fies</keyname><forenames>Bruno</forenames><affiliation>CSTB Sophia Antipolis</affiliation></author><author><keyname>Bourdeau</keyname><forenames>Marc</forenames><affiliation>CSTB Sophia Antipolis</affiliation></author><author><keyname>Faron-Zucker</keyname><forenames>Catherine</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Le-Thanh</keyname><forenames>Nhan</forenames><affiliation>I3S</affiliation></author></authors><title>An Ontology for Modelling and Supporting the Process of Authoring
  Technical Assessments</title><categories>cs.IR cs.CL cs.DL</categories><comments>In the International Council for Building Conference, CIB 2011 (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a semantic web approach for modelling the process
of creating new technical and regulatory documents related to the Building
sector. This industry, among other industries, is currently experiencing a
phenomenal growth in its technical and regulatory texts. Therefore, it is
urgent and crucial to improve the process of creating regulations by automating
it as much as possible. We focus on the creation of particular technical
documents issued by the French Scientific and Technical Centre for Building
(CSTB), called Technical Assessments, and we propose services based on Semantic
Web models and techniques for modelling the process of their creation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4735</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4735</id><created>2013-02-17</created><authors><author><keyname>Macdonald</keyname><forenames>Brian</forenames></author><author><keyname>Pulleyblank</keyname><forenames>William</forenames></author></authors><title>Realignment in the NHL, MLB, the NFL, and the NBA</title><categories>stat.AP cs.SI physics.soc-ph</categories><comments>20 figures, 1 table</comments><msc-class>90C27 - Combinatorial optimization, 90C11 - Mixed integer
  programming, 62P99 - Statistics Applications</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sports leagues consist of conferences subdivided into divisions. Teams play a
number of games within their divisions and fewer games against teams in
different divisions and conferences. Usually, a league structure remains stable
from one season to the next. However, structures change when growth or
contraction occurs, and realignment of the four major professional sports
leagues in North America has occurred more than twenty-five times since 1967.
In this paper, we describe a method for realigning sports leagues that is
flexible, adaptive, and that enables construction of schedules that minimize
travel while satisfying other criteria. We do not build schedules; we develop
league structures which support the subsequent construction of efficient
schedules. Our initial focus is the NHL, which has an urgent need for
realignment following the recent move of the Atlanta Thrashers to Winnipeg, but
our methods can be adapted to virtually any situation. We examine a variety of
scenarios for the NHL, and apply our methods to the NBA, MLB, and NFL. We find
the biggest improvements for MLB and the NFL, where adopting the best solutions
would reduce league travel by about 20%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4739</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4739</id><created>2013-02-18</created><updated>2013-03-03</updated><authors><author><keyname>Dai</keyname><forenames>Liyun</forenames></author><author><keyname>Xia</keyname><forenames>Bican</forenames></author><author><keyname>Zhan</keyname><forenames>Naijun</forenames></author></authors><title>Generating Non-Linear Interpolants by Semidefinite Programming</title><categories>cs.LO</categories><comments>22 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interpolation-based techniques have been widely and successfully applied in
the verification of hardware and software, e.g., in bounded-model check- ing,
CEGAR, SMT, etc., whose hardest part is how to synthesize interpolants. Various
work for discovering interpolants for propositional logic, quantifier-free
fragments of first-order theories and their combinations have been proposed.
However, little work focuses on discovering polynomial interpolants in the
literature. In this paper, we provide an approach for constructing non-linear
interpolants based on semidefinite programming, and show how to apply such
results to the verification of programs by examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4755</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4755</id><created>2013-02-19</created><authors><author><keyname>Jeon</keyname><forenames>Jeongho</forenames></author><author><keyname>Ephremides</keyname><forenames>Anthony</forenames></author></authors><title>Channel-Aware Random Access in the Presence of Channel Estimation Errors</title><categories>cs.IT math.IT</categories><comments>The material in this paper was presented in part at the IEEE
  International Symposium on Information Theory, Cambridge, MA, USA, July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider the random access of nodes adapting their
transmission probability based on the local channel state information (CSI) in
a decentralized manner, which is called CARA. The CSI is not directly available
to each node but estimated with some errors in our scenario. Thus, the impact
of imperfect CSI on the performance of CARA is our main concern. Specifically,
an exact stability analysis is carried out when a pair of bursty sources are
competing for a common receiver and, thereby, have interdependent services. The
analysis also takes into account the compound effects of the multipacket
reception (MPR) capability at the receiver. The contributions in this paper are
twofold: first, we obtain the exact stability region of CARA in the presence of
channel estimation errors; such an assessment is necessary as the errors in
channel estimation are inevitable in the practical situation. Secondly, we
compare the performance of CARA to that achieved by the class of stationary
scheduling policies that make decisions in a centralized manner based on the
CSI feedback. It is shown that the stability region of CARA is not necessarily
a subset of that of centralized schedulers as the MPR capability improves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4760</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4760</id><created>2013-02-19</created><updated>2013-06-10</updated><authors><author><keyname>Costa</keyname><forenames>Lauro Beltr&#xe3;o</forenames></author><author><keyname>Barros</keyname><forenames>Abmar</forenames></author><author><keyname>Al-Kiswany</keyname><forenames>Samer</forenames></author><author><keyname>Yang</keyname><forenames>Hao</forenames></author><author><keyname>Vairavanathan</keyname><forenames>Emalayan</forenames></author><author><keyname>Ripeanu</keyname><forenames>Matei</forenames></author></authors><title>Predicting Intermediate Storage Performance for Workflow Applications</title><categories>cs.DC cs.PF</categories><report-no>NetSysLab - TR 2013/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Configuring a storage system to better serve an application is a challenging
task complicated by a multidimensional, discrete configuration space and the
high cost of space exploration (e.g., by running the application with different
storage configurations). To enable selecting the best configuration in a
reasonable time, we design an end-to-end performance prediction mechanism that
estimates the turn-around time of an application using storage system under a
given configuration. This approach focuses on a generic object-based storage
system design, supports exploring the impact of optimizations targeting
workflow applications (e.g., various data placement schemes) in addition to
other, more traditional, configuration knobs (e.g., stripe size or replication
level), and models the system operation at data-chunk and control message
level.
  This paper presents our experience to date with designing and using this
prediction mechanism. We evaluate this mechanism using micro- as well as
synthetic benchmarks mimicking real workflow applications, and a real
application.. A preliminary evaluation shows that we are on a good track to
meet our objectives: it can scale to model a workflow application run on an
entire cluster while offering an over 200x speedup factor (normalized by
resource) compared to running the actual application, and can achieve, in the
limited number of scenarios we study, a prediction accuracy that enables
identifying the best storage system configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4761</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4761</id><created>2013-02-19</created><authors><author><keyname>Cao</keyname><forenames>Yongcan</forenames></author><author><keyname>Ren</keyname><forenames>Wei</forenames></author></authors><title>Finite-time Consensus for Multi-agent Networks with Unknown Inherent
  Nonlinear Dynamics</title><categories>math.OC cs.SY</categories><msc-class>cooperative control, consensus, multi-agent systems</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on analyzing the finite-time convergence of a nonlinear
consensus algorithm for multi-agent networks with unknown inherent nonlinear
dynamics. Due to the existence of the unknown inherent nonlinear dynamics, the
stability analysis and the finite-time convergence analysis of the closed-loop
system under the proposed consensus algorithm are more challenging than those
under the well-studied consensus algorithms for known linear systems. For this
purpose, we propose a novel stability tool based on a generalized comparison
lemma. With the aid of the novel stability tool, it is shown that the proposed
nonlinear consensus algorithm can guarantee finite-time convergence if the
directed switching interaction graph has a directed spanning tree at each time
interval. Specifically, the finite-time convergence is shown by comparing the
closed-loop system under the proposed consensus algorithm with some
well-designed closed-loop system whose stability properties are easier to
obtain. Moreover, the stability and the finite-time convergence of the
closed-loop system using the proposed consensus algorithm under a (general)
directed switching interaction graph can even be guaranteed by the stability
and the finite-time convergence of some special well-designed nonlinear
closed-loop system under some special directed switching interaction graph,
where each agent has at most one neighbor whose state is either the maximum of
those states that are smaller than its own state or the minimum of those states
that are larger than its own state. This provides a stimulating example for the
potential applications of the proposed novel stability tool in the stability
analysis of linear/nonlinear closed-loop systems by making use of known results
in linear/nonlinear systems. For illustration of the theoretical result, we
provide a simulation example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4765</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4765</id><created>2013-02-19</created><authors><author><keyname>Davies</keyname><forenames>Todd</forenames></author><author><keyname>Mintz</keyname><forenames>Mike D.</forenames></author></authors><title>Design Features for the Social Web: The Architecture of Deme</title><categories>cs.SI cs.SE</categories><comments>Appeared in Luis Olsina, Oscar Pastor, Daniel Schwabe, Gustavo Rossi,
  and Marco Winckler (Editors), Proceedings of the 8th International Workshop
  on Web-Oriented Software Technologies (IWWOST 2009), CEUR Workshop
  Proceedings, Volume 493, August 2009, pp. 40-51; 12 pages, 2 figures, 1 table</comments><acm-class>H.5.3; D.2.2; D.2.10; D.1.5; D.3.3</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We characterize the &quot;social Web&quot; and argue for several features that are
desirable for users of socially oriented web applications. We describe the
architecture of Deme, a web content management system (WCMS) and extensible
framework, and show how it implements these desired features. We then compare
Deme on our desiderata with other web technologies: traditional HTML, previous
open source WCMSs (illustrated by Drupal), commercial Web 2.0 applications, and
open-source, object-oriented web application frameworks. The analysis suggests
that a WCMS can be well suited to building social websites if it makes more of
the features of object-oriented programming, such as polymorphism, and class
inheritance, available to non-programmers in an accessible vocabulary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4767</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4767</id><created>2013-02-19</created><authors><author><keyname>Renna</keyname><forenames>Francesco</forenames></author><author><keyname>Laurenti</keyname><forenames>Nicola</forenames></author><author><keyname>Tomasin</keyname><forenames>Stefano</forenames></author><author><keyname>Baldi</keyname><forenames>Marco</forenames></author><author><keyname>Maturo</keyname><forenames>Nicola</forenames></author><author><keyname>Bianchi</keyname><forenames>Marco</forenames></author><author><keyname>Chiaraluce</keyname><forenames>Franco</forenames></author><author><keyname>Bloch</keyname><forenames>Matthieu</forenames></author></authors><title>Low-power Secret-key Agreement over OFDM</title><categories>cs.IT cs.CR math.IT</categories><comments>9 pages, 4 figures; this is the authors prepared version of the paper
  with the same name accepted for HotWiSec 2013, the Second ACM Workshop on Hot
  Topics on Wireless Network Security and Privacy, Budapest, Hungary 17-19
  April 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information-theoretic secret-key agreement is perhaps the most practically
feasible mechanism that provides unconditional security at the physical layer
to date. In this paper, we consider the problem of secret-key agreement by
sharing randomness at low power over an orthogonal frequency division
multiplexing (OFDM) link, in the presence of an eavesdropper. The low power
assumption greatly simplifies the design of the randomness sharing scheme, even
in a fading channel scenario. We assess the performance of the proposed system
in terms of secrecy key rate and show that a practical approach to key sharing
is obtained by using low-density parity check (LDPC) codes for information
reconciliation. Numerical results confirm the merits of the proposed approach
as a feasible and practical solution. Moreover, the outage formulation allows
to implement secret-key agreement even when only statistical knowledge of the
eavesdropper channel is available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4773</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4773</id><created>2013-02-19</created><authors><author><keyname>Urriza</keyname><forenames>Paulo</forenames></author><author><keyname>Rebeiz</keyname><forenames>Eric</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Optimal Discriminant Functions Based On Sampled Distribution Distance
  for Modulation Classification</title><categories>stat.ML cs.LG cs.PF</categories><comments>4 pages, 3 figures, submitted to IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we derive the optimal discriminant functions for modulation
classification based on the sampled distribution distance. The proposed method
classifies various candidate constellations using a low complexity approach
based on the distribution distance at specific testpoints along the cumulative
distribution function. This method, based on the Bayesian decision criteria,
asymptotically provides the minimum classification error possible given a set
of testpoints. Testpoint locations are also optimized to improve classification
performance. The method provides significant gains over existing approaches
that also use the distribution of the signal features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4774</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4774</id><created>2013-02-19</created><authors><author><keyname>Chen</keyname><forenames>Chih-Chun</forenames></author></authors><title>A theoretical framework for conducting multi-level studies of complex
  social systems with agent-based models and empirical data</title><categories>cs.MA cs.SI stat.AP</categories><comments>50 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A formal but intuitive framework is introduced to bridge the gap between data
obtained from empirical studies and that generated by agent-based models. This
is based on three key tenets. Firstly, a simulation can be given multiple
formal descriptions corresponding to static and dynamic properties at different
levels of observation. These can be easily mapped to empirically observed
phenomena and data obtained from them. Secondly, an agent-based model generates
a set of closed systems, and computational simulation is the means by which we
sample from this set. Thirdly, properties at different levels and statistical
relationships between them can be used to classify simulations as those that
instantiate a more sophisticated set of constraints. These can be validated
with models obtained from statistical models of empirical data (for example,
structural equation or multi-level models) and hence provide more stringent
criteria for validating the agent-based model itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4776</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4776</id><created>2013-02-19</created><updated>2014-04-01</updated><authors><author><keyname>Li</keyname><forenames>Yun</forenames></author><author><keyname>Nitinawarat</keyname><forenames>Sirin</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Universal Outlier Hypothesis Testing</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>IEEE Trans. Inf. Theory, to appear, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Outlier hypothesis testing is studied in a universal setting. Multiple
sequences of observations are collected, a small subset of which are outliers.
A sequence is considered an outlier if the observations in that sequence are
distributed according to an ``outlier'' distribution, distinct from the
``typical'' distribution governing the observations in all the other sequences.
Nothing is known about the outlier and typical distributions except that they
are distinct and have full supports. The goal is to design a universal test to
best discern the outlier sequence(s). It is shown that the generalized
likelihood test is universally exponentially consistent under various settings.
The achievable error exponent is also characterized. In the other settings, it
is also shown that there cannot exist any universally exponentially consistent
test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4779</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4779</id><created>2013-02-19</created><authors><author><keyname>Lu</keyname><forenames>Charng-Da</forenames></author></authors><title>Failure Data Analysis of HPC Systems</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Continuous availability of HPC systems built from commodity components have
become a primary concern as system size grows to thousands of processors. In
this paper, we present the analysis of 8-24 months of real failure data
collected from three HPC systems at the National Center for Supercomputing
Applications (NCSA) during 2001-2004. The results show that the availability is
98.7-99.8% and most outages are due to software halts. On the other hand, the
downtime are mostly contributed by hardware halts or scheduled maintenance. We
also used failure clustering analysis to identify several correlated failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4780</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4780</id><created>2013-02-19</created><authors><author><keyname>Buhnova</keyname><forenames>Barbora</forenames></author><author><keyname>Happe</keyname><forenames>Lucia</forenames></author><author><keyname>Kofro&#x148;</keyname><forenames>Jan</forenames></author></authors><title>Proceedings 10th International Workshop on Formal Engineering Approaches
  to Software Components and Architectures</title><categories>cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 108, 2013</journal-ref><doi>10.4204/EPTCS.108</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These are the proceedings of the 10th International Workshop on Formal
Engineering approaches to Software Components and Architectures (FESCA). The
workshop was held on March 23, 2013 in Rome (Italy) as a satellite event to the
European Joint Conference on Theory and Practice of Software (ETAPS'13).
  The aim of the FESCA workshop is to bring together both young and senior
researchers from formal methods, software engineering, and industry interested
in the development and application of formal modelling approaches as well as
associated analysis and reasoning techniques with practical benefits for
component-based software engineering.
  FESCA aims to address the open question of how formal methods can be applied
effectively to these new contexts and challenges. FESCA is interested in both
the development and application of formal methods in component-based
development and tries to cross-fertilize their research and application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4783</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4783</id><created>2013-02-19</created><updated>2015-05-03</updated><authors><author><keyname>Hou</keyname><forenames>Zhe</forenames></author><author><keyname>Tiu</keyname><forenames>Alwen</forenames></author><author><keyname>Gore</keyname><forenames>Rajeev</forenames></author></authors><title>A Labelled Sequent Calculus for BBI: Proof Theory and Proof Search</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a labelled sequent calculus for Boolean BI, a classical variant of
O'Hearn and Pym's logic of Bunched Implication. The calculus is simple, sound,
complete, and enjoys cut-elimination. We show that all the structural rules in
our proof system, including those rules that manipulate labels, can be
localised around applications of certain logical rules, thereby localising the
handling of these rules in proof search. Based on this, we demonstrate a free
variable calculus that deals with the structural rules lazily in a constraint
system. A heuristic method to solve the constraints is proposed in the end,
with some experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4784</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4784</id><created>2013-02-19</created><authors><author><keyname>Yi-zhou</keyname><forenames>Tan</forenames></author><author><keyname>Hai-bo</keyname><forenames>Liu</forenames></author><author><keyname>Shui-hua</keyname><forenames>Huang</forenames></author><author><keyname>Ben-jian</keyname><forenames>Sheng</forenames></author><author><keyname>Zhong-ming</keyname><forenames>Pan</forenames></author></authors><title>An Optical Watermarking Solution for Color Personal Identification
  Pictures</title><categories>cs.MM cs.CV physics.optics</categories><journal-ref>2009 International Conference on Optical Instruments and
  Technology: Optoelectronic</journal-ref><doi>10.1117/12.839630</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach for embedding authentication information
into image on printed materials based on optical projection technique. Our
experimental setup consists of two parts, one is a common camera, and the other
is a LCD projector, which project a pattern on personnel's body (especially on
the face). The pattern, generated by a computer, act as the illumination light
source with sinusoidal distribution and it is also the watermark signal. For a
color image, the watermark is embedded into the blue channel. While we take
pictures (256 *256 and 512*512, 567*390 pixels, respectively), an invisible
mark is embedded directly into magnitude oefficients of Discrete Fourier
transform (DFT) at exposure moment. Both optical an d digital correlation is
suitable for detection of this type of watermark. The decoded watermark is a
set of concentric circles or sectors in the DFT domain (middle frequencies
region) which is robust to photographing, printing and scanning. The unlawful
people modify or replace the original photograph, and make fake passport
(drivers' license and so on). Experiments show, it is difficult to forge
certificates in which a watermark was embedded by our projector-camera
combination based on analogue watermark method rather than classical digital
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4785</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4785</id><created>2013-02-19</created><authors><author><keyname>Maso</keyname><forenames>Marco</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author><author><keyname>Vangelista</keyname><forenames>Lorenzo</forenames></author></authors><title>A Distributed Approach to Interference Alignment in OFDM-based
  Two-tiered Networks</title><categories>cs.IT math.IT</categories><comments>15 pages, 10 figures, accepted and to appear in IEEE Transactions on
  Vehicular Technology Special Section: Self-Organizing Radio Networks, 2013.
  Authors' final version. Copyright transferred to IEEE</comments><doi>10.1109/TVT.2013.2245516</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this contribution, we consider a two-tiered network and focus on the
coexistence between the two tiers at physical layer. We target our efforts on a
long term evolution advanced (LTE-A) orthogonal frequency division multiple
access (OFDMA) macro-cell sharing the spectrum with a randomly deployed second
tier of small-cells. In such networks, high levels of co-channel interference
between the macro and small base stations (MBS/SBS) may largely limit the
potential spectral efficiency gains provided by the frequency reuse 1. To
address this issue, we propose a novel cognitive interference alignment based
scheme to protect the macro-cell from the cross-tier interference, while
mitigating the co-tier interference in the second tier. Remarkably, only local
channel state information (CSI) and autonomous operations are required in the
second tier, resulting in a completely self-organizing approach for the SBSs.
The optimal precoder that maximizes the spectral efficiency of the link between
each SBS and its served user equipment is found by means of a distributed
one-shot strategy. Numerical findings reveal non-negligible spectral efficiency
enhancements with respect to traditional time division multiple access
approaches at any signal to noise (SNR) regime. Additionally, the proposed
technique exhibits significant robustness to channel estimation errors,
achieving remarkable results for the imperfect CSI case and yielding consistent
performance enhancements to the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4786</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4786</id><created>2013-02-19</created><authors><author><keyname>Maso</keyname><forenames>Marco</forenames></author><author><keyname>Cardoso</keyname><forenames>Leonardo S.</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author><author><keyname>Vangelista</keyname><forenames>Lorenzo</forenames></author></authors><title>Cognitive Orthogonal Precoder for Two-tiered Networks Deployment</title><categories>cs.IT math.IT</categories><comments>11 pages, 9 figures, accepted and to appear in IEEE Journal on
  Selected Areas in Communications: Cognitive Radio Series, 2013. Copyright
  transferred to IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, the problem of cross-tier interference in a two-tiered
(macro-cell and cognitive small-cells) network, under the complete spectrum
sharing paradigm, is studied. A new orthogonal precoder transmit scheme for the
small base stations, called multi-user Vandermonde-subspace frequency division
multiplexing (MU-VFDM), is proposed. MU-VFDM allows several cognitive small
base stations to coexist with legacy macro-cell receivers, by nulling the
small- to macro-cell cross-tier interference, without any cooperation between
the two tiers. This cleverly designed cascaded precoder structure, not only
cancels the cross-tier interference, but avoids the co-tier interference for
the small-cell network. The achievable sum-rate of the small-cell network,
satisfying the interference cancelation requirements, is evaluated for perfect
and imperfect channel state information at the transmitter. Simulation results
for the cascaded MU-VFDM precoder show a comparable performance to that of
state-of-the-art dirty paper coding technique, for the case of a dense cellular
layout. Finally, a comparison between MU-VFDM and a standard complete spectrum
separation strategy is proposed. Promising gains in terms of achievable
sum-rate are shown for the two-tiered network w.r.t. the traditional bandwidth
management approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4788</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4788</id><created>2013-02-19</created><updated>2013-11-03</updated><authors><author><keyname>Abdoli</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>Layered Interference Networks with Delayed CSI: DoF Scaling with
  Distributed Transmitters</title><categories>cs.IT math.IT</categories><comments>32 pages, 6 figures, 4 tables; Accepted for publication in IEEE
  Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The layered interference network is investigated with delayed channel state
information (CSI) at all nodes. It is demonstrated how multi-hopping can be
utilized to increase the achievable degrees of freedom (DoF). In particular, a
multi-phase transmission scheme is proposed for the $K$-user $2K$-hop
interference network in order to systematically exploit the layered structure
of the network and delayed CSI to achieve DoF values that scale with $K$. This
result provides the first example of a network with distributed transmitters
and delayed CSI whose DoF scales with the number of users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4793</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4793</id><created>2013-02-19</created><updated>2013-07-12</updated><authors><author><keyname>Lee</keyname><forenames>Seunghyun</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Huang</keyname><forenames>Kaibin</forenames></author></authors><title>Opportunistic Wireless Energy Harvesting in Cognitive Radio Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>This is the longer version of a paper to appear in IEEE Transactions
  on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless networks can be self-sustaining by harvesting energy from ambient
radio-frequency (RF) signals. Recently, researchers have made progress on
designing efficient circuits and devices for RF energy harvesting suitable for
low-power wireless applications. Motivated by this and building upon the
classic cognitive radio (CR) network model, this paper proposes a novel method
for wireless networks coexisting where low-power mobiles in a secondary
network, called secondary transmitters (STs), harvest ambient RF energy from
transmissions by nearby active transmitters in a primary network, called
primary transmitters (PTs), while opportunistically accessing the spectrum
licensed to the primary network. We consider a stochastic-geometry model in
which PTs and STs are distributed as independent homogeneous Poisson point
processes (HPPPs) and communicate with their intended receivers at fixed
distances. Each PT is associated with a guard zone to protect its intended
receiver from ST's interference, and at the same time delivers RF energy to STs
located in its harvesting zone. Based on the proposed model, we analyze the
transmission probability of STs and the resulting spatial throughput of the
secondary network. The optimal transmission power and density of STs are
derived for maximizing the secondary network throughput under the given
outage-probability constraints in the two coexisting networks, which reveal key
insights to the optimal network design. Finally, we show that our analytical
result can be generally applied to a non-CR setup, where distributed wireless
power chargers are deployed to power coexisting wireless transmitters in a
sensor network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4796</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4796</id><created>2013-02-19</created><authors><author><keyname>Sharma</keyname><forenames>Asankhaya</forenames></author></authors><title>End to End Verification and Validation with SPIN</title><categories>cs.PL cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last several years the tools used for model checking have become
more efficient and usable. This has enabled users to apply model checking to
industrial-scale problems, however the task of validating the implementation of
the model is usually much harder. In this paper we present an approach to do
end to end verification and validation of a real time system using the SPIN
model checker. Taking the example of the cardiac pacemaker system proposed in
the SQRL Pacemaker Formal Methods Challenge we demonstrate our framework by
building a formal model for the cardiac pacemaker in SPIN, checking for
desirable temporal properties of the model (expressed as LTL formulas),
generating C code from the model (by refinement of PROMELA) and validating the
generated implementation (using SPIN). We argue that a state of the art model
checking tool like SPIN can be used to do formal specification as well as
validation of the implementation. To evaluate our approach we show that our
pacemaker model is expressive enough to derive consistent operating modes and
that the refinement rules preserve LTL properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4798</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4798</id><created>2013-02-19</created><authors><author><keyname>Sharma</keyname><forenames>Asankhaya</forenames></author></authors><title>An Empirical Study of Path Feasibility Queries</title><categories>cs.SE cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a comparative study of path feasibility queries
generated during path exploration based software engineering methods. Symbolic
execution based methods are gaining importance in different aspects of software
engineering e.g. proving properties about programs, test case generation,
comparing different executions of programs. These methods use SMT solvers to
check the satisfiability of path feasibility queries written as a formula in
the supported theories. We study the performance of solving such path
feasibility queries using SMT solvers for real world programs. Our path
condition formulas are generated in a theory of quantifier free bit vectors
with arrays (QF_ABV). We show that among the different SMT solvers, STP is
better than Z3 by an order of magnitude for such kind of queries. As an
application we design a new program analysis (Change Value Analysis) based on
our study which exploits undefined behaviors in programs. We have implemented
our analysis in LLVM and tested it with the benchmark of SIR programs. It
reduces the time taken for solving path feasibility queries by 48%. The study
can serve as guidance to practitioners using path feasibility queries to create
scalable software engineering methods based on symbolic execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4805</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4805</id><created>2013-02-19</created><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Lei</keyname><forenames>Lei</forenames></author></authors><title>Energy-Efficient Optimization for Physical Layer Security in
  Multi-Antenna Downlink Networks with QoS Guarantee</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures</comments><journal-ref>IEEE Communications Letters 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this letter, we consider a multi-antenna downlink network where a secure
user (SU) coexists with a passive eavesdropper. There are two design
requirements for such a network. First, the information should be transferred
in a secret and efficient manner. Second, the quality of service (QoS), i.e.
delay sensitivity, should be take into consideration to satisfy the demands of
real-time wireless services. In order to fulfill the two requirements, we
combine the physical layer security technique based on switched beam
beamforming with an energy-efficient power allocation. The problem is
formulated as the maximization of the secrecy energy efficiency subject to
delay and power constraints. By solving the optimization problem, we derive an
energy-efficient power allocation scheme. Numerical results validate the
effectiveness of the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4808</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4808</id><created>2013-02-20</created><updated>2015-01-30</updated><authors><author><keyname>Cachin</keyname><forenames>Christian</forenames></author><author><keyname>Ohrimenko</keyname><forenames>Olga</forenames></author></authors><title>Verifying the Consistency of Remote Untrusted Services with Commutative
  Operations</title><categories>cs.DC</categories><comments>A shorter version of this paper appears in the proceedings of OPODIS
  2014, Lecture Notes in Computer Science, vol.~8878, Springer, 2014</comments><doi>10.1007/978-3-319-14472-6_1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A group of mutually trusting clients outsources a computation service to a
remote server, which they do not fully trust and that may be subject to
attacks. The clients do not communicate with each other and would like to
verify the correctness of the remote computation and the consistency of the
server's responses. This paper first presents the Commutative-Operation
verification Protocol (COP) that ensures linearizability when the server is
correct and preserves fork-linearizability in any other case. All clients that
observe each other's operations are consistent, in the sense that their own
operations and those operations of other clients that they see are
linearizable. Second, this work extends COP through authenticated data
structures to Authenticated COP, which allows consistency verification of
outsourced services whose state is kept only remotely, by the server. This
yields the first fork-linearizable consistency verification protocol for
generic outsourced services that (1) relieves clients from storing the state,
(2) supports wait-free client operations, and (3) handles sequences of
arbitrary commutative operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4811</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4811</id><created>2013-02-20</created><authors><author><keyname>Bouzidi</keyname><forenames>Khalil Riad</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Faron-Zucker</keyname><forenames>Catherine</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Fies</keyname><forenames>Bruno</forenames><affiliation>CSTB Sophia Antipolis</affiliation></author><author><keyname>Corby</keyname><forenames>Olivier</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Nhan</keyname><forenames>Le-Thanh</forenames><affiliation>I3S</affiliation></author></authors><title>Towards a Semantic-based Approach for Modeling Regulatory Documents in
  Building Industry</title><categories>cs.CL</categories><proxy>ccsd</proxy><journal-ref>9th European Conference on Product \&amp; Process Modelling, ECPPM
  2012 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regulations in the Building Industry are becoming increasingly complex and
involve more than one technical area. They cover products, components and
project implementation. They also play an important role to ensure the quality
of a building, and to minimize its environmental impact. In this paper, we are
particularly interested in the modeling of the regulatory constraints derived
from the Technical Guides issued by CSTB and used to validate Technical
Assessments. We first describe our approach for modeling regulatory constraints
in the SBVR language, and formalizing them in the SPARQL language. Second, we
describe how we model the processes of compliance checking described in the
CSTB Technical Guides. Third, we show how we implement these processes to
assist industrials in drafting Technical Documents in order to acquire a
Technical Assessment; a compliance report is automatically generated to explain
the compliance or noncompliance of this Technical Documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4813</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4813</id><created>2013-02-20</created><authors><author><keyname>Cheung</keyname><forenames>Jackie Chi Kit</forenames></author><author><keyname>Poon</keyname><forenames>Hoifung</forenames></author><author><keyname>Vanderwende</keyname><forenames>Lucy</forenames></author></authors><title>Probabilistic Frame Induction</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In natural-language discourse, related events tend to appear near each other
to describe a larger scenario. Such structures can be formalized by the notion
of a frame (a.k.a. template), which comprises a set of related events and
prototypical participants and event transitions. Identifying frames is a
prerequisite for information extraction and natural language generation, and is
usually done manually. Methods for inducing frames have been proposed recently,
but they typically use ad hoc procedures and are difficult to diagnose or
extend. In this paper, we propose the first probabilistic approach to frame
induction, which incorporates frames, events, participants as latent topics and
learns those frame and event transitions that best explain the text. The number
of frames is inferred by a novel application of a split-merge method from
syntactic parsing. In end-to-end evaluations from text to induced frames and
extracted facts, our method produced state-of-the-art results while
substantially reducing engineering effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4814</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4814</id><created>2013-02-20</created><authors><author><keyname>Antoniadis</keyname><forenames>Georges</forenames><affiliation>LIDILEM</affiliation></author><author><keyname>Granger</keyname><forenames>Sylviane</forenames><affiliation>LIDILEM</affiliation></author><author><keyname>Kraif</keyname><forenames>Olivier</forenames><affiliation>LIDILEM</affiliation></author><author><keyname>Ponton</keyname><forenames>Claude</forenames><affiliation>LIDILEM</affiliation></author><author><keyname>Zampa</keyname><forenames>Virginie</forenames><affiliation>LIDILEM</affiliation></author></authors><title>NLP and CALL: integration is working</title><categories>cs.CL</categories><proxy>ccsd</proxy><journal-ref>TaLC7, France (2006)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the first part of this article, we explore the background of
computer-assisted learning from its beginnings in the early XIXth century and
the first teaching machines, founded on theories of learning, at the start of
the XXth century. With the arrival of the computer, it became possible to offer
language learners different types of language activities such as comprehension
tasks, simulations, etc. However, these have limits that cannot be overcome
without some contribution from the field of natural language processing (NLP).
In what follows, we examine the challenges faced and the issues raised by
integrating NLP into CALL. We hope to demonstrate that the key to success in
integrating NLP into CALL is to be found in multidisciplinary work between
computer experts, linguists, language teachers, didacticians and NLP
specialists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4840</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4840</id><created>2013-02-20</created><authors><author><keyname>Xu</keyname><forenames>Kui</forenames></author><author><keyname>Lv</keyname><forenames>Zhenxing</forenames></author><author><keyname>Xu</keyname><forenames>Youyun</forenames></author><author><keyname>Zhang</keyname><forenames>Dongmei</forenames></author><author><keyname>Zhong</keyname><forenames>Xinyi</forenames></author><author><keyname>Liang</keyname><forenames>Wenwen</forenames></author></authors><title>Joint Physical Network Coding and LDPC decoding for Two Way Wireless
  Relaying</title><categories>cs.IT math.IT</categories><comments>This paper was presented in Physical Communication, Elsevier, 2013</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we investigate the joint design of channel and network coding
in bi-directional relaying systems and propose a combined low complexity
physical network coding and LDPC decoding scheme. For the same LDPC codes
employed at both source nodes, we show that the relay can decodes the network
coded codewords from the superimposed signal received from the BPSK-modulated
multiple-access channel. Simulation results shown that this novel joint
physical network coding and LDPC decoding method outperforms the existing MMSE
network coding and LDPC decoding method over AWGN and complex MAC channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4856</identifier>
 <datestamp>2013-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4856</id><created>2013-02-20</created><updated>2013-09-05</updated><authors><author><keyname>Brengos</keyname><forenames>Tomasz</forenames></author></authors><title>An algebraic approach to weak and delay bismulation in coalgebra</title><categories>cs.LO</categories><comments>This paper has been withdrawn due to existing better attempt to model
  saturators coalgebraically</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to introduce an algebraic structure on the set of
all coalgebras with the same state space over the given type which allows us to
present definitions of weak and delay bisimulation for coalgebras.
Additionally, we introduce an algebraic structure on the carrier set of the
final coalgebra and characterize a special subcoalgebra of the final coalgebra
which is used in the formulation of the weak coinduction principle. Finally,
the new algebraic setting allows us to present a definition of an approximated
weak bisimulation, study its properties and compare it with previously defined
weak bisimulation for coalgebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4858</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4858</id><created>2013-02-20</created><authors><author><keyname>Shahzad</keyname><forenames>Mohammad</forenames><affiliation>LAAS</affiliation></author><author><keyname>Mora-Camino</keyname><forenames>F&#xe9;lix</forenames><affiliation>MAIAA</affiliation></author><author><keyname>Slama</keyname><forenames>Jules Ghislain</forenames><affiliation>COPPE-UFRJ</affiliation></author><author><keyname>Achaibou</keyname><forenames>Karim</forenames><affiliation>LAAS</affiliation></author></authors><title>Trajectory generation and display for free flight</title><categories>math.OC cs.RO</categories><proxy>ccsd</proxy><journal-ref>ICAS 2000, 22nd Congress of International Council of the
  Aeronautical Sciences, Harrogate : United Kingdom (2000)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study a new approach is proposed for the generation of aircraft
trajectories. The relative guidance of an aircraft, which is aimed to join in
minimum time the track of a leader aircraft, is particularly considered. In a
first place, a minimum time relative convergence problem is considered and
optimal trajectories are characterized. Then the synthesis of a neural
approximator for optimal trajectories is discussed. Trained neural networks are
used in an adaptive manner to generate intent trajectories during operation.
Finally simulation results involving two wide body aircraft are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4864</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4864</id><created>2013-02-20</created><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Meyer</keyname><forenames>Martin</forenames></author></authors><title>Technology Transfer and the End of the Bayh-Dole Effect: Patents as an
  Analytical Lens on University-Industry-Government Relations</title><categories>cs.DL</categories><comments>in press in Scientometrics with the title: &quot;A Reply to Etzkowitz'
  Comments to Leydesdorff &amp; Martin (2010)&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three periods can be distinguished in university patenting at the U.S. Patent
and Trade Office (USPTO) since the Bayh-Dole Act of 1980: (1) a first period of
exponential increase in university patenting till 1995 (filing date) or 1999
(issuing date); (2) a period of relative decline since 1999; and (3) in most
recent years -- since 2008 -- a linear increase in university patenting. We
argue that this last period is driven by specific non-US universities (e.g.,
Tokyo University and Chinese universities) patenting increasingly in the U.S.A.
as the most competitive market for high-tech patents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4870</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4870</id><created>2013-02-20</created><authors><author><keyname>Sultana</keyname><forenames>Shaheena</forenames></author><author><keyname>Rahman</keyname><forenames>Md. Saidur</forenames></author><author><keyname>Roy</keyname><forenames>Arpita</forenames></author><author><keyname>Tairin</keyname><forenames>Suraiya</forenames></author></authors><title>Bar 1-Visibility Drawings of 1-Planar Graphs</title><categories>cs.DM cs.DS math.CO</categories><comments>15 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A bar 1-visibility drawing of a graph $G$ is a drawing of $G$ where each
vertex is drawn as a horizontal line segment called a bar, each edge is drawn
as a vertical line segment where the vertical line segment representing an edge
must connect the horizontal line segments representing the end vertices and a
vertical line segment corresponding to an edge intersects at most one bar which
is not an end point of the edge. A graph $G$ is bar 1-visible if $G$ has a bar
1-visibility drawing. A graph $G$ is 1-planar if $G$ has a drawing in a
2-dimensional plane such that an edge crosses at most one other edge. In this
paper we give linear-time algorithms to find bar 1-visibility drawings of
diagonal grid graphs and maximal outer 1-planar graphs. We also show that
recursive quadrangle 1-planar graphs and pseudo double wheel 1-planar graphs
are bar 1-visible graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4872</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4872</id><created>2013-02-20</created><authors><author><keyname>S&#xee;rbu</keyname><forenames>Alina</forenames></author><author><keyname>Loreto</keyname><forenames>Vittorio</forenames></author><author><keyname>Servedio</keyname><forenames>Vito D. P.</forenames></author><author><keyname>Tria</keyname><forenames>Francesca</forenames></author></authors><title>Cohesion, consensus and extreme information in opinion dynamics</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>20 pages, 11 figures</comments><journal-ref>ADVANCES IN COMPLEX SYSTEMS online, 1350035 (2013)</journal-ref><doi>10.1142/S0219525913500355</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opinion formation is an important element of social dynamics. It has been
widely studied in the last years with tools from physics, mathematics and
computer science. Here, a continuous model of opinion dynamics for multiple
possible choices is analysed. Its main features are the inclusion of
disagreement and possibility of modulating information, both from one and
multiple sources. The interest is in identifying the effect of the initial
cohesion of the population, the interplay between cohesion and information
extremism, and the effect of using multiple sources of information that can
influence the system. Final consensus, especially with external information,
depends highly on these factors, as numerical simulations show. When no
information is present, consensus or segregation is determined by the initial
cohesion of the population. Interestingly, when only one source of information
is present, consensus can be obtained, in general, only when this is extremely
mild, i.e. there is not a single opinion strongly promoted, or in the special
case of a large initial cohesion and low information exposure. On the contrary,
when multiple information sources are allowed, consensus can emerge with an
information source even when this is not extremely mild, i.e. it carries a
strong message, for a large range of initial conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4874</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4874</id><created>2013-02-20</created><authors><author><keyname>Sim&#xf5;es</keyname><forenames>Gon&#xe7;alo</forenames></author><author><keyname>Galhardas</keyname><forenames>Helena</forenames></author><author><keyname>Matos</keyname><forenames>David</forenames></author></authors><title>A Labeled Graph Kernel for Relationship Extraction</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an approach for Relationship Extraction (RE) based
on labeled graph kernels. The kernel we propose is a particularization of a
random walk kernel that exploits two properties previously studied in the RE
literature: (i) the words between the candidate entities or connecting them in
a syntactic representation are particularly likely to carry information
regarding the relationship; and (ii) combining information from distinct
sources in a kernel may help the RE system make better decisions. We performed
experiments on a dataset of protein-protein interactions and the results show
that our approach obtains effectiveness values that are comparable with the
state-of-the art kernel methods. Moreover, our approach is able to outperform
the state-of-the-art kernels when combined with other kernel methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4882</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4882</id><created>2013-02-20</created><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author></authors><title>Detection of Cooperative Black Hole Attack in Wireless Ad Hoc Networks</title><categories>cs.CR cs.NI</categories><comments>8 pages, 14 figures, 2 tables. This journal paper is an extended
  version of the already published conference paper of the author which is
  having arXiv identifier: arXiv:1111.0387. arXiv admin note: the majority of
  this single-authored submission is identical to arXiv:1111.0387, co-authored
  with two others</comments><journal-ref>International Journal of Simulation,Systems,Science and
  Technology. Vol 12, No 4, August 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mobile ad hoc network (MANET) is a collection of autonomous nodes that
communicate with each other by forming a multi-hop radio network and
maintaining connections in a decentralized manner. Security remains a major
challenge for these networks due to their features of open medium, dynamically
changing topologies, reliance on cooperative algorithms, absence of centralized
monitoring points, and lack of clear lines of defense. Protecting the network
layer of a MANET from malicious attacks is an important and challenging
security issue, since most of the routing protocols for MANETs are vulnerable
to various types of attacks. Ad hoc on-demand distance vector routing (AODV) is
a very popular routing algorithm. However, it is vulnerable to the well-known
black hole attack, where a malicious node falsely advertises good paths to a
destination node during the route discovery process but drops all packets in
the data forwarding phase. This attack becomes more severe when a group of
malicious nodes cooperate each other. The proposed mechanism does not apply any
cryptographic primitives on the routing messages. Instead, it protects the
network by detecting and reacting to malicious activities of the nodes.
Simulation results show that the scheme has a significantly high detection rate
with moderate network traffic overhead and computation overhead in the nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4886</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4886</id><created>2013-02-20</created><updated>2014-03-05</updated><authors><author><keyname>Aravkin</keyname><forenames>Aleksandr Y.</forenames></author><author><keyname>Kumar</keyname><forenames>Rajiv</forenames></author><author><keyname>Mansour</keyname><forenames>Hassan</forenames></author><author><keyname>Recht</keyname><forenames>Ben</forenames></author><author><keyname>Herrmann</keyname><forenames>Felix J.</forenames></author></authors><title>Fast methods for denoising matrix completion formulations, with
  applications to robust seismic data interpolation</title><categories>stat.ML cs.LG</categories><comments>26 pages, 13 figures</comments><msc-class>62F35, 65K10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent SVD-free matrix factorization formulations have enabled rank
minimization for systems with millions of rows and columns, paving the way for
matrix completion in extremely large-scale applications, such as seismic data
interpolation.
  In this paper, we consider matrix completion formulations designed to hit a
target data-fitting error level provided by the user, and propose an algorithm
called LR-BPDN that is able to exploit factorized formulations to solve the
corresponding optimization problem. Since practitioners typically have strong
prior knowledge about target error level, this innovation makes it easy to
apply the algorithm in practice, leaving only the factor rank to be determined.
  Within the established framework, we propose two extensions that are highly
relevant to solving practical challenges of data interpolation. First, we
propose a weighted extension that allows known subspace information to improve
the results of matrix completion formulations. We show how this weighting can
be used in the context of frequency continuation, an essential aspect to
seismic data interpolation. Second, we propose matrix completion formulations
that are robust to large measurement errors in the available data.
  We illustrate the advantages of LR-BPDN on the collaborative filtering
problem using the MovieLens 1M, 10M, and Netflix 100M datasets. Then, we use
the new method, along with its robust and subspace re-weighted extensions, to
obtain high-quality reconstructions for large scale seismic interpolation
problems with real data, even in the presence of data contamination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4888</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4888</id><created>2013-02-20</created><updated>2013-12-24</updated><authors><author><keyname>Shi</keyname><forenames>Yue</forenames></author><author><keyname>Larson</keyname><forenames>Martha</forenames></author><author><keyname>Hanjalic</keyname><forenames>Alan</forenames></author></authors><title>Exploiting Social Tags for Cross-Domain Collaborative Filtering</title><categories>cs.IR cs.AI</categories><comments>Manuscript under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most challenging problems in recommender systems based on the
collaborative filtering (CF) concept is data sparseness, i.e., limited user
preference data is available for making recommendations. Cross-domain
collaborative filtering (CDCF) has been studied as an effective mechanism to
alleviate data sparseness of one domain using the knowledge about user
preferences from other domains. A key question to be answered in the context of
CDCF is what common characteristics can be deployed to link different domains
for effective knowledge transfer. In this paper, we assess the usefulness of
user-contributed (social) tags in this respect. We do so by means of the
Generalized Tag-induced Cross-domain Collaborative Filtering (GTagCDCF)
approach that we propose in this paper and that we developed based on the
general collective matrix factorization framework. Assessment is done by a
series of experiments, using publicly available CF datasets that represent
three cross-domain cases, i.e., two two-domain cases and one three-domain case.
A comparative analysis on two-domain cases involving GTagCDCF and several
state-of-the-art CDCF approaches indicates the increased benefit of using
social tags as representatives of explicit links between domains for CDCF as
compared to the implicit links deployed by the existing CDCF methods. In
addition, we show that users from different domains can already benefit from
GTagCDCF if they only share a few common tags. Finally, we use the three-domain
case to validate the robustness of GTagCDCF with respect to the scale of
datasets and the varying number of domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4916</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4916</id><created>2013-02-20</created><authors><author><keyname>Zubiaga</keyname><forenames>Arkaitz</forenames></author><author><keyname>Garc&#xed;a-Plaza</keyname><forenames>Alberto P&#xe9;rez</forenames></author><author><keyname>Fresno</keyname><forenames>V&#xed;ctor</forenames></author><author><keyname>Mart&#xed;nez</keyname><forenames>Raquel</forenames></author></authors><title>Stacking from Tags: Clustering Bookmarks around a Theme</title><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Since very recently, users on the social bookmarking service Delicious can
stack web pages in addition to tagging them. Stacking enables users to group
web pages around specific themes with the aim of recommending to others.
However, users still stack a small subset of what they tag, and thus many web
pages remain unstacked. This paper presents early research towards
automatically clustering web pages from tags to find stacks and extend
recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4922</identifier>
 <datestamp>2013-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4922</id><created>2013-02-20</created><updated>2013-05-13</updated><authors><author><keyname>Duvenaud</keyname><forenames>David</forenames></author><author><keyname>Lloyd</keyname><forenames>James Robert</forenames></author><author><keyname>Grosse</keyname><forenames>Roger</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joshua B.</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>Structure Discovery in Nonparametric Regression through Compositional
  Kernel Search</title><categories>stat.ML cs.LG stat.ME</categories><comments>9 pages, 7 figures, To appear in proceedings of the 2013
  International Conference on Machine Learning</comments><acm-class>G.3; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite its importance, choosing the structural form of the kernel in
nonparametric regression remains a black art. We define a space of kernel
structures which are built compositionally by adding and multiplying a small
number of base kernels. We present a method for searching over this space of
structures which mirrors the scientific discovery process. The learned
structures can often decompose functions into interpretable components and
enable long-range extrapolation on time-series datasets. Our structure search
method outperforms many widely used kernels and kernel combination methods on a
variety of prediction tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4928</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4928</id><created>2013-02-20</created><authors><author><keyname>Bacchus</keyname><forenames>Fahiem</forenames></author><author><keyname>Grove</keyname><forenames>Adam J.</forenames></author></authors><title>Graphical Models for Preference and Utility</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-3-10</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic independence can dramatically simplify the task of eliciting,
representing, and computing with probabilities in large domains. A key
technique in achieving these benefits is the idea of graphical modeling. We
survey existing notions of independence for utility functions in a
multi-attribute space, and suggest that these can be used to achieve similar
advantages. Our new results concern conditional additive independence, which we
show always has a perfect representation as separation in an undirected graph
(a Markov network). Conditional additive independencies entail a particular
functional for the utility function that is analogous to a product
decomposition of a probability function, and confers analogous benefits. This
functional form has been utilized in the Bayesian network and influence diagram
literature, but generally without an explanation in terms of independence. The
functional form yields a decomposition of the utility function that can greatly
speed up expected utility calculations, particularly when the utility graph has
a similar topology to the probabilistic network being used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4929</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4929</id><created>2013-02-20</created><authors><author><keyname>Balke</keyname><forenames>Alexander</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Counterfactuals and Policy Analysis in Structural Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-11-18</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluation of counterfactual queries (e.g., &quot;If A were true, would C have
been true?&quot;) is important to fault diagnosis, planning, determination of
liability, and policy analysis. We present a method of revaluating
counterfactuals when the underlying causal model is represented by structural
models - a nonlinear generalization of the simultaneous equations models
commonly used in econometrics and social sciences. This new method provides a
coherent means for evaluating policies involving the control of variables
which, prior to enacting the policy were influenced by other variables in the
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4930</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4930</id><created>2013-02-20</created><authors><author><keyname>Benferhat</keyname><forenames>Salem</forenames></author><author><keyname>Saffiotti</keyname><forenames>Alessandro</forenames></author><author><keyname>Smets</keyname><forenames>Philippe</forenames></author></authors><title>Belief Functions and Default Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-19-26</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach to dealing with default information based on the
theory of belief functions. Our semantic structures, inspired by Adams'
epsilon-semantics, are epsilon-belief assignments, where values committed to
focal elements are either close to 0 or close to 1. We define two systems based
on these structures, and relate them to other non-monotonic systems presented
in the literature. We show that our second system correctly addresses the
well-known problems of specificity, irrelevance, blocking of inheritance,
ambiguity, and redundancy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4931</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4931</id><created>2013-02-20</created><authors><author><keyname>Boldrin</keyname><forenames>Luca</forenames></author><author><keyname>Sossai</keyname><forenames>Claudio</forenames></author></authors><title>An Algebraic Semantics for Possibilistic Logic</title><categories>cs.AI cs.LO</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-27-35</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The first contribution of this paper is the presentation of a Pavelka - like
formulation of possibilistic logic in which the language is naturally enriched
by two connectives which represent negation (eg) and a new type of conjunction
(otimes). The space of truth values for this logic is the lattice of
possibility functions, that, from an algebraic point of view, forms a quantal.
A second contribution comes from the understanding of the new conjunction as
the combination of tokens of information coming from different sources, which
makes our language &quot;dynamic&quot;. A Gentzen calculus is presented, which is proved
sound and complete with respect to the given semantics. The problem of truth
functionality is discussed in this context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4932</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4932</id><created>2013-02-20</created><authors><author><keyname>Breese</keyname><forenames>John S.</forenames></author><author><keyname>Blake</keyname><forenames>Russ</forenames></author></authors><title>Automating Computer Bottleneck Detection with Belief Nets</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-36-45</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an application of belief networks to the diagnosis of bottlenecks
in computer systems. The technique relies on a high-level functional model of
the interaction between application workloads, the Windows NT operating system,
and system hardware. Given a workload description, the model predicts the
values of observable system counters available from the Windows NT performance
monitoring tool. Uncertainty in workloads, predictions, and counter values are
characterized with Gaussian distributions. During diagnostic inference, we use
observed performance monitor values to find the most probable assignment to the
workload parameters. In this paper we provide some background on automated
bottleneck detection, describe the structure of the system model, and discuss
empirical procedures for model calibration and verification. Part of the
calibration process includes generating a dataset to estimate a multivariate
Gaussian error model. Initial results in diagnosing bottlenecks are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4933</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4933</id><created>2013-02-20</created><authors><author><keyname>Buntine</keyname><forenames>Wray L.</forenames></author></authors><title>Chain Graphs for Learning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-46-54</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chain graphs combine directed and undirected graphs and their underlying
mathematics combines properties of the two. This paper gives a simplified
definition of chain graphs based on a hierarchical combination of Bayesian
(directed) and Markov (undirected) networks. Examples of a chain graph are
multivariate feed-forward networks, clustering with conditional interaction
between variables, and forms of Bayes classifiers. Chain graphs are then
extended using the notation of plates so that samples and data analysis
problems can be represented in a graphical model as well. Implications for
learning are discussed in the conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4934</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4934</id><created>2013-02-20</created><authors><author><keyname>Castillo</keyname><forenames>Enrique F.</forenames></author><author><keyname>Bouckaert</keyname><forenames>Remco R.</forenames></author><author><keyname>Sarabia</keyname><forenames>Jose M.</forenames></author><author><keyname>Solares</keyname><forenames>Cristina</forenames></author></authors><title>Error Estimation in Approximate Bayesian Belief Network Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-55-62</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We can perform inference in Bayesian belief networks by enumerating
instantiations with high probability thus approximating the marginals. In this
paper, we present a method for determining the fraction of instantiations that
has to be considered such that the absolute error in the marginals does not
exceed a predefined value. The method is based on extreme value theory.
Essentially, the proposed method uses the reversed generalized Pareto
distribution to model probabilities of instantiations below a given threshold.
Based on this distribution, an estimate of the maximal absolute error if
instantiations with probability smaller than u are disregarded can be made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4935</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4935</id><created>2013-02-20</created><authors><author><keyname>Castro</keyname><forenames>Juan Luis</forenames></author><author><keyname>Zurita</keyname><forenames>Jose Manuel</forenames></author></authors><title>Generating the Structure of a Fuzzy Rule under Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-63-67</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to present a method for identifying the structure of
a rule in a fuzzy model. For this purpose, an ATMS shall be used (Zurita 1994).
An algorithm obtaining the identification of the structure will be suggested
(Castro 1995). The minimal structure of the rule (with respect to the number of
variables that must appear in the rule) will be found by this algorithm.
Furthermore, the identification parameters shall be obtained simultaneously.
The proposed method shall be applied for classification to an example. The {em
Iris Plant Database} shall be learnt for all three kinds of plants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4936</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4936</id><created>2013-02-20</created><authors><author><keyname>Cayrac</keyname><forenames>Didier</forenames></author><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>Practical Model-Based Diagnosis with Qualitative Possibilistic
  Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-68-76</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An approach to fault isolation that exploits vastly incomplete models is
presented. It relies on separate descriptions of each component behavior,
together with the links between them, which enables focusing of the reasoning
to the relevant part of the system. As normal observations do not need
explanation, the behavior of the components is limited to anomaly propagation.
Diagnostic solutions are disorders (fault modes or abnormal signatures) that
are consistent with the observations, as well as abductive explanations. An
ordinal representation of uncertainty based on possibility theory provides a
simple exception-tolerant description of the component behaviors. We can for
instance distinguish between effects that are more or less certainly present
(or absent) and effects that are more or less certainly present (or absent)
when a given anomaly is present. A realistic example illustrates the benefits
of this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4937</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4937</id><created>2013-02-20</created><authors><author><keyname>Chavez</keyname><forenames>Tom</forenames></author><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>Decision Flexibility</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-77-86</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of new methods and representations for temporal
decision-making requires a principled basis for characterizing and measuring
the flexibility of decision strategies in the face of uncertainty. Our goal in
this paper is to provide a framework - not a theory - for observing how
decision policies behave in the face of informational perturbations, to gain
clues as to how they might behave in the face of unanticipated, possibly
unarticulated uncertainties. To this end, we find it beneficial to distinguish
between two types of uncertainty: &quot;Small World&quot; and &quot;Large World&quot; uncertainty.
The first type can be resolved by posing an unambiguous question to a
&quot;clairvoyant,&quot; and is anchored on some well-defined aspect of a decision frame.
The second type is more troublesome, yet it is often of greater interest when
we address the issue of flexibility; this type of uncertainty can be resolved
only by consulting a &quot;psychic.&quot; We next observe that one approach to
flexibility used in the economics literature is already implicitly accounted
for in the Maximum Expected Utility (MEU) principle from decision theory.
Though simple, the observation establishes the context for a more illuminating
notion of flexibility, what we term flexibility with respect to information
revelation. We show how to perform flexibility analysis of a static (i.e.,
single period) decision problem using a simple example, and we observe that the
most flexible alternative thus identified is not necessarily the MEU
alternative. We extend our analysis for a dynamic (i.e., multi-period) model,
and we demonstrate how to calculate the value of flexibility for decision
strategies that allow downstream revision of an upstream commitment decision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4938</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4938</id><created>2013-02-20</created><authors><author><keyname>Chickering</keyname><forenames>David Maxwell</forenames></author></authors><title>A Transformational Characterization of Equivalent Bayesian Network
  Structures</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-87-98</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple characterization of equivalent Bayesian network
structures based on local transformations. The significance of the
characterization is twofold. First, we are able to easily prove several new
invariant properties of theoretical interest for equivalent structures. Second,
we use the characterization to derive an efficient algorithm that identifies
all of the compelled edges in a structure. Compelled edge identification is of
particular importance for learning Bayesian network structures from data
because these edges indicate causal relationships when certain assumptions
hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4939</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4939</id><created>2013-02-20</created><authors><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>Conditioning Methods for Exact and Approximate Inference in Causal
  Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-99-107</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two algorithms for exact and approximate inference in causal
networks. The first algorithm, dynamic conditioning, is a refinement of cutset
conditioning that has linear complexity on some networks for which cutset
conditioning is exponential. The second algorithm, B-conditioning, is an
algorithm for approximate inference that allows one to trade-off the quality of
approximations with the computation time. We also present some experimental
results illustrating the properties of the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4940</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4940</id><created>2013-02-20</created><authors><author><keyname>de Campos</keyname><forenames>Luis M.</forenames></author><author><keyname>Moral</keyname><forenames>Serafin</forenames></author></authors><title>Independence Concepts for Convex Sets of Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-108-115</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study different concepts of independence for convex sets of
probabilities. There will be two basic ideas for independence. The first is
irrelevance. Two variables are independent when a change on the knowledge about
one variable does not affect the other. The second one is factorization. Two
variables are independent when the joint convex set of probabilities can be
decomposed on the product of marginal convex sets. In the case of the Theory of
Probability, these two starting points give rise to the same definition. In the
case of convex sets of probabilities, the resulting concepts will be strongly
related, but they will not be equivalent. As application of the concept of
independence, we shall consider the problem of building a global convex set
from marginal convex sets of probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4941</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4941</id><created>2013-02-20</created><authors><author><keyname>Draper</keyname><forenames>Denise L.</forenames></author></authors><title>Clustering Without (Thinking About) Triangulation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-125-133</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The undirected technique for evaluating belief networks [Jensen, et.al.,
1990, Lauritzen and Spiegelhalter, 1988] requires clustering the nodes in the
network into a junction tree. In the traditional view, the junction tree is
constructed from the cliques of the moralized and triangulated belief network:
triangulation is taken to be the primitive concept, the goal towards which any
clustering algorithm (e.g. node elimination) is directed. In this paper, we
present an alternative conception of clustering, in which clusters and the
junction tree property play the role of primitives: given a graph (not a tree)
of clusters which obey (a modified version of) the junction tree property, we
transform this graph until we have obtained a tree. There are several
advantages to this approach: it is much clearer and easier to understand, which
is important for humans who are constructing belief networks; it admits a wider
range of heuristics which may enable more efficient or superior clustering
algorithms; and it serves as the natural basis for an incremental clustering
scheme, which we describe.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4942</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4942</id><created>2013-02-20</created><authors><author><keyname>Driver</keyname><forenames>Eric</forenames></author><author><keyname>Morrell</keyname><forenames>Darryl</forenames></author></authors><title>Implementation of Continuous Bayesian Networks Using Sums of Weighted
  Gaussians</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-134-140</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian networks provide a method of representing conditional independence
between random variables and computing the probability distributions associated
with these random variables. In this paper, we extend Bayesian network
structures to compute probability density functions for continuous random
variables. We make this extension by approximating prior and conditional
densities using sums of weighted Gaussian distributions and then finding the
propagation rules for updating the densities in terms of these weights. We
present a simple example that illustrates the Bayesian network for continuous
variables; this example shows the effect of the network structure and
approximation errors on the computation of densities for variables in the
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4943</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4943</id><created>2013-02-20</created><authors><author><keyname>Druzdzel</keyname><forenames>Marek J.</forenames></author><author><keyname>van der Gaag</keyname><forenames>Linda C.</forenames></author></authors><title>Elicitation of Probabilities for Belief Networks: Combining Qualitative
  and Quantitative Information</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-141-148</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the usefulness of belief networks for reasoning under uncertainty is
widely accepted, obtaining numerical probabilities that they require is still
perceived a major obstacle. Often not enough statistical data is available to
allow for reliable probability estimation. Available information may not be
directly amenable for encoding in the network. Finally, domain experts may be
reluctant to provide numerical probabilities. In this paper, we propose a
method for elicitation of probabilities from a domain expert that is
non-invasive and accommodates whatever probabilistic information the expert is
willing to state. We express all available information, whether qualitative or
quantitative in nature, in a canonical form consisting of (in) equalities
expressing constraints on the hyperspace of possible joint probability
distributions. We then use this canonical form to derive second-order
probability distributions over the desired probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4944</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4944</id><created>2013-02-20</created><authors><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>Numerical Representations of Acceptance</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-149-156</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accepting a proposition means that our confidence in this proposition is
strictly greater than the confidence in its negation. This paper investigates
the subclass of uncertainty measures, expressing confidence, that capture the
idea of acceptance, what we call acceptance functions. Due to the monotonicity
property of confidence measures, the acceptance of a proposition entails the
acceptance of any of its logical consequences. In agreement with the idea that
a belief set (in the sense of Gardenfors) must be closed under logical
consequence, it is also required that the separate acceptance o two
propositions entail the acceptance of their conjunction. Necessity (and
possibility) measures agree with this view of acceptance while probability and
belief functions generally do not. General properties of acceptance functions
are estabilished. The motivation behind this work is the investigation of a
setting for belief revision more general than the one proposed by Alchourron,
Gardenfors and Makinson, in connection with the notion of conditioning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4945</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4945</id><created>2013-02-20</created><authors><author><keyname>Ezawa</keyname><forenames>Kazuo J.</forenames></author><author><keyname>Schuermann</keyname><forenames>Til</forenames></author></authors><title>Fraud/Uncollectible Debt Detection Using a Bayesian Network Based
  Learning System: A Rare Binary Outcome with Mixed Data Structures</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-157-166</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fraud/uncollectible debt problem in the telecommunications industry
presents two technical challenges: the detection and the treatment of the
account given the detection. In this paper, we focus on the first problem of
detection using Bayesian network models, and we briefly discuss the application
of a normative expert system for the treatment at the end. We apply Bayesian
network models to the problem of fraud/uncollectible debt detection for
telecommunication services. In addition to being quite successful at predicting
rare event outcomes, it is able to handle a mixture of categorical and
continuous data. We present a performance comparison using linear and
non-linear discriminant analysis, classification and regression trees, and
Bayesian network models
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4946</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4946</id><created>2013-02-20</created><authors><author><keyname>Fargier</keyname><forenames>Helene</forenames></author><author><keyname>Lang</keyname><forenames>Jerome</forenames></author><author><keyname>Martin-Clouaire</keyname><forenames>Roger</forenames></author><author><keyname>Schiex</keyname><forenames>Thomas</forenames></author></authors><title>A Constraint Satisfaction Approach to Decision under Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-167-174</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Constraint Satisfaction Problem (CSP) framework offers a simple and sound
basis for representing and solving simple decision problems, without
uncertainty. This paper is devoted to an extension of the CSP framework
enabling us to deal with some decisions problems under uncertainty. This
extension relies on a differentiation between the agent-controllable decision
variables and the uncontrollable parameters whose values depend on the
occurrence of uncertain events. The uncertainty on the values of the parameters
is assumed to be given under the form of a probability distribution. Two
algorithms are given, for computing respectively decisions solving the problem
with a maximal probability, and conditional decisions mapping the largest
possible amount of possible cases to actual decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4947</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4947</id><created>2013-02-20</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Plausibility Measures: A User's Guide</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-175-184</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine a new approach to modeling uncertainty based on plausibility
measures, where a plausibility measure just associates with an event its
plausibility, an element is some partially ordered set. This approach is easily
seen to generalize other approaches to modeling uncertainty, such as
probability measures, belief functions, and possibility measures. The lack of
structure in a plausibility measure makes it easy for us to add structure on an
&quot;as needed&quot; basis, letting us examine what is required to ensure that a
plausibility measure has certain properties of interest. This gives us insight
into the essential features of the properties in question, while allowing us to
prove general results that apply to many approaches to reasoning about
uncertainty. Plausibility measures have already proved useful in analyzing
default reasoning. In this paper, we examine their &quot;algebraic properties,&quot;
analogues to the use of + and * in probability theory. An understanding of such
properties will be essential if plausibility measures are to be used in
practice as a representation tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4948</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4948</id><created>2013-02-20</created><authors><author><keyname>Galles</keyname><forenames>David</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Testing Identifiability of Causal Effects</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-185-195</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns the probabilistic evaluation of the effects of actions in
the presence of unmeasured variables. We show that the identification of causal
effect between a singleton variable X and a set of variables Y can be
accomplished systematically, in time polynomial in the number of variables in
the graph. When the causal effect is identifiable, a closed-form expression can
be obtained for the probability that the action will achieve a specified goal,
or a set of goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4949</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4949</id><created>2013-02-20</created><authors><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>A Characterization of the Dirichlet Distribution with Application to
  Learning Bayesian Networks</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-196-207</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a new characterization of the Dirichlet distribution. This
characterization implies that under assumptions made by several previous
authors for learning belief networks, a Dirichlet prior on the parameters is
inevitable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4950</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4950</id><created>2013-02-20</created><authors><author><keyname>Goldszmidt</keyname><forenames>Moises</forenames></author></authors><title>Fast Belief Update Using Order-of-Magnitude Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-208-216</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm, called Predict, for updating beliefs in causal
networks quantified with order-of-magnitude probabilities. The algorithm takes
advantage of both the structure and the quantification of the network and
presents a polynomial asymptotic complexity. Predict exhibits a conservative
behavior in that it is always sound but not always complete. We provide
sufficient conditions for completeness and present algorithms for testing these
conditions and for computing a complete set of plausible values. We propose
Predict as an efficient method to estimate probabilistic values and illustrate
its use in conjunction with two known algorithms for probabilistic inference.
Finally, we describe an application of Predict to plan evaluation, present
experimental results, and discuss issues regarding its use with conditional
logics of belief, and in the characterization of irrelevance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4951</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4951</id><created>2013-02-20</created><authors><author><keyname>Grosof</keyname><forenames>Benjamin N.</forenames></author></authors><title>Transforming Prioritized Defaults and Specificity into Parallel Defaults</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-217-228</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to transform any set of prioritized propositional defaults into
an equivalent set of parallel (i.e., unprioritized) defaults, in
circumscription. We give an algorithm to implement the transform. We show how
to use the transform algorithm as a generator of a whole family of inferencing
algorithms for circumscription. The method is to employ the transform algorithm
as a front end to any inferencing algorithm, e.g., one of the previously
available, that handles the parallel (empty) case of prioritization. Our
algorithms provide not just coverage of a new expressive class, but also
alternatives to previous algorithms for implementing the previously covered
class (?layered?) of prioritization. In particular, we give a new
query-answering algorithm for prioritized cirumscription which is sound and
complete for the full expressive class of unrestricted finite prioritization
partial orders, for propositional defaults (or minimized predicates). By
contrast, previous algorithms required that the prioritization partial order be
layered, i.e., structured similar to the system of rank in the military. Our
algorithm enables, for the first time, the implementation of the most useful
class of prioritization: non-layered prioritization partial orders. Default
inheritance, for example, typically requires non-layered prioritization to
represent specificity adequately. Our algorithm enables not only the
implementation of default inheritance (and specificity) within prioritized
circumscription, but also the extension and combination of default inheritance
with other kinds of prioritized default reasoning, e.g.: with stratified logic
programs with negation-as-failure. Such logic programs are previously known to
be representable equivalently as layered-priority predicate circumscriptions.
Worst-case, the transform increases the number of defaults exponentially. We
discuss how inferencing is practically implementable nevertheless in two kinds
of situations: general expressiveness but small numbers of defaults, or
expressive special cases with larger numbers of defaults. One such expressive
special case is non-?top-heaviness? of the prioritization partial order. In
addition to its direct implementation, the transform can also be exploited
analytically to generate special case algorithms, e.g., a tractable transform
for a class within default inheritance (detailed in another, forthcoming
paper). We discuss other aspects of the significance of the fundamental result.
One can view the transform as reducing n degrees of partially ordered belief
confidence to just 2 degrees of confidence: for-sure and (unprioritized)
default. Ordinary, parallel default reasoning, e.g., in parallel
circumscription or Poole's Theorist, can be viewed in these terms as reducing 2
degrees of confidence to just 1 degree of confidence: that of the non-monotonic
theory's conclusions. The expressive reduction's computational complexity
suggests that prioritization is valuable for its expressive conciseness, just
as defaults are for theirs. For Reiter's Default Logic and Poole's Theorist,
the transform implies how to extend those formalisms so as to equip them with a
concept of prioritization that is exactly equivalent to that in
circumscription. This provides an interesting alternative to Brewka's approach
to equipping them with prioritization-type precedence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4952</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4952</id><created>2013-02-20</created><authors><author><keyname>Haddawy</keyname><forenames>Peter</forenames></author><author><keyname>Doan</keyname><forenames>AnHai</forenames></author><author><keyname>Goodwin</keyname><forenames>Richard</forenames></author></authors><title>Efficient Decision-Theoretic Planning: Techniques and Empirical Analysis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-229-236</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses techniques for performing efficient decision-theoretic
planning. We give an overview of the DRIPS decision-theoretic refinement
planning system, which uses abstraction to efficiently identify optimal plans.
We present techniques for automatically generating search control information,
which can significantly improve the planner's performance. We evaluate the
efficiency of DRIPS both with and without the search control rules on a complex
medical planning problem and compare its performance to that of a
branch-and-bound decision tree algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4953</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4953</id><created>2013-02-20</created><authors><author><keyname>Hajek</keyname><forenames>Petr</forenames></author><author><keyname>Godo</keyname><forenames>Lluis</forenames></author><author><keyname>Esteva</keyname><forenames>Francesc</forenames></author></authors><title>Fuzzy Logic and Probability</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-237-244</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we deal with a new approach to probabilistic reasoning in a
logical framework. Nearly almost all logics of probability that have been
proposed in the literature are based on classical two-valued logic. After
making clear the differences between fuzzy logic and probability theory, here
we propose a {em fuzzy} logic of probability for which completeness results (in
a probabilistic sense) are provided. The main idea behind this approach is that
probability values of crisp propositions can be understood as truth-values of
some suitable fuzzy propositions associated to the crisp ones. Moreover,
suggestions and examples of how to extend the formalism to cope with
conditional probabilities and with other uncertainty formalisms are also
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4954</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4954</id><created>2013-02-20</created><authors><author><keyname>Hanks</keyname><forenames>Steve</forenames></author><author><keyname>Madigan</keyname><forenames>David</forenames></author><author><keyname>Gavrin</keyname><forenames>Jonathan</forenames></author></authors><title>Probabilistic Temporal Reasoning with Endogenous Change</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-245-254</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a probabilistic model for reasoning about the state of a
system as it changes over time, both due to exogenous and endogenous
influences. Our target domain is a class of medical prediction problems that
are neither so urgent as to preclude careful diagnosis nor progress so slowly
as to allow arbitrary testing and treatment options. In these domains there is
typically enough time to gather information about the patient's state and
consider alternative diagnoses and treatments, but the temporal interaction
between the timing of tests, treatments, and the course of the disease must
also be considered. Our approach is to elicit a qualitative structural model of
the patient from a human expert---the model identifies important attributes,
the way in which exogenous changes affect attribute values, and the way in
which the patient's condition changes endogenously. We then elicit
probabilistic information to capture the expert's uncertainty about the effects
of tests and treatments and the nature and timing of endogenous state changes.
This paper describes the model in the context of a problem in treating vehicle
accident trauma, and suggests a method for solving the model based on the
technique of sequential imputation. A complementary goal of this work is to
understand and synthesize a disparate collection of research efforts all using
the name ?probabilistic temporal reasoning.? This paper analyzes related work
and points out essential differences between our proposed model and other
approaches in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4955</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4955</id><created>2013-02-20</created><authors><author><keyname>Harmanec</keyname><forenames>David</forenames></author></authors><title>Toward a Characterization of Uncertainty Measure for the Dempster-Shafer
  Theory</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-255-261</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a working paper summarizing results of an ongoing research project
whose aim is to uniquely characterize the uncertainty measure for the
Dempster-Shafer Theory. A set of intuitive axiomatic requirements is presented,
some of their implications are shown, and the proof is given of the minimality
of recently proposed measure AU among all measures satisfying the proposed
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4956</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4956</id><created>2013-02-20</created><updated>2015-05-16</updated><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>A Definition and Graphical Representation for Causality</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1995-PG-262-273</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a precise definition of cause and effect in terms of a fundamental
notion called unresponsiveness. Our definition is based on Savage's (1954)
formulation of decision theory and departs from the traditional view of
causation in that our causal assertions are made relative to a set of
decisions. An important consequence of this departure is that we can reason
about cause locally, not requiring a causal explanation for every dependency.
Such local reasoning can be beneficial because it may not be necessary to
determine whether a particular dependency is causal to make a decision. Also in
this paper, we examine the graphical encoding of causal relationships. We show
that influence diagrams in canonical form are an accurate and efficient
representation of causal relationships. In addition, we establish a
correspondence between canonical form and Pearl's causal theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4957</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4957</id><created>2013-02-20</created><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Geiger</keyname><forenames>Dan</forenames></author></authors><title>Learning Bayesian Networks: A Unification for Discrete and Gaussian
  Domains</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-274-284</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine Bayesian methods for learning Bayesian networks from a combination
of prior knowledge and statistical data. In particular, we unify the approaches
we presented at last year's conference for discrete and Gaussian domains. We
derive a general Bayesian scoring metric, appropriate for both domains. We then
use this metric in combination with well-known statistical facts about the
Dirichlet and normal--Wishart distributions to derive our metrics for discrete
and Gaussian domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4958</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4958</id><created>2013-02-20</created><updated>2015-05-16</updated><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>A Bayesian Approach to Learning Causal Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1995-PG-285-295</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whereas acausal Bayesian networks represent probabilistic independence,
causal Bayesian networks represent causal relationships. In this paper, we
examine Bayesian methods for learning both types of networks. Bayesian methods
for learning acausal networks are fairly well developed. These methods often
employ assumptions to facilitate the construction of priors, including the
assumptions of parameter independence, parameter modularity, and likelihood
equivalence. We show that although these assumptions also can be appropriate
for learning causal networks, we need additional assumptions in order to learn
causal networks. We introduce two sufficient assumptions, called {em mechanism
independence} and {em component independence}. We show that these new
assumptions, when combined with parameter independence, parameter modularity,
and likelihood equivalence, allow us to apply methods for learning acausal
networks to learn causal networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4959</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4959</id><created>2013-02-20</created><authors><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author><author><keyname>Barry</keyname><forenames>Matthew</forenames></author></authors><title>Display of Information for Time-Critical Decision Making</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-296-305</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe methods for managing the complexity of information displayed to
people responsible for making high-stakes, time-critical decisions. The
techniques provide tools for real-time control of the configuration and
quantity of information displayed to a user, and a methodology for designing
flexible human-computer interfaces for monitoring applications. After defining
a prototypical set of display decision problems, we introduce the expected
value of revealed information (EVRI) and the related measure of expected value
of displayed information (EVDI). We describe how these measures can be used to
enhance computer displays used for monitoring complex systems. We motivate the
presentation by discussing our efforts to employ decision-theoretic control of
displays for a time-critical monitoring application at the NASA Mission Control
Center in Houston.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4960</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4960</id><created>2013-02-20</created><authors><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author><author><keyname>Klein</keyname><forenames>Adrian</forenames></author></authors><title>Reasoning, Metareasoning, and Mathematical Truth: Studies of Theorem
  Proving under Limited Resources</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-306-314</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In earlier work, we introduced flexible inference and decision-theoretic
metareasoning to address the intractability of normative inference. Here,
rather than pursuing the task of computing beliefs and actions with decision
models composed of distinctions about uncertain events, we examine methods for
inferring beliefs about mathematical truth before an automated theorem prover
completes a proof. We employ a Bayesian analysis to update belief in truth,
given theorem-proving progress, and show how decision-theoretic methods can be
used to determine the value of continuing to deliberate versus taking immediate
action in time-critical situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4961</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4961</id><created>2013-02-20</created><authors><author><keyname>Hulme</keyname><forenames>Mark</forenames></author></authors><title>Improved Sampling for Diagnostic Reasoning in Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-315-322</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian networks offer great potential for use in automating large scale
diagnostic reasoning tasks. Gibbs sampling is the main technique used to
perform diagnostic reasoning in large richly interconnected Bayesian networks.
Unfortunately Gibbs sampling can take an excessive time to generate a
representative sample. In this paper we describe and test a number of heuristic
strategies for improving sampling in noisy-or Bayesian networks. The strategies
include Monte Carlo Markov chain sampling techniques other than Gibbs sampling.
Emphasis is put on strategies that can be implemented in distributed systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4962</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4962</id><created>2013-02-20</created><authors><author><keyname>Jensen</keyname><forenames>Finn Verner</forenames></author></authors><title>Cautious Propagation in Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-323-328</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the situation where some evidence e has been entered to a Bayesian
network. When performing conflict analysis, sensitivity analysis, or when
answering questions like &quot;What if the finding on X had been y instead of x?&quot;
you need probabilities P (e'| h), where e' is a subset of e, and h is a
configuration of a (possibly empty) set of variables. Cautious propagation is a
modification of HUGIN propagation into a Shafer-Shenoy-like architecture. It is
less efficient than HUGIN propagation; however, it provides easy access to P
(e'| h) for a great deal of relevant subsets e'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4963</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4963</id><created>2013-02-20</created><authors><author><keyname>Jenzarli</keyname><forenames>Ali</forenames></author></authors><title>Information/Relevance Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-329-337</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we extend the influence diagram (ID) representation for
decisions under uncertainty. In the standard ID, arrows into a decision node
are only informational; they do not represent constraints on what the decision
maker can do. We can represent such constraints only indirectly, using arrows
to the children of the decision and sometimes adding more variables to the
influence diagram, thus making the ID more complicated. Users of influence
diagrams often want to represent constraints by arrows into decision nodes. We
represent constraints on decisions by allowing relevance arrows into decision
nodes. We call the resulting representation information/relevance influence
diagrams (IRIDs). Information/relevance influence diagrams allow for direct
representation and specification of constrained decisions. We use a combination
of stochastic dynamic programming and Gibbs sampling to solve IRIDs. This
method is especially useful when exact methods for solving IDs fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4964</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4964</id><created>2013-02-20</created><authors><author><keyname>John</keyname><forenames>George H.</forenames></author><author><keyname>Langley</keyname><forenames>Pat</forenames></author></authors><title>Estimating Continuous Distributions in Bayesian Classifiers</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-338-345</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When modeling a probability distribution with a Bayesian network, we are
faced with the problem of how to handle continuous variables. Most previous
work has either solved the problem by discretizing, or assumed that the data
are generated by a single Gaussian. In this paper we abandon the normality
assumption and instead use statistical methods for nonparametric density
estimation. For a naive Bayesian classifier, we present experimental results on
a variety of natural and artificial domains, comparing two methods of density
estimation: assuming normality and modeling each conditional distribution with
a single Gaussian; and using nonparametric kernel density estimation. We
observe large reductions in error on several natural and artificial data sets,
which suggests that kernel estimation is a useful tool for learning Bayesian
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4965</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4965</id><created>2013-02-20</created><authors><author><keyname>Kanazawa</keyname><forenames>Keiji</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author><author><keyname>Russell</keyname><forenames>Stuart</forenames></author></authors><title>Stochastic Simulation Algorithms for Dynamic Probabilistic Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-346-351</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic simulation algorithms such as likelihood weighting often give
fast, accurate approximations to posterior probabilities in probabilistic
networks, and are the methods of choice for very large networks. Unfortunately,
the special characteristics of dynamic probabilistic networks (DPNs), which are
used to represent stochastic temporal processes, mean that standard simulation
algorithms perform very poorly. In essence, the simulation trials diverge
further and further from reality as the process is observed over time. In this
paper, we present simulation algorithms that use the evidence observed at each
time step to push the set of trials back towards reality. The first algorithm,
?evidence reversal? (ER) restructures each time slice of the DPN so that the
evidence nodes for the slice become ancestors of the state variables. The
second algorithm, called ?survival of the fittestz? sampling (SOF),
?repopulates? the set of trials at each time step using a stochastic
reproduction rate weighted by the likelihood of the evidence according to each
trial. We compare the performance of each algorithm with likelihood weighting
on the original network, and also investigate the benefits of combining the ER
and SOF methods. The ER/SOF combination appears to maintain bounded error
independent of the number of time steps in the simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4966</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4966</id><created>2013-02-20</created><authors><author><keyname>Karakoulas</keyname><forenames>Grigoris I.</forenames></author></authors><title>Probabilistic Exploration in Planning while Learning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-352-361</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequential decision tasks with incomplete information are characterized by
the exploration problem; namely the trade-off between further exploration for
learning more about the environment and immediate exploitation of the accrued
information for decision-making. Within artificial intelligence, there has been
an increasing interest in studying planning-while-learning algorithms for these
decision tasks. In this paper we focus on the exploration problem in
reinforcement learning and Q-learning in particular. The existing exploration
strategies for Q-learning are of a heuristic nature and they exhibit limited
scaleability in tasks with large (or infinite) state and action spaces.
Efficient experimentation is needed for resolving uncertainties when possible
plans are compared (i.e. exploration). The experimentation should be sufficient
for selecting with statistical significance a locally optimal plan (i.e.
exploitation). For this purpose, we develop a probabilistic hill-climbing
algorithm that uses a statistical selection procedure to decide how much
exploration is needed for selecting a plan which is, with arbitrarily high
probability, arbitrarily close to a locally optimal one. Due to its generality
the algorithm can be employed for the exploration strategy of robust
Q-learning. An experiment on a relatively complex control task shows that the
proposed exploration strategy performs better than a typical exploration
strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4967</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4967</id><created>2013-02-20</created><authors><author><keyname>Kim</keyname><forenames>Young-Gyun</forenames></author><author><keyname>Valtorta</keyname><forenames>Marco</forenames></author></authors><title>On the Detection of Conflicts in Diagnostic Bayesian Networks Using
  Abstraction</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-362-367</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important issue in the use of expert systems is the so-called brittleness
problem. Expert systems model only a limited part of the world. While the
explicit management of uncertainty in expert systems itigates the brittleness
problem, it is still possible for a system to be used, unwittingly, in ways
that the system is not prepared to address. Such a situation may be detected by
the method of straw models, first presented by Jensen et al. [1990] and later
generalized and justified by Laskey [1991]. We describe an algorithm, which we
have implemented, that takes as input an annotated diagnostic Bayesian network
(the base model) and constructs, without assistance, a bipartite network to be
used as a straw model. We show that in some cases this straw model is better
that the independent straw model of Jensen et al., the only other straw model
for which a construction algorithm has been designed and implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4968</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4968</id><created>2013-02-20</created><authors><author><keyname>Kj&#xe6;rulff</keyname><forenames>Uffe</forenames></author></authors><title>HUGS: Combining Exact Inference and Gibbs Sampling in Junction Trees</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-368-375</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dawid, Kjaerulff and Lauritzen (1994) provided a preliminary description of a
hybrid between Monte-Carlo sampling methods and exact local computations in
junction trees. Utilizing the strengths of both methods, such hybrid inference
methods has the potential of expanding the class of problems which can be
solved under bounded resources as well as solving problems which otherwise
resist exact solutions. The paper provides a detailed description of a
particular instance of such a hybrid scheme; namely, combination of exact
inference and Gibbs sampling in discrete Bayesian networks. We argue that this
combination calls for an extension of the usual message passing scheme of
ordinary junction trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4969</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4969</id><created>2013-02-20</created><authors><author><keyname>Kozlov</keyname><forenames>Alexander V.</forenames></author><author><keyname>Singh</keyname><forenames>Jaswinder Pal</forenames></author></authors><title>Sensitivities: An Alternative to Conditional Probabilities for Bayesian
  Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-376-385</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show an alternative way of representing a Bayesian belief network by
sensitivities and probability distributions. This representation is equivalent
to the traditional representation by conditional probabilities, but makes
dependencies between nodes apparent and intuitively easy to understand. We also
propose a QR matrix representation for the sensitivities and/or conditional
probabilities which is more efficient, in both memory requirements and
computational speed, than the traditional representation for computer-based
implementations of probabilistic inference. We use sensitivities to show that
for a certain class of binary networks, the computation time for approximate
probabilistic inference with any positive upper bound on the error of the
result is independent of the size of the network. Finally, as an alternative to
traditional algorithms that use conditional probabilities, we describe an exact
algorithm for probabilistic inference that uses the QR-representation for
sensitivities and updates probability distributions of nodes in a network
according to messages from the neighbors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4970</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4970</id><created>2013-02-20</created><authors><author><keyname>Krause</keyname><forenames>Paul J.</forenames></author><author><keyname>Fox</keyname><forenames>John</forenames></author><author><keyname>Judson</keyname><forenames>Philip</forenames></author></authors><title>Is There a Role for Qualitative Risk Assessment?</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-386-393</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classically, risk is characterized by a point value probability indicating
the likelihood of occurrence of an adverse effect. However, there are domains
where the attainability of objective numerical risk characterizations is
increasingly being questioned. This paper reviews the arguments in favour of
extending classical techniques of risk assessment to incorporate meaningful
qualitative and weak quantitative risk characterizations. A technique in which
linguistic uncertainty terms are defined in terms of patterns of argument is
then proposed. The technique is demonstrated using a prototype computer-based
system for predicting the carcinogenic risk due to novel chemical compounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4971</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4971</id><created>2013-02-20</created><authors><author><keyname>Littman</keyname><forenames>Michael L.</forenames></author><author><keyname>Dean</keyname><forenames>Thomas L.</forenames></author><author><keyname>Kaelbling</keyname><forenames>Leslie Pack</forenames></author></authors><title>On the Complexity of Solving Markov Decision Problems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-394-402</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov decision problems (MDPs) provide the foundations for a number of
problems of interest to AI researchers studying automated planning and
reinforcement learning. In this paper, we summarize results regarding the
complexity of solving MDPs and the running time of MDP solution algorithms. We
argue that, although MDPs can be solved efficiently in theory, more study is
needed to reveal practical algorithms for solving large problems quickly. To
encourage future research, we sketch some alternative methods of analysis that
rely on the structure of MDPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4972</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4972</id><created>2013-02-20</created><authors><author><keyname>Meek</keyname><forenames>Christopher</forenames></author></authors><title>Causal Inference and Causal Explanation with Background Knowledge</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-403-410</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents correct algorithms for answering the following two
questions; (i) Does there exist a causal explanation consistent with a set of
background knowledge which explains all of the observed independence facts in a
sample? (ii) Given that there is such a causal explanation what are the causal
relationships common to every such causal explanation?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4973</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4973</id><created>2013-02-20</created><authors><author><keyname>Meek</keyname><forenames>Christopher</forenames></author></authors><title>Strong Completeness and Faithfulness in Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-411-418</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A completeness result for d-separation applied to discrete Bayesian networks
is presented and it is shown that in a strong measure-theoretic sense almost
all discrete distributions for a given network structure are faithful; i.e. the
independence facts true of the distribution are all and only those entailed by
the network structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4974</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4974</id><created>2013-02-20</created><authors><author><keyname>Ngo</keyname><forenames>Liem</forenames></author><author><keyname>Haddawy</keyname><forenames>Peter</forenames></author><author><keyname>Helwig</keyname><forenames>James</forenames></author></authors><title>A Theoretical Framework for Context-Sensitive Temporal Probability Model
  Construction with Application to Plan Projection</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-419-426</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a context-sensitive temporal probability logic for representing
classes of discrete-time temporal Bayesian networks. Context constraints allow
inference to be focused on only the relevant portions of the probabilistic
knowledge. We provide a declarative semantics for our language. We present a
Bayesian network construction algorithm whose generated networks give sound and
complete answers to queries. We use related concepts in logic programming to
justify our approach. We have implemented a Bayesian network construction
algorithm for a subset of the theory and demonstrate it's application to the
problem of evaluating the effectiveness of treatments for acute cardiac
conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4975</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4975</id><created>2013-02-20</created><authors><author><keyname>Parsons</keyname><forenames>Simon</forenames></author></authors><title>Refining Reasoning in Qualitative Probabilistic Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-427-434</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years there has been a spate of papers describing systems for
probabilisitic reasoning which do not use numerical probabilities. In some
cases the simple set of values used by these systems make it impossible to
predict how a probability will change or which hypothesis is most likely given
certain evidence. This paper concentrates on such situations, and suggests a
number of ways in which they may be resolved by refining the representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4976</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4976</id><created>2013-02-20</created><authors><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>On the Testability of Causal Models with Latent and Instrumental
  Variables</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-435-443</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Certain causal models involving unmeasured variables induce no independence
constraints among the observed variables but imply, nevertheless, inequality
contraints on the observed distribution. This paper derives a general formula
for such instrumental variables, that is, exogenous variables that directly
affect some variables but not all. With the help of this formula, it is
possible to test whether a model involving instrumental variables may account
for the data, or, conversely, whether a given variables can be deemed
instrumental.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4977</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4977</id><created>2013-02-20</created><authors><author><keyname>Pearl</keyname><forenames>Judea</forenames></author><author><keyname>Robins</keyname><forenames>James M.</forenames></author></authors><title>Probabilistic Evaluation of Sequential Plans from Causal Models with
  Hidden Variables</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-444-453</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper concerns the probabilistic evaluation of plans in the presence of
unmeasured variables, each plan consisting of several concurrent or sequential
actions. We establish a graphical criterion for recognizing when the effects of
a given plan can be predicted from passive observations on measured variables
only. When the criterion is satisfied, a closed-form expression is provided for
the probability that the plan will achieve a specified goal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4978</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4978</id><created>2013-02-20</created><authors><author><keyname>Poole</keyname><forenames>David L.</forenames></author></authors><title>Exploiting the Rule Structure for Decision Making within the Independent
  Choice Logic</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-454-463</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the independent choice logic, and in particular the
&quot;single agent with nature&quot; instance of the independent choice logic, namely
ICLdt. This is a logical framework for decision making uncertainty that extends
both logic programming and stochastic models such as influence diagrams. This
paper shows how the representation of a decision problem within the independent
choice logic can be exploited to cut down the combinatorics of dynamic
programming. One of the main problems with influence diagram evaluation
techniques is the need to optimise a decision for all values of the 'parents'
of a decision variable. In this paper we show how the rule based nature of the
ICLdt can be exploited so that we only make distinctions in the values of the
information available for a decision that will make a difference to utility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4979</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4979</id><created>2013-02-20</created><authors><author><keyname>Provan</keyname><forenames>Gregory M.</forenames></author></authors><title>Abstraction in Belief Networks: The Role of Intermediate States in
  Diagnostic Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-464-473</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian belief networks are bing increasingly used as a knowledge
representation for diagnostic reasoning. One simple method for conducting
diagnostic reasoning is to represent system faults and observations only. In
this paper, we investigate how having intermediate nodes-nodes other than fault
and observation nodes affects the diagnostic performance of a Bayesian belief
network. We conducted a series of experiments on a set of real belief networks
for medical diagnosis in liver and bile disease. We compared the effects on
diagnostic performance of a two-level network consisting just of disease and
finding nodes with that of a network which models intermediate
pathophysiological disease states as well. We provide some theoretical evidence
for differences observed between the abstracted two-level network and the full
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4980</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4980</id><created>2013-02-20</created><authors><author><keyname>Pynadath</keyname><forenames>David V.</forenames></author><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author></authors><title>Accounting for Context in Plan Recognition, with Application to Traffic
  Monitoring</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-472-481</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Typical approaches to plan recognition start from a representation of an
agent's possible plans, and reason evidentially from observations of the
agent's actions to assess the plausibility of the various candidates. A more
expansive view of the task (consistent with some prior work) accounts for the
context in which the plan was generated, the mental state and planning process
of the agent, and consequences of the agent's actions in the world. We present
a general Bayesian framework encompassing this view, and focus on how context
can be exploited in plan recognition. We demonstrate the approach on a problem
in traffic monitoring, where the objective is to induce the plan of the driver
from observation of vehicle movements. Starting from a model of how the driver
generates plans, we show how the highway context can appropriately influence
the recognizer's interpretation of observed driver behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4981</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4981</id><created>2013-02-20</created><authors><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author></authors><title>A New Pruning Method for Solving Decision Trees and Game Trees</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-482-490</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main goal of this paper is to describe a new pruning method for solving
decision trees and game trees. The pruning method for decision trees suggests a
slight variant of decision trees that we call scenario trees. In scenario
trees, we do not need a conditional probability for each edge emanating from a
chance node. Instead, we require a joint probability for each path from the
root node to a leaf node. We compare the pruning method to the traditional
rollback method for decision trees and game trees. For problems that require
Bayesian revision of probabilities, a scenario tree representation with the
pruning method is more efficient than a decision tree representation with the
rollback method. For game trees, the pruning method is more efficient than the
rollback method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4982</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4982</id><created>2013-02-20</created><authors><author><keyname>Spirtes</keyname><forenames>Peter L.</forenames></author></authors><title>Directed Cyclic Graphical Representations of Feedback Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-491-498</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of directed acyclic graphs (DAGs) to represent conditional
independence relations among random variables has proved fruitful in a variety
of ways. Recursive structural equation models are one kind of DAG model.
However, non-recursive structural equation models of the kinds used to model
economic processes are naturally represented by directed cyclic graphs with
independent errors, a characterization of conditional independence errors, a
characterization of conditional independence constraints is obtained, and it is
shown that the result generalizes in a natural way to systems in which the
error variables or noises are statistically dependent. For non-linear systems
with independent errors a sufficient condition for conditional independence of
variables in associated distributions is obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4983</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4983</id><created>2013-02-20</created><authors><author><keyname>Spirtes</keyname><forenames>Peter L.</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author><author><keyname>Richardson</keyname><forenames>Thomas S.</forenames></author></authors><title>Causal Inference in the Presence of Latent Variables and Selection Bias</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-499-506</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there is a general, informative and reliable procedure for
discovering causal relations when, for all the investigator knows, both latent
variables and selection bias may be at work. Given information about
conditional independence and dependence relations between measured variables,
even when latent variables and selection bias may be present, there are
sufficient conditions for reliably concluding that there is a causal path from
one variable to another, and sufficient conditions for reliably concluding when
no such causal path exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4984</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4984</id><created>2013-02-20</created><authors><author><keyname>Srinivas</keyname><forenames>Sampath</forenames></author></authors><title>Modeling Failure Priors and Persistence in Model-Based Diagnosis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-507-514</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic model-based diagnosis computes the posterior probabilities of
failure of components from the prior probabilities of component failure and
observations of system behavior. One problem with this method is that such
priors are almost never directly available. One of the reasons is that the
prior probability estimates include an implicit notion of a time interval over
which they are specified -- for example, if the probability of failure of a
component is 0.05, is this over the period of a day or is this over a week? A
second problem facing probabilistic model-based diagnosis is the modeling of
persistence. Say we have an observation about a system at time t_1 and then
another observation at a later time t_2. To compute posterior probabilities
that take into account both the observations, we need some model of how the
state of the system changes from time t_1 to t_2. In this paper, we address
these problems using techniques from Reliability theory. We show how to compute
the failure prior of a component from an empirical measure of its reliability
-- the Mean Time Between Failure (MTBF). We also develop a scheme to model
persistence when handling multiple time tagged observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4985</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4985</id><created>2013-02-20</created><authors><author><keyname>Srinivas</keyname><forenames>Sampath</forenames></author></authors><title>A Polynomial Algorithm for Computing the Optimal Repair Strategy in a
  System with Independent Component Failures</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-515-522</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of diagnosis is to compute good repair strategies in response to
anomalous system behavior. In a decision theoretic framework, a good repair
strategy has low expected cost. In a general formulation of the problem, the
computation of the optimal (lowest expected cost) repair strategy for a system
with multiple faults is intractable. In this paper, we consider an interesting
and natural restriction on the behavior of the system being diagnosed: (a) the
system exhibits faulty behavior if and only if one or more components is
malfunctioning. (b) The failures of the system components are independent.
Given this restriction on system behavior, we develop a polynomial time
algorithm for computing the optimal repair strategy. We then go on to introduce
a system hierarchy and the notion of inspecting (testing) components before
repair. We develop a linear time algorithm for computing an optimal repair
strategy for the hierarchical system which includes both repair and inspection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4986</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4986</id><created>2013-02-20</created><authors><author><keyname>Srinivas</keyname><forenames>Sampath</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author></authors><title>Exploiting System Hierarchy to Compute Repair Plans in Probabilistic
  Model-based Diagnosis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-523-531</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of model-based diagnosis is to isolate causes of anomalous system
behavior and recommend inexpensive repair actions in response. In general,
precomputing optimal repair policies is intractable. To date, investigators
addressing this problem have explored approximations that either impose
restrictions on the system model (such as a single fault assumption) or compute
an immediate best action with limited lookahead. In this paper, we develop a
formulation of repair in model-based diagnosis and a repair algorithm that
computes optimal sequences of actions. This optimal approach is costly but can
be applied to precompute an optimal repair strategy for compact systems. We
show how we can exploit a hierarchical system specification to make this
approach tractable for large systems. When introducing hierarchy, we also
consider the tradeoff between simply replacing a component and decomposing it
to repair its subcomponents. The hierarchical repair algorithm is suitable for
off-line precomputation of an optimal repair strategy. A modification of the
algorithm takes advantage of an iterative deepening scheme to trade off
inference time and the quality of the computed strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4987</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4987</id><created>2013-02-20</created><authors><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author><author><keyname>Ford</keyname><forenames>Matthew</forenames></author><author><keyname>Larson</keyname><forenames>Kenneth</forenames></author></authors><title>Path Planning under Time-Dependent Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-532-539</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard algorithms for finding the shortest path in a graph require that the
cost of a path be additive in edge costs, and typically assume that costs are
deterministic. We consider the problem of uncertain edge costs, with potential
probabilistic dependencies among the costs. Although these dependencies violate
the standard dynamic-programming decomposition, we identify a weaker stochastic
consistency condition that justifies a generalized dynamic-programming approach
based on stochastic dominance. We present a revised path-planning algorithm and
prove that it produces optimal paths under time-dependent uncertain costs. We
test the algorithm by applying it to a model of stochastic bus networks, and
present empirical performance results comparing it to some alternatives.
Finally, we consider extensions of these concepts to a more general class of
problems of heuristic search under uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4988</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4988</id><created>2013-02-20</created><authors><author><keyname>Weydert</keyname><forenames>Emil</forenames></author></authors><title>Defaults and Infinitesimals: Defeasible Inference by Nonarchimedean
  Entropy-Maximization</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-540-547</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new semantics for defeasible inference based on extended
probability measures allowed to take infinitesimal values, on the
interpretation of defaults as generalized conditional probability constraints
and on a preferred-model implementation of entropy maximization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4989</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4989</id><created>2013-02-20</created><authors><author><keyname>Wilson</keyname><forenames>Nic</forenames></author></authors><title>An Order of Magnitude Calculus</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-548-555</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a simple calculus for order of magnitude reasoning. A
semantics is given with soundness and completeness results. Order of magnitude
probability functions are easily defined and turn out to be equivalent to kappa
functions, which are slight generalizations of Spohn's Natural Conditional
Functions. The calculus also gives rise to an order of magnitude decision
theory, which can be used to justify an amended version of Pearl's decision
theory for kappa functions, although the latter is weaker and less expressive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4990</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4990</id><created>2013-02-20</created><authors><author><keyname>Wong</keyname><forenames>Michael S. K. M.</forenames></author><author><keyname>Butz</keyname><forenames>C. J.</forenames></author><author><keyname>Xiang</keyname><forenames>Yang</forenames></author></authors><title>A Method for Implementing a Probabilistic Model as a Relational Database</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-556-564</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a method for implementing a probabilistic inference
system based on an extended relational data model. This model provides a
unified approach for a variety of applications such as dynamic programming,
solving sparse linear equations, and constraint propagation. In this framework,
the probability model is represented as a generalized relational database.
Subsequent probabilistic requests can be processed as standard relational
queries. Conventional database management systems can be easily adopted for
implementing such an approximate reasoning system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4991</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4991</id><created>2013-02-20</created><authors><author><keyname>Xiang</keyname><forenames>Yang</forenames></author></authors><title>Optimization of Inter-Subnet Belief Updating in Multiply Sectioned
  Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-565-573</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments show that Multiply Sectioned Bayesian Networks (MSBNs)
can be used for diagnosis of natural systems as well as for model-based
diagnosis of artificial systems. They can be applied to single-agent oriented
reasoning systems as well as multi-agent distributed probabilistic reasoning
systems. Belief propagation between a pair of subnets plays a central role in
maintenance of global consistency in a MSBN. This paper studies the operation
UpdateBelief, presented originally with MSBNs, for inter-subnet propagation. We
analyze how the operation achieves its intended functionality, which provides
hints as for how its efficiency can be improved. We then define two new
versions of UpdateBelief that reduce the computation time for inter-subnet
propagation. One of them is optimal in the sense that the minimum amount of
computation for coordinating multi-linkage belief propagation is required. The
optimization problem is solved through the solution of a graph-theoretic
problem: the minimum weight open tour in a tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4992</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4992</id><created>2013-02-20</created><authors><author><keyname>Xu</keyname><forenames>Hong</forenames></author><author><keyname>Smets</keyname><forenames>Philippe</forenames></author></authors><title>Generating Explanations for Evidential Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-574-581</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present two methods to provide explanations for reasoning
with belief functions in the valuation-based systems. One approach, inspired by
Strat's method, is based on sensitivity analysis, but its computation is
simpler thus easier to implement than Strat's. The other one is to examine the
impact of evidence on the conclusion based on the measure of the information
content in the evidence. We show the property of additivity for the pieces of
evidence that are conditional independent within the context of the
valuation-based systems. We will give an example to show how these approaches
are applied in an evidential network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.4993</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.4993</id><created>2013-02-20</created><authors><author><keyname>Zhang</keyname><forenames>Nevin Lianwen</forenames></author></authors><title>Inference with Causal Independence in the CPSC Network</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)</comments><proxy>auai</proxy><report-no>UAI-P-1995-PG-582-589</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports experiments with the causal independence inference
algorithm proposed by Zhang and Poole (1994b) on the CPSC network created by
Pradhan et al. (1994). It is found that the algorithm is able to answer 420 of
the 422 possible zero-observation queries, 94 of 100 randomly generated
five-observation queries, 87 of 100 randomly generated ten-observation queries,
and 69 of 100 randomly generated twenty-observation queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5002</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5002</id><created>2013-02-20</created><updated>2013-12-25</updated><authors><author><keyname>Govindasamy</keyname><forenames>Siddhartan</forenames></author></authors><title>Asymptotic Data Rates of Receive-Diversity Systems with MMSE Estimation
  and Spatially Correlated Interferers</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An asymptotic technique is presented to characterize the bits/symbol
achievable on a representative wireless link in a spatially distributed network
with active interferers at correlated positions, N receive diversity branches,
and linear Minimum-Mean-Square-Error (MMSE) receivers. This framework is then
applied to systems including analogs to Matern type I and type II networks
which are useful to model systems with Medium-Access Control (MAC), cellular
uplinks with orthogonal transmissions and frequency reuse, and Boolean cluster
networks. It is found that for our network models, with moderately large N, the
correlation between interferer positions does not significantly influence the
bits/symbol resulting in simple approximations for the data rates achievable in
such networks which are known to be difficult to analyze and for which few
analytical results are available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5010</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5010</id><created>2013-02-20</created><updated>2014-12-23</updated><authors><author><keyname>Tan</keyname><forenames>Mingkui</forenames></author><author><keyname>Tsang</keyname><forenames>Ivor W.</forenames></author><author><keyname>Wang</keyname><forenames>Li</forenames></author></authors><title>Matching Pursuit LASSO Part II: Applications and Sparse Recovery over
  Batch Signals</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matching Pursuit LASSIn Part I \cite{TanPMLPart1}, a Matching Pursuit LASSO
({MPL}) algorithm has been presented for solving large-scale sparse recovery
(SR) problems. In this paper, we present a subspace search to further improve
the performance of MPL, and then continue to address another major challenge of
SR -- batch SR with many signals, a consideration which is absent from most of
previous $\ell_1$-norm methods. As a result, a batch-mode {MPL} is developed to
vastly speed up sparse recovery of many signals simultaneously. Comprehensive
numerical experiments on compressive sensing and face recognition tasks
demonstrate the superior performance of MPL and BMPL over other methods
considered in this paper, in terms of sparse recovery ability and efficiency.
In particular, BMPL is up to 400 times faster than existing $\ell_1$-norm
methods considered to be state-of-the-art.O Part II: Applications and Sparse
Recovery over Batch Signals
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5021</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5021</id><created>2013-02-20</created><authors><author><keyname>Lalitha</keyname><forenames>V.</forenames></author><author><keyname>Prakash</keyname><forenames>N.</forenames></author><author><keyname>Vinodh</keyname><forenames>K.</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijay</forenames></author><author><keyname>Pradhan</keyname><forenames>S. Sandeep</forenames></author></authors><title>Linear Coding Schemes for the Distributed Computation of Subspaces</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Journal of Selected Areas in Communications
  (In-Network Computation: Exploring the Fundamental Limits), April 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $X_1, ..., X_m$ be a set of $m$ statistically dependent sources over the
common alphabet $\mathbb{F}_q$, that are linearly independent when considered
as functions over the sample space. We consider a distributed function
computation setting in which the receiver is interested in the lossless
computation of the elements of an $s$-dimensional subspace $W$ spanned by the
elements of the row vector $[X_1, \ldots, X_m]\Gamma$ in which the $(m \times
s)$ matrix $\Gamma$ has rank $s$. A sequence of three increasingly refined
approaches is presented, all based on linear encoders.
  The first approach uses a common matrix to encode all the sources and a
Korner-Marton like receiver to directly compute $W$. The second improves upon
the first by showing that it is often more efficient to compute a carefully
chosen superspace $U$ of $W$. The superspace is identified by showing that the
joint distribution of the $\{X_i\}$ induces a unique decomposition of the set
of all linear combinations of the $\{X_i\}$, into a chain of subspaces
identified by a normalized measure of entropy. This subspace chain also
suggests a third approach, one that employs nested codes. For any joint
distribution of the $\{X_i\}$ and any $W$, the sum-rate of the nested code
approach is no larger than that under the Slepian-Wolf (SW) approach. Under the
SW approach, $W$ is computed by first recovering each of the $\{X_i\}$. For a
large class of joint distributions and subspaces $W$, the nested code approach
is shown to improve upon SW. Additionally, a class of source distributions and
subspaces are identified, for which the nested-code approach is sum-rate
optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5039</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5039</id><created>2013-02-20</created><authors><author><keyname>Maso</keyname><forenames>Marco</forenames></author><author><keyname>Cardoso</keyname><forenames>Leonardo S.</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author><author><keyname>Vangelista</keyname><forenames>Lorenzo</forenames></author></authors><title>Cognitive Interference Alignment for OFDM Two-tiered Networks</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures. Accepted and presented at the IEEE 13th
  International Workshop on Signal Processing Advances in Wireless
  Communications (SPAWC), 2012. Authors' final version. Copyright transferred
  to IEEE</comments><doi>10.1109/SPAWC.2012.6292902</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this contribution, we introduce an interference alignment scheme that
allows the coexistence of an orthogonal frequency division multiplexing (OFDM)
macro-cell and a cognitive small-cell, deployed in a two-tiered structure and
transmitting over the same bandwidth. We derive the optimal linear strategy for
the single antenna secondary base station, maximizing the spectral efficiency
of the opportunistic link, accounting for both signal sub-space structure and
power loading strategy. Our analytical and numerical findings prove that the
precoder structure proposed is optimal for the considered scenario in the face
of Rayleigh and exponential decaying channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5056</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5056</id><created>2013-01-15</created><authors><author><keyname>Jia</keyname><forenames>Yangqing</forenames></author><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>Pooling-Invariant Image Feature Learning</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised dictionary learning has been a key component in state-of-the-art
computer vision recognition architectures. While highly effective methods exist
for patch-based dictionary learning, these methods may learn redundant features
after the pooling stage in a given early vision architecture. In this paper, we
offer a novel dictionary learning scheme to efficiently take into account the
invariance of learned features after the spatial pooling stage. The algorithm
is built on simple clustering, and thus enjoys efficiency and scalability. We
discuss the underlying mechanism that justifies the use of clustering
algorithms, and empirically show that the algorithm finds better dictionaries
than patch-based methods with the same dictionary size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5062</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5062</id><created>2013-02-20</created><authors><author><keyname>Ra</keyname><forenames>Moo-Ryong</forenames></author><author><keyname>Govindan</keyname><forenames>Ramesh</forenames></author><author><keyname>Ortega</keyname><forenames>Antonio</forenames></author></authors><title>P3: Toward Privacy-Preserving Photo Sharing</title><categories>cs.CR cs.MM</categories><comments>15 pages. Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With increasing use of mobile devices, photo sharing services are
experiencing greater popularity. Aside from providing storage, photo sharing
services enable bandwidth-efficient downloads to mobile devices by performing
server-side image transformations (resizing, cropping). On the flip side, photo
sharing services have raised privacy concerns such as leakage of photos to
unauthorized viewers and the use of algorithmic recognition technologies by
providers. To address these concerns, we propose a privacy-preserving photo
encoding algorithm that extracts and encrypts a small, but significant,
component of the photo, while preserving the remainder in a public,
standards-compatible, part. These two components can be separately stored. This
technique significantly reduces the signal-to-noise ratio and the accuracy of
automated detection and recognition on the public part, while preserving the
ability of the provider to perform server-side transformations to conserve
download bandwidth usage. Our prototype privacy-preserving photo sharing
system, P3, works with Facebook, and can be extended to other services as well.
P3 requires no changes to existing services or mobile application software, and
adds minimal photo storage overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5082</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5082</id><created>2013-02-20</created><authors><author><keyname>Schlegel</keyname><forenames>Christian</forenames></author><author><keyname>Schultz</keyname><forenames>Ulrik P.</forenames></author><author><keyname>Stinckwich</keyname><forenames>Serge</forenames></author></authors><title>Proceedings of the Third International Workshop on Domain-Specific
  Languages and Models for Robotic Systems (DSLRob 2012)</title><categories>cs.RO</categories><comments>Index submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proceedings of the Third International Workshop on Domain-Specific Languages
and Models for Robotic Systems (DSLRob'12), held at the 2012 International
Conference on Simulation, Modeling, and Programming for Autonomous Robots
(SIMPAR 2012), November 2012 in Tsukuba, Japan.
  The main topics of the workshop were Domain-Specific Languages (DSLs) and
Model-driven Architecture (MDA) for robotics. A domain-specific language (DSL)
is a programming language dedicated to a particular problem domain that offers
specific notations and abstractions that increase programmer productivity
within that domain. Models-driven architecture (MDA) offers a high-level way
for domain users to specify the functionality of their system at the right
level of abstraction. DSLs and models have historically been used for
programming complex systems. However recently they have garnered interest as a
separate field of study. Robotic systems blend hardware and software in a
holistic way that intrinsically raises many crosscutting concerns (concurrency,
uncertainty, time constraints, ...), for which reason, traditional
general-purpose languages often lead to a poor fit between the language
features and the implementation requirements. DSLs and models offer a powerful,
systematic way to overcome this problem, enabling the programmer to quickly and
precisely implement novel software solutions to complex problems within the
robotics domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5085</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5085</id><created>2013-02-20</created><authors><author><keyname>Trojanek</keyname><forenames>Piotr</forenames></author></authors><title>Model-driven engineering approach to design and implementation of robot
  control system</title><categories>cs.RO cs.SE</categories><comments>Presented at DSLRob 2011 (arXiv:cs/1212.3308)</comments><report-no>DSLRob/2011/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we apply a model-driven engineering approach to designing
domain-specific solutions for robot control system development. We present a
case study of the complete process, including identification of the domain
meta-model, graphical notation definition and source code generation for
subsumption architecture -- a well-known example of robot control architecture.
Our goal is to show that both the definition of the robot-control architecture
and its supporting tools fits well into the typical workflow of model-driven
engineering development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5101</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5101</id><created>2013-02-20</created><updated>2013-02-25</updated><authors><author><keyname>Blocki</keyname><forenames>Jeremiah</forenames></author><author><keyname>Komanduri</keyname><forenames>Saranga</forenames></author><author><keyname>Procaccia</keyname><forenames>Ariel</forenames></author><author><keyname>Sheffet</keyname><forenames>Or</forenames></author></authors><title>Optimizing Password Composition Policies</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A password composition policy restricts the space of allowable passwords to
eliminate weak passwords that are vulnerable to statistical guessing attacks.
Usability studies have demonstrated that existing password composition policies
can sometimes result in weaker password distributions; hence a more principled
approach is needed. We introduce the first theoretical model for optimizing
password composition policies. We study the computational and sample complexity
of this problem under different assumptions on the structure of policies and on
users' preferences over passwords. Our main positive result is an algorithm
that -- with high probability --- constructs almost optimal policies (which are
specified as a union of subsets of allowed passwords), and requires only a
small number of samples of users' preferred passwords. We complement our
theoretical results with simulations using a real-world dataset of 32 million
passwords.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5109</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5109</id><created>2013-02-20</created><authors><author><keyname>Sironi</keyname><forenames>Marco</forenames></author><author><keyname>Tisato</keyname><forenames>Francesco</forenames></author></authors><title>Capturing Information Flows inside Android and Qemu Environments</title><categories>cs.OS</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The smartphone market has grown so wide that it assumed a strategic
relevance. Today the most common smartphone OSs are Google's Android and
Apple's iOS. The former is particularly interesting due to its open source
nature, that allows everyone to deeply inspect every aspect of the OS. Android
source code is also bundled with an hardware emulator, based on the open source
software Qemu, that allows the user to run the Android OS without the need of a
physical device. We first present a procedure to extract information flows from
a generic system. We then focus on Android and Qemu architectures and their
logging infrastructures. Finally, we detail what happens inside an Android
device in a particular scenario: the system boot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5120</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5120</id><created>2013-02-20</created><authors><author><keyname>Landais</keyname><forenames>Gr&#xe9;gory</forenames></author><author><keyname>Tillich</keyname><forenames>Jean-Pierre</forenames></author></authors><title>An efficient attack of a McEliece cryptosystem variant based on
  convolutional codes</title><categories>cs.CR</categories><comments>12 pages Submitted to PQCRYPTO 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  L\&quot;ondahl and Johansson proposed last year a variant of the McEliece
cryptosystem which replaces Goppa codes by convolutional codes. This
modification is supposed to make structural attacks more difficult since the
public generator matrix of this scheme contains large parts which are generated
completely at random. They proposed two schemes of this kind, one of them
consists in taking a Goppa code and extending it by adding a generator matrix
of a time varying convolutional code. We show here that this scheme can be
successfully attacked by looking for low-weight codewords in the public code of
this scheme and using it to unravel the convolutional part. It remains to break
the Goppa part of this scheme which can be done in less than a day of
computation in the case at hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5121</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5121</id><created>2013-02-20</created><authors><author><keyname>Dvorak</keyname><forenames>Zdenek</forenames></author><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author><author><keyname>Thomas</keyname><forenames>Robin</forenames></author></authors><title>Three-coloring triangle-free planar graphs in linear time</title><categories>math.CO cs.DM cs.DS</categories><comments>22 pages</comments><journal-ref>ACM Transactions on Algorithms 7 (2011), Article 41</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grotzsch's theorem states that every triangle-free planar graph is
3-colorable. Several relatively simple proofs of this fact were provided by
Thomassen and other authors. It is easy to convert these proofs into
quadratic-time algorithms to find a 3-coloring, but it is not clear how to find
such a coloring in linear time (Kowalik used a nontrivial data structure to
construct an O(n log n) algorithm).
  We design a linear-time algorithm to find a 3-coloring of a given
triangle-free planar graph. The algorithm avoids using any complex data
structures, which makes it easy to implement. As a by-product we give a yet
simpler proof of Grotzsch's theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5122</identifier>
 <datestamp>2013-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5122</id><created>2013-02-20</created><updated>2013-09-09</updated><authors><author><keyname>Blocki</keyname><forenames>Jeremiah</forenames></author><author><keyname>Blum</keyname><forenames>Manuel</forenames></author><author><keyname>Datta</keyname><forenames>Anupam</forenames></author></authors><title>Naturally Rehearsing Passwords</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce quantitative usability and security models to guide the design
of password management schemes --- systematic strategies to help users create
and remember multiple passwords. In the same way that security proofs in
cryptography are based on complexity-theoretic assumptions (e.g., hardness of
factoring and discrete logarithm), we quantify usability by introducing
usability assumptions. In particular, password management relies on assumptions
about human memory, e.g., that a user who follows a particular rehearsal
schedule will successfully maintain the corresponding memory. These assumptions
are informed by research in cognitive science and validated through empirical
studies. Given rehearsal requirements and a user's visitation schedule for each
account, we use the total number of extra rehearsals that the user would have
to do to remember all of his passwords as a measure of the usability of the
password scheme. Our usability model leads us to a key observation: password
reuse benefits users not only by reducing the number of passwords that the user
has to memorize, but more importantly by increasing the natural rehearsal rate
for each password. We also present a security model which accounts for the
complexity of password management with multiple accounts and associated
threats, including online, offline, and plaintext password leak attacks.
Observing that current password management schemes are either insecure or
unusable, we present Shared Cues--- a new scheme in which the underlying secret
is strategically shared across accounts to ensure that most rehearsal
requirements are satisfied naturally while simultaneously providing strong
security. The construction uses the Chinese Remainder Theorem to achieve these
competing goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5125</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5125</id><created>2013-02-20</created><authors><author><keyname>Rippel</keyname><forenames>Oren</forenames></author><author><keyname>Adams</keyname><forenames>Ryan Prescott</forenames></author></authors><title>High-Dimensional Probability Estimation with Deep Density Models</title><categories>stat.ML cs.LG</categories><comments>12 pages, 4 figures, 1 table. Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the fundamental problems in machine learning is the estimation of a
probability distribution from data. Many techniques have been proposed to study
the structure of data, most often building around the assumption that
observations lie on a lower-dimensional manifold of high probability. It has
been more difficult, however, to exploit this insight to build explicit,
tractable density models for high-dimensional data. In this paper, we introduce
the deep density model (DDM), a new approach to density estimation. We exploit
insights from deep learning to construct a bijective map to a representation
space, under which the transformation of the distribution of the data is
approximately factorized and has identical and known marginal densities. The
simplicity of the latent distribution under the model allows us to feasibly
explore it, and the invertibility of the map to characterize contraction of
measure across it. This enables us to compute normalized densities for
out-of-sample data. This combination of tractability and flexibility allows us
to tackle a variety of probabilistic tasks on high-dimensional datasets,
including: rapid computation of normalized densities at test-time without
evaluating a partition function; generation of samples without MCMC; and
characterization of the joint entropy of the data.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="41000" completeListSize="102538">1122234|42001</resumptionToken>
</ListRecords>
</OAI-PMH>
