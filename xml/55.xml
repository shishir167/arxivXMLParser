<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T02:04:07Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|54001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4353</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4353</id><created>2013-12-16</created><updated>2013-12-19</updated><authors><author><keyname>Genewein</keyname><forenames>Tim</forenames></author><author><keyname>Braun</keyname><forenames>Daniel A.</forenames></author></authors><title>Abstraction in decision-makers with limited information processing
  capabilities</title><categories>cs.AI cs.IT math.IT stat.ML</categories><comments>Presented at the NIPS 2013 Workshop on Planning with Information
  Constraints</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distinctive property of human and animal intelligence is the ability to
form abstractions by neglecting irrelevant information which allows to separate
structure from noise. From an information theoretic point of view abstractions
are desirable because they allow for very efficient information processing. In
artificial systems abstractions are often implemented through computationally
costly formations of groups or clusters. In this work we establish the relation
between the free-energy framework for decision making and rate-distortion
theory and demonstrate how the application of rate-distortion for
decision-making leads to the emergence of abstractions. We argue that
abstractions are induced due to a limit in information processing capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4354</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4354</id><created>2013-12-16</created><updated>2014-03-04</updated><authors><author><keyname>Kirisits</keyname><forenames>Clemens</forenames></author><author><keyname>Lang</keyname><forenames>Lukas F.</forenames></author><author><keyname>Scherzer</keyname><forenames>Otmar</forenames></author></authors><title>Decomposition of Optical Flow on the Sphere</title><categories>math.OC cs.CV</categories><comments>The final publication is available at link.springer.com</comments><msc-class>92C55, 92C37, 92C17, 35A15, 68U10, 33C55</msc-class><doi>10.1007/s13137-013-0055-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a number of variational regularisation methods for the estimation
and decomposition of motion fields on the $2$-sphere. While motion estimation
is based on the optical flow equation, the presented decomposition models are
motivated by recent trends in image analysis. In particular we treat $u+v$
decomposition as well as hierarchical decomposition. Helmholtz decomposition of
motion fields is obtained as a natural by-product of the chosen numerical
method based on vector spherical harmonics. All models are tested on time-lapse
microscopy data depicting fluorescently labelled endodermal cells of a
zebrafish embryo.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4358</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4358</id><created>2013-12-16</created><authors><author><keyname>Bardet</keyname><forenames>Magali</forenames></author><author><keyname>Bayen</keyname><forenames>T&#xe9;rence</forenames></author></authors><title>On the degree of the polynomial defining a planar algebraic curves of
  constant width</title><categories>math.AG cs.CG</categories><comments>13 pages</comments><msc-class>14H50, 13P05, 13P15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a family of closed planar algebraic curves
$\mathcal{C}$ which are given in parametrization form via a trigonometric
polynomial $p$. When $\mathcal{C}$ is the boundary of a compact convex set, the
polynomial $p$ represents the support function of this set. Our aim is to
examine properties of the degree of the defining polynomial of this family of
curves in terms of the degree of $p$. Thanks to the theory of elimination, we
compute the total degree and the partial degrees of this polynomial, and we
solve in addition a question raised by Rabinowitz in \cite{Rabi} on the lowest
degree polynomial whose graph is a non-circular curve of constant width.
Computations of partial degrees of the defining polynomial of algebraic
surfaces of constant width are also provided in the same way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4359</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4359</id><created>2013-12-16</created><updated>2014-03-22</updated><authors><author><keyname>Venkatesan</keyname><forenames>R. C.</forenames></author><author><keyname>Plastino</keyname><forenames>A.</forenames></author></authors><title>Legendre transform structure and extremal properties of the relative
  Fisher information</title><categories>cond-mat.stat-mech cs.IT math-ph math.IT math.MP</categories><comments>15 pages. Minor mathematical typesetting changes made</comments><doi>10.1016/j.physleta.2014.03.027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variational extremization of the relative Fisher information (RFI, hereafter)
is performed. Reciprocity relations, akin to those of thermodynamics are
derived, employing the extremal results of the RFI expressed in terms of
probability amplitudes. A time independent Schr\&quot;{o}dinger-like equation
(Schr\&quot;{o}dinger-like link) for the RFI is derived. The concomitant Legendre
transform structure (LTS, hereafter) is developed by utilizing a generalized
RFI-Euler theorem, which shows that the entire mathematical structure of
thermodynamics translates into the RFI framework, both for equilibrium and
non-equilibrium cases. The qualitatively distinct nature of the present results
\textit{vis-\'{a}-vis} those of prior studies utilizing the Shannon entropy
and/or the Fisher information measure (FIM, hereafter) is discussed. A
principled relationship between the RFI and the FIM frameworks is derived. The
utility of this relationship is demonstrated by an example wherein the energy
eigenvalues of the Schr\&quot;{o}dinger-like link for the RFI is inferred solely
using the quantum mechanical virial theorem and the LTS of the RFI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4363</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4363</id><created>2013-12-16</created><authors><author><keyname>Strangio</keyname><forenames>Maurizio Adriano</forenames></author></authors><title>On the Security of Wang's Provably Secure Identity-based Key Agreement
  Protocol</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a 2005 IACR report, Wang published an efficient identity-based key
agreement protocol (IDAK) suitable for resource constrained devices.
  The author shows that the IDAK key agreement protocol is secure in the
Bellare-Rogaway model with random oracles and also provides separate ad-hoc
security proofs claiming that the IDAK protocol is not vulnerable to Key
Compromise Impersonation attacks and also enjoys Perfect Forward Secrecy (PFS).
  In this report, we review the security properties of the protocol and point
out that it is vulnerable to Unknown Key Share attacks. Although such attacks
are often difficult to setup in a real world environment they are nevertheless
interesting from a theoretical point of view so we provide a version of the
protocol that fixes the problem in a standard way. We also provide a security
proof of the IDAK protocol based on the Gap Bilinear Diffie Hellman and random
oracle assumptions in the stronger extended Canetti-Krawczyk security model of
distributed computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4370</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4370</id><created>2013-12-16</created><authors><author><keyname>Dong</keyname><forenames>Daoyi</forenames></author><author><keyname>Chen</keyname><forenames>Chunlin</forenames></author><author><keyname>Long</keyname><forenames>Ruixing</forenames></author><author><keyname>Qi</keyname><forenames>Bo</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Sampling-based Learning Control for Quantum Systems with Hamiltonian
  Uncertainties</title><categories>cs.SY</categories><comments>6 pages, 6 figures, 52nd IEEE Conference on Decision and Control,
  December 10-13, 2013, Florence, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust control design for quantum systems has been recognized as a key task
in the development of practical quantum technology. In this paper, we present a
systematic numerical methodology of sampling-based learning control (SLC) for
control design of quantum systems with Hamiltonian uncertainties. The SLC
method includes two steps of &quot;training&quot; and &quot;testing and evaluation&quot;. In the
training step, an augmented system is constructed by sampling uncertainties
according to possible distributions of uncertainty parameters. A gradient flow
based learning and optimization algorithm is adopted to find the control for
the augmented system. In the process of testing and evaluation, a number of
samples obtained through sampling the uncertainties are tested to evaluate the
control performance. Numerical results demonstrate the success of the SLC
approach. The SLC method has potential applications for robust control design
of quantum systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4378</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4378</id><created>2013-12-16</created><updated>2014-03-17</updated><authors><author><keyname>Bidokhti</keyname><forenames>Shirin Saeedi</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author></authors><title>Is Non-Unique Decoding Necessary?</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multi-terminal communication systems, signals carrying messages meant for
different destinations are often observed together at any given destination
receiver. Han and Kobayashi (1981) proposed a receiving strategy which performs
a joint unique decoding of messages of interest along with a subset of messages
which are not of interest. It is now well-known that this provides an
achievable region which is, in general, larger than if the receiver treats all
messages not of interest as noise. Nair and El Gamal (2009) and Chong, Motani,
Garg, and El Gamal (2008) independently proposed a generalization called
indirect or non-unique decoding where the receiver uses the codebook structure
of the messages to uniquely decode only its messages of interest. Non-unique
decoding has since been used in various scenarios.
  The main result in this paper is to provide an interpretation and a
systematic proof technique for why non-unique decoding, in all known cases
where it has been employed, can be replaced by a particularly designed joint
unique decoding strategy, without any penalty from a rate region viewpoint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4381</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4381</id><created>2013-12-16</created><authors><author><keyname>Alama</keyname><forenames>Jesse</forenames></author></authors><title>A machine-assisted view of paraconsistency</title><categories>math.LO cs.LO</categories><comments>7 pages. Submitted to EBL 2014 (Encontro Brasileiro de L\'ogica /
  Brazilian Logic Meeting 2014)</comments><msc-class>03B53, 03F99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a newcomer, paraconsistent logics can be difficult to grasp. Even experts
in logic can find the concept of paraconsistency to be suspicious or misguided,
if not actually wrong. The problem is that although they usually have much in
common with more familiar logics (such as intuitionistic or classical logic),
paraconsistent logics necessarily disagree in other parts of the logical
terrain which one might have thought were not up for debate. Thus, one's
logical intuitions may need to be recalibrated to work skillfully with
paraconsistency. To get started, one should clearly appreciate the possibility
of paraconsistent logics and the genuineness of the distinctions to which
paraconsistency points. For this purpose, one typically encounters matrices
involving more than two truth values to characterize suitable consequence
relations. In the eyes of a two-valued skeptic, such an approach might seem
dubious. Even a non-skeptic might wonder if there's another way. To this end,
to explore the basic notions of paraconsistent logic with the assistance of
automated reasoning techniques. Such an approach has merit because by
delegating some of the logical work to a machine, one's logical &quot;biases&quot; become
externalized. The result is a new way to appreciate that the distinctions to
which paraconsistent logic points are indeed genuine. Our approach can even
suggest new questions and problems for the paraconsistent logic community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4384</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4384</id><created>2013-12-16</created><authors><author><keyname>Golge</keyname><forenames>Eren</forenames></author><author><keyname>Duygulu</keyname><forenames>Pinar</forenames></author></authors><title>Rectifying Self Organizing Maps for Automatic Concept Learning from Web
  Images</title><categories>cs.CV cs.LG cs.NE</categories><comments>present CVPR2014 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We attack the problem of learning concepts automatically from noisy web image
search results. Going beyond low level attributes, such as colour and texture,
we explore weakly-labelled datasets for the learning of higher level concepts,
such as scene categories. The idea is based on discovering common
characteristics shared among subsets of images by posing a method that is able
to organise the data while eliminating irrelevant instances. We propose a novel
clustering and outlier detection method, namely Rectifying Self Organizing Maps
(RSOM). Given an image collection returned for a concept query, RSOM provides
clusters pruned from outliers. Each cluster is used to train a model
representing a different characteristics of the concept. The proposed method
outperforms the state-of-the-art studies on the task of learning low-level
concepts, and it is competitive in learning higher level concepts as well. It
is capable to work at large scale with no supervision through exploiting the
available sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4398</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4398</id><created>2013-12-12</created><authors><author><keyname>Chhabra</keyname><forenames>Shantanu</forenames></author></authors><title>On the number of proper $k$-colorings in an $n$-gon</title><categories>cs.DM</categories><comments>8 pages, 2 tables</comments><msc-class>05-XX, 11-XX, 68-XX, 90-XX</msc-class><acm-class>F.2; G.2; G.4; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define an $n$-gon to be any convex polygon with $n$ vertices. Let $V$
represent the set of vertices of the polygon. A proper $k$-coloring refers to a
function, $f$ : $V$ $\rightarrow$ $\{1, 2, 3, ... k\}$, such that for any two
vertices $u$ and $v$, if $f(u) = f(v)$, $u$ is not adjacent to $v$. The purpose
of this paper is to develop a recursive algorithm to compute the number of
proper $k$-colorings in an $n$-gon. The proposed algorithm can easily be solved
to obtain the explicit expression. This matches the explicit expression
obtained from the popular conventional solutions, which confirms the
correctness of the proposed algorithm. Often, for huge values of $n$ and $k$,
it becomes impractical to display the output numbers, which would consist of
thousands of digits. We report the answer modulo a certain number. In such
situations, the proposed algorithm is observed to run slightly faster than the
conventional recursive algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4400</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4400</id><created>2013-12-16</created><updated>2014-03-04</updated><authors><author><keyname>Lin</keyname><forenames>Min</forenames></author><author><keyname>Chen</keyname><forenames>Qiang</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Network In Network</title><categories>cs.NE cs.CV cs.LG</categories><comments>10 pages, 4 figures, for iclr2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel deep network structure called &quot;Network In Network&quot; (NIN)
to enhance model discriminability for local patches within the receptive field.
The conventional convolutional layer uses linear filters followed by a
nonlinear activation function to scan the input. Instead, we build micro neural
networks with more complex structures to abstract the data within the receptive
field. We instantiate the micro neural network with a multilayer perceptron,
which is a potent function approximator. The feature maps are obtained by
sliding the micro networks over the input in a similar manner as CNN; they are
then fed into the next layer. Deep NIN can be implemented by stacking mutiple
of the above described structure. With enhanced local modeling via the micro
network, we are able to utilize global average pooling over feature maps in the
classification layer, which is easier to interpret and less prone to
overfitting than traditional fully connected layers. We demonstrated the
state-of-the-art classification performances with NIN on CIFAR-10 and
CIFAR-100, and reasonable performances on SVHN and MNIST datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4405</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4405</id><created>2013-12-16</created><authors><author><keyname>Zhang</keyname><forenames>Xiao-Lei</forenames></author></authors><title>Learning Deep Representations By Distributed Random Samplings</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an extremely simple deep model for the unsupervised
nonlinear dimensionality reduction -- deep distributed random samplings, which
performs like a stack of unsupervised bootstrap aggregating. First, its network
structure is novel: each layer of the network is a group of mutually
independent $k$-centers clusterings. Second, its learning method is extremely
simple: the $k$ centers of each clustering are only $k$ randomly selected
examples from the training data; for small-scale data sets, the $k$ centers are
further randomly reconstructed by a simple cyclic-shift operation. Experimental
results on nonlinear dimensionality reduction show that the proposed method can
learn abstract representations on both large-scale and small-scale problems,
and meanwhile is much faster than deep neural networks on large-scale problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4410</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4410</id><created>2013-12-09</created><updated>2013-12-25</updated><authors><author><keyname>Yoon</keyname><forenames>Changseok</forenames></author><author><keyname>Nam</keyname><forenames>Sung Sik</forenames></author><author><keyname>Cho</keyname><forenames>Sung Ho</forenames></author></authors><title>Technical Report: A New Multi-Device Wireless Power Transfer Scheme
  Using an Intermediate Energy Storage Circuit</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new multi-device wireless power transfer scheme that reduces the overall
charging time is presented. The proposed scheme employs the intermediated
energy storage (IES) circuit which consists of a constant power driving circuit
and a super-capacitor. By utilizing the characteristic of high power density of
the super-capacitor, the receiver can receive and store the energy in short
duration and supply to the battery for long time. This enables the overlap of
charging duration between all receivers. As a result, the overall charging time
can be reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4413</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4413</id><created>2013-12-16</created><authors><author><keyname>Alstrup</keyname><forenames>Stephen</forenames></author><author><keyname>Halvorsen</keyname><forenames>Esben Bistrup</forenames></author><author><keyname>Larsen</keyname><forenames>Kasper Green</forenames></author></authors><title>Near-optimal labeling schemes for nearest common ancestors</title><categories>cs.DS</categories><acm-class>E.1; G.2.2; E.4</acm-class><doi>10.1137/1.9781611973402.72</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider NCA labeling schemes: given a rooted tree $T$, label the nodes of
$T$ with binary strings such that, given the labels of any two nodes, one can
determine, by looking only at the labels, the label of their nearest common
ancestor.
  For trees with $n$ nodes we present upper and lower bounds establishing that
labels of size $(2\pm \epsilon)\log n$, $\epsilon&lt;1$ are both sufficient and
necessary. (All logarithms in this paper are in base 2.)
  Alstrup, Bille, and Rauhe (SIDMA'05) showed that ancestor and NCA labeling
schemes have labels of size $\log n +\Omega(\log \log n)$. Our lower bound
increases this to $\log n + \Omega(\log n)$ for NCA labeling schemes. Since
Fraigniaud and Korman (STOC'10) established that labels in ancestor labeling
schemes have size $\log n +\Theta(\log \log n)$, our new lower bound separates
ancestor and NCA labeling schemes. Our upper bound improves the $10 \log n$
upper bound by Alstrup, Gavoille, Kaplan and Rauhe (TOCS'04), and our
theoretical result even outperforms some recent experimental studies by Fischer
(ESA'09) where variants of the same NCA labeling scheme are shown to all have
labels of size approximately $8 \log n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4414</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4414</id><created>2013-12-16</created><authors><author><keyname>Ivanov</keyname><forenames>Sergiu</forenames></author><author><keyname>Pelz</keyname><forenames>Elisabeth</forenames></author><author><keyname>Verlan</keyname><forenames>Sergey</forenames></author></authors><title>Small Universal Petri Nets with Inhibitor Arcs</title><categories>cs.FL cs.CC cs.DM</categories><msc-class>68Q05, 68Q10, 68Q17</msc-class><acm-class>F.4.3; F.1.1; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of construction of small-size universal Petri nets
with inhibitor arcs. We consider four descriptional complexity parameters: the
number of places, transitions, inhibitor arcs, and the maximal degree of a
transition, each of which we try to minimize.
  We give six constructions having the following values of parameters (listed
in the above order): $(30,34,13,3)$, $(14, 31, 51, 8)$, $(11, 31, 79, 11)$,
$(21,25,13,5)$, $(67, 64, 8, 3)$, $(58, 55, 8, 5)$ that improve the few known
results on this topic. Our investigation also highlights several interesting
trade-offs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4415</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4415</id><created>2013-12-16</created><authors><author><keyname>Towfic</keyname><forenames>Zaid J.</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Adaptive Penalty-Based Distributed Stochastic Convex Optimization</title><categories>math.OC cs.DC cs.MA</categories><comments>13 pages, 1 figure</comments><doi>10.1109/TSP.2014.2331615</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the task of distributed optimization over a network of
learners in which each learner possesses a convex cost function, a set of
affine equality constraints, and a set of convex inequality constraints. We
propose a fully-distributed adaptive diffusion algorithm based on penalty
methods that allows the network to cooperatively optimize the global cost
function, which is defined as the sum of the individual costs over the network,
subject to all constraints. We show that when small constant step-sizes are
employed, the expected distance between the optimal solution vector and that
obtained at each node in the network can be made arbitrarily small. Two
distinguishing features of the proposed solution relative to other related
approaches is that the developed strategy does not require the use of
projections and is able to adapt to and track drifts in the location of the
minimizer due to changes in the constraints or in the aggregate cost itself.
The proposed strategy is also able to cope with changing network topology, is
robust to network disruptions, and does not require global information or rely
on central processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4422</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4422</id><created>2013-12-16</created><authors><author><keyname>Gale</keyname><forenames>Ella M.</forenames></author><author><keyname>Costello</keyname><forenames>Benjamin de Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Which Memristor Theory is Best for Relating Devices Properties to
  Memristive Function?</title><categories>cond-mat.mtrl-sci cond-mat.mes-hall cs.ET physics.chem-ph</categories><comments>32 pages, 18 figures</comments><acm-class>B.3.1; B.7.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are three theoretical models which purport to relate
experimentally-measurable or fabrication-controllable device properties to the
memristor's operation: 1. Strukov et al's phenomenological model; 2. Georgiou
et al's Bernoulli rewrite of that phenomenological model; 3. Gale's
memory-conservation model. They differ in their prediction of the effect on
memristance of changing the electrode size and factors that affect the
hysteresis. Using a batch of TiO$_2$ sol-gel memristors fabricated with
different top electrode widths we test and compare these three theories. It was
found that, contrary to model 2's prediction, the `dimensionless lumped
parameter', $\beta$, did not correlate to any measure of the hysteresis.
Contrary to model 1, memristance was found to be dependent on the three spatial
dimensions of the TiO$_2$ layer, as was predicted by model 3. Model 3 was found
to fit the change in resistance value with electrode size. Simulations using
model 3 and experimentally derived values for contact resistance gave
hysteresis values that were linearly related to (and only one order of
magnitude out) from the experimentally-measured values. Memristor hysteresis
was found to be related to the ON state resistance and thus the electrode size
(as those two are related). These results offer a verification of the
memory-conservation theory of memristance and its association of the vacancy
magnetic flux with the missing magnetic flux in memristor theory. This is the
first paper to experimentally test various theories pertaining to the operation
of memristor devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4423</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4423</id><created>2013-12-16</created><authors><author><keyname>Song</keyname><forenames>Changick</forenames></author><author><keyname>Ling</keyname><forenames>Cong</forenames></author></authors><title>Achievable Diversity-Rate Tradeoff of MIMO AF Relaying Systems with MMSE
  Transceivers</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the diversity order of the minimum mean squared error
(MMSE) based optimal transceivers in multiple-input multiple-output (MIMO)
amplify-and-forward (AF) relaying systems. While the diversity-multiplexing
tradeoff (DMT) analysis accurately predicts the behavior of the MMSE receiver
for the positive multiplexing gain, it turned out that the performance is very
unpredictable via DMT for the case of fixed rates, because MMSE strategies
exhibit a complicated rate dependent behavior. In this paper, we establish the
diversity-rate tradeoff performance of MIMO AF relaying systems with the MMSE
transceivers as a closed-form for all fixed rates, thereby providing a complete
characterization of the diversity order together with the earlier work on DMT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4425</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4425</id><created>2013-12-16</created><authors><author><keyname>G&#xf6;dert</keyname><forenames>Winfried</forenames></author></authors><title>An Ontology-based Model for Indexing and Retrieval</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Starting from an unsolved problem of information retrieval this paper
presents an ontology-based model for indexing and retrieval. The model combines
the methods and experiences of cognitive-to-interpret indexing languages with
the strengths and possibilities of formal knowledge representation. The core
component of the model uses inferences along the paths of typed relations
between the entities of a knowledge representation for enabling the
determination of hit quantities in the context of retrieval processes. The
entities are arranged in aspect-oriented facets to ensure a consistent
hierarchical structure. The possible consequences for indexing and retrieval
are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4426</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4426</id><created>2013-12-16</created><authors><author><keyname>Vanderbei</keyname><forenames>Robert</forenames></author><author><keyname>Liu</keyname><forenames>Han</forenames></author><author><keyname>Wang</keyname><forenames>Lie</forenames></author><author><keyname>Lin</keyname><forenames>Kevin</forenames></author></authors><title>Optimization for Compressed Sensing: the Simplex Method and Kronecker
  Sparsification</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present two new approaches to efficiently solve large-scale
compressed sensing problems. These two ideas are independent of each other and
can therefore be used either separately or together. We consider all
possibilities.
  For the first approach, we note that the zero vector can be taken as the
initial basic (infeasible) solution for the linear programming problem and
therefore, if the true signal is very sparse, some variants of the simplex
method can be expected to take only a small number of pivots to arrive at a
solution. We implemented one such variant and demonstrate a dramatic
improvement in computation time on very sparse signals.
  The second approach requires a redesigned sensing mechanism in which the
vector signal is stacked into a matrix. This allows us to exploit the Kronecker
compressed sensing (KCS) mechanism. We show that the Kronecker sensing requires
stronger conditions for perfect recovery compared to the original vector
problem. However, the Kronecker sensing, modeled correctly, is a much sparser
linear optimization problem. Hence, algorithms that benefit from sparse problem
representation, such as interior-point methods, can solve the Kronecker sensing
problems much faster than the corresponding vector problem. In our numerical
studies, we demonstrate a ten-fold improvement in the computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4428</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4428</id><created>2013-12-16</created><updated>2013-12-17</updated><authors><author><keyname>Egri</keyname><forenames>Laszlo</forenames></author></authors><title>On Constraint Satisfaction Problems below P</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symmetric Datalog, a fragment of the logic programming language Datalog, is
conjectured to capture all constraint satisfaction problems (CSP) in L.
Therefore developing tools that help us understand whether or not a CSP can be
defined in symmetric Datalog is an important task. It is widely known that a
CSP is definable in Datalog and linear Datalog if and only if that CSP has
bounded treewidth and bounded pathwidth duality, respectively. In the case of
symmetric Datalog, Bulatov, Krokhin and Larose ask for such a duality (2008).
We provide two such dualities, and give applications. In particular, we give a
short and simple new proof of the result of Dalmau and Larose that &quot;Maltsev +
Datalog -&gt; symmetric Datalog&quot; (2008).
  In the second part of the paper, we provide some evidence for the conjecture
of Dalmau (2002) that every CSP in NL is definable in linear Datalog. Our
results also show that a wide class of CSPs-CSPs which do not have bounded
pathwidth duality (e.g., the P-complete Horn-3Sat problem)-cannot be defined by
any polynomial size family of monotone read-once nondeterministic branching
programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4429</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4429</id><created>2013-12-16</created><updated>2016-01-29</updated><authors><author><keyname>Ackerman</keyname><forenames>Eyal</forenames></author><author><keyname>Allen</keyname><forenames>Michelle M.</forenames></author><author><keyname>Barequet</keyname><forenames>Gill</forenames></author><author><keyname>L&#xf6;ffler</keyname><forenames>Maarten</forenames></author><author><keyname>Mermelstein</keyname><forenames>Joshua</forenames></author><author><keyname>Souvaine</keyname><forenames>Diane L.</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>Csaba D.</forenames></author></authors><title>The Flip Diameter of Rectangulations and Convex Subdivisions</title><categories>cs.DM cs.CG math.CO</categories><comments>17 pages, 12 figures, an extended abstract has been presented at
  LATIN 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the configuration space of rectangulations and convex subdivisions
of $n$ points in the plane. It is shown that a sequence of $O(n\log n)$
elementary flip and rotate operations can transform any rectangulation to any
other rectangulation on the same set of $n$ points. This bound is the best
possible for some point sets, while $\Theta(n)$ operations are sufficient and
necessary for others. Some of our bounds generalize to convex subdivisions of
$n$ points in the plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4454</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4454</id><created>2013-12-16</created><updated>2013-12-19</updated><authors><author><keyname>Komendantskaya</keyname><forenames>Ekaterina</forenames></author><author><keyname>Schmidt</keyname><forenames>Martin</forenames></author><author><keyname>Heras</keyname><forenames>J&#xf3;nathan</forenames></author></authors><title>Exploiting Parallelism in Coalgebraic Logic Programming</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a parallel implementation of Coalgebraic Logic Programming (CoALP)
in the programming language Go. CoALP was initially introduced to reflect
coalgebraic semantics of logic programming, with coalgebraic derivation
algorithm featuring both corecursion and parallelism. Here, we discuss how the
coalgebraic semantics influenced our parallel implementation of logic
programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4461</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4461</id><created>2013-12-16</created><updated>2014-01-28</updated><authors><author><keyname>Davis</keyname><forenames>Andrew</forenames></author><author><keyname>Arel</keyname><forenames>Itamar</forenames></author></authors><title>Low-Rank Approximations for Conditional Feedforward Computation in Deep
  Neural Networks</title><categories>cs.LG</categories><comments>10 pages, 5 figures. Submitted to ICLR 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scalability properties of deep neural networks raise key research questions,
particularly as the problems considered become larger and more challenging.
This paper expands on the idea of conditional computation introduced by Bengio,
et. al., where the nodes of a deep network are augmented by a set of gating
units that determine when a node should be calculated. By factorizing the
weight matrix into a low-rank approximation, an estimation of the sign of the
pre-nonlinearity activation can be efficiently obtained. For networks using
rectified-linear hidden units, this implies that the computation of a hidden
unit with an estimated negative pre-nonlinearity can be ommitted altogether, as
its value will become zero when nonlinearity is applied. For sparse neural
networks, this can result in considerable speed gains. Experimental results
using the MNIST and SVHN data sets with a fully-connected deep neural network
demonstrate the performance robustness of the proposed scheme with respect to
the error introduced by the conditional computation process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4468</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4468</id><created>2013-12-16</created><authors><author><keyname>Alsan</keyname><forenames>Mine</forenames></author></authors><title>Extremality for Gallager's Reliability Function $E_0$</title><categories>cs.IT math.IT</categories><comments>39 pages, submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe certain extremalities for Gallager's $E_0$ function evaluated
under the uniform input distribution for binary input discrete memoryless
channels. The results characterize the extremality of the $E_0(\rho)$ curves of
the binary erasure channel and the binary symmetric channel among all the
$E_0(\rho)$ curves that can be generated by the class of binary discrete
memoryless channels whose $E_0(\rho)$ curves pass through a given point
$(\rho_0, e_0)$, for some $\rho_0 &gt; -1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4476</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4476</id><created>2013-12-16</created><updated>2014-04-01</updated><authors><author><keyname>Kaczmirek</keyname><forenames>Lars</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author><author><keyname>Vatrapu</keyname><forenames>Ravi</forenames></author><author><keyname>Bleier</keyname><forenames>Arnim</forenames></author><author><keyname>Blumenberg</keyname><forenames>Manuela</forenames></author><author><keyname>Gummer</keyname><forenames>Tobias</forenames></author><author><keyname>Hussain</keyname><forenames>Abid</forenames></author><author><keyname>Kinder-Kurlanda</keyname><forenames>Katharina</forenames></author><author><keyname>Manshaei</keyname><forenames>Kaveh</forenames></author><author><keyname>Thamm</keyname><forenames>Mark</forenames></author><author><keyname>Weller</keyname><forenames>Katrin</forenames></author><author><keyname>Wenz</keyname><forenames>Alexander</forenames></author><author><keyname>Wolf</keyname><forenames>Christof</forenames></author></authors><title>Social Media Monitoring of the Campaigns for the 2013 German Bundestag
  Elections on Facebook and Twitter</title><categories>cs.SI cs.CY</categories><comments>29 pages, 2 figures, GESIS-Working Papers No. 31</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As more and more people use social media to communicate their view and
perception of elections, researchers have increasingly been collecting and
analyzing data from social media platforms. Our research focuses on social
media communication related to the 2013 election of the German parlia-ment
[translation: Bundestagswahl 2013]. We constructed several social media
datasets using data from Facebook and Twitter. First, we identified the most
relevant candidates (n=2,346) and checked whether they maintained social media
accounts. The Facebook data was collected in November 2013 for the period of
January 2009 to October 2013. On Facebook we identified 1,408 Facebook walls
containing approximately 469,000 posts. Twitter data was collected between June
and December 2013 finishing with the constitution of the government. On Twitter
we identified 1,009 candidates and 76 other agents, for example, journalists.
We estimated the number of relevant tweets to exceed eight million for the
period from July 27 to September 27 alone. In this document we summarize past
research in the literature, discuss possibilities for research with our data
set, explain the data collection procedures, and provide a description of the
data and a discussion of issues for archiving and dissemination of social media
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4477</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4477</id><created>2013-12-13</created><authors><author><keyname>Al-Naymat</keyname><forenames>Ghazi</forenames></author></authors><title>GCG: Mining Maximal Complete Graph Patterns from Large Spatial Data</title><categories>cs.DB</categories><comments>11</comments><journal-ref>International Conference on Computer Systems and Applications
  (AICCSA), pp.1,8. Fes, Morocco.27-30 May 2013</journal-ref><doi>10.1109/AICCSA.2013.6616417</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recent research on pattern discovery has progressed from mining frequent
patterns and sequences to mining structured patterns, such as trees and graphs.
Graphs as general data structure can model complex relations among data with
wide applications in web exploration and social networks. However, the process
of mining large graph patterns is a challenge due to the existence of large
number of subgraphs. In this paper, we aim to mine only frequent complete graph
patterns. A graph g in a database is complete if every pair of distinct
vertices is connected by a unique edge. Grid Complete Graph (GCG) is a mining
algorithm developed to explore interesting pruning techniques to extract
maximal complete graphs from large spatial dataset existing in Sloan Digital
Sky Survey (SDSS) data. Using a divide and conquer strategy, GCG shows high
efficiency especially in the presence of large number of patterns. In this
paper, we describe GCG that can mine not only simple co-location spatial
patterns but also complex ones. To the best of our knowledge, this is the first
algorithm used to exploit the extraction of maximal complete graphs in the
process of mining complex co-location patterns in large spatial dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4479</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4479</id><created>2013-12-16</created><authors><author><keyname>Fernique</keyname><forenames>Pierre</forenames><affiliation>VP, AGAP</affiliation></author><author><keyname>Durand</keyname><forenames>Jean-Baptiste</forenames><affiliation>VP, INRIA Grenoble Rh&#xf4;ne-Alpes / LJK Laboratoire Jean Kuntzmann</affiliation></author><author><keyname>Gu&#xe9;don</keyname><forenames>Yann</forenames><affiliation>VP, AGAP</affiliation></author></authors><title>Parametric Modelling of Multivariate Count Data Using Probabilistic
  Graphical Models</title><categories>stat.ML cs.LG stat.ME</categories><proxy>ccsd</proxy><journal-ref>3rd Workshop on Algorithmic issues for Inference in Graphical
  Models - AIGM13, Paris : France (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multivariate count data are defined as the number of items of different
categories issued from sampling within a population, which individuals are
grouped into categories. The analysis of multivariate count data is a recurrent
and crucial issue in numerous modelling problems, particularly in the fields of
biology and ecology (where the data can represent, for example, children counts
associated with multitype branching processes), sociology and econometrics. We
focus on I) Identifying categories that appear simultaneously, or on the
contrary that are mutually exclusive. This is achieved by identifying
conditional independence relationships between the variables; II)Building
parsimonious parametric models consistent with these relationships; III)
Characterising and testing the effects of covariates on the joint distribution
of the counts. To achieve these goals, we propose an approach based on
graphical probabilistic models, and more specifically partially directed
acyclic graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4490</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4490</id><created>2013-12-16</created><authors><author><keyname>Dinkla</keyname><forenames>Kasper</forenames></author><author><keyname>El-Kebir</keyname><forenames>Mohammed</forenames></author><author><keyname>Bucur</keyname><forenames>Cristina-Iulia</forenames></author><author><keyname>Siderius</keyname><forenames>Marco</forenames></author><author><keyname>Smit</keyname><forenames>Martine J.</forenames></author><author><keyname>Westenberg</keyname><forenames>Michel A.</forenames></author><author><keyname>Klau</keyname><forenames>Gunnar W.</forenames></author></authors><title>eXamine: a Cytoscape app for exploring annotated modules in networks</title><categories>q-bio.MN cs.CE cs.DS</categories><msc-class>76M27, 92C42, 62P10, 92C40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background. Biological networks have growing importance for the
interpretation of high-throughput &quot;omics&quot; data. Statistical and combinatorial
methods allow to obtain mechanistic insights through the extraction of smaller
subnetwork modules. Further enrichment analyses provide set-based annotations
of these modules.
  Results. We present eXamine, a set-oriented visual analysis approach for
annotated modules that displays set membership as contours on top of a
node-link layout. Our approach extends upon Self Organizing Maps to
simultaneously lay out nodes, links, and set contours.
  Conclusions. We implemented eXamine as a freely available Cytoscape app.
Using eXamine we study a module that is activated by the virally-encoded
G-protein coupled receptor US28 and formulate a novel hypothesis about its
functioning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4494</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4494</id><created>2013-12-16</created><updated>2016-01-07</updated><authors><author><keyname>Anantharam</keyname><forenames>Venkat</forenames></author><author><keyname>Salez</keyname><forenames>Justin</forenames></author></authors><title>The densest subgraph problem in sparse random graphs</title><categories>math.PR cs.DM math.CO</categories><comments>Published at http://dx.doi.org/10.1214/14-AAP1091 in the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP1091</report-no><journal-ref>Annals of Applied Probability 2016, Vol. 26, No. 1, 305-327</journal-ref><doi>10.1214/14-AAP1091</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We determine the asymptotic behavior of the maximum subgraph density of large
random graphs with a prescribed degree sequence. The result applies in
particular to the Erd\H{o}s-R\'{e}nyi model, where it settles a conjecture of
Hajek [IEEE Trans. Inform. Theory 36 (1990) 1398-1414]. Our proof consists in
extending the notion of balanced loads from finite graphs to their local weak
limits, using unimodularity. This is a new illustration of the objective method
described by Aldous and Steele [In Probability on Discrete Structures (2004)
1-72 Springer].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4507</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4507</id><created>2013-12-16</created><authors><author><keyname>D&#xed;az-Caro</keyname><forenames>Alejandro</forenames><affiliation>LIPN</affiliation></author><author><keyname>Manzonetto</keyname><forenames>Giulio</forenames><affiliation>LIPN</affiliation></author><author><keyname>Pagani</keyname><forenames>Michele</forenames><affiliation>LIPN</affiliation></author></authors><title>Call-by-value non-determinism in a linear logic type discipline</title><categories>cs.LO</categories><proxy>ccsd</proxy><journal-ref>Logical Foundations of Computer Science 7734 (2013) 164-178</journal-ref><doi>10.1007/978-3-642-35722-0_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the call-by-value lambda-calculus extended with a may-convergent
non-deterministic choice and a must-convergent parallel composition. Inspired
by recent works on the relational semantics of linear logic and non-idempotent
intersection types, we endow this calculus with a type system based on the
so-called Girard's second translation of intuitionistic logic into linear
logic. We prove that a term is typable if and only if it is converging, and
that its typing tree carries enough information to give a bound on the length
of its lazy call-by-value reduction. Moreover, when the typing tree is minimal,
such a bound becomes the exact length of the reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4508</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4508</id><created>2013-12-16</created><authors><author><keyname>Paillard</keyname><forenames>Gabriel</forenames><affiliation>LIPN</affiliation></author><author><keyname>Lavault</keyname><forenames>Christian</forenames><affiliation>LIPN</affiliation></author><author><keyname>Franca</keyname><forenames>Felipe</forenames><affiliation>PESC</affiliation></author></authors><title>A distributed prime sieving algorithm based on Scheduling by Multiple
  Edge Reversal</title><categories>cs.DC cs.DM cs.DS math.CO</categories><comments>11 pages. Special issue : Selected papers from the ISPDC'05
  Conference (4th International Symposium on Parallel and Distributed
  Computing); 4th International Symposium on Parallel and Distributed
  Computing, Lille : France (2005)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new distributed approach for generating all prime
numbers in a given interval of integers. From Eratosthenes, who elaborated the
first prime sieve (more than 2000 years ago), to the current generation of
parallel computers, which have permitted to reach larger bounds on the interval
or to obtain previous results in a shorter time, prime numbers generation still
represents an attractive domain of research and plays a central role in
cryptography. We propose a fully distributed algorithm for finding all primes
in the interval $[2\ldots, n]$, based on the \emph{wheel sieve} and the SMER
(\emph{Scheduling by Multiple Edge Reversal}) multigraph dynamics. Given a
multigraph $\mathcal{M}$ of arbitrary topology, having $N$ nodes, a SMER-driven
system is defined by the number of directed edges (arcs) between any two nodes
of $\mathcal{M}$, and by the global period length of all &quot;arc reversals&quot; in
$\mathcal{M}$. The new prime number generation method inherits the distributed
and parallel nature of SMER and requires at most $n + \lfloor \sqrt{n}\rfloor$
time steps. The message complexity achieves at most $n\Delta_N + \lfloor
\sqrt{n}\rfloor \Delta_N$, where $1\le \Delta_N\le N - 1$ is the maximal
multidegree of $\mathcal{M}$, and the maximal amount of memory space required
per process is $\mathcal{O}(n)$ bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4509</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4509</id><created>2013-12-16</created><authors><author><keyname>Zaourar</keyname><forenames>Lilia</forenames><affiliation>LIST</affiliation></author><author><keyname>Jan</keyname><forenames>Mathieu</forenames><affiliation>LIST</affiliation></author><author><keyname>Pitel</keyname><forenames>Maurice</forenames></author></authors><title>Cache-aware static scheduling for hard real-time multicore systems based
  on communication affinities</title><categories>cs.OS</categories><proxy>ccsd</proxy><journal-ref>34th IEEE Real-Time Systems Symposium (RTSS'13), WiP session,
  Vancouver : Canada (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing need for continuous processing capabilities has led to the
development of multicore systems with a complex cache hierarchy. Such multicore
systems are generally designed for improving the performance in average case,
while hard real-time systems must consider worst-case scenarios. An open
challenge is therefore to efficiently schedule hard real-time tasks on a
multicore architecture. In this work, we propose a mathematical formulation for
computing a static scheduling that minimize L1 data cache misses between hard
real-time tasks on a multicore architecture using communication affinities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4510</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4510</id><created>2013-12-16</created><updated>2014-03-06</updated><authors><author><keyname>Bassino</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames><affiliation>LIPN</affiliation></author><author><keyname>Nicaud</keyname><forenames>Cyril</forenames><affiliation>LIGM</affiliation></author><author><keyname>Weil</keyname><forenames>Pascal</forenames><affiliation>LaBRI</affiliation></author></authors><title>On the genericity of Whitehead minimality</title><categories>math.GR cs.CC math.CO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a finitely generated subgroup of a free group, chosen uniformly
at random, is strictly Whitehead minimal with overwhelming probability.
Whitehead minimality is one of the key elements of the solution of the orbit
problem in free groups. The proofs strongly rely on combinatorial tools,
notably those of analytic combinatorics. The result we prove actually depends
implicitly on the choice of a distribution on finitely generated subgroups, and
we establish it for the two distributions which appear in the literature on
random subgroups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4511</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4511</id><created>2013-12-16</created><updated>2016-02-22</updated><authors><author><keyname>Abisheva</keyname><forenames>Adiya</forenames></author><author><keyname>Garimella</keyname><forenames>Venkata Rama Kiran</forenames></author><author><keyname>Garcia</keyname><forenames>David</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author></authors><title>Who Watches (and Shares) What on YouTube? And When? Using Twitter to
  Understand YouTube Viewership</title><categories>cs.SI physics.soc-ph</categories><comments>12 pages, 8 figures and 10 tables</comments><report-no>Report-no: ETH-2013-YouTube-Twitter-ETH-QCRI</report-no><journal-ref>Proceedings of the 7th International ACM Conference on Web Science
  and Data Mining, pp.593-602 (2014)</journal-ref><doi>10.1145/2556195.2566588</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We combine user-centric Twitter data with video-centric YouTube data to
analyze who watches and shares what on YouTube. Combination of two data sets,
with 87k Twitter users, 5.6mln YouTube videos and 15mln video sharing events,
allows rich analysis going beyond what could be obtained with either of the two
data sets individually. For Twitter, we generate user features relating to
activity, interests and demographics. For YouTube, we obtain video features for
topic, popularity and polarization. These two feature sets are combined through
sharing events for YouTube URLs on Twitter. This combination is done both in a
user-, a video- and a sharing-event-centric manner. For the user-centric
analysis, we show how Twitter user features correlate both with YouTube
features and with sharing-related features. As two examples, we show urban
users are quicker to share than rural users and for some notions of &quot;influence&quot;
influential users on Twitter share videos with a higher number of views. For
the video-centric analysis, we find a superlinear relation between initial
Twitter shares and the final amounts of views, showing the correlated behavior
of Twitter. On user impact, we find the total amount of followers of users that
shared the video in the first week does not affect its final popularity.
However, aggregated user retweet rates serve as a better predictor for YouTube
video popularity. For the sharing-centric analysis, we reveal existence of
correlated behavior concerning the time between video creation and sharing
within certain timescales, showing the time onset for a coherent response, and
the time limit after which collective responses are extremely unlikely. We show
that response times depend on video category, revealing that Twitter sharing of
a video is highly dependent on its content. To the best of our knowledge this
is the first large-scale study combining YouTube and Twitter data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4521</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4521</id><created>2013-11-24</created><authors><author><keyname>Cvetkovic</keyname><forenames>Zoran</forenames></author><author><keyname>Sinn</keyname><forenames>Vincent</forenames></author></authors><title>Weyl-Heisenberg Spaces for Robust Orthogonal Frequency Division
  Multiplexing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Design of Weyl-Heisenberg sets of waveforms for robust orthogonal frequency
division multiplex- ing (OFDM) has been the subject of a considerable volume of
work. In this paper, a complete parameterization of orthogonal Weyl-Heisenberg
sets and their corresponding biorthogonal sets is given. Several examples of
Weyl-Heisenberg sets designed using this parameterization are pre- sented,
which in simulations show a high potential for enabling OFDM robust to
frequency offset, timing mismatch, and narrow-band interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4524</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4524</id><created>2013-12-16</created><updated>2015-10-25</updated><authors><author><keyname>Schwerdtfeger</keyname><forenames>Konrad W.</forenames></author></authors><title>A Computational Trichotomy for Connectivity of Boolean Satisfiability</title><categories>cs.CC cs.LO</categories><comments>27 pages; severe error in the proof of Lemma 19 (now Lemma 23)
  corrected; all results remain true, but some new definitions and lemmas were
  necessary; also, a further error of Gopalan et al.'s paper is explained and
  corrected; several other improvements. Text overlap with arXiv:cs/0609072 due
  to corrections of that paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For Boolean satisfiability problems, the structure of the solution space is
characterized by the solution graph, where the vertices are the solutions, and
two solutions are connected iff they differ in exactly one variable. In 2006,
Gopalan et al. studied connectivity properties of the solution graph and
related complexity issues for CSPs, motivated mainly by research on
satisfiability algorithms and the satisfiability threshold. They proved
dichotomies for the diameter of connected components and for the complexity of
the st-connectivity question, and conjectured a trichotomy for the connectivity
question.
  Building on this work, we here prove the trichotomy: Connectivity is either
in P, coNP-complete, or PSPACE-complete. Also, we correct a minor mistake of
Gopalan et al., which leads to a slight shift of the boundaries towards the
hard side.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4527</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4527</id><created>2013-12-16</created><authors><author><keyname>Than</keyname><forenames>Khoat</forenames></author><author><keyname>Ho</keyname><forenames>Tu Bao</forenames></author></authors><title>Probable convexity and its application to Correlated Topic Models</title><categories>cs.LG stat.ML</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-convex optimization problems often arise from probabilistic modeling,
such as estimation of posterior distributions. Non-convexity makes the problems
intractable, and poses various obstacles for us to design efficient algorithms.
In this work, we attack non-convexity by first introducing the concept of
\emph{probable convexity} for analyzing convexity of real functions in
practice. We then use the new concept to analyze an inference problem in the
\emph{Correlated Topic Model} (CTM) and related nonconjugate models. Contrary
to the existing belief of intractability, we show that this inference problem
is concave under certain conditions. One consequence of our analyses is a novel
algorithm for learning CTM which is significantly more scalable and qualitative
than existing methods. Finally, we highlight that stochastic gradient
algorithms might be a practical choice to resolve efficiently non-convex
problems. This finding might find beneficial in many contexts which are beyond
probabilistic modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4551</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4551</id><created>2013-12-16</created><authors><author><keyname>Allahverdyan</keyname><forenames>Armen E.</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>Comparative Analysis of Viterbi Training and Maximum Likelihood
  Estimation for HMMs</title><categories>stat.ML cs.LG</categories><comments>Appeared in Neural Information Processing Systems (NIPS) 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an asymptotic analysis of Viterbi Training (VT) and contrast it
with a more conventional Maximum Likelihood (ML) approach to parameter
estimation in Hidden Markov Models. While ML estimator works by (locally)
maximizing the likelihood of the observed data, VT seeks to maximize the
probability of the most likely hidden state sequence. We develop an analytical
framework based on a generating function formalism and illustrate it on an
exactly solvable model of HMM with one unambiguous symbol. For this particular
model the ML objective function is continuously degenerate. VT objective, in
contrast, is shown to have only finite degeneracy. Furthermore, VT converges
faster and results in sparser (simpler) models, thus realizing an automatic
Occam's razor for HMM learning. For more general scenario VT can be worse
compared to ML but still capable of correctly recovering most of the
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4552</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4552</id><created>2013-11-25</created><updated>2013-12-22</updated><authors><author><keyname>Zohaib</keyname><forenames>Muhammad</forenames></author><author><keyname>Pasha</keyname><forenames>Syed Mustafa</forenames></author><author><keyname>Javaid</keyname><forenames>Nadeem</forenames></author><author><keyname>Iqbal</keyname><forenames>Jamshed</forenames></author></authors><title>Intelligent Bug Algorithm (IBA): A Novel Strategy to Navigate Mobile
  Robots Autonomously</title><categories>cs.RO</categories><comments>Springer's International Multi Topic Conference 2013 (IMTIC '13)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research proposed an intelligent obstacle avoidance algorithm to
navigate an autonomous mobile robot. The presented Intelligent Bug Algorithm
(IBA) over performs and reaches the goal in relatively less time as compared to
existing Bug algorithms. The improved algorithm offers a goal oriented strategy
by following smooth and short trajectory. This has been achieved by
continuously considering the goal position during obstacle avoidance. The
proposed algorithm is computationally inexpensive and easy to tune. The paper
also presents the performance comparison of IBA and reported Bug algorithms.
Simulation results of robot navigation in an environment with obstacles
demonstrate the performance of the improved algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4564</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4564</id><created>2013-12-16</created><updated>2014-06-09</updated><authors><author><keyname>Zhao</keyname><forenames>Peilin</forenames></author><author><keyname>Yang</keyname><forenames>Jinwei</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>Adaptive Stochastic Alternating Direction Method of Multipliers</title><categories>stat.ML cs.LG</categories><comments>13 pages</comments><acm-class>I.2.6; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Alternating Direction Method of Multipliers (ADMM) has been studied for
years. The traditional ADMM algorithm needs to compute, at each iteration, an
(empirical) expected loss function on all training examples, resulting in a
computational complexity proportional to the number of training examples. To
reduce the time complexity, stochastic ADMM algorithms were proposed to replace
the expected function with a random loss function associated with one uniformly
drawn example plus a Bregman divergence. The Bregman divergence, however, is
derived from a simple second order proximal function, the half squared norm,
which could be a suboptimal choice.
  In this paper, we present a new family of stochastic ADMM algorithms with
optimal second order proximal functions, which produce a new family of adaptive
subgradient methods. We theoretically prove that their regret bounds are as
good as the bounds which could be achieved by the best proximal function that
can be chosen in hindsight. Encouraging empirical results on a variety of
real-world datasets confirm the effectiveness and efficiency of the proposed
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4568</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4568</id><created>2013-11-02</created><authors><author><keyname>Seraj</keyname><forenames>Samer</forenames></author></authors><title>Dispersion and Diffusion</title><categories>cs.IT cs.CR math.IT</categories><msc-class>11T71, 05D99</msc-class><acm-class>G.2.1; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Shannon's landmark 1949 paper Communication Theory of Secrecy Systems, the
idea of diffusive functions is briefly mentioned. We consider two common
definitions of such functions mapping between binary strings of fixed length.
Given the dimension of the input space, we determine the minimum dimension of
the output space for which such a function exists, by explicit construction and
with respect to each definition. It will follow that each larger output
dimension allows for such a function as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4569</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4569</id><created>2013-11-05</created><updated>2014-03-10</updated><authors><author><keyname>Pham</keyname><forenames>Vu</forenames></author><author><keyname>Bluche</keyname><forenames>Th&#xe9;odore</forenames></author><author><keyname>Kermorvant</keyname><forenames>Christopher</forenames></author><author><keyname>Louradour</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author></authors><title>Dropout improves Recurrent Neural Networks for Handwriting Recognition</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent neural networks (RNNs) with Long Short-Term memory cells currently
hold the best known results in unconstrained handwriting recognition. We show
that their performance can be greatly improved using dropout - a recently
proposed regularization method for deep architectures. While previous works
showed that dropout gave superior performance in the context of convolutional
networks, it had never been applied to RNNs. In our approach, dropout is
carefully used in the network so that it does not affect the recurrent
connections, hence the power of RNNs in modeling sequence is preserved.
Extensive experiments on a broad range of handwritten databases confirm the
effectiveness of dropout on deep architectures even when the network mainly
consists of recurrent and shared connections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4575</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4575</id><created>2013-12-16</created><authors><author><keyname>Ferris</keyname><forenames>Andrew J.</forenames></author><author><keyname>Poulin</keyname><forenames>David</forenames></author></authors><title>Branching MERA codes: a natural extension of polar codes</title><categories>quant-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new class of circuits for constructing efficiently decodable
error-correction codes, based on a recently discovered contractible tensor
network. We perform an in-depth study of a particular example that can be
thought of as an extension to Arikan's polar code. Notably, our numerical
simulation show that this code polarizes the logical channels more strongly
while retaining the log-linear decoding complexity using the successive
cancellation decoder. These codes also display improved error-correcting
capability with only a minor impact on decoding complexity. Efficient decoding
is realized using powerful graphical calculus tools developed in the field of
quantum many-body physics. In a companion paper, we generalize our construction
to the quantum setting and describe more in-depth the relation between
classical and quantum error correction and the graphical calculus of tensor
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4578</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4578</id><created>2013-12-16</created><updated>2014-06-25</updated><authors><author><keyname>Ferris</keyname><forenames>Andrew J.</forenames></author><author><keyname>Poulin</keyname><forenames>David</forenames></author></authors><title>Tensor Networks and Quantum Error Correction</title><categories>quant-ph cs.IT math.IT</categories><comments>Accepted in Phys. Rev. Lett. 8 pages, 9 figures</comments><journal-ref>Phys. Rev. Lett. 113, 030501 (2014)</journal-ref><doi>10.1103/PhysRevLett.113.030501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish several relations between quantum error correction (QEC) and
tensor network (TN) methods of quantum many-body physics. We exhibit
correspondences between well-known families of QEC codes and TNs, and
demonstrate a formal equivalence between decoding a QEC code and contracting a
TN. We build on this equivalence to propose a new family of quantum codes and
decoding algorithms that generalize and improve upon quantum polar codes and
successive cancellation decoding in a natural way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4587</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4587</id><created>2013-12-16</created><authors><author><keyname>Lu</keyname><forenames>Jingwei</forenames></author><author><keyname>Chen</keyname><forenames>Pengwen</forenames></author><author><keyname>Chang</keyname><forenames>Chin-Chih</forenames></author><author><keyname>Sha</keyname><forenames>Lu</forenames></author><author><keyname>Huang</keyname><forenames>Dennis Jen-Hsin</forenames></author><author><keyname>Teng</keyname><forenames>Chin-Chi</forenames></author><author><keyname>Cheng</keyname><forenames>Chung-Kuan</forenames></author></authors><title>FFTPL: An Analytic Placement Algorithm Using Fast Fourier Transform for
  Density Equalization</title><categories>cs.CE cs.AR cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a flat nonlinear placement algorithm FFTPL using fast Fourier
transform for density equalization. The placement instance is modeled as an
electrostatic system with the analogy of density cost to the potential energy.
A well-defined Poisson's equation is proposed for gradient and cost
computation. Our placer outperforms state-of-the-art placers with better
solution quality and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4597</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4597</id><created>2013-12-16</created><updated>2015-03-12</updated><authors><author><keyname>Kov&#xe1;cs</keyname><forenames>Istv&#xe1;n</forenames></author></authors><title>Indecomposable coverings with homothetic polygons</title><categories>cs.CG math.CO math.MG</categories><msc-class>52C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for any convex polygon $S$ with at least four sides, or a
concave one with no parallel sides, and any $m&gt;0$, there is an $m$-fold
covering of the plane with homothetic copies of $S$ that cannot be decomposed
into two coverings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4598</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4598</id><created>2013-12-16</created><authors><author><keyname>Ishii</keyname><forenames>Tohru</forenames></author><author><keyname>Takahashi</keyname><forenames>Yasutake</forenames></author><author><keyname>Maeda</keyname><forenames>Yoichiro</forenames></author><author><keyname>Nakamura</keyname><forenames>Takayuki</forenames></author></authors><title>Tethered Flying Robot for Information Gathering System</title><categories>cs.SY</categories><comments>6 pages, 16 figures, ROSIN2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information from the sky is important for rescue activity in large-scale
disaster or dangerous areas. Observation system using a balloon or an airplane
has been studied as an information gathering system from the sky. A balloon
observation system needs helium gas and relatively long time to be ready. An
airplane observation system can be prepared in a short time and its mobility is
good. However, a long time flight is difficult because of limited amount of
fuel.
  This paper proposes a kite-based observation system that complements
activities of balloon and airplane observation systems by short preparation
time and long time flight. This research aims at construction of the autonomous
flight information gathering system using a tethered flying unit that consists
of the kite and the ground tether line control unit with a winding machine.
This paper reports development of the kite type tethered flying robot and an
autonomous flying control system inspired by how to fly a kite by a human.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4599</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4599</id><created>2013-12-16</created><authors><author><keyname>Bhattacharya</keyname><forenames>Arka</forenames></author></authors><title>Evolution and Computational Learning Theory: A survey on Valiant's paper</title><categories>cs.LG</categories><comments>17 pages, 4 figures</comments><msc-class>68Q32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Darwin's theory of evolution is considered to be one of the greatest
scientific gems in modern science. It not only gives us a description of how
living things evolve, but also shows how a population evolves through time and
also, why only the fittest individuals continue the generation forward. The
paper basically gives a high level analysis of the works of Valiant[1]. Though,
we know the mechanisms of evolution, but it seems that there does not exist any
strong quantitative and mathematical theory of the evolution of certain
mechanisms. What is defined exactly as the fitness of an individual, why is
that only certain individuals in a population tend to mutate, how computation
is done in finite time when we have exponentially many examples: there seems to
be a lot of questions which need to be answered. [1] basically treats Darwinian
theory as a form of computational learning theory, which calculates the net
fitness of the hypotheses and thus distinguishes functions and their classes
which could be evolvable using polynomial amount of resources. Evolution is
considered as a function of the environment and the previous evolutionary
stages that chooses the best hypothesis using learning techniques that makes
mutation possible and hence, gives a quantitative idea that why only the
fittest individuals tend to survive and have the power to mutate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4601</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4601</id><created>2013-12-16</created><authors><author><keyname>Flushing</keyname><forenames>Eduardo Feo</forenames></author><author><keyname>Gambardella</keyname><forenames>Luca M.</forenames></author><author><keyname>Di Caro</keyname><forenames>Gianni A.</forenames></author></authors><title>Strategic Control of Proximity Relationships in Heterogeneous Search and
  Rescue Teams</title><categories>cs.RO</categories><comments>In Proceedings of the 3rd IROS Workshop on Robots and Sensors
  integration in future rescue INformation system (ROSIN), Tokyo, Japan,
  November 7, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of search and rescue, we consider the problem of mission
planning for heterogeneous teams that can include human, robotic, and animal
agents. The problem is tackled using a mixed integer mathematical programming
formulation that jointly determines the path and the activity scheduling of
each agent in the team. Based on the mathematical formulation, we propose the
use of soft constraints and penalties that allow the flexible strategic control
of spatio-temporal relations among the search trajectories of the agents. In
this way, we can enable the mission planner to obtain solutions that maximize
the area coverage and, at the same time, control the spatial proximity among
the agents (e.g., to minimize mutual task interference, or to promote local
cooperation and data sharing). Through simulation experiments, we show the
application of the strategic framework considering a number of scenarios of
interest for real-world search and rescue missions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4605</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4605</id><created>2013-12-16</created><updated>2014-05-25</updated><authors><author><keyname>Wang</keyname><forenames>Xiangyu</forenames></author><author><keyname>Dunson</keyname><forenames>David B.</forenames></author></authors><title>Parallelizing MCMC via Weierstrass Sampler</title><categories>stat.CO cs.DC stat.ML</categories><comments>The original Algorithm 1 removed. Provided some theoretical
  justification for refinement sampling (Theorem 2). Added a new algorithm in
  addition to the rejection sampling for handling dimensionality curse. New
  simulations and graphs (with new colors and designs). A real data analysis is
  also provided</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapidly growing scales of statistical problems, subset based
communication-free parallel MCMC methods are a promising future for large scale
Bayesian analysis. In this article, we propose a new Weierstrass sampler for
parallel MCMC based on independent subsets. The new sampler approximates the
full data posterior samples via combining the posterior draws from independent
subset MCMC chains, and thus enjoys a higher computational efficiency. We show
that the approximation error for the Weierstrass sampler is bounded by some
tuning parameters and provide suggestions for choice of the values. Simulation
study shows the Weierstrass sampler is very competitive compared to other
methods for combining MCMC chains generated for subsets, including averaging
and kernel smoothing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4617</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4617</id><created>2013-12-16</created><updated>2014-04-16</updated><authors><author><keyname>Adedoyin-Olowe</keyname><forenames>Mariam</forenames></author><author><keyname>Gaber</keyname><forenames>Mohamed Medhat</forenames></author><author><keyname>Stahl</keyname><forenames>Frederic</forenames></author></authors><title>A Survey of Data Mining Techniques for Social Media Analysis</title><categories>cs.SI cs.CL</categories><comments>25 pages, 9 figures</comments><journal-ref>Journal of Data Mining &amp; Digital Humanities, 2014 (June 24, 2014)
  jdmdh:18</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Social network has gained remarkable attention in the last decade. Accessing
social network sites such as Twitter, Facebook LinkedIn and Google+ through the
internet and the web 2.0 technologies has become more affordable. People are
becoming more interested in and relying on social network for information, news
and opinion of other users on diverse subject matters. The heavy reliance on
social network sites causes them to generate massive data characterised by
three computational issues namely; size, noise and dynamism. These issues often
make social network data very complex to analyse manually, resulting in the
pertinent use of computational means of analysing them. Data mining provides a
wide range of techniques for detecting useful knowledge from massive datasets
like trends, patterns and rules [44]. Data mining techniques are used for
information retrieval, statistical modelling and machine learning. These
techniques employ data pre-processing, data analysis, and data interpretation
processes in the course of data analysis. This survey discusses different data
mining techniques used in mining diverse aspects of the social network over
decades going from the historical techniques to the up-to-date models,
including our novel technique named TRCM. All the techniques covered in this
survey are listed in the Table.1 including the tools employed as well as names
of their authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4626</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4626</id><created>2013-12-16</created><authors><author><keyname>Hamid</keyname><forenames>Raffay</forenames></author><author><keyname>Xiao</keyname><forenames>Ying</forenames></author><author><keyname>Gittens</keyname><forenames>Alex</forenames></author><author><keyname>DeCoste</keyname><forenames>Dennis</forenames></author></authors><title>Compact Random Feature Maps</title><categories>stat.ML cs.LG</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel approximation using randomized feature maps has recently gained a lot
of interest. In this work, we identify that previous approaches for polynomial
kernel approximation create maps that are rank deficient, and therefore do not
utilize the capacity of the projected feature space effectively. To address
this challenge, we propose compact random feature maps (CRAFTMaps) to
approximate polynomial kernels more concisely and accurately. We prove the
error bounds of CRAFTMaps demonstrating their superior kernel reconstruction
performance compared to the previous approximation schemes. We show how
structured random matrices can be used to efficiently generate CRAFTMaps, and
present a single-pass algorithm using CRAFTMaps to learn non-linear multi-class
classifiers. We present experiments on multiple standard data-sets with
performance competitive with state-of-the-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4628</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4628</id><created>2013-12-16</created><authors><author><keyname>Alvarez</keyname><forenames>Victor</forenames></author><author><keyname>Bringmann</keyname><forenames>Karl</forenames></author><author><keyname>Curticapean</keyname><forenames>Radu</forenames></author><author><keyname>Ray</keyname><forenames>Saurabh</forenames></author></authors><title>Counting Triangulations and other Crossing-free Structures via Onion
  Layers</title><categories>cs.CG cs.CC cs.DS math.CO</categories><comments>33 pages, 10 figures, 9 tables. A preliminary version appeared at
  SoCG 2012. This version contains experimental results comparing algorithms
  for counting triangulations. This paper has been submitted to a journal</comments><acm-class>F.2.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a set of $n$ points in the plane. A crossing-free structure on $P$
is a plane graph with vertex set $P$. Examples of crossing-free structures
include triangulations of $P$, spanning cycles of $P$, also known as
polygonalizations of $P$, among others. In this paper we develop a general
technique for computing the number of crossing-free structures of an input set
$P$. We apply the technique to obtain algorithms for computing the number of
triangulations, matchings, and spanning cycles of $P$. The running time of our
algorithms is upper bounded by $n^{O(k)}$, where $k$ is the number of onion
layers of $P$. In particular, for $k = O(1)$ our algorithms run in polynomial
time. In addition, we show that our algorithm for counting triangulations is
never slower than $O^{*}(3.1414^{n})$, even when $k = \Theta(n)$. Given that
there are several well-studied configurations of points with at least
$\Omega(3.464^{n})$ triangulations, and some even with $\Omega(8^{n})$
triangulations, our algorithm asymptotically outperforms any enumeration
algorithm for such instances. In fact, it is widely believed that any set of
$n$ points must have at least $\Omega(3.464^{n})$ triangulations. If this is
true, then our algorithm is strictly sub-linear in the number of triangulations
counted. We also show that our techniques are general enough to solve the
&quot;Restricted-Triangulation-Counting-Problem&quot;, which we prove to be $W[2]$-hard
in the parameter $k$. This implies a &quot;no free lunch&quot; result: In order to be
fixed-parameter tractable, our general algorithm must rely on additional
properties that are specific to the considered class of structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4634</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4634</id><created>2013-12-16</created><authors><author><keyname>Agrawal</keyname><forenames>Sharul</forenames></author><author><keyname>Prakash</keyname><forenames>Mr. Ravi</forenames></author><author><keyname>Narmawala</keyname><forenames>Prof. Zunnun</forenames></author></authors><title>Implementation of WSN which can simultaneously monitor Temperature
  conditions and control robot for positional accuracy</title><categories>cs.RO cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensor networks and robots are both quickly evolving fields, the union of two
fields seems inherently symbiotic. Collecting data from stationary sensors can
be time consuming task and thus can be automated by adding wireless
communication capabilities to the sensors. This proposed project takes
advantage of wireless sensor networks in remote handling environment which can
send signals over far distances by using a mesh topology, transfers the data
wirelessly and also consumes low power. In this paper a testbed is created for
wireless sensor network using custom build sensor nodes for temperature
monitoring in labs and to control a robot moving in another lab. The two
temperature sensor nodes used here consists of a Arduino microcontroller and
XBee wireless communication module based on IEEE 802.15.4 standard while the
robot has inherent FPGA board as a processing unit with xbee module connected
via Rs-2332 cable for serial communication between zigbee device and FPGA. A
simple custom packet is designed so that uniformity is maintained while
collection of data from temperature nodes and a moving robot and passing to a
remote terminal. The coordinator Zigbee is connected to remote terminal (PC)
through its USB port where Graphical user interface (GUI) can be run to monitor
Temperature readings and position of Robot dynamically and save those readings
in database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4637</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4637</id><created>2013-12-16</created><updated>2014-04-21</updated><authors><author><keyname>Zhang</keyname><forenames>Zhen</forenames></author><author><keyname>Shi</keyname><forenames>Qinfeng</forenames></author><author><keyname>Zhang</keyname><forenames>Yanning</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Constraint Reduction using Marginal Polytope Diagrams for MAP LP
  Relaxations</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LP relaxation-based message passing algorithms provide an effective tool for
MAP inference over Probabilistic Graphical Models. However, different LP
relaxations often have different objective functions and variables of differing
dimensions, which presents a barrier to effective comparison and analysis. In
addition, the computational complexity of LP relaxation-based methods grows
quickly with the number of constraints. Reducing the number of constraints
without sacrificing the quality of the solutions is thus desirable.
  We propose a unified formulation under which existing MAP LP relaxations may
be compared and analysed. Furthermore, we propose a new tool called Marginal
Polytope Diagrams. Some properties of Marginal Polytope Diagrams are exploited
such as node redundancy and edge equivalence. We show that using Marginal
Polytope Diagrams allows the number of constraints to be reduced without
loosening the LP relaxations. Then, using Marginal Polytope Diagrams and
constraint reduction, we develop three novel message passing algorithms, and
demonstrate that two of these show a significant improvement in speed over
state-of-art algorithms while delivering a competitive, and sometimes higher,
quality of solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4638</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4638</id><created>2013-12-16</created><updated>2015-02-12</updated><authors><author><keyname>Liu</keyname><forenames>Yan</forenames></author><author><keyname>Yan</keyname><forenames>Haode</forenames></author></authors><title>A Class of Five-weight Cyclic Codes and Their Weight Distribution</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1311.3391, and
  text overlap with arXiv:1302.0952 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a family of five-weight reducible cyclic codes is presented.
Furthermore, the weight distribution of these cyclic codes is determined, which
follows from the determination of value distributions of certain exponential
sums.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4640</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4640</id><created>2013-12-17</created><authors><author><keyname>Madeo</keyname><forenames>Renata Cristina Barros</forenames></author><author><keyname>Wagner</keyname><forenames>Priscilla Koch</forenames></author><author><keyname>Peres</keyname><forenames>Sarajane Marques</forenames></author></authors><title>A Review of Temporal Aspects of Hand Gesture Analysis Applied to
  Discourse Analysis and Natural Conversation</title><categories>cs.HC cs.AI</categories><comments>20 pages, International Journal of Computer Science &amp; Information
  Technology (IJCSIT) Vol 5, No 4, August 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lately, there has been an increasing interest in hand gesture analysis
systems. Recent works have employed pattern recognition techniques and have
focused on the development of systems with more natural user interfaces. These
systems may use gestures to control interfaces or recognize sign language
gestures, which can provide systems with multimodal interaction; or consist in
multimodal tools to help psycholinguists to understand new aspects of discourse
analysis and to automate laborious tasks. Gestures are characterized by several
aspects, mainly by movements and sequence of postures. Since data referring to
movements or sequences carry temporal information, this paper presents a
literature review about temporal aspects of hand gesture analysis, focusing on
applications related to natural conversation and psycholinguistic analysis,
using Systematic Literature Review methodology. In our results, we organized
works according to type of analysis, methods, highlighting the use of Machine
Learning techniques, and applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4652</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4652</id><created>2013-12-17</created><updated>2014-09-29</updated><authors><author><keyname>Naidenko</keyname><forenames>Vladimir</forenames><affiliation>Institute of Mathematics, National Academy of Sciences of Belarus</affiliation></author></authors><title>Logics for complexity classes</title><categories>cs.CC cs.LO</categories><comments>This article has been accepted for publication in Logic Journal of
  IGPL Published by Oxford University Press; 23 pages, 2 figures</comments><msc-class>68Q19</msc-class><acm-class>F.1.3; F.4.1</acm-class><journal-ref>Logic Journal of the IGPL (2014) 22 (6): 1075-1093</journal-ref><doi>10.1093/jigpal/jzu027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new syntactic characterization of problems complete via Turing reductions
is presented. General canonical forms are developed in order to define such
problems. One of these forms allows us to define complete problems on ordered
structures, and another form to define them on unordered non-Aristotelian
structures. Using the canonical forms, logics are developed for complete
problems in various complexity classes. Evidence is shown that there cannot be
any complete problem on Aristotelian structures for several complexity classes.
Our approach is extended beyond complete problems. Using a similar form, a
logic is developed to capture the complexity class $NP\cap coNP$ which very
likely contains no complete problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4659</identifier>
 <datestamp>2014-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4659</id><created>2013-12-17</created><updated>2014-08-20</updated><authors><author><keyname>Toshev</keyname><forenames>Alexander</forenames></author><author><keyname>Szegedy</keyname><forenames>Christian</forenames></author></authors><title>DeepPose: Human Pose Estimation via Deep Neural Networks</title><categories>cs.CV</categories><comments>IEEE Conference on Computer Vision and Pattern Recognition, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for human pose estimation based on Deep Neural Networks
(DNNs). The pose estimation is formulated as a DNN-based regression problem
towards body joints. We present a cascade of such DNN regressors which results
in high precision pose estimates. The approach has the advantage of reasoning
about pose in a holistic fashion and has a simple but yet powerful formulation
which capitalizes on recent advances in Deep Learning. We present a detailed
empirical analysis with state-of-art or better performance on four academic
benchmarks of diverse real-world images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4666</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4666</id><created>2013-12-17</created><authors><author><keyname>Kostyukov</keyname><forenames>Vladimir</forenames></author></authors><title>A Functional Approach to Standard Binary Heaps</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper describes a new and purely functional implementation technique of
binary heaps. A binary heap is a tree-based data structure that implements
priority queue operations (insert, remove, minimum/maximum) and guarantees at
worst logarithmic running time for them. Approaches and ideas described in this
paper present a simple and asymptotically optimal implementation of immutable
binary heap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4673</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4673</id><created>2013-12-17</created><updated>2014-09-16</updated><authors><author><keyname>Kobayashi</keyname><forenames>Hirotada</forenames></author><author><keyname>Gall</keyname><forenames>Fran&#xe7;ois Le</forenames></author><author><keyname>Nishimura</keyname><forenames>Harumichi</forenames></author></authors><title>Generalized Quantum Arthur-Merlin Games</title><categories>quant-ph cs.CC</categories><comments>31 pages + cover page, the proof of Lemma 27 (Lemma 24 in v1) is
  corrected, and a new completeness result is added</comments><journal-ref>Proceedings of the 30th Conference on Computational Complexity
  (CCC2015), pp. 488-511</journal-ref><doi>10.4230/LIPIcs.CCC.2015.488</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the role of interaction and coins in public-coin
quantum interactive proof systems (also called quantum Arthur-Merlin games).
While prior works focused on classical public coins even in the quantum
setting, the present work introduces a generalized version of quantum
Arthur-Merlin games where the public coins can be quantum as well: the verifier
can send not only random bits, but also halves of EPR pairs. First, it is
proved that the class of two-turn quantum Arthur-Merlin games with quantum
public coins, denoted qq-QAM in this paper, does not change by adding a
constant number of turns of classical interactions prior to the communications
of the qq-QAM proof systems. This can be viewed as a quantum analogue of the
celebrated collapse theorem for AM due to Babai. To prove this collapse
theorem, this paper provides a natural complete problem for qq-QAM: deciding
whether the output of a given quantum circuit is close to a totally mixed
state. This complete problem is on the very line of the previous studies
investigating the hardness of checking the properties related to quantum
circuits, and is of independent interest. It is further proved that the class
qq-QAM_1 of two-turn quantum-public-coin quantum Arthur-Merlin proof systems
with perfect completeness gives new bounds for standard well-studied classes of
two-turn interactive proof systems. Finally, the collapse theorem above is
extended to comprehensively classify the role of interaction and public coins
in quantum Arthur-Merlin games: it is proved that, for any constant m&gt;1, the
class of problems having an m-turn quantum Arthur-Merlin proof system is either
equal to PSPACE or equal to the class of problems having a two-turn quantum
Arthur-Merlin game of a specific type, which provides a complete set of quantum
analogues of Babai's collapse theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4676</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4676</id><created>2013-12-17</created><authors><author><keyname>Orman</keyname><forenames>G&#xfc;nce Keziban</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Labatut</keyname><forenames>Vincent</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Plantevit</keyname><forenames>Marc</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Boulicaut</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>LIRIS</affiliation></author></authors><title>Une m\'ethode pour caract\'eriser les communaut\'es des r\'eseaux
  dynamiques \`a attributs</title><categories>cs.SI</categories><comments>in French</comments><proxy>ccsd</proxy><journal-ref>Une m\'ethode pour caract\'eriser les communaut\'es des r\'eseaux
  dynamiques \`a attributs, Rennes : France (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many complex systems are modeled through complex networks whose analysis
reveals typical topological properties. Amongst those, the community structure
is one of the most studied. Many methods are proposed to detect communities,
not only in plain, but also in attributed, directed or even dynamic networks. A
community structure takes the form of a partition of the node set, which must
then be characterized relatively to the properties of the studied system. We
propose a method to support such a characterization task. We define a
sequence-based representation of networks, combining temporal information,
topological measures, and nodal attributes. We then characterize communities
using the most representative emerging sequential patterns of its nodes. This
also allows detecting unusual behavior in a community. We describe an empirical
study of a network of scientific collaborations.---De nombreux syst\`emes
complexes sont \'etudi\'es via l'analyse de r\'eseaux dits complexes ayant des
propri\'et\'es topologiques typiques. Parmi cellesci, les structures de
communaut\'es sont particuli\`erement \'etudi\'ees. De nombreuses m\'ethodes
permettent de les d\'etecter, y compris dans des r\'eseaux contenant des
attributs nodaux, des liens orient\'es ou \'evoluant dans le temps. La
d\'etection prend la forme d'une partition de l'ensemble des noeuds, qu'il faut
ensuite caract\'eriser relativement au syst\`eme mod\'elis\'e. Nous travaillons
sur l'assistance \`a cette t\^ache de caract\'erisation. Nous proposons une
repr\'esentation des r\'eseaux sous la forme de s\'equences de descripteurs de
noeuds, qui combinent les informations temporelles, les mesures topologiques,
et les valeurs des attributs nodaux. Les communaut\'es sont caract\'eris\'ees
au moyen des motifs s\'equentiels \'emergents les plus repr\'esentatifs issus
de leurs noeuds. Ceci permet notamment la d\'etection de comportements
inhabituels au sein d'une communaut\'e. Nous d\'ecrivons une \'etude empirique
sur un r\'eseau de collaboration scientifique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4678</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4678</id><created>2013-12-17</created><updated>2014-08-22</updated><authors><author><keyname>Chegrane</keyname><forenames>Ibrahim</forenames></author><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author></authors><title>Simple, compact and robust approximate string dictionary</title><categories>cs.DS cs.DB</categories><comments>Accepted to a journal (19 pages, 2 figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with practical implementations of approximate string
dictionaries that allow edit errors. In this problem, we have as input a
dictionary $D$ of $d$ strings of total length $n$ over an alphabet of size
$\sigma$. Given a bound $k$ and a pattern $x$ of length $m$, a query has to
return all the strings of the dictionary which are at edit distance at most $k$
from $x$, where the edit distance between two strings $x$ and $y$ is defined as
the minimum-cost sequence of edit operations that transform $x$ into $y$. The
cost of a sequence of operations is defined as the sum of the costs of the
operations involved in the sequence. In this paper, we assume that each of
these operations has unit cost and consider only three operations: deletion of
one character, insertion of one character and substitution of a character by
another. We present a practical implementation of the data structure we
recently proposed and which works only for one error. We extend the scheme to
$2\leq k&lt;m$. Our implementation has many desirable properties: it has a very
fast and space-efficient building algorithm. The dictionary data structure is
compact and has fast and robust query time. Finally our data structure is
simple to implement as it only uses basic techniques from the literature,
mainly hashing (linear probing and hash signatures) and succinct data
structures (bitvectors supporting rank queries).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4692</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4692</id><created>2013-12-17</created><authors><author><keyname>Ernvall</keyname><forenames>Toni</forenames><affiliation>Francis</affiliation></author><author><keyname>Lahtonen</keyname><forenames>Jyrki</forenames><affiliation>Francis</affiliation></author><author><keyname>Hsiao-feng</keyname><affiliation>Francis</affiliation></author><author><keyname>Lu</keyname></author><author><keyname>Vehkalahti</keyname><forenames>Roope</forenames></author></authors><title>An error event sensitive trade-off between rate and coding gain in MIMO
  MAC</title><categories>cs.IT math.IT</categories><comments>29 pages, 3 figures, submitted to IEEE Transactions on Information
  Theory</comments><journal-ref>IEEE Transactions on Information Theory, vol. 61, issue 11, 2015,
  pages 5931-5947</journal-ref><doi>10.1109/TIT.2015.2479638</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers space-time block coding for the Rayleigh fading
multiple-input multiple-output (MIMO) multiple access channel (MAC). If we
suppose that the receiver is performing joint maximum-likelihood (ML) decoding,
optimizing a MIMO MAC code against a fixed error event leads to a situation
where the joint codewords of the users in error can be seen as a single user
MIMO code. In such a case pair-wise error probability (PEP) based determinant
criterion of Tarokh et al. can be used to upper bound the error probability.
  It was already proven by Lahtonen et al. that irrespective of the used codes
the determinants of the differences of codewords of the overall codematrices
will decay as a function of the rates of the users.
  This work will study this decay phenomenon further and derive upper bounds
for the decay of determinants corresponding any error event. Lower bounds for
the optimal decay are studied by constructions based on algebraic number theory
and Diophantine approximation. For some error profiles the constructed codes
will be proven to be optimal.
  While the perspective of the paper is that of PEP, the final part of the
paper proves how the achieved decay results can be turned into statements about
the diversity-multiplexing gain trade-off (DMT).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4695</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4695</id><created>2013-12-17</created><updated>2014-02-18</updated><authors><author><keyname>Mlynarski</keyname><forenames>Wiktor</forenames></author></authors><title>Sparse, complex-valued representations of natural sounds learned with
  phase and amplitude continuity priors</title><categories>cs.LG cs.SD q-bio.NC</categories><comments>11 + 7 pages This version includes changes suggested by ICLR 2014
  reviewers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex-valued sparse coding is a data representation which employs a
dictionary of two-dimensional subspaces, while imposing a sparse, factorial
prior on complex amplitudes. When trained on a dataset of natural image
patches, it learns phase invariant features which closely resemble receptive
fields of complex cells in the visual cortex. Features trained on natural
sounds however, rarely reveal phase invariance and capture other aspects of the
data. This observation is a starting point of the present work. As its first
contribution, it provides an analysis of natural sound statistics by means of
learning sparse, complex representations of short speech intervals. Secondly,
it proposes priors over the basis function set, which bias them towards
phase-invariant solutions. In this way, a dictionary of complex basis functions
can be learned from the data statistics, while preserving the phase invariance
property. Finally, representations trained on speech sounds with and without
priors are compared. Prior-based basis functions reveal performance comparable
to unconstrained sparse coding, while explicitely representing phase as a
temporal shift. Such representations can find applications in many perceptual
and machine learning tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4704</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4704</id><created>2013-12-17</created><authors><author><keyname>Stolz</keyname><forenames>Alex</forenames></author><author><keyname>Rodriguez-Castro</keyname><forenames>Bene</forenames></author><author><keyname>Hepp</keyname><forenames>Martin</forenames></author></authors><title>RDF Translator: A RESTful Multi-Format Data Converter for the Semantic
  Web</title><categories>cs.DL cs.AI</categories><comments>Technical report, 15 pages</comments><report-no>TR-2013-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interdisciplinary nature of the Semantic Web and the many projects put
forward by the community led to a large number of widely accepted serialization
formats for RDF. Most of these RDF syntaxes have been developed out of a
necessity to serve specific purposes better than existing ones, e.g. RDFa was
proposed as an extension to HTML for embedding non-intrusive RDF statements in
human-readable documents. Nonetheless, the RDF serialization formats are
generally transducible among themselves given that they are commonly based on
the RDF model. In this paper, we present (1) a RESTful Web service based on the
HTTP protocol that translates between different serializations. In addition to
its core functionality, our proposed solution provides (2) features to
accommodate frequent needs of Semantic Web developers, namely a straightforward
user interface with copy-to-clipboard functionality, syntax highlighting,
persistent URI links for easy sharing, cool URI patterns, and content
negotiation using respective HTTP headers. We demonstrate the benefit of our
converter by presenting two use cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4706</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4706</id><created>2013-12-17</created><authors><author><keyname>Vakharia</keyname><forenames>Donna</forenames></author><author><keyname>Gibbs</keyname><forenames>Rachel</forenames></author></authors><title>Designing Spontaneous Speech Search Interface for Historical Archives</title><categories>cs.HC cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spontaneous speech in the form of conversations, meetings, voice-mail,
interviews, oral history, etc. is one of the most ubiquitous forms of human
communication. Search engines providing access to such speech collections have
the potential to better inform intelligence and make relevant data over vast
audio/video archives available to users. This project presents a search user
interface design supporting search tasks over a speech collection consisting of
an historical archive with nearly 52,000 audiovisual testimonies of survivors
and witnesses of the Holocaust and other genocides. The design incorporates
faceted search, along with other UI elements like highlighted search items,
tags, snippets, etc., to promote discovery and exploratory search. Two
different designs have been created to support both manual and automated
transcripts. Evaluation was performed using human subjects to measure accuracy
in retrieving results, understanding user-perspective on the design elements,
and ease of parsing information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4707</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4707</id><created>2013-12-17</created><authors><author><keyname>Nomikos</keyname><forenames>George</forenames></author><author><keyname>Pantazopoulos</keyname><forenames>Panagiotis</forenames></author><author><keyname>Karaliopoulos</keyname><forenames>Merkourios</forenames></author><author><keyname>Stavrakakis</keyname><forenames>Ioannis</forenames></author></authors><title>The Multiple Instances of Node Centrality and their Implications on the
  Vulnerability of ISP Networks</title><categories>cs.SI</categories><comments>15 pages, 10 figures, 9 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The position of the nodes within a network topology largely determines the
level of their involvement in various networking functions. Yet numerous node
centrality indices, proposed to quantify how central individual nodes are in
this respect, yield very different views of their relative significance. Our
first contribution in this paper is then an exhaustive survey and
categorization of centrality indices along several attributes including the
type of information (local vs. global) and processing complexity required for
their computation. We next study the seven most popular of those indices in the
context of Internet vulnerability to address issues that remain under-explored
in literature so far. First, we carry out a correlation study to assess the
consistency of the node rankings those indices generate over ISP router-level
topologies. For each pair of indices, we compute the full ranking correlation,
which is the standard choice in literature, and the percentage overlap between
the k top nodes. Then, we let these rankings guide the removal of highly
central nodes and assess the impact on both the connectivity properties and
traffic-carrying capacity of the network. Our results confirm that the top-k
overlap predicts the comparative impact of indices on the network vulnerability
better than the full-ranking correlation. Importantly, the locally computed
degree centrality index approximates closely the global indices with the most
dramatic impact on the traffic-carrying capacity; whereas, its approximative
power in terms of connectivity is more topology-dependent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4716</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4716</id><created>2013-12-17</created><updated>2013-12-30</updated><authors><author><keyname>Wu</keyname><forenames>Gaofei</forenames></author><author><keyname>Li</keyname><forenames>Nian</forenames></author><author><keyname>Helleseth</keyname><forenames>Tor</forenames></author><author><keyname>Zhang</keyname><forenames>Yuqing</forenames></author></authors><title>More Classes of Complete Permutation Polynomials over $\F_q$</title><categories>cs.IT math.IT math.NT</categories><comments>17 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, by using a powerful criterion for permutation polynomials
given by Zieve, we give several classes of complete permutation monomials over
$\F_{q^r}$. In addition, we present a class of complete permutation
multinomials, which is a generalization of recent work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4722</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4722</id><created>2013-12-17</created><updated>2014-08-22</updated><authors><author><keyname>Assuncao</keyname><forenames>Marcos D.</forenames></author><author><keyname>Calheiros</keyname><forenames>Rodrigo N.</forenames></author><author><keyname>Bianchi</keyname><forenames>Silvia</forenames></author><author><keyname>Netto</keyname><forenames>Marco A. S.</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>Big Data Computing and Clouds: Trends and Future Directions</title><categories>cs.DC</categories><doi>10.1016/j.jpdc.2014.08.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses approaches and environments for carrying out analytics
on Clouds for Big Data applications. It revolves around four important areas of
analytics and Big Data, namely (i) data management and supporting
architectures; (ii) model development and scoring; (iii) visualisation and user
interaction; and (iv) business models. Through a detailed survey, we identify
possible gaps in technology and provide recommendations for the research
community on future directions on Cloud-supported Big Data computing and
analytics solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4740</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4740</id><created>2013-12-17</created><updated>2013-12-20</updated><authors><author><keyname>Bai</keyname><forenames>Yalong</forenames></author><author><keyname>Yang</keyname><forenames>Kuiyuan</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author><author><keyname>Ma</keyname><forenames>Wei-Ying</forenames></author><author><keyname>Zhao</keyname><forenames>Tiejun</forenames></author></authors><title>Learning High-level Image Representation for Image Retrieval via
  Multi-Task DNN using Clickthrough Data</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image retrieval refers to finding relevant images from an image database for
a query, which is considered difficult for the gap between low-level
representation of images and high-level representation of queries. Recently
further developed Deep Neural Network sheds light on automatically learning
high-level image representation from raw pixels. In this paper, we proposed a
multi-task DNN learned for image retrieval, which contains two parts, i.e.,
query-sharing layers for image representation computation and query-specific
layers for relevance estimation. The weights of multi-task DNN are learned on
clickthrough data by Ring Training. Experimental results on both simulated and
real dataset show the effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4742</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4742</id><created>2013-12-17</created><authors><author><keyname>Ocampo</keyname><forenames>Alexis</forenames></author><author><keyname>Bella</keyname><forenames>Fabio</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Software Process Commonality Analysis</title><categories>cs.SE</categories><comments>26 pages. The final publication is available at
  http://onlinelibrary.wiley.com/doi/10.1002/spip.229/abstract</comments><journal-ref>International Journal on Software Process: Improvement and
  Practice, 10(3):273-285, 2005</journal-ref><doi>10.1002/spip.229</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To remain viable and thrive, software organizations must rapidly adapt to
frequent, and often rather far-ranging, changes to their operational context.
These changes typically concern many factors, including the nature of the
organization's marketplace in general, its customers' demands, and its business
needs. In today's most highly dynamic contexts, such as web services
development, other changes create additional, severe challenges. Most critical
are changes to the technology in which a software product is written or which
the software product has to control or use to provide its functionality. These
product-support technology changes are frequently relatively 'small' and
incremental. They are, therefore, often handled by relatively 'small,'
incremental changes to the organization's software processes. However, the
frequency of these changes is high, and their impact is elevated by
time-to-market and requirements change demands. The net result is an extremely
challenging need to create and manage a large number of customized process
variants, collectively having more commonalities than differences, and
incorporating experience-based, proven 'best practices'. This paper describes a
tool-based approach to coping with product-support technology changes. The
approach utilizes established capabilities such as descriptive process modeling
and the creation of reference models. It incorporates a new, innovative,
tool-based capability to analyze commonalities and differences among processes.
The paper includes an example-based evaluation of the approach in the domain of
Wireless Internet Services as well as a discussion of its potentially broader
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4746</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4746</id><created>2013-12-17</created><authors><author><keyname>Nieuwenhuis</keyname><forenames>Claudia</forenames></author><author><keyname>Cremers</keyname><forenames>Daniel</forenames></author><author><keyname>Hawe</keyname><forenames>Simon</forenames></author><author><keyname>Kleinsteuber</keyname><forenames>Martin</forenames></author></authors><title>Co-Sparse Textural Similarity for Image Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an algorithm for segmenting natural images based on texture and
color information, which leverages the co-sparse analysis model for image
segmentation within a convex multilabel optimization framework. As a key
ingredient of this method, we introduce a novel textural similarity measure,
which builds upon the co-sparse representation of image patches. We propose a
Bayesian approach to merge textural similarity with information about color and
location. Combined with recently developed convex multilabel optimization
methods this leads to an efficient algorithm for both supervised and
unsupervised segmentation, which is easily parallelized on graphics hardware.
The approach provides competitive results in unsupervised segmentation and
outperforms state-of-the-art interactive segmentation methods on the Graz
Benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4752</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4752</id><created>2013-12-17</created><authors><author><keyname>Martins</keyname><forenames>Ricardo</forenames></author></authors><title>BW - Eye Ophthalmologic decision support system based on clinical
  workflow and data mining techniques-image registration algorithm</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blueworks - Medical Expert Diagnosis is developing an application, BWEye, to
be used as an ophthalmology consultation decision support system. The
implementation of this application involves several different tasks and one of
them is the implementation of an ophthalmology images registration algorithm.
The work reported in this document is related with the implementation of an
algorithm to register images of angiography, colour retinography and redfree
retinography. The implementations described were developed in the software
MATLAB.
  The implemented algorithm is based in the detection of the bifurcation points
(y-features) of the vascular structures of the retina that usually are visible
in the referred type of images. There are proposed two approaches to establish
an initial set of features correspondences. The first approach is based in the
maximization of the mutual information of the bifurcation regions of the
features of images. The second approach is based in the characterization of
each bifurcation point and in the minimization of the Euclidean distance
between the descriptions of the features of the images in the descriptors
space. The final set of the matching features for a pair of images is defined
through the application of the RANSAC algorithm.
  Although, it was not achieved the implementation of a full functional
algorithm, there were made several analysis that can be important to future
improvement of the current implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4758</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4758</id><created>2013-12-17</created><updated>2014-04-10</updated><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author></authors><title>On physical problems that are slightly more difficult than QMA</title><categories>quant-ph cs.CC</categories><comments>25 pages, v2 various small changes, to appear in CCC'2014</comments><acm-class>F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of computational problems from quantum physics.
Typically, they are studied using the complexity class QMA (quantum counterpart
of NP) but some natural computational problems appear to be slightly harder
than QMA. We introduce new complexity classes consisting of problems that are
solvable with a small number of queries to a QMA oracle and use these
complexity classes to quantify the complexity of several natural computational
problems (for example, the complexity of estimating the spectral gap of a
Hamiltonian).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4793</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4793</id><created>2013-12-17</created><authors><author><keyname>Mishra</keyname><forenames>Dheerendra</forenames></author><author><keyname>Chaturvedi</keyname><forenames>Ankita</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Sourav</forenames></author></authors><title>Cryptanalysis and Improvement of Jiang et al.'s Smart Card Based Remote
  User Authentication Scheme</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The smart card based authentication protocols try to ensure secure and
authorized communication between remote entities. In 2012, Wei et al. presented
an improvement of Wu et al.'s two-factor authentication scheme for TMIS which
is proven vulnerable to off-line password guessing attack by Zhu. Zhu also
proposed a modified scheme to overcome with weakness of Wei et al.'s scheme,
although Lee and Liu showed the failure of his scheme to resist parallel
session attacks. Moreover, Lee and Liu introduced an improved scheme. We
analyze Wei et al.'s, Zhu's and Lee and Liu's schemes and identify that none of
the schemes resist on-line password guessing attack. Moreover, these schemes do
not present efficient login and password chance phase. We also show that how
inefficient password change phase causes denial of service attack. Further, we
propose an improved password based remote user authentication scheme with the
aim to eliminate all the drawbacks of previously presented schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4794</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4794</id><created>2013-12-17</created><authors><author><keyname>Slimani</keyname><forenames>Thabet</forenames></author></authors><title>Semantic Annotation: The Mainstay of Semantic Web</title><categories>cs.DL cs.AI cs.IR</categories><comments>8 pages, 3 figures</comments><journal-ref>International Journal of Computer Applications Technology and
  Research, Volume 2, Issue 6, 763-770, 2013</journal-ref><doi>10.7753/IJCATR0206.1025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given that semantic Web realization is based on the critical mass of metadata
accessibility and the representation of data with formal knowledge, it needs to
generate metadata that is specific, easy to understand and well-defined.
However, semantic annotation of the web documents is the successful way to make
the Semantic Web vision a reality. This paper introduces the Semantic Web and
its vision (stack layers) with regard to some concept definitions that helps
the understanding of semantic annotation. Additionally, this paper introduces
the semantic annotation categories, tools, domains and models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4798</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4798</id><created>2013-12-17</created><authors><author><keyname>Bacinoglu</keyname><forenames>Baran Tan</forenames></author><author><keyname>Uysal-Biyikoglu</keyname><forenames>Elif</forenames></author></authors><title>Finite Horizon Online Lazy Scheduling with Energy Harvesting
  Transmitters over Fading Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lazy scheduling, i.e. setting transmit power and rate in response to data
traffic as low as possible so as to satisfy delay constraints, is a known
method for energy efficient transmission.This paper addresses an online lazy
scheduling problem over finite time-slotted transmission window and introduces
low-complexity heuristics which attain near-optimal performance.Particularly,
this paper generalizes lazy scheduling problem for energy harvesting systems to
deal with packet arrival, energy harvesting and time-varying channel processes
simultaneously. The time-slotted formulation of the problem and depiction of
its offline optimal solution provide explicit expressions allowing to derive
good online policies and algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4800</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4800</id><created>2013-12-17</created><authors><author><keyname>Slimani</keyname><forenames>Thabet</forenames></author></authors><title>New Approach to Optimize the Time of Association Rules Extraction</title><categories>cs.DB</categories><comments>10 pages, 6 figures</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 5, No 1, September 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The knowledge discovery algorithms have become ineffective at the abundance
of data and the need for fast algorithms or optimizing methods is required. To
address this limitation, the objective of this work is to adapt a new method
for optimizing the time of association rules extractions from large databases.
Indeed, given a relational database (one relation) represented as a set of
tuples, also called set of attributes, we transform the original database as a
binary table (Bitmap table) containing binary numbers. Then, we use this Bitmap
table to construct a data structure called Peano Tree stored as a binary file
on which we apply a new algorithm called BF-ARM (extension of the well known
Apriori algorithm). Since the database is loaded into a binary file, our
proposed algorithm will traverse this file, and the processes of association
rules extractions will be based on the file stored on disk. The BF-ARM
algorithm is implemented and compared with Apriori, Apriori+ and RS-Rules+
algorithms. The evaluation process is based on three benchmarks (Mushroom, Car
Evaluation and Adult). Our preliminary experimental results showed that our
algorithm produces association rules with a minimum time compared to other
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4802</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4802</id><created>2013-12-17</created><authors><author><keyname>Singh</keyname><forenames>Niraj Kumar</forenames></author><author><keyname>Chakraborty</keyname><forenames>Soubhik</forenames></author><author><keyname>Mallick</keyname><forenames>Dheeresh Kumar</forenames></author></authors><title>A Statistical Peek into Average Case Complexity</title><categories>cs.DS stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present paper gives a statistical adventure towards exploring the average
case complexity behavior of computer algorithms. Rather than following the
traditional count based analytical (pen and paper) approach, we instead talk in
terms of the weight based analysis that permits mixing of distinct operations
into a conceptual bound called the statistical bound and its empirical
estimate, the so called &quot;empirical O&quot;. Based on careful analysis of the results
obtained, we have introduced two new conjectures in the domain of algorithmic
analysis. The analytical way of average case analysis falls flat when it comes
to a data model for which the expectation does not exist (e.g. Cauchy
distribution for continuous input data and certain discrete distribution inputs
as those studied in the paper). The empirical side of our approach, with a
thrust in computer experiments and applied statistics in its paradigm, lends a
helping hand by complimenting and supplementing its theoretical counterpart.
Computer science is or at least has aspects of an experimental science as well,
and hence hopefully, our statistical findings will be equally recognized among
theoretical scientists as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4805</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4805</id><created>2013-12-17</created><authors><author><keyname>Baldi</keyname><forenames>Marco</forenames></author><author><keyname>Cancellieri</keyname><forenames>Giovanni</forenames></author><author><keyname>Chiaraluce</keyname><forenames>Franco</forenames></author></authors><title>Array Convolutional Low-Density Parity-Check Codes</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures, accepted for publication in IEEE Communications
  Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a design technique for obtaining regular time-invariant
low-density parity-check convolutional (RTI-LDPCC) codes with low complexity
and good performance. We start from previous approaches which unwrap a
low-density parity-check (LDPC) block code into an RTI-LDPCC code, and we
obtain a new method to design RTI-LDPCC codes with better performance and
shorter constraint length. Differently from previous techniques, we start the
design from an array LDPC block code. We show that, for codes with high rate, a
performance gain and a reduction in the constraint length are achieved with
respect to previous proposals. Additionally, an increase in the minimum
distance is observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4811</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4811</id><created>2013-12-17</created><updated>2016-02-12</updated><authors><author><keyname>Yang</keyname><forenames>Shenghao</forenames></author><author><keyname>Ng</keyname><forenames>Tsz-Ching</forenames></author><author><keyname>Yeung</keyname><forenames>Raymond W.</forenames></author></authors><title>Finite-Length Analysis of BATS Codes</title><categories>cs.IT math.IT</categories><comments>47 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BATS codes were proposed for communication through networks with packet loss.
A BATS code consists of an outer code and an inner code. The outer code is a
matrix generation of a fountain code, which works with the inner code that
comprises random linear coding at the intermediate network nodes. In this
paper, the performance of finite-length BATS codes is analyzed with respect to
both belief propagation (BP) decoding and inactivation decoding. Our results
enable us to evaluate efficiently the finite-length performance in terms of the
number of batches used for decoding ranging from 1 to a given maximum number,
and provide new insights on the decoding performance. Specifically, for a fixed
number of input symbols and a range of the number of batches used for decoding,
we obtain recursive formulae to calculate respectively the stopping time
distribution of BP decoding and the inactivation probability in inactivation
decoding. We also find that both the failure probability of BP decoding and the
expected number of inactivations in inactivation decoding can be expressed in a
power-sum form where the number of batches appears only as the exponent. This
power-sum expression reveals clearly how the decoding failure probability and
the expected number of inactivation decrease with the number of batches. When
the number of batches used for decoding follows a Poisson distribution, we
further derive recursive formulae with potentially lower computational
complexity for both decoding algorithms. For the BP decoder that consumes
batches one by one, three formulae are provided to characterize the expected
number of consumed batches until all the input symbols are decoded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4814</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4814</id><created>2013-12-17</created><authors><author><keyname>Macedo</keyname><forenames>Hugo Daniel</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Touili</keyname><forenames>Tayssir</forenames><affiliation>LIAFA</affiliation></author></authors><title>Mining Malware Specifications through Static Reachability Analysis</title><categories>cs.CR cs.AI cs.LO</categories><comments>Lecture notes in computer science (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of malicious software (malware) is growing out of control.
Syntactic signature based detection cannot cope with such growth and manual
construction of malware signature databases needs to be replaced by computer
learning based approaches. Currently, a single modern signature capturing the
semantics of a malicious behavior can be used to replace an arbitrarily large
number of old-fashioned syntactical signatures. However teaching computers to
learn such behaviors is a challenge. Existing work relies on dynamic analysis
to extract malicious behaviors, but such technique does not guarantee the
coverage of all behaviors. To sidestep this limitation we show how to learn
malware signatures using static reachability analysis. The idea is to model
binary programs using pushdown systems (that can be used to model the stack
operations occurring during the binary code execution), use reachability
analysis to extract behaviors in the form of trees, and use subtrees that are
common among the trees extracted from a training set of malware files as
signatures. To detect malware we propose to use a tree automaton to compactly
store malicious behavior trees and check if any of the subtrees extracted from
the file under analysis is malicious. Experimental data shows that our approach
can be used to learn signatures from a training set of malware files and use
them to detect a test set of malware that is 5 times the size of the training
set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4818</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4818</id><created>2013-12-17</created><authors><author><keyname>Macedo</keyname><forenames>Hugo Daniel</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Oliveira</keyname><forenames>Jos&#xe9; N.</forenames><affiliation>HASLab</affiliation></author></authors><title>Typing linear algebra: A biproduct-oriented approach</title><categories>cs.SE math.CT</categories><comments>Science of Computer Programming (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interested in formalizing the generation of fast running code for linear
algebra applications, the authors show how an index-free, calculational
approach to matrix algebra can be developed by regarding matrices as morphisms
of a category with biproducts. This shifts the traditional view of matrices as
indexed structures to a type-level perspective analogous to that of the
pointfree algebra of programming. The derivation of fusion, cancellation and
abide laws from the biproduct equations makes it easy to calculate algorithms
implementing matrix multiplication, the central operation of matrix algebra,
ranging from its divide-and-conquer version to its vectorization
implementation. From errant attempts to learn how particular products and
coproducts emerge from biproducts, not only blocked matrix algebra is
rediscovered but also a way of extending other operations (e.g. Gaussian
elimination) blockwise, in a calculational style, is found. The prospect of
building biproduct-based type checkers for computer algebra systems such as
MatlabTM is also considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4824</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4824</id><created>2013-12-17</created><updated>2014-01-15</updated><authors><author><keyname>Pande</keyname><forenames>B. P.</forenames></author><author><keyname>Tamta</keyname><forenames>Pawan</forenames></author><author><keyname>Dhami</keyname><forenames>H. S.</forenames></author></authors><title>Generation, Implementation and Appraisal of an N-gram based Stemming
  Algorithm</title><categories>cs.IR cs.CL</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A language independent stemmer has always been looked for. Single N-gram
tokenization technique works well, however, it often generates stems that start
with intermediate characters, rather than initial ones. We present a novel
technique that takes the concept of N gram stemming one step ahead and compare
our method with an established algorithm in the field, Porter's Stemmer.
Results indicate that our N gram stemmer is not inferior to Porter's linguistic
stemmer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4826</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4826</id><created>2013-12-17</created><updated>2013-12-25</updated><authors><author><keyname>Zattoni</keyname><forenames>Elena</forenames></author></authors><title>Geometric Methods for Invariant-Zero Cancellation in Linear
  Multivariable Systems: Illustrative Examples</title><categories>cs.SY</categories><comments>With respect to version 1, cross references to [1] have been revised</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note presents some numerical examples worked out in order to show the
reader how to implement, within a widely accessible computational setting, the
methodology for achieving zero cancellation in linear multivariable systems
discussed in [1]. The results are evaluated in the light of applicability and
performance of different methods available in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4828</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4828</id><created>2013-11-19</created><authors><author><keyname>Cerutti</keyname><forenames>Federico</forenames></author><author><keyname>Toniolo</keyname><forenames>Alice</forenames></author><author><keyname>Oren</keyname><forenames>Nir</forenames></author><author><keyname>Norman</keyname><forenames>Timothy J.</forenames></author></authors><title>Subjective Logic Operators in Trust Assessment: an Empirical Study</title><categories>cs.CR cs.AI cs.LO</categories><comments>Submitted to Information Systems Frontiers Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational trust mechanisms aim to produce trust ratings from both direct
and indirect information about agents' behaviour. Subjective Logic (SL) has
been widely adopted as the core of such systems via its fusion and discount
operators. In recent research we revisited the semantics of these operators to
explore an alternative, geometric interpretation. In this paper we present a
principled desiderata for discounting and fusion operators in SL. Building upon
this we present operators that satisfy these desirable properties, including a
family of discount operators. We then show, through a rigorous empirical study,
that specific, geometrically interpreted operators significantly outperform
standard SL operators in estimating ground truth. These novel operators offer
real advantages for computational models of trust and reputation, in which they
may be employed without modifying other aspects of an existing system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4833</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4833</id><created>2013-11-21</created><authors><author><keyname>Iwase</keyname><forenames>Ryo</forenames><affiliation>Osaka University</affiliation></author><author><keyname>Ishihara</keyname><forenames>Yasunori</forenames><affiliation>Osaka University</affiliation></author><author><keyname>Fujiwara</keyname><forenames>Toru</forenames><affiliation>Osaka University</affiliation></author></authors><title>Toward Security Verification against Inference Attacks on Data Trees</title><categories>cs.CR cs.DB cs.FL</categories><comments>In Proceedings TTATT 2013, arXiv:1311.5058</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 134, 2013, pp. 49-59</journal-ref><doi>10.4204/EPTCS.134.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes our ongoing work on security verification against
inference attacks on data trees. We focus on infinite secrecy against inference
attacks, which means that attackers cannot narrow down the candidates for the
value of the sensitive information to finite by available information to the
attackers. Our purpose is to propose a model under which infinite secrecy is
decidable. To be specific, we first propose tree transducers which are
expressive enough to represent practical queries. Then, in order to represent
attackers' knowledge, we propose data tree types such that type inference and
inverse type inference on those tree transducers are possible with respect to
data tree types, and infiniteness of data tree types is decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4839</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4839</id><created>2013-11-19</created><authors><author><keyname>Bisdikian</keyname><forenames>Chatschik</forenames></author><author><keyname>Cerutti</keyname><forenames>Federico</forenames></author><author><keyname>Tang</keyname><forenames>Yuqing</forenames></author><author><keyname>Oren</keyname><forenames>Nir</forenames></author></authors><title>Reasoning about the Impacts of Information Sharing</title><categories>cs.AI</categories><comments>Submitted to Information Systems Frontiers Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a decision process framework allowing an agent to
decide what information it should reveal to its neighbours within a
communication graph in order to maximise its utility. We assume that these
neighbours can pass information onto others within the graph. The inferences
made by agents receiving the messages can have a positive or negative impact on
the information providing agent, and our decision process seeks to identify how
a message should be modified in order to be most beneficial to the information
producer. Our decision process is based on the provider's subjective beliefs
about others in the system, and therefore makes extensive use of the notion of
trust. Our core contributions are therefore the construction of a model of
information propagation; the description of the agent's decision procedure; and
an analysis of some of its properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4840</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4840</id><created>2013-12-17</created><authors><author><keyname>Cheney</keyname><forenames>James</forenames></author></authors><title>A simple sequent calculus for nominal logic</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nominal logic is a variant of first-order logic that provides support for
reasoning about bound names in abstract syntax. A key feature of nominal logic
is the new-quantifier, which quantifies over fresh names (names not appearing
in any values considered so far). Previous attempts have been made to develop
convenient rules for reasoning with the new-quantifier, but we argue that none
of these attempts is completely satisfactory.
  In this article we develop a new sequent calculus for nominal logic in which
the rules for the new- quantifier are much simpler than in previous attempts.
We also prove several structural and metatheoretic properties, including
cut-elimination, consistency, and equivalence to Pitts' axiomatization of
nominal logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4851</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4851</id><created>2013-12-17</created><updated>2013-12-21</updated><authors><author><keyname>Thanh</keyname><forenames>Le Nguyen Tuan</forenames></author><author><keyname>Hanachi</keyname><forenames>Chihab</forenames></author><author><keyname>Stinckwich</keyname><forenames>Serge</forenames></author><author><keyname>Vinh</keyname><forenames>Ho Tuong</forenames></author></authors><title>Representing, Simulating and Analysing Ho Chi Minh City Tsunami Plan by
  Means of Process Models</title><categories>cs.CY cs.AI</categories><comments>7 pages. ISCRAM (Information Systems for Crisis Response and
  Management) Vietnam 2013 Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the textual plan (guidelines) proposed by People's
Committee of Ho Chi Minh City (Vietnam) to manage earthquake and tsunami, and
try to represent it in a more formal way, in order to provide means to
simulate, analyse and adapt it. We first present a state of the art about
coordination models for disaster management with a focus on process oriented
approaches. We give an overview of the different dimensions of the textual
tsunami plan of Ho Chi Minh City and then the graphical representation of its
process with BPMN (Business Process Model and Notation). We finally show how to
exploit this process with workflow tools to simulate (YAWL tool) and analyse it
(ProM tool).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4852</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4852</id><created>2013-12-17</created><authors><author><keyname>Frigola</keyname><forenames>Roger</forenames></author><author><keyname>Lindsten</keyname><forenames>Fredrik</forenames></author><author><keyname>Sch&#xf6;n</keyname><forenames>Thomas B.</forenames></author><author><keyname>Rasmussen</keyname><forenames>Carl E.</forenames></author></authors><title>Identification of Gaussian Process State-Space Models with Particle
  Stochastic Approximation EM</title><categories>stat.ML cs.SY</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Gaussian process state-space models (GP-SSMs) are a very flexible family of
models of nonlinear dynamical systems. They comprise a Bayesian nonparametric
representation of the dynamics of the system and additional (hyper-)parameters
governing the properties of this nonparametric representation. The Bayesian
formalism enables systematic reasoning about the uncertainty in the system
dynamics. We present an approach to maximum likelihood identification of the
parameters in GP-SSMs, while retaining the full nonparametric description of
the dynamics. The method is based on a stochastic approximation version of the
EM algorithm that employs recent developments in particle Markov chain Monte
Carlo for efficient identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4853</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4853</id><created>2013-12-17</created><authors><author><keyname>Healy</keyname><forenames>Philip</forenames></author><author><keyname>Meyer</keyname><forenames>Stefan</forenames></author><author><keyname>Morrison</keyname><forenames>John</forenames></author><author><keyname>Lynn</keyname><forenames>Theo</forenames></author><author><keyname>Paya</keyname><forenames>Ashkan</forenames></author><author><keyname>Marinescu</keyname><forenames>Dan C.</forenames></author></authors><title>Bid-Centric Cloud Service Provisioning</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bid-centric service descriptions have the potential to offer a new cloud
service provisioning model that promotes portability, diversity of choice and
differentiation between providers. A bid matching model based on requirements
and capabilities is presented that provides the basis for such an approach. In
order to facilitate the bidding process, tenders should be specified as
abstractly as possible so that the solution space is not needlessly restricted.
To this end, we describe how partial TOSCA service descriptions allow for a
range of diverse solutions to be proposed by multiple providers in response to
tenders. Rather than adopting a lowest common denominator approach, true
portability should allow for the relative strengths and differentiating
features of cloud service providers to be applied to bids. With this in mind,
we describe how TOSCA service descriptions could be augmented with additional
information in order to facilitate heterogeneity in proposed solutions, such as
the use of coprocessors and provider-specific services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4860</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4860</id><created>2013-12-15</created><updated>2014-07-24</updated><authors><author><keyname>Browet</keyname><forenames>Arnaud</forenames></author><author><keyname>Van Dooren</keyname><forenames>Paul</forenames></author></authors><title>Low-rank Similarity Measure for Role Model Extraction</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages, 2 columns, 4 figures, conference paper for MTNS2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing meaningful clusters of nodes is crucial to analyze large networks.
In this paper, we present a pairwise node similarity measure that allows to
extract roles, i.e. group of nodes sharing similar flow patterns within a
network. We propose a low rank iterative scheme to approximate the similarity
measure for very large networks. Finally, we show that our low rank similarity
score successfully extracts the different roles in random graphs and that its
performances are similar to the pairwise similarity measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4863</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4863</id><created>2013-12-17</created><updated>2014-01-22</updated><authors><author><keyname>Abasi</keyname><forenames>Hasan</forenames></author><author><keyname>Bshouty</keyname><forenames>Nader H.</forenames></author><author><keyname>Gabizon</keyname><forenames>Ariel</forenames></author><author><keyname>Haramaty</keyname><forenames>Elad</forenames></author></authors><title>On $r$-Simple $k$-Path</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $r$-simple $k$-path is a {path} in the graph of length $k$ that passes
through each vertex at most $r$ times. The $r$-SIMPLE $k$-PATH problem, given a
graph $G$ as input, asks whether there exists an $r$-simple $k$-path in $G$. We
first show that this problem is NP-Complete. We then show that there is a graph
$G$ that contains an $r$-simple $k$-path and no simple path of length greater
than $4\log k/\log r$. So this, in a sense, motivates this problem especially
when one's goal is to find a short path that visits many vertices in the graph
while bounding the number of visits at each vertex.
  We then give a randomized algorithm that runs in time $$\mathrm{poly}(n)\cdot
2^{O( k\cdot \log r/r)}$$ that solves the $r$-SIMPLE $k$-PATH on a graph with
$n$ vertices with one-sided error. We also show that a randomized algorithm
with running time $\mathrm{poly}(n)\cdot 2^{(c/2)k/ r}$ with $c&lt;1$ gives a
randomized algorithm with running time $\poly(n)\cdot 2^{cn}$ for the
Hamiltonian path problem in a directed graph - an outstanding open problem. So
in a sense our algorithm is optimal up to an $O(\log r)$ factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4874</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4874</id><created>2013-12-17</created><updated>2013-12-19</updated><authors><author><keyname>Maggi</keyname><forenames>Fabrizio Maria</forenames></author><author><keyname>Di Francescomarino</keyname><forenames>Chiara</forenames></author><author><keyname>Dumas</keyname><forenames>Marlon</forenames></author><author><keyname>Ghidini</keyname><forenames>Chiara</forenames></author></authors><title>Predictive Monitoring of Business Processes</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern information systems that support complex business processes generally
maintain significant amounts of process execution data, particularly records of
events corresponding to the execution of activities (event logs). In this
paper, we present an approach to analyze such event logs in order to
predictively monitor business goals during business process execution. At any
point during an execution of a process, the user can define business goals in
the form of linear temporal logic rules. When an activity is being executed,
the framework identifies input data values that are more (or less) likely to
lead to the achievement of each business goal. Unlike reactive compliance
monitoring approaches that detect violations only after they have occurred, our
predictive monitoring approach provides early advice so that users can steer
ongoing process executions towards the achievement of business goals. In other
words, violations are predicted (and potentially prevented) rather than merely
detected. The approach has been implemented in the ProM process mining toolset
and validated on a real-life log pertaining to the treatment of cancer patients
in a large hospital.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4875</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4875</id><created>2013-12-17</created><authors><author><keyname>Roncal</keyname><forenames>William Gray</forenames></author><author><keyname>Koterba</keyname><forenames>Zachary H.</forenames></author><author><keyname>Mhembere</keyname><forenames>Disa</forenames></author><author><keyname>Kleissas</keyname><forenames>Dean M.</forenames></author><author><keyname>Vogelstein</keyname><forenames>Joshua T.</forenames></author><author><keyname>Burns</keyname><forenames>Randal</forenames></author><author><keyname>Bowles</keyname><forenames>Anita R.</forenames></author><author><keyname>Donavos</keyname><forenames>Dimitrios K.</forenames></author><author><keyname>Ryman</keyname><forenames>Sephira</forenames></author><author><keyname>Jung</keyname><forenames>Rex E.</forenames></author><author><keyname>Wu</keyname><forenames>Lei</forenames></author><author><keyname>Calhoun</keyname><forenames>Vince</forenames></author><author><keyname>Vogelstein</keyname><forenames>R. Jacob</forenames></author></authors><title>MIGRAINE: MRI Graph Reliability Analysis and Inference for Connectomics</title><categories>q-bio.QM cs.CE</categories><comments>Published as part of 2013 IEEE GlobalSIP conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, connectomes (e.g., functional or structural brain graphs) can be
estimated in humans at $\approx 1~mm^3$ scale using a combination of diffusion
weighted magnetic resonance imaging, functional magnetic resonance imaging and
structural magnetic resonance imaging scans. This manuscript summarizes a
novel, scalable implementation of open-source algorithms to rapidly estimate
magnetic resonance connectomes, using both anatomical regions of interest
(ROIs) and voxel-size vertices. To assess the reliability of our pipeline, we
develop a novel nonparametric non-Euclidean reliability metric. Here we provide
an overview of the methods used, demonstrate our implementation, and discuss
available user extensions. We conclude with results showing the efficacy and
reliability of the pipeline over previous state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4892</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4892</id><created>2013-12-17</created><authors><author><keyname>Wytock</keyname><forenames>Matt</forenames></author><author><keyname>Kolter</keyname><forenames>J. Zico</forenames></author></authors><title>A Fast Algorithm for Sparse Controller Design</title><categories>math.OC cs.DC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of designing sparse control laws for large-scale systems
by directly minimizing an infinite horizon quadratic cost with an $\ell_1$
penalty on the feedback controller gains. Our focus is on an improved algorithm
that allows us to scale to large systems (i.e. those where sparsity is most
useful) with convergence times that are several orders of magnitude faster than
existing algorithms. In particular, we develop an efficient proximal Newton
method which minimizes per-iteration cost with a coordinate descent active set
approach and fast numerical solutions to the Lyapunov equations. Experimentally
we demonstrate the appeal of this approach on synthetic examples and real power
networks significantly larger than those previously considered in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4894</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4894</id><created>2013-12-17</created><updated>2014-04-14</updated><authors><author><keyname>Gong</keyname><forenames>Yunchao</forenames></author><author><keyname>Jia</keyname><forenames>Yangqing</forenames></author><author><keyname>Leung</keyname><forenames>Thomas</forenames></author><author><keyname>Toshev</keyname><forenames>Alexander</forenames></author><author><keyname>Ioffe</keyname><forenames>Sergey</forenames></author></authors><title>Deep Convolutional Ranking for Multilabel Image Annotation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multilabel image annotation is one of the most important challenges in
computer vision with many real-world applications. While existing work usually
use conventional visual features for multilabel annotation, features based on
Deep Neural Networks have shown potential to significantly boost performance.
In this work, we propose to leverage the advantage of such features and analyze
key components that lead to better performances. Specifically, we show that a
significant performance gain could be obtained by combining convolutional
architectures with approximate top-$k$ ranking objectives, as thye naturally
fit the multilabel tagging problem. Our experiments on the NUS-WIDE dataset
outperforms the conventional visual features by about 10%, obtaining the best
reported performance in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4895</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4895</id><created>2013-12-17</created><authors><author><keyname>Freris</keyname><forenames>Nikolaos M.</forenames></author><author><keyname>&#xd6;&#xe7;al</keyname><forenames>Orhan</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>Recursive Compressed Sensing</title><categories>stat.ML cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><msc-class>94</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a recursive algorithm for performing compressed sensing on
streaming data. The approach consists of a) recursive encoding, where we sample
the input stream via overlapping windowing and make use of the previous
measurement in obtaining the next one, and b) recursive decoding, where the
signal estimate from the previous window is utilized in order to achieve faster
convergence in an iterative optimization scheme applied to decode the new one.
To remove estimation bias, a two-step estimation procedure is proposed
comprising support set detection and signal amplitude estimation. Estimation
accuracy is enhanced by a non-linear voting method and averaging estimates over
multiple windows. We analyze the computational complexity and estimation error,
and show that the normalized error variance asymptotically goes to zero for
sublinear sparsity. Our simulation results show speed up of an order of
magnitude over traditional CS, while obtaining significantly lower
reconstruction error under mild conditions on the signal magnitudes and the
noise level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4917</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4917</id><created>2013-12-17</created><updated>2014-03-03</updated><authors><author><keyname>Lescanne</keyname><forenames>Pierre</forenames><affiliation>LIP</affiliation></author></authors><title>An exercise on streams: convergence acceleration</title><categories>cs.NA cs.LO cs.PL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents convergence acceleration, a method for computing
efficiently the limit of numerical sequences as a typical application of
streams and higher-order functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4920</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4920</id><created>2013-12-17</created><authors><author><keyname>Lannes</keyname><forenames>Andr&#xe9;</forenames><affiliation>Supelec</affiliation></author><author><keyname>Prieur</keyname><forenames>Jean-Louis</forenames><affiliation>UPS-OMP-IRAP</affiliation></author></authors><title>Integer-ambiguity resolution in astronomy and geodesy</title><categories>cs.DM astro-ph.IM math.OC</categories><comments>12 pages. Soumis et accept\'e pour publication dans &quot;Astronomische
  Nachrichten&quot;</comments><proxy>ccsd</proxy><doi>10.1002/asna.201211947</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent theoretical developments in astronomical aperture synthesis have
revealed the existence of integer-ambiguity problems. Those problems, which
appear in the self-calibration procedures of radio imaging, have been shown to
be similar to the nearest-lattice point (NLP) problems encountered in
high-precision geodetic positioning, and in global navigation satellite
systems. In this paper, we analyse the theoretical aspects of the matter and
propose new methods for solving those NLP problems. The related optimization
aspects concern both the preconditioning stage, and the discrete-search stage
in which the integer ambiguities are finally fixed. Our algorithms, which are
described in an explicit manner, can easily be implemented. They lead to
substantial gains in the processing time of both stages. Their efficiency was
shown via intensive numerical tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4921</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4921</id><created>2013-12-17</created><updated>2014-04-25</updated><authors><author><keyname>Akdeniz</keyname><forenames>Mustafa Riza</forenames></author><author><keyname>Liu</keyname><forenames>Yuanpeng</forenames></author><author><keyname>Samimi</keyname><forenames>Mathew K.</forenames></author><author><keyname>Sun</keyname><forenames>Shu</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Millimeter Wave Channel Modeling and Cellular Capacity Evaluation</title><categories>cs.NI</categories><comments>15 pages, 13 figures. arXiv admin note: substantial text overlap with
  arXiv:1304.3963</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the severe spectrum shortage in conventional cellular bands, millimeter
wave (mmW) frequencies between 30 and 300 GHz have been attracting growing
attention as a possible candidate for next-generation micro- and picocellular
wireless networks. The mmW bands offer orders of magnitude greater spectrum
than current cellular allocations and enable very high-dimensional antenna
arrays for further gains via beamforming and spatial multiplexing. This paper
uses recent real-world measurements at 28 and 73 GHz in New York City to derive
detailed spatial statistical models of the channels and uses these models to
provide a realistic assessment of mmW micro- and picocellular networks in a
dense urban deployment. Statistical models are derived for key channel
parameters including the path loss, number of spatial clusters, angular
dispersion and outage. It is found that, even in highly non-line-of-sight
environments, strong signals can be detected 100 m to 200 m from potential cell
sites, potentially with multiple clusters to support spatial multiplexing.
Moreover, a system simulation based on the models predicts that mmW systems can
offer an order of magnitude increase in capacity over current state-of-the-art
4G cellular networks with no increase in cell density from current urban
deployments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4931</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4931</id><created>2013-12-17</created><authors><author><keyname>Sani</keyname><forenames>Ardalan Amiri</forenames></author><author><keyname>Boos</keyname><forenames>Kevin</forenames></author><author><keyname>Yun</keyname><forenames>Min Hong</forenames></author><author><keyname>Zhong</keyname><forenames>Lin</forenames></author></authors><title>Rio: A System Solution for Sharing I/O between Mobile Systems</title><categories>cs.OS</categories><report-no>Rice University ECE Technical Report 2013-12-17</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile systems are equipped with a diverse collection of I/O devices,
including cameras, microphones, sensors, and modems. There exist many novel use
cases for allowing an application on one mobile system to utilize I/O devices
from another. This paper presents Rio, an I/O sharing solution that supports
unmodified applications and exposes all the functionality of an I/O device for
sharing. Rio's design is common to many classes of I/O devices, thus
significantly reducing the engineering effort to support new I/O devices. Our
implementation of Rio on Android consists of 6700 total lines of code and
supports four I/O classes with fewer than 450 class-specific lines of code. Rio
also supports I/O sharing between mobile systems of different form factors,
including smartphones and tablets. We show that Rio achieves performance close
to that of local I/O for audio, sensors, and modems, but suffers noticeable
performance degradation for camera due to network throughput limitations
between the two systems, which is likely to be alleviated by emerging wireless
standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4932</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4932</id><created>2013-12-17</created><authors><author><keyname>Akanbi</keyname><forenames>Adeyinka K.</forenames></author><author><keyname>Kumar</keyname><forenames>Santosh</forenames></author><author><keyname>Fidelis</keyname><forenames>Uwaya</forenames></author></authors><title>Application of Remote Sensing, GIS and GPS for efficient Urban
  Management Plan, A case study of part of Hyderabad city</title><categories>cs.CY</categories><comments>14 Pages, 9 Figures</comments><journal-ref>Novus International Journal of Engineering &amp; Technology, 2(4), pp.
  1-14</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Role of urban planning and management in Hyderabad is becoming more and more
crucial due to the dramatic increase in urban population and allied urban
problems. Hyderabad is experiencing a rapid urbanization rate. Urbanization
contributes many advantages in terms of economics, but if uncontrolled, would
produce negative consequences to the physical, social and natural environment.
With the advancement of GIS, which considerably influenced the dynamic nature
of urban and regional planning, incorporation of GIS becomes imperative for
better and improved decision-making in urban planning and management. It offers
a solution to the urban problems and decision-making, which is more reliant to
the real-time spatial modelling. The integration of Geographical Information
System (GIS) and Remote Sensing has provided a tool, which can contribute to
much clearer understanding of real planning problems as well as prescriptive
planning scenarios to enhance the quality of urban planning and management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4967</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4967</id><created>2013-12-17</created><updated>2014-06-26</updated><authors><author><keyname>Wuhrer</keyname><forenames>Stefanie</forenames></author><author><keyname>Pishchulin</keyname><forenames>Leonid</forenames></author><author><keyname>Brunton</keyname><forenames>Alan</forenames></author><author><keyname>Shu</keyname><forenames>Chang</forenames></author><author><keyname>Lang</keyname><forenames>Jochen</forenames></author></authors><title>Estimation of Human Body Shape and Posture Under Clothing</title><categories>cs.CV cs.GR</categories><comments>23 pages, 11 figures</comments><journal-ref>Computer Vision and Image Understanding, 127, pp. 31-42, 2014</journal-ref><doi>10.1016/j.cviu.2014.06.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the body shape and posture of a dressed human subject in motion
represented as a sequence of (possibly incomplete) 3D meshes is important for
virtual change rooms and security. To solve this problem, statistical shape
spaces encoding human body shape and posture variations are commonly used to
constrain the search space for the shape estimate. In this work, we propose a
novel method that uses a posture-invariant shape space to model body shape
variation combined with a skeleton-based deformation to model posture
variation. Our method can estimate the body shape and posture of both static
scans and motion sequences of dressed human body scans. In case of motion
sequences, our method takes advantage of motion cues to solve for a single body
shape estimate along with a sequence of posture estimates. We apply our
approach to both static scans and motion sequences and demonstrate that using
our method, higher fitting accuracy is achieved than when using a variant of
the popular SCAPE model as statistical model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4986</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4986</id><created>2013-12-17</created><authors><author><keyname>Smith</keyname><forenames>Michael R.</forenames></author><author><keyname>Martinez</keyname><forenames>Tony</forenames></author></authors><title>A Comparative Evaluation of Curriculum Learning with Filtering and
  Boosting</title><categories>cs.LG</categories><comments>19 pages, 2 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Not all instances in a data set are equally beneficial for inferring a model
of the data. Some instances (such as outliers) are detrimental to inferring a
model of the data. Several machine learning techniques treat instances in a
data set differently during training such as curriculum learning, filtering,
and boosting. However, an automated method for determining how beneficial an
instance is for inferring a model of the data does not exist. In this paper, we
present an automated method that orders the instances in a data set by
complexity based on the their likelihood of being misclassified (instance
hardness). The underlying assumption of this method is that instances with a
high likelihood of being misclassified represent more complex concepts in a
data set. Ordering the instances in a data set allows a learning algorithm to
focus on the most beneficial instances and ignore the detrimental ones. We
compare ordering the instances in a data set in curriculum learning, filtering
and boosting. We find that ordering the instances significantly increases
classification accuracy and that filtering has the largest impact on
classification accuracy. On a set of 52 data sets, ordering the instances
increases the average accuracy from 81% to 84%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4993</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4993</id><created>2013-12-17</created><updated>2014-02-15</updated><authors><author><keyname>Paulino</keyname><forenames>Herv&#xe9;</forenames></author><author><keyname>Marques</keyname><forenames>Eduardo</forenames></author></authors><title>Heterogeneous Programming with Single Operation Multiple Data</title><categories>cs.DC</categories><comments>28 pages, Preprint of the article accepted for publication in a
  special issue of Journal of Computer and System Sciences{\dag} dedicated to
  the 14th IEEE International Conference on High Performance Computing and
  Communication (HPCC 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneity is omnipresent in today's commodity computational systems,
which comprise at least one multi-core Central Processing Unit (CPU) and one
Graphics Processing Unit (GPU). Nonetheless, all this computing power is not
being exploited in mainstream computing, as the programming of these systems
entails many details of the underlying architecture and of its distinct
execution models. Current research on parallel programming is addressing these
issues but, still, the systems' heterogeneity is exposed at language level.
This paper proposes a uniform framework, grounded on the Single Operation
Multiple Data model, for the programming of such heterogeneous systems. The
model is declarative, empowering the compiler to generate code for multiple
architectures from the same source. To this extent, we designed a simple
extension of the Java programming language that embodies the model, and
developed a compiler that generates code for both multi-core CPUs and GPUs. A
performance evaluation attests the validity of the approach that, despite being
based on a simple programming model, is able to deliver performance gains on
par with hand-tuned data parallel multi-threaded Java applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5021</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5021</id><created>2013-12-17</created><authors><author><keyname>Qin</keyname><forenames>Zhen</forenames></author><author><keyname>Petricek</keyname><forenames>Vaclav</forenames></author><author><keyname>Karampatziakis</keyname><forenames>Nikos</forenames></author><author><keyname>Li</keyname><forenames>Lihong</forenames></author><author><keyname>Langford</keyname><forenames>John</forenames></author></authors><title>Efficient Online Bootstrapping for Large Scale Learning</title><categories>cs.LG</categories><comments>5 pages, appeared at Big Learning Workshop at Neural Information
  Processing Systems 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bootstrapping is a useful technique for estimating the uncertainty of a
predictor, for example, confidence intervals for prediction. It is typically
used on small to moderate sized datasets, due to its high computation cost.
This work describes a highly scalable online bootstrapping strategy,
implemented inside Vowpal Wabbit, that is several times faster than traditional
strategies. Our experiments indicate that, in addition to providing a black
box-like method for estimating uncertainty, our implementation of online
bootstrapping may also help to train models with better prediction performance
due to model averaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5023</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5023</id><created>2013-12-17</created><authors><author><keyname>Wytock</keyname><forenames>Matt</forenames></author><author><keyname>Kolter</keyname><forenames>J. Zico</forenames></author></authors><title>Contextually Supervised Source Separation with Application to Energy
  Disaggregation</title><categories>stat.ML cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new framework for single-channel source separation that lies
between the fully supervised and unsupervised setting. Instead of supervision,
we provide input features for each source signal and use convex methods to
estimate the correlations between these features and the unobserved signal
decomposition. We analyze the case of $\ell_2$ loss theoretically and show that
recovery of the signal components depends only on cross-correlation between
features for different signals, not on correlations between features for the
same signal. Contextually supervised source separation is a natural fit for
domains with large amounts of data but no explicit supervision; our motivating
application is energy disaggregation of hourly smart meter data (the separation
of whole-home power signals into different energy uses). Here we apply
contextual supervision to disaggregate the energy usage of thousands homes over
four years, a significantly larger scale than previously published efforts, and
demonstrate on synthetic data that our method outperforms the unsupervised
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5029</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5029</id><created>2013-12-17</created><updated>2014-02-25</updated><authors><author><keyname>Feng</keyname><forenames>Ruyong</forenames></author></authors><title>Hrushovski's Algorithm for Computing the Galois Group of a Linear
  Differential Equation</title><categories>cs.SC</categories><comments>27 pages</comments><msc-class>12H05</msc-class><acm-class>I.1.2</acm-class><doi>10.1016/j.aam.2015.01.001</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a detailed and simplified version of Hrushovski's algorithm that
determines the Galois group of a linear differential equation. There are three
major ingredients in this algorithm. The first is to look for a degree bound
for proto-Galois groups, which enables one to compute one of them. The second
is to determine the identity component of the Galois group that is the pullback
of a torus to the proto-Galois group. The third is to recover the Galois group
from its identity component and a finite Galois group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5033</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5033</id><created>2013-12-17</created><authors><author><keyname>Fujiwara</keyname><forenames>Tomofumi</forenames></author><author><keyname>Kamegawa</keyname><forenames>Tetsushi</forenames></author><author><keyname>Gofuku</keyname><forenames>Akio</forenames></author></authors><title>Evaluation of Plane Detection with RANSAC According to Density of 3D
  Point Clouds</title><categories>cs.RO cs.CV</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have implemented a method that detects planar regions from 3D scan data
using Random Sample Consensus (RANSAC) algorithm to address the issue of a
trade-off between the scanning speed and the point density of 3D scanning.
However, the limitation of the implemented method has not been clear yet. In
this paper, we conducted an additional experiment to evaluate the implemented
method by changing its parameter and environments in both high and low point
density data. As a result, the number of detected planes in high point density
data was different from that in low point density data with the same parameter
value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5035</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5035</id><created>2013-12-17</created><updated>2014-06-07</updated><authors><author><keyname>Gong</keyname><forenames>Neil Zhenqiang</forenames></author><author><keyname>Frank</keyname><forenames>Mario</forenames></author><author><keyname>Mittal</keyname><forenames>Prateek</forenames></author></authors><title>SybilBelief: A Semi-supervised Learning Approach for Structure-based
  Sybil Detection</title><categories>cs.CR cs.SI</categories><comments>12 pages</comments><journal-ref>IEEE Transactions on Information Forensics and Security, 9(6),
  2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sybil attacks are a fundamental threat to the security of distributed
systems. Recently, there has been a growing interest in leveraging social
networks to mitigate Sybil attacks. However, the existing approaches suffer
from one or more drawbacks, including bootstrapping from either only known
benign or known Sybil nodes, failing to tolerate noise in their prior knowledge
about known benign or Sybil nodes, and being not scalable.
  In this work, we aim to overcome these drawbacks. Towards this goal, we
introduce SybilBelief, a semi-supervised learning framework, to detect Sybil
nodes. SybilBelief takes a social network of the nodes in the system, a small
set of known benign nodes, and, optionally, a small set of known Sybils as
input. Then SybilBelief propagates the label information from the known benign
and/or Sybil nodes to the remaining nodes in the system.
  We evaluate SybilBelief using both synthetic and real world social network
topologies. We show that SybilBelief is able to accurately identify Sybil nodes
with low false positive rates and low false negative rates. SybilBelief is
resilient to noise in our prior knowledge about known benign and Sybil nodes.
Moreover, SybilBelief performs orders of magnitudes better than existing Sybil
classification mechanisms and significantly better than existing Sybil ranking
mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5045</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5045</id><created>2013-12-18</created><authors><author><keyname>Gogna</keyname><forenames>Anupriya</forenames></author><author><keyname>Tayal</keyname><forenames>Akash</forenames></author></authors><title>Comparative analysis of evolutionary algorithms for image enhancement</title><categories>cs.CV cs.NE</categories><journal-ref>International Journal of Metaheuristics Volume 2 Issue 1, July
  2012 Pages 80-100</journal-ref><doi>10.1504/IJMHEUR.2012.048219</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary algorithms are metaheuristic techniques that derive inspiration
from the natural process of evolution. They can efficiently solve (generate
acceptable quality of solution in reasonable time) complex optimization
(NP-Hard) problems. In this paper, automatic image enhancement is considered as
an optimization problem and three evolutionary algorithms (Genetic Algorithm,
Differential Evolution and Self Organizing Migration Algorithm) are employed to
search for an optimum solution. They are used to find an optimum parameter set
for an image enhancement transfer function. The aim is to maximize a fitness
criterion which is a measure of image contrast and the visibility of details in
the enhanced image. The enhancement results obtained using all three
evolutionary algorithms are compared amongst themselves and also with the
output of histogram equalization method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5047</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5047</id><created>2013-12-18</created><updated>2015-01-15</updated><authors><author><keyname>Ozyesil</keyname><forenames>Onur</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author><author><keyname>Basri</keyname><forenames>Ronen</forenames></author></authors><title>Stable Camera Motion Estimation Using Convex Programming</title><categories>cs.CV</categories><comments>40 pages, 12 figures, 6 tables; notation and some unclear parts
  updated, some typos corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the inverse problem of estimating n locations $t_1, ..., t_n$ (up to
global scale, translation and negation) in $R^d$ from noisy measurements of a
subset of the (unsigned) pairwise lines that connect them, that is, from noisy
measurements of $\pm (t_i - t_j)/\|t_i - t_j\|$ for some pairs (i,j) (where the
signs are unknown). This problem is at the core of the structure from motion
(SfM) problem in computer vision, where the $t_i$'s represent camera locations
in $R^3$. The noiseless version of the problem, with exact line measurements,
has been considered previously under the general title of parallel rigidity
theory, mainly in order to characterize the conditions for unique realization
of locations. For noisy pairwise line measurements, current methods tend to
produce spurious solutions that are clustered around a few locations. This
sensitivity of the location estimates is a well-known problem in SfM,
especially for large, irregular collections of images.
  In this paper we introduce a semidefinite programming (SDP) formulation,
specially tailored to overcome the clustering phenomenon. We further identify
the implications of parallel rigidity theory for the location estimation
problem to be well-posed, and prove exact (in the noiseless case) and stable
location recovery results. We also formulate an alternating direction method to
solve the resulting semidefinite program, and provide a distributed version of
our formulation for large numbers of locations. Specifically for the camera
location estimation problem, we formulate a pairwise line estimation method
based on robust camera orientation and subspace estimation. Lastly, we
demonstrate the utility of our algorithm through experiments on real images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5050</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5050</id><created>2013-12-18</created><authors><author><keyname>Chen</keyname><forenames>Liang</forenames></author><author><keyname>Zhou</keyname><forenames>Yipeng</forenames></author><author><keyname>Chiu</keyname><forenames>Dah Ming</forenames></author></authors><title>Fake View Analytics in Online Video Services</title><categories>cs.MM cs.CR</categories><comments>25 pages, 15 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Online video-on-demand(VoD) services invariably maintain a view count for
each video they serve, and it has become an important currency for various
stakeholders, from viewers, to content owners, advertizers, and the online
service providers themselves. There is often significant financial incentive to
use a robot (or a botnet) to artificially create fake views. How can we detect
the fake views? Can we detect them (and stop them) using online algorithms as
they occur? What is the extent of fake views with current VoD service
providers? These are the questions we study in the paper. We develop some
algorithms and show that they are quite effective for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5058</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5058</id><created>2013-12-18</created><authors><author><keyname>Fagerholm</keyname><forenames>Fabian</forenames></author><author><keyname>Oza</keyname><forenames>Nilay</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>A Platform for Teaching Applied Distributed Software Development: The
  Ongoing Journey of the Helsinki Software Factory</title><categories>cs.SE</categories><comments>5 pages. The final publication is available at
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=6635237</comments><journal-ref>In Proceedings of the 3rd International Workshop on Collaborative
  Teaching of Globally Distributed Software Development (CTGDSD 2013), pages
  1-5, San Francisco, United States, May 25 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Teaching distributed software development (DSD) in project courses where
student teams are geographically distributed promises several benefits. One
main benefit is that in contrast to traditional classroom courses, students can
experience the effects of distribution and the mechanisms for coping with
distribution by themselves, therefore understanding their relevance for
software development. They can thus learn to take more care of distribution
challenges and risks when starting to develop software in industry. However,
providing a sustainable environment for such project courses is difficult. A
development environment is needed that can connect to different distributed
teams and an ongoing routine to conduct such courses needs to be established.
This article sketches a picture of the Software Factory, a platform that
supports teaching distributed student projects and that has now been
operational for more than three years. We describe the basic steps of
conducting Software Factory projects, and portray experiences from past factory
projects. In addition, we provide a short overview of related approaches and
future activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5061</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5061</id><created>2013-12-18</created><authors><author><keyname>Kuhrmann</keyname><forenames>Marco</forenames></author><author><keyname>Fern&#xe1;ndez</keyname><forenames>Daniel M&#xe9;ndez</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Teaching Software Process Modeling</title><categories>cs.SE</categories><comments>10 pages. The final publication is available at
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=6606665</comments><journal-ref>Proceedings of the 35th International Conference on Software
  Engineering (ICSE), pages 1138-1147, San Francisco, United States, May 2013</journal-ref><doi>10.1109/ICSE.2013.6606665</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most university curricula consider software processes to be on the fringes of
software engineering (SE). Students are told there exists a plethora of
software processes ranging from RUP over V-shaped processes to agile methods.
Furthermore, the usual students' programming tasks are of a size that either
one student or a small group of students can manage the work. Comprehensive
processes being essential for large companies in terms of reflecting the
organization structure, coordinating teams, or interfaces to business processes
such as contracting or sales, are complex and hard to teach in a lecture, and,
therefore, often out of scope. We experienced tutorials on using Java or C#, or
on developing applications for the iPhone to gather more attention by students,
simply speaking, as these are more fun for them. So, why should students spend
their time in software processes? From our experiences and the discussions with
a variety of industrial partners, we learned that students often face trouble
when taking their first &quot;real&quot; jobs, even if the company is organized in a lean
or agile shape. Therefore, we propose to include software processes more
explicitly into the SE curricula. We designed and implemented a course at
Master's level in which students learn why software processes are necessary,
and how they can be analyzed, designed, implemented, and continuously improved.
In this paper, we present our course's structure, its goals, and corresponding
teaching methods. We evaluate the course and further discuss our experiences so
that lecturers and researchers can directly use our lessons learned in their
own curricula.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5067</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5067</id><created>2013-12-18</created><authors><author><keyname>Das</keyname><forenames>Anita</forenames></author><author><keyname>Suresh</keyname><forenames>P.</forenames></author><author><keyname>Subrahmanya</keyname><forenames>S. V.</forenames></author></authors><title>Rainbow path and color degree in edge colored graphs</title><categories>cs.DM math.CO</categories><comments>4 pages</comments><msc-class>68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be an edge colored graph. A {\it}{rainbow path} in $G$ is a path in
which all the edges are colored with distinct colors. Let $d^c(v)$ be the color
degree of a vertex $v$ in $G$, i.e. the number of distinct colors present on
the edges incident on the vertex $v$. Let $t$ be the maximum length of a
rainbow path in $G$. Chen and Li showed that if $d^c \geq k$, for every vertex
$v$ of $G$, then $t \geq \left \lceil \frac{3 k}{5}\right \rceil + 1$ (Long
heterochromatic paths in edge-colored graphs, The Electronic Journal of
Combinatorics 12 (2005), # R33, Pages:1-33.) Unfortunately, proof by Chen and
Li is very long and comes to about 23 pages in the journal version. Chen and Li
states in their paper that it was conjectured by Akira Saito, that $t \ge \left
\lceil \frac {2k} {3} \right \rceil$. They also states in their paper that they
believe $t \ge k - c$ for some constant $c$.
  In this note, we give a short proof to show that $t \ge \left \lceil \frac{3
k}{5}\right \rceil$, using an entirely different method. Our proof is only
about 2 pages long. The draw-back is that our bound is less by 1, than the
bound given by Chen and Li. We hope that the new approach adopted in this paper
would eventually lead to the settlement of the conjectures by Saito and/or Chen
and Li.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5080</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5080</id><created>2013-12-18</created><authors><author><keyname>John</keyname><forenames>Wolfgang</forenames></author><author><keyname>Pentikousis</keyname><forenames>Kostas</forenames></author><author><keyname>Agapiou</keyname><forenames>George</forenames></author><author><keyname>Jacob</keyname><forenames>Eduardo</forenames></author><author><keyname>Kind</keyname><forenames>Mario</forenames></author><author><keyname>Manzalini</keyname><forenames>Antonio</forenames></author><author><keyname>Risso</keyname><forenames>Fulvio</forenames></author><author><keyname>Staessens</keyname><forenames>Dimitri</forenames></author><author><keyname>Steinert</keyname><forenames>Rebecca</forenames></author><author><keyname>Meirosu</keyname><forenames>Catalin</forenames></author></authors><title>Research Directions in Network Service Chaining</title><categories>cs.NI</categories><comments>Presented at the 2013 &quot;Software Defined Networking for Future
  Networks and Services&quot; (SDN4FNS) conference in Trento, Italy
  (http://sites.ieee.org/sdn4fns/). Will be published in the SDN4FNS
  proceedings at IEEE</comments><acm-class>C.2.1; C.2.3; C.2.4</acm-class><doi>10.1109/SDN4FNS.2013.6702549</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network Service Chaining (NSC) is a service deployment concept that promises
increased flexibility and cost efficiency for future carrier networks. NSC has
received considerable attention in the standardization and research communities
lately. However, NSC is largely undefined in the peer-reviewed literature. In
fact, a literature review reveals that the role of NSC enabling technologies is
up for discussion, and so are the key research challenges lying ahead. This
paper addresses these topics by motivating our research interest towards
advanced dynamic NSC and detailing the main aspects to be considered in the
context of carrier-grade telecommunication networks. We present design
considerations and system requirements alongside use cases that illustrate the
advantages of adopting NSC. We detail prominent research challenges during the
typical lifecycle of a network service chain in an operational
telecommunications network, including service chain description, programming,
deployment, and debugging, and summarize our security considerations. We
conclude this paper with an outlook on future work in this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5096</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5096</id><created>2013-12-18</created><authors><author><keyname>Tchao</keyname><forenames>E. T.</forenames></author><author><keyname>Diawuo</keyname><forenames>K.</forenames></author><author><keyname>Ofosu</keyname><forenames>W. K.</forenames></author><author><keyname>Affum</keyname><forenames>E.</forenames></author></authors><title>Analysis of MIMO Systems used in planning a 4G-WiMAX Network in Ghana</title><categories>cs.IT math.IT</categories><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA), 4(7), 2013</journal-ref><doi>10.14569/IJACSA.2013.040728</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing demand for mobile data services, Broadband Wireless
Access (BWA) is emerging as one of the fastest growing areas within mobile
communications. Innovative wireless communication systems, such as WiMAX, are
expected to offer highly reliable broadband radio access in order to meet the
increasing demands of emerging high speed data and multimedia services. In
Ghana, deployment of WiMAX technology has recently begun. Planning these high
capacity networks in the presence of multiple interferences in order to achieve
the aim of enabling users enjoy cheap and reliable internet services is a
critical design issue. This paper has used a deterministic approach for
simulating the Bit-Error-Rate (BER) of initial MIMO antenna configurations
which were considered in deploying a high capacity 4G-WiMAX network in Ghana.
The radiation pattern of the antenna used in the deploying the network has been
simulated with Genex-Unet and NEC and results presented. An adaptive 4x4 MIMO
antenna configuration with optimally suppressed sidelobes has been suggested
for future network deployment since the adaptive 2x2 MIMO antenna
configuration, which was used in the initial network deployment provides poor
estimates for average BER performance as compared to 4x4 antenna configuration
which seem less affected in the presence of multiple interferers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5097</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5097</id><created>2013-12-18</created><authors><author><keyname>Darer</keyname><forenames>Alexander</forenames></author><author><keyname>Lewis</keyname><forenames>Peter</forenames></author></authors><title>A Cellular Automaton Based Controller for a Ms. Pac-Man Agent</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video games can be used as an excellent test bed for Artificial Intelligence
(AI) techniques. They are challenging and non-deterministic, this makes it very
difficult to write strong AI players. An example of such a video game is Ms.
Pac-Man. In this paper we will outline some of the previous techniques used to
build AI controllers for Ms. Pac-Man as well as presenting a new and novel
solution. My technique utilises a Cellular Automaton (CA) to build a
representation of the environment at each time step of the game. The basis of
the representation is a 2-D grid of cells. Each cell has a state and a set of
rules which determine whether or not that cell will dominate (i.e. pass its
state value onto) adjacent cells at each update. Once a certain number of
update iterations have been completed, the CA represents the state of the
environment in the game, the goals and the dangers. At this point, Ms. Pac-Man
will decide her next move based only on her adjacent cells, that is to say, she
has no knowledge of the state of the environment as a whole, she will simply
follow the strongest path. This technique shows promise and allows the
controller to achieve high scores in a live game, the behaviour it exhibits is
interesting and complex. Moreover, this behaviour is produced by using very
simple rules which are applied many times to each cell in the grid. Simple
local interactions with complex global results are truly achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5105</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5105</id><created>2013-12-18</created><authors><author><keyname>Bonchi</keyname><forenames>Francesco</forenames></author><author><keyname>Garc&#xed;a-Soriano</keyname><forenames>David</forenames></author><author><keyname>Kutzkov</keyname><forenames>Konstantin</forenames></author></authors><title>Local correlation clustering</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlation clustering is perhaps the most natural formulation of clustering.
Given $n$ objects and a pairwise similarity measure, the goal is to cluster the
objects so that, to the best possible extent, similar objects are put in the
same cluster and dissimilar objects are put in different clusters. Despite its
theoretical appeal, the practical relevance of correlation clustering still
remains largely unexplored, mainly due to the fact that correlation clustering
requires the $\Theta(n^2)$ pairwise similarities as input.
  In this paper we initiate the investigation into \emph{local} algorithms for
correlation clustering. In \emph{local correlation clustering} we are given the
identifier of a single object and we want to return the cluster to which it
belongs in some globally consistent near-optimal clustering, using a small
number of similarity queries. Local algorithms for correlation clustering open
the door to \emph{sublinear-time} algorithms, which are particularly useful
when the similarity between items is costly to compute, as it is often the case
in many practical application domains. They also imply $(i)$ distributed and
streaming clustering algorithms, $(ii)$ constant-time estimators and testers
for cluster edit distance, and $(iii)$ property-preserving parallel
reconstruction algorithms for clusterability.
  Specifically, we devise a local clustering algorithm attaining a $(3,
\varepsilon)$-approximation in time $O(1/\varepsilon^2)$ independently of the
dataset size. An explicit approximate clustering for all objects can be
produced in time $O(n/\varepsilon)$ (which is provably optimal). We also
provide a fully additive $(1,\varepsilon)$-approximation with local query
complexity $poly(1/\varepsilon)$ and time complexity $2^{poly(1/\varepsilon)}$.
The latter yields the fastest polynomial-time approximation scheme for
correlation clustering known to date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5106</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5106</id><created>2013-12-18</created><authors><author><keyname>Ernvall</keyname><forenames>Toni</forenames></author></authors><title>Codes between MBR and MSR Points with Exact Repair Property</title><categories>cs.IT math.IT</categories><comments>21 pages, 16 figures, submitted to IEEE Transactions on Information
  Theory</comments><journal-ref>IEEE Transactions on Information Theory, vol. 60, issue 11, 2014,
  pages 6993-7005</journal-ref><doi>10.1109/TIT.2014.2351252</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper distributed storage systems with exact repair are studied. A
construction for regenerating codes between the minimum storage regenerating
(MSR) and the minimum bandwidth regenerating (MBR) points is given. To the best
of author's knowledge, no previous construction of exact-regenerating codes
between MBR and MSR points is done except in the work by Tian et al. On
contrast to their work, the methods used here are elementary.
  In this paper it is shown that in the case that the parameters $n$, $k$, and
$d$ are close to each other, the given construction is close to optimal when
comparing to the known functional repair capacity. This is done by showing that
when the distances of the parameters $n$, $k$, and $d$ are fixed but the actual
values approach to infinity, the fraction of the performance of constructed
codes with exact repair and the known capacity of codes with functional repair,
approaches to one. Also a simple variation of the constructed codes with almost
the same performance is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5109</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5109</id><created>2013-12-18</created><authors><author><keyname>Affum</keyname><forenames>E.</forenames></author><author><keyname>Tchao</keyname><forenames>E. T.</forenames></author><author><keyname>Diawuo</keyname><forenames>K.</forenames></author><author><keyname>Agyekum</keyname><forenames>K.</forenames></author></authors><title>Wideband Parameters Analysis and Validation for Indoor radio Channel at
  60/70/80GHz for Gigabit Wireless Communication employing Isotropic, Horn and
  Omni directional Antenna</title><categories>cs.NI</categories><comments>International Journal of Advanced Computer Science and
  Applications(IJACSA), Volume 4 Issue 6, 2013</comments><doi>10.14569/IJACSA.2013.040638</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, applications of millimeter (mm) waves for high-speed broadband
wireless local area network communication systems in indoor environment are
increasingly gaining recognition as it provides gigabit-speed wireless
communications with carrier-class performances over distances of a mile or more
due to spectrum availability and wider bandwidth requirements. Collectively
referred to as E-Band, the millimeter wave wireless technology present the
potential to offer bandwidth delivery comparable to that of fiber optic, but
without the financial and logistic challenges of deploying fiber. This paper
investigates the wideband parameters using the ray tracing technique for indoor
propagation systems with rms delay spread for Omnidirectional and Horn Antennas
for Bent Tunnel at 80GHz. The results obtained were 2.03 and 1.95 respectively,
besides, the normalized received power with 0:55?x10^8 excess delay at 70GHz
for Isotropic Antenna was at 0.97.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5111</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5111</id><created>2013-12-18</created><authors><author><keyname>Kowald</keyname><forenames>Dominik</forenames></author><author><keyname>Seitlinger</keyname><forenames>Paul</forenames></author><author><keyname>Trattner</keyname><forenames>Christoph</forenames></author><author><keyname>Ley</keyname><forenames>Tobias</forenames></author></authors><title>Long Time No See: The Probability of Reusing Tags as a Function of
  Frequency and Recency</title><categories>cs.IR</categories><acm-class>H.2.8; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a tag recommendation algorithm that mimics the
way humans draw on items in their long-term memory. This approach uses the
frequency and recency of previous tag assignments to estimate the probability
of reusing a particular tag. Using three real-world folksonomies gathered from
bookmarks in BibSonomy, CiteULike and Flickr, we show how adding a
time-dependent component outperforms conventional &quot;most popular tags&quot;
approaches and another existing and very effective but less theory-driven,
time-dependent recommendation mechanism. By combining our approach with a
simple resource-specific frequency analysis, our algorithm outperforms other
well-established algorithms, such as FolkRank, Pairwise Interaction Tensor
Factorization and Collaborative Filtering. We conclude that our approach
provides an accurate and computationally efficient model of a user's temporal
tagging behavior. We show how effective principles for information retrieval
can be designed and implemented if human memory processes are taken into
account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5124</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5124</id><created>2013-12-18</created><authors><author><keyname>Fogel</keyname><forenames>Paul</forenames></author></authors><title>Permuted NMF: A Simple Algorithm Intended to Minimize the Volume of the
  Score Matrix</title><categories>stat.AP cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-Negative Matrix Factorization, NMF, attempts to find a number of
archetypal response profiles, or parts, such that any sample profile in the
dataset can be approximated by a close profile among these archetypes or a
linear combination of these profiles. The non-negativity constraint is imposed
while estimating archetypal profiles, due to the non-negative nature of the
observed signal. Apart from non negativity, a volume constraint can be applied
on the Score matrix W to enhance the ability of learning parts of NMF. In this
report, we describe a very simple algorithm, which in effect achieves volume
minimization, although indirectly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5129</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5129</id><created>2013-12-18</created><updated>2013-12-19</updated><authors><author><keyname>Yin</keyname><forenames>Wenpeng</forenames></author><author><keyname>Sch&#xfc;tze</keyname><forenames>Hinrich</forenames></author></authors><title>Deep Learning Embeddings for Discontinuous Linguistic Units</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning embeddings have been successfully used for many natural
language processing problems. Embeddings are mostly computed for word forms
although a number of recent papers have extended this to other linguistic units
like morphemes and phrases. In this paper, we argue that learning embeddings
for discontinuous linguistic units should also be considered. In an
experimental evaluation on coreference resolution, we show that such embeddings
perform better than word form embeddings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5138</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5138</id><created>2013-12-14</created><authors><author><keyname>Song</keyname><forenames>Lei</forenames></author><author><keyname>Wang</keyname><forenames>Yongcai</forenames></author></authors><title>Locating Multiple Ultrasound Targets in Chorus</title><categories>cs.IT math.IT</categories><comments>9 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Ranging by Time of Arrival (TOA) of Narrow-band ultrasound (NBU) has been
widely used by many locating systems for its characteristics of low cost and
high accuracy. However, because it is hard to support code division multiple
access in narrowband signal, to track multiple targets, existing NBU-based
locating systems generally need to assign exclusive time slot to each target to
avoid the signal conflicts. Because the propagation speed of ultrasound is slow
in air, dividing exclusive time slots on a single channel causes the location
updating rate for each target rather low, leading to unsatisfied tracking
performances as the number of targets increases. In this paper, we investigated
a new multiple target locating method using NBU, called UltraChorus, which is
to locate multiple targets while allowing them sending NBU signals
simultaneously, i.e., in chorus mode. It can dramatically increase the location
updating rate. In particular, we investigated by both experiments and
theoretical analysis on the necessary and sufficient conditions for resolving
the conflicts of multiple NBU signals on a single channel, which is referred as
the conditions for chorus ranging and chorus locating. To tackle the difficulty
caused by the anonymity of the measured distances, we further developed
consistent position generation algorithm and probabilistic particle filter
algorithm}to label the distances by sources, to generate reasonable location
estimations, and to disambiguate the motion trajectories of the multiple
concurrent targets based on the anonymous distance measurements. Extensive
evaluations by both simulation and testbed were carried out, which verified the
effectiveness of our proposed theories and algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5148</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5148</id><created>2013-12-18</created><updated>2014-04-14</updated><authors><author><keyname>Lu</keyname><forenames>Xiaolu</forenames></author><author><keyname>Li</keyname><forenames>Dongxu</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author><author><keyname>Feng</keyname><forenames>Ling</forenames></author></authors><title>Object Selection under Team Context</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context-aware database has drawn increasing attention from both industry and
academia recently by taking users' current situation and environment into
consideration. However, most of the literature focus on individual context,
overlooking the team users. In this paper, we investigate how to integrate team
context into database query process to help the users' get top-ranked database
tuples and make the team more competitive. We introduce naive and optimized
query algorithm to select the suitable records and show that they output the
same results while the latter is more computational efficient. Extensive
empirical studies are conducted to evaluate the query approaches and
demonstrate their effectiveness and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5150</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5150</id><created>2013-12-18</created><authors><author><keyname>Heyn</keyname><forenames>Velten</forenames></author><author><keyname>Paschke</keyname><forenames>Adrian</forenames></author></authors><title>Semantic Jira - Semantic Expert Finder in the Bug Tracking Tool Jira</title><categories>cs.SE</categories><comments>published in proceedings of the 9th International Workshop on
  Semantic Web Enabled Software Engineering (SWESE2013), Berlin, Germany,
  December 2-5, 2013</comments><acm-class>K.6.3; D.2.5; F.4.1</acm-class><journal-ref>in Proceedings of the 9th International Workshop on Semantic Web
  Enabled Software Engineering (SWESE2013), Berlin, Germany, December 2-5, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The semantic expert recommender extension for the Jira bug tracking system
semantically searches for similar tickets in Jira and recommends experts and
links to existing organizational (Wiki) knowledge for each ticket. This helps
to avoid redundant work and supports the search and collaboration with experts
in the project management and maintenance phase based on semantically enriched
tickets in Jira.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5155</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5155</id><created>2013-12-16</created><authors><author><keyname>Esmaili</keyname><forenames>Kyumars Sheykh</forenames></author><author><keyname>Datta</keyname><forenames>Anwitaman</forenames></author></authors><title>On the Effectiveness of Polynomial Realization of Reed-Solomon Codes for
  Storage Systems</title><categories>cs.IT cs.PF math.IT</categories><comments>8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are different ways to realize Reed Solomon (RS) codes. While in the
storage community, using the generator matrices to implement RS codes is more
popular, in the coding theory community the generator polynomials are typically
used to realize RS codes. Prominent exceptions include HDFS-RAID, which uses
generator polynomial based erasure codes, and extends the Apache Hadoop's file
system.
  In this paper we evaluate the performance of an implementation of polynomial
realization of Reed-Solomon codes, along with our optimized version of it,
against that of a widely-used library (Jerasure) that implements the main
matrix realization alternatives. Our experimental study shows that despite
significant performance gains yielded by our optimizations, the polynomial
implementations' performance is constantly inferior to those of matrix
realization alternatives in general, and that of Cauchy bit matrices in
particular.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5162</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5162</id><created>2013-12-17</created><authors><author><keyname>Ariani</keyname><forenames>Ardina</forenames></author><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author><author><keyname>Syakti</keyname><forenames>Firamon</forenames></author></authors><title>Sistem pendukung keputusan kelayakan TKI ke luar negeri menggunakan
  FMADM</title><categories>cs.AI</categories><comments>Jurnal Sistem Informasi (SISFO)</comments><journal-ref>Jurnal Sistem Informasi (SISFO), vol. 4, pp. 336-343, September
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BP3TKI Palembang is the government agencies that coordinate, execute and
selection of prospective migrants registration and placement. To simplify the
existing procedures and improve decision-making is necessary to build a
decision support system (DSS) to determine eligibility for employment abroad by
applying Fuzzy Multiple Attribute Decision Making (FMADM), using the linear
sequential systems development methods. The system is built using Microsoft
Visual Basic. Net 2010 and SQL Server 2008 database. The design of the system
using use case diagrams and class diagrams to identify the needs of users and
systems as well as systems implementation guidelines. This Decision Support
System able to rank and produce the prospective migrants, making it easier for
parties to take decision BP3TKI the workers who will be working out of the
country.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5169</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5169</id><created>2013-12-17</created><authors><author><keyname>McCurdy</keyname><forenames>Micah Blake</forenames></author><author><keyname>Egger</keyname><forenames>Jeffrey</forenames></author><author><keyname>Kyriakidis</keyname><forenames>Jordan</forenames></author></authors><title>Decomposition and Gluing for Adiabatic Quantum Optimization</title><categories>cs.ET quant-ph</categories><comments>15 pages, many figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Farhi and others have introduced the notion of solving NP problems using
adiabatic quantum com- puters. We discuss an application of this idea to the
problem of integer factorization, together with a technique we call gluing
which can be used to build adiabatic models of interesting problems. Although
adiabatic quantum computers already exist, they are likely to be too small to
directly tackle problems of interesting practical sizes for the foreseeable
future. Therefore, we discuss techniques for decomposition of large problems,
which permits us to fully exploit such hardware as may be available. Numerical
re- sults suggest that even simple decomposition techniques may yield
acceptable results with subexponential overhead, independent of the performance
of the underlying device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5172</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5172</id><created>2013-12-14</created><authors><author><keyname>Khalil</keyname><forenames>Wajeeha</forenames></author><author><keyname>Schikuta</keyname><forenames>Erich</forenames></author></authors><title>A Design Blueprint for Virtual Organizations in a Service Oriented
  Landscape</title><categories>cs.CY</categories><acm-class>H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;United we stand, divided we fall&quot; is a well known saying. We are living in
the era of virtual collaborations. Advancement on conceptual and technological
level has enhanced the way people communicate. Everything-as-a-Service once a
dream, now becoming a reality.
  Problem nature has also been changed over the time. Today, e-Collaborations
are applied to all the domains possible. Extensive data and computing resources
are in need and assistance from human experts is also becoming essential. This
puts a great responsibility on Information Technology (IT) researchers and
developers to provide generic platforms where user can easily communicate and
solve their problems. To realize this concept, distributed computing has
offered many paradigms, e.g. cluster, grid, cloud computing. Virtual
Organization (VO) is a logical orchestration of globally dispersed resources to
achieve common goals.
  Existing paradigms and technology are used to form Virtual Organization, but
lack of standards remained a critical issue for last two decades. Our research
endeavor focuses on developing a design blueprint for Virtual Organization
building process. The proposed standardization process is a two phase activity.
First phase provides requirement analysis and the second phase presents a
Reference Architecture for Virtual Organization (RAVO). This form of
standardization is chosen to accommodate both technological and paradigm shift.
We categorize our efforts in two parts. First part consists of a pattern to
identify the requirements and components of a Virtual Organization. Second part
details a generic framework based on the concept of Everything-as-a-Service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5173</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5173</id><created>2013-12-18</created><authors><author><keyname>Tang</keyname><forenames>Xiaohu</forenames></author><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Li</keyname><forenames>Jie</forenames></author></authors><title>New Repair strategy of Hadamard Minimum Storage Regenerating Code for
  Distributed Storage System</title><categories>cs.IT math.IT</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The newly presented $(k+2,k)$ Hadamard minimum storage regenerating (MSR)
code is the first class of high rate storage code with optimal repair property
for all single node failures. In this paper, we propose a new simple repair
strategy, which can considerably reduces the computation load of the node
repair in contrast to the original one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5179</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5179</id><created>2013-12-18</created><authors><author><keyname>Hein</keyname><forenames>Matthias</forenames></author><author><keyname>Setzer</keyname><forenames>Simon</forenames></author><author><keyname>Jost</keyname><forenames>Leonardo</forenames></author><author><keyname>Rangapuram</keyname><forenames>Syama Sundar</forenames></author></authors><title>The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited</title><categories>stat.ML cs.LG math.OC</categories><comments>Long version of paper accepted at NIPS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hypergraphs allow one to encode higher-order relationships in data and are
thus a very flexible modeling tool. Current learning methods are either based
on approximations of the hypergraphs via graphs or on tensor methods which are
only applicable under special conditions. In this paper, we present a new
learning framework on hypergraphs which fully uses the hypergraph structure.
The key element is a family of regularization functionals based on the total
variation on hypergraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5180</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5180</id><created>2013-12-18</created><authors><author><keyname>Basavaraju</keyname><forenames>Manu</forenames></author><author><keyname>Heggernes</keyname><forenames>Pinar</forenames></author><author><keyname>Hof</keyname><forenames>Pim van 't</forenames></author><author><keyname>Saei</keyname><forenames>Reza</forenames></author><author><keyname>Villanger</keyname><forenames>Yngve</forenames></author></authors><title>Maximal induced matchings in triangle-free graphs</title><categories>math.CO cs.DS</categories><comments>17 pages</comments><msc-class>05C30, 05C85</msc-class><acm-class>G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An induced matching in a graph is a set of edges whose endpoints induce a
$1$-regular subgraph. It is known that any $n$-vertex graph has at most
$10^{n/5} \approx 1.5849^n$ maximal induced matchings, and this bound is best
possible. We prove that any $n$-vertex triangle-free graph has at most $3^{n/3}
\approx 1.4423^n$ maximal induced matchings, and this bound is attained by any
disjoint union of copies of the complete bipartite graph $K_{3,3}$. Our result
implies that all maximal induced matchings in an $n$-vertex triangle-free graph
can be listed in time $O(1.4423^n)$, yielding the fastest known algorithm for
finding a maximum induced matching in a triangle-free graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5192</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5192</id><created>2013-12-18</created><updated>2014-03-24</updated><authors><author><keyname>Jost</keyname><forenames>Leonardo</forenames></author><author><keyname>Setzer</keyname><forenames>Simon</forenames></author><author><keyname>Hein</keyname><forenames>Matthias</forenames></author></authors><title>Nonlinear Eigenproblems in Data Analysis - Balanced Graph Cuts and the
  RatioDCA-Prox</title><categories>stat.ML cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been recently shown that a large class of balanced graph cuts allows
for an exact relaxation into a nonlinear eigenproblem. We review briefly some
of these results and propose a family of algorithms to compute nonlinear
eigenvectors which encompasses previous work as special cases. We provide a
detailed analysis of the properties and the convergence behavior of these
algorithms and then discuss their application in the area of balanced graph
cuts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5198</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5198</id><created>2013-12-18</created><updated>2014-04-25</updated><authors><author><keyname>Modi</keyname><forenames>Ashutosh</forenames></author><author><keyname>Titov</keyname><forenames>Ivan</forenames></author></authors><title>Learning Semantic Script Knowledge with Event Embeddings</title><categories>cs.LG cs.CL stat.ML</categories><comments>4 Pages, 1 figure, ICLR Workshop</comments><acm-class>I.2.6; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Induction of common sense knowledge about prototypical sequences of events
has recently received much attention. Instead of inducing this knowledge in the
form of graphs, as in much of the previous work, in our method, distributed
representations of event realizations are computed based on distributed
representations of predicates and their arguments, and then these
representations are used to predict prototypical event orderings. The
parameters of the compositional process for computing the event representations
and the ranking component of the model are jointly estimated from texts. We
show that this approach results in a substantial boost in ordering performance
with respect to previous methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5202</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5202</id><created>2013-12-18</created><authors><author><keyname>Khan</keyname><forenames>Usman A.</forenames></author><author><keyname>Aeron</keyname><forenames>Shuchin</forenames></author></authors><title>Consensus in the presence of interference</title><categories>cs.SY</categories><comments>Submitted for peer-reviewed publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies distributed strategies for average-consensus of arbitrary
vectors in the presence of network interference. We assume that the underlying
communication on any \emph{link} suffers from \emph{additive interference}
caused due to the communication by other agents following their own consensus
protocol. Additionally, no agent knows how many or which agents are interfering
with its communication. Clearly, the standard consensus protocol does not
remain applicable in such scenarios. In this paper, we cast an algebraic
structure over the interference and show that the standard protocol can be
modified such that the average is reachable in a subspace whose dimension is
complimentary to the maximal dimension of the interference subspaces (over all
of the communication links). To develop the results, we use \emph{information
alignment} to align the intended transmission (over each link) to the
null-space of the interference (on that link). We show that this alignment is
indeed invertible, i.e. the intended transmission can be recovered over which,
subsequently, consensus protocol is implemented. That \emph{local} protocols
exist even when the collection of the interference subspaces span the entire
vector space is somewhat surprising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5225</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5225</id><created>2013-12-18</created><updated>2014-10-05</updated><authors><author><keyname>Merkle</keyname><forenames>Johannes</forenames></author><author><keyname>Tams</keyname><forenames>Benjamin</forenames></author></authors><title>Security of the Improved Fuzzy Vault Scheme in the Presence of Record
  Multiplicity (Full Version)</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dodis et al. proposed an improved version of the fuzzy vault scheme, one of
the most popular primitives used in biometric cryptosystems, requiring less
storage and leaking less information. Recently, Blanton and Aliasgari have
shown that the relation of two improved fuzzy vault records of the same
individual may be determined by solving a system of non-linear equations.
However, they conjectured that this is feasible for small parameters only. In
this paper, we present a new attack against the improved fuzzy vault scheme
based on the extended Euclidean algorithm that determines if two records are
related and recovers the elements by which the protected features, e.g., the
biometric templates, differ. Our theoretical and empirical analysis
demonstrates that the attack is very effective and efficient for practical
parameters. Furthermore, we show how this attack can be extended to fully
recover both feature sets from related vault records much more efficiently than
possible by attacking each record individually. We complement this work by
deriving lower bounds for record multiplicity attacks and use these to show
that our attack is asymptotically optimal in an information theoretic sense.
Finally, we propose remedies to harden the scheme against record multiplicity
attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5242</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5242</id><created>2013-12-18</created><updated>2014-02-16</updated><authors><author><keyname>Dosovitskiy</keyname><forenames>Alexey</forenames></author><author><keyname>Springenberg</keyname><forenames>Jost Tobias</forenames></author><author><keyname>Brox</keyname><forenames>Thomas</forenames></author></authors><title>Unsupervised feature learning by augmenting single images</title><categories>cs.CV cs.LG cs.NE</categories><comments>ICLR 2014 workshop track submission (7 pages, 4 figures, 1 table)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When deep learning is applied to visual object recognition, data augmentation
is often used to generate additional training data without extra labeling cost.
It helps to reduce overfitting and increase the performance of the algorithm.
In this paper we investigate if it is possible to use data augmentation as the
main component of an unsupervised feature learning architecture. To that end we
sample a set of random image patches and declare each of them to be a separate
single-image surrogate class. We then extend these trivial one-element classes
by applying a variety of transformations to the initial 'seed' patches. Finally
we train a convolutional neural network to discriminate between these surrogate
classes. The feature representation learned by the network can then be used in
various vision tasks. We find that this simple feature learning algorithm is
surprisingly successful, achieving competitive classification results on
several popular vision datasets (STL-10, CIFAR-10, Caltech-101).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5257</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5257</id><created>2013-12-06</created><authors><author><keyname>Saggar</keyname><forenames>Hemant</forenames></author><author><keyname>Mehra</keyname><forenames>D. K.</forenames></author></authors><title>Cyclostationary Spectrum Sensing in Cognitive Radios Using FRESH Filters</title><categories>cs.OH</categories><comments>Presented at Advances in Wireless Cellular Telecommunications:
  Technologies &amp; Services, 1st ICEIT National Conference on, April 14-15, 2011,
  New Delhi</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with spectrum sensing in Cognitive Radios to enable
unlicensed secondary users to opportunistically access a licensed band. The
ability to detect the presence of a primary user at a low signal to noise ratio
(SNR) is a challenging prerequisite to spectrum sensing and earlier proposed
techniques like energy detection and cyclostationary detection have only been
partially successful. This paper proposes the use of FRESH (FREquency SHift)
filters [1] to enable spectrum sensing at low SNR by optimally estimating a
cyclostationary signal using its spectral coherence properties. We establish
the mean square error convergence of the adaptive FRESH filter through
simulation. Subsequently, we formulate a cyclostationarity based binary
hypothesis test on the filtered signal and observe the resultant detection
performance. Simulation results show that the proposed approach performs better
than energy detection and cyclostationary detection techniques for spectrum
sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5258</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5258</id><created>2013-12-18</created><updated>2014-10-24</updated><authors><author><keyname>Dumoulin</keyname><forenames>Vincent</forenames></author><author><keyname>Goodfellow</keyname><forenames>Ian J.</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>On the Challenges of Physical Implementations of RBMs</title><categories>stat.ML cs.LG</categories><journal-ref>Proc. AAAI 2014, pp. 1199-1205</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Restricted Boltzmann machines (RBMs) are powerful machine learning models,
but learning and some kinds of inference in the model require sampling-based
approximations, which, in classical digital computers, are implemented using
expensive MCMC. Physical computation offers the opportunity to reduce the cost
of sampling by building physical systems whose natural dynamics correspond to
drawing samples from the desired RBM distribution. Such a system avoids the
burn-in and mixing cost of a Markov chain. However, hardware implementations of
this variety usually entail limitations such as low-precision and limited range
of the parameters and restrictions on the size and topology of the RBM. We
conduct software simulations to determine how harmful each of these
restrictions is. Our simulations are designed to reproduce aspects of the
D-Wave quantum computer, but the issues we investigate arise in most forms of
physical computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5271</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5271</id><created>2013-12-18</created><authors><author><keyname>Fliess</keyname><forenames>Michel</forenames><affiliation>LIX, AL.I.E.N.</affiliation></author><author><keyname>Join</keyname><forenames>C&#xe9;dric</forenames><affiliation>AL.I.E.N., CRAN, INRIA Lille - Nord Europe</affiliation></author></authors><title>Systematic and multifactor risk models revisited</title><categories>q-fin.RM cs.CE math.LO q-fin.CP stat.ML</categories><comments>First Paris Financial Management Conference, Paris : France (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systematic and multifactor risk models are revisited via methods which were
already successfully developed in signal processing and in automatic control.
The results, which bypass the usual criticisms on those risk modeling, are
illustrated by several successful computer experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5276</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5276</id><created>2013-12-18</created><updated>2014-04-18</updated><authors><author><keyname>Nourdin</keyname><forenames>Ivan</forenames></author><author><keyname>Peccati</keyname><forenames>Giovanni</forenames></author><author><keyname>Swan</keyname><forenames>Yvik</forenames></author></authors><title>Integration by parts and representation of information functionals</title><categories>math.PR cs.IT math.IT</categories><comments>Accepted for publication in the Proceedings of the 2014 IEEE
  International Symposium on Information Theory</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new formalism for computing expectations of functionals of
arbitrary random vectors, by using generalised integration by parts formulae.
In doing so we extend recent representation formulae for the score function
introduced in Nourdin, Peccati and Swan (JFA, to appear) and also provide a new
proof of a central identity first discovered in Guo, Shamai, and Verd{\'u}
(IEEE Trans. Information Theory, 2005). We derive a representation for the
standardized Fisher information of sums of i.i.d. random vectors which use our
identities to provide rates of convergence in information theoretic central
limit theorems (both in Fisher information distance and in relative entropy).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5280</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5280</id><created>2013-12-18</created><updated>2015-01-29</updated><authors><author><keyname>Gastineau</keyname><forenames>Nicolas</forenames></author></authors><title>Dichotomies properties on computational complexity of S-packing coloring
  problems</title><categories>cs.DM cs.CC math.CO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work establishes the complexity class of several instances of the
S-packing coloring problem: for a graph G, a positive integer k and a non
decreasing list of integers S = (s\_1 , ..., s\_k ), G is S-colorable, if its
vertices can be partitioned into sets S\_i , i = 1,... , k, where each S\_i
being a s\_i -packing (a set of vertices at pairwise distance greater than
s\_i). For a list of three integers, a dichotomy between NP-complete problems
and polynomial time solvable problems is determined for subcubic graphs.
Moreover, for an unfixed size of list, the complexity of the S-packing coloring
problem is determined for several instances of the problem. These properties
are used in order to prove a dichotomy between NP-complete problems and
polynomial time solvable problems for lists of at most four integers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5297</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5297</id><created>2013-12-18</created><updated>2013-12-29</updated><authors><author><keyname>Zubiaga</keyname><forenames>Arkaitz</forenames></author><author><keyname>Ji</keyname><forenames>Heng</forenames></author></authors><title>Tweet, but Verify: Epistemic Study of Information Verification on
  Twitter</title><categories>cs.SI cs.CY</categories><comments>Pre-print of paper accepted to Social Network Analysis and Mining
  (Springer)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While Twitter provides an unprecedented opportunity to learn about breaking
news and current events as they happen, it often produces skepticism among
users as not all the information is accurate but also hoaxes are sometimes
spread. While avoiding the diffusion of hoaxes is a major concern during
fast-paced events such as natural disasters, the study of how users trust and
verify information from tweets in these contexts has received little attention
so far. We survey users on credibility perceptions regarding witness pictures
posted on Twitter related to Hurricane Sandy. By examining credibility
perceptions on features suggested for information verification in the field of
Epistemology, we evaluate their accuracy in determining whether pictures were
real or fake compared to professional evaluations performed by experts. Our
study unveils insight about tweet presentation, as well as features that users
should look at when assessing the veracity of tweets in the context of
fast-paced events. Some of our main findings include that while author details
not readily available on Twitter feeds should be emphasized in order to
facilitate verification of tweets, showing multiple tweets corroborating a fact
misleads users to trusting what actually is a hoax. We contrast some of the
behavioral patterns found on tweets with literature in Psychology research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5306</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5306</id><created>2013-12-18</created><updated>2014-09-01</updated><authors><author><keyname>Olhede</keyname><forenames>Sofia C.</forenames></author><author><keyname>Wolfe</keyname><forenames>Patrick J.</forenames></author></authors><title>Network histograms and universality of blockmodel approximation</title><categories>stat.ME cs.SI math.CO math.ST stat.TH</categories><comments>27 pages, 4 figures; revised version with link to software</comments><journal-ref>Proceedings of the National Academy of Sciences of the USA 2014,
  Vol. 111, No. 41, 14722-14727</journal-ref><doi>10.1073/pnas.1400374111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we introduce the network histogram: a statistical summary of
network interactions, to be used as a tool for exploratory data analysis. A
network histogram is obtained by fitting a stochastic blockmodel to a single
observation of a network dataset. Blocks of edges play the role of histogram
bins, and community sizes that of histogram bandwidths or bin sizes. Just as
standard histograms allow for varying bandwidths, different blockmodel
estimates can all be considered valid representations of an underlying
probability model, subject to bandwidth constraints. Here we provide methods
for automatic bandwidth selection, by which the network histogram approximates
the generating mechanism that gives rise to exchangeable random graphs. This
makes the blockmodel a universal network representation for unlabeled graphs.
With this insight, we discuss the interpretation of network communities in
light of the fact that many different community assignments can all give an
equally valid representation of such a network. To demonstrate the
fidelity-versus-interpretability tradeoff inherent in considering different
numbers and sizes of communities, we analyze two publicly available networks -
political weblogs and student friendships - and discuss how to interpret the
network histogram when additional information related to node and edge labeling
is present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5307</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5307</id><created>2013-12-18</created><updated>2015-01-02</updated><authors><author><keyname>Feigenbaum</keyname><forenames>Joan</forenames></author><author><keyname>Ford</keyname><forenames>Bryan</forenames></author></authors><title>Seeking Anonymity in an Internet Panopticon</title><categories>cs.CR</categories><comments>8 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Obtaining and maintaining anonymity on the Internet is challenging. The state
of the art in deployed tools, such as Tor, uses onion routing (OR) to relay
encrypted connections on a detour passing through randomly chosen relays
scattered around the Internet. Unfortunately, OR is known to be vulnerable at
least in principle to several classes of attacks for which no solution is known
or believed to be forthcoming soon. Current approaches to anonymity also appear
unable to offer accurate, principled measurement of the level or quality of
anonymity a user might obtain.
  Toward this end, we offer a high-level view of the Dissent project, the first
systematic effort to build a practical anonymity system based purely on
foundations that offer measurable and formally provable anonymity properties.
Dissent builds on two key pre-existing primitives - verifiable shuffles and
dining cryptographers - but for the first time shows how to scale such
techniques to offer measurable anonymity guarantees to thousands of
participants. Further, Dissent represents the first anonymity system designed
from the ground up to incorporate some systematic countermeasure for each of
the major classes of known vulnerabilities in existing approaches, including
global traffic analysis, active attacks, and intersection attacks. Finally,
because no anonymity protocol alone can address risks such as software exploits
or accidental self-identification, we introduce WiNon, an experimental
operating system architecture to harden the uses of anonymity tools such as Tor
and Dissent against such attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5345</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5345</id><created>2013-12-18</created><authors><author><keyname>Liao</keyname><forenames>Wei-Cheng</forenames></author><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author><author><keyname>Farmanbar</keyname><forenames>Hamid</forenames></author><author><keyname>Li</keyname><forenames>Xu</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author><author><keyname>Zhang</keyname><forenames>Hang</forenames></author></authors><title>Min Flow Rate Maximization for Software Defined Radio Access Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to JSAC special issue on 5G Wireless Communication Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a heterogeneous network (HetNet) of base stations (BSs) connected
via a backhaul network of routers and wired/wireless links with limited
capacity. The optimal provision of such networks requires proper resource
allocation across the radio access links in conjunction with appropriate
traffic engineering within the backhaul network. In this paper we propose an
efficient algorithm for joint resource allocation across the wireless links and
the flow control within the backhaul network. The proposed algorithm, which
maximizes the minimum rate among all the users and/or flows, is based on a
decomposition approach that leverages both the Alternating Direction Method of
Multipliers (ADMM) and the weighted-MMSE (WMMSE) algorithm. We show that this
algorithm is easily parallelizable and converges globally to a stationary
solution of the joint optimization problem. The proposed algorithm can also be
extended to deal with per-flow quality of service constraint, or to networks
with multi-antenna nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5349</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5349</id><created>2013-12-18</created><updated>2014-01-20</updated><authors><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Kim</keyname><forenames>Seung-Jun</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Moving-Horizon Dynamic Power System State Estimation Using Semidefinite
  Relaxation</title><categories>cs.SY</categories><comments>Proc. of IEEE PES General Mtg., Washnigton, DC, July 27-31, 2014.
  (Submitted)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate power system state estimation (PSSE) is an essential prerequisite
for reliable operation of power systems. Different from static PSSE, dynamic
PSSE can exploit past measurements based on a dynamical state evolution model,
offering improved accuracy and state predictability. A key challenge is the
nonlinear measurement model, which is often tackled using linearization,
despite divergence and local optimality issues. In this work, a moving-horizon
estimation (MHE) strategy is advocated, where model nonlinearity can be
accurately captured with strong performance guarantees. To mitigate local
optimality, a semidefinite relaxation approach is adopted, which often provides
solutions close to the global optimum. Numerical tests show that the proposed
method can markedly improve upon an extended Kalman filter (EKF)-based
alternative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5354</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5354</id><created>2013-12-18</created><updated>2014-07-28</updated><authors><author><keyname>Alwan</keyname><forenames>Yaqub</forenames></author><author><keyname>Cvetkovic</keyname><forenames>Zoran</forenames></author><author><keyname>Curtis</keyname><forenames>Michael</forenames></author></authors><title>Classification of Human Ventricular Arrhythmia in High Dimensional
  Representation Spaces</title><categories>cs.CE cs.LG</categories><comments>9 pages, 2 tables, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We studied classification of human ECGs labelled as normal sinus rhythm,
ventricular fibrillation and ventricular tachycardia by means of support vector
machines in different representation spaces, using different observation
lengths. ECG waveform segments of duration 0.5-4 s, their Fourier magnitude
spectra, and lower dimensional projections of Fourier magnitude spectra were
used for classification. All considered representations were of much higher
dimension than in published studies. Classification accuracy improved with
segment duration up to 2 s, with 4 s providing little improvement. We found
that it is possible to discriminate between ventricular tachycardia and
ventricular fibrillation by the present approach with much shorter runs of ECG
(2 s, minimum 86% sensitivity per class) than previously imagined. Ensembles of
classifiers acting on 1 s segments taken over 5 s observation windows gave best
results, with sensitivities of detection for all classes exceeding 93%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5355</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5355</id><created>2013-12-18</created><authors><author><keyname>Verbancsics</keyname><forenames>Phillip</forenames></author><author><keyname>Harguess</keyname><forenames>Josh</forenames></author></authors><title>Generative NeuroEvolution for Deep Learning</title><categories>cs.NE cs.CV</categories><comments>9 pages</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  An important goal for the machine learning (ML) community is to create
approaches that can learn solutions with human-level capability. One domain
where humans have held a significant advantage is visual processing. A
significant approach to addressing this gap has been machine learning
approaches that are inspired from the natural systems, such as artificial
neural networks (ANNs), evolutionary computation (EC), and generative and
developmental systems (GDS). Research into deep learning has demonstrated that
such architectures can achieve performance competitive with humans on some
visual tasks; however, these systems have been primarily trained through
supervised and unsupervised learning algorithms. Alternatively, research is
showing that evolution may have a significant role in the development of visual
systems. Thus this paper investigates the role neuro-evolution (NE) can take in
deep learning. In particular, the Hypercube-based NeuroEvolution of Augmenting
Topologies is a NE approach that can effectively learn large neural structures
by training an indirect encoding that compresses the ANN weight pattern as a
function of geometry. The results show that HyperNEAT struggles with performing
image classification by itself, but can be effective in training a feature
extractor that other ML approaches can learn from. Thus NeuroEvolution combined
with other ML methods provides an intriguing area of research that can
replicate the processes in nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5371</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5371</id><created>2013-12-18</created><authors><author><keyname>Ghanavati</keyname><forenames>Goodarz</forenames></author><author><keyname>Hines</keyname><forenames>Paul D. H.</forenames></author><author><keyname>Lakoba</keyname><forenames>Taras I.</forenames></author></authors><title>Investigating early warning signs of oscillatory instability in
  simulated phasor measurements</title><categories>nlin.CD cs.SY physics.soc-ph</categories><comments>5 pages, 7 figures, IEEE Power &amp; Energy Society General Meeting 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that the variance of load bus voltage magnitude in a small
power system test case increases monotonically as the system approaches a Hopf
bifurcation. This property can potentially be used as a method for monitoring
oscillatory stability in power grid using high-resolution phasor measurements.
Increasing variance in data from a dynamical system is a common sign of a
phenomenon known as critical slowing down (CSD). CSD is slower recovery of
dynamical systems from perturbations as they approach critical transitions.
Earlier work has focused on studying CSD in systems approaching voltage
collapse; In this paper, we investigate its occurrence as a power system
approaches a Hopf bifurcation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5378</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5378</id><created>2013-12-18</created><updated>2014-03-05</updated><authors><author><keyname>Broeck</keyname><forenames>Guy Van den</forenames></author><author><keyname>Meert</keyname><forenames>Wannes</forenames></author><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>Skolemization for Weighted First-Order Model Counting</title><categories>cs.AI</categories><comments>To appear in Proceedings of the 14th International Conference on
  Principles of Knowledge Representation and Reasoning (KR), Vienna, Austria,
  July 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  First-order model counting emerged recently as a novel reasoning task, at the
core of efficient algorithms for probabilistic logics. We present a
Skolemization algorithm for model counting problems that eliminates existential
quantifiers from a first-order logic theory without changing its weighted model
count. For certain subsets of first-order logic, lifted model counters were
shown to run in time polynomial in the number of objects in the domain of
discourse, where propositional model counters require exponential time.
However, these guarantees apply only to Skolem normal form theories (i.e., no
existential quantifiers) as the presence of existential quantifiers reduces
lifted model counters to propositional ones. Since textbook Skolemization is
not sound for model counting, these restrictions precluded efficient model
counting for directed models, such as probabilistic logic programs, which rely
on existential quantification. Our Skolemization procedure extends the
applicability of first-order model counters to these representations. Moreover,
it simplifies the design of lifted model counting algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5394</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5394</id><created>2013-12-18</created><authors><author><keyname>Gashler</keyname><forenames>Michael S.</forenames></author><author><keyname>Smith</keyname><forenames>Michael R.</forenames></author><author><keyname>Morris</keyname><forenames>Richard</forenames></author><author><keyname>Martinez</keyname><forenames>Tony</forenames></author></authors><title>Missing Value Imputation With Unsupervised Backpropagation</title><categories>cs.NE cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many data mining and data analysis techniques operate on dense matrices or
complete tables of data. Real-world data sets, however, often contain unknown
values. Even many classification algorithms that are designed to operate with
missing values still exhibit deteriorated accuracy. One approach to handling
missing values is to fill in (impute) the missing values. In this paper, we
present a technique for unsupervised learning called Unsupervised
Backpropagation (UBP), which trains a multi-layer perceptron to fit to the
manifold sampled by a set of observed point-vectors. We evaluate UBP with the
task of imputing missing values in datasets, and show that UBP is able to
predict missing values with significantly lower sum-squared error than other
collaborative filtering and imputation techniques. We also demonstrate with 24
datasets and 9 supervised learning algorithms that classification accuracy is
usually higher when randomly-withheld values are imputed using UBP, rather than
with other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5398</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5398</id><created>2013-12-18</created><updated>2014-02-17</updated><authors><author><keyname>Tetelman</keyname><forenames>Michael</forenames></author></authors><title>Continuous Learning: Engineering Super Features With Feature Algebras</title><categories>cs.LG stat.ML</categories><comments>Submitted to ICLR 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a problem of searching a space of predictive models
for a given training data set. We propose an iterative procedure for deriving a
sequence of improving models and a corresponding sequence of sets of non-linear
features on the original input space. After a finite number of iterations N,
the non-linear features become 2^N -degree polynomials on the original space.
We show that in a limit of an infinite number of iterations derived non-linear
features must form an associative algebra: a product of two features is equal
to a linear combination of features from the same feature space for any given
input point. Because each iteration consists of solving a series of convex
problems that contain all previous solutions, the likelihood of the models in
the sequence is increasing with each iteration while the dimension of the model
parameter space is set to a limited controlled value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5402</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5402</id><created>2013-12-18</created><authors><author><keyname>Howard</keyname><forenames>Andrew G.</forenames></author></authors><title>Some Improvements on Deep Convolutional Neural Network Based Image
  Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate multiple techniques to improve upon the current state of the
art deep convolutional neural network based image classification pipeline. The
techiques include adding more image transformations to training data, adding
more transformations to generate additional predictions at test time and using
complementary models applied to higher resolution images. This paper summarizes
our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our
system achieved a top 5 classification error rate of 13.55% using no external
data which is over a 20% relative improvement on the previous year's winner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5408</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5408</id><created>2013-12-19</created><updated>2014-04-17</updated><authors><author><keyname>Bryant</keyname><forenames>David</forenames></author><author><keyname>Tupper</keyname><forenames>Paul F.</forenames></author></authors><title>Diversities and the Geometry of Hypergraphs</title><categories>math.MG cs.DS math.CO</categories><comments>19 pages, no figures. This version: further small corrections</comments><msc-class>51F99, 68M10, 90C27, 05C65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The embedding of finite metrics in $\ell_1$ has become a fundamental tool for
both combinatorial optimization and large-scale data analysis. One important
application is to network flow problems in which there is close relation
between max-flow min-cut theorems and the minimal distortion embeddings of
metrics into $\ell_1$. Here we show that this theory can be generalized
considerably to encompass Steiner tree packing problems in both graphs and
hypergraphs. Instead of the theory of $\ell_1$ metrics and minimal distortion
embeddings, the parallel is the theory of diversities recently introduced by
Bryant and Tupper, and the corresponding theory of $\ell_1$ diversities and
embeddings which we develop here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5412</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5412</id><created>2013-12-19</created><updated>2014-01-06</updated><authors><author><keyname>Kiwaki</keyname><forenames>Taichi</forenames></author><author><keyname>Makino</keyname><forenames>Takaki</forenames></author><author><keyname>Aihara</keyname><forenames>Kazuyuki</forenames></author></authors><title>Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural
  Images</title><categories>stat.ML cs.LG</categories><comments>9 pages with 1 page appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We pursue an early stopping technique that helps Gaussian Restricted
Boltzmann Machines (GRBMs) to gain good natural image representations in terms
of overcompleteness and data fitting. GRBMs are widely considered as an
unsuitable model for natural images because they gain non-overcomplete
representations which include uniform filters that do not represent useful
image features. We have recently found that GRBMs once gain and subsequently
lose useful filters during their training, contrary to this common perspective.
We attribute this phenomenon to a tradeoff between overcompleteness of GRBM
representations and data fitting. To gain GRBM representations that are
overcomplete and fit data well, we propose a measure for GRBM representation
quality, approximated mutual information, and an early stopping technique based
on this measure. The proposed method boosts performance of classifiers trained
on GRBM representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5417</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5417</id><created>2013-12-19</created><authors><author><keyname>Adak</keyname><forenames>Chandranath</forenames></author></authors><title>Robust Steganography Using LSB-XOR and Image Sharing</title><categories>cs.CR</categories><comments>International Conference on Computation and Communication Advancement
  (IC3A)-2013</comments><journal-ref>Tata McGraw-Hill, ISBN (13): 978-1-25-906393-0, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hiding and securing the secret digital information and data that are
transmitted over the internet is of widespread and most challenging interest.
This paper presents a new idea of robust steganography using bitwise-XOR
operation between stego-key-image-pixel LSB (Least Significant Bit) value and
secret message-character ASCII-binary value (or, secret image-pixel value). The
stego-key-image is shared in dual-layer using odd-even position of each pixel
to make the system robust. Due to image sharing, the detection can only be done
with all the image shares.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5419</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5419</id><created>2013-12-19</created><updated>2014-05-15</updated><authors><author><keyname>Nam</keyname><forenames>Jinseok</forenames></author><author><keyname>Kim</keyname><forenames>Jungi</forenames></author><author><keyname>Menc&#xed;a</keyname><forenames>Eneldo Loza</forenames></author><author><keyname>Gurevych</keyname><forenames>Iryna</forenames></author><author><keyname>F&#xfc;rnkranz</keyname><forenames>Johannes</forenames></author></authors><title>Large-scale Multi-label Text Classification - Revisiting Neural Networks</title><categories>cs.LG</categories><comments>16 pages, 4 figures, submitted to ECML 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks have recently been proposed for multi-label classification
because they are able to capture and model label dependencies in the output
layer. In this work, we investigate limitations of BP-MLL, a neural network
(NN) architecture that aims at minimizing pairwise ranking error. Instead, we
propose to use a comparably simple NN approach with recently proposed learning
techniques for large-scale multi-label text classification tasks. In
particular, we show that BP-MLL's ranking loss minimization can be efficiently
and effectively replaced with the commonly used cross entropy error function,
and demonstrate that several advances in neural network training that have been
developed in the realm of deep learning can be effectively employed in this
setting. Our experimental results show that simple NN models equipped with
advanced techniques such as rectified linear units, dropout, and AdaGrad
perform as well as or even outperform state-of-the-art approaches on six
large-scale textual datasets with diverse characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5420</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5420</id><created>2013-12-19</created><authors><author><keyname>Boudard</keyname><forenames>M&#xe9;lanie</forenames><affiliation>PRISM</affiliation></author><author><keyname>Hermant</keyname><forenames>Olivier</forenames><affiliation>CRI</affiliation></author></authors><title>Polarizing Double Negation Translations</title><categories>cs.LO</categories><proxy>ccsd</proxy><journal-ref>LPAR 8312 (2013) 182-197</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Double-negation translations are used to encode and decode classical proofs
in intuitionistic logic. We show that, in the cut-free fragment, we can
simplify the translations and introduce fewer negations. To achieve this, we
consider the polarization of the formul{\ae}{} and adapt those translation to
the different connectives and quantifiers. We show that the embedding results
still hold, using a customized version of the focused classical sequent
calculus. We also prove the latter equivalent to more usual versions of the
sequent calculus. This polarization process allows lighter embeddings, and
sheds some light on the relationship between intuitionistic and classical
connectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5424</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5424</id><created>2013-12-19</created><authors><author><keyname>Adak</keyname><forenames>Chandranath</forenames></author></authors><title>Dual Layer Textual Message Cryptosystem with Randomized Sequence of
  Symmetric Key</title><categories>cs.CR</categories><comments>Ethical Hacking-Issues &amp; Challenges (EHIC)-2012</comments><journal-ref>The Bulletin of Engineering and Science, ISSN: 0974-7176, Vol. 4
  No. 2, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new concept of textual message encryption and
decryption through a pool of randomized symmetric key and the dual layer
cryptosystem with the concept of visual cryptography and steganography. A
textual message is converted into two image slides, and the images are
encrypted through two different randomized sequences of symmetric key. The
decryption is done in the reverse way. The encrypted images are decrypted by
those two symmetric keys. The decrypted image slides are merged together and
converted into textual message. Here the image sharing is done through the
concept of visual cryptography and the textual message to image conversion is
done through the concept of steganography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5429</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5429</id><created>2013-12-19</created><authors><author><keyname>Keil</keyname><forenames>Matthias</forenames></author><author><keyname>Thiemann</keyname><forenames>Peter</forenames></author></authors><title>On the Proxy Identity Crisis</title><categories>cs.PL</categories><comments>Position Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A proxy, in general, is an object mediating access to an arbitrary target
object. The proxy is then intended to be used in place of the target object.
Ideally, a proxy is not distinguishable from other objects. Running a program
with a proxy leads to the same outcome as running the program with the target
object. Even though the approach provides a lot of power to the user, proxies
come with a limitation. Because a proxy, wrapping a target object, is a new
object and different from its target, the interposition changes the behaviour
of some core components. For distinct proxies the double == and triple ===
equal operator returns false, even if the target object is the same. More
precisely, the expected result depends on use case. To overcome this limitation
we will discuss alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5434</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5434</id><created>2013-12-19</created><updated>2014-12-16</updated><authors><author><keyname>Zhao</keyname><forenames>Xiaochuan</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Asynchronous Adaptation and Learning over Networks --- Part I: Modeling
  and Stability Analysis</title><categories>cs.SY cs.IT cs.LG math.IT math.OC</categories><comments>40 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work and the supporting Parts II [2] and III [3], we provide a rather
detailed analysis of the stability and performance of asynchronous strategies
for solving distributed optimization and adaptation problems over networks. We
examine asynchronous networks that are subject to fairly general sources of
uncertainties, such as changing topologies, random link failures, random data
arrival times, and agents turning on and off randomly. Under this model, agents
in the network may stop updating their solutions or may stop sending or
receiving information in a random manner and without coordination with other
agents. We establish in Part I conditions on the first and second-order moments
of the relevant parameter distributions to ensure mean-square stable behavior.
We derive in Part II expressions that reveal how the various parameters of the
asynchronous behavior influence network performance. We compare in Part III the
performance of asynchronous networks to the performance of both centralized
solutions and synchronous networks. One notable conclusion is that the
mean-square-error performance of asynchronous networks shows a degradation only
of the order of $O(\nu)$, where $\nu$ is a small step-size parameter, while the
convergence rate remains largely unaltered. The results provide a solid
justification for the remarkable resilience of cooperative networks in the face
of random failures at multiple levels: agents, links, data arrivals, and
topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5438</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5438</id><created>2013-12-19</created><updated>2014-12-16</updated><authors><author><keyname>Zhao</keyname><forenames>Xiaochuan</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Asynchronous Adaptation and Learning over Networks - Part II:
  Performance Analysis</title><categories>cs.SY cs.IT cs.LG math.IT math.OC</categories><comments>43 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Part I \cite{Zhao13TSPasync1}, we introduced a fairly general model for
asynchronous events over adaptive networks including random topologies, random
link failures, random data arrival times, and agents turning on and off
randomly. We performed a stability analysis and established the notable fact
that the network is still able to converge in the mean-square-error sense to
the desired solution. Once stable behavior is guaranteed, it becomes important
to evaluate how fast the iterates converge and how close they get to the
optimal solution. This is a demanding task due to the various asynchronous
events and due to the fact that agents influence each other. In this Part II,
we carry out a detailed analysis of the mean-square-error performance of
asynchronous strategies for solving distributed optimization and adaptation
problems over networks. We derive analytical expressions for the mean-square
convergence rate and the steady-state mean-square-deviation. The expressions
reveal how the various parameters of the asynchronous behavior influence
network performance. In the process, we establish the interesting conclusion
that even under the influence of asynchronous events, all agents in the
adaptive network can still reach an $O(\nu^{1 + \gamma_o'})$ near-agreement
with some $\gamma_o' &gt; 0$ while approaching the desired solution within
$O(\nu)$ accuracy, where $\nu$ is proportional to the small step-size parameter
for adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5439</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5439</id><created>2013-12-19</created><updated>2014-12-16</updated><authors><author><keyname>Zhao</keyname><forenames>Xiaochuan</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Asynchronous Adaptation and Learning over Networks - Part III:
  Comparison Analysis</title><categories>cs.SY cs.IT cs.LG math.IT math.OC</categories><comments>39 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Part II [3] we carried out a detailed mean-square-error analysis of the
performance of asynchronous adaptation and learning over networks under a
fairly general model for asynchronous events including random topologies,
random link failures, random data arrival times, and agents turning on and off
randomly. In this Part III, we compare the performance of synchronous and
asynchronous networks. We also compare the performance of decentralized
adaptation against centralized stochastic-gradient (batch) solutions. Two
interesting conclusions stand out. First, the results establish that the
performance of adaptive networks is largely immune to the effect of
asynchronous events: the mean and mean-square convergence rates and the
asymptotic bias values are not degraded relative to synchronous or centralized
implementations. Only the steady-state mean-square-deviation suffers a
degradation in the order of $\nu$, which represents the small step-size
parameters used for adaptation. Second, the results show that the adaptive
distributed network matches the performance of the centralized solution. These
conclusions highlight another critical benefit of cooperation by networked
agents: cooperation does not only enhance performance in comparison to
stand-alone single-agent processing, but it also endows the network with
remarkable resilience to various forms of random failure events and is able to
deliver performance that is as powerful as batch solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5444</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5444</id><created>2013-12-19</created><updated>2014-05-28</updated><authors><author><keyname>Moussallam</keyname><forenames>Manuel</forenames></author><author><keyname>Gramfort</keyname><forenames>Alexandre</forenames></author><author><keyname>Daudet</keyname><forenames>Laurent</forenames></author><author><keyname>Richard</keyname><forenames>Ga&#xeb;l</forenames></author></authors><title>Blind Denoising with Random Greedy Pursuits</title><categories>cs.IT math.IT</categories><comments>4 page draft submitted so SPL with supplementary material. Open
  source implementation available at http://manuel.moussallam.net/birdcode</comments><doi>10.1109/LSP.2014.2334231</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Denoising methods require some assumptions about the signal of interest and
the noise. While most denoising procedures require some knowledge about the
noise level, which may be unknown in practice, here we assume that the signal
expansion in a given dictionary has a distribution that is more heavy-tailed
than the noise. We show how this hypothesis leads to a stopping criterion for
greedy pursuit algorithms which is independent from the noise level. Inspired
by the success of ensemble methods in machine learning, we propose a strategy
to reduce the variance of greedy estimates by averaging pursuits obtained from
randomly subsampled dictionaries. We call this denoising procedure Blind Random
Pursuit Denoising (BIRD). We offer a generalization to multidimensional
signals, with a structured sparse model (S-BIRD). The relevance of this
approach is demonstrated on synthetic and experimental MEG signals where,
without any parameter tuning, BIRD outperforms state-of-the-art algorithms even
when they are informed by the noise level. Code is available to reproduce all
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5457</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5457</id><created>2013-12-19</created><authors><author><keyname>Vaizman</keyname><forenames>Yonatan</forenames></author><author><keyname>McFee</keyname><forenames>Brian</forenames></author><author><keyname>Lanckriet</keyname><forenames>Gert</forenames></author></authors><title>Codebook based Audio Feature Representation for Music Information
  Retrieval</title><categories>cs.IR cs.LG cs.MM</categories><comments>Journal paper. Submitted to IEEE transactions on Audio, Speech and
  Language Processing. Submitted on Dec 18th, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital music has become prolific in the web in recent decades. Automated
recommendation systems are essential for users to discover music they love and
for artists to reach appropriate audience. When manual annotations and user
preference data is lacking (e.g. for new artists) these systems must rely on
\emph{content based} methods. Besides powerful machine learning tools for
classification and retrieval, a key component for successful recommendation is
the \emph{audio content representation}.
  Good representations should capture informative musical patterns in the audio
signal of songs. These representations should be concise, to enable efficient
(low storage, easy indexing, fast search) management of huge music
repositories, and should also be easy and fast to compute, to enable real-time
interaction with a user supplying new songs to the system.
  Before designing new audio features, we explore the usage of traditional
local features, while adding a stage of encoding with a pre-computed
\emph{codebook} and a stage of pooling to get compact vectorial
representations. We experiment with different encoding methods, namely
\emph{the LASSO}, \emph{vector quantization (VQ)} and \emph{cosine similarity
(CS)}. We evaluate the representations' quality in two music information
retrieval applications: query-by-tag and query-by-example. Our results show
that concise representations can be used for successful performance in both
applications. We recommend using top-$\tau$ VQ encoding, which consistently
performs well in both applications, and requires much less computation time
than the LASSO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5465</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5465</id><created>2013-12-19</created><updated>2014-09-24</updated><authors><author><keyname>Lin</keyname><forenames>Shaobo</forenames></author><author><keyname>Zeng</keyname><forenames>Jinshan</forenames></author><author><keyname>Fang</keyname><forenames>Jian</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author></authors><title>Learning rates of $l^q$ coefficient regularization learning with
  Gaussian kernel</title><categories>cs.LG stat.ML</categories><comments>26 pages, 3 figures</comments><msc-class>68T05</msc-class><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regularization is a well recognized powerful strategy to improve the
performance of a learning machine and $l^q$ regularization schemes with
$0&lt;q&lt;\infty$ are central in use. It is known that different $q$ leads to
different properties of the deduced estimators, say, $l^2$ regularization leads
to smooth estimators while $l^1$ regularization leads to sparse estimators.
Then, how does the generalization capabilities of $l^q$ regularization learning
vary with $q$? In this paper, we study this problem in the framework of
statistical learning theory and show that implementing $l^q$ coefficient
regularization schemes in the sample dependent hypothesis space associated with
Gaussian kernel can attain the same almost optimal learning rates for all
$0&lt;q&lt;\infty$. That is, the upper and lower bounds of learning rates for $l^q$
regularization learning are asymptotically identical for all $0&lt;q&lt;\infty$. Our
finding tentatively reveals that, in some modeling contexts, the choice of $q$
might not have a strong impact with respect to the generalization capability.
From this perspective, $q$ can be arbitrarily specified, or specified merely by
other no generalization criteria like smoothness, computational complexity,
sparsity, etc..
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5469</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5469</id><created>2013-12-19</created><authors><author><keyname>Anjali</keyname><forenames>PP</forenames></author><author><keyname>Binu</keyname><forenames>A</forenames></author></authors><title>Network Traffic Analysis:Hadoop Pig vs Typical MapReduce</title><categories>cs.DC</categories><comments>7 pages, 5 figures, Conference - ITCSE 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data analysis has become much popular in the present day scenario and the
manipulation of big data has gained the keen attention of researchers in the
field of data analytics. Analysis of big data is currently considered as an
integral part of many computational and statistical departments. As a result,
novel approaches in data analysis are evolving on a daily basis. Thousands of
transaction requests are handled and processed everyday by different websites
associated with e-commerce, e-banking, e-shopping carts etc. The network
traffic and weblog analysis comes to play a crucial role in such situations
where Hadoop can be suggested as an efficient solution for processing the
Netflow data collected from switches as well as website access-logs during
fixed intervals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5479</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5479</id><created>2013-12-19</created><updated>2014-02-16</updated><authors><author><keyname>Masci</keyname><forenames>Jonathan</forenames></author><author><keyname>Bronstein</keyname><forenames>Alex M.</forenames></author><author><keyname>Bronstein</keyname><forenames>Michael M.</forenames></author><author><keyname>Sprechmann</keyname><forenames>Pablo</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Sparse similarity-preserving hashing</title><categories>cs.CV cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, a lot of attention has been devoted to efficient nearest
neighbor search by means of similarity-preserving hashing. One of the plights
of existing hashing techniques is the intrinsic trade-off between performance
and computational complexity: while longer hash codes allow for lower false
positive rates, it is very difficult to increase the embedding dimensionality
without incurring in very high false negatives rates or prohibiting
computational costs. In this paper, we propose a way to overcome this
limitation by enforcing the hash codes to be sparse. Sparse high-dimensional
codes enjoy from the low false positive rates typical of long hashes, while
keeping the false negative rates similar to those of a shorter dense hashing
scheme with equal number of degrees of freedom. We use a tailored feed-forward
neural network for the hashing function. Extensive experimental evaluation
involving visual and multi-modal data shows the benefits of the proposed
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5480</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5480</id><created>2013-12-19</created><authors><author><keyname>Bocca</keyname><forenames>Maurizio</forenames></author><author><keyname>Luong</keyname><forenames>Anh</forenames></author><author><keyname>Patwari</keyname><forenames>Neal</forenames></author><author><keyname>Schmid</keyname><forenames>Thomas</forenames></author></authors><title>Dial It In: Rotating RF Sensors to Enhance Radio Tomography</title><categories>cs.NI</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A radio tomographic imaging (RTI) system uses the received signal strength
(RSS) measured by RF sensors in a static wireless network to localize people in
the deployment area, without having them to carry or wear an electronic device.
This paper addresses the fact that small-scale changes in the position and
orientation of the antenna of each RF sensor can dramatically affect imaging
and localization performance of an RTI system. However, the best placement for
a sensor is unknown at the time of deployment. Improving performance in a
deployed RTI system requires the deployer to iteratively &quot;guess-and-retest&quot;,
i.e., pick a sensor to move and then re-run a calibration experiment to
determine if the localization performance had improved or degraded. We present
an RTI system of servo-nodes, RF sensors equipped with servo motors which
autonomously &quot;dial it in&quot;, i.e., change position and orientation to optimize
the RSS on links of the network. By doing so, the localization accuracy of the
RTI system is quickly improved, without requiring any calibration experiment
from the deployer. Experiments conducted in three indoor environments
demonstrate that the servo-nodes system reduces localization error on average
by 32% compared to a standard RTI system composed of static RF sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5481</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5481</id><created>2013-12-19</created><authors><author><keyname>Mustaffa</keyname><forenames>Azlina</forenames></author><author><keyname>Najid</keyname><forenames>Norazura Ezuana Mohd</forenames></author><author><keyname>Sawari</keyname><forenames>Siti Salwa Md.</forenames></author></authors><title>Students' Perceptions and Attitude towards the effectiveness of Prezi
  Uses in learning Islamic Subject</title><categories>cs.CY</categories><journal-ref>International Journal of Computer Science (IJASCSE)2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prezi is a Hungarian software company, producing a cloud-based presentation
software and storytelling tool for presenting ideas on a virtual canvas
(Prezi.com). Prezi is one of the teaching materials that help students in
learning process. This study aims to explore the effectiveness of using Prezi
in Islamic education subject among secondary schools under the topic of
marriage in Islam: polygamy. Specifically its aims to identify students
interest and second examine their attitude towards the uses of Prezi in
learning Islamic educations. A total of 22 students participated in the survey,
employing a 22-item questionnaire. The data was analyzed quantitatively using
Statistical Package for the Social Sciences (SPSS). Result from this study
revealed that student shows their interest in learning Islamic Educations when
teachers uses Prezi. In additions students also show positive attitude towards
uses of Prezi in Classroom. As conclusions, the uses of Prezi presentation is
easy and its technique for developing a more creative and innovative approach
in teaching strategies among Islamic educators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5486</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5486</id><created>2013-12-19</created><authors><author><keyname>Chou</keyname><forenames>Chun Tung</forenames></author></authors><title>Molecular communication networks with general molecular circuit
  receivers</title><categories>q-bio.MN cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a molecular communication network, transmitters may encode information in
concentration or frequency of signalling molecules. When the signalling
molecules reach the receivers, they react, via a set of chemical reactions or a
molecular circuit, to produce output molecules. The counts of output molecules
over time is the output signal of the receiver. The aim of this paper is to
investigate the impact of different reaction types on the information
transmission capacity of molecular communication networks. We realise this aim
by using a general molecular circuit model. We derive general expressions of
mean receiver output, and signal and noise spectra. We use these expressions to
investigate the information transmission capacities of a number of molecular
circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5505</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5505</id><created>2013-12-19</created><updated>2014-11-11</updated><authors><author><keyname>Silberstein</keyname><forenames>Natalia</forenames></author><author><keyname>G&#xe1;l</keyname><forenames>Anna</forenames></author></authors><title>Optimal Combinatorial Batch Codes based on Block Designs</title><categories>cs.DM math.CO</categories><comments>Accepted for publication in Designs, Codes and Cryptography
  (Springer)</comments><doi>10.1007/s10623-014-0007-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Batch codes, introduced by Ishai, Kushilevitz, Ostrovsky and Sahai, represent
the distributed storage of an $n$-element data set on $m$ servers in such a way
that any batch of $k$ data items can be retrieved by reading at most one (or
more generally, $t$) items from each server, while keeping the total storage
over $m$ servers equal to $N$. This paper considers a class of batch codes (for
$t=1$), called combinatorial batch codes (CBC), where each server stores a
subset of a database. A CBC is called optimal if the total storage $N$ is
minimal for given $n,m$, and $k$. A $c$-uniform CBC is a combinatorial batch
code where each item is stored in exactly $c$ servers. A $c$-uniform CBC is
called optimal if its parameter $n$ has maximum value for given $m$ and $k$.
Optimal $c$-uniform CBCs have been known only for $c\in \{2,k-1,k-2\}$.
  In this paper we present new constructions of optimal CBCs in both the
uniform and general settings, for values of the parameters where tight bounds
have not been established previously. In the uniform setting, we provide
constructions of two new families of optimal uniform codes with $c\sim
\sqrt{k}$. Our constructions are based on affine planes and transversal
designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5515</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5515</id><created>2013-12-19</created><authors><author><keyname>Kurdej</keyname><forenames>Marek</forenames><affiliation>HEUDIASYC</affiliation></author><author><keyname>Cherfaoui</keyname><forenames>V&#xe9;ronique</forenames><affiliation>HEUDIASYC</affiliation></author></authors><title>Conservative, Proportional and Optimistic Contextual Discounting in the
  Belief Functions Theory</title><categories>cs.AI</categories><comments>7 pages</comments><proxy>ccsd</proxy><journal-ref>16th International Conference on Information Fusion, Istanbul :
  Turkey (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information discounting plays an important role in the theory of belief
functions and, generally, in information fusion. Nevertheless, neither
classical uniform discounting nor contextual cannot model certain use cases,
notably temporal discounting. In this article, new contextual discounting
schemes, conservative, proportional and optimistic, are proposed. Some
properties of these discounting operations are examined. Classical discounting
is shown to be a special case of these schemes. Two motivating cases are
discussed: modelling of source reliability and application to temporal
discounting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5520</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5520</id><created>2013-12-19</created><authors><author><keyname>Evans</keyname><forenames>William</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Lenhart</keyname><forenames>William</forenames></author><author><keyname>Liotta</keyname><forenames>Giuseppe</forenames></author><author><keyname>Mchedlidze</keyname><forenames>Tamara</forenames></author><author><keyname>Wismath</keyname><forenames>Stephen</forenames></author></authors><title>Bar 1-Visibility Graphs and their relation to other Nearly Planar Graphs</title><categories>cs.DS cs.CG math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is called a strong (resp. weak) bar 1-visibility graph if its
vertices can be represented as horizontal segments (bars) in the plane so that
its edges are all (resp. a subset of) the pairs of vertices whose bars have a
$\epsilon$-thick vertical line connecting them that intersects at most one
other bar.
  We explore the relation among weak (resp. strong) bar 1-visibility graphs and
other nearly planar graph classes. In particular, we study their relation to
1-planar graphs, which have a drawing with at most one crossing per edge;
quasi-planar graphs, which have a drawing with no three mutually crossing
edges; the squares of planar 1-flow networks, which are upward digraphs with
in- or out-degree at most one. Our main results are that 1-planar graphs and
the (undirected) squares of planar 1-flow networks are weak bar 1-visibility
graphs and that these are quasi-planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5542</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5542</id><created>2013-12-19</created><updated>2014-03-18</updated><authors><author><keyname>Lebret</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Collobert</keyname><forenames>Ronan</forenames></author></authors><title>Word Emdeddings through Hellinger PCA</title><categories>cs.CL cs.LG</categories><comments>9 pages, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word embeddings resulting from neural language models have been shown to be
successful for a large variety of NLP tasks. However, such architecture might
be difficult to train and time-consuming. Instead, we propose to drastically
simplify the word embeddings computation through a Hellinger PCA of the word
co-occurence matrix. We compare those new word embeddings with some well-known
embeddings on NER and movie review tasks and show that we can reach similar or
even better performance. Although deep learning is not really necessary for
generating good word embeddings, we show that it can provide an easy way to
adapt embeddings to specific tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5547</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5547</id><created>2013-12-19</created><updated>2014-04-10</updated><authors><author><keyname>Liikkanen</keyname><forenames>Lassi A</forenames></author></authors><title>Three Metrics for Measuring User Engagement with Online Media and a
  YouTube Case Study</title><categories>cs.HC cs.MM</categories><comments>4 pages, 1 figure, 3 tables, 2 appendixes</comments><acm-class>H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical report discusses three metrics of user engagement with online
media. They are Commenting frequency, Voting frequency, and Voting balance.
These relative figures can be derived from established, basic statistics
available for many services, prominently YouTube. The paper includes case a
study of popular YouTube videos to illustrate the characteristics and
usefulness of the measures. The study documents the range of observed values
and their relationships. The empirical sample shows the three measures to be
only moderately correlated with the original statistics despite the common
numerators and denominators. The paper concludes by discussing future
applications and the needs of the quantification of user interaction with new
media services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5548</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5548</id><created>2013-12-19</created><authors><author><keyname>Schmidhuber</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013</title><categories>cs.NE</categories><comments>11 pages. As a machine learning researcher I am obsessed with proper
  credit assignment. This draft is the result of an experiment in rapid massive
  open online peer review. Since 20 September 2013, subsequent revisions
  published under http://www.deeplearning.me have absorbed many suggestions for
  improvements by experts</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Learning has attracted significant attention in recent years. Here I
present a brief overview of my first Deep Learner of 1991, and its historic
context, with a timeline of Deep Learning highlights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5551</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5551</id><created>2013-12-19</created><authors><author><keyname>Nirmala</keyname><forenames>A. M</forenames></author><author><keyname>Subramaniam</keyname><forenames>P.</forenames></author><author><keyname>priya</keyname><forenames>A. Anusha</forenames></author><author><keyname>Ravi</keyname><forenames>M.</forenames></author></authors><title>Enriched Performance on Wireless Sensor Network using Fuzzy based
  Clustering Technique</title><categories>cs.NI</categories><comments>In this paper fuzzy logic concept hybrid with wireless sensor
  networking and include 5 figures and 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wireless sensor networks combines sensing, computation, and communication
into a single small device. These devices depend on battery power and may be
placed in hostile environments replacing them becomes a tedious task. Thus
improving the energy of these networks becomes important. Clustering in
wireless sensor network looks several challenges such as selection of an
optimal group of sensor nodes as cluster, optimum selection of cluster head,
energy balanced optimal strategy for rotating the role of cluster head in a
cluster, maintaining intra and inter cluster connectivity and optimal data
routing in the network.
  In this paper, we study a protocol supporting an energy efficient clustering,
cluster head selection and data routing method to extend the lifetime of sensor
network. Simulation results demonstrate that the proposed protocol prolongs
network lifetime due to the use of efficient clustering, cluster head selection
and data routing. The results of simulation show that at the end of some
certain part of running the EECS and Fuzzy based clustering algorithm increases
the number of alive nodes comparing with the LEACH and HEED methods and this
can lead to an increase in sensor network lifetime. By using the EECS method
the total number of messages received at base station is increased when
compared with LEACH and HEED methods. The Fuzzy based clustering method
compared with the K-Means Clustering by means of iteration count and time taken
to die first node in wireless sensor network, as the result shows that the
fuzzy based clustering method perform well than kmeans clustering methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5555</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5555</id><created>2013-12-19</created><updated>2014-03-28</updated><authors><author><keyname>Ali</keyname><forenames>Ramy E.</forenames></author><author><keyname>Seddik</keyname><forenames>Karim G.</forenames></author><author><keyname>Nafie</keyname><forenames>Mohammed</forenames></author><author><keyname>Digham</keyname><forenames>Fadel F.</forenames></author></authors><title>A Pricing-Based Cooperative Spectrum Sharing Stackelberg Game</title><categories>cs.NI cs.GT</categories><comments>7 pages. IEEE, WiOpt 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of cooperative spectrum sharing among a primary user
(PU) and multiple secondary users (SUs) under quality of service (QoS)
constraints. The SUs network is controlled by the PU through a relay which gets
a revenue for amplifying and forwarding the SUs signals to their respective
destinations. The relay charges each SU a different price depending on its
received signal-to-interference and-noise ratio (SINR). The relay can control
the SUs network and maximize any desired PU utility function. The PU utility
function represents its rate, which is affected by the SUs access, and its
gained revenue to allow the access of the SUs. The SU network can be formulated
as a game in which each SU wants to maximize its utility function; the problem
is formulated as a Stackelberg game. Finally, the problem of maximizing the
primary utility function is solved through three different approaches, namely,
the optimal, the heuristic and the suboptimal algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5559</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5559</id><created>2013-12-19</created><updated>2014-02-18</updated><authors><author><keyname>Sergienya</keyname><forenames>Irina</forenames></author><author><keyname>Sch&#xfc;tze</keyname><forenames>Hinrich</forenames></author></authors><title>Distributional Models and Deep Learning Embeddings: Combining the Best
  of Both Worlds</title><categories>cs.CL</categories><comments>4 pages, 1 table, ICLR Workshop; main experimental table was extended
  with more experimental results; related word added</comments><acm-class>I.2.6; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are two main approaches to the distributed representation of words:
low-dimensional deep learning embeddings and high-dimensional distributional
models, in which each dimension corresponds to a context word. In this paper,
we combine these two approaches by learning embeddings based on
distributional-model vectors - as opposed to one-hot vectors as is standardly
done in deep learning. We show that the combined approach has better
performance on a word relatedness judgment task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5568</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5568</id><created>2013-12-19</created><authors><author><keyname>Wei</keyname><forenames>Xian</forenames></author><author><keyname>Shen</keyname><forenames>Hao</forenames></author><author><keyname>Kleinsteuber</keyname><forenames>Martin</forenames></author></authors><title>An Adaptive Dictionary Learning Approach for Modeling Dynamical Textures</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video representation is an important and challenging task in the computer
vision community. In this paper, we assume that image frames of a moving scene
can be modeled as a Linear Dynamical System. We propose a sparse coding
framework, named adaptive video dictionary learning (AVDL), to model a video
adaptively. The developed framework is able to capture the dynamics of a moving
scene by exploring both sparse properties and the temporal correlations of
consecutive video frames. The proposed method is compared with state of the art
video processing methods on several benchmark data sequences, which exhibit
appearance changes and heavy occlusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5572</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5572</id><created>2013-12-19</created><authors><author><keyname>Liu</keyname><forenames>Wenjie</forenames></author><author><keyname>Liu</keyname><forenames>Chao</forenames></author><author><keyname>Wang</keyname><forenames>Haibin</forenames></author><author><keyname>Jia</keyname><forenames>Tingting</forenames></author></authors><title>Quantum Private Comparison: A Review</title><categories>cs.CR</categories><journal-ref>IETE Tech Rev 2013;30:439-445</journal-ref><doi>10.4103/0256-4602.123129</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an important branch of quantum secure multiparty computation, quantum
private comparison (QPC) has attracted more and more attention recently. In
this paper, according to the quantum implementation mechanism that these
protocols used, we divide these protocols into three categories: The quantum
cryptography QPC, the superdense coding QPC, and the entanglement swapping QPC.
And then, a more in-depth analysis on the research progress, design idea, and
substantive characteristics of corresponding QPC categories is carried out,
respectively. Finally, the applications of QPC and quantum secure multi-party
computation issues are discussed and, in addition, three possible research
mainstream directions are pointed out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5578</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5578</id><created>2013-12-19</created><updated>2014-01-24</updated><authors><author><keyname>Ozair</keyname><forenames>Sherjil</forenames></author><author><keyname>Yao</keyname><forenames>Li</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Multimodal Transitions for Generative Stochastic Networks</title><categories>cs.LG stat.ML</categories><comments>7 figures, 9 pages, submitted to ICLR14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative Stochastic Networks (GSNs) have been recently introduced as an
alternative to traditional probabilistic modeling: instead of parametrizing the
data distribution directly, one parametrizes a transition operator for a Markov
chain whose stationary distribution is an estimator of the data generating
distribution. The result of training is therefore a machine that generates
samples through this Markov chain. However, the previously introduced GSN
consistency theorems suggest that in order to capture a wide class of
distributions, the transition operator in general should be multimodal,
something that has not been done before this paper. We introduce for the first
time multimodal transition distributions for GSNs, in particular using models
in the NADE family (Neural Autoregressive Density Estimator) as output
distributions of the transition operator. A NADE model is related to an RBM
(and can thus model multimodal distributions) but its likelihood (and
likelihood gradient) can be computed easily. The parameters of the NADE are
obtained as a learned function of the previous state of the learned Markov
chain. Experiments clearly illustrate the advantage of such multimodal
transition distributions over unimodal GSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5598</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5598</id><created>2013-12-19</created><updated>2014-12-19</updated><authors><author><keyname>Bozzo</keyname><forenames>Enrico</forenames></author><author><keyname>Franceschet</keyname><forenames>Massimo</forenames></author><author><keyname>Rinaldi</keyname><forenames>Franca</forenames></author></authors><title>Vulnerability and power on networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by socio-political scenarios, like dictatorships, in which a
minority of people exercise control over a majority of weakly interconnected
individuals, we propose vulnerability and power measures defined on groups of
actors of networks. We establish an unexpected connection between network
vulnerability and graph regularizability. We use the Shapley value of coalition
games to introduce fresh notions of vulnerability and power at node level
defined in terms of the corresponding measures at group level. We investigate
the computational complexity of computing the defined measures, both at group
and node levels, and provide effective methods to quantify them. Finally we
test vulnerability and power on both artificial and real networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5602</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5602</id><created>2013-12-19</created><authors><author><keyname>Mnih</keyname><forenames>Volodymyr</forenames></author><author><keyname>Kavukcuoglu</keyname><forenames>Koray</forenames></author><author><keyname>Silver</keyname><forenames>David</forenames></author><author><keyname>Graves</keyname><forenames>Alex</forenames></author><author><keyname>Antonoglou</keyname><forenames>Ioannis</forenames></author><author><keyname>Wierstra</keyname><forenames>Daan</forenames></author><author><keyname>Riedmiller</keyname><forenames>Martin</forenames></author></authors><title>Playing Atari with Deep Reinforcement Learning</title><categories>cs.LG</categories><comments>NIPS Deep Learning Workshop 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5604</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5604</id><created>2013-12-19</created><updated>2014-02-06</updated><authors><author><keyname>Qiu</keyname><forenames>Qiang</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Learning Transformations for Classification Forests</title><categories>cs.CV cs.LG stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:1309.2074</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces a transformation-based learner model for classification
forests. The weak learner at each split node plays a crucial role in a
classification tree. We propose to optimize the splitting objective by learning
a linear transformation on subspaces using nuclear norm as the optimization
criteria. The learned linear transformation restores a low-rank structure for
data from the same class, and, at the same time, maximizes the separation
between different classes, thereby improving the performance of the split
function. Theoretical and experimental results support the proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5620</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5620</id><created>2013-12-19</created><authors><author><keyname>Borozan</keyname><forenames>Valentin</forenames></author><author><keyname>Montero</keyname><forenames>Leandro</forenames></author><author><keyname>Narayanan</keyname><forenames>Narayanan</forenames></author></authors><title>Further results on strong edge-colourings in outerplanar graphs</title><categories>cs.DM math.CO</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An edge-colouring is {\em strong} if every colour class is an induced
matching. In this work we give a formulae that determines either the optimal or
the optimal plus one strong chromatic index of bipartite outerplanar graphs.
Further, we give an improved upper bound for any outerplanar graph which is
close to optimal. All our proofs yield efficient algorithms to construct such
colourings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5641</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5641</id><created>2013-12-19</created><updated>2014-03-27</updated><authors><author><keyname>Qiu</keyname><forenames>Chenlu</forenames></author><author><keyname>Vaswani</keyname><forenames>Namrata</forenames></author><author><keyname>Lois</keyname><forenames>Brian</forenames></author><author><keyname>Hogben</keyname><forenames>Leslie</forenames></author></authors><title>Recursive Robust PCA or Recursive Sparse Recovery in Large but
  Structured Noise (parts 1 and 2 combined)</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn because it is available at
  arXiv:1211.3754</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the recursive robust principal components analysis (PCA)
problem. If the outlier is the signal-of-interest, this problem can be
interpreted as one of recursively recovering a time sequence of sparse vectors,
$S_t$, in the presence of large but structured noise, $L_t$. The structure that
we assume on $L_t$ is that $L_t$ is dense and lies in a low dimensional
subspace that is either fixed or changes &quot;slowly enough&quot;. A key application
where this problem occurs is in video surveillance where the goal is to
separate a slowly changing background ($L_t$) from moving foreground objects
($S_t$) on-the-fly. To solve the above problem, in recent work, we introduced a
novel solution called Recursive Projected CS (ReProCS). In this work we develop
a simple modification of the original ReProCS idea and analyze it. This
modification assumes knowledge of a subspace change model on the $L_t$'s. Under
mild assumptions and a denseness assumption on the unestimated part of the
subspace of $L_t$ at various times, we show that, with high probability
(w.h.p.), the proposed approach can exactly recover the support set of $S_t$ at
all times; and the reconstruction errors of both $S_t$ and $L_t$ are upper
bounded by a time-invariant and small value. In simulation experiments, we
observe that the last assumption holds as long as there is some support change
of $S_t$ every few frames.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5650</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5650</id><created>2013-12-19</created><updated>2014-03-21</updated><authors><author><keyname>Norouzi</keyname><forenames>Mohammad</forenames></author><author><keyname>Mikolov</keyname><forenames>Tomas</forenames></author><author><keyname>Bengio</keyname><forenames>Samy</forenames></author><author><keyname>Singer</keyname><forenames>Yoram</forenames></author><author><keyname>Shlens</keyname><forenames>Jonathon</forenames></author><author><keyname>Frome</keyname><forenames>Andrea</forenames></author><author><keyname>Corrado</keyname><forenames>Greg S.</forenames></author><author><keyname>Dean</keyname><forenames>Jeffrey</forenames></author></authors><title>Zero-Shot Learning by Convex Combination of Semantic Embeddings</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several recent publications have proposed methods for mapping images into
continuous semantic embedding spaces. In some cases the embedding space is
trained jointly with the image transformation. In other cases the semantic
embedding space is established by an independent natural language processing
task, and then the image transformation into that space is learned in a second
stage. Proponents of these image embedding systems have stressed their
advantages over the traditional \nway{} classification framing of image
understanding, particularly in terms of the promise for zero-shot learning --
the ability to correctly annotate images of previously unseen object
categories. In this paper, we propose a simple method for constructing an image
embedding system from any existing \nway{} image classifier and a semantic word
embedding model, which contains the $\n$ class labels in its vocabulary. Our
method maps images into the semantic embedding space via convex combination of
the class label embedding vectors, and requires no additional training. We show
that this simple and direct method confers many of the advantages associated
with more complex image embedding schemes, and indeed outperforms state of the
art methods on the ImageNet zero-shot learning task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5663</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5663</id><created>2013-12-19</created><updated>2014-03-22</updated><authors><author><keyname>Makhzani</keyname><forenames>Alireza</forenames></author><author><keyname>Frey</keyname><forenames>Brendan</forenames></author></authors><title>k-Sparse Autoencoders</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it has been observed that when representations are learnt in a way
that encourages sparsity, improved performance is obtained on classification
tasks. These methods involve combinations of activation functions, sampling
steps and different kinds of penalties. To investigate the effectiveness of
sparsity by itself, we propose the k-sparse autoencoder, which is an
autoencoder with linear activation function, where in hidden layers only the k
highest activities are kept. When applied to the MNIST and NORB datasets, we
find that this method achieves better classification results than denoising
autoencoders, networks trained with dropout, and RBMs. k-sparse autoencoders
are simple to train and the encoding stage is very fast, making them
well-suited to large problem sizes, where conventional sparse coding algorithms
cannot be applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5667</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5667</id><created>2013-12-19</created><authors><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author><author><keyname>Deb</keyname><forenames>Suash</forenames></author><author><keyname>Loomes</keyname><forenames>M.</forenames></author><author><keyname>Karamanoglu</keyname><forenames>M.</forenames></author></authors><title>A Framework for Self-Tuning Optimization Algorithm</title><categories>math.OC cs.DS nlin.AO</categories><comments>12 pages</comments><msc-class>80M50</msc-class><journal-ref>Neural Computing and Applications, Vol. 23, No. 7-8, pp. 2051-2057
  (2013)</journal-ref><doi>10.1007/s00521-013-1498-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of any algorithm will largely depend on the setting of its
algorithm-dependent parameters. The optimal setting should allow the algorithm
to achieve the best performance for solving a range of optimization problems.
However, such parameter-tuning itself is a tough optimization problem. In this
paper, we present a framework for self-tuning algorithms so that an algorithm
to be tuned can be used to tune the algorithm itself. Using the firefly
algorithm as an example, we show that this framework works well. It is also
found that different parameters may have different sensitivities, and thus
require different degrees of tuning. Parameters with high sensitivities require
fine-tuning to achieve optimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5670</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5670</id><created>2013-12-19</created><authors><author><keyname>Vines</keyname><forenames>Timothy</forenames></author><author><keyname>Albert</keyname><forenames>Arianne</forenames></author><author><keyname>Andrew</keyname><forenames>Rose</forenames></author><author><keyname>Debarr&#xe9;</keyname><forenames>Florence</forenames></author><author><keyname>Bock</keyname><forenames>Dan</forenames></author><author><keyname>Franklin</keyname><forenames>Michelle</forenames></author><author><keyname>Gilbert</keyname><forenames>Kimberley</forenames></author><author><keyname>Moore</keyname><forenames>Jean-S&#xe9;bastien</forenames></author><author><keyname>Renaut</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Rennison</keyname><forenames>Diana J.</forenames></author></authors><title>The availability of research data declines rapidly with article age</title><categories>cs.DL physics.soc-ph q-bio.PE</categories><comments>14 pages, 2 figures</comments><doi>10.1016/j.cub.2013.11.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Policies ensuring that research data are available on public archives are
increasingly being implemented at the government [1], funding agency [2-4], and
journal [5,6] level. These policies are predicated on the idea that authors are
poor stewards of their data, particularly over the long term [7], and indeed
many studies have found that authors are often unable or unwilling to share
their data [8-11]. However, there are no systematic estimates of how the
availability of research data changes with time since publication. We therefore
requested datasets from a relatively homogenous set of 516 articles published
between 2 and 22 years ago, and found that availability of the data was
strongly affected by article age. For papers where the authors gave the status
of their data, the odds of a dataset being extant fell by 17% per year. In
addition, the odds that we could find a working email address for the first,
last or corresponding author fell by 7% per year. Our results reinforce the
notion that, in the long term, research data cannot be reliably preserved by
individual researchers, and further demonstrate the urgent need for policies
mandating data sharing via public archives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5673</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5673</id><created>2013-12-19</created><authors><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author></authors><title>Flower Pollination Algorithm for Global Optimization</title><categories>math.OC cs.NE nlin.AO</categories><comments>10 pages</comments><journal-ref>Unconventional Computation and Natural Computation 2012, Lecture
  Notes in Computer Science, Vol. 7445, pp. 240-249 (2012)</journal-ref><doi>10.1007/978-3-642-32894-7_27</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flower pollination is an intriguing process in the natural world. Its
evolutionary characteristics can be used to design new optimization algorithms.
In this paper, we propose a new algorithm, namely, flower pollination
algorithm, inspired by the pollination process of flowers. We first use ten
test functions to validate the new algorithm, and compare its performance with
genetic algorithms and particle swarm optimization. Our simulation results show
the flower algorithm is more efficient than both GA and PSO. We also use the
flower algorithm to solve a nonlinear design benchmark, which shows the
convergence rate is almost exponential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5686</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5686</id><created>2013-12-19</created><updated>2016-02-04</updated><authors><author><keyname>Schmitz</keyname><forenames>Sylvain</forenames></author></authors><title>Complexity Hierarchies Beyond Elementary</title><categories>cs.CC cs.LO</categories><comments>Version 3 is the published version in TOCT 8(1:3), 2016. I will keep
  updating the catalogue of problems from Section 6 in future revisions</comments><acm-class>F.1.3</acm-class><journal-ref>ACM Transactions on Computation Theory vol. 8, number 1, article
  3, 2016</journal-ref><doi>10.1145/2858784</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a hierarchy of fast-growing complexity classes and show its
suitability for completeness statements of many non elementary problems. This
hierarchy allows the classification of many decision problems with a
non-elementary complexity, which occur naturally in logic, combinatorics,
formal languages, verification, etc., with complexities ranging from simple
towers of exponentials to Ackermannian and beyond.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5691</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5691</id><created>2013-12-19</created><authors><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Tardieu</keyname><forenames>Olivier</forenames></author><author><keyname>Grove</keyname><forenames>David</forenames></author><author><keyname>Herta</keyname><forenames>Benjamin</forenames></author><author><keyname>Kamada</keyname><forenames>Tomio</forenames></author><author><keyname>Saraswat</keyname><forenames>Vijay</forenames></author><author><keyname>Takeuchi</keyname><forenames>Mikio</forenames></author></authors><title>GLB: Lifeline-based Global Load Balancing library in X10</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present GLB, a programming model and an associated implementation that can
handle a wide range of irregular paral- lel programming problems running over
large-scale distributed systems. GLB is applicable both to problems that are
easily load-balanced via static scheduling and to problems that are hard to
statically load balance. GLB hides the intricate syn- chronizations (e.g.,
inter-node communication, initialization and startup, load balancing,
termination and result collection) from the users. GLB internally uses a
version of the lifeline graph based work-stealing algorithm proposed by
Saraswat et al. Users of GLB are simply required to write several pieces of
sequential code that comply with the GLB interface. GLB then schedules and
orchestrates the parallel execution of the code correctly and efficiently at
scale. We have applied GLB to two representative benchmarks: Betweenness
Centrality (BC) and Unbalanced Tree Search (UTS). Among them, BC can be
statically load-balanced whereas UTS cannot. In either case, GLB scales well--
achieving nearly linear speedup on different computer architectures (Power,
Blue Gene/Q, and K) -- up to 16K cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5692</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5692</id><created>2013-12-17</created><authors><author><keyname>Mayer</keyname><forenames>Robert V</forenames></author></authors><title>Multi-component model of learning and its use for research didactic
  system</title><categories>cs.CY</categories><comments>5 pages, 2 figures, on russian</comments><msc-class>68U20</msc-class><acm-class>I.6.0</acm-class><journal-ref>Fundamental research 2013 10 part 11 pp. 2524 - 2528</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computer simulation of the learning process is usually assumed that all
elements of the training material are assimilated equally durable. But in
practice, the knowledge, which a student uses in its operations, are remembered
much better. For a more precise study of didactic systems the multi component
model of learning are proposed. It takes into account: 1) the transition of
weak knowledge in trustworthy knowledge; 2) the difference in the rate of
forgetting the trustworthy and weak knowledge. It is assumed that the rate of
increase of student's knowledge is proportional to: 1) the difference between
the level of the requirements of teachers and the number of learned knowledge;
2) the amount of learned knowledge, raised to some power. Examples of the use
of a multi component model for the study of situations in the learning process
are considered, the resulting graphs of the student's level of knowledge of the
time are presented. A generalized model of learning, which allows to take into
account the complexity of the various elements of the educational material are
proposed. The possibility of creating a training program for the training of
students of pedagogical institutes are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5697</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5697</id><created>2013-12-19</created><updated>2013-12-20</updated><authors><author><keyname>Bengio</keyname><forenames>Samy</forenames></author><author><keyname>Dean</keyname><forenames>Jeff</forenames></author><author><keyname>Erhan</keyname><forenames>Dumitru</forenames></author><author><keyname>Ie</keyname><forenames>Eugene</forenames></author><author><keyname>Le</keyname><forenames>Quoc</forenames></author><author><keyname>Rabinovich</keyname><forenames>Andrew</forenames></author><author><keyname>Shlens</keyname><forenames>Jonathon</forenames></author><author><keyname>Singer</keyname><forenames>Yoram</forenames></author></authors><title>Using Web Co-occurrence Statistics for Improving Image Categorization</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object recognition and localization are important tasks in computer vision.
The focus of this work is the incorporation of contextual information in order
to improve object recognition and localization. For instance, it is natural to
expect not to see an elephant to appear in the middle of an ocean. We consider
a simple approach to encapsulate such common sense knowledge using
co-occurrence statistics from web documents. By merely counting the number of
times nouns (such as elephants, sharks, oceans, etc.) co-occur in web
documents, we obtain a good estimate of expected co-occurrences in visual data.
We then cast the problem of combining textual co-occurrence statistics with the
predictions of image-based classifiers as an optimization problem. The
resulting optimization problem serves as a surrogate for our inference
procedure. Albeit the simplicity of the resulting optimization problem, it is
effective in improving both recognition and localization accuracy. Concretely,
we observe significant improvements in recognition and localization rates for
both ImageNet Detection 2012 and Sun 2012 datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5704</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5704</id><created>2013-12-19</created><authors><author><keyname>Achballah</keyname><forenames>Ahmed Ben</forenames></author><author><keyname>Othman</keyname><forenames>Slim Ben</forenames></author><author><keyname>Saoud</keyname><forenames>Slim Ben</forenames></author></authors><title>Design of Field Programmable Gate Array (FPGA) Based Emulators for Motor
  Control Applications</title><categories>cs.SY</categories><journal-ref>Am J. Applied Sci., 9: 1166-1181</journal-ref><doi>10.3844/ajassp.2012.1166.1181</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Problem Statement: Field Programmable Gate Array (FPGA) circuits play a
significant role in major recent embedded process control designs. However,
exploiting these platforms requires deep hardware conception skills and remains
an important time consuming stage in a design flow. High Level Synthesis
technique avoids this bottleneck and increases design productivity as witnessed
by industry specialists. Approach: This study proposes to apply this technique
for the conception and implementation of a Real Time Direct Current Machine
(RTDCM) emulator for an embedded control application. Results: Several
FPGA-based configuration scenarios are studied. A series of tests including
design and timing-precision analysis were conducted to discuss and validate the
obtained hardware architectures. Conclusion/Recommendations: The proposed
methodology has accelerated the design time besides it has provided an extra
time to refine the hardware core of the DCM emulator. The high level synthesis
technique can be applied to the control field especially to test with low cost
and short delays newest algorithms and motor models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5713</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5713</id><created>2013-12-19</created><updated>2015-03-31</updated><authors><author><keyname>Dobrev</keyname><forenames>Dimiter</forenames></author></authors><title>Giving the AI definition a form suitable for the engineer</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial Intelligence - what is this? That is the question! In earlier
papers we already gave a formal definition for AI, but if one desires to build
an actual AI implementation, the following issues require attention and are
treated here: the data format to be used, the idea of Undef and Nothing
symbols, various ways for defining the &quot;meaning of life&quot;, and finally, a new
notion of &quot;incorrect move&quot;. These questions are of minor importance in the
theoretical discussion, but we already know the answer of the question &quot;Does AI
exist?&quot; Now we want to make the next step and to create this program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5714</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5714</id><created>2013-12-19</created><updated>2015-02-18</updated><authors><author><keyname>Connor</keyname><forenames>Patrick C.</forenames></author><author><keyname>Trappenberg</keyname><forenames>Thomas P.</forenames></author></authors><title>Avoiding Confusion between Predictors and Inhibitors in Value Function
  Approximation</title><categories>cs.AI</categories><comments>14 pages, 3 figures, 23 references, Workshop paper in ICLR 2014
  (updated based on reviewer comments)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In reinforcement learning, the goal is to seek rewards and avoid punishments.
A simple scalar captures the value of a state or of taking an action, where
expected future rewards increase and punishments decrease this quantity.
Naturally an agent should learn to predict this quantity to take beneficial
actions, and many value function approximators exist for this purpose. In the
present work, however, we show how value function approximators can cause
confusion between predictors of an outcome of one valence (e.g., a signal of
reward) and the inhibitor of the opposite valence (e.g., a signal canceling
expectation of punishment). We show this to be a problem for both linear and
non-linear value function approximators, especially when the amount of data (or
experience) is limited. We propose and evaluate a simple resolution: to instead
predict reward and punishment values separately, and rectify and add them to
get the value needed for decision making. We evaluate several function
approximators in this slightly different value function approximation
architecture and show that this approach is able to circumvent the confusion
and thereby achieve lower value-prediction errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5734</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5734</id><created>2013-12-19</created><authors><author><keyname>Lan</keyname><forenames>Andrew S.</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Time-varying Learning and Content Analytics via Sparse Factor Analysis</title><categories>stat.ML cs.LG math.OC stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose SPARFA-Trace, a new machine learning-based framework for
time-varying learning and content analytics for education applications. We
develop a novel message passing-based, blind, approximate Kalman filter for
sparse factor analysis (SPARFA), that jointly (i) traces learner concept
knowledge over time, (ii) analyzes learner concept knowledge state transitions
(induced by interacting with learning resources, such as textbook sections,
lecture videos, etc, or the forgetting effect), and (iii) estimates the content
organization and intrinsic difficulty of the assessment questions. These
quantities are estimated solely from binary-valued (correct/incorrect) graded
learner response data and a summary of the specific actions each learner
performs (e.g., answering a question or studying a learning resource) at each
time instance. Experimental results on two online course datasets demonstrate
that SPARFA-Trace is capable of tracing each learner's concept knowledge
evolution over time, as well as analyzing the quality and content organization
of learning resources, the question-concept associations, and the question
intrinsic difficulties. Moreover, we show that SPARFA-Trace achieves comparable
or better performance in predicting unobserved learner responses than existing
collaborative filtering and knowledge tracing approaches for personalized
education.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5739</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5739</id><created>2013-12-19</created><authors><author><keyname>Ensafi</keyname><forenames>Roya</forenames></author><author><keyname>Knockel</keyname><forenames>Jeffrey</forenames></author><author><keyname>Alexander</keyname><forenames>Geoffrey</forenames></author><author><keyname>Crandall</keyname><forenames>Jedidiah R.</forenames></author></authors><title>Detecting Intentional Packet Drops on the Internet via TCP/IP Side
  Channels: Extended Version</title><categories>cs.NI</categories><comments>This is the extended version of a paper from the 2014 Passive and
  Active Measurements Conference (PAM), March 10th-11th, 2014, Los Angeles,
  California</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method for remotely detecting intentional packet drops on the
Internet via side channel inferences. That is, given two arbitrary IP addresses
on the Internet that meet some simple requirements, our proposed technique can
discover packet drops (e.g., due to censorship) between the two remote
machines, as well as infer in which direction the packet drops are occurring.
The only major requirements for our approach are a client with a global IP
Identifier (IPID) and a target server with an open port. We require no special
access to the client or server. Our method is robust to noise because we apply
intervention analysis based on an autoregressive-moving-average (ARMA) model.
In a measurement study using our method featuring clients from multiple
continents, we observed that, of all measured client connections to Tor
directory servers that were censored, 98% of those were from China, and only
0.63% of measured client connections from China to Tor directory servers were
not censored. This is congruent with current understandings about global
Internet censorship, leading us to conclude that our method is effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5753</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5753</id><created>2013-12-18</created><authors><author><keyname>Kind</keyname><forenames>M. Carrasco</forenames><affiliation>Department of Astronomy, University of Illinois at Urbana-Champaign</affiliation></author><author><keyname>Brunner</keyname><forenames>R. J.</forenames><affiliation>Department of Astronomy, University of Illinois at Urbana-Champaign</affiliation></author></authors><title>SOMz: photometric redshift PDFs with self organizing maps and random
  atlas</title><categories>astro-ph.IM astro-ph.CO cs.LG stat.ML</categories><comments>14 pages, 8 figures. Accepted for publication in MNRAS. The code can
  be found at http://lcdm.astro.illinois.edu/research/SOMZ.html</comments><doi>10.1093/mnras/stt2456</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore the applicability of the unsupervised machine
learning technique of Self Organizing Maps (SOM) to estimate galaxy photometric
redshift probability density functions (PDFs). This technique takes a
spectroscopic training set, and maps the photometric attributes, but not the
redshifts, to a two dimensional surface by using a process of competitive
learning where neurons compete to more closely resemble the training data
multidimensional space. The key feature of a SOM is that it retains the
topology of the input set, revealing correlations between the attributes that
are not easily identified. We test three different 2D topological mapping:
rectangular, hexagonal, and spherical, by using data from the DEEP2 survey. We
also explore different implementations and boundary conditions on the map and
also introduce the idea of a random atlas where a large number of different
maps are created and their individual predictions are aggregated to produce a
more robust photometric redshift PDF. We also introduced a new metric, the
$I$-score, which efficiently incorporates different metrics, making it easier
to compare different results (from different parameters or different
photometric redshift codes). We find that by using a spherical topology mapping
we obtain a better representation of the underlying multidimensional topology,
which provides more accurate results that are comparable to other,
state-of-the-art machine learning algorithms. Our results illustrate that
unsupervised approaches have great potential for many astronomical problems,
and in particular for the computation of photometric redshifts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5763</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5763</id><created>2013-12-19</created><authors><author><keyname>Ahmed</keyname><forenames>Rashid</forenames></author><author><keyname>Avaritsiotis</keyname><forenames>John N.</forenames></author></authors><title>Identification of Employees Using RFID in IE-NTUA</title><categories>cs.SY</categories><comments>5 pages, 6 figures, 1 table, IJACSA journal 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the last decade with the rapid increase in indoor wireless
communications, location-aware services have received a great deal of attention
for commercial, public-safety, and a military application, the greatest
challenge associated with indoor positioning methods is moving object data and
identification. Mobility tracking and localization are multifaceted problems,
which have been studied for a long time in different contexts. Many potential
applications in the domain of WSNs require such capabilities. The mobility
tracking needs inherent in many surveillance, security and logistic
applications. This paper presents the identification of employees in National
Technical University in Athens (IE-NTUA), when the employees access to a
certain area of the building (enters and leaves to/from the college), Radio
Frequency Identification (RFID) applied for identification by offering special
badges containing RFID-tags.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5764</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5764</id><created>2013-12-19</created><authors><author><keyname>Benhaoua</keyname><forenames>Mohammed kamel</forenames></author><author><keyname>Benyamina</keyname><forenames>Abbou El Hassen</forenames></author><author><keyname>Boulet</keyname><forenames>Pierre</forenames></author></authors><title>Heuristics for Routing and Spiral Run-time Task Mapping in NoC-based
  Heterogeneous MPSOCs</title><categories>cs.OH</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 4, No 1, July 2013 ISSN (Print): 1694-0814 | ISSN (Online): 1694-0784</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new Spiral Dynamic Task Mapping heuristic for mapping
applications onto NoC-based Heterogeneous MPSoC. The heuristic proposed in this
paper attempts to map the tasks of an applications that are most related to
each other in spiral manner and to find the best possible path load that
minimizes the communication overhead. In this context, we have realized a
simulation environment for experimental evaluations to map applications with
varying number of tasks onto an 8x8 NoC-based Heterogeneous MPSoCs platform, we
demonstrate that the new mapping heuristics with the new modified dijkstra
routing algorithm proposed are capable of reducing the total execution time and
energy consumption of applications when compared to state-of the-art run-time
mapping heuristics reported in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5765</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5765</id><created>2013-12-19</created><updated>2014-07-01</updated><authors><author><keyname>Rossi</keyname><forenames>Marco</forenames></author><author><keyname>Haimovich</keyname><forenames>Alexander M.</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Multi-Branch Matching Pursuit with applications to MIMO radar</title><categories>cs.IT math.IT math.OC</categories><comments>Submitted to IEEE Transaction on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm, dubbed Multi-Branch Matching Pursuit (MBMP), to
solve the sparse recovery problem over redundant dictionaries. MBMP combines
three different paradigms: being a greedy method, it performs iterative signal
support estimation; as a rank-aware method, it is able to exploit signal
subspace information when multiple snapshots are available; and, as its name
foretells, it leverages a multi-branch (i.e., tree-search) strategy that allows
us to trade-off hardware complexity (e.g. measurements) for computational
complexity. We derive a sufficient condition under which MBMP can recover a
sparse signal from noiseless measurements. This condition, named MB-coherence,
is met when the dictionary is sufficiently incoherent. It incorporates the
number of branches of MBMP and it requires fewer measurements than other
conditions (e.g. the Neuman ERC or the cumulative coherence). As such,
successful recovery with MBMP is guaranteed for dictionaries that do not
satisfy previously known conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5766</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5766</id><created>2013-12-19</created><updated>2013-12-30</updated><authors><author><keyname>Lee</keyname><forenames>Seunghak</forenames></author><author><keyname>Kim</keyname><forenames>Jin Kyu</forenames></author><author><keyname>Ho</keyname><forenames>Qirong</forenames></author><author><keyname>Gibson</keyname><forenames>Garth A.</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Structure-Aware Dynamic Scheduler for Parallel Machine Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training large machine learning (ML) models with many variables or parameters
can take a long time if one employs sequential procedures even with stochastic
updates. A natural solution is to turn to distributed computing on a cluster;
however, naive, unstructured parallelization of ML algorithms does not usually
lead to a proportional speedup and can even result in divergence, because
dependencies between model elements can attenuate the computational gains from
parallelization and compromise correctness of inference. Recent efforts toward
this issue have benefited from exploiting the static, a priori block structures
residing in ML algorithms. In this paper, we take this path further by
exploring the dynamic block structures and workloads therein present during ML
program execution, which offers new opportunities for improving convergence,
correctness, and load balancing in distributed ML. We propose and showcase a
general-purpose scheduler, STRADS, for coordinating distributed updates in ML
algorithms, which harnesses the aforementioned opportunities in a systematic
way. We provide theoretical guarantees for our scheduler, and demonstrate its
efficacy versus static block structures on Lasso and Matrix Factorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5770</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5770</id><created>2013-12-19</created><updated>2014-02-04</updated><authors><author><keyname>Kpotufe</keyname><forenames>Samory</forenames></author><author><keyname>Sgouritsa</keyname><forenames>Eleni</forenames></author><author><keyname>Janzing</keyname><forenames>Dominik</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Consistency of Causal Inference under the Additive Noise Model</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a family of methods for statistical causal inference from sample
under the so-called Additive Noise Model. While most work on the subject has
concentrated on establishing the soundness of the Additive Noise Model, the
statistical consistency of the resulting inference methods has received little
attention. We derive general conditions under which the given family of
inference methods consistently infers the causal direction in a nonparametric
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5777</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5777</id><created>2013-12-19</created><updated>2014-10-24</updated><authors><author><keyname>Bytev</keyname><forenames>Vladimir V.</forenames></author><author><keyname>Kalmykov</keyname><forenames>Mikhail Yu.</forenames></author><author><keyname>Moch</keyname><forenames>Sven-Olaf</forenames></author></authors><title>HYPERDIRE: HYPERgeometric functions DIfferential REduction: MATHEMATICA
  based packages for differential reduction of generalized hypergeometric
  functions: $F_D$ and $F_S$ Horn-type hypergeometric functions of three
  variables</title><categories>math-ph cs.SC hep-ph hep-th math.MP</categories><comments>v3=v2: A new section 5 is added; published version</comments><journal-ref>Comput.Phys. Commun. 185 (2014) 3041-3058</journal-ref><doi>10.1016/j.cpc.2014.07.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  HYPERDIRE is a project devoted to the creation of a set of Mathematica based
programs for the differential reduction of hypergeometric functions. The
current version includes two parts: the first one, FdFunction, for
manipulations with Appell hypergeometric functions $F_D$ of $r$ variables; and
the second one, FsFunction, for manipulations with Lauricella-Saran
hypergeometric functions $F_S$ of three variables. Both functions are related
with one-loop Feynman diagrams.
  The published version includes also Chapter 5 with two theorems about
structure of coefficients of epsilon-expansion of the Horn-type hypergeometric
functions. As illustration, the first three coefficients of epsilon-expansion
for the Appell hypergeometric function FD of r-variables are explicitly
evaluated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5783</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5783</id><created>2013-12-19</created><authors><author><keyname>He</keyname><forenames>Yunlong</forenames></author><author><keyname>Kavukcuoglu</keyname><forenames>Koray</forenames></author><author><keyname>Wang</keyname><forenames>Yun</forenames></author><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author><author><keyname>Qi</keyname><forenames>Yanjun</forenames></author></authors><title>Unsupervised Feature Learning by Deep Sparse Coding</title><categories>cs.LG cs.CV cs.NE</categories><comments>9 pages, submitted to ICLR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new unsupervised feature learning framework,
namely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layer
architecture for visual object recognition tasks. The main innovation of the
framework is that it connects the sparse-encoders from different layers by a
sparse-to-dense module. The sparse-to-dense module is a composition of a local
spatial pooling step and a low-dimensional embedding process, which takes
advantage of the spatial smoothness information in the image. As a result, the
new method is able to learn several levels of sparse representation of the
image which capture features at a variety of abstraction levels and
simultaneously preserve the spatial smoothness between the neighboring image
patches. Combining the feature representations from multiple layers, DeepSC
achieves the state-of-the-art performance on multiple object recognition tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5785</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5785</id><created>2013-12-19</created><updated>2014-03-27</updated><authors><author><keyname>Tran</keyname><forenames>Du</forenames></author><author><keyname>Torresani</keyname><forenames>Lorenzo</forenames></author></authors><title>EXMOVES: Classifier-based Features for Scalable Action Recognition</title><categories>cs.CV</categories><comments>In Proceedings of the 2nd International Conference on Learning
  Representations, Banff, Canada, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces EXMOVES, learned exemplar-based features for efficient
recognition of actions in videos. The entries in our descriptor are produced by
evaluating a set of movement classifiers over spatial-temporal volumes of the
input sequence. Each movement classifier is a simple exemplar-SVM trained on
low-level features, i.e., an SVM learned using a single annotated positive
space-time volume and a large number of unannotated videos.
  Our representation offers two main advantages. First, since our mid-level
features are learned from individual video exemplars, they require minimal
amount of supervision. Second, we show that simple linear classification models
trained on our global video descriptor yield action recognition accuracy
approaching the state-of-the-art but at orders of magnitude lower cost, since
at test-time no sliding window is necessary and linear models are efficient to
train and test. This enables scalable action recognition, i.e., efficient
classification of a large number of different actions even in large video
databases. We show the generality of our approach by building our mid-level
descriptors from two different low-level feature representations. The accuracy
and efficiency of the approach are demonstrated on several large-scale action
recognition benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5794</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5794</id><created>2013-12-19</created><authors><author><keyname>Kim</keyname><forenames>Dong Min</forenames></author><author><keyname>Hwang</keyname><forenames>Young Ju</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author><author><keyname>Jin</keyname><forenames>Gwang-Ja</forenames></author><author><keyname>Kim</keyname><forenames>Bong-Soo</forenames></author></authors><title>Random Basketball Routing for ZigBee based Sensor Networks</title><categories>cs.NI cs.IT math.IT</categories><journal-ref>in Proc. IEEE APWCS 2007, Hsinchu, Taiwan, August, 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random basketball routing (BR) \cite {Hwang} is a simple protocol that
integrates MAC and multihop routing in a cross-layer optimized manner. Due to
its lightness and performance, BR would be quite suitable for sensor networks,
where communication nodes are usually simple devices. In this paper, we
describe how we implemented BR in a ZigBee-based (IEEE 802.15.4) sensor
network. In \cite{Hwang}, it is verified that BR takes advantages of dynamic
environments (in particular, node mobility), however, here we focus on how BR
works under static situations. For implementation purposes, we add some
features such as destination RSSI measuring and loop-free procedure, to the
original BR. With implemented testbed, we compare the performance of BR with
that of the simplified AODV with CSMA/CA. The result is that BR has merits in
terms of number of hops to traverse the network. Considering the simple
structure of BR and its possible energy-efficiency, we can conclude that BR can
be a good candidate for sensor networks both under dynamic- and static
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5797</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5797</id><created>2013-12-19</created><authors><author><keyname>Kim</keyname><forenames>Dong Min</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>Network Coded Rate Scheduling for Two-way Relay Networks</title><categories>cs.IT math.IT</categories><journal-ref>in Proc. European Wireless 2011, Vienna, Austria, April 27-29,
  2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a scheduling scheme incorporating network coding and channel
varying information for the two-way relay networks. Our scheduler aims at
minimizing the time span needed to send all the data of each source of the
network. We consider three channel models, time invariant channels, time
varying channels with finite states and time varying Rayleigh fading channels.
We formulate the problem into a manageable optimization problem, and get a
closed form scheduler under time invariant channels and time varying channel
with finite channel states. For Rayleigh fading channels, we focus on the relay
node operation and propose heuristic power allocation algorithm resemblant to
water filling algorithm. By simulations, we find that even if the channel
fluctuates randomly, the average time span is minimized when the relay node
transmits/schedules network coded data as much as possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5799</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5799</id><created>2013-12-19</created><updated>2014-03-01</updated><authors><author><keyname>Fercoq</keyname><forenames>Olivier</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author></authors><title>Accelerated, Parallel and Proximal Coordinate Descent</title><categories>math.OC cs.DC cs.NA math.NA stat.ML</categories><comments>25 pages, 2 algorithms, 6 tables, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new stochastic coordinate descent method for minimizing the sum
of convex functions each of which depends on a small number of coordinates
only. Our method (APPROX) is simultaneously Accelerated, Parallel and PROXimal;
this is the first time such a method is proposed. In the special case when the
number of processors is equal to the number of coordinates, the method
converges at the rate $2\bar{\omega}\bar{L} R^2/(k+1)^2 $, where $k$ is the
iteration counter, $\bar{\omega}$ is an average degree of separability of the
loss function, $\bar{L}$ is the average of Lipschitz constants associated with
the coordinates and individual functions in the sum, and $R$ is the distance of
the initial point from the minimizer. We show that the method can be
implemented without the need to perform full-dimensional vector operations,
which is the major bottleneck of existing accelerated coordinate descent
methods. The fact that the method depends on the average degree of
separability, and not on the maximum degree of separability, can be attributed
to the use of new safe large stepsizes, leading to improved expected separable
overapproximation (ESO). These are of independent interest and can be utilized
in all existing parallel stochastic coordinate descent algorithms based on the
concept of ESO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5800</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5800</id><created>2013-12-19</created><authors><author><keyname>Kim</keyname><forenames>Dong Min</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>An Iterative Algorithm for Optimal Carrier Sensing Threshold in Random
  CSMA/CA Wireless Networks</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Communications Letters, 17 (11), pp. 2076-2079, November 2013</journal-ref><doi>10.1109/LCOMM.2013.092013.131519</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the optimal carrier sensing threshold in random CSMA/CA
networks considering the effect of binary exponential backoff. We propose an
iterative algorithm for optimizing the carrier sensing threshold and hence
maximizing the area spectral efficiency. We verify that simulations are
consistent with our analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5813</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5813</id><created>2013-12-20</created><updated>2014-06-09</updated><authors><author><keyname>Li</keyname><forenames>Jun</forenames></author><author><keyname>Luo</keyname><forenames>Wei</forenames></author><author><keyname>Yang</keyname><forenames>Jian</forenames></author><author><keyname>Yuan</keyname><forenames>Xiaotong</forenames></author></authors><title>Unsupervised Pretraining Encourages Moderate-Sparseness</title><categories>cs.LG cs.NE</categories><comments>6 pages, 2 figures, (to appear) ICML-Workshop on Unsupervised
  Learning from Bioacoustic Big Data (uLearnBio) 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  It is well known that direct training of deep neural networks will generally
lead to poor results. A major progress in recent years is the invention of
various pretraining methods to initialize network parameters and it was shown
that such methods lead to good prediction performance. However, the reason for
the success of pretraining has not been fully understood, although it was
argued that regularization and better optimization play certain roles. This
paper provides another explanation for the effectiveness of pretraining, where
we show pretraining leads to a sparseness of hidden unit activation in the
resulting neural networks. The main reason is that the pretraining models can
be interpreted as an adaptive sparse coding. Compared to deep neural network
with sigmoid function, our experimental results on MNIST and Birdsong further
support this sparseness observation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5814</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5814</id><created>2013-12-20</created><authors><author><keyname>chittineni</keyname><forenames>suneetha</forenames></author><author><keyname>Bhogapathi</keyname><forenames>Raveendra Babu</forenames></author></authors><title>Optimal parameter selection for unsupervised neural network using
  genetic algorithm</title><categories>cs.NE</categories><comments>15 pages,4 figures,4 tables, International Journal of Computer
  Science, Engineering and Applications (IJCSEA) Vol.3, No.5, October 2013</comments><doi>10.5121/ijcsea.2013.3502</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  K-means Fast Learning Artificial Neural Network (K-FLANN) is an unsupervised
neural network requires two parameters: tolerance and vigilance. Best
Clustering results are feasible only by finest parameters specified to the
neural network. Selecting optimal values for these parameters is a major
problem. To solve this issue, Genetic Algorithm (GA) is used to determine
optimal parameters of K-FLANN for finding groups in multidimensional data.
K-FLANN is a simple topological network, in which output nodes grows
dynamically during the clustering process on receiving input patterns. Original
K-FLANN is enhanced to select winner unit out of the matched nodes so that
stable clusters are formed with in a less number of epochs. The experimental
results show that the GA is efficient in finding optimal values of parameters
from the large search space and is tested using artificial and synthetic data
sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5817</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5817</id><created>2013-12-20</created><updated>2014-06-10</updated><authors><author><keyname>Chen</keyname><forenames>Kuang-hua</forenames></author><author><keyname>Hsueh</keyname><forenames>Bi-Shin</forenames></author></authors><title>Exploring Regional Development of Digital Humanities Research: A Case
  Study for Taiwan</title><categories>cs.DL</categories><comments>25 pages, 10 tables, 5 figures</comments><msc-class>91F99</msc-class><journal-ref>Journal of Data Mining &amp; Digital Humanities, 2014 (June 24, 2014)
  jdmdh:17</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study analyzed references and source papers of the Proceedings of
2009-2012 International Conference of Digital Archives and Digital Humanities
(DADH), which was held annually in Taiwan. A total of 59 sources and 1,104
references were investigated, based on descriptive analysis and subject
analysis of library practices on cataloguing. Preliminary results showed
historical materials, events, bureaucracies, and people of Taiwan and China in
the Qing Dynasty were the major subjects in the tempo-spatial dimensions. The
subject-date figure depicted a long-low head and short-high tail curve, which
demonstrated both characteristics of research of humanities and application of
technology in digital humanities. The dates of publication of the references
spanned over 360 years, which shows a long time span in research materials. A
majority of the papers (61.41%) were single-authored, which is in line with the
common research practice in the humanities. Books published by general
publishers were the major type of references, and this was the same as that of
established humanities research. The next step of this study will focus on the
comparison of characteristics of both sources and references of international
journals with those reported in this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5830</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5830</id><created>2013-12-20</created><authors><author><keyname>Shim</keyname><forenames>Taehyoung</forenames></author><author><keyname>Kim</keyname><forenames>Dong Min</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>An Introduction to Socially Connected Machines: Characteristics and
  Applications</title><categories>cs.SI cs.CY</categories><journal-ref>&quot;Machine Social Networks: An Introduction to Socially Connected
  Machines,&quot; in Proc. The 3rd IFAC Symposium on Telematics Applications, Seoul,
  Korea, November 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the development of information and communication technologies, it is
difficult to handle the billions of connected machines. In this paper, to cope
with the problem, we introduce machine social networks, where they freely
follow each other and share common interests with their neighbors. We classify
characteristics and describe required functionalities of socially connected
machines. We also illustrate two examples; a twit-bot and maze scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5841</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5841</id><created>2013-12-20</created><authors><author><keyname>Nourdin</keyname><forenames>Ivan</forenames><affiliation>IECL</affiliation></author><author><keyname>Nualart</keyname><forenames>David</forenames></author></authors><title>Fisher Information and the Fourth Moment Theorem</title><categories>math.PR cs.IT math.IT</categories><comments>23 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using a representation of the score function by means of the divergence
operator we exhibit a sufficient condition, in terms of the negative moments of
the norm of the Malliavin derivative, under which convergence in Fisher
information to the standard Gaussian of sequences belonging to a given Wiener
chaos is actually equivalent to convergence of only the fourth moment. Thus,
our result may be considered as a further building block associated to the
recent but already rich literature dedicated to the Fourth Moment Theorem of
Nualart and Peccati. To illustrate the power of our approach we prove a local
limit theorem together with some rates of convergence for the normal
convergence of a standardized version of the quadratic variation of the
fractional Brownian motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5845</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5845</id><created>2013-12-20</created><updated>2015-02-16</updated><authors><author><keyname>Shinozaki</keyname><forenames>Takashi</forenames></author><author><keyname>Naruse</keyname><forenames>Yasushi</forenames></author></authors><title>Competitive Learning with Feedforward Supervisory Signal for Pre-trained
  Multilayered Networks</title><categories>cs.NE cs.CV cs.LG stat.ML</categories><comments>This paper has been withdrawn by the author since the review process
  for the conference to which it was applied ended</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel learning method for multilayered neural networks which
uses feedforward supervisory signal and associates classification of a new
input with that of pre-trained input. The proposed method effectively uses rich
input information in the earlier layer for robust leaning and revising internal
representation in a multilayer neural network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5847</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5847</id><created>2013-12-20</created><updated>2014-02-19</updated><authors><author><keyname>Plis</keyname><forenames>Sergey M.</forenames></author><author><keyname>Hjelm</keyname><forenames>Devon R.</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author><author><keyname>Calhoun</keyname><forenames>Vince D.</forenames></author></authors><title>Deep learning for neuroimaging: a validation study</title><categories>cs.NE cs.LG stat.ML</categories><comments>ICLR 2014 revisions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning methods have recently made notable advances in the tasks of
classification and representation learning. These tasks are important for brain
imaging and neuroscience discovery, making the methods attractive for porting
to a neuroimager's toolbox. Success of these methods is, in part, explained by
the flexibility of deep learning models. However, this flexibility makes the
process of porting to new areas a difficult parameter optimization problem. In
this work we demonstrate our results (and feasible parameter ranges) in
application of deep learning methods to structural and functional brain imaging
data. We also describe a novel constraint-based approach to visualizing high
dimensional data. We use it to analyze the effect of parameter choices on data
transformations. Our results show that deep learning methods are able to learn
physiologically important representations and detect latent relations in
neuroimaging data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5851</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5851</id><created>2013-12-20</created><updated>2014-03-06</updated><authors><author><keyname>Mathieu</keyname><forenames>Michael</forenames></author><author><keyname>Henaff</keyname><forenames>Mikael</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Fast Training of Convolutional Networks through FFTs</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional networks are one of the most widely employed architectures in
computer vision and machine learning. In order to leverage their ability to
learn complex functions, large amounts of data are required for training.
Training a large convolutional network to produce state-of-the-art results can
take weeks, even when using modern GPUs. Producing labels using a trained
network can also be costly when dealing with web-scale datasets. In this work,
we present a simple algorithm which accelerates training and inference by a
significant factor, and can yield improvements of over an order of magnitude
compared to existing state-of-the-art implementations. This is done by
computing convolutions as pointwise products in the Fourier domain while
reusing the same transformed feature map many times. The algorithm is
implemented on a GPU architecture and addresses a number of related challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5853</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5853</id><created>2013-12-20</created><updated>2014-02-18</updated><authors><author><keyname>Yadan</keyname><forenames>Omry</forenames></author><author><keyname>Adams</keyname><forenames>Keith</forenames></author><author><keyname>Taigman</keyname><forenames>Yaniv</forenames></author><author><keyname>Ranzato</keyname><forenames>Marc'Aurelio</forenames></author></authors><title>Multi-GPU Training of ConvNets</title><categories>cs.LG cs.NE</categories><comments>Machine Learning, Deep Learning, Convolutional Networks, Computer
  Vision, GPU, CUDA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we evaluate different approaches to parallelize computation of
convolutional neural networks across several GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5857</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5857</id><created>2013-12-20</created><updated>2014-11-25</updated><authors><author><keyname>Liang</keyname><forenames>Dawen</forenames></author><author><keyname>Hoffman</keyname><forenames>Matthew D.</forenames></author><author><keyname>Mysore</keyname><forenames>Gautham J.</forenames></author></authors><title>A Generative Product-of-Filters Model of Audio</title><categories>stat.ML cs.LG</categories><comments>ICLR 2014 conference-track submission. Added link to the source code</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the product-of-filters (PoF) model, a generative model that
decomposes audio spectra as sparse linear combinations of &quot;filters&quot; in the
log-spectral domain. PoF makes similar assumptions to those used in the classic
homomorphic filtering approach to signal processing, but replaces hand-designed
decompositions built of basic signal processing operations with a learned
decomposition based on statistical inference. This paper formulates the PoF
model and derives a mean-field method for posterior inference and a variational
EM algorithm to estimate the model's free parameters. We demonstrate PoF's
potential for audio processing on a bandwidth expansion task, and show that PoF
can serve as an effective unsupervised feature extractor for a speaker
identification task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5869</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5869</id><created>2013-12-20</created><updated>2014-02-18</updated><authors><author><keyname>Athanasakis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Shawe-Taylor</keyname><forenames>John</forenames></author><author><keyname>Fernandez-Reyes</keyname><forenames>Delmiro</forenames></author></authors><title>Principled Non-Linear Feature Selection</title><categories>cs.LG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1311.5636</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent non-linear feature selection approaches employing greedy optimisation
of Centred Kernel Target Alignment(KTA) exhibit strong results in terms of
generalisation accuracy and sparsity. However, they are computationally
prohibitive for large datasets. We propose randSel, a randomised feature
selection algorithm, with attractive scaling properties. Our theoretical
analysis of randSel provides strong probabilistic guarantees for correct
identification of relevant features. RandSel's characteristics make it an ideal
candidate for identifying informative learned representations. We've conducted
experimentation to establish the performance of this approach, and present
encouraging results, including a 3rd position result in the recent ICML black
box learning challenge as well as competitive results for signal peptide
prediction, an important problem in bioinformatics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5891</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5891</id><created>2013-12-20</created><authors><author><keyname>Asteris</keyname><forenames>Megasthenis</forenames></author><author><keyname>Papailiopoulos</keyname><forenames>Dimitris S.</forenames></author><author><keyname>Karystinos</keyname><forenames>George N.</forenames></author></authors><title>The Sparse Principal Component of a Constant-rank Matrix</title><categories>cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computation of the sparse principal component of a matrix is equivalent
to the identification of its principal submatrix with the largest maximum
eigenvalue. Finding this optimal submatrix is what renders the problem
${\mathcal{NP}}$-hard. In this work, we prove that, if the matrix is positive
semidefinite and its rank is constant, then its sparse principal component is
polynomially computable. Our proof utilizes the auxiliary unit vector technique
that has been recently developed to identify problems that are polynomially
solvable. Moreover, we use this technique to design an algorithm which, for any
sparsity value, computes the sparse principal component with complexity
${\mathcal O}\left(N^{D+1}\right)$, where $N$ and $D$ are the matrix size and
rank, respectively. Our algorithm is fully parallelizable and memory efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5892</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5892</id><created>2013-12-20</created><authors><author><keyname>Schmidt</keyname><forenames>Florian</forenames></author><author><keyname>Orlea</keyname><forenames>David</forenames></author><author><keyname>Wehrle</keyname><forenames>Klaus</forenames></author></authors><title>Support for Error Tolerance in the Real-Time Transport Protocol</title><categories>cs.NI cs.IT cs.OS math.IT</categories><comments>18 pages, 9 figures, published as technical report of the Department
  of Computer Science of RWTH Aachen University</comments><report-no>AIB-2013-19</report-no><acm-class>C.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Streaming applications often tolerate bit errors in their received data well.
This is contrasted by the enforcement of correctness of the packet headers and
payload by network protocols. We investigate a solution for the Real-time
Transport Protocol (RTP) that is tolerant to errors by accepting erroneous
data. It passes potentially corrupted stream data payloads to the codecs. If
errors occur in the header, our solution recovers from these by leveraging the
known state and expected header values for each stream. The solution is fully
receiver-based and incrementally deployable, and as such requires neither
support from the sender nor changes to the RTP specification. Evaluations show
that our header error recovery scheme can recover from almost all errors, with
virtually no erroneous recoveries, up to bit error rates of about 10%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5912</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5912</id><created>2013-12-20</created><updated>2013-12-31</updated><authors><author><keyname>Cal&#xec;</keyname><forenames>Andrea</forenames></author><author><keyname>Torlone</keyname><forenames>Riccardo</forenames></author></authors><title>Containment of Schema Mappings for Data Exchange (Preliminary Report)</title><categories>cs.DB</categories><comments>11 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In data exchange, data are materialised from a source schema to a target
schema, according to suitable source-to-target constraints. Constraints are
also expressed on the target schema to represent the domain of interest. A
schema mapping is the union of the source-to-target and of the target
constraints.
  In this paper, we address the problem of containment of schema mappings for
data exchange, which has been recently proposed in this framework as a step
towards the optimization of data exchange settings. We refer to a natural
notion of containment that relies on the behaviour of schema mappings with
respect to conjunctive query answering, in the presence of so-called LAV TGDs
as target constraints. Our contribution is a practical technique for testing
the containment based on the existence of a homomorphism between special
&quot;dummy&quot; instances, which can be easily built from schema mappings.
  We argue that containment of schema mappings is decidable for most practical
cases, and we set the basis for further investigations in the topic. This paper
extends our preliminary results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5914</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5914</id><created>2013-12-20</created><updated>2013-12-31</updated><authors><author><keyname>Cal&#xec;</keyname><forenames>Andrea</forenames></author><author><keyname>Console</keyname><forenames>Marco</forenames></author><author><keyname>Frosini</keyname><forenames>Riccardo</forenames></author></authors><title>Deep Separability of Ontological Constraints</title><categories>cs.DB</categories><comments>14 pages, no figures. arXiv admin note: text overlap with
  arXiv:1112.0343 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When data schemata are enriched with expressive constraints that aim at
representing the domain of interest, in order to answer queries one needs to
consider the logical theory consisting of both the data and the constraints.
Query answering in such a context is called ontological query answering.
Commonly adopted database constraints in this field are tuple-generating
dependencies (TGDs) and equality-generating dependencies (EGDs). It is well
known that their interaction leads to intractability or undecidability of query
answering even in the case of simple subclasses. Several conditions have been
found to guarantee separability, that is lack of interaction, between TGDs and
EGDs. Separability makes EGDs (mostly) irrelevant for query answering and
therefore often guarantees tractability, as long as the theory is satisfiable.
In this paper we review the two notions of separability found in the
literature, as well as several syntactic conditions that are sufficient to
prove them. We then shed light on the issue of satisfiability checking, showing
that under a sufficient condition called deep separability it can be done by
considering the TGDs only.
  We show that, fortunately, in the case of TGDs and EGDs, separability implies
deep separability. This result generalizes several analogous ones, proved ad
hoc for particular classes of constraints. Applications include the class of
sticky TGDs and EGDs, for which we provide a syntactic separability condition
which extends the analogous one for linear TGDs; preliminary experiments show
the feasibility of query answering in this case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5921</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5921</id><created>2013-12-20</created><updated>2014-02-18</updated><authors><author><keyname>Klami</keyname><forenames>Arto</forenames></author><author><keyname>Bouchard</keyname><forenames>Guillaume</forenames></author><author><keyname>Tripathi</keyname><forenames>Abhishek</forenames></author></authors><title>Group-sparse Embeddings in Collective Matrix Factorization</title><categories>stat.ML cs.LG</categories><comments>9+2 pages, submitted for International Conference on Learning
  Representations 2014. This version fixes minor typographic mistakes, has one
  new paragraph on computational efficiency, and describes the algorithm in
  more detail in the Supplementary material</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CMF is a technique for simultaneously learning low-rank representations based
on a collection of matrices with shared entities. A typical example is the
joint modeling of user-item, item-property, and user-feature matrices in a
recommender system. The key idea in CMF is that the embeddings are shared
across the matrices, which enables transferring information between them. The
existing solutions, however, break down when the individual matrices have
low-rank structure not shared with others. In this work we present a novel CMF
solution that allows each of the matrices to have a separate low-rank structure
that is independent of the other matrices, as well as structures that are
shared only by a subset of them. We compare MAP and variational Bayesian
solutions based on alternating optimization algorithms and show that the model
automatically infers the nature of each factor using group-wise sparsity. Our
approach supports in a principled way continuous, binary and count observations
and is efficient for sparse matrices involving missing data. We illustrate the
solution on a number of examples, focusing in particular on an interesting
use-case of augmented multi-view learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5936</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5936</id><created>2013-12-20</created><authors><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author></authors><title>Measuring voting power in convex policy spaces</title><categories>cs.GT</categories><comments>31 pages, 9 tables</comments><msc-class>91A12, 91B12, 91A06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical power index analysis considers the individual's ability to
influence the aggregated group decision by changing its own vote, where all
decisions and votes are assumed to be binary. In many practical applications we
have more options than either &quot;yes&quot; or &quot;no&quot;. Here we generalize three important
power indices to continuous convex policy spaces. This allows the analysis of a
collection of economic problems like e.g. tax rates or spending that otherwise
would not be covered in binary models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5937</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5937</id><created>2013-12-20</created><updated>2014-02-17</updated><authors><author><keyname>Atserias</keyname><forenames>Albert</forenames></author><author><keyname>Dawar</keyname><forenames>Anuj</forenames></author><author><keyname>Verbitsky</keyname><forenames>Oleg</forenames></author></authors><title>On the dynamic width of the 3-colorability problem</title><categories>cs.CC</categories><comments>18 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ is 3-colorable if and only if it maps homomorphically to the
complete 3-vertex graph $K_3$. The last condition can be checked by a
$k$-consistency algorithm where the parameter $k$ has to be chosen large
enough, dependent on $G$. Let $W(G)$ denote the minimum $k$ sufficient for this
purpose. For a non-3-colorable graph $G$, $W(G)$ is equal to the minimum $k$
such that $G$ can be distinguished from $K_3$ in the $k$-variable
existential-positive first-order logic. We define the dynamic width of the
3-colorability problem as the function $W(n)=\max_G W(G)$, where the maximum is
taken over all non-3-colorable $G$ with $n$ vertices.
  The assumption $\mathrm{NP}\ne\mathrm{P}$ implies that $W(n)$ is unbounded.
Indeed, a lower bound $W(n)=\Omega(\log\log n/\log\log\log n)$ follows
unconditionally from the work of Nesetril and Zhu on bounded treewidth duality.
The Exponential Time Hypothesis implies a much stronger bound
$W(n)=\Omega(n/\log n)$ and indeed we unconditionally prove that
$W(n)=\Omega(n)$. In fact, an even stronger statement is true: A first-order
sentence distinguishing any 3-colorable graph on $n$ vertices from any
non-3-colorable graph on $n$ vertices must have $\Omega(n)$ variables.
  On the other hand, we observe that $W(G)\le 3\,\alpha(G)+1$ and $W(G)\le
n-\alpha(G)+1$ for every non-3-colorable graph $G$ with $n$ vertices, where
$\alpha(G)$ denotes the independence number of $G$. This implies that
$W(n)\le\frac34\,n+1$, improving on the trivial upper bound $W(n)\le n$.
  We also show that $W(G)&gt;\frac1{16}\, g(G)$ for every non-3-colorable graph
$G$, where $g(G)$ denotes the girth of $G$.
  Finally, we consider the function $W(n)$ over planar graphs and prove that
$W(n)=\Theta(\sqrt n)$ in the case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5938</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5938</id><created>2013-12-20</created><updated>2014-05-08</updated><authors><author><keyname>Tanbourgi</keyname><forenames>Ralph</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author><author><keyname>Jondral</keyname><forenames>Friedrich K.</forenames></author></authors><title>Dual-Branch MRC Receivers under Spatial Interference Correlation and
  Nakagami Fading</title><categories>cs.IT math.IT math.PR</categories><comments>to appear in IEEE Transactions on Communications</comments><doi>10.1109/TCOMM.2014.2321553</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite being ubiquitous in practice, the performance of maximal-ratio
combining (MRC) in the presence of interference is not well understood. Because
the interference received at each antenna originates from the same set of
interferers, but partially de-correlates over the fading channel, it possesses
a complex correlation structure. This work develops a realistic analytic model
that accurately accounts for the interference correlation using stochastic
geometry. Modeling interference by a Poisson shot noise process with
independent Nakagami fading, we derive the link success probability for
dual-branch interference-aware MRC. Using this result, we show that the common
assumption that all receive antennas experience equal interference power
underestimates the true performance, although this gap rapidly decays with
increasing the Nakagami parameter $m_{\text{I}}$ of the interfering links. In
contrast, ignoring interference correlation leads to a highly optimistic
performance estimate for MRC, especially for large $m_{\text{I}}$. In the low
outage probability regime, our success probability expression can be
considerably simplified. Observations following from the analysis include: (i)
for small path loss exponents, MRC and minimum mean square error combining
exhibit similar performance, and (ii) the gains of MRC over selection combining
are smaller in the interference-limited case than in the well-studied
noise-limited case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5940</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5940</id><created>2013-12-20</created><updated>2014-03-10</updated><authors><author><keyname>Oyallon</keyname><forenames>Edouard</forenames></author><author><keyname>Mallat</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Sifre</keyname><forenames>Laurent</forenames></author></authors><title>Generic Deep Networks with Wavelet Scattering</title><categories>cs.CV</categories><comments>Workshop, 3 pages, prepared for ICLR 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a two-layer wavelet scattering network, for object
classification. This scattering transform computes a spatial wavelet transform
on the first layer and a new joint wavelet transform along spatial, angular and
scale variables in the second layer. Numerical experiments demonstrate that
this two layer convolution network, which involves no learning and no max
pooling, performs efficiently on complex image data sets such as CalTech, with
structural objects variability and clutter. It opens the possibility to
simplify deep neural network learning by initializing the first layers with
wavelet filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5941</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5941</id><created>2013-12-20</created><authors><author><keyname>Van Truong</keyname><forenames>Hong</forenames></author><author><keyname>Beck</keyname><forenames>Elise</forenames></author><author><keyname>Dugdale</keyname><forenames>Julie</forenames></author><author><keyname>Adam</keyname><forenames>Carole</forenames></author></authors><title>Developing a model of evacuation after an earthquake in Lebanon</title><categories>cs.MA</categories><comments>8 pages, 11 figures, ISCRAM Vietnam Conference, November 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes the development of an agent-based model (AMEL,
Agent-based Model for Earthquake evacuation in Lebanon) that aims at simulating
the movement of pedestrians shortly after an earthquake. The GAMA platform was
chosen to implement the model. AMEL is applied to a real case study, a district
of the city of Beirut, Lebanon, which potentially could be stricken by a M7
earthquake. The objective of the model is to reproduce real life mobility
behaviours that have been gathered through a survey in Beirut and to test
different future scenarios, which may help the local authorities to target
information campaigns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5946</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5946</id><created>2013-12-20</created><authors><author><keyname>Bl&#xf6;mer</keyname><forenames>Johannes</forenames></author><author><keyname>Bujna</keyname><forenames>Kathrin</forenames></author></authors><title>Simple Methods for Initializing the EM Algorithm for Gaussian Mixture
  Models</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider simple and fast approaches to initialize the
Expectation-Maximization algorithm (EM) for multivariate Gaussian mixture
models. We present new initialization methods based on the well-known
$K$-means++ algorithm and the Gonzalez algorithm. These methods close the gap
between simple uniform initialization techniques and complex methods, that have
been specifically designed for Gaussian mixture models and depend on the right
choice of hyperparameters. In our evaluation we compare our methods with a
commonly used random initialization method, an approach based on agglomerative
hierarchical clustering, and a known, plain adaption of the Gonzalez algorithm.
Our results indicate that algorithms based on $K$-means++ outperform the other
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5951</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5951</id><created>2013-12-20</created><authors><author><keyname>Ardeshir-Larijani</keyname><forenames>Ebrahim</forenames></author><author><keyname>Gay</keyname><forenames>Simon J.</forenames></author><author><keyname>Nagarajan</keyname><forenames>Rajagopal</forenames></author></authors><title>Automated Verification of Quantum Protocols by Equivalence Checking</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a technique and a tool for formal verification of
various quantum information processing protocols. The tool uses stabilizer
formalism and is capable of representing concurrent quantum protocol, thus is
more expressive than quantum circuits. We also report on experimental results
of using our Quantum Equivalence Checker (QEC) to analyse a range of quantum
information processing protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5952</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5952</id><created>2013-12-20</created><authors><author><keyname>Schlegel</keyname><forenames>Christian</forenames></author><author><keyname>Schultz</keyname><forenames>Ulrik Pagh</forenames></author><author><keyname>Stinckwich</keyname><forenames>Serge</forenames></author></authors><title>Proceedings of the Fourth International Workshop on Domain-Specific
  Languages and Models for Robotic Systems (DSLRob 2013)</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fourth International Workshop on Domain-Specific Languages and Models for
Robotic Systems (DSLRob'13) was held in conjunction with the 2013 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS 2013),
November 2013 in Tokyo, Japan.
  The main topics of the workshop were Domain-Specific Languages (DSLs) and
Model-driven Software Development (MDSD) for robotics. A domain-specific
language is a programming language dedicated to a particular problem domain
that offers specific notations and abstractions that increase programmer
productivity within that domain. Model-driven software development offers a
high-level way for domain users to specify the functionality of their system at
the right level of abstraction. DSLs and models have historically been used for
programming complex systems. However recently they have garnered interest as a
separate field of study. Robotic systems blend hardware and software in a
holistic way that intrinsically raises many crosscutting concerns (concurrency,
uncertainty, time constraints, ...), for which reason, traditional
general-purpose languages often lead to a poor fit between the language
features and the implementation requirements. DSLs and models offer a powerful,
systematic way to overcome this problem, enabling the programmer to quickly and
precisely implement novel software solutions to complex problems within the
robotics domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5972</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5972</id><created>2013-12-20</created><updated>2016-01-11</updated><authors><author><keyname>Au</keyname><forenames>Yu Hin</forenames></author><author><keyname>Tun&#xe7;el</keyname><forenames>Levent</forenames></author></authors><title>A Comprehensive Analysis of Polyhedral Lift-and-Project Methods</title><categories>math.CO cs.CC cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider lift-and-project methods for combinatorial optimization problems
and focus mostly on those lift-and-project methods which generate polyhedral
relaxations of the convex hull of integer solutions. We introduce many new
variants of Sherali--Adams and Bienstock--Zuckerberg operators. These new
operators fill the spectrum of polyhedral lift-and-project operators in a way
which makes all of them more transparent, easier to relate to each other, and
easier to analyze. We provide new techniques to analyze the worst-case
performances as well as relative strengths of these operators in a unified way.
In particular, using the new techniques and a result of Mathieu and Sinclair
from 2009, we prove that the polyhedral Bienstock--Zuckerberg operator requires
at least $\sqrt{2n}- \frac{3}{2}$ iterations to compute the matching polytope
of the $(2n+1)$-clique. We further prove that the operator requires
approximately $\frac{n}{2}$ iterations to reach the stable set polytope of the
$n$-clique, if we start with the fractional stable set polytope. Lastly, we
show that some of the worst-case instances for the positive semidefinite
Lov\'asz--Schrijver lift-and-project operator are also bad instances for the
strongest variants of the Sherali--Adams operator with positive semidefinite
strengthenings, and discuss some consequences for integrality gaps of convex
relaxations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5978</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5978</id><created>2013-12-20</created><authors><author><keyname>Kumar</keyname><forenames>Mrinal</forenames></author><author><keyname>Saraf</keyname><forenames>Shubhangi</forenames></author></authors><title>Superpolynomial lower bounds for general homogeneous depth 4 arithmetic
  circuits</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove superpolynomial lower bounds for the class of
homogeneous depth 4 arithmetic circuits. We give an explicit polynomial in VNP
of degree $n$ in $n^2$ variables such that any homogeneous depth 4 arithmetic
circuit computing it must have size $n^{\Omega(\log \log n)}$.
  Our results extend the works of Nisan-Wigderson [NW95] (which showed
superpolynomial lower bounds for homogeneous depth 3 circuits),
Gupta-Kamath-Kayal-Saptharishi and Kayal-Saha-Saptharishi [GKKS13, KSS13]
(which showed superpolynomial lower bounds for homogeneous depth 4 circuits
with bounded bottom fan-in), Kumar-Saraf [KS13a] (which showed superpolynomial
lower bounds for homogeneous depth 4 circuits with bounded top fan-in) and
Raz-Yehudayoff and Fournier-Limaye-Malod-Srinivasan [RY08, FLMS13] (which
showed superpolynomial lower bounds for multilinear depth 4 circuits). Several
of these results in fact showed exponential lower bounds.
  The main ingredient in our proof is a new complexity measure of {\it bounded
support} shifted partial derivatives. This measure allows us to prove
exponential lower bounds for homogeneous depth 4 circuits where all the
monomials computed at the bottom layer have {\it bounded support} (but possibly
unbounded degree/fan-in), strengthening the results of Gupta et al and Kayal et
al [GKKS13, KSS13]. This new lower bound combined with a careful &quot;random
restriction&quot; procedure (that transforms general depth 4 homogeneous circuits to
depth 4 circuits with bounded support) gives us our final result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5985</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5985</id><created>2013-12-20</created><updated>2014-02-18</updated><authors><author><keyname>Polajnar</keyname><forenames>Tamara</forenames></author><author><keyname>Fagarasan</keyname><forenames>Luana</forenames></author><author><keyname>Clark</keyname><forenames>Stephen</forenames></author></authors><title>Learning Type-Driven Tensor-Based Meaning Representations</title><categories>cs.CL cs.LG</categories><comments>Submitted as part of the open review process for ICLR'14. The paper
  contains 10 pages, 3 figures, 4 tables</comments><acm-class>H.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the learning of 3rd-order tensors representing the
semantics of transitive verbs. The meaning representations are part of a
type-driven tensor-based semantic framework, from the newly emerging field of
compositional distributional semantics. Standard techniques from the neural
networks literature are used to learn the tensors, which are tested on a
selectional preference-style task with a simple 2-dimensional sentence space.
Promising results are obtained against a competitive corpus-based baseline. We
argue that extending this work beyond transitive verbs, and to
higher-dimensional sentence spaces, is an interesting and challenging problem
for the machine learning community to consider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.5990</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.5990</id><created>2013-12-20</created><updated>2014-04-01</updated><authors><author><keyname>Sutter</keyname><forenames>David</forenames></author><author><keyname>Renes</keyname><forenames>Joseph M.</forenames></author></authors><title>Universal Polar Codes for More Capable and Less Noisy Channels and
  Sources</title><categories>cs.IT math.IT</categories><comments>10 pages, 3 figures</comments><doi>10.1109/ISIT.2014.6875075</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove two results on the universality of polar codes for source coding and
channel communication. First, we show that for any polar code built for a
source $P_{X,Z}$ there exists a slightly modified polar code - having the same
rate, the same encoding and decoding complexity and the same error rate - that
is universal for every source $P_{X,Y}$ when using successive cancellation
decoding, at least when the channel $P_{Y|X}$ is more capable than $P_{Z|X}$
and $P_X$ is such that it maximizes $I(X;Y) - I(X;Z)$ for the given channels
$P_{Y|X}$ and $P_{Z|X}$. This result extends to channel coding for discrete
memoryless channels. Second, we prove that polar codes using successive
cancellation decoding are universal for less noisy discrete memoryless
channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6002</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6002</id><created>2013-12-20</created><updated>2014-02-14</updated><authors><author><keyname>Berglund</keyname><forenames>Mathias</forenames></author><author><keyname>Raiko</keyname><forenames>Tapani</forenames></author></authors><title>Stochastic Gradient Estimate Variance in Contrastive Divergence and
  Persistent Contrastive Divergence</title><categories>cs.NE cs.LG stat.ML</categories><comments>ICLR2014 Workshop Track submission. Rephrased parts of text. Results
  unchanged</comments><msc-class>62M45</msc-class><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) are
popular methods for training the weights of Restricted Boltzmann Machines.
However, both methods use an approximate method for sampling from the model
distribution. As a side effect, these approximations yield significantly
different biases and variances for stochastic gradient estimates of individual
data points. It is well known that CD yields a biased gradient estimate. In
this paper we however show empirically that CD has a lower stochastic gradient
estimate variance than exact sampling, while the mean of subsequent PCD
estimates has a higher variance than exact sampling. The results give one
explanation to the finding that CD can be used with smaller minibatches or
higher learning rates than PCD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6024</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6024</id><created>2013-12-20</created><authors><author><keyname>Artan</keyname><forenames>Yusuf</forenames></author><author><keyname>Paul</keyname><forenames>Peter</forenames></author></authors><title>Occupancy Detection in Vehicles Using Fisher Vector Image Representation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the high volume of traffic on modern roadways, transportation agencies
have proposed High Occupancy Vehicle (HOV) lanes and High Occupancy Tolling
(HOT) lanes to promote car pooling. However, enforcement of the rules of these
lanes is currently performed by roadside enforcement officers using visual
observation. Manual roadside enforcement is known to be inefficient, costly,
potentially dangerous, and ultimately ineffective. Violation rates up to
50%-80% have been reported, while manual enforcement rates of less than 10% are
typical. Therefore, there is a need for automated vehicle occupancy detection
to support HOV/HOT lane enforcement. A key component of determining vehicle
occupancy is to determine whether or not the vehicle's front passenger seat is
occupied. In this paper, we examine two methods of determining vehicle front
seat occupancy using a near infrared (NIR) camera system pointed at the
vehicle's front windshield. The first method examines a state-of-the-art
deformable part model (DPM) based face detection system that is robust to
facial pose. The second method examines state-of- the-art local aggregation
based image classification using bag-of-visual-words (BOW) and Fisher vectors
(FV). A dataset of 3000 images was collected on a public roadway and is used to
perform the comparison. From these experiments it is clear that the image
classification approach is superior for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6026</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6026</id><created>2013-12-20</created><updated>2014-04-24</updated><authors><author><keyname>Pascanu</keyname><forenames>Razvan</forenames></author><author><keyname>Gulcehre</keyname><forenames>Caglar</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>How to Construct Deep Recurrent Neural Networks</title><categories>cs.NE cs.LG stat.ML</categories><comments>Accepted at ICLR 2014 (Conference Track). 10-page text + 3-page
  references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore different ways to extend a recurrent neural network
(RNN) to a \textit{deep} RNN. We start by arguing that the concept of depth in
an RNN is not as clear as it is in feedforward neural networks. By carefully
analyzing and understanding the architecture of an RNN, however, we find three
points of an RNN which may be made deeper; (1) input-to-hidden function, (2)
hidden-to-hidden transition and (3) hidden-to-output function. Based on this
observation, we propose two novel architectures of a deep RNN which are
orthogonal to an earlier attempt of stacking multiple recurrent layers to build
a deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an
alternative interpretation of these deep RNNs using a novel framework based on
neural operators. The proposed deep RNNs are empirically evaluated on the tasks
of polyphonic music prediction and language modeling. The experimental result
supports our claim that the proposed deep RNNs benefit from the depth and
outperform the conventional, shallow RNNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6029</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6029</id><created>2013-12-20</created><updated>2014-03-17</updated><authors><author><keyname>&#xd3;dor</keyname><forenames>G&#xe9;za</forenames></author><author><keyname>Kelling</keyname><forenames>Jeffrey</forenames></author><author><keyname>Gemming</keyname><forenames>Sibylle</forenames></author></authors><title>Ageing of the 2+1 dimensional Kardar-Parisi-Zhang model</title><categories>cond-mat.stat-mech cond-mat.mtrl-sci cs.DC nlin.CG physics.comp-ph</categories><comments>6 pages, 5 figs, 1 table, accepted version in PRE</comments><journal-ref>Phys. Rev. E 89, 032146 (2014)</journal-ref><doi>10.1103/PhysRevE.89.032146</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extended dynamical simulations have been performed on a 2+1 dimensional
driven dimer lattice gas model to estimate ageing properties. The
auto-correlation and the auto-response functions are determined and the
corresponding scaling exponents are tabulated. Since this model can be mapped
onto the 2+1 dimensional Kardar-Parisi-Zhang surface growth model, our results
contribute to the understanding of the universality class of that basic system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6034</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6034</id><created>2013-12-20</created><updated>2014-04-19</updated><authors><author><keyname>Simonyan</keyname><forenames>Karen</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author><author><keyname>Zisserman</keyname><forenames>Andrew</forenames></author></authors><title>Deep Inside Convolutional Networks: Visualising Image Classification
  Models and Saliency Maps</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the visualisation of image classification models, learnt
using deep Convolutional Networks (ConvNets). We consider two visualisation
techniques, based on computing the gradient of the class score with respect to
the input image. The first one generates an image, which maximises the class
score [Erhan et al., 2009], thus visualising the notion of the class, captured
by a ConvNet. The second technique computes a class saliency map, specific to a
given image and class. We show that such maps can be employed for weakly
supervised object segmentation using classification ConvNets. Finally, we
establish the connection between the gradient-based ConvNet visualisation
methods and deconvolutional networks [Zeiler et al., 2013].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6036</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6036</id><created>2013-12-20</created><authors><author><keyname>Frommberger</keyname><forenames>Lutz</forenames></author><author><keyname>Schmid</keyname><forenames>Falko</forenames></author></authors><title>Crowdsourced bi-directional disaster reporting and alerting on
  smartphones in Lao PDR</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural disasters are a large threat for people especially in developing
countries such as Laos. ICT-based disaster management systems aim at supporting
disaster warning and response efforts. However, the ability to directly
communicate in both directions between local and administrative level is often
not supported, and a tight integration into administrative workflows is
missing. In this paper, we present the smartphone-based disaster and reporting
system Mobile4D. It allows for bi-directional communication while being fully
involved in administrative processes. We present the system setup and discuss
integration into administrative structures in Lao PDR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6039</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6039</id><created>2013-12-20</created><authors><author><keyname>Tsur</keyname><forenames>Dekel</forenames></author></authors><title>Succinct representation of labeled trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a representation for labeled ordered trees that supports labeled
queries such as finding the i-th ancestor of a node with a given label. Our
representation is succinct, namely the redundancy is small-o of the optimal
space for storing the tree. This improves the representation of He et al. which
is succinct unless the entropy of the labels is small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6042</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6042</id><created>2013-12-20</created><updated>2014-06-17</updated><authors><author><keyname>Contardo</keyname><forenames>Gabriella</forenames></author><author><keyname>Denoyer</keyname><forenames>Ludovic</forenames></author><author><keyname>Artieres</keyname><forenames>Thierry</forenames></author><author><keyname>Gallinari</keyname><forenames>Patrick</forenames></author></authors><title>Learning States Representations in POMDP</title><categories>cs.LG</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to deal with sequential processes where only partial observations
are available by learning a latent representation space on which policies may
be accurately learned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6052</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6052</id><created>2013-12-20</created><authors><author><keyname>Frank</keyname><forenames>Mario</forenames></author><author><keyname>Hwu</keyname><forenames>Tiffany</forenames></author><author><keyname>Jain</keyname><forenames>Sakshi</forenames></author><author><keyname>Knight</keyname><forenames>Robert</forenames></author><author><keyname>Martinovic</keyname><forenames>Ivan</forenames></author><author><keyname>Mittal</keyname><forenames>Prateek</forenames></author><author><keyname>Perito</keyname><forenames>Daniele</forenames></author><author><keyname>Song</keyname><forenames>Dawn</forenames></author></authors><title>Subliminal Probing for Private Information via EEG-Based BCI Devices</title><categories>cs.CR</categories><comments>under review for a journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Martinovic et al. proposed a Brain-Computer-Interface (BCI) -based attack in
which an adversary is able to infer private information about a user, such as
their bank or area-of-living, by analyzing the user's brain activities.
However, a key limitation of the above attack is that it is intrusive,
requiring user cooperation, and is thus easily detectable and can be reported
to other users. In this paper, we identify and analyze a more serious threat
for users of BCI devices. We propose a it subliminal attack in which the victim
is attacked at the levels below his cognitive perception. Our attack involves
exposing the victim to visual stimuli for a duration of 13.3 milliseconds -- a
duration usually not sufficient for conscious perception. The attacker analyzes
subliminal brain activity in response to these short visual stimuli to infer
private information about the user. If carried out carefully, for example by
hiding the visual stimuli within screen content that the user expects to see,
the attack may remain undetected. As a consequence, the attacker can scale it
to many victims and expose them to the attack for a long time. We
experimentally demonstrate the feasibility of our subliminal attack via a
proof-of-concept study carried out with 27 subjects. We conducted experiments
on users wearing Electroencephalography-based BCI devices, and used portrait
pictures of people as visual stimuli which were embedded within the background
of an innocuous video for a time duration not exceeding 13.3 milliseconds. Our
experimental results show that it is feasible for an attacker to learn relevant
private information about the user, such as whether the user knows the identity
of the person for which the attacker is probing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6055</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6055</id><created>2013-12-20</created><updated>2014-02-25</updated><authors><author><keyname>Schaul</keyname><forenames>Tom</forenames></author><author><keyname>Antonoglou</keyname><forenames>Ioannis</forenames></author><author><keyname>Silver</keyname><forenames>David</forenames></author></authors><title>Unit Tests for Stochastic Optimization</title><categories>cs.LG</categories><comments>Final submission to ICLR 2014 (revised according to reviews,
  additional results added)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimization by stochastic gradient descent is an important component of many
large-scale machine learning algorithms. A wide variety of such optimization
algorithms have been devised; however, it is unclear whether these algorithms
are robust and widely applicable across many different optimization landscapes.
In this paper we develop a collection of unit tests for stochastic
optimization. Each unit test rapidly evaluates an optimization algorithm on a
small-scale, isolated, and well-understood difficulty, rather than in
real-world scenarios where many such issues are entangled. Passing these unit
tests is not sufficient, but absolutely necessary for any algorithms with
claims to generality or robustness. We give initial quantitative and
qualitative results on numerous established algorithms. The testing framework
is open-source, extensible, and easy to apply to new algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6057</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6057</id><created>2013-12-20</created><updated>2014-06-12</updated><authors><author><keyname>Wildman</keyname><forenames>Jeffrey</forenames></author><author><keyname>Nardelli</keyname><forenames>Pedro H J</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author><author><keyname>Weber</keyname><forenames>Steven</forenames></author></authors><title>On the Joint Impact of Beamwidth and Orientation Error on Throughput in
  Directional Wireless Poisson Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>14 pages, 9 figures. Submitted on 2013-12-20 to IEEE Transactions on
  Wireless Communications. Accepted on 2014-06-03 to IEEE Transactions on
  Wireless Communications</comments><journal-ref>IEEE Trans. Wireless Commun. (Vol. 13, Issue. 12, Pp. 7072--7085,
  Dec. 2014)</journal-ref><doi>10.1109/TWC.2014.2331055</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model for capturing the effects of beam misdirection on
coverage and throughput in a directional wireless network using stochastic
geometry. In networks employing ideal sector antennas without sidelobes, we
find that concavity of the orientation error distribution is sufficient to
prove monotonicity and quasi-concavity (both with respect to antenna beamwidth)
of spatial throughput and transmission capacity, respectively. Additionally, we
identify network conditions that produce opposite extremal choices in beamwidth
(absolutely directed versus omni-directional) that maximize the two related
throughput metrics. We conclude our paper with a numerical exploration of the
relationship between mean orientation error, throughput-maximizing beamwidths,
and maximum throughput, across radiation patterns of varied complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6061</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6061</id><created>2013-12-20</created><authors><author><keyname>Gargiulo</keyname><forenames>Floriana</forenames></author><author><keyname>Carletti</keyname><forenames>Timoteo</forenames></author></authors><title>Driving forces in researchers mobility</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Starting from the dataset of the publication corpus of the APS during the
period 1955-2009, we reconstruct the individual researchers trajectories,
namely the list of the consecutive affiliations for each scholar. Crossing this
information with different geographic datasets we embed these trajectories in a
spatial framework. Using methods from network theory and complex systems
analysis we characterise these patterns in terms of topological network
properties and we analyse the dependence of an academic path across different
dimensions: the distance between two subsequent positions, the relative
importance of the institutions (in terms of number of publications) and some
socio-cultural traits. We show that distance is not always a good predictor for
the next affiliation while other factors like &quot;the previous steps&quot; of the
career of the researchers (in particular the first position) or the linguistic
and historical similarity between two countries can have an important impact.
Finally we show that the dataset exhibit a memory effect, hence the fate of a
career strongly depends from the first two affiliations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6062</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6062</id><created>2013-12-20</created><updated>2014-04-09</updated><authors><author><keyname>Buchaca</keyname><forenames>David</forenames></author><author><keyname>Romero</keyname><forenames>Enrique</forenames></author><author><keyname>Mazzanti</keyname><forenames>Ferran</forenames></author><author><keyname>Delgado</keyname><forenames>Jordi</forenames></author></authors><title>Stopping Criteria in Contrastive Divergence: Alternatives to the
  Reconstruction Error</title><categories>cs.LG</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Restricted Boltzmann Machines (RBMs) are general unsupervised learning
devices to ascertain generative models of data distributions. RBMs are often
trained using the Contrastive Divergence learning algorithm (CD), an
approximation to the gradient of the data log-likelihood. A simple
reconstruction error is often used to decide whether the approximation provided
by the CD algorithm is good enough, though several authors (Schulz et al.,
2010; Fischer &amp; Igel, 2010) have raised doubts concerning the feasibility of
this procedure. However, not many alternatives to the reconstruction error have
been used in the literature. In this manuscript we investigate simple
alternatives to the reconstruction error in order to detect as soon as possible
the decrease in the log-likelihood during learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6064</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6064</id><created>2013-12-20</created><authors><author><keyname>Degwekar</keyname><forenames>Akshay</forenames></author><author><keyname>Guenda</keyname><forenames>Kenza</forenames></author><author><keyname>Gulliver</keyname><forenames>T. Aaron</forenames></author></authors><title>Extending Construction X for Quantum Error-Correcting Codes</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we extend the work of Lisonek and Singh on construction X for
quantum error-correcting codes to finite fields of order $p^2^ where p is
prime. The results obtained are applied to the dual of Hermitian repeated root
cyclic codes to generate new quantum error-correcting codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6077</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6077</id><created>2013-12-20</created><updated>2014-12-17</updated><authors><author><keyname>Shan</keyname><forenames>Honghao</forenames></author><author><keyname>Cottrell</keyname><forenames>Garrison</forenames></author></authors><title>Efficient Visual Coding: From Retina To V2</title><categories>cs.CV q-bio.NC</categories><comments>For the ICLR 2014 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The human visual system has a hierarchical structure consisting of layers of
processing, such as the retina, V1, V2, etc. Understanding the functional roles
of these visual processing layers would help to integrate the
psychophysiological and neurophysiological models into a consistent theory of
human vision, and would also provide insights to computer vision research. One
classical theory of the early visual pathway hypothesizes that it serves to
capture the statistical structure of the visual inputs by efficiently coding
the visual information in its outputs. Until recently, most computational
models following this theory have focused upon explaining the receptive field
properties of one or two visual layers. Recent work in deep networks has
eliminated this concern, however, there is till the retinal layer to consider.
Here we improve on a previously-described hierarchical model Recursive ICA
(RICA) [1] which starts with PCA, followed by a layer of sparse coding or ICA,
followed by a component-wise nonlinearity derived from considerations of the
variable distributions expected by ICA. This process is then repeated. In this
work, we improve on this model by using a new version of sparse PCA (sPCA),
which results in biologically-plausible receptive fields for both the sPCA and
ICA/sparse coding. When applied to natural image patches, our model learns
visual features exhibiting the receptive field properties of retinal ganglion
cells/lateral geniculate nucleus (LGN) cells, V1 simple cells, V1 complex
cells, and V2 cells. Our work provides predictions for experimental
neuroscience studies. For example, our result suggests that a previous
neurophysiological study improperly discarded some of their recorded neurons;
we predict that their discarded neurons capture the shape contour of objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6079</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6079</id><created>2013-12-20</created><authors><author><keyname>Sasidharan</keyname><forenames>Birenjith</forenames></author><author><keyname>Senthoor</keyname><forenames>Kaushik</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijay</forenames></author></authors><title>An Improved Outer Bound on the Storage-Repair-Bandwidth Tradeoff of
  Exact-Repair Regenerating Codes</title><categories>cs.IT math.IT</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we establish an improved outer bound on the
storage-repair-bandwidth tradeoff of regenerating codes under exact repair. The
result shows that in particular, it is not possible to construct exact-repair
regenerating codes that asymptotically achieve the tradeoff that holds for
functional repair. While this had been shown earlier by Tian for the special
case of $[n,k,d]=[4,3,3]$ the present result holds for general $[n,k,d]$. The
new outer bound is obtained by building on the framework established earlier by
Shah et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6082</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6082</id><created>2013-12-20</created><updated>2014-04-14</updated><authors><author><keyname>Goodfellow</keyname><forenames>Ian J.</forenames></author><author><keyname>Bulatov</keyname><forenames>Yaroslav</forenames></author><author><keyname>Ibarz</keyname><forenames>Julian</forenames></author><author><keyname>Arnoud</keyname><forenames>Sacha</forenames></author><author><keyname>Shet</keyname><forenames>Vinay</forenames></author></authors><title>Multi-digit Number Recognition from Street View Imagery using Deep
  Convolutional Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognizing arbitrary multi-character text in unconstrained natural
photographs is a hard problem. In this paper, we address an equally hard
sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from
Street View imagery. Traditional approaches to solve this problem typically
separate out the localization, segmentation, and recognition steps. In this
paper we propose a unified approach that integrates these three steps via the
use of a deep convolutional neural network that operates directly on the image
pixels. We employ the DistBelief implementation of deep neural networks in
order to train large, distributed neural networks on high quality images. We
find that the performance of this approach increases with the depth of the
convolutional network, with the best performance occurring in the deepest
architecture we trained, with eleven hidden layers. We evaluate this approach
on the publicly available SVHN dataset and achieve over $96\%$ accuracy in
recognizing complete street numbers. We show that on a per-digit recognition
task, we improve upon the state-of-the-art, achieving $97.84\%$ accuracy. We
also evaluate this approach on an even more challenging dataset generated from
Street View imagery containing several tens of millions of street number
annotations and achieve over $90\%$ accuracy. To further explore the
applicability of the proposed system to broader text recognition tasks, we
apply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of the
most secure reverse turing tests that uses distorted text to distinguish humans
from bots. We report a $99.8\%$ accuracy on the hardest category of reCAPTCHA.
Our evaluations on both tasks indicate that at specific operating thresholds,
the performance of the proposed system is comparable to, and in some cases
exceeds, that of human operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6086</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6086</id><created>2013-12-20</created><authors><author><keyname>K&#xe9;gl</keyname><forenames>Bal&#xe1;zs</forenames></author></authors><title>The return of AdaBoost.MH: multi-class Hamming trees</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the framework of AdaBoost.MH, we propose to train vector-valued
decision trees to optimize the multi-class edge without reducing the
multi-class problem to $K$ binary one-against-all classifications. The key
element of the method is a vector-valued decision stump, factorized into an
input-independent vector of length $K$ and label-independent scalar classifier.
At inner tree nodes, the label-dependent vector is discarded and the binary
classifier can be used for partitioning the input space into two regions. The
algorithm retains the conceptual elegance, power, and computational efficiency
of binary AdaBoost. In experiments it is on par with support vector machines
and with the best existing multi-class boosting algorithm AOSOLogitBoost, and
it is significantly better than other known implementations of AdaBoost.MH.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6090</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6090</id><created>2013-12-20</created><authors><author><keyname>Maugey</keyname><forenames>Thomas</forenames></author><author><keyname>Ortega</keyname><forenames>Antonio</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Graph-based representation for multiview image coding</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new representation for multiview image sets. Our
approach relies on graphs to describe geometry information in a compact and
controllable way. The links of the graph connect pixels in different images and
describe the proximity between pixels in the 3D space. These connections are
dependent on the geometry of the scene and provide the right amount of
information that is necessary for coding and reconstructing multiple views.
This multiview image representation is very compact and adapts the transmitted
geometry information as a function of the complexity of the prediction
performed at the decoder side. To achieve this, our GBR adapts the accuracy of
the geometry representation, in contrast with depth coding, which directly
compresses with losses the original geometry signal. We present the principles
of this graph-based representation (GBR) and we build a complete prototype
coding scheme for multiview images. Experimental results demonstrate the
potential of this new representation as compared to a depth-based approach. GBR
can achieve a gain of 2 dB in reconstructed quality over depth-based schemes
operating at similar rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6094</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6094</id><created>2013-12-20</created><updated>2014-04-18</updated><authors><author><keyname>Borisevich</keyname><forenames>Alex</forenames></author><author><keyname>Schullerus</keyname><forenames>Gernot</forenames></author></authors><title>Energy Efficient Control of an Induction Machine under Load Torque Step
  Change</title><categories>cs.SY</categories><comments>11 pages, start preparing for IEEE submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal control of magnetizing current for minimizing induction motor power
losses during load torque step change was developed. Obtained strategy has
feedback form and is exactly optimal of ideal speed controller performance and
absence of saturation in motor. The impact of limited bandwidth of real speed
controller is analyzed. For case of main induction saturation the sub-optimal
optimal control is suggested. Relative accuracy of sub-optimality is studied.
Hardware implementation of optimal strategy and experimentation conducted with
induction motors under vector control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6095</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6095</id><created>2013-12-20</created><updated>2014-02-16</updated><authors><author><keyname>Pepik</keyname><forenames>Bojan</forenames></author><author><keyname>Stark</keyname><forenames>Michael</forenames></author><author><keyname>Gehler</keyname><forenames>Peter</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>Multi-View Priors for Learning Detectors from Sparse Viewpoint Data</title><categories>cs.CV</categories><comments>13 pages, 7 figures, 4 tables, International Conference on Learning
  Representations 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the majority of today's object class models provide only 2D bounding
boxes, far richer output hypotheses are desirable including viewpoint,
fine-grained category, and 3D geometry estimate. However, models trained to
provide richer output require larger amounts of training data, preferably well
covering the relevant aspects such as viewpoint and fine-grained categories. In
this paper, we address this issue from the perspective of transfer learning,
and design an object class model that explicitly leverages correlations between
visual features. Specifically, our model represents prior distributions over
permissible multi-view detectors in a parametric way -- the priors are learned
once from training data of a source object class, and can later be used to
facilitate the learning of a detector for a target class. As we show in our
experiments, this transfer is not only beneficial for detectors based on
basic-level category representations, but also enables the robust learning of
detectors that represent classes at finer levels of granularity, where training
data is typically even scarcer and more unbalanced. As a result, we report
largely improved performance in simultaneous 2D object localization and
viewpoint estimation on a recent dataset of challenging street scenes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6096</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6096</id><created>2013-12-20</created><authors><author><keyname>Alviano</keyname><forenames>Mario</forenames></author><author><keyname>Faber</keyname><forenames>Wolfgang</forenames></author></authors><title>Properties of Answer Set Programming with Convex Generalized Atoms</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, Answer Set Programming (ASP), logic programming under the
stable model or answer set semantics, has seen several extensions by
generalizing the notion of an atom in these programs: be it aggregate atoms,
HEX atoms, generalized quantifiers, or abstract constraints, the idea is to
have more complicated satisfaction patterns in the lattice of Herbrand
interpretations than traditional, simple atoms. In this paper we refer to any
of these constructs as generalized atoms. Several semantics with differing
characteristics have been proposed for these extensions, rendering the big
picture somewhat blurry. In this paper, we analyze the class of programs that
have convex generalized atoms (originally proposed by Liu and Truszczynski in
[10]) in rule bodies and show that for this class many of the proposed
semantics coincide. This is an interesting result, since recently it has been
shown that this class is the precise complexity boundary for the FLP semantics.
We investigate whether similar results also hold for other semantics, and
discuss the implications of our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6098</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6098</id><created>2013-12-20</created><updated>2014-02-14</updated><authors><author><keyname>Pascanu</keyname><forenames>Razvan</forenames></author><author><keyname>Montufar</keyname><forenames>Guido</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>On the number of response regions of deep feed forward networks with
  piece-wise linear activations</title><categories>cs.LG cs.NE</categories><comments>17 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the complexity of deep feedforward networks with linear
pre-synaptic couplings and rectified linear activations. This is a contribution
to the growing body of work contrasting the representational power of deep and
shallow network architectures. In particular, we offer a framework for
comparing deep and shallow models that belong to the family of piecewise linear
functions based on computational geometry. We look at a deep rectifier
multi-layer perceptron (MLP) with linear outputs units and compare it with a
single layer version of the model. In the asymptotic regime, when the number of
inputs stays constant, if the shallow model has $kn$ hidden units and $n_0$
inputs, then the number of linear regions is $O(k^{n_0}n^{n_0})$. For a $k$
layer model with $n$ hidden units on each layer it is $\Omega(\left\lfloor
{n}/{n_0}\right\rfloor^{k-1}n^{n_0})$. The number
$\left\lfloor{n}/{n_0}\right\rfloor^{k-1}$ grows faster than $k^{n_0}$ when $n$
tends to infinity or when $k$ tends to infinity and $n \geq 2n_0$.
Additionally, even when $k$ is small, if we restrict $n$ to be $2n_0$, we can
show that a deep model has considerably more linear regions that a shallow one.
We consider this as a first step towards understanding the complexity of these
models and specifically towards providing suitable mathematical tools for
future analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6101</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6101</id><created>2013-12-20</created><authors><author><keyname>Yu</keyname><forenames>Geunyeong</forenames></author><author><keyname>Moon</keyname><forenames>Jaekyun</forenames></author></authors><title>Concatenated Raptor Codes in NAND Flash Memory</title><categories>cs.IT math.IT</categories><comments>13 pages, 9 figures, accepted to IEEE JSAC Storage 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two concatenated coding schemes based on fixed-rate Raptor codes are proposed
for error control in NAND flash memory. One is geared for off-line recovery of
uncorrectable pages and the other is designed for page error correction during
the normal read mode. Both proposed coding strategies assume hard-decision
decoding of the inner code with inner decoding failure generating erasure
symbols for the outer Raptor code. Raptor codes allow low-complexity decoding
of very long codewords while providing capacity- approaching performance for
erasure channels. For the off-line page recovery scheme, one whole NAND block
forms a Raptor codeword with each inner codeword typically made up of several
Raptor symbols. An efficient look-up-table strategy is devised for Raptor
encoding and decoding which avoids using large buffers in the controller
despite the substantial size of the Raptor code employed. The potential
performance benefit of the proposed scheme is evaluated in terms of the
probability of block recovery conditioned on the presence of uncorrectable
pages. In the suggested page-error-correction strategy, on the other hand, a
hard-decision-iterating product code is used as the inner code. The specific
product code employed in this work is based on row-column concatenation with
multiple intersecting bits allowing the use of longer component codes. In this
setting the collection of bits captured within each intersection of the
row-column codes acts as the Raptor symbol(s), and the intersections of failed
row codes and column codes are declared as erasures. The error rate analysis
indicates that the proposed concatenation provides a considerable performance
boost relative to the existing error correcting system based on long
Bose-Chaudhuri-Hocquenghem (BCH) codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6105</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6105</id><created>2013-12-20</created><authors><author><keyname>Balduccini</keyname><forenames>Marcello</forenames></author><author><keyname>Lierler</keyname><forenames>Yulia</forenames></author></authors><title>Hybrid Automated Reasoning Tools: from Black-box to Clear-box
  Integration</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, researchers in answer set programming and constraint programming
spent significant efforts in the development of hybrid languages and solving
algorithms combining the strengths of these traditionally separate fields.
These efforts resulted in a new research area: constraint answer set
programming (CASP). CASP languages and systems proved to be largely successful
at providing efficient solutions to problems involving hybrid reasoning tasks,
such as scheduling problems with elements of planning. Yet, the development of
CASP systems is difficult, requiring non-trivial expertise in multiple areas.
This suggests a need for a study identifying general development principles of
hybrid systems. Once these principles and their implications are well
understood, the development of hybrid languages and systems may become a
well-established and well-understood routine process. As a step in this
direction, in this paper we conduct a case study aimed at evaluating various
integration schemas of CASP methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6108</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6108</id><created>2013-12-20</created><updated>2014-02-17</updated><authors><author><keyname>Wang</keyname><forenames>Nan</forenames></author><author><keyname>Jancke</keyname><forenames>Dirk</forenames></author><author><keyname>Wiskott</keyname><forenames>Laurenz</forenames></author></authors><title>Modeling correlations in spontaneous activity of visual cortex with
  centered Gaussian-binary deep Boltzmann machines</title><categories>cs.NE cs.LG q-bio.NC</categories><comments>9 pages, 4 figures, for openreview ICLR2014, 2nd revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spontaneous cortical activity -- the ongoing cortical activities in absence
of intentional sensory input -- is considered to play a vital role in many
aspects of both normal brain functions and mental dysfunctions. We present a
centered Gaussian-binary Deep Boltzmann Machine (GDBM) for modeling the
activity in early cortical visual areas and relate the random sampling in GDBMs
to the spontaneous cortical activity. After training the proposed model on
natural image patches, we show that the samples collected from the model's
probability distribution encompass similar activity patterns as found in the
spontaneous activity. Specifically, filters having the same orientation
preference tend to be active together during random sampling. Our work
demonstrates the centered GDBM is a meaningful model approach for basic
receptive field properties and the emergence of spontaneous activity patterns
in early cortical visual areas. Besides, we show empirically that centered
GDBMs do not suffer from the difficulties during training as GDBMs do and can
be properly trained without the layer-wise pretraining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6110</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6110</id><created>2013-12-20</created><updated>2015-02-21</updated><authors><author><keyname>Tang</keyname><forenames>Yichuan</forenames></author><author><keyname>Srivastava</keyname><forenames>Nitish</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author></authors><title>Learning Generative Models with Visual Attention</title><categories>cs.CV</categories><comments>In the proceedings of Neural Information Processing Systems, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attention has long been proposed by psychologists as important for
effectively dealing with the enormous sensory stimulus available in the
neocortex. Inspired by the visual attention models in computational
neuroscience and the need of object-centric data for generative models, we
describe for generative learning framework using attentional mechanisms.
Attentional mechanisms can propagate signals from region of interest in a scene
to an aligned canonical representation, where generative modeling takes place.
By ignoring background clutter, generative models can concentrate their
resources on the object of interest. Our model is a proper graphical model
where the 2D Similarity transformation is a part of the top-down process. A
ConvNet is employed to provide good initializations during posterior inference
which is based on Hamiltonian Monte Carlo. Upon learning images of faces, our
model can robustly attend to face regions of novel test subjects. More
importantly, our model can learn generative models of new faces from a novel
dataset of large images where the face locations are not known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6113</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6113</id><created>2013-12-20</created><authors><author><keyname>Banbara</keyname><forenames>Mutsunori</forenames></author><author><keyname>Gebser</keyname><forenames>Martin</forenames></author><author><keyname>Inoue</keyname><forenames>Katsumi</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author><author><keyname>Soh</keyname><forenames>Takehide</forenames></author><author><keyname>Tamura</keyname><forenames>Naoyuki</forenames></author><author><keyname>Weise</keyname><forenames>Matthias</forenames></author></authors><title>Aspartame: Solving Constraint Satisfaction Problems with Answer Set
  Programming</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Encoding finite linear CSPs as Boolean formulas and solving them by using
modern SAT solvers has proven to be highly effective, as exemplified by the
award-winning sugar system. We here develop an alternative approach based on
ASP. This allows us to use first-order encodings providing us with a high
degree of flexibility for easy experimentation with different implementations.
The resulting system aspartame re-uses parts of sugar for parsing and
normalizing CSPs. The obtained set of facts is then combined with an ASP
encoding that can be grounded and solved by off-the-shelf ASP systems. We
establish the competitiveness of our approach by empirically contrasting
aspartame and sugar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6114</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6114</id><created>2013-12-20</created><updated>2014-05-01</updated><authors><author><keyname>Kingma</keyname><forenames>Diederik P</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Auto-Encoding Variational Bayes</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can we perform efficient inference and learning in directed probabilistic
models, in the presence of continuous latent variables with intractable
posterior distributions, and large datasets? We introduce a stochastic
variational inference and learning algorithm that scales to large datasets and,
under some mild differentiability conditions, even works in the intractable
case. Our contributions is two-fold. First, we show that a reparameterization
of the variational lower bound yields a lower bound estimator that can be
straightforwardly optimized using standard stochastic gradient methods. Second,
we show that for i.i.d. datasets with continuous latent variables per
datapoint, posterior inference can be made especially efficient by fitting an
approximate inference model (also called a recognition model) to the
intractable posterior using the proposed lower bound estimator. Theoretical
advantages are reflected in experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6115</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6115</id><created>2013-12-20</created><updated>2014-03-22</updated><authors><author><keyname>Reichert</keyname><forenames>David P.</forenames></author><author><keyname>Serre</keyname><forenames>Thomas</forenames></author></authors><title>Neuronal Synchrony in Complex-Valued Deep Networks</title><categories>stat.ML cs.LG cs.NE q-bio.NC</categories><comments>ICLR 2014, accepted to conference track. This version: added
  proceedings note, minor additions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning has recently led to great successes in tasks such as image
recognition (e.g Krizhevsky et al., 2012). However, deep networks are still
outmatched by the power and versatility of the brain, perhaps in part due to
the richer neuronal computations available to cortical circuits. The challenge
is to identify which neuronal mechanisms are relevant, and to find suitable
abstractions to model them. Here, we show how aspects of spike timing, long
hypothesized to play a crucial role in cortical information processing, could
be incorporated into deep networks to build richer, versatile representations.
  We introduce a neural network formulation based on complex-valued neuronal
units that is not only biologically meaningful but also amenable to a variety
of deep learning frameworks. Here, units are attributed both a firing rate and
a phase, the latter indicating properties of spike timing. We show how this
formulation qualitatively captures several aspects thought to be related to
neuronal synchrony, including gating of information processing and dynamic
binding of distributed object representations. Focusing on the latter, we
demonstrate the potential of the approach in several simple experiments. Thus,
neuronal synchrony could be a flexible mechanism that fulfills multiple
functional roles in deep networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6116</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6116</id><created>2013-12-20</created><updated>2014-02-19</updated><authors><author><keyname>Springenberg</keyname><forenames>Jost Tobias</forenames></author><author><keyname>Riedmiller</keyname><forenames>Martin</forenames></author></authors><title>Improving Deep Neural Networks with Probabilistic Maxout Units</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a probabilistic variant of the recently introduced maxout unit.
The success of deep neural networks utilizing maxout can partly be attributed
to favorable performance under dropout, when compared to rectified linear
units. It however also depends on the fact that each maxout unit performs a
pooling operation over a group of linear transformations and is thus partially
invariant to changes in its input. Starting from this observation we ask the
question: Can the desirable properties of maxout units be preserved while
improving their invariance properties ? We argue that our probabilistic maxout
(probout) units successfully achieve this balance. We quantitatively verify
this claim and report classification performance matching or exceeding the
current state of the art on three challenging image classification benchmarks
(CIFAR-10, CIFAR-100 and SVHN).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6117</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6117</id><created>2013-12-19</created><updated>2014-11-13</updated><authors><author><keyname>Kowsari</keyname><forenames>Kamran</forenames></author></authors><title>Comparison three methods of clustering: k-means, spectral clustering and
  hierarchical clustering</title><categories>cs.LG</categories><comments>This paper has been withdrawn by the author due to improve add more
  results</comments><msc-class>68T10</msc-class><acm-class>H.3.3; I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comparison of three kind of the clustering and find cost function and loss
function and calculate them. Error rate of the clustering methods and how to
calculate the error percentage always be one on the important factor for
evaluating the clustering methods, so this paper introduce one way to calculate
the error rate of clustering methods. Clustering algorithms can be divided into
several categories including partitioning clustering algorithms, hierarchical
algorithms and density based algorithms. Generally speaking we should compare
clustering algorithms by Scalability, Ability to work with different attribute,
Clusters formed by conventional, Having minimal knowledge of the computer to
recognize the input parameters, Classes for dealing with noise and extra
deposition that same error rate for clustering a new data, Thus, there is no
effect on the input data, different dimensions of high levels, K-means is one
of the simplest approach to clustering that clustering is an unsupervised
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6119</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6119</id><created>2013-12-20</created><authors><author><keyname>Borsche</keyname><forenames>Theodor</forenames></author><author><keyname>Ulbig</keyname><forenames>Andreas</forenames></author><author><keyname>Andersson</keyname><forenames>G&#xf6;ran</forenames></author></authors><title>A New Frequency Control Reserve Framework based on Energy-Constrained
  Units</title><categories>cs.SY</categories><comments>working paper, submitted to PSCC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frequency control reserves are an essential ancillary service in any electric
power system, guaranteeing that generation and demand of active power are
balanced at all times. Traditionally, conventional power plants are used for
frequency reserves. There are economical and technical benefits of instead
using energy constrained units such as storage systems and demand response, but
so far they have not been widely adopted as their energy constraints prevent
them from following traditional regulation signals, which sometimes are biased
over long time-spans. This paper proposes a frequency control framework that
splits the control signals according to the frequency spectrum. This guarantees
that all control signals are zero-mean over well-defined time-periods, which is
a crucial requirement for the usage of energy-constraint units such as
batteries. A case-study presents a possible implementation, and shows how
different technologies with widely varying characteristics can all participate
in frequency control reserve provision, while guaranteeing that their
respective energy constraints are always fulfilled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6120</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6120</id><created>2013-12-20</created><updated>2014-02-19</updated><authors><author><keyname>Saxe</keyname><forenames>Andrew M.</forenames></author><author><keyname>McClelland</keyname><forenames>James L.</forenames></author><author><keyname>Ganguli</keyname><forenames>Surya</forenames></author></authors><title>Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks</title><categories>cs.NE cond-mat.dis-nn cs.CV cs.LG q-bio.NC stat.ML</categories><comments>Submission to ICLR2014. Revised based on reviewer feedback</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the widespread practical success of deep learning methods, our
theoretical understanding of the dynamics of learning in deep neural networks
remains quite sparse. We attempt to bridge the gap between the theory and
practice of deep learning by systematically analyzing learning dynamics for the
restricted case of deep linear neural networks. Despite the linearity of their
input-output map, such networks have nonlinear gradient descent dynamics on
weights that change with the addition of each new hidden layer. We show that
deep linear networks exhibit nonlinear learning phenomena similar to those seen
in simulations of nonlinear networks, including long plateaus followed by rapid
transitions to lower error solutions, and faster convergence from greedy
unsupervised pretraining initial conditions than from random initial
conditions. We provide an analytical description of these phenomena by finding
new exact solutions to the nonlinear dynamics of deep learning. Our theoretical
analysis also reveals the surprising finding that as the depth of a network
approaches infinity, learning speed can nevertheless remain finite: for a
special class of initial conditions on the weights, very deep networks incur
only a finite, depth independent, delay in learning speed relative to shallow
networks. We show that, under certain conditions on the training data,
unsupervised pretraining can find this special class of initial conditions,
while scaled random Gaussian initializations cannot. We further exhibit a new
class of random orthogonal initial conditions on weights that, like
unsupervised pre-training, enjoys depth independent learning times. We further
show that these initial conditions also lead to faithful propagation of
gradients even in deep nonlinear networks, as long as they operate in a special
regime known as the edge of chaos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6122</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6122</id><created>2013-12-20</created><authors><author><keyname>Bagrow</keyname><forenames>James P.</forenames></author><author><keyname>Desu</keyname><forenames>Suma</forenames></author><author><keyname>Frank</keyname><forenames>Morgan R.</forenames></author><author><keyname>Manukyan</keyname><forenames>Narine</forenames></author><author><keyname>Mitchell</keyname><forenames>Lewis</forenames></author><author><keyname>Reagan</keyname><forenames>Andrew</forenames></author><author><keyname>Bloedorn</keyname><forenames>Eric E.</forenames></author><author><keyname>Booker</keyname><forenames>Lashon B.</forenames></author><author><keyname>Branting</keyname><forenames>Luther K.</forenames></author><author><keyname>Smith</keyname><forenames>Michael J.</forenames></author><author><keyname>Tivnan</keyname><forenames>Brian F.</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter S.</forenames></author><author><keyname>Bongard</keyname><forenames>Joshua C.</forenames></author></authors><title>Shadow networks: Discovering hidden nodes with models of information
  flow</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI physics.data-an</categories><comments>12 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex, dynamic networks underlie many systems, and understanding these
networks is the concern of a great span of important scientific and engineering
problems. Quantitative description is crucial for this understanding yet, due
to a range of measurement problems, many real network datasets are incomplete.
Here we explore how accidentally missing or deliberately hidden nodes may be
detected in networks by the effect of their absence on predictions of the speed
with which information flows through the network. We use Symbolic Regression
(SR) to learn models relating information flow to network topology. These
models show localized, systematic, and non-random discrepancies when applied to
test networks with intentionally masked nodes, demonstrating the ability to
detect the presence of missing nodes and where in the network those nodes are
likely to reside.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6130</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6130</id><created>2013-12-20</created><authors><author><keyname>Bartholomew</keyname><forenames>Michael</forenames></author><author><keyname>Lee</keyname><forenames>Joohyung</forenames></author></authors><title>A Functional View of Strong Negation in Answer Set Programming</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The distinction between strong negation and default negation has been useful
in answer set programming. We present an alternative account of strong
negation, which lets us view strong negation in terms of the functional stable
model semantics by Bartholomew and Lee. More specifically, we show that, under
complete interpretations, minimizing both positive and negative literals in the
traditional answer set semantics is essentially the same as ensuring the
uniqueness of Boolean function values under the functional stable model
semantics. The same account lets us view Lifschitz's two-valued logic programs
as a special case of the functional stable model semantics. In addition, we
show how non-Boolean intensional functions can be eliminated in favor of
Boolean intensional functions, and furthermore can be represented using strong
negation, which provides a way to compute the functional stable model semantics
using existing ASP solvers. We also note that similar results hold with the
functional stable model semantics by Cabalar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6134</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6134</id><created>2013-12-20</created><authors><author><keyname>Cabalar</keyname><forenames>Pedro</forenames></author><author><keyname>Fandinno</keyname><forenames>Jorge</forenames></author></authors><title>An Algebra of Causal Chains</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose a multi-valued extension of logic programs under the
stable models semantics where each true atom in a model is associated with a
set of justifications, in a similar spirit than a set of proof trees. The main
contribution of this paper is that we capture justifications into an algebra of
truth values with three internal operations: an addition '+' representing
alternative justifications for a formula, a commutative product '*'
representing joint interaction of causes and a non-commutative product '.'
acting as a concatenation or proof constructor. Using this multi-valued
semantics, we obtain a one-to-one correspondence between the syntactic proof
tree of a standard (non-causal) logic program and the interpretation of each
true atom in a model. Furthermore, thanks to this algebraic characterization we
can detect semantic properties like redundancy and relevance of the obtained
justifications. We also identify a lattice-based characterization of this
algebra, defining a direct consequences operator, proving its continuity and
that its least fix point can be computed after a finite number of iterations.
Finally, we define the concept of causal stable model by introducing an
analogous transformation to Gelfond and Lifschitz's program reduct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6138</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6138</id><created>2013-12-20</created><authors><author><keyname>Chaudhri</keyname><forenames>Vinay K.</forenames></author><author><keyname>Heymans</keyname><forenames>Stijn</forenames></author><author><keyname>Wessel</keyname><forenames>Michael</forenames></author><author><keyname>Son</keyname><forenames>Tran Cao</forenames></author></authors><title>Query Answering in Object Oriented Knowledge Bases in Logic Programming:
  Description and Challenge for ASP</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on developing efficient and scalable ASP solvers can substantially
benefit by the availability of data sets to experiment with. KB_Bio_101
contains knowledge from a biology textbook, has been developed as part of
Project Halo, and has recently become available for research use. KB_Bio_101 is
one of the largest KBs available in ASP and the reasoning with it is
undecidable in general. We give a description of this KB and ASP programs for a
suite of queries that have been of practical interest. We explain why these
queries pose significant practical challenges for the current ASP solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6140</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6140</id><created>2013-12-20</created><authors><author><keyname>Ellmauthaler</keyname><forenames>Stefan</forenames></author><author><keyname>Strass</keyname><forenames>Hannes</forenames></author></authors><title>The DIAMOND System for Argumentation: Preliminary Report</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstract dialectical frameworks (ADFs) are a powerful generalisation of
Dung's abstract argumentation frameworks. In this paper we present an answer
set programming based software system, called DIAMOND (DIAlectical MOdels
eNcoDing). It translates ADFs into answer set programs whose stable models
correspond to models of the ADF with respect to several semantics (i.e.
admissible, complete, stable, grounded).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6143</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6143</id><created>2013-12-20</created><authors><author><keyname>Gebser</keyname><forenames>Martin</forenames></author><author><keyname>Obermeier</keyname><forenames>Philipp</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author></authors><title>A System for Interactive Query Answering with Answer Set Programming</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reactive answer set programming has paved the way for incorporating online
information into operative solving processes. Although this technology was
originally devised for dealing with data streams in dynamic environments, like
assisted living and cognitive robotics, it can likewise be used to incorporate
facts, rules, or queries provided by a user. As a result, we present the design
and implementation of a system for interactive query answering with reactive
answer set programming. Our system quontroller is based on the reactive solver
oclingo and implemented as a dedicated front-end. We describe its functionality
and implementation, and we illustrate its features by some selected use cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6146</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6146</id><created>2013-12-20</created><authors><author><keyname>G&#xfc;ni&#xe7;en</keyname><forenames>Canan</forenames></author><author><keyname>Erdem</keyname><forenames>Esra</forenames></author><author><keyname>Yenig&#xfc;n</keyname><forenames>H&#xfc;sn&#xfc;</forenames></author></authors><title>Generating Shortest Synchronizing Sequences using Answer Set Programming</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a finite state automaton, a synchronizing sequence is an input sequence
that takes all the states to the same state. Checking the existence of a
synchronizing sequence and finding a synchronizing sequence, if one exists, can
be performed in polynomial time. However, the problem of finding a shortest
synchronizing sequence is known to be NP-hard. In this work, the usefulness of
Answer Set Programming to solve this optimization problem is investigated, in
comparison with brute-force algorithms and SAT-based approaches.
  Keywords: finite automata, shortest synchronizing sequence, ASP
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6149</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6149</id><created>2013-12-20</created><authors><author><keyname>Harrison</keyname><forenames>Amelia</forenames></author><author><keyname>Lifschitz</keyname><forenames>Vladimir</forenames></author><author><keyname>Yang</keyname><forenames>Fangkai</forenames></author></authors><title>On the Semantics of Gringo</title><categories>cs.AI cs.LO</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Input languages of answer set solvers are based on the mathematically simple
concept of a stable model. But many useful constructs available in these
languages, including local variables, conditional literals, and aggregates,
cannot be easily explained in terms of stable models in the sense of the
original definition of this concept and its straightforward generalizations.
Manuals written by designers of answer set solvers usually explain such
constructs using examples and informal comments that appeal to the user's
intuition, without references to any precise semantics. We propose to approach
the problem of defining the semantics of gringo programs by translating them
into the language of infinitary propositional formulas. This semantics allows
us to study equivalent transformations of gringo programs using natural
deduction in infinitary propositional logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6150</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6150</id><created>2013-12-16</created><authors><author><keyname>Roy</keyname><forenames>Sudipta</forenames></author><author><keyname>Nag</keyname><forenames>Sanjay</forenames></author><author><keyname>Maitra</keyname><forenames>Indra Kanta</forenames></author><author><keyname>Bandyopadhyay</keyname><forenames>Samir Kumar</forenames></author></authors><title>A Review on Automated Brain Tumor Detection and Segmentation from MRI of
  Brain</title><categories>cs.CV</categories><comments>30 figures. arXiv admin note: text overlap with arXiv:1205.6572 by
  other authors</comments><journal-ref>International Journal of Advanced Research in Computer Science and
  Software Engineering, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tumor segmentation from magnetic resonance imaging (MRI) data is an important
but time consuming manual task performed by medical experts. Automating this
process is a challenging task because of the high diversity in the appearance
of tumor tissues among different patients and in many cases similarity with the
normal tissues. MRI is an advanced medical imaging technique providing rich
information about the human soft-tissue anatomy. There are different brain
tumor detection and segmentation methods to detect and segment a brain tumor
from MRI images. These detection and segmentation approaches are reviewed with
an importance placed on enlightening the advantages and drawbacks of these
methods for brain tumor detection and segmentation. The use of MRI image
detection and segmentation in different procedures are also described. Here a
brief review of different segmentation for detection of brain tumor from MRI of
brain has been discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6151</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6151</id><created>2013-12-20</created><authors><author><keyname>Lierler</keyname><forenames>Yuliya</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>Abstract Modular Systems and Solvers</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrating diverse formalisms into modular knowledge representation systems
offers increased expressivity, modeling convenience and computational benefits.
We introduce concepts of abstract modules and abstract modular systems to study
general principles behind the design and analysis of model-finding programs, or
solvers, for integrated heterogeneous multi-logic systems. We show how abstract
modules and abstract modular systems give rise to transition systems, which are
a natural and convenient representation of solvers pioneered by the SAT
community. We illustrate our approach by showing how it applies to answer set
programming and propositional logic, and to multi-logic systems based on these
two formalisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6155</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6155</id><created>2013-12-20</created><updated>2014-06-24</updated><authors><author><keyname>Hlad&#xed;k</keyname><forenames>Milan</forenames></author><author><keyname>Ratschan</keyname><forenames>Stefan</forenames></author></authors><title>Efficient Solution of a Class of Quantified Constraints with Quantifier
  Prefix Exists-Forall</title><categories>cs.LO cs.DS cs.NA</categories><msc-class>65G20</msc-class><acm-class>F.4.1; G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In various applications the search for certificates for certain properties
(e.g., stability of dynamical systems, program termination) can be formulated
as a quantified constraint solving problem with quantifier prefix
exists-forall. In this paper, we present an algorithm for solving a certain
class of such problems based on interval techniques in combination with
conservative linear programming approximation. In comparison with previous
work, the method is more general - allowing general Boolean structure in the
input constraint, and more efficient - using splitting heuristics that learn
from the success of previous linear programming approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6156</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6156</id><created>2013-12-20</created><authors><author><keyname>Vennekens</keyname><forenames>Joost</forenames></author></authors><title>Negation in the Head of CP-logic Rules</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CP-logic is a probabilistic extension of the logic FO(ID). Unlike ASP, both
of these logics adhere to a Tarskian informal semantics, in which
interpretations represent objective states-of-affairs. In other words, these
logics lack the epistemic component of ASP, in which interpretations represent
the beliefs or knowledge of a rational agent. Consequently, neither CP-logic
nor FO(ID) have the need for two kinds of negations: there is only one
negation, and its meaning is that of objective falsehood. Nevertheless, the
formal semantics of this objective negation is mathematically more similar to
ASP's negation-as-failure than to its classical negation. The reason is that
both CP-logic and FO(ID) have a constructive semantics in which all atoms start
out as false, and may only become true as the result of a rule application.
This paper investigates the possibility of adding the well-known ASP feature of
allowing negation in the head of rules to CP-logic. Because CP-logic only has
one kind of negation, it is of necessity this ''negation-as-failure like''
negation that will be allowed in the head. We investigate the intuitive meaning
of such a construct and the benefits that arise from it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6157</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6157</id><created>2013-12-20</created><updated>2014-01-02</updated><authors><author><keyname>Pezeshki</keyname><forenames>Mohammad</forenames></author><author><keyname>Gholami</keyname><forenames>Sajjad</forenames></author><author><keyname>Nickabadi</keyname><forenames>Ahmad</forenames></author></authors><title>Distinction between features extracted using deep belief networks</title><categories>cs.LG cs.NE</categories><comments>4 pages, 4 figures, ICLR 2014 workshop track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data representation is an important pre-processing step in many machine
learning algorithms. There are a number of methods used for this task such as
Deep Belief Networks (DBNs) and Discrete Fourier Transforms (DFTs). Since some
of the features extracted using automated feature extraction methods may not
always be related to a specific machine learning task, in this paper we propose
two methods in order to make a distinction between extracted features based on
their relevancy to the task. We applied these two methods to a Deep Belief
Network trained for a face recognition task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6158</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6158</id><created>2013-12-20</created><updated>2014-01-02</updated><authors><author><keyname>Keyvanrad</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Pezeshki</keyname><forenames>Mohammad</forenames></author><author><keyname>Homayounpour</keyname><forenames>Mohammad Ali</forenames></author></authors><title>Deep Belief Networks for Image Denoising</title><categories>cs.LG cs.CV cs.NE</categories><comments>ICLR 2014 Conference track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Belief Networks which are hierarchical generative models are effective
tools for feature representation and extraction. Furthermore, DBNs can be used
in numerous aspects of Machine Learning such as image denoising. In this paper,
we propose a novel method for image denoising which relies on the DBNs' ability
in feature representation. This work is based upon learning of the noise
behavior. Generally, features which are extracted using DBNs are presented as
the values of the last layer nodes. We train a DBN a way that the network
totally distinguishes between nodes presenting noise and nodes presenting image
content in the last later of DBN, i.e. the nodes in the last layer of trained
DBN are divided into two distinct groups of nodes. After detecting the nodes
which are presenting the noise, we are able to make the noise nodes inactive
and reconstruct a noiseless image. In section 4 we explore the results of
applying this method on the MNIST dataset of handwritten digits which is
corrupted with additive white Gaussian noise (AWGN). A reduction of 65.9% in
average mean square error (MSE) was achieved when the proposed method was used
for the reconstruction of the noisy images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6159</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6159</id><created>2013-12-20</created><authors><author><keyname>Bogovic</keyname><forenames>John A.</forenames></author><author><keyname>Huang</keyname><forenames>Gary B.</forenames></author><author><keyname>Jain</keyname><forenames>Viren</forenames></author></authors><title>Learned versus Hand-Designed Feature Representations for 3d
  Agglomeration</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For image recognition and labeling tasks, recent results suggest that machine
learning methods that rely on manually specified feature representations may be
outperformed by methods that automatically derive feature representations based
on the data. Yet for problems that involve analysis of 3d objects, such as mesh
segmentation, shape retrieval, or neuron fragment agglomeration, there remains
a strong reliance on hand-designed feature descriptors. In this paper, we
evaluate a large set of hand-designed 3d feature descriptors alongside features
learned from the raw data using both end-to-end and unsupervised learning
techniques, in the context of agglomeration of 3d neuron fragments. By
combining unsupervised learning techniques with a novel dynamic pooling scheme,
we show how pure learning-based methods are for the first time competitive with
hand-designed 3d shape descriptors. We investigate data augmentation strategies
for dramatically increasing the size of the training set, and show how
combining both learned and hand-designed features leads to the highest
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6168</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6168</id><created>2013-12-20</created><updated>2014-02-18</updated><authors><author><keyname>Nepal</keyname><forenames>Anjan</forenames></author><author><keyname>Yates</keyname><forenames>Alexander</forenames></author></authors><title>Factorial Hidden Markov Models for Learning Representations of Natural
  Language</title><categories>cs.LG cs.CL</categories><comments>12 pages, 2 tables, ICLR-2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most representation learning algorithms for language and image processing are
local, in that they identify features for a data point based on surrounding
points. Yet in language processing, the correct meaning of a word often depends
on its global context. As a step toward incorporating global context into
representation learning, we develop a representation learning algorithm that
incorporates joint prediction into its technique for producing features for a
word. We develop efficient variational methods for learning Factorial Hidden
Markov Models from large texts, and use variational distributions to produce
features for each word that are sensitive to the entire input sequence, not
just to a local context window. Experiments on part-of-speech tagging and
chunking indicate that the features are competitive with or better than
existing state-of-the-art representation learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6169</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6169</id><created>2013-12-20</created><updated>2014-02-02</updated><authors><author><keyname>Lagnier</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Bourigault</keyname><forenames>Simon</forenames></author><author><keyname>Lamprier</keyname><forenames>Sylvain</forenames></author><author><keyname>Denoyer</keyname><forenames>Ludovic</forenames></author><author><keyname>Gallinari</keyname><forenames>Patrick</forenames></author></authors><title>Learning Information Spread in Content Networks</title><categories>cs.LG cs.SI physics.soc-ph</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model for predicting the diffusion of content information on
social media. When propagation is usually modeled on discrete graph structures,
we introduce here a continuous diffusion model, where nodes in a diffusion
cascade are projected onto a latent space with the property that their
proximity in this space reflects the temporal diffusion process. We focus on
the task of predicting contaminated users for an initial initial information
source and provide preliminary results on differents datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6170</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6170</id><created>2013-12-20</created><authors><author><keyname>Alhamazani</keyname><forenames>Khalid</forenames><affiliation>University of Delhi, India</affiliation></author><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames><affiliation>University of Delhi, India</affiliation></author><author><keyname>Mitra</keyname><forenames>Karan</forenames><affiliation>University of Delhi, India</affiliation></author><author><keyname>Rabhi</keyname><forenames>Fethi</forenames><affiliation>University of Delhi, India</affiliation></author><author><keyname>Khan</keyname><forenames>Samee Ullah</forenames><affiliation>University of Delhi, India</affiliation></author><author><keyname>Guabtni</keyname><forenames>Adnene</forenames><affiliation>University of Delhi, India</affiliation></author><author><keyname>Bhatnagar</keyname><forenames>Vasudha</forenames><affiliation>University of Delhi, India</affiliation></author></authors><title>An Overview of the Commercial Cloud Monitoring Tools: Research
  Dimensions, Design Issues, and State-of-the-Art</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud monitoring activity involves dynamically tracking the Quality of
Service (QoS) parameters related to virtualized resources (e.g., VM, storage,
network, appliances, etc.), the physical resources they share, the applications
running on them and data hosted on them. Applications and resources
configuration in cloud computing environment is quite challenging considering a
large number of heterogeneous cloud resources. Further, considering the fact
that at each point of time, there will be a different and specific cloud
service which may be massively required. Hence, cloud monitoring tools can
assist a cloud providers or application developers in: (i) keeping their
resources and applications operating at peak efficiency; (ii) detecting
variations in resource and application performance; (iii) accounting the
Service Level Agreement (SLA) violations of certain QoS parameters; and (iv)
tracking the leave and join operations of cloud resources due to failures and
other dynamic configuration changes.
  In this paper, we identify and discuss the major research dimensions and
design issues related to engineering cloud monitoring tools. We further discuss
how aforementioned research dimensions and design issues are handled by current
academic research as well as by commercial monitoring tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6171</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6171</id><created>2013-12-20</created><updated>2014-01-10</updated><authors><author><keyname>Wang</keyname><forenames>Ti</forenames></author><author><keyname>Silver</keyname><forenames>Daniel L.</forenames></author></authors><title>Learning Paired-associate Images with An Unsupervised Deep Learning
  Architecture</title><categories>cs.NE cs.CV cs.LG</categories><comments>9 pages, for ICLR-2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an unsupervised multi-modal learning system that learns
associative representation from two input modalities, or channels, such that
input on one channel will correctly generate the associated response at the
other and vice versa. In this way, the system develops a kind of supervised
classification model meant to simulate aspects of human associative memory. The
system uses a deep learning architecture (DLA) composed of two input/output
channels formed from stacked Restricted Boltzmann Machines (RBM) and an
associative memory network that combines the two channels. The DLA is trained
on pairs of MNIST handwritten digit images to develop hierarchical features and
associative representations that are able to reconstruct one image given its
paired-associate. Experiments show that the multi-modal learning system
generates models that are as accurate as back-propagation networks but with the
advantage of a bi-directional network and unsupervised learning from either
paired or non-paired training examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6173</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6173</id><created>2013-12-20</created><updated>2014-03-20</updated><authors><author><keyname>Hermann</keyname><forenames>Karl Moritz</forenames></author><author><keyname>Blunsom</keyname><forenames>Phil</forenames></author></authors><title>Multilingual Distributed Representations without Word Alignment</title><categories>cs.CL</categories><comments>To appear at ICLR 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed representations of meaning are a natural way to encode covariance
relationships between words and phrases in NLP. By overcoming data sparsity
problems, as well as providing information about semantic relatedness which is
not available in discrete representations, distributed representations have
proven useful in many NLP tasks. Recent work has shown how compositional
semantic representations can successfully be applied to a number of monolingual
applications such as sentiment analysis. At the same time, there has been some
initial success in work on learning shared word-level representations across
languages. We combine these two approaches by proposing a method for learning
distributed representations in a multilingual setup. Our model learns to assign
similar embeddings to aligned sentences and dissimilar ones to sentence which
are not aligned while not requiring word alignments. We show that our
representations are semantically informative and apply them to a cross-lingual
document classification task where we outperform the previous state of the art.
Further, by employing parallel corpora of multiple language pairs we find that
our model learns representations that capture semantic relationships across
languages for which no parallel data was used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6180</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6180</id><created>2013-12-20</created><authors><author><keyname>Liu</keyname><forenames>W.</forenames></author><author><keyname>Liu</keyname><forenames>H.</forenames></author><author><keyname>Tao</keyname><forenames>D.</forenames></author><author><keyname>Wang</keyname><forenames>Y.</forenames></author><author><keyname>Lu</keyname><forenames>K.</forenames></author></authors><title>Manifold regularized kernel logistic regression for web image annotation</title><categories>cs.LG cs.MM</categories><comments>submitted to Neurocomputing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid advance of Internet technology and smart devices, users often
need to manage large amounts of multimedia information using smart devices,
such as personal image and video accessing and browsing. These requirements
heavily rely on the success of image (video) annotation, and thus large scale
image annotation through innovative machine learning methods has attracted
intensive attention in recent years. One representative work is support vector
machine (SVM). Although it works well in binary classification, SVM has a
non-smooth loss function and can not naturally cover multi-class case. In this
paper, we propose manifold regularized kernel logistic regression (KLR) for web
image annotation. Compared to SVM, KLR has the following advantages: (1) the
KLR has a smooth loss function; (2) the KLR produces an explicit estimate of
the probability instead of class label; and (3) the KLR can naturally be
generalized to the multi-class case. We carefully conduct experiments on MIR
FLICKR dataset and demonstrate the effectiveness of manifold regularized kernel
logistic regression for image annotation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6182</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6182</id><created>2013-12-20</created><authors><author><keyname>Liu</keyname><forenames>W.</forenames></author><author><keyname>Zhang</keyname><forenames>H.</forenames></author><author><keyname>Tao</keyname><forenames>D.</forenames></author><author><keyname>Wang</keyname><forenames>Y.</forenames></author><author><keyname>Lu</keyname><forenames>K.</forenames></author></authors><title>Large-Scale Paralleled Sparse Principal Component Analysis</title><categories>cs.MS cs.LG cs.NA stat.ML</categories><comments>submitted to Multimedia Tools and Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal component analysis (PCA) is a statistical technique commonly used
in multivariate data analysis. However, PCA can be difficult to interpret and
explain since the principal components (PCs) are linear combinations of the
original variables. Sparse PCA (SPCA) aims to balance statistical fidelity and
interpretability by approximating sparse PCs whose projections capture the
maximal variance of original data. In this paper we present an efficient and
paralleled method of SPCA using graphics processing units (GPUs), which can
process large blocks of data in parallel. Specifically, we construct parallel
implementations of the four optimization formulations of the generalized power
method of SPCA (GP-SPCA), one of the most efficient and effective SPCA
approaches, on a GPU. The parallel GPU implementation of GP-SPCA (using CUBLAS)
is up to eleven times faster than the corresponding CPU implementation (using
CBLAS), and up to 107 times faster than a MatLab implementation. Extensive
comparative experiments in several real-world datasets confirm that SPCA offers
a practical advantage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6184</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6184</id><created>2013-12-20</created><updated>2014-10-10</updated><authors><author><keyname>Ba</keyname><forenames>Lei Jimmy</forenames></author><author><keyname>Caruana</keyname><forenames>Rich</forenames></author></authors><title>Do Deep Nets Really Need to be Deep?</title><categories>cs.LG cs.NE</categories><comments>final revision coming soon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, deep neural networks are the state of the art on problems such as
speech recognition and computer vision. In this extended abstract, we show that
shallow feed-forward networks can learn the complex functions previously
learned by deep nets and achieve accuracies previously only achievable with
deep models. Moreover, in some cases the shallow neural nets can learn these
deep functions using a total number of parameters similar to the original deep
model. We evaluate our method on the TIMIT phoneme recognition task and are
able to train shallow fully-connected nets that perform similarly to complex,
well-engineered, deep convolutional architectures. Our success in training
shallow neural nets to mimic deeper models suggests that there probably exist
better algorithms for training shallow feed-forward nets than those currently
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6186</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6186</id><created>2013-12-20</created><authors><author><keyname>Paine</keyname><forenames>Thomas</forenames></author><author><keyname>Jin</keyname><forenames>Hailin</forenames></author><author><keyname>Yang</keyname><forenames>Jianchao</forenames></author><author><keyname>Lin</keyname><forenames>Zhe</forenames></author><author><keyname>Huang</keyname><forenames>Thomas</forenames></author></authors><title>GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network
  Training</title><categories>cs.CV cs.DC cs.LG cs.NE</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to train large-scale neural networks has resulted in
state-of-the-art performance in many areas of computer vision. These results
have largely come from computational break throughs of two forms: model
parallelism, e.g. GPU accelerated training, which has seen quick adoption in
computer vision circles, and data parallelism, e.g. A-SGD, whose large scale
has been used mostly in industry. We report early experiments with a system
that makes use of both model parallelism and data parallelism, we call GPU
A-SGD. We show using GPU A-SGD it is possible to speed up training of large
convolutional neural networks useful for computer vision. We believe GPU A-SGD
will make it possible to train larger networks on larger training sets in a
reasonable amount of time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6189</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6189</id><created>2013-12-20</created><updated>2013-12-24</updated><authors><author><keyname>Gold</keyname><forenames>Omer</forenames></author><author><keyname>Cohen</keyname><forenames>Reuven</forenames></author></authors><title>Coping with Physical Attacks on Random Network Structures</title><categories>cs.SY cs.SI math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication networks are vulnerable to natural disasters, such as
earthquakes or floods, as well as to physical attacks, such as an
Electromagnetic Pulse (EMP) attack. Such real-world events happen at specific
geographical locations and disrupt specific parts of the network. Therefore,
the geographical layout of the network determines the impact of such events on
the network's physical topology in terms of capacity, connectivity, and flow.
  Recent works focused on assessing the vulnerability of a deterministic
network to such events. In this work, we focus on assessing the vulnerability
of (geographical) random networks to such disasters. We consider stochastic
graph models in which nodes and links are probabilistically distributed on a
plane, and model the disaster event as a circular cut that destroys any node or
link within or intersecting the circle.
  We develop algorithms for assessing the damage of both targeted and
non-targeted (random) attacks and determining which attack locations have the
expected most disruptive impact on the network. Then, we provide experimental
results for assessing the impact of circular disasters to communications
networks in the USA, where the network's geographical layout was modeled
probabilistically, relying on demographic information only. Our results
demonstrates the applicability of our algorithms to real-world scenarios.
  Our algorithms allows to examine how valuable is public information about the
network's geographical area (e.g., demography, topography, economy) to an
attacker's destruction assessment capabilities in the case the network's
physical topology is hidden or examine the affect of hiding the actual physical
location of the fibers on the attack strategy. Thereby, our schemes can be used
as a tool for policy makers and engineers to design more robust networks and
identifying locations which require additional protection efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6190</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6190</id><created>2013-12-20</created><updated>2014-05-28</updated><authors><author><keyname>Tran</keyname><forenames>Son N.</forenames></author><author><keyname>Garcez</keyname><forenames>Artur d'Avila</forenames></author></authors><title>Adaptive Feature Ranking for Unsupervised Transfer Learning</title><categories>cs.LG</categories><comments>9 pages 7 figures, new experimental results on ranking and transfer
  have been added, typo fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transfer Learning is concerned with the application of knowledge gained from
solving a problem to a different but related problem domain. In this paper, we
propose a method and efficient algorithm for ranking and selecting
representations from a Restricted Boltzmann Machine trained on a source domain
to be transferred onto a target domain. Experiments carried out using the
MNIST, ICDAR and TiCC image datasets show that the proposed adaptive feature
ranking and transfer learning method offers statistically significant
improvements on the training of RBMs. Our method is general in that the
knowledge chosen by the ranking function does not depend on its relation to any
specific target domain, and it works with unsupervised learning and
knowledge-based transfer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6192</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6192</id><created>2013-12-20</created><updated>2014-02-15</updated><authors><author><keyname>Bowman</keyname><forenames>Samuel R.</forenames></author></authors><title>Can recursive neural tensor networks learn logical reasoning?</title><categories>cs.CL cs.LG</categories><comments>Submitted for presentation at ICLR 2014. Source code and data:
  http://goo.gl/PSyF5u</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recursive neural network models and their accompanying vector representations
for words have seen success in an array of increasingly semantically
sophisticated tasks, but almost nothing is known about their ability to
accurately capture the aspects of linguistic meaning that are necessary for
interpretation or reasoning. To evaluate this, I train a recursive model on a
new corpus of constructed examples of logical reasoning in short sentences,
like the inference of &quot;some animal walks&quot; from &quot;some dog walks&quot; or &quot;some cat
walks,&quot; given that dogs and cats are animals. This model learns representations
that generalize well to new types of reasoning pattern in all but a few cases,
a result which is promising for the ability of learned representation models to
capture logical reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6197</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6197</id><created>2013-12-20</created><updated>2014-01-02</updated><authors><author><keyname>Warde-Farley</keyname><forenames>David</forenames></author><author><keyname>Goodfellow</keyname><forenames>Ian J.</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>An empirical analysis of dropout in piecewise linear networks</title><categories>stat.ML cs.LG cs.NE</categories><comments>Extensive updates; 8 pages plus acknowledgements/references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently introduced dropout training criterion for neural networks has
been the subject of much attention due to its simplicity and remarkable
effectiveness as a regularizer, as well as its interpretation as a training
procedure for an exponentially large ensemble of networks that share
parameters. In this work we empirically investigate several questions related
to the efficacy of dropout, specifically as it concerns networks employing the
popular rectified linear activation function. We investigate the quality of the
test time weight-scaling inference procedure by evaluating the geometric
average exactly in small models, as well as compare the performance of the
geometric mean to the arithmetic mean more commonly employed by ensemble
techniques. We explore the effect of tied weights on the ensemble
interpretation by training ensembles of masked networks without tied weights.
Finally, we investigate an alternative criterion based on a biased estimator of
the maximum likelihood ensemble gradient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6199</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6199</id><created>2013-12-20</created><updated>2014-02-19</updated><authors><author><keyname>Szegedy</keyname><forenames>Christian</forenames></author><author><keyname>Zaremba</keyname><forenames>Wojciech</forenames></author><author><keyname>Sutskever</keyname><forenames>Ilya</forenames></author><author><keyname>Bruna</keyname><forenames>Joan</forenames></author><author><keyname>Erhan</keyname><forenames>Dumitru</forenames></author><author><keyname>Goodfellow</keyname><forenames>Ian</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author></authors><title>Intriguing properties of neural networks</title><categories>cs.CV cs.LG cs.NE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Deep neural networks are highly expressive models that have recently achieved
state of the art performance on speech and visual recognition tasks. While
their expressiveness is the reason they succeed, it also causes them to learn
uninterpretable solutions that could have counter-intuitive properties. In this
paper we report two such properties.
  First, we find that there is no distinction between individual high level
units and random linear combinations of high level units, according to various
methods of unit analysis. It suggests that it is the space, rather than the
individual units, that contains of the semantic information in the high layers
of neural networks.
  Second, we find that deep neural networks learn input-output mappings that
are fairly discontinuous to a significant extend. We can cause the network to
misclassify an image by applying a certain imperceptible perturbation, which is
found by maximizing the network's prediction error. In addition, the specific
nature of these perturbations is not a random artifact of learning: the same
perturbation can cause a different network, that was trained on a different
subset of the dataset, to misclassify the same input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6201</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6201</id><created>2013-12-20</created><authors><author><keyname>Wang</keyname><forenames>Farn</forenames></author><author><keyname>Wu</keyname><forenames>Jung-Hsuan</forenames></author><author><keyname>Schewe</keyname><forenames>Sven</forenames></author><author><keyname>Huang</keyname><forenames>Chung-Hao</forenames></author></authors><title>Coverage Games for Testing Nondeterministic Systems</title><categories>cs.SE cs.FL</categories><comments>21 pages, 5 figures, 15 pages in the main text, 6 pages in appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern software systems may exhibit a nondeterministic behavior due to many
unpredictable factors. In this work, we propose the node coverage game, a two
player turn-based game played on a finite game graph, as a formalization of the
problem to test such systems. Each node in the graph represents a {\em
functional equivalence class} of the software under test (SUT). One player, the
tester, wants to maximize the node coverage, measured by the number of nodes
visited when exploring the game graphs, while his opponent, the SUT, wants to
minimize it. An optimal test would maximize the cover, and it is an interesting
problem to find the maximal number of nodes that the tester can guarantee to
visit, irrespective of the responses of the SUT. We show that the decision
problem of whether the guarantee is less than a given number is NP-complete.
Then we present techniques for testing nondeterministic SUTs with existing test
suites for deterministic models. Finally, we report our implementation and
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6203</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6203</id><created>2013-12-20</created><updated>2014-05-21</updated><authors><author><keyname>Bruna</keyname><forenames>Joan</forenames></author><author><keyname>Zaremba</keyname><forenames>Wojciech</forenames></author><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Spectral Networks and Locally Connected Networks on Graphs</title><categories>cs.LG cs.CV cs.NE</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks are extremely efficient architectures in image
and audio recognition tasks, thanks to their ability to exploit the local
translational invariance of signal classes over their domain. In this paper we
consider possible generalizations of CNNs to signals defined on more general
domains without the action of a translation group. In particular, we propose
two constructions, one based upon a hierarchical clustering of the domain, and
another based on the spectrum of the graph Laplacian. We show through
experiments that for low-dimensional graphs it is possible to learn
convolutional layers with a number of parameters independent of the input size,
resulting in efficient deep architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6204</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6204</id><created>2013-12-20</created><updated>2014-02-17</updated><authors><author><keyname>Hoffman</keyname><forenames>Judy</forenames></author><author><keyname>Tzeng</keyname><forenames>Eric</forenames></author><author><keyname>Donahue</keyname><forenames>Jeff</forenames></author><author><keyname>Jia</keyname><forenames>Yangqing</forenames></author><author><keyname>Saenko</keyname><forenames>Kate</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>One-Shot Adaptation of Supervised Deep Convolutional Models</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dataset bias remains a significant barrier towards solving real world
computer vision tasks. Though deep convolutional networks have proven to be a
competitive approach for image classification, a question remains: have these
models have solved the dataset bias problem? In general, training or
fine-tuning a state-of-the-art deep model on a new domain requires a
significant amount of data, which for many applications is simply not
available. Transfer of models directly to new domains without adaptation has
historically led to poor recognition performance. In this paper, we pose the
following question: is a single image dataset, much larger than previously
explored for adaptation, comprehensive enough to learn general deep models that
may be effectively applied to new image domains? In other words, are deep CNNs
trained on large amounts of labeled data as susceptible to dataset bias as
previous methods have been shown to be? We show that a generic supervised deep
CNN model trained on a large dataset reduces, but does not remove, dataset
bias. Furthermore, we propose several methods for adaptation with deep models
that are able to operate with little (one example per category) or no labeled
domain specific data. Our experiments show that adaptation of deep models on
benchmark visual domain adaptation datasets can provide a significant
performance boost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6205</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6205</id><created>2013-12-20</created><updated>2014-01-02</updated><authors><author><keyname>Wang</keyname><forenames>Sida I.</forenames></author><author><keyname>Frostig</keyname><forenames>Roy</forenames></author><author><keyname>Liang</keyname><forenames>Percy</forenames></author><author><keyname>Manning</keyname><forenames>Christopher D.</forenames></author></authors><title>Relaxations for inference in restricted Boltzmann machines</title><categories>stat.ML cs.LG</categories><comments>ICLR 2014 workshop track submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a relaxation-based approximate inference algorithm that samples
near-MAP configurations of a binary pairwise Markov random field. We experiment
on MAP inference tasks in several restricted Boltzmann machines. We also use
our underlying sampler to estimate the log-partition function of restricted
Boltzmann machines and compare against other sampling-based methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6208</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6208</id><created>2013-12-21</created><authors><author><keyname>Liu</keyname><forenames>Gang</forenames></author><author><keyname>Huang</keyname><forenames>Ting-Zhu</forenames></author><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Lv</keyname><forenames>Xiao-Guang</forenames></author></authors><title>Total variation with overlapping group sparsity for image deblurring
  under impulse noise</title><categories>math.NA cs.CV</categories><comments>22 pages, 57 figures, submitted</comments><journal-ref>PLOS ONE 2015 10(4): e0122562</journal-ref><doi>10.1371/journal.pone.0122562</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The total variation (TV) regularization method is an effective method for
image deblurring in preserving edges. However, the TV based solutions usually
have some staircase effects. In this paper, in order to alleviate the staircase
effect, we propose a new model for restoring blurred images with impulse noise.
The model consists of an $\ell_1$-fidelity term and a TV with overlapping group
sparsity (OGS) regularization term. Moreover, we impose a box constraint to the
proposed model for getting more accurate solutions. An efficient and effective
algorithm is proposed to solve the model under the framework of the alternating
direction method of multipliers (ADMM). We use an inner loop which is nested
inside the majorization minimization (MM) iteration for the subproblem of the
proposed method. Compared with other methods, numerical results illustrate that
the proposed method, can significantly improve the restoration quality, both in
avoiding staircase effects and in terms of peak signal-to-noise ratio (PSNR)
and relative error (ReE).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6211</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6211</id><created>2013-12-21</created><updated>2015-03-03</updated><authors><author><keyname>Goodfellow</keyname><forenames>Ian J.</forenames></author><author><keyname>Mirza</keyname><forenames>Mehdi</forenames></author><author><keyname>Xiao</keyname><forenames>Da</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>An Empirical Investigation of Catastrophic Forgetting in Gradient-Based
  Neural Networks</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Catastrophic forgetting is a problem faced by many machine learning models
and algorithms. When trained on one task, then trained on a second task, many
machine learning models &quot;forget&quot; how to perform the first task. This is widely
believed to be a serious problem for neural networks. Here, we investigate the
extent to which the catastrophic forgetting problem occurs for modern neural
networks, comparing both established and recent gradient-based training
algorithms and activation functions. We also examine the effect of the
relationship between the first task and the second task on catastrophic
forgetting. We find that it is always best to train using the dropout
algorithm--the dropout algorithm is consistently best at adapting to the new
task, remembering the old task, and has the best tradeoff curve between these
two extremes. We find that different tasks and relationships between tasks
result in very different rankings of activation function performance. This
suggests the choice of activation function should always be cross-validated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6214</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6214</id><created>2013-12-21</created><updated>2014-05-25</updated><authors><author><keyname>Hazan</keyname><forenames>Elad</forenames></author><author><keyname>Karnin</keyname><forenames>Zohar</forenames></author><author><keyname>Mehka</keyname><forenames>Raghu</forenames></author></authors><title>Volumetric Spanners: an Efficient Exploration Basis for Learning</title><categories>cs.LG cs.AI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous machine learning problems require an exploration basis - a mechanism
to explore the action space. We define a novel geometric notion of exploration
basis with low variance, called volumetric spanners, and give efficient
algorithms to construct such a basis.
  We show how efficient volumetric spanners give rise to the first efficient
and optimal regret algorithm for bandit linear optimization over general convex
sets. Previously such results were known only for specific convex sets, or
under special conditions such as the existence of an efficient self-concordant
barrier for the underlying set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6215</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6215</id><created>2013-12-21</created><updated>2014-04-10</updated><authors><author><keyname>Hoang</keyname><forenames>Hung Gia</forenames></author><author><keyname>Vo</keyname><forenames>Ba Tuong</forenames></author></authors><title>Sensor management for multi-target tracking via Multi-Bernoulli
  filtering</title><categories>cs.SY</categories><comments>Final published version, 10 pages, 3 figures</comments><journal-ref>Automatica 50 (2014) 1135-1142</journal-ref><doi>10.1016/j.automatica.2014.02.007</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In multi-object stochastic systems, the issue of sensor management is a
theoretically and computationally challenging problem. In this paper, we
present a novel random finite set (RFS) approach to the multi-target sensor
management problem within the partially observed Markov decision process
(POMDP) framework. The multi-target state is modelled as a multi-Bernoulli RFS,
and the multi-Bernoulli filter is used in conjunction with two different
control objectives: maximizing the expected R\'enyi divergence between the
predicted and updated densities, and minimizing the expected posterior
cardinality variance. Numerical studies are presented in two scenarios where a
mobile sensor tracks five moving targets with different levels of
observability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6219</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6219</id><created>2013-12-21</created><authors><author><keyname>Ray</keyname><forenames>Kasturika B.</forenames></author></authors><title>Extracting Region of Interest for Palm Print Authentication</title><categories>cs.CV</categories><comments>8 pages,4 figures, 1 table, 1 photo of author and 1 photo of
  co-author, 3 paper has published (2011, 2011, 2012)</comments><journal-ref>IJASCSE Journal, Volume 2, Issue 6, December2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biometrics authentication is an effective method for automatically
recognizing individuals. The authentication consists of an enrollment phase and
an identification or verification phase. In the stages of enrollment known
(training) samples after the pre-processing stage are used for suitable feature
extraction to generate the template database. In the verification stage, the
test sample is similarly pre processed and subjected to feature extraction
modules, and then it is matched with the training feature templates to decide
whether it is a genuine or not. This paper presents use of a region of interest
(ROI) for palm print technology. First some of the existing methods for palm
print identification have been introduced. Then focus has been given on
extraction of a suitable smaller region from the acquired palm print to improve
the identification method accuracy. Several existing work in the topic of
region extraction have been examined. Subsequently, a simple and original
method has then proposed for locating the ROI that can be effectively used for
palm print analysis. The ROI extracted using this new technique is suitable for
different types of processing as it creates a rectangular or square area around
the center of activity represented by the lines, wrinkles and ridges of the
palm print. The effectiveness of the ROI approach has been tested by
integrating it with a texture based identification / authentication system
proposed earlier. The improvement has been shown by comparing the
identification accuracy rate before and after the ROI pre-processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6224</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6224</id><created>2013-12-21</created><updated>2015-07-19</updated><authors><author><keyname>Hoang</keyname><forenames>Hung Gia</forenames></author><author><keyname>Vo</keyname><forenames>Ba-Ngu</forenames></author><author><keyname>Vo</keyname><forenames>Ba-Tuong</forenames></author><author><keyname>Mahler</keyname><forenames>Ronald</forenames></author></authors><title>The Cauchy-Schwarz divergence for Poisson point processes</title><categories>cs.IT math.IT</categories><comments>Two colunms, 11 pages, 5 figures. This paper has been published in
  the IEEE Transaction on Information Theory. Part of the paper was presented
  at the 2014 IEEE Workshop on Statistical Signal Processing, Gold Coast,
  Australia</comments><journal-ref>IEEE Trans. Inf. Theory (2015), vol. 61, no. 8, pp. 4475-4485</journal-ref><doi>10.1109/TIT.2015.2441709</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we extend the notion of Cauchy-Schwarz divergence to point
processes and establish that the Cauchy-Schwarz divergence between the
probability densities of two Poisson point processes is half the squared
$\mathbf{L^{2}}$-distance between their intensity functions. Extension of this
result to mixtures of Poisson point processes and, in the case where the
intensity functions are Gaussian mixtures, closed form expressions for the
Cauchy-Schwarz divergence are presented. Our result also implies that the
Bhattachryaa distance between the probability distributions of two Poisson
point processes is equal to the square of the Hellinger distance between their
intensity measures. We illustrate the result via a sensor management
application where the system states are modeled as point processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6229</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6229</id><created>2013-12-21</created><updated>2014-02-23</updated><authors><author><keyname>Sermanet</keyname><forenames>Pierre</forenames></author><author><keyname>Eigen</keyname><forenames>David</forenames></author><author><keyname>Zhang</keyname><forenames>Xiang</forenames></author><author><keyname>Mathieu</keyname><forenames>Michael</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>OverFeat: Integrated Recognition, Localization and Detection using
  Convolutional Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an integrated framework for using Convolutional Networks for
classification, localization and detection. We show how a multiscale and
sliding window approach can be efficiently implemented within a ConvNet. We
also introduce a novel deep learning approach to localization by learning to
predict object boundaries. Bounding boxes are then accumulated rather than
suppressed in order to increase detection confidence. We show that different
tasks can be learned simultaneously using a single shared network. This
integrated framework is the winner of the localization task of the ImageNet
Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very
competitive results for the detection and classifications tasks. In
post-competition work, we establish a new state of the art for the detection
task. Finally, we release a feature extractor from our best model called
OverFeat.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6242</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6242</id><created>2013-12-21</created><updated>2014-09-03</updated><authors><author><keyname>Li</keyname><forenames>Fu</forenames></author><author><keyname>Tzameret</keyname><forenames>Iddo</forenames></author></authors><title>Generating Matrix Identities and Proof Complexity</title><categories>cs.CC</categories><comments>46 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the fundamental lower bounds questions in proof complexity, we
initiate the study of matrix identities as hard instances for strong proof
systems. A matrix identity of $d \times d$ matrices over a field $\mathbb{F}$,
is a non-commutative polynomial $f(x_1,\ldots,x_n)$ over $\mathbb{F}$ such that
$f$ vanishes on every $d \times d$ matrix assignment to its variables.
  We focus on arithmetic proofs, which are proofs of polynomial identities
operating with arithmetic circuits and whose axioms are the polynomial-ring
axioms (these proofs serve as an algebraic analogue of the Extended Frege
propositional proof system; and over $GF(2)$ they constitute formally a
sub-system of Extended Frege [HT12]). We introduce a decreasing in strength
hierarchy of proof systems within arithmetic proofs, in which the $d$th level
is a sound and complete proof system for proving $d \times d$ matrix identities
(over a given field). For each level $d&gt;2$ in the hierarchy, we establish a
proof-size lower bound in terms of the number of variables in the matrix
identity proved: we show the existence of a family of matrix identities $f_n$
with $n$ variables, such that any proof of $f_n=0$ requires $\Omega(n^{2d})$
number of lines. The lower bound argument uses fundamental results from the
theory of algebras with polynomial identities together with a generalization of
the arguments in [Hru11].
  We then set out to study matrix identities as hard instances for (full)
arithmetic proofs. We present two conjectures, one about non-commutative
arithmetic circuit complexity and the other about proof complexity, under which
up to exponential-size lower bounds on arithmetic proofs (in terms of the
arithmetic circuit size of the identities proved) hold. Finally, we discuss the
applicability of our approach to strong propositional proof systems such as
Extended Frege.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6246</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6246</id><created>2013-12-21</created><authors><author><keyname>Levine</keyname><forenames>John</forenames></author><author><keyname>Ritchie</keyname><forenames>Graeme</forenames></author><author><keyname>Andrew</keyname><forenames>Alastair</forenames></author><author><keyname>Gates</keyname><forenames>Simon</forenames></author></authors><title>New Results for the Heterogeneous Multi-Processor Scheduling Problem
  using a Fast, Effective Local Search and Random Disruption</title><categories>cs.DC</categories><comments>6 pages. Proceedings of PlanSIG 2012, Teeside University, December
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The efficient scheduling of independent computational tasks in a
heterogeneous computing environment is an important problem that occurs in
domains such as Grid and Cloud computing. Finding optimal schedules is an
NP-hard problem in general, so we have to rely on approximate algorithms to
come up schedules that are as near to optimal as possible. In our previous work
on this problem, we applied a fast, effective local search to generate
reasonably good schedules in a short amount of time and used ant colony
optimisation (ACO) to incrementally improve those schedules over a longer time
period. In this work, we replace the ACO component with a random disruption
algorithm and find that this produces results which are competitive with the
current state of the art over a 90 second execution time. We also ran our
algorithm for a longer time period on 12 well-known benchmark instances and as
a result provide new upper bounds for these instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6249</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6249</id><created>2013-12-21</created><updated>2014-09-30</updated><authors><author><keyname>Othman</keyname><forenames>Abraham</forenames></author><author><keyname>Papadimitriou</keyname><forenames>Christos</forenames></author><author><keyname>Rubinstein</keyname><forenames>Aviad</forenames></author></authors><title>The Complexity of Fairness through Equilibrium</title><categories>cs.GT</categories><comments>Appeared in EC 2014</comments><acm-class>F.2; J.4; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Competitive equilibrium with equal incomes (CEEI) is a well known fair
allocation mechanism; however, for indivisible resources a CEEI may not exist.
It was shown in [Budish '11] that in the case of indivisible resources there is
always an allocation, called A-CEEI, that is approximately fair, approximately
truthful, and approximately efficient, for some favorable approximation
parameters. This approximation is used in practice to assign students to
classes. In this paper we show that finding the A-CEEI allocation guaranteed to
exist by Budish's theorem is PPAD-complete. We further show that finding an
approximate equilibrium with better approximation guarantees is even harder:
NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6259</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6259</id><created>2013-12-21</created><authors><author><keyname>Mayer</keyname><forenames>Robert V</forenames></author></authors><title>Generalized simulation model of teaching and its research on PC</title><categories>cs.CY</categories><comments>6 pages, 2 figures, 2 programs; Psychology, sociology and pedagogy
  April 2013</comments><msc-class>68U20</msc-class><acm-class>I.6.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the important problems of cyber pedagogy is the following: how,
knowing the parameters of the student, his initial level of knowledge and the
impact of the teacher to predict knowledge of student at subsequent times.
Simulation method allows you to create a computer program that simulates the
behavior of the 'teacher-student'-system and investigate the influence of
system parameters on the results of learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6260</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6260</id><created>2013-12-21</created><authors><author><keyname>Xiao</keyname><forenames>Mingyu</forenames></author><author><keyname>Nagamochi</keyname><forenames>Hiroshi</forenames></author></authors><title>Exact Algorithms for Maximum Independent Set</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the maximum independent set problem (MIS) on an $n$-vertex graph
can be solved in $1.1996^nn^{O(1)}$ time and polynomial space, which even is
faster than Robson's $1.2109^{n}n^{O(1)}$-time exponential-space algorithm
published in 1986. We also obtain improved algorithms for MIS in graphs with
maximum degree 6 and 7, which run in time of $1.1893^nn^{O(1)}$ and
$1.1970^nn^{O(1)}$, respectively. Our algorithms are obtained by using fast
algorithms for MIS in low-degree graphs in a hierarchical way and making a
careful analyses on the structure of bounded-degree graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6273</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6273</id><created>2013-12-21</created><authors><author><keyname>Alouane-Ksouri</keyname><forenames>Sonia</forenames></author><author><keyname>Sassi-Hidri</keyname><forenames>Minyar</forenames></author><author><keyname>Barkaoui</keyname><forenames>Kamel</forenames></author></authors><title>Parallel architectures for fuzzy triadic similarity learning</title><categories>cs.DC cs.LG stat.ML</categories><journal-ref>International Conference on Control, Engineering &amp; Information
  Technology (CEIT), Proceedings Engineering &amp; Technology, Vol. 1, pp. 121-126,
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a context of document co-clustering, we define a new similarity measure
which iteratively computes similarity while combining fuzzy sets in a
three-partite graph. The fuzzy triadic similarity (FT-Sim) model can deal with
uncertainty offers by the fuzzy sets. Moreover, with the development of the Web
and the high availability of storage spaces, more and more documents become
accessible. Documents can be provided from multiple sites and make similarity
computation an expensive processing. This problem motivated us to use parallel
computing. In this paper, we introduce parallel architectures which are able to
treat large and multi-source data sets by a sequential, a merging or a
splitting-based process. Then, we proceed to a local and a central (or global)
computing using the basic FT-Sim measure. The idea behind these architectures
is to reduce both time and space complexities thanks to parallel computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6282</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6282</id><created>2013-12-21</created><authors><author><keyname>Denis</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Gybels</keyname><forenames>Mattias</forenames></author><author><keyname>Habrard</keyname><forenames>Amaury</forenames></author></authors><title>Dimension-free Concentration Bounds on Hankel Matrices for Spectral
  Learning</title><categories>cs.LG</categories><comments>Extended version of a paper to appear at ICML 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning probabilistic models over strings is an important issue for many
applications. Spectral methods propose elegant solutions to the problem of
inferring weighted automata from finite samples of variable-length strings
drawn from an unknown target distribution. These methods rely on a singular
value decomposition of a matrix $H_S$, called the Hankel matrix, that records
the frequencies of (some of) the observed strings. The accuracy of the learned
distribution depends both on the quantity of information embedded in $H_S$ and
on the distance between $H_S$ and its mean $H_r$. Existing concentration bounds
seem to indicate that the concentration over $H_r$ gets looser with the size of
$H_r$, suggesting to make a trade-off between the quantity of used information
and the size of $H_r$. We propose new dimension-free concentration bounds for
several variants of Hankel matrices. Experiments demonstrate that these bounds
are tight and that they significantly improve existing bounds. These results
suggest that the concentration rate of the Hankel matrix around its mean does
not constitute an argument for limiting its size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6290</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6290</id><created>2013-12-21</created><updated>2016-02-25</updated><authors><author><keyname>Montina</keyname><forenames>Alberto</forenames></author><author><keyname>Wolf</keyname><forenames>Stefan</forenames></author></authors><title>Information-based measure of nonlocality</title><categories>quant-ph cs.IT math.IT</categories><comments>Substantial improvements have been made to the readablity of the
  paper. In particular, we provide a detailed justification of the adopted
  measure of communication. The order of Theorem 1 and 2 has been swapped and
  we have first considered the case of single-shot simulations, which gives a
  clearer clue of the ideas used for the case of parallel simulations. An
  appendix added</comments><journal-ref>New J. Phys. 18, 013035 (2016)</journal-ref><doi>10.1088/1367-2630/18/1/013035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum nonlocality concerns correlations among spatially separated systems
that cannot be classically explained without post-measurement communication
among the parties. Thus, a natural measure of nonlocal correlations is provided
by the minimal amount of communication required for classically simulating
them. In this paper, we present a method to compute the minimal communication
cost, which we call nonlocal capacity, for any general nonsignaling
correlations. This measure turns out to have an important role in communication
complexity and can be used to discriminate between local and nonlocal
correlations, as an alternative to the violation of Bell's inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6293</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6293</id><created>2013-12-21</created><authors><author><keyname>Ferrarons</keyname><forenames>Jaume</forenames><affiliation>ERIC</affiliation></author><author><keyname>Adhana</keyname><forenames>Mulu</forenames><affiliation>ERIC</affiliation></author><author><keyname>Colmenares</keyname><forenames>Carlos</forenames><affiliation>ERIC</affiliation></author><author><keyname>Pietrowska</keyname><forenames>Sandra</forenames><affiliation>ERIC</affiliation></author><author><keyname>Bentayeb</keyname><forenames>Fadila</forenames><affiliation>ERIC</affiliation></author><author><keyname>Darmont</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>ERIC</affiliation></author></authors><title>PRIMEBALL: a Parallel Processing Framework Benchmark for Big Data
  Applications in the Cloud</title><categories>cs.DC cs.DB</categories><comments>5th TPC Technology Conference on Performance Evaluation and
  Benchmarking (VLDB/TPCTC 13), Riva del Garda : Italy (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we draw the specifications of a novel benchmark for comparing
parallel processing frameworks in the context of big data applications hosted
in the cloud. We aim at filling several gaps in already existing cloud data
processing benchmarks, which lack a real-life context for their processes, thus
losing relevance when trying to assess performance for real applications.
Hence, we propose a fictitious news site hosted in the cloud that is to be
managed by the framework under analysis, together with several objective use
case scenarios and measures for evaluating system performance. The main
strengths of our benchmark are parallelization capabilities supporting cloud
features and big data properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6305</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6305</id><created>2013-12-21</created><authors><author><keyname>Sharir</keyname><forenames>Micha</forenames></author><author><keyname>Zaban</keyname><forenames>Shai</forenames></author></authors><title>Output-Sensitive Tools for Range Searching in Higher Dimensions</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a set of $n$ points in ${\mathbb R}^{d}$. A point $p \in P$ is
$k$\emph{-shallow} if it lies in a halfspace which contains at most $k$ points
of $P$ (including $p$). We show that if all points of $P$ are $k$-shallow, then
$P$ can be partitioned into $\Theta(n/k)$ subsets, so that any hyperplane
crosses at most $O((n/k)^{1-1/(d-1)} \log^{2/(d-1)}(n/k))$ subsets. Given such
a partition, we can apply the standard construction of a spanning tree with
small crossing number within each subset, to obtain a spanning tree for the
point set $P$, with crossing number $O(n^{1-1/(d-1)}k^{1/d(d-1)}
\log^{2/(d-1)}(n/k))$. This allows us to extend the construction of Har-Peled
and Sharir \cite{hs11} to three and higher dimensions, to obtain, for any set
of $n$ points in ${\mathbb R}^{d}$ (without the shallowness assumption), a
spanning tree $T$ with {\em small relative crossing number}. That is, any
hyperplane which contains $w \leq n/2$ points of $P$ on one side, crosses
$O(n^{1-1/(d-1)}w^{1/d(d-1)} \log^{2/(d-1)}(n/w))$ edges of $T$. Using a
similar mechanism, we also obtain a data structure for halfspace range
counting, which uses $O(n \log \log n)$ space (and somewhat higher
preprocessing cost), and answers a query in time $O(n^{1-1/(d-1)}k^{1/d(d-1)}
(\log (n/k))^{O(1)})$, where $k$ is the output size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6323</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6323</id><created>2013-12-21</created><updated>2014-12-24</updated><authors><author><keyname>Leivant</keyname><forenames>Daniel M</forenames><affiliation>Indiana University</affiliation></author></authors><title>Global semantic typing for inductive and coinductive computing</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (December
  25, 2014) lmcs:1116</journal-ref><doi>10.2168/LMCS-10(4:18)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inductive and coinductive types are commonly construed as ontological
(Church-style) types, denoting canonical data-sets such as natural numbers,
lists, and streams. For various purposes, notably the study of programs in the
context of global semantics, it is preferable to think of types as semantical
properties (Curry-style). Intrinsic theories were introduced in the late 1990s
to provide a purely logical framework for reasoning about programs and their
semantic types. We extend them here to data given by any combination of
inductive and coinductive definitions. This approach is of interest because it
fits tightly with syntactic, semantic, and proof theoretic fundamentals of
formal logic, with potential applications in implicit computational complexity
as well as extraction of programs from proofs. We prove a Canonicity Theorem,
showing that the global definition of program typing, via the usual (Tarskian)
semantics of first-order logic, agrees with their operational semantics in the
intended model. Finally, we show that every intrinsic theory is interpretable
in a conservative extension of first-order arithmetic. This means that
quantification over infinite data objects does not lead, on its own, to
proof-theoretic strength beyond that of Peano Arithmetic. Intrinsic theories
are perfectly amenable to formulas-as-types Curry-Howard morphisms, and were
used to characterize major computational complexity classes Their extensions
described here have similar potential which has already been applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6331</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6331</id><created>2013-12-21</created><authors><author><keyname>Orevkov</keyname><forenames>S. Yu.</forenames></author></authors><title>On modular computation of Groebner bases with integer coefficients</title><categories>math.AC cs.SC</categories><comments>3 pages</comments><msc-class>13P10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $I_1\subset I_2\subset\dots$ be an increasing sequence of ideals of the
ring $\Bbb Z[X]$, $X=(x_1,\dots,x_n)$ and let $I$ be their union. We propose an
algorithm to compute the Gr\&quot;obner base of $I$ under the assumption that the
Gr\&quot;obner bases of the ideal $\Bbb Q I$ of the ring $\Bbb Q[X]$ and the the
ideals $I\otimes(\Bbb Z/m\Bbb Z)$ of the rings $(\Bbb Z/m\Bbb Z)[X]$ are known.
  Such an algorithmic problem arises, for example, in the construction of
Markov and semi-Markov traces on cubic Hecke algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6335</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6335</id><created>2013-12-21</created><authors><author><keyname>Pei</keyname><forenames>Sen</forenames></author><author><keyname>Makse</keyname><forenames>Hernan A.</forenames></author></authors><title>Spreading dynamics in complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>23 pages, 2 figures</comments><journal-ref>Journal of Statistical Mechanics: Theory and Experiment 2013 (12),
  P12002</journal-ref><doi>10.1088/1742-5468/2013/12/P12002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Searching for influential spreaders in complex networks is an issue of great
significance for applications across various domains, ranging from the epidemic
control, innovation diffusion, viral marketing, social movement to idea
propagation. In this paper, we first display some of the most important
theoretical models that describe spreading processes, and then discuss the
problem of locating both the individual and multiple influential spreaders
respectively. Recent approaches in these two topics are presented. For the
identification of privileged single spreaders, we summarize several widely used
centralities, such as degree, betweenness centrality, PageRank, k-shell, etc.
We investigate the empirical diffusion data in a large scale online social
community -- LiveJournal. With this extensive dataset, we find that various
measures can convey very distinct information of nodes. Of all the users in
LiveJournal social network, only a small fraction of them involve in spreading.
For the spreading processes in LiveJournal, while degree can locate nodes
participating in information diffusion with higher probability, k-shell is more
effective in finding nodes with large influence. Our results should provide
useful information for designing efficient spreading strategies in reality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6349</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6349</id><created>2013-12-22</created><authors><author><keyname>Mohaisen</keyname><forenames>Aziz</forenames></author><author><keyname>Kim</keyname><forenames>Joongheon</forenames></author></authors><title>The Sybil Attacks and Defenses: A Survey</title><categories>cs.CR</categories><comments>10 pages, 1 figure</comments><journal-ref>Smart Computing Review, vol. 3, no. 6 pp 480-489, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we have a close look at the Sybil attack and advances in
defending against it, with particular emphasis on the recent work. We identify
three major veins of literature work to defend against the attack: using
trusted certification, using resources testing, and using social networks. The
first vein of literature considers defending against the attack using trusted
certification, which is done by either centralized certification or distributed
certification using cryptographic primitives that can replace the centralized
certification entity. The second vein of literature considers defending against
the attack by resources testing, which can by in the form of IP testing,
network coordinates, recurring cost as by requiring clients to solve puzzles.
The third and last vein of literature is by mitigating the attack combining
social networks used as bootstrapping security and tools from random walk
theory that have shown to be effective in defending against the attack under
certain assumptions. Our survey and analyses of the different schemes in the
three veins of literature show several shortcomings which form several
interesting directions and research questions worthy of investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6370</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6370</id><created>2013-12-22</created><authors><author><keyname>Mohammed</keyname><forenames>Jahangir</forenames></author><author><keyname>Nayak</keyname><forenames>Deepak Ranjan</forenames></author></authors><title>An Efficient Edge Detection Technique by Two Dimensional Rectangular
  Cellular Automata</title><categories>cs.CV</categories><comments>4 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new pattern of two dimensional cellular automata linear
rules that are used for efficient edge detection of an image. Since cellular
automata is inherently parallel in nature, it has produced desired output
within a unit time interval. We have observed four linear rules among 512 total
linear rules of a rectangular cellular automata in adiabatic or reflexive
boundary condition that produces an optimal result. These four rules are
directly applied once to the images and produced edge detected output. We
compare our results with the existing edge detection algorithms and found that
our results shows better edge detection with an enhancement of edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6393</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6393</id><created>2013-12-22</created><authors><author><keyname>Asghar</keyname><forenames>Muhammad Rizwan</forenames></author></authors><title>Privacy Preserving Enforcement of Sensitive Policies in Outsourced and
  Distributed Environments</title><categories>cs.CR</categories><comments>Ph.D. Dissertation. http://eprints-phd.biblio.unitn.it/1124/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The enforcement of sensitive policies in untrusted environments is still an
open challenge for policy-based systems. On the one hand, taking any
appropriate security decision requires access to these policies. On the other
hand, if such access is allowed in an untrusted environment then confidential
information might be leaked by the policies. The key challenge is how to
enforce sensitive policies and protect content in untrusted environments. In
the context of untrusted environments, we mainly distinguish between outsourced
and distributed environments. The most attractive paradigms concerning
outsourced and distributed environments are cloud computing and opportunistic
networks, respectively.
  In this dissertation, we present the design, technical and implementation
details of our proposed policy-based access control mechanisms for untrusted
environments. First of all, we provide full confidentiality of access policies
in outsourced environments, where service providers do not learn private
information about policies. We support expressive policies and take into
account contextual information. The system entities do not share any encryption
keys. For complex user management, we offer the full-fledged Role-Based Access
Control (RBAC) policies.
  In opportunistic networks, we protect content by specifying expressive
policies. In our proposed approach, brokers match subscriptions against
policies associated with content without compromising privacy of subscribers.
As a result, unauthorised brokers neither gain access to content nor learn
policies and authorised nodes gain access only if they satisfy policies
specified by publishers. Our proposed system provides scalable key management
in which loosely-coupled publishers and subscribers communicate without any
prior contact. Finally, we have developed a prototype of the system that runs
on real smartphones and analysed its performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6410</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6410</id><created>2013-12-22</created><authors><author><keyname>Chennamma</keyname><forenames>H. R.</forenames></author><author><keyname>Yuan</keyname><forenames>Xiaohui</forenames></author></authors><title>A Survey on Eye-Gaze Tracking Techniques</title><categories>cs.CV</categories><comments>6 pages, Journal</comments><journal-ref>Indian Journal of Computer Science and Engineering, ISSN :
  0976-5166, Vol. 4, No. 5, Oct-Nov 2013, pp. 388-393</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Study of eye-movement is being employed in Human Computer Interaction (HCI)
research. Eye - gaze tracking is one of the most challenging problems in the
area of computer vision. The goal of this paper is to present a review of
latest research in this continued growth of remote eye-gaze tracking. This
overview includes the basic definitions and terminologies, recent advances in
the field and finally the need of future development in the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6415</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6415</id><created>2013-12-22</created><updated>2015-02-18</updated><authors><author><keyname>Savic</keyname><forenames>Vladimir</forenames></author><author><keyname>Ferrer-Coll</keyname><forenames>Javier</forenames></author><author><keyname>Angskog</keyname><forenames>Per</forenames></author><author><keyname>Chilo</keyname><forenames>Jose</forenames></author><author><keyname>Stenumgaard</keyname><forenames>Peter</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Measurement Analysis and Channel Modeling for TOA-Based Ranging in
  Tunnels</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Wireless Communications, vol. 14, issue 1,
  pp. 456-467, Jan. 2015</journal-ref><doi>10.1109/TWC.2014.2350493</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robust and accurate positioning solution is required to increase the safety
in GPS-denied environments. Although there is a lot of available research in
this area, little has been done for confined environments such as tunnels.
Therefore, we organized a measurement campaign in a basement tunnel of
Link\&quot;{o}ping university, in which we obtained ultra-wideband (UWB) complex
impulse responses for line-of-sight (LOS), and three non-LOS (NLOS) scenarios.
This paper is focused on time-of-arrival (TOA) ranging since this technique can
provide the most accurate range estimates, which are required for range-based
positioning. We describe the measurement setup and procedure, select the
threshold for TOA estimation, analyze the channel propagation parameters
obtained from the power delay profile (PDP), and provide statistical model for
ranging. According to our results, the rise-time should be used for NLOS
identification, and the maximum excess delay should be used for NLOS error
mitigation. However, the NLOS condition cannot be perfectly determined, so the
distance likelihood has to be represented in a Gaussian mixture form. We also
compared these results with measurements from a mine tunnel, and found a
similar behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6421</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6421</id><created>2013-12-22</created><authors><author><keyname>Bai</keyname><forenames>He</forenames></author><author><keyname>Shafi</keyname><forenames>S. Yusef</forenames></author></authors><title>Output Synchronization of Nonlinear Systems under Input Disturbances</title><categories>cs.SY math.OC</categories><comments>8 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study synchronization of nonlinear systems that satisfy an incremental
passivity property. We consider the case where the control input is subject to
a class of disturbances, including constant and sinusoidal disturbances with
unknown phases and magnitudes and known frequencies. We design a distributed
control law that recovers the synchronization of the nonlinear systems in the
presence of the disturbances. Simulation results of Goodwin oscillators
illustrate the effectiveness of the control law. Finally, we highlight the
connection of the proposed control law to the dynamic average consensus
estimator developed in [1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6426</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6426</id><created>2013-12-22</created><authors><author><keyname>Mullins</keyname><forenames>John</forenames></author><author><keyname>Yeddes</keyname><forenames>Moez</forenames></author></authors><title>Opacity with Orwellian Observers and Intransitive Non-interference</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opacity is a general behavioural security scheme flexible enough to account
for several specific properties. Some secret set of behaviors of a system is
opaque if a passive attacker can never tell whether the observed behavior is a
secret one or not. Instead of considering the case of static observability
where the set of observable events is fixed off line or dynamic observability
where the set of observable events changes over time depending on the history
of the trace, we consider Orwellian partial observability where unobservable
events are not revealed unless a downgrading event occurs in the future of the
trace. We show how to verify that some regular secret is opaque for a regular
language L w.r.t. an Orwellian projection while it has been proved undecidable
even for a regular language L w.r.t. a general Orwellian observation function.
We finally illustrate relevancy of our results by proving the equivalence
between the opacity property of regular secrets w.r.t. Orwellian projection and
the intransitive non-interference property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6430</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6430</id><created>2013-12-22</created><updated>2014-07-14</updated><authors><author><keyname>Hara</keyname><forenames>Kota</forenames></author><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author></authors><title>Growing Regression Forests by Classification: Applications to Object
  Pose Estimation</title><categories>cs.CV cs.LG stat.ML</categories><comments>Paper accepted by ECCV 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a novel node splitting method for regression trees
and incorporate it into the regression forest framework. Unlike traditional
binary splitting, where the splitting rule is selected from a predefined set of
binary splitting rules via trial-and-error, the proposed node splitting method
first finds clusters of the training data which at least locally minimize the
empirical loss without considering the input space. Then splitting rules which
preserve the found clusters as much as possible are determined by casting the
problem into a classification problem. Consequently, our new node splitting
method enjoys more freedom in choosing the splitting rules, resulting in more
efficient tree structures. In addition to the Euclidean target space, we
present a variant which can naturally deal with a circular target space by the
proper use of circular statistics. We apply the regression forest employing our
node splitting to head pose estimation (Euclidean target space) and car
direction estimation (circular target space) and demonstrate that the proposed
method significantly outperforms state-of-the-art methods (38.5% and 22.5%
error reduction respectively).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6444</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6444</id><created>2013-12-22</created><updated>2014-02-09</updated><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author></authors><title>A note on the undercut procedure</title><categories>cs.GT</categories><comments>5 pages</comments><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The undercut procedure was presented by Brams et al. [2] as a procedure for
identifying an envy-free allocation when agents have preferences over sets of
objects. They assumed that agents have strict preferences over objects and
their preferences are extended over to sets of objects via the responsive set
extension. We point out some shortcomings of the undercut procedure. We then
simplify the undercut procedure of Brams et al. [2] and show that it works
under a more general condition where agents may express indifference between
objects and they may not necessarily have responsive preferences over sets of
objects. Finally, we show that the procedure works even if agents have unequal
claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6447</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6447</id><created>2013-12-22</created><updated>2014-06-20</updated><authors><author><keyname>Kalinowski</keyname><forenames>Thomas</forenames></author><author><keyname>Matsypura</keyname><forenames>Dmytro</forenames></author><author><keyname>Savelsbergh</keyname><forenames>Martin W. P.</forenames></author></authors><title>Incremental Network Design with Maximum Flows</title><categories>cs.DM cs.DS</categories><comments>26 pages</comments><journal-ref>European Journal of Operational Research 242 (2015), pp. 51-62</journal-ref><doi>10.1016/j.ejor.2014.10.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an incremental network design problem, where in each time period of
the planning horizon an arc can be added to the network and a maximum flow
problem is solved, and where the objective is to maximize the cumulative flow
over the entire planning horizon. After presenting two mixed integer
programming (MIP) formulations for this NP-complete problem, we describe
several heuristics and prove performance bounds for some special cases. In a
series of computational experiments, we compare the performance of the MIP
formulations as well as the heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6456</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6456</id><created>2013-12-22</created><authors><author><keyname>Mousavi</keyname><forenames>Mohammad</forenames></author><author><keyname>Glynn</keyname><forenames>Peter W.</forenames></author></authors><title>Exact Simulation of Non-stationary Reflected Brownian Motion</title><categories>math.PR cs.CE q-fin.CP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops the first method for the exact simulation of reflected
Brownian motion (RBM) with non-stationary drift and infinitesimal variance. The
running time of generating exact samples of non-stationary RBM at any time $t$
is uniformly bounded by $\mathcal{O}(1/\bar\gamma^2)$ where $\bar\gamma$ is the
average drift of the process. The method can be used as a guide for planning
simulations of complex queueing systems with non-stationary arrival rates
and/or service time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6457</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6457</id><created>2013-12-22</created><updated>2014-09-03</updated><authors><author><keyname>Wang</keyname><forenames>Pengwei</forenames></author><author><keyname>Safavi-Naini</keyname><forenames>Reihaneh</forenames></author></authors><title>A Model for Adversarial Wiretap Channel</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wiretap model of secure communication the goal is to provide (asymptotic)
perfect secrecy and reliable communication over a noisy channel that is
eavesdropped by an adversary with unlimited computational power. This goal is
achieved by taking advantage of the channel noise and without requiring a
shared key. The model has attracted attention in recent years because it
captures eavesdropping attack in wireless communication. The wiretap adversary
is a passive eavesdropping adversary at the physical layer of communication. In
this paper we propose a model for adversarial wiretap (AWTP) channel that
models active adversaries at this layer. We consider a $(\rho_r, \rho_w)$
wiretap adversary who can see a fraction $\rho_r$, and modify a fraction
$\rho_w$, of the sent codeword. The code components that are read and/or
modified can be chosen adaptively, and the subsets of read and modified
components in general, can be different. AWTP codes provide secrecy and
reliability for communication over these channels. We give security and
reliability definitions and measures for these codes, and define secrecy
capacity of an AWTP channel that represents the secrecy potential of the
channel. The paper has two main contributions. First, we prove a tight upper
bound on the rate of AWTP codes with perfect secrecy for $(\rho_r,
\rho_w)$-AWTP channels, and use the bound to derive the secrecy capacity of the
channel. We prove a similar bound for $\epsilon$-secure codes also, but in this
case the bound is not tight. Second, we give an explicit construction for a
capacity achieving AWTP code family, and prove its security and efficiency
properties. We show that AWTP model is a natural generalization of Wyner's
wiretap models and somewhat surprisingly, also provides a direct generalization
for a seemingly unrelated cryptographic primitive, Secure Message Transmission
(SMT).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6461</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6461</id><created>2013-12-22</created><updated>2014-02-19</updated><authors><author><keyname>Sonoda</keyname><forenames>Sho</forenames></author><author><keyname>Murata</keyname><forenames>Noboru</forenames></author></authors><title>Nonparametric Weight Initialization of Neural Networks via Integral
  Representation</title><categories>cs.LG cs.NE</categories><comments>For ICLR2014, revised into 9 pages; revised into 12 pages (with
  supplements)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new initialization method for hidden parameters in a neural network is
proposed. Derived from the integral representation of the neural network, a
nonparametric probability distribution of hidden parameters is introduced. In
this proposal, hidden parameters are initialized by samples drawn from this
distribution, and output parameters are fitted by ordinary linear regression.
Numerical experiments show that backpropagation with proposed initialization
converges faster than uniformly random initialization. Also it is shown that
the proposed method achieves enough accuracy by itself without backpropagation
in some cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6468</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6468</id><created>2013-12-23</created><updated>2014-07-12</updated><authors><author><keyname>Takaguchi</keyname><forenames>Taro</forenames></author><author><keyname>Hasegawa</keyname><forenames>Takehisa</forenames></author><author><keyname>Yoshida</keyname><forenames>Yuichi</forenames></author></authors><title>Suppressing epidemics on networks by exploiting observer nodes</title><categories>physics.soc-ph cs.SI</categories><comments>14 pages, 6 figures</comments><journal-ref>Phys. Rev. E 90, 012807 (2014)</journal-ref><doi>10.1103/PhysRevE.90.012807</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To control infection spreading on networks, we investigate the effect of
observer nodes that recognize infection in a neighboring node and make the rest
of the neighbor nodes immune. We numerically show that random placement of
observer nodes works better on networks with clustering than on locally
treelike networks, implying that our model is promising for realistic social
networks. The efficiency of several heuristic schemes for observer placement is
also examined for synthetic and empirical networks. In parallel with numerical
simulations of epidemic dynamics, we also show that the effect of observer
placement can be assessed by the size of the largest connected component of
networks remaining after removing observer nodes and links between their
neighboring nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6485</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6485</id><created>2013-12-23</created><authors><author><keyname>Li</keyname><forenames>Zheng</forenames></author><author><keyname>Liang</keyname><forenames>Mingfei</forenames></author><author><keyname>O'Brien</keyname><forenames>Liam</forenames></author><author><keyname>Zhang</keyname><forenames>He</forenames></author></authors><title>The Cloud's Cloudy Moment: A Systematic Survey of Public Cloud Service
  Outage</title><categories>cs.DC</categories><comments>11 pages</comments><journal-ref>International Journal of Cloud Computing and Services Science
  (IJ-CLOSER), vol. 2, no. 5, 2013</journal-ref><doi>10.11591/closer.v2i5.5125</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inadequate service availability is the top concern when employing Cloud
computing. It has been recognized that zero downtime is impossible for
large-scale Internet services. By learning from the previous and others'
mistakes, nevertheless, it is possible for Cloud vendors to minimize the risk
of future downtime or at least keep the downtime short. To facilitate
summarizing lessons for Cloud providers, we performed a systematic survey of
public Cloud service outage events. This paper reports the result of this
survey. In addition to a set of findings, our work generated a lessons
framework by classifying the outage root causes. The framework can in turn be
used to arrange outage lessons for reference by Cloud providers. By including
potentially new root causes, this lessons framework will be smoothly expanded
in our future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6488</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6488</id><created>2013-12-23</created><authors><author><keyname>Li</keyname><forenames>Zheng</forenames></author><author><keyname>O'Brien</keyname><forenames>Liam</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames></author><author><keyname>Zhang</keyname><forenames>Miranda</forenames></author></authors><title>Early Observations on Performance of Google Compute Engine for
  Scientific Computing</title><categories>cs.DC cs.MS cs.NA</categories><comments>Proceedings of the 5th International Conference on Cloud Computing
  Technologies and Science (CloudCom 2013), pp. 1-8, Bristol, UK, December 2-5,
  2013</comments><doi>10.1109/CloudCom.2013.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although Cloud computing emerged for business applications in industry,
public Cloud services have been widely accepted and encouraged for scientific
computing in academia. The recently available Google Compute Engine (GCE) is
claimed to support high-performance and computationally intensive tasks, while
little evaluation studies can be found to reveal GCE's scientific capabilities.
Considering that fundamental performance benchmarking is the strategy of
early-stage evaluation of new Cloud services, we followed the Cloud Evaluation
Experiment Methodology (CEEM) to benchmark GCE and also compare it with Amazon
EC2, to help understand the elementary capability of GCE for dealing with
scientific problems. The experimental results and analyses show both potential
advantages of, and possible threats to applying GCE to scientific computing.
For example, compared to Amazon's EC2 service, GCE may better suit applications
that require frequent disk operations, while it may not be ready yet for single
VM-based parallel computing. Following the same evaluation methodology,
different evaluators can replicate and/or supplement this fundamental
evaluation of GCE. Based on the fundamental evaluation results, suitable GCE
environments can be further established for case studies of solving real
science problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6490</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6490</id><created>2013-12-23</created><updated>2014-05-29</updated><authors><author><keyname>Csirmaz</keyname><forenames>Laszlo</forenames></author></authors><title>Book inequalities</title><categories>cs.IT math.IT</categories><msc-class>05B35, 26A12, 52B12, 90C29, 94A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information theoretical inequalities have strong ties with polymatroids and
their representability. A polymatroid is entropic if its rank function is given
by the Shannon entropy of the subsets of some discrete random variables. The
book is a special iterated adhesive extension of a polymatroid with the
property that entropic polymatroids have $n$-page book extensions over an
arbitrary spine. We prove that every polymatroid has an $n$-page book extension
over a single element and over an all-but-one-element spine. Consequently, for
polymatroids on four elements, only book extensions over a two-element spine
should be considered. F. Mat\'{u}\v{s} proved that the Zhang-Yeung inequalities
characterize polymatroids on four elements which have such a 2-page book
extension. The $n$-page book inequalities, defined in this paper, are
conjectured to characterize polymatroids on four elements which have $n$-page
book extensions over a two-element spine. We prove that the condition is
necessary; consequently every book inequality is an information inequality on
four random variables. Using computer-aided multiobjective optimization, the
sufficiency of the condition is verified up to 9-page book extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6492</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6492</id><created>2013-12-23</created><authors><author><keyname>Tamta</keyname><forenames>Pawan</forenames></author><author><keyname>Pande</keyname><forenames>Bhagwati Prasad</forenames></author><author><keyname>Dhami</keyname><forenames>H. S.</forenames></author></authors><title>Cardinality Maximum Flow Network Interdiction Problem Vs. The Clique
  Problem</title><categories>math.OC cs.DS</categories><comments>10 pages,3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cardinality Maximum Flow Network Interdiction Problem (CMFNIP) is known to be
strongly NP-hard problem in the literature. A particular case of CMFNIP has
been shown to have reduction from clique problem. In the present work,an effort
is being made to solve this particular case of CMFNIP in polynomial time.
Direct implication of this solution is that the clique problem gets solved in
polynomial time. 3-CNF Satisfiability and Vertex Cover problems, having
reductions to and from the Clique Problem respectively, are also being solved
in polynomial time by same algorithm. The obvious conclusion of the work is P =
NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6493</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6493</id><created>2013-12-23</created><updated>2014-04-02</updated><authors><author><keyname>Mastrolilli</keyname><forenames>Monaldo</forenames></author></authors><title>The Lasserre Hierarchy in Almost Diagonal Form</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Lasserre hierarchy is a systematic procedure for constructing a sequence
of increasingly tight relaxations that capture the convex formulations used in
the best available approximation algorithms for a wide variety of optimization
problems. Despite the increasing interest, there are very few techniques for
analyzing Lasserre integrality gaps. Satisfying the positive semi-definite
requirement is one of the major hurdles to constructing Lasserre gap examples.
  We present a novel characterization of the Lasserre hierarchy based on moment
matrices that differ from diagonal ones by matrices of rank one (almost
diagonal form). We provide a modular recipe to obtain positive semi-definite
feasibility conditions by iteratively diagonalizing rank one matrices.
  Using this, we prove strong lower bounds on integrality gaps of Lasserre
hierarchy for two basic capacitated covering problems. For the min-knapsack
problem, we show that the integrality gap remains arbitrarily large even at
level $n-1$ of Lasserre hierarchy. For the min-sum of tardy jobs scheduling
problem, we show that the integrality gap is unbounded at level
$\Omega(\sqrt{n})$ (even when the objective function is integrated as a
constraint). These bounds are interesting on their own, since both problems
admit FPTAS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6494</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6494</id><created>2013-12-23</created><updated>2014-09-15</updated><authors><author><keyname>Lipowski</keyname><forenames>Adam</forenames></author><author><keyname>Lipowska</keyname><forenames>Dorota</forenames></author></authors><title>Generic criticality of community structure in random graphs</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>4 pages, non-greedy optimization added, Phys. Rev. E (accepted)</comments><journal-ref>Phys. Rev. E 90, 032815 (2014)</journal-ref><doi>10.1103/PhysRevE.90.032815</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine a community structure in random graphs of size $n$ and link
probability $p/n$ determined with the Newman greedy optimization of modularity.
Calculations show that for $p&lt;1$ communities are nearly identical with
clusters. For $p=1$ the average sizes of a community $s_{av}$ and of the giant
community $s_g$ show a power-law increase $s_{av}\sim n^{\alpha'}$ and $s_g\sim
n^{\alpha}$. From numerical results we estimate $\alpha'\approx 0.26(1)$,
$\alpha\approx 0.50(1)$, and using the probability distribution of sizes of
communities we suggest that $\alpha'=\alpha/2$ should hold. For $p&gt;1$ the
community structure remains critical: (i) $s_{av}$ and $s_g$ have a power law
increase with $\alpha'\approx\alpha &lt;1$; (ii) the probability distribution of
sizes of communities is very broad and nearly flat for all sizes up to $s_g$.
For large $p$ the modularity $Q$ decays as $Q\sim p^{-0.55}$, which is
intermediate between some previous estimations. To check the validity of the
results, we also determined the community structure using another method,
namely a non-greedy optimization of modularity. Tests with some benchmark
networks show that the method outperforms the greedy version. For random
graphs, however, the characteristics of the community structure determined
using both greedy an non-greedy optimizations are, within small statistical
fluctuations, the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6497</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6497</id><created>2013-12-23</created><authors><author><keyname>Estrela</keyname><forenames>Vania V.</forenames></author><author><keyname>Coelho</keyname><forenames>Alessandra M.</forenames></author></authors><title>State-of-the Art Motion Estimation in the Context of 3D TV</title><categories>cs.MM</categories><journal-ref>Multimedia Networking and Coding. IGI Global, 2013. 148-173. Web.
  23 Dec. 2013</journal-ref><doi>10.4018/978-1-4666-2660-7.ch006</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Progress in image sensors and computation power has fueled studies to improve
acquisition, processing, and analysis of 3D streams along with 3D
scenes/objects reconstruction. The role of motion compensation/motion
estimation (MCME) in 3D TV from end-to-end user is investigated in this
chapter. Motion vectors (MVs) are closely related to the concept of
disparities, and they can help improving dynamic scene acquisition, content
creation, 2D to 3D conversion, compression coding, decompression/decoding,
scene rendering, error concealment, virtual/augmented reality handling,
intelligent content retrieval, and displaying. Although there are different 3D
shape extraction methods, this chapter focuses mostly on shape-from-motion
(SfM) techniques due to their relevance to 3D TV. SfM extraction can restore 3D
shape information from a single camera data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6501</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6501</id><created>2013-12-23</created><authors><author><keyname>Sandholm</keyname><forenames>Thomas</forenames></author><author><keyname>Magnusson</keyname><forenames>Boris</forenames></author><author><keyname>Johnsson</keyname><forenames>Bjorn A.</forenames></author></authors><title>On-Demand WebRTC Tunneling in Restricted Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the implementation of a WebRTC gateway service that
can forward ad-hoc RTP data plane traffic from a browser on one local network
to a browser on another local network. The advantage compared to the existing
IETF STUN (RFC 5389), TURN (RFC 5766) and ICE (RFC 5245) protocols is that it
does not require a public host and port mapping for each participating local
host, and it works with more restrictive firewall policies. WebRTC implements
ICE which combines STUN and TURN probes to automatically find the best
connection between two peers who want to communicate. In corporate networks,
simple hole punching and NAT traversal techniques typically do not work, e.g.
because of symmetric NATs. Dynamic allocation of ports on an external 3rd party
relay service is also typically blocked on restricted hosts. In our use case,
doctors at hospitals can only access port 80 through the hospital firewall on
external machines, and they need to communicate with patients who are typically
behind a NAT in a local WiFi network. VPN solutions only work for staff but not
between patients and staff. Our solution solves this problem by redirecting all
WebRTC traffic through a gateway service on the local network that has a secure
tunnel established with a public gateway. The public gateway redirects traffic
from multiple concurrent streams securely between local gateway services that
connect to it. The local gateways also communicate with browsers on their local
network to mimic a direct browser-to-browser connection without having to
change the browser runtime. We have demonstrated that this technique works well
within the hospital network and arbitrary patient networks, without the need
for any individual host configuration. In our evaluation we show that the
latency overhead is 18-20 ms for each concurrent stream added to the same
gateway service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6503</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6503</id><created>2013-12-23</created><updated>2014-05-19</updated><authors><author><keyname>Gastineau</keyname><forenames>Nicolas</forenames><affiliation>Le2i, LIRIS</affiliation></author><author><keyname>Kheddouci</keyname><forenames>Hamamache</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Togni</keyname><forenames>Olivier</forenames><affiliation>Le2i</affiliation></author></authors><title>On the family of $r$-regular graphs with Grundy number $r+1$</title><categories>cs.DM math.CO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Grundy number of a graph $G$, denoted by $\Gamma(G)$, is the largest $k$
such that there exists a partition of $V(G)$, into $k$ independent sets
$V_1,\ldots, V_k$ and every vertex of $V_i$ is adjacent to at least one vertex
in $V_j$, for every $j &lt; i$. The objects which are studied in this article are
families of $r$-regular graphs such that $\Gamma(G) = r + 1$. Using the notion
of independent module, a characterization of this family is given for $r=3$.
Moreover, we determine classes of graphs in this family, in particular the
class of $r$-regular graphs without induced $C_4$, for $r \le 4$. Furthermore,
our propositions imply results on partial Grundy number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6506</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6506</id><created>2013-12-23</created><updated>2013-12-25</updated><authors><author><keyname>Singhal</keyname><forenames>Prateek</forenames></author><author><keyname>Deshpande</keyname><forenames>Aditya</forenames></author><author><keyname>Reddy</keyname><forenames>N Dinesh</forenames></author><author><keyname>Krishna</keyname><forenames>K Madhava</forenames></author></authors><title>Top Down Approach to Multiple Plane Detection</title><categories>cs.CV</categories><comments>6 pages, 22 figures, ICPR conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting multiple planes in images is a challenging problem, but one with
many applications. Recent work such as J-Linkage and Ordered Residual Kernels
have focussed on developing a domain independent approach to detect multiple
structures. These multiple structure detection methods are then used for
estimating multiple homographies given feature matches between two images.
Features participating in the multiple homographies detected, provide us the
multiple scene planes. We show that these methods provide locally optimal
results and fail to merge detected planar patches to the true scene planes.
These methods use only residues obtained on applying homography of one plane to
another as cue for merging. In this paper, we develop additional cues such as
local consistency of planes, local normals, texture etc. to perform better
classification and merging . We formulate the classification as an MRF problem
and use TRWS message passing algorithm to solve non metric energy terms and
complex sparse graph structure. We show results on challenging dataset common
in robotics navigation scenarios where our method shows accuracy of more than
85 percent on average while being close or same as the actual number of scene
planes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6532</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6532</id><created>2013-12-23</created><authors><author><keyname>Dupressoir</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Gordon</keyname><forenames>Andrew D.</forenames></author><author><keyname>J&#xfc;rjens</keyname><forenames>Jan</forenames></author><author><keyname>Naumann</keyname><forenames>David A.</forenames></author></authors><title>Guiding a General-Purpose C Verifier to Prove Cryptographic Protocols</title><categories>cs.CR</categories><comments>To appear in Journal of Computer Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe how to verify security properties of C code for cryptographic
protocols by using a general-purpose verifier. We prove security theorems in
the symbolic model of cryptography. Our techniques include: use of ghost state
to attach formal algebraic terms to concrete byte arrays and to detect
collisions when two distinct terms map to the same byte array; decoration of a
crypto API with contracts based on symbolic terms; and expression of the
attacker model in terms of C programs. We rely on the general-purpose verifier
VCC; we guide VCC to prove security simply by writing suitable header files and
annotations in implementation files, rather than by changing VCC itself. We
formalize the symbolic model in Coq in order to justify the addition of axioms
to VCC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6533</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6533</id><created>2013-12-23</created><updated>2014-09-23</updated><authors><author><keyname>Pham</keyname><forenames>Quang-Cuong</forenames></author></authors><title>A General, Fast, and Robust Implementation of the Time-Optimal Path
  Parameterization Algorithm</title><categories>cs.RO</categories><comments>7 pages, 5 figures, 4 tables</comments><doi>10.1109/TRO.2014.2351113</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the Time-Optimal Parameterization of a given Path (TOPP) subject to
kinodynamic constraints is an essential component in many robotic theories and
applications. The objective of this article is to provide a general, fast and
robust implementation of this component. For this, we give a complete solution
to the issue of dynamic singularities, which are the main cause of failure in
existing implementations. We then present an open-source implementation of the
algorithm in C++/Python and demonstrate its robustness and speed in various
robotics settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6546</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6546</id><created>2013-12-23</created><updated>2015-06-17</updated><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Mackenzie</keyname><forenames>Simon</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Fair assignment of indivisible objects under ordinal preferences</title><categories>cs.GT cs.AI</categories><comments>extended version of a paper presented at AAMAS 2014</comments><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the discrete assignment problem in which agents express ordinal
preferences over objects and these objects are allocated to the agents in a
fair manner. We use the stochastic dominance relation between fractional or
randomized allocations to systematically define varying notions of
proportionality and envy-freeness for discrete assignments. The computational
complexity of checking whether a fair assignment exists is studied for these
fairness notions. We also characterize the conditions under which a fair
assignment is guaranteed to exist. For a number of fairness concepts,
polynomial-time algorithms are presented to check whether a fair assignment
exists. Our algorithmic results also extend to the case of unequal entitlements
of agents. Our NP-hardness result, which holds for several variants of
envy-freeness, answers an open question posed by Bouveret, Endriss, and Lang
(ECAI 2010). We also propose fairness concepts that always suggest a non-empty
set of assignments with meaningful fairness properties. Among these concepts,
optimal proportionality and optimal weak proportionality appear to be desirable
fairness concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6547</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6547</id><created>2013-12-23</created><authors><author><keyname>Anthony</keyname><forenames>Eleanor</forenames></author><author><keyname>Grant</keyname><forenames>Sheridan</forenames></author><author><keyname>Gritzmann</keyname><forenames>Peter</forenames></author><author><keyname>Rojas</keyname><forenames>J. Maurice</forenames></author></authors><title>Polynomial-Time Amoeba Neighborhood Membership and Faster Localized
  Solving</title><categories>math.AG cs.CC math.OC</categories><comments>15 pages, 9 figures. Submitted to a conference proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive efficient algorithms for coarse approximation of algebraic
hypersurfaces, useful for estimating the distance between an input polynomial
zero set and a given query point. Our methods work best on sparse polynomials
of high degree (in any number of variables) but are nevertheless completely
general. The underlying ideas, which we take the time to describe in an
elementary way, come from tropical geometry. We thus reduce a hard algebraic
problem to high-precision linear optimization, proving new upper and lower
complexity estimates along the way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6549</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6549</id><created>2013-12-23</created><authors><author><keyname>Esposito</keyname><forenames>Corbo Antonio</forenames></author><author><keyname>Fabiola</keyname><forenames>Didone</forenames></author></authors><title>Some considerations about Java implementation of two provably secure
  pseudorandom bit generators</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The quest for a cryptographically secure pseudorandom bit generator (PRBG)
was initiated long ago, and for a long time the proposed pseudorandom
generators were very slow. More recently some &quot;provably secure&quot; PRBG capable to
achieve a throughput rate greater than 1Mbit/sec. We noticed, anyway, the
absence of Java implementations of such PRBGs, provably due to poor expected
values for throughput rate. In the present paper we show that it is quite easy
to write down Java implementations for them, achieving a throughput rae into
range $0,5\div 7$ Mbit/sec on very common mobile low-end devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6550</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6550</id><created>2013-12-23</created><updated>2014-07-22</updated><authors><author><keyname>Byrka</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Fleszar</keyname><forenames>Krzysztof</forenames></author><author><keyname>Rybicki</keyname><forenames>Bartosz</forenames></author><author><keyname>Spoerhase</keyname><forenames>Joachim</forenames></author></authors><title>Bi-Factor Approximation Algorithms for Hard Capacitated $k$-Median
  Problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the classical $k$-median problem the goal is to select a subset of at most
$k$ facilities in order to minimize the total cost of opened facilities and
established connections between clients and opened facilities. We consider the
capacitated version of the problem, where a single facility may only serve a
limited number of clients. We construct approximation algorithms slightly
violating the capacities based on rounding a fractional solution to the
standard LP.
  It is well known that the standard LP has unbounded integrality gap if we
only allow violating capacities by a factor smaller than $2$, or if we only
allow violating the number of facilities by a factor smaller than $2$. In an
earlier version of our work we showed that violating capacities by a factor of
$2+\varepsilon$ is sufficient to obtain constant factor approximation of the
connection cost. In this paper we substantially extend this result in the
following two directions. First, we extend the $2+\varepsilon$ capacity
violating algorithm to the more general $k$-facility location problem with
uniform capacities, where opening facilities incurs a location specific opening
cost. Second, we show that violating capacities by a slightly bigger factor of
$3+\varepsilon$ is sufficient to obtain constant factor approximation of the
connection cost also in the case of the non-uniform hard capacitated $k$-median
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6552</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6552</id><created>2013-12-23</created><authors><author><keyname>Xia</keyname><forenames>Feng</forenames></author><author><keyname>Liu</keyname><forenames>Li</forenames></author><author><keyname>Li</keyname><forenames>Jie</forenames></author><author><keyname>Ma</keyname><forenames>Jianhua</forenames></author><author><keyname>Vasilakos</keyname><forenames>Athanasios V.</forenames></author></authors><title>Socially-Aware Networking: A Survey</title><categories>cs.SI cs.NI physics.soc-ph</categories><comments>accepted. IEEE Systems Journal, 2013</comments><msc-class>68M14</msc-class><acm-class>C.2; K.4</acm-class><doi>10.1109/JSYST.2013.2281262</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The widespread proliferation of handheld devices enables mobile carriers to
be connected at anytime and anywhere. Meanwhile, the mobility patterns of
mobile devices strongly depend on the users' movements, which are closely
related to their social relationships and behaviors. Consequently, today's
mobile networks are becoming increasingly human centric. This leads to the
emergence of a new field which we call socially-aware networking (SAN). One of
the major features of SAN is that social awareness becomes indispensable
information for the design of networking solutions. This emerging paradigm is
applicable to various types of networks (e.g. opportunistic networks, mobile
social networks, delay tolerant networks, ad hoc networks, etc) where the users
have social relationships and interactions. By exploiting social properties of
nodes, SAN can provide better networking support to innovative applications and
services. In addition, it facilitates the convergence of human society and
cyber physical systems. In this paper, for the first time, to the best of our
knowledge, we present a survey of this emerging field. Basic concepts of SAN
are introduced. We intend to generalize the widely-used social properties in
this regard. The state-of-the-art research on SAN is reviewed with focus on
three aspects: routing and forwarding, incentive mechanisms and data
dissemination. Some important open issues with respect to mobile social sensing
and learning, privacy, node selfishness and scalability are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6558</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6558</id><created>2013-12-23</created><authors><author><keyname>Zliobaite</keyname><forenames>Indre</forenames></author><author><keyname>Pechenizkiy</keyname><forenames>Mykola</forenames></author></authors><title>Predictive User Modeling with Actionable Attributes</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different machine learning techniques have been proposed and used for
modeling individual and group user needs, interests and preferences. In the
traditional predictive modeling instances are described by observable
variables, called attributes. The goal is to learn a model for predicting the
target variable for unseen instances. For example, for marketing purposes a
company consider profiling a new user based on her observed web browsing
behavior, referral keywords or other relevant information. In many real world
applications the values of some attributes are not only observable, but can be
actively decided by a decision maker. Furthermore, in some of such applications
the decision maker is interested not only to generate accurate predictions, but
to maximize the probability of the desired outcome. For example, a direct
marketing manager can choose which type of a special offer to send to a client
(actionable attribute), hoping that the right choice will result in a positive
response with a higher probability. We study how to learn to choose the value
of an actionable attribute in order to maximize the probability of a desired
outcome in predictive modeling. We emphasize that not all instances are equally
sensitive to changes in actions. Accurate choice of an action is critical for
those instances, which are on the borderline (e.g. users who do not have a
strong opinion one way or the other). We formulate three supervised learning
approaches for learning to select the value of an actionable attribute at an
instance level. We also introduce a focused training procedure which puts more
emphasis on the situations where varying the action is the most likely to take
the effect. The proof of concept experimental validation on two real-world case
studies in web analytics and e-learning domains highlights the potential of the
proposed approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6564</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6564</id><created>2013-12-23</created><authors><author><keyname>Esposito</keyname><forenames>Corbo Antonio</forenames></author><author><keyname>Fabiola</keyname><forenames>Didone</forenames></author></authors><title>A proposal of a faster variant of known provably secure PRBGs</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We make a new proposal about how to use in an effective way a CSPRBG
(Computationally Secure Pseudo Random Bit Generator) for cryptographic
purposes. We introduce the definitions of TCSPRBG (Typical CSPRBG) and SCSPRBG
(Special CSPRBG). In particular the definition of SCSPRBG synthetizes in a
simple way our proposal of how to modify a CSPRBG in order to achieve a higher
throughput rate, while retaining some essential features of its computational
security.
  We then summarize which should be, in our opinion, a &quot;standard way&quot; to use a
CSPRBG for cryptographic purposes. We eventually present as an application, a
particular SCSPRBG for which we can achieve throughput rates greater than $100$
Mbits/sec on current mobile devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6565</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6565</id><created>2013-12-23</created><authors><author><keyname>Xia</keyname><forenames>Feng</forenames></author><author><keyname>Asabere</keyname><forenames>Nana Yaw</forenames></author><author><keyname>Ahmed</keyname><forenames>Ahmedin Mohammed</forenames></author><author><keyname>Li</keyname><forenames>Jing</forenames></author><author><keyname>Kong</keyname><forenames>Xiangjie</forenames></author></authors><title>Mobile Multimedia Recommendation in Smart Communities: A Survey</title><categories>cs.IR cs.MM</categories><msc-class>68M14</msc-class><acm-class>K.4; H.3</acm-class><journal-ref>IEEE Access, vol.1, pp.606-624, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the rapid growth of internet broadband access and proliferation of
modern mobile devices, various types of multimedia (e.g. text, images, audios
and videos) have become ubiquitously available anytime. Mobile device users
usually store and use multimedia contents based on their personal interests and
preferences. Mobile device challenges such as storage limitation have however
introduced the problem of mobile multimedia overload to users. In order to
tackle this problem, researchers have developed various techniques that
recommend multimedia for mobile users. In this survey paper, we examine the
importance of mobile multimedia recommendation systems from the perspective of
three smart communities, namely, mobile social learning, mobile event guide and
context-aware services. A cautious analysis of existing research reveals that
the implementation of proactive, sensor-based and hybrid recommender systems
can improve mobile multimedia recommendations. Nevertheless, there are still
challenges and open issues such as the incorporation of context and social
properties, which need to be tackled in order to generate accurate and
trustworthy mobile multimedia recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6568</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6568</id><created>2013-12-23</created><authors><author><keyname>Komendantskaya</keyname><forenames>Ekaterina</forenames></author><author><keyname>Power</keyname><forenames>John</forenames></author><author><keyname>Schmidt</keyname><forenames>Martin</forenames></author></authors><title>Coalgebraic Logic Programming: from Semantics to Implementation</title><categories>cs.PL</categories><comments>Accepted for publication in Journal of Logic and Computation, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coinductive definitions, such as that of an infinite stream, may often be
described by elegant logic programs, but ones for which SLD-refutation is of no
value as SLD-derivations fall into infinite loops. Such definitions give rise
to questions of lazy corecursive derivations and parallelism, as execution of
such logic programs can have both recursive and corecursive features at once.
Observational and coalgebraic semantics have been used to study them
abstractly. The programming developments have often occurred separately and
have usually been implementation-led. Here, we give a coherent semantics-led
account of the issues, starting with abstract category theoretic semantics,
developing coalgebra to characterise naturally arising trees, and proceeding
towards implementation of a new dialect, CoALP, of logic programming,
characterised by guarded lazy corecursion and parallelism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6573</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6573</id><created>2013-12-19</created><authors><author><keyname>Klein</keyname><forenames>Kyle</forenames></author><author><keyname>Suri</keyname><forenames>Subhash</forenames></author></authors><title>Trackability with Imprecise Localization</title><categories>cs.RO cs.SY</categories><comments>17 pages, 9 figures</comments><acm-class>I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Imagine a tracking agent $P$ who wants to follow a moving target $Q$ in
$d$-dimensional Euclidean space. The tracker has access to a noisy location
sensor that reports an estimate $\tilde{Q}(t)$ of the target's true location
$Q(t)$ at time $t$, where $||Q(T) - \tilde{Q}(T)||$ represents the sensor's
localization error. We study the limits of tracking performance under this kind
of sensing imprecision. In particular, we investigate (1) what is $P$'s best
strategy to follow $Q$ if both $P$ and $Q$ can move with equal speed, (2) at
what rate does the distance $||Q(t) - P(t)||$ grow under worst-case
localization noise, (3) if $P$ wants to keep $Q$ within a prescribed distance
$L$, how much faster does it need to move, and (4) what is the effect of
obstacles on the tracking performance, etc. Under a relative error model of
noise, we are able to give upper and lower bounds for the worst-case tracking
performance, both with or without obstacles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6582</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6582</id><created>2013-12-23</created><updated>2014-09-04</updated><authors><author><keyname>Basu</keyname><forenames>Saugata</forenames></author><author><keyname>Riener</keyname><forenames>Cordian</forenames></author></authors><title>Bounding the equivariant Betti numbers and computing the generalized
  Euler-Poincar\'e characteristic of symmetric semi-algebraic sets</title><categories>math.AG cs.CC math.AT</categories><comments>55 pages, 1 figure. Minor revisions</comments><msc-class>14P10, 14P25, 14Q20, 68Q15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathbf{R}$ be a real closed field. The problem of obtaining tight
bounds on the Betti numbers of semi-algebraic subsets of $\mathbf{R}^k$ in
terms of the number and degrees of the defining polynomials has been an
important problem in real algebraic geometry with the first results due to
Ole{\u\i}nik and Petrovski{\u\i}, Thom and Milnor. These bounds are all
exponential in the number of variables $k$. Motivated by several applications
in real algebraic geometry, as well as in theoretical computer science, where
such bounds have found applications, we consider in this paper the problem of
bounding the \emph{equivariant} Betti numbers of symmetric algebraic and
semi-algebraic subsets of $\mathbf{R}^k$. We obtain several asymptotically
tight upper bounds. In particular, we prove that if $S\subset \mathbf{R}^k$ is
a semi-algebraic subset defined by a finite set of $s$ symmetric polynomials of
degree at most $d$, then the sum of the $\mathfrak{S}_k$-equivariant Betti
numbers of $S$ with coefficients in $\mathbb{Q}$ is bounded by $(skd)^{O(d)}$.
As an application we improve the best known bound on the ordinary Betti numbers
of the projection of a compact semi-algebraic set improving for any fixed
degree the best previously known bound for this problem due to Gabrielov,
Vorobjov and Zell. As another application of our methods we obtain polynomial
time (for fixed degrees) algorithms for computing the generalized
Euler-Poincar{\'e} characteristic of semi-algebraic sets defined by symmetric
polynomials. This is in contrast to the best complexity of the known algorithms
for the same problem in the non-symmetric situation, which is singly
exponential. This singly exponential complexity for the latter problem is
unlikely to be improved because of hardness result ($\#\mathbf{P}$-hardness)
coming from discrete complexity theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6584</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6584</id><created>2013-12-23</created><authors><author><keyname>Giles</keyname><forenames>Brett</forenames></author><author><keyname>Selinger</keyname><forenames>Peter</forenames></author></authors><title>Remarks on Matsumoto and Amano's normal form for single-qubit Clifford+T
  operators</title><categories>quant-ph cs.ET</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matsumoto and Amano (2008) showed that every single-qubit Clifford+T operator
can be uniquely written of a particular form, which we call the Matsumoto-Amano
normal form. In this mostly expository paper, we give a detailed and
streamlined presentation of Matsumoto and Amano's results, simplifying some
proofs along the way. We also point out some corollaries to Matsumoto and
Amano's work, including an intrinsic characterization of the Clifford+T
subgroup of SO(3), which also yields an efficient T-optimal exact single-qubit
synthesis algorithm. Interestingly, this also gives an alternative proof of
Kliuchnikov, Maslov, and Mosca's exact synthesis result for the Clifford+T
subgroup of U(2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6585</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6585</id><created>2013-12-23</created><updated>2014-11-20</updated><authors><author><keyname>Garnero</keyname><forenames>Valentin</forenames></author><author><keyname>Paul</keyname><forenames>Christophe</forenames></author><author><keyname>Sau</keyname><forenames>Ignasi</forenames></author><author><keyname>Thilikos</keyname><forenames>Dimitrios M.</forenames></author></authors><title>Explicit linear kernels via dynamic programming</title><categories>cs.DS cs.DM math.CO</categories><comments>32 pages</comments><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several algorithmic meta-theorems on kernelization have appeared in the last
years, starting with the result of Bodlaender et al. [FOCS 2009] on graphs of
bounded genus, then generalized by Fomin et al. [SODA 2010] to graphs excluding
a fixed minor, and by Kim et al. [ICALP 2013] to graphs excluding a fixed
topological minor. Typically, these results guarantee the existence of linear
or polynomial kernels on sparse graph classes for problems satisfying some
generic conditions but, mainly due to their generality, it is not clear how to
derive from them constructive kernels with explicit constants. In this paper we
make a step toward a fully constructive meta-kernelization theory on sparse
graphs. Our approach is based on a more explicit protrusion replacement
machinery that, instead of expressibility in CMSO logic, uses dynamic
programming, which allows us to find an explicit upper bound on the size of the
derived kernels. We demonstrate the usefulness of our techniques by providing
the first explicit linear kernels for $r$-Dominating Set and $r$-Scattered Set
on apex-minor-free graphs, and for Planar-\mathcal{F}-Deletion on graphs
excluding a fixed (topological) minor in the case where all the graphs in
\mathcal{F} are connected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6594</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6594</id><created>2013-12-20</created><updated>2014-02-11</updated><authors><author><keyname>Dulac-Arnold</keyname><forenames>Gabriel</forenames></author><author><keyname>Denoyer</keyname><forenames>Ludovic</forenames></author><author><keyname>Thome</keyname><forenames>Nicolas</forenames></author><author><keyname>Cord</keyname><forenames>Matthieu</forenames></author><author><keyname>Gallinari</keyname><forenames>Patrick</forenames></author></authors><title>Sequentially Generated Instance-Dependent Image Representations for
  Classification</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate a new framework for image classification that
adaptively generates spatial representations. Our strategy is based on a
sequential process that learns to explore the different regions of any image in
order to infer its category. In particular, the choice of regions is specific
to each image, directed by the actual content of previously selected
regions.The capacity of the system to handle incomplete image information as
well as its adaptive region selection allow the system to perform well in
budgeted classification tasks by exploiting a dynamicly generated
representation of each image. We demonstrate the system's abilities in a series
of image-based exploration and classification tasks that highlight its learned
exploration and inference abilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6597</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6597</id><created>2013-12-23</created><updated>2014-01-24</updated><authors><author><keyname>Marujo</keyname><forenames>Luis</forenames></author><author><keyname>Gershman</keyname><forenames>Anatole</forenames></author><author><keyname>Carbonell</keyname><forenames>Jaime</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author><author><keyname>Neto</keyname><forenames>Jo&#xe3;o P.</forenames></author></authors><title>Co-Multistage of Multiple Classifiers for Imbalanced Multiclass Learning</title><categories>cs.LG cs.IR</categories><comments>Preliminary version of the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose two stochastic architectural models (CMC and CMC-M)
with two layers of classifiers applicable to datasets with one and multiple
skewed classes. This distinction becomes important when the datasets have a
large number of classes. Therefore, we present a novel solution to imbalanced
multiclass learning with several skewed majority classes, which improves
minority classes identification. This fact is particularly important for text
classification tasks, such as event detection. Our models combined with
pre-processing sampling techniques improved the classification results on six
well-known datasets. Finally, we have also introduced a new metric SG-Mean to
overcome the multiplication by zero limitation of G-Mean.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6599</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6599</id><created>2013-12-23</created><authors><author><keyname>Modi</keyname><forenames>Shatrughan</forenames></author><author><keyname>Bawa</keyname><forenames>Dr. Seema</forenames></author></authors><title>Image Processing based Systems and Techniques for the Recognition of
  Ancient and Modern Coins</title><categories>cs.CV cs.AI</categories><comments>5 pages, 1 table, Published with International Journal of Computer
  Applications (IJCA)</comments><journal-ref>International Journal of Computer Applications 47(10):1-5, June
  2012</journal-ref><doi>10.5120/7221-0041</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coins are frequently used in everyday life at various places like in banks,
grocery stores, supermarkets, automated weighing machines, vending machines
etc. So, there is a basic need to automate the counting and sorting of coins.
For this machines need to recognize the coins very fast and accurately, as
further transaction processing depends on this recognition. Three types of
systems are available in the market: Mechanical method based systems,
Electromagnetic method based systems and Image processing based systems. This
paper presents an overview of available systems and techniques based on image
processing to recognize ancient and modern coins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6606</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6606</id><created>2013-12-19</created><authors><author><keyname>Ko&#xe7;</keyname><forenames>Yakup</forenames></author><author><keyname>Warnier</keyname><forenames>Martijn</forenames></author><author><keyname>Kooij</keyname><forenames>Robert E.</forenames></author><author><keyname>Brazier</keyname><forenames>Frances M. T.</forenames></author></authors><title>Structural Vulnerability Assessment of Electric Power Grids</title><categories>physics.soc-ph cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cascading failures are the typical reasons of black- outs in power grids. The
grid topology plays an important role in determining the dynamics of cascading
failures in power grids. Measures for vulnerability analysis are crucial to
assure a higher level of robustness of power grids. Metrics from Complex
Networks are widely used to investigate the grid vulnerability. Yet, these
purely topological metrics fail to capture the real behaviour of power grids.
This paper proposes a metric, the effective graph resistance, as a
vulnerability measure to de- termine the critical components in a power grid.
Differently than the existing purely topological measures, the effective graph
resistance accounts for the electrical properties of power grids such as power
flow allocation according to Kirchoff laws. To demonstrate the applicability of
the effective graph resistance, a quantitative vulnerability assessment of the
IEEE 118 buses power system is performed. The simulation results verify the
effectiveness of the effective graph resistance to identify the critical
transmission lines in a power grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6607</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6607</id><created>2013-12-23</created><authors><author><keyname>Martin</keyname><forenames>Victorin</forenames></author><author><keyname>Lasgouttes</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Furtlehner</keyname><forenames>Cyril</forenames></author></authors><title>Using Latent Binary Variables for Online Reconstruction of Large Scale
  Systems</title><categories>math.PR cs.LG stat.ML</categories><doi>10.1007/s10472-015-9470-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a probabilistic graphical model realizing a minimal encoding of
real variables dependencies based on possibly incomplete observation and an
empirical cumulative distribution function per variable. The target application
is a large scale partially observed system, like e.g. a traffic network, where
a small proportion of real valued variables are observed, and the other
variables have to be predicted. Our design objective is therefore to have good
scalability in a real-time setting. Instead of attempting to encode the
dependencies of the system directly in the description space, we propose a way
to encode them in a latent space of binary variables, reflecting a rough
perception of the observable (congested/non-congested for a traffic road). The
method relies in part on message passing algorithms, i.e. belief propagation,
but the core of the work concerns the definition of meaningful latent variables
associated to the variables of interest and their pairwise dependencies.
Numerical experiments demonstrate the applicability of the method in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6609</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6609</id><created>2013-12-23</created><authors><author><keyname>Fister</keyname><forenames>Iztok</forenames></author><author><keyname>Fister</keyname><forenames>Iztok</forenames><suffix>Jr.</suffix></author><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author><author><keyname>Brest</keyname><forenames>Janez</forenames></author></authors><title>A comprehensive review of firefly algorithms</title><categories>cs.NE</categories><journal-ref>I. Fister, I. Fister Jr., X.-S. Yang, and J. Brest, A
  comprehensive review of firefly algorithms, Swarm and Evolutionary
  Computation, vol. 13, pp. 34-46, 2013</journal-ref><doi>10.1016/j.swevo.2013.06.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The firefly algorithm has become an increasingly important tool of Swarm
Intelligence that has been applied in almost all areas of optimization, as well
as engineering practice. Many problems from various areas have been
successfully solved using the firefly algorithm and its variants. In order to
use the algorithm to solve diverse problems, the original firefly algorithm
needs to be modified or hybridized. This paper carries out a comprehensive
review of this living and evolving discipline of Swarm Intelligence, in order
to show that the firefly algorithm could be applied to every problem arising in
practice. On the other hand, it encourages new researchers and algorithm
developers to use this simple and yet very efficient algorithm for problem
solving. It often guarantees that the obtained results will meet the
expectations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6615</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6615</id><created>2013-12-23</created><authors><author><keyname>Modi</keyname><forenames>Shatrughan</forenames></author><author><keyname>Bawa</keyname><forenames>Dr. Seema</forenames></author></authors><title>Automated Coin Recognition System using ANN</title><categories>cs.CV cs.AI</categories><comments>6 pages, 11 figures, 1 table, Published with International Journal of
  Computer Applications (IJCA)</comments><journal-ref>International Journal of Computer Applications 26(4):13-18, July
  2011</journal-ref><doi>10.5120/3093-4244</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coins are integral part of our day to day life. We use coins everywhere like
grocery store, banks, buses, trains etc. So it becomes a basic need that coins
can be sorted and counted automatically. For this it is necessary that coins
can be recognized automatically. In this paper we have developed an ANN
(Artificial Neural Network) based Automated Coin Recognition System for the
recognition of Indian Coins of denomination Rs. 1, 2, 5 and 10 with rotation
invariance. We have taken images from both sides of coin. So this system is
capable of recognizing coins from both sides. Features are extracted from
images using techniques of Hough Transformation, Pattern Averaging etc. Then,
the extracted features are passed as input to a trained Neural Network. 97.74%
recognition rate has been achieved during the experiments i.e. only 2.26% miss
recognition, which is quite encouraging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6624</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6624</id><created>2013-12-23</created><updated>2014-07-09</updated><authors><author><keyname>Calvanese</keyname><forenames>Diego</forenames></author><author><keyname>Kotek</keyname><forenames>Tomer</forenames></author><author><keyname>&#x160;imkus</keyname><forenames>Mantas</forenames></author><author><keyname>Veith</keyname><forenames>Helmut</forenames></author><author><keyname>Zuleger</keyname><forenames>Florian</forenames></author></authors><title>Shape and Content: Incorporating Domain Knowledge into Shape Analysis</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The verification community has studied dynamic data structures primarily in a
bottom-up way by analyzing pointers and the shapes induced by them. Recent work
in fields such as separation logic has made significant progress in extracting
shapes from program source code. Many real world programs however manipulate
complex data whose structure and content is most naturally described by
formalisms from object oriented programming and databases. In this paper, we
look at the verification of programs with dynamic data structures from the
perspective of content representation. Our approach is based on description
logic, a widely used knowledge representation paradigm which gives a logical
underpinning for diverse modeling frameworks such as UML and ER. Technically,
we assume that we have separation logic shape invariants obtained from a shape
analysis tool, and requirements on the program data in terms of description
logic. We show that the two-variable fragment of first order logic with
counting and trees %(whose decidability was proved at LICS 2013) can be used as
a joint framework to embed suitable fragments of description logic and
separation logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6635</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6635</id><created>2013-12-23</created><authors><author><keyname>Dacres</keyname><forenames>Shana</forenames></author><author><keyname>Haddadi</keyname><forenames>Hamed</forenames></author><author><keyname>Purver</keyname><forenames>Matthew</forenames></author></authors><title>Topic and Sentiment Analysis on OSNs: a Case Study of Advertising
  Strategies on Twitter</title><categories>cs.SI physics.soc-ph</categories><acm-class>H.3.1; I.2.7</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Social media have substantially altered the way brands and businesses
advertise: Online Social Networks provide brands with more versatile and
dynamic channels for advertisement than traditional media (e.g., TV and radio).
Levels of engagement in such media are usually measured in terms of content
adoption (e.g., likes and retweets) and sentiment, around a given topic.
However, sentiment analysis and topic identification are both non-trivial
tasks.
  In this paper, using data collected from Twitter as a case study, we analyze
how engagement and sentiment in promoted content spread over a 10-day period.
We find that promoted tweets lead to higher positive sentiment than promoted
trends; although promoted trends pay off in response volume. We observe that
levels of engagement for the brand and promoted content are highest on the
first day of the campaign, and fall considerably thereafter. However, we show
that these insights depend on the use of robust machine learning and natural
language processing techniques to gather focused, relevant datasets, and to
accurately gauge sentiment, rather than relying on the simple keyword- or
frequency-based metrics sometimes used in social media research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6650</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6650</id><created>2013-12-23</created><updated>2014-02-03</updated><authors><author><keyname>Nafchi</keyname><forenames>Samaneh Kazemi</forenames></author><author><keyname>Garg</keyname><forenames>Rohan</forenames></author><author><keyname>Cooperman</keyname><forenames>Gene</forenames></author></authors><title>Transparent Checkpoint-Restart for Hardware-Accelerated 3D Graphics</title><categories>cs.OS</categories><comments>20 pages, 6 figures, 4 tables</comments><acm-class>D.4.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing fault-tolerance for long-running GPU-intensive jobs requires
application-specific solutions, and often involves saving the state of complex
data structures spread among many graphics libraries. This work describes a
mechanism for transparent GPU-independent checkpoint-restart of 3D graphics.
The approach is based on a record-prune-replay paradigm: all OpenGL calls
relevant to the graphics driver state are recorded; calls not relevant to the
internal driver state as of the last graphics frame prior to checkpoint are
discarded; and the remaining calls are replayed on restart. A previous approach
for OpenGL 1.5, based on a shadow device driver, required more than 78,000
lines of OpenGL-specific code. In contrast, the new approach, based on
record-prune-replay, is used to implement the same case in just 4,500 lines of
code. The speed of this approach varies between 80 per cent and nearly 100 per
cent of the speed of the native hardware acceleration for OpenGL 1.5, as
measured when running the ioquake3 game under Linux. This approach has also
been extended to demonstrate checkpointing of OpenGL 3.0 for the first time,
with a demonstration for PyMol, for molecular visualization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6652</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6652</id><created>2013-12-23</created><authors><author><keyname>Barak</keyname><forenames>Boaz</forenames></author><author><keyname>Kelner</keyname><forenames>Jonathan</forenames></author><author><keyname>Steurer</keyname><forenames>David</forenames></author></authors><title>Rounding Sum-of-Squares Relaxations</title><categories>cs.DS cs.LG quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general approach to rounding semidefinite programming
relaxations obtained by the Sum-of-Squares method (Lasserre hierarchy). Our
approach is based on using the connection between these relaxations and the
Sum-of-Squares proof system to transform a *combining algorithm* -- an
algorithm that maps a distribution over solutions into a (possibly weaker)
solution -- into a *rounding algorithm* that maps a solution of the relaxation
to a solution of the original problem.
  Using this approach, we obtain algorithms that yield improved results for
natural variants of three well-known problems:
  1) We give a quasipolynomial-time algorithm that approximates the maximum of
a low degree multivariate polynomial with non-negative coefficients over the
Euclidean unit sphere. Beyond being of interest in its own right, this is
related to an open question in quantum information theory, and our techniques
have already led to improved results in this area (Brand\~{a}o and Harrow, STOC
'13).
  2) We give a polynomial-time algorithm that, given a d dimensional subspace
of R^n that (almost) contains the characteristic function of a set of size n/k,
finds a vector $v$ in the subspace satisfying $|v|_4^4 &gt; c(k/d^{1/3}) |v|_2^2$,
where $|v|_p = (E_i v_i^p)^{1/p}$. Aside from being a natural relaxation, this
is also motivated by a connection to the Small Set Expansion problem shown by
Barak et al. (STOC 2012) and our results yield a certain improvement for that
problem.
  3) We use this notion of L_4 vs. L_2 sparsity to obtain a polynomial-time
algorithm with substantially improved guarantees for recovering a planted
$\mu$-sparse vector v in a random d-dimensional subspace of R^n. If v has mu n
nonzero coordinates, we can recover it with high probability whenever $\mu &lt;
O(\min(1,n/d^2))$, improving for $d &lt; n^{2/3}$ prior methods which
intrinsically required $\mu &lt; O(1/\sqrt(d))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6661</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6661</id><created>2013-12-23</created><updated>2014-04-18</updated><authors><author><keyname>Kinney</keyname><forenames>Justin B.</forenames></author></authors><title>Rapid and deterministic estimation of probability densities using
  scale-free field theories</title><categories>physics.data-an cs.LG math.ST q-bio.QM stat.ML stat.TH</categories><comments>4 pages, 4 figures. Major revision in v3. The &quot;Density Estimation
  using Field Theory&quot; (DEFT) software package is available at
  https://github.com/jbkinney/13_deft</comments><journal-ref>Phys. Rev. E 90, 011301 (2014)</journal-ref><doi>10.1103/PhysRevE.90.011301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question of how best to estimate a continuous probability density from
finite data is an intriguing open problem at the interface of statistics and
physics. Previous work has argued that this problem can be addressed in a
natural way using methods from statistical field theory. Here I describe new
results that allow this field-theoretic approach to be rapidly and
deterministically computed in low dimensions, making it practical for use in
day-to-day data analysis. Importantly, this approach does not impose a
privileged length scale for smoothness of the inferred probability density, but
rather learns a natural length scale from the data due to the tradeoff between
goodness-of-fit and an Occam factor. Open source software implementing this
method in one and two dimensions is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6668</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6668</id><created>2013-12-23</created><updated>2015-07-30</updated><authors><author><keyname>Meunier</keyname><forenames>Pierre-&#xc9;tienne</forenames></author><author><keyname>Regnault</keyname><forenames>Damien</forenames></author></authors><title>A pumping lemma for non-cooperative self-assembly</title><categories>cs.CC cs.DM</categories><comments>Major rewriting: explicit pumping algorithm (+online program), and
  bug fixes</comments><acm-class>F.1.1; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the computational weakness of a model of tile assembly that has so
far resisted many attempts of formal analysis or positive constructions.
Specifically, we prove that, in Winfree's abstract Tile Assembly Model, when
restricted to use only noncooperative bindings, any long enough path that can
grow in all terminal assemblies is pumpable, meaning that this path can be
extended into an infinite, ultimately periodic path.
  This result can be seen as a geometric generalization of the pumping lemma of
finite state automata, and closes the question of what can be computed
deterministically in this model. Moreover, this question has motivated the
development of a new method called visible glues. We believe that this method
can also be used to tackle other long-standing problems in computational
geometry, in relation for instance with self-avoiding paths.
  Tile assembly (including non-cooperative tile assembly) was originally
introduced by Winfree and Rothemund in STOC 2000 to understand how to program
shapes. The non-cooperative variant, also known as temperature 1 tile assembly,
is the model where tiles are allowed to bind as soon as they match on one side,
whereas in cooperative tile assembly, some tiles need to match on several sides
in order to bind. In this work, we prove that only very simple shapes can
indeed be programmed, whereas exactly one known result (SODA 2014) showed a
restriction on the assemblies general non-cooperative self-assembly could
achieve, without any implication on its computational expressiveness. With
non-square tiles (like polyominos, SODA 2015), other recent works have shown
that the model quickly becomes computationally powerful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6675</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6675</id><created>2013-12-23</created><updated>2014-03-15</updated><authors><author><keyname>Atzmueller</keyname><forenames>Martin</forenames></author></authors><title>Data Mining on Social Interaction Networks</title><categories>cs.SI cs.DB physics.soc-ph</categories><comments>minor rev/corrections; enlarged figures, typos, commas, updated the
  &quot;in press&quot;/&quot;to appear&quot; entries in references</comments><journal-ref>Journal of Data Mining &amp; Digital Humanities, 2014 (June 24, 2014)
  jdmdh:11</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media and social networks have already woven themselves into the very
fabric of everyday life. This results in a dramatic increase of social data
capturing various relations between the users and their associated artifacts,
both in online networks and the real world using ubiquitous devices. In this
work, we consider social interaction networks from a data mining perspective -
also with a special focus on real-world face-to-face contact networks: We
combine data mining and social network analysis techniques for examining the
networks in order to improve our understanding of the data, the modeled
behavior, and its underlying emergent processes. Furthermore, we adapt, extend
and apply known predictive data mining algorithms on social interaction
networks. Additionally, we present novel methods for descriptive data mining
for uncovering and extracting relations and patterns for hypothesis generation
and exploration, in order to provide characteristic information about the data
and networks. The presented approaches and methods aim at extracting valuable
knowledge for enhancing the understanding of the respective data, and for
supporting the users of the respective systems. We consider data from several
social systems, like the social bookmarking system BibSonomy, the social
resource sharing system flickr, and ubiquitous social systems: Specifically, we
focus on data from the social conference guidance system Conferator and the
social group interaction system MyGroup. This work first gives a short
introduction into social interaction networks, before we describe several
analysis results in the context of online social networks and real-world
face-to-face contact networks. Next, we present predictive data mining methods,
i.e., for localization, recommendation and link prediction. After that, we
present novel descriptive data mining methods for mining communities and
patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6677</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6677</id><created>2013-12-23</created><updated>2015-03-05</updated><authors><author><keyname>Lee</keyname><forenames>Yin Tat</forenames></author><author><keyname>Sidford</keyname><forenames>Aaron</forenames></author></authors><title>Path Finding I :Solving Linear Programs with \~O(sqrt(rank)) Linear
  System Solves</title><categories>cs.DS math.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new algorithm for solving linear programs that
requires only $\tilde{O}(\sqrt{rank(A)}L)$ iterations to solve a linear program
with $m$ constraints, $n$ variables, and constraint matrix $A$, and bit
complexity $L$. Each iteration of our method consists of solving $\tilde{O}(1)$
linear systems and additional nearly linear time computation.
  Our method improves upon the previous best iteration bound by factor of
$\tilde{\Omega}((m/rank(A))^{1/4})$ for methods with polynomial time computable
iterations and by $\tilde{\Omega}((m/rank(A))^{1/2})$ for methods which solve
at most $\tilde{O}(1)$ linear systems in each iteration. Our method is
parallelizable and amenable to linear algebraic techniques for accelerating the
linear system solver. As such, up to polylogarithmic factors we either match or
improve upon the best previous running times in both depth and work for
different ratios of $m$ and $rank(A)$.
  Moreover, our method matches up to polylogarithmic factors a theoretical
limit established by Nesterov and Nemirovski in 1994 regarding the use of a
&quot;universal barrier&quot; for interior point methods, thereby resolving a
long-standing open question regarding the running time of polynomial time
interior point methods for linear programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6679</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6679</id><created>2013-12-23</created><updated>2015-10-25</updated><authors><author><keyname>Schwerdtfeger</keyname><forenames>Konrad W.</forenames></author></authors><title>The Connectivity of Boolean Satisfiability: Dichotomies for Formulas and
  Circuits</title><categories>cs.CC cs.LO</categories><comments>20 pages, several improvements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For Boolean satisfiability problems, the structure of the solution space is
characterized by the solution graph, where the vertices are the solutions, and
two solutions are connected iff they differ in exactly one variable. In 2006,
Gopalan et al. studied connectivity properties of the solution graph and
related complexity issues for CSPs, motivated mainly by research on
satisfiability algorithms and the satisfiability threshold. They proved
dichotomies for the diameter of connected components and for the complexity of
the st-connectivity question, and conjectured a trichotomy for the connectivity
question. Recently, we were able to establish the trichotomy [arXiv:1312.4524].
  Here, we consider connectivity issues of satisfiability problems defined by
Boolean circuits and propositional formulas that use gates, resp. connectives,
from a fixed set of Boolean functions. We obtain dichotomies for the diameter
and the two connectivity problems: on one side, the diameter is linear in the
number of variables, and both problems are in P, while on the other side, the
diameter can be exponential, and the problems are PSPACE-complete. For
partially quantified formulas, we show an analogous dichotomy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6680</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6680</id><created>2013-12-23</created><updated>2014-05-21</updated><authors><author><keyname>Williams</keyname><forenames>Ryan</forenames></author></authors><title>Faster all-pairs shortest paths via circuit complexity</title><categories>cs.DS cs.CC math.CO</categories><comments>24 pages. Updated version now has slightly faster running time. To
  appear in ACM Symposium on Theory of Computing (STOC), 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a new randomized method for computing the min-plus product
(a.k.a., tropical product) of two $n \times n$ matrices, yielding a faster
algorithm for solving the all-pairs shortest path problem (APSP) in dense
$n$-node directed graphs with arbitrary edge weights. On the real RAM, where
additions and comparisons of reals are unit cost (but all other operations have
typical logarithmic cost), the algorithm runs in time
\[\frac{n^3}{2^{\Omega(\log n)^{1/2}}}\] and is correct with high probability.
On the word RAM, the algorithm runs in $n^3/2^{\Omega(\log n)^{1/2}} +
n^{2+o(1)}\log M$ time for edge weights in $([0,M] \cap {\mathbb
Z})\cup\{\infty\}$. Prior algorithms used either $n^3/(\log^c n)$ time for
various $c \leq 2$, or $O(M^{\alpha}n^{\beta})$ time for various $\alpha &gt; 0$
and $\beta &gt; 2$.
  The new algorithm applies a tool from circuit complexity, namely the
Razborov-Smolensky polynomials for approximately representing ${\sf AC}^0[p]$
circuits, to efficiently reduce a matrix product over the $(\min,+)$ algebra to
a relatively small number of rectangular matrix products over ${\mathbb F}_2$,
each of which are computable using a particularly efficient method due to
Coppersmith. We also give a deterministic version of the algorithm running in
$n^3/2^{\log^{\delta} n}$ time for some $\delta &gt; 0$, which utilizes the
Yao-Beigel-Tarui translation of ${\sf AC}^0[m]$ circuits into &quot;nice&quot; depth-two
circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6693</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6693</id><created>2013-12-23</created><authors><author><keyname>DuPrie</keyname><forenames>Kimberly</forenames></author><author><keyname>Allen</keyname><forenames>Alice</forenames></author><author><keyname>Berriman</keyname><forenames>Bruce</forenames></author><author><keyname>Hanisch</keyname><forenames>Robert J.</forenames></author><author><keyname>Mink</keyname><forenames>Jessica</forenames></author><author><keyname>Nemiroff</keyname><forenames>Robert J.</forenames></author><author><keyname>Shamir</keyname><forenames>Lior</forenames></author><author><keyname>Shortridge</keyname><forenames>Keith</forenames></author><author><keyname>Taylor</keyname><forenames>Mark B.</forenames></author><author><keyname>Teuben</keyname><forenames>Peter</forenames></author><author><keyname>Wallin</keyname><forenames>John F.</forenames></author></authors><title>Astrophysics Source Code Library: Incite to Cite!</title><categories>astro-ph.IM cs.DL</categories><comments>Four pages, two figures. To be published in the Proceedings of ADASS
  XXIII, which took place September 29-October 3, 2013. The Astrophysics Source
  Code Library can be accessed at http://www.ascl.net; a concise directory of
  codes can be accessed at http://asterisk.apod.com/wp/?page_id=12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Astrophysics Source Code Library (ASCL, http://ascl.net/) is an online
registry of over 700 source codes that are of interest to astrophysicists, with
more being added regularly. The ASCL actively seeks out codes as well as
accepting submissions from the code authors, and all entries are citable and
indexed by ADS. All codes have been used to generate results published in or
submitted to a refereed journal and are available either via a download site or
froman identified source. In addition to being the largest directory of
scientist-written astrophysics programs available, the ASCL is also an active
participant in the reproducible research movement with presentations at various
conferences, numerous blog posts and a journal article. This poster provides a
description of the ASCL and the changes that we are starting to see in the
astrophysics community as a result of the work we are doing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6700</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6700</id><created>2013-12-19</created><authors><author><keyname>Neary</keyname><forenames>Turlough</forenames></author></authors><title>Undecidability in binary tag systems and the Post correspondence problem
  for four pairs of words</title><categories>cs.FL cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since Cocke and Minsky proved 2-tag systems universal, they have been
extensively used to prove the universality of numerous computational models.
Unfortunately, all known algorithms give universal 2-tag systems that have a
large number of symbols. In this work, tag systems with only 2 symbols (the
minimum possible) are proved universal via an intricate construction showing
that they simulate cyclic tag systems. Our simulation algorithm has a
polynomial time overhead, and thus shows that binary tag systems simulate
Turing machines in polynomial time.
  We immediately find applications of our result. We reduce the halting problem
for binary tag systems to the Post correspondence problem for 4 pairs of words.
This improves on 7 pairs, the previous bound for undecidability in this
problem. Following our result, only the case for 3 pairs of words remains open,
as the problem is known to be decidable for 2 pairs. As a further application,
we find that the matrix mortality problem is undecidable for sets with five
$3\times 3$ matrices and for sets with two $15\times 15$ matrices. The previous
bounds for the undecidability in this problem was seven $3\times 3$ matrices
and two $ 21\times 21$ matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6712</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6712</id><created>2013-12-23</created><authors><author><keyname>Grabocka</keyname><forenames>Josif</forenames></author><author><keyname>Schmidt-Thieme</keyname><forenames>Lars</forenames></author></authors><title>Invariant Factorization Of Time-Series</title><categories>cs.LG</categories><doi>10.1007/s10618-014-0364-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-series classification is an important domain of machine learning and a
plethora of methods have been developed for the task. In comparison to existing
approaches, this study presents a novel method which decomposes a time-series
dataset into latent patterns and membership weights of local segments to those
patterns. The process is formalized as a constrained objective function and a
tailored stochastic coordinate descent optimization is applied. The time-series
are projected to a new feature representation consisting of the sums of the
membership weights, which captures frequencies of local patterns. Features from
various sliding window sizes are concatenated in order to encapsulate the
interaction of patterns from different sizes. Finally, a large-scale
experimental comparison against 6 state of the art baselines and 43 real life
datasets is conducted. The proposed method outperforms all the baselines with
statistically significant margins in terms of prediction accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6713</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6713</id><created>2013-12-23</created><updated>2015-03-05</updated><authors><author><keyname>Lee</keyname><forenames>Yin Tat</forenames></author><author><keyname>Sidford</keyname><forenames>Aaron</forenames></author></authors><title>Path Finding II : An \~O(m sqrt(n)) Algorithm for the Minimum Cost Flow
  Problem</title><categories>cs.DS math.NA math.OC</categories><comments>arXiv admin note: text overlap with arXiv:1312.6677</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an $\tilde{O}(m\sqrt{n}\log^{O(1)}U)$ time algorithm
for solving the maximum flow problem on directed graphs with $m$ edges, $n$
vertices, and capacity ratio $U$. This improves upon the previous fastest
running time of
$O(m\min\left(n^{2/3},m^{1/2}\right)\log\left(n^{2}/m\right)\log U)$ achieved
over 15 years ago by Goldberg and Rao. In the special case of solving dense
directed unit capacity graphs our algorithm improves upon the previous fastest
running times of of $O(\min\{m^{3/2},mn^{^{2/3}}\})$ achieved by Even and
Tarjan and Karzanov over 35 years ago and of $\tilde{O}(m^{10/7})$ achieved
recently by M\k{a}dry.
  We achieve these results through the development and application of a new
general interior point method that we believe is of independent interest. The
number of iterations required by this algorithm is better than that predicted
by analyzing the best self-concordant barrier of the feasible region. By
applying this method to the linear programming formulations of maximum flow,
minimum cost flow, and lossy generalized minimum cost flow and applying
analysis by Daitch and Spielman we achieve running time of
$\tilde{O}(m\sqrt{n}\log^{O(1)}(U/\epsilon))$ for these problems as well.
Furthermore, our algorithm is parallelizable and using a recent nearly linear
time work polylogarithmic depth Laplacian system solver of Spielman and Peng we
achieve a $\tilde{O}(\sqrt{n}\log^{O(1)}(U/\epsilon))$ depth algorithm and
$\tilde{O}(m\sqrt{n}\log^{O(1)}(U/\epsilon))$ work algorithm for solving these
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6715</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6715</id><created>2013-12-23</created><authors><author><keyname>Bendtsen</keyname><forenames>Kristian Moss</forenames></author><author><keyname>Uekermann</keyname><forenames>Florian</forenames></author><author><keyname>Haerter</keyname><forenames>Jan O.</forenames></author></authors><title>The expert game -- Cooperation in social communication</title><categories>cs.SI nlin.AO physics.soc-ph</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large parts of professional human communication proceed in a request-reply
fashion, whereby requests contain specifics of the information desired while
replies can deliver the required information. However, time limitations often
force individuals to prioritize some while neglecting others. This dilemma will
inevitably force individuals into defecting against some communication partners
to give attention to others. Furthermore, communication entirely breaks down
when individuals act purely egoistically as replies would never be issued and
quest for desired information would always be prioritized. Here we present an
experiment, termed &quot;The expert game&quot;, where a number of individuals communicate
with one-another through an electronic messaging system. By imposing a strict
limit on the number of sent messages, individuals were required to decide
between requesting information that is beneficial for themselves or helping
others by replying to their requests. In the experiment, individuals were
assigned the task to find the expert on a specific topic and receive a reply
from that expert. Tasks and expertise of each player were periodically
re-assigned to randomize the required interactions. Resisting this
randomization, a non-random network of cooperative communication between
individuals formed. We use a simple Bayesian inference algorithm to model each
player's trust in the cooperativity of others with good experimental agreement.
Our results suggest that human communication in groups of individuals is
strategic and favors cooperation with trusted parties at the cost of defection
against others. To establish and maintain trusted links a significant fraction
of time-resources is allocated, even in situations where the information
transmitted is negligible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6720</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6720</id><created>2013-12-23</created><authors><author><keyname>Menichetti</keyname><forenames>Giulia</forenames></author><author><keyname>Remondini</keyname><forenames>Daniel</forenames></author><author><keyname>Panzarasa</keyname><forenames>Pietro</forenames></author><author><keyname>Mondrag&#xf3;n</keyname><forenames>Ra&#xfa;l J.</forenames></author><author><keyname>Bianconi</keyname><forenames>Ginestra</forenames></author></authors><title>Weighted Multiplex Networks</title><categories>physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech cs.DL cs.SI</categories><comments>(22 pages, 10 figures)</comments><journal-ref>PLoS ONE 9(6): e97857 (2014)</journal-ref><doi>10.1371/journal.pone.0097857</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important challenges in network science is to quantify the
information encoded in complex network structures. Disentangling randomness
from organizational principles is even more demanding when networks have a
multiplex nature. Multiplex networks are multilayer systems of $N$ nodes that
can be linked in multiple interacting and co-evolving layers. In these
networks, relevant information might not be captured if the single layers were
analyzed separately. Here we demonstrate that such partial analysis of layers
fails to capture significant correlations between weights and topology of
complex multiplex networks. To this end, we study two weighted multiplex
co-authorship and citation networks involving the authors included in the
American Physical Society. We show that in these networks weights are strongly
correlated with multiplex structure, and provide empirical evidence in favor of
the advantage of studying weighted measures of multiplex networks, such as
multistrength and the inverse multiparticipation ratio. Finally, we introduce a
theoretical framework based on the entropy of multiplex ensembles to quantify
the information stored in multiplex networks that would remain undetected if
the single layers were analyzed in isolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6721</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6721</id><created>2013-12-23</created><authors><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Jayaraman</keyname><forenames>Prem Prakash</forenames></author><author><keyname>Zaslavsky</keyname><forenames>Arkady</forenames></author><author><keyname>Christen</keyname><forenames>Peter</forenames></author><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author></authors><title>Sensor Discovery and Configuration Framework for The Internet of Things
  Paradigm</title><categories>cs.NI</categories><comments>6 pages. arXiv admin note: substantial text overlap with
  arXiv:1311.2134</comments><journal-ref>Proceedings of the IEEE World Forum on Internet of Things
  (WF-IoT), Seoul, Korea, March, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of Things (IoT) will comprise billions of devices that can sense,
communicate, compute and potentially actuate. The data generated by the
Internet of Things are valuable and have the potential to drive innovative and
novel applications. The data streams coming from these devices will challenge
the traditional approaches to data management and contribute to the emerging
paradigm of big data. One of the most challenging tasks before collecting and
processing data from these devices (e.g. sensors) is discovering and
configuring the sensors and the associated data streams. In this paper, we
propose a tool called SmartLink that can be used to discover and configure
sensors. Specifically, SmartLink, is capable of discovering sensors deployed in
a particular location despite their heterogeneity (e.g. different communication
protocols, communication sequences, capabilities). SmartLink establishes the
direct communication between the sensor hardware and cloud-based IoT
middleware. We address the challenge of heterogeneity using a plugin
architecture. Our prototype tool is developed on the Android platform. We
evaluate the significance of our approach by discovering and configuring 52
different types of Libelium sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6722</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6722</id><created>2013-12-23</created><updated>2015-04-30</updated><authors><author><keyname>Benzi</keyname><forenames>Michele</forenames></author><author><keyname>Klymko</keyname><forenames>Christine</forenames></author></authors><title>On the limiting behavior of parameter-dependent network centrality
  measures</title><categories>math.NA cs.SI physics.soc-ph</categories><comments>First 22 pages are the paper, pages 22-38 are the supplementary
  materials</comments><msc-class>05C50, 15A16</msc-class><journal-ref>SIAM J. Matrix Anal. App., Vol. 36, No. 2, pp. 686-706, (2015)</journal-ref><doi>10.1137/130950550</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a broad class of walk-based, parameterized node centrality
measures for network analysis. These measures are expressed in terms of
functions of the adjacency matrix and generalize various well-known centrality
indices, including Katz and subgraph centrality. We show that the parameter can
be &quot;tuned&quot; to interpolate between degree and eigenvector centrality, which
appear as limiting cases. Our analysis helps explain certain correlations often
observed between the rankings obtained using different centrality measures, and
provides some guidance for the tuning of parameters. We also highlight the
roles played by the spectral gap of the adjacency matrix and by the number of
triangles in the network. Our analysis covers both undirected and directed
networks, including weighted ones. A brief discussion of PageRank is also
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6723</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6723</id><created>2013-12-23</created><authors><author><keyname>Berriman</keyname><forenames>G. Bruce</forenames></author><author><keyname>Deelman</keyname><forenames>Ewa</forenames></author><author><keyname>Good</keyname><forenames>John</forenames></author><author><keyname>Juve</keyname><forenames>Gideon</forenames></author><author><keyname>Kinney</keyname><forenames>Jamie</forenames></author><author><keyname>Merrihew</keyname><forenames>Ann</forenames></author><author><keyname>Rynge</keyname><forenames>Mats</forenames></author></authors><title>Creating A Galactic Plane Atlas With Amazon Web Services</title><categories>astro-ph.IM cs.DC</categories><comments>7 pages, 1 table, 2 figures. Submitted to IEEE Special Edition on
  Computing in Astronomy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes by example how astronomers can use cloud-computing
resources offered by Amazon Web Services (AWS) to create new datasets at scale.
We have created from existing surveys an atlas of the Galactic Plane at 16
wavelengths from 1 {\mu}m to 24 {\mu}m with pixels co-registered at spatial
sampling of 1 arcsec. We explain how open source tools support management and
operation of a virtual cluster on AWS platforms to process data at scale, and
describe the technical issues that users will need to consider, such as
optimization of resources, resource costs, and management of virtual machine
instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6724</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6724</id><created>2013-12-23</created><updated>2015-03-19</updated><authors><author><keyname>Awasthi</keyname><forenames>Pranjal</forenames></author><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Voevodski</keyname><forenames>Konstantin</forenames></author></authors><title>Local algorithms for interactive clustering</title><categories>cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the design of interactive clustering algorithms for data sets
satisfying natural stability assumptions. Our algorithms start with any initial
clustering and only make local changes in each step; both are desirable
features in many applications. We show that in this constrained setting one can
still design provably efficient algorithms that produce accurate clusterings.
We also show that our algorithms perform well on real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6726</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6726</id><created>2013-12-23</created><authors><author><keyname>Grau-Moya</keyname><forenames>Jordi</forenames></author><author><keyname>Braun</keyname><forenames>Daniel A.</forenames></author></authors><title>Bounded Rational Decision-Making in Changing Environments</title><categories>cs.AI</categories><comments>9 pages, 2 figures, NIPS 2013 Workshop on Planning with Information
  Constraints</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A perfectly rational decision-maker chooses the best action with the highest
utility gain from a set of possible actions. The optimality principles that
describe such decision processes do not take into account the computational
costs of finding the optimal action. Bounded rational decision-making addresses
this problem by specifically trading off information-processing costs and
expected utility. Interestingly, a similar trade-off between energy and entropy
arises when describing changes in thermodynamic systems. This similarity has
been recently used to describe bounded rational agents. Crucially, this
framework assumes that the environment does not change while the decision-maker
is computing the optimal policy. When this requirement is not fulfilled, the
decision-maker will suffer inefficiencies in utility, that arise because the
current policy is optimal for an environment in the past. Here we borrow
concepts from non-equilibrium thermodynamics to quantify these inefficiencies
and illustrate with simulations its relationship with computational resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6740</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6740</id><created>2013-12-23</created><authors><author><keyname>Xia</keyname><forenames>Feng</forenames></author><author><keyname>Hsu</keyname><forenames>Ching-Hsien</forenames></author><author><keyname>Liu</keyname><forenames>Xiaojing</forenames></author><author><keyname>Ding</keyname><forenames>Fangwei</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author></authors><title>The Power of Smartphones</title><categories>cs.NI</categories><comments>accepted; Multimedia Systems, 2013. arXiv admin note: text overlap
  with arXiv:1201.0219</comments><msc-class>68M14</msc-class><acm-class>C.2</acm-class><doi>10.1007/s00530-013-0337-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smartphones have been shipped with multiple wireless network interfaces in
order to meet their diverse communication and networking demands. However, as
smartphones increasingly rely on wireless network connections to realize more
functions, the demand of energy has been significantly increased, which has
become the limit for people to explore smartphones' real power. In this paper,
we first review typical smartphone computing systems, energy consumption of
smartphone, and state-of-the-art techniques of energy saving for smartphones.
Then we propose a location-assisted Wi-Fi discovery scheme, which discovers the
nearest Wi-Fi network access points (APs) by using the user's location
information. This allows the user to switch to the Wi-Fi interface in an
intelligent manner when he/she arrives at the nearest Wi-Fi network AP. Thus we
can meet the user's bandwidth needs and provide the best connectivity.
Additionally, it avoids the long periods in idle state and greatly reduces the
number of unnecessary Wi-Fi scans on the mobile device. Our experiments and
simulations demonstrate that our scheme effectively saves energy for
smartphones integrated with Wi-Fi and cellular interfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6743</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6743</id><created>2013-12-23</created><updated>2014-07-14</updated><authors><author><keyname>Luo</keyname><forenames>Shixin</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Lim</keyname><forenames>Teng Joon</forenames></author></authors><title>Joint Transmitter and Receiver Energy Minimization in Multiuser OFDM
  Systems</title><categories>cs.IT math.IT</categories><comments>33 pages, 4 figures, submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we formulate and solve a weighted-sum transmitter and receiver
energy minimization (WSTREMin) problem in the downlink of an orthogonal
frequency division multiplexing (OFDM) based multiuser wireless system. The
proposed approach offers the flexibility of assigning different levels of
importance to base station (BS) and mobile terminal (MT) power consumption,
corresponding to the BS being connected to the grid and the MT relying on
batteries. To obtain insights into the characteristics of the problem, we first
consider two extreme cases separately, i.e., weighted-sum receiver-side energy
minimization (WSREMin) for MTs and transmitter-side energy minimization (TEMin)
for the BS. It is shown that Dynamic TDMA (D-TDMA), where MTs are scheduled for
single-user OFDM transmissions over orthogonal time slots, is the optimal
transmission strategy for WSREMin at MTs, while OFDMA is optimal for TEMin at
the BS. As a hybrid of the two extreme cases, we further propose a new multiple
access scheme, i.e., Time-Slotted OFDMA (TS-OFDMA) scheme, in which MTs are
grouped into orthogonal time slots with OFDMA applied to users assigned within
the same slot. TS-OFDMA can be shown to include both D-TDMA and OFDMA as
special cases. Numerical results confirm that the proposed schemes enable a
flexible range of energy consumption tradeoffs between the BS and MTs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6756</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6756</id><created>2013-12-23</created><authors><author><keyname>Lucia</keyname><forenames>William</forenames></author><author><keyname>Akcora</keyname><forenames>Cuneyt Gurcan</forenames></author><author><keyname>Ferrari</keyname><forenames>Elena</forenames></author></authors><title>Multi-dimensional Conversation Analysis across Online Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Datasets will be anonymized and published at:
  http://akcora.wordpress.com/2013/12/24/pointer-for-datasets/</comments><journal-ref>In Social Computing and its applications (SCA), 2013 Third
  International Conference on. IEEE</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  With the advance of the Internet, ordinary users have created multiple
personal accounts on online social networks, and interactions among these
social network users have recently been tagged with location information. In
this work, we observe user interactions across two popular online social
networks, Facebook and Twitter, and analyze which factors lead to retweet/like
interactions for tweets/posts. In addition to the named entities, lexical
errors and expressed sentiments in these data items, we also consider the
impact of shared user locations on user interactions. In particular, we show
that geolocations of users can greatly affect which social network post/tweet
will be liked/ retweeted. We believe that the results of our analysis can help
researchers to understand which social network content will have better
visibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6764</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6764</id><created>2013-12-24</created><authors><author><keyname>Nivel</keyname><forenames>E.</forenames></author><author><keyname>Th&#xf3;risson</keyname><forenames>K. R.</forenames></author><author><keyname>Steunebrink</keyname><forenames>B. R.</forenames></author><author><keyname>Dindo</keyname><forenames>H.</forenames></author><author><keyname>Pezzulo</keyname><forenames>G.</forenames></author><author><keyname>Rodriguez</keyname><forenames>M.</forenames></author><author><keyname>Hernandez</keyname><forenames>C.</forenames></author><author><keyname>Ognibene</keyname><forenames>D.</forenames></author><author><keyname>Schmidhuber</keyname><forenames>J.</forenames></author><author><keyname>Sanz</keyname><forenames>R.</forenames></author><author><keyname>Helgason</keyname><forenames>H. P.</forenames></author><author><keyname>Chella</keyname><forenames>A.</forenames></author><author><keyname>Jonsson</keyname><forenames>G. K.</forenames></author></authors><title>Bounded Recursive Self-Improvement</title><categories>cs.AI</categories><report-no>RUTR-SCS13006</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have designed a machine that becomes increasingly better at behaving in
underspecified circumstances, in a goal-directed way, on the job, by modeling
itself and its environment as experience accumulates. Based on principles of
autocatalysis, endogeny, and reflectivity, the work provides an architectural
blueprint for constructing systems with high levels of operational autonomy in
underspecified circumstances, starting from a small seed. Through value-driven
dynamic priority scheduling controlling the parallel execution of a vast number
of reasoning threads, the system achieves recursive self-improvement after it
leaves the lab, within the boundaries imposed by its designers. A prototype
system has been implemented and demonstrated to learn a complex real-world
task, real-time multimodal dialogue with humans, by on-line observation. Our
work presents solutions to several challenges that must be solved for achieving
artificial general intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6782</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6782</id><created>2013-12-24</created><authors><author><keyname>Bhute</keyname><forenames>Avinash N</forenames></author><author><keyname>Meshram</keyname><forenames>B. B.</forenames></author></authors><title>IVSS Integration of Color Feature Extraction Techniques for Intelligent
  Video Search Systems</title><categories>cs.CV cs.IR cs.MM</categories><comments>5 pages, 9 figures. 2012 4th International Conference on Electronics
  Computer Technology - ICECT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As large amount of visual Information is available on web in form of images,
graphics, animations and videos, so it is important in internet era to have an
effective video search system. As there are number of video search engine
(blinkx, Videosurf, Google, YouTube, etc.) which search for relevant videos
based on user keyword or term, But very less commercial video search engine are
available which search videos based on visual image/clip/video. In this paper
we are recommending a system that will search for relevant video using color
feature of video in response of user Query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6784</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6784</id><created>2013-12-24</created><updated>2015-12-08</updated><authors><author><keyname>Dai</keyname><forenames>Bin</forenames></author><author><keyname>Yu</keyname><forenames>Linman</forenames></author><author><keyname>Ma</keyname><forenames>Zheng</forenames></author></authors><title>Relay Broadcast Channel with Confidential Messages</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Transactions on Information Forensic and Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the effects of an additional relay node on the secrecy of
broadcast channels by considering the model of relay broadcast channels with
confidential messages. We show that this additional relay node can increase the
achievable secrecy rate region of the broadcast channels with confidential
messages. More specifically, first, we investigate the discrete memoryless
relay broadcast channels with two confidential messages and one common message.
Three inner bounds (with respect to decode-forward, generalized noise-forward
and compress-forward strategies) and an outer bound on the
capacity-equivocation region are provided. Second, we investigate the discrete
memoryless relay broadcast channels with two confidential messages. Inner and
outer bounds on the capacity-equivocation region are provided. Finally, we
investigate the discrete memoryless relay broadcast channels with one
confidential message and one common message. Inner and outer bounds on the
capacity-equivocation region are provided, and the results are further
explained via a Gaussian example. Compared with Csiszar-Korner's work on
broadcast channels with confidential messages (BCC), we find that with the help
of the relay node, the secrecy capacity region of the Gaussian BCC is enhanced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6791</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6791</id><created>2013-12-24</created><authors><author><keyname>Kalka</keyname><forenames>Arkadius</forenames></author><author><keyname>Teicher</keyname><forenames>Mina</forenames></author></authors><title>Iterated LD-Problem in non-associative key establishment</title><categories>cs.CR math.GR</categories><comments>30 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1305.4401</comments><msc-class>20N02, 20F36</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct new non-associative key establishment protocols for all left
self-distributive (LD), multi-LD-, and mutual LD-systems. The hardness of these
protocols relies on variations of the (simultaneous) iterated LD-problem and
its generalizations. We discuss instantiations of these protocols using
generalized shifted conjugacy in braid groups and their quotients, LD-conjugacy
and $f$-symmetric conjugacy in groups. We suggest parameter choices for
instantiations in braid groups, symmetric groups and several matrix groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6794</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6794</id><created>2013-12-24</created><authors><author><keyname>Kalka</keyname><forenames>Arkadius</forenames></author><author><keyname>Teicher</keyname><forenames>Mina</forenames></author></authors><title>Non-associative key establishment protocols and their implementation</title><categories>cs.CR math.GR</categories><comments>10 pages, 2 figures, conference emac13</comments><msc-class>20N02, 20F36</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide implementation details for non-associative key establishment
protocols. In particular, we describe the implementation of non-associative key
establishment protocols for all left self-distributive and all mutually left
distributive systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6802</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6802</id><created>2013-12-24</created><authors><author><keyname>Pande</keyname><forenames>B. P.</forenames></author><author><keyname>Tamta</keyname><forenames>Pawan</forenames></author><author><keyname>Dhami</keyname><forenames>H. S.</forenames></author></authors><title>Suffix Stripping Problem as an Optimization Problem</title><categories>cs.IR cs.CL</categories><comments>14 pages, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stemming or suffix stripping, an important part of the modern Information
Retrieval systems, is to find the root word (stem) out of a given cluster of
words. Existing algorithms targeting this problem have been developed in a
haphazard manner. In this work, we model this problem as an optimization
problem. An Integer Program is being developed to overcome the shortcomings of
the existing approaches. The sample results of the proposed method are also
being compared with an established technique in the field for English language.
An AMPL code for the same IP has also been given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6805</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6805</id><created>2013-12-24</created><authors><author><keyname>Feng</keyname><forenames>Lin</forenames></author><author><keyname>Qiu</keyname><forenames>Tie</forenames></author><author><keyname>Sun</keyname><forenames>Zhenlong</forenames></author><author><keyname>Xia</keyname><forenames>Feng</forenames></author><author><keyname>Zhou</keyname><forenames>Yu</forenames></author></authors><title>A Coverage Strategy for Wireless Sensor Networks in a Three-dimensional
  Environment</title><categories>cs.NI</categories><comments>accepted. Int. J. Ad Hoc and Ubiquitous Computing, 2013</comments><msc-class>68M14</msc-class><acm-class>C.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coverage is one of the fundamental issues in wireless sensor networks (WSNs).
It reflects the ability of WSNs to detect the fields of interest. In a real
sensor networks application, the detection area is always non-ideal and the
terrain of the detection area is often more complex in applications of
three-dimensional sensor networks. Consequently, many of the existing coverage
strategies cannot be directly applied to three-dimensional spaces. This paper
presents a new coverage strategy for the three-dimensional sensor networks.
Sensor nodes are uniformly distributed. The cost factor is utilized to
construct the perceived probability and the classical watershed algorithm after
the transformation of points from the three-dimensional space to the
two-dimensional plane using the dimensionality reduction method, which can
maintain the topology characteristic of the non-linear terrain. The detection
probability in the optimal breath path is used as the measure to evaluate the
coverage. Simulation results indicate that the proposed strategy can determine
the coverage with fewer nodes, while achieving the coverage requirements of the
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6807</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6807</id><created>2013-12-24</created><authors><author><keyname>Li</keyname><forenames>Fengqi</forenames></author><author><keyname>Yu</keyname><forenames>Chuang</forenames></author><author><keyname>Yang</keyname><forenames>Nanhai</forenames></author><author><keyname>Xia</keyname><forenames>Feng</forenames></author><author><keyname>Li</keyname><forenames>Guangming</forenames></author><author><keyname>Kaveh-Yazdy</keyname><forenames>Fatemeh</forenames></author></authors><title>Iterative Nearest Neighborhood Oversampling in Semisupervised Learning
  from Imbalanced Data</title><categories>cs.LG</categories><msc-class>68P20</msc-class><acm-class>H.3.3</acm-class><journal-ref>The Scientific World Journal, Volume 2013, Article ID 875450, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transductive graph-based semi-supervised learning methods usually build an
undirected graph utilizing both labeled and unlabeled samples as vertices.
Those methods propagate label information of labeled samples to neighbors
through their edges in order to get the predicted labels of unlabeled samples.
Most popular semi-supervised learning approaches are sensitive to initial label
distribution happened in imbalanced labeled datasets. The class boundary will
be severely skewed by the majority classes in an imbalanced classification. In
this paper, we proposed a simple and effective approach to alleviate the
unfavorable influence of imbalance problem by iteratively selecting a few
unlabeled samples and adding them into the minority classes to form a balanced
labeled dataset for the learning methods afterwards. The experiments on UCI
datasets and MNIST handwritten digits dataset showed that the proposed approach
outperforms other existing state-of-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6808</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6808</id><created>2013-12-24</created><authors><author><keyname>Xia</keyname><forenames>Feng</forenames></author><author><keyname>Asabere</keyname><forenames>Nana Yaw</forenames></author><author><keyname>Rodrigues</keyname><forenames>Joel J. P. C.</forenames></author><author><keyname>Basso</keyname><forenames>Filippo</forenames></author><author><keyname>Deonauth</keyname><forenames>Nakema</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author></authors><title>Socially-Aware Venue Recommendation for Conference Participants</title><categories>cs.IR cs.SI</categories><msc-class>68P20</msc-class><acm-class>H.3.3; H.1.2</acm-class><journal-ref>The 10th IEEE International Conference on Ubiquitous Intelligence
  and Computing (UIC), Vietri sul Mare, Italy, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current research environments are witnessing high enormities of presentations
occurring in different sessions at academic conferences. This situation makes
it difficult for researchers (especially juniors) to attend the right
presentation session(s) for effective collaboration. In this paper, we propose
an innovative venue recommendation algorithm to enhance smart conference
participation. Our proposed algorithm, Social Aware Recommendation of Venues
and Environments (SARVE), computes the Pearson Correlation and social
characteristic information of conference participants. SARVE further
incorporates the current context of both the smart conference community and
participants in order to model a recommendation process using distributed
community detection. Through the integration of the above computations and
techniques, we are able to recommend presentation sessions of active
participant presenters that may be of high interest to a particular
participant. We evaluate SARVE using a real world dataset. Our experimental
results demonstrate that SARVE outperforms other state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6809</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6809</id><created>2013-12-24</created><updated>2015-12-05</updated><authors><author><keyname>Bruggeman</keyname><forenames>Jeroen</forenames></author></authors><title>Interaction Rituals and Cooperation</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A significant challenge is to explain how people cooperate for public goods.
The problem is more difficult for people who hardly know one another, their
public good is unclear at the outset and its timing and costs are uncertain.
However, history shows that even under adverse conditions, people can
cooperate. As a prelude to cooperation, people can establish (or reinforce)
social ties and increase their solidarity through interaction rituals.
Consequently, individuals' commitments and psychological states may
synchronize, so that they can depend on like-minded people rather than on a
rational grasp of their situation, which is not feasible under difficult
circumstances. A necessary condition is that the network that is formed (or
used) during the ritual compensates for participants' initial differences. A
model shows exactly what network patterns are optimal, and it predicts that at
a critical level of solidarity, a heterogeneous majority homogenizes in a
sudden phase transition. This synchronization yields a boost of emotional
energy for a burst of collective action.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6813</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6813</id><created>2013-12-24</created><updated>2014-05-09</updated><authors><author><keyname>Liu</keyname><forenames>Gang</forenames></author><author><keyname>Huang</keyname><forenames>Ting-Zhu</forenames></author><author><keyname>Lv</keyname><forenames>Xiao-Guang</forenames></author><author><keyname>Liu</keyname><forenames>Jun</forenames></author></authors><title>New explicit thresholding/shrinkage formulas for one class of
  regularization problems with overlapping group sparsity and their
  applications</title><categories>math.NA cs.CV</categories><comments>22 pages, 30 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The least-square regression problems or inverse problems have been widely
studied in many fields such as compressive sensing, signal processing, and
image processing. To solve this kind of ill-posed problems, a regularization
term (i.e., regularizer) should be introduced, under the assumption that the
solutions have some specific properties, such as sparsity and group sparsity.
Widely used regularizers include the $\ell_1$ norm, total variation (TV)
semi-norm, and so on.
  Recently, a new regularization term with overlapping group sparsity has been
considered. Majorization minimization iteration method or variable duplication
methods are often applied to solve them. However, there have been no direct
methods for solve the relevant problems because of the difficulty of
overlapping. In this paper, we proposed new explicit shrinkage formulas for one
class of these relevant problems, whose regularization terms have translation
invariant overlapping groups. Moreover, we apply our results in TV deblurring
and denoising with overlapping group sparsity. We use alternating direction
method of multipliers to iterate solve it. Numerical results also verify the
validity and effectiveness of our new explicit shrinkage formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6820</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6820</id><created>2013-12-24</created><authors><author><keyname>Farahat</keyname><forenames>Ahmed K.</forenames></author><author><keyname>Ghodsi</keyname><forenames>Ali</forenames></author><author><keyname>Kamel</keyname><forenames>Mohamed S.</forenames></author></authors><title>A Fast Greedy Algorithm for Generalized Column Subset Selection</title><categories>cs.DS cs.LG stat.ML</categories><comments>NIPS'13 Workshop on Greedy Algorithms, Frank-Wolfe and Friends</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper defines a generalized column subset selection problem which is
concerned with the selection of a few columns from a source matrix A that best
approximate the span of a target matrix B. The paper then proposes a fast
greedy algorithm for solving this problem and draws connections to different
problems that can be efficiently solved using the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6823</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6823</id><created>2013-12-24</created><authors><author><keyname>Qiu</keyname><forenames>Tie</forenames></author><author><keyname>Ding</keyname><forenames>Yanhong</forenames></author><author><keyname>Xia</keyname><forenames>Feng</forenames></author><author><keyname>Ma</keyname><forenames>Honglian</forenames></author></authors><title>A Search Strategy of Level-Based Flooding for the Internet of Things</title><categories>cs.NI</categories><msc-class>68M14</msc-class><acm-class>C.2</acm-class><journal-ref>Sensors, 2012, 12(8):10163-10195</journal-ref><doi>10.3390/s120810163</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the query problem in the Internet of Things (IoT).
Flooding is an important query strategy. However, original flooding is prone to
cause heavy network loads. To address this problem, we propose a variant of
flooding, called Level-Based Flooding (LBF). With LBF, the whole network is
divided into several levels according to the distances (i.e., hops) between the
sensor nodes and the sink node. The sink node knows the level information of
each node. Query packets are broadcast in the network according to the levels
of nodes. Upon receiving a query packet, sensor nodes decide how to process it
according to the percentage of neighbors that have processed it. When the
target node receives the query packet, it sends its data back to the sink node
via random walk. We show by extensive simulations that the performance of LBF
in terms of cost and latency is much better than that of original flooding, and
LBF can be used in IoT of different scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6824</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6824</id><created>2013-12-24</created><authors><author><keyname>Biedl</keyname><forenames>Therese</forenames></author><author><keyname>Derka</keyname><forenames>Martin</forenames></author><author><keyname>Kiazyk</keyname><forenames>Stephen</forenames></author><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author><author><keyname>Vosoughpour</keyname><forenames>Hamide</forenames></author></authors><title>Dihedral angles and orthogonal polyhedra</title><categories>cs.CG</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider an orthogonal polyhedron, i.e., a polyhedron where (at least after a
suitable rotation) all faces are perpendicular to a coordinate axis, and hence
all edges are parallel to a coordinate axis. Clearly, any facial angle and any
dihedral angle is a multiple of $\pi/2$.
  In this note we explore the converse: if the facial and/or dihedral angles
are all multiples of $\pi /2$, is the polyhedron necessarily orthogonal? The
case of facial angles was answered previously. In this note we show that if
both the facial and dihedral angles are multiples of $\pi /2$ then the
polyhedron is orthogonal (presuming connectivity), and we give examples to show
that the condition for dihedral angles alone does not suffice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6826</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6826</id><created>2013-12-24</created><authors><author><keyname>Teran</keyname><forenames>Leizer</forenames></author><author><keyname>Mordohai</keyname><forenames>Philippos</forenames></author></authors><title>3D Interest Point Detection via Discriminative Learning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of detecting the interest points in 3D meshes has typically been
handled by geometric methods. These methods, while greatly describing human
preference, can be ill-equipped for handling the variety and subjectivity in
human responses. Different tasks have different requirements for interest point
detection; some tasks may necessitate high precision while other tasks may
require high recall. Sometimes points with high curvature may be desirable,
while in other cases high curvature may be an indication of noise. Geometric
methods lack the required flexibility to adapt to such changes. As a
consequence, interest point detection seems to be well suited for machine
learning methods that can be trained to match the criteria applied on the
annotated training data. In this paper, we formulate interest point detection
as a supervised binary classification problem using a random forest as our
classifier. Among other challenges, we are faced with an imbalanced learning
problem due to the substantial difference in the priors between interest and
non-interest points. We address this by re-sampling the training set. We
validate the accuracy of our method and compare our results to those of five
state of the art methods on a new, standard benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6827</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6827</id><created>2013-12-24</created><authors><author><keyname>Sun</keyname><forenames>Weifeng</forenames></author><author><keyname>Xia</keyname><forenames>Feng</forenames></author><author><keyname>Ma</keyname><forenames>Jianhua</forenames></author><author><keyname>Fu</keyname><forenames>Tong</forenames></author><author><keyname>Sun</keyname><forenames>Yu</forenames></author></authors><title>An Optimal ODAM-Based Broadcast Algorithm for Vehicular Ad-Hoc Networks</title><categories>cs.NI</categories><msc-class>68M12</msc-class><acm-class>C.2</acm-class><journal-ref>KSII Transactions on Internet and Information Systems, 6(12):
  3257-3274, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broadcast routing has become an important research field for vehicular ad-hoc
networks (VANETs) recently. However, the packet delivery rate is generally low
in existing VANET broadcast routing protocols. Therefore, the design of an
appropriate broadcast protocol based on the features of VANET has become a
crucial part of the development of VANET. This paper analyzes the disadvantage
of existing broadcast routing protocols in VANETs, and proposes an improved
algorithm (namely ODAM-C) based on the ODAM (Optimized Dissemination of Alarm
Messages) protocol. The ODAM-C algorithm improves the packet delivery rate by
two mechanisms based on the forwarding features of ODAM. The first
distance-based mechanism reduces the possibility of packet loss by considering
the angles between source nodes, forwarding nodes and receiving nodes. The
second mechanism increases the redundancy of forwarding nodes to guarantee the
packet success delivery ratio. We show by analysis and simulations that the
proposed algorithm can improve packet delivery rate for vehicular networks
compared against two widely-used existing protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6829</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6829</id><created>2013-12-24</created><authors><author><keyname>Qiu</keyname><forenames>Tie</forenames></author><author><keyname>Zhou</keyname><forenames>Yu</forenames></author><author><keyname>Xia</keyname><forenames>Feng</forenames></author><author><keyname>Jin</keyname><forenames>Naigao</forenames></author><author><keyname>Feng</keyname><forenames>Lin</forenames></author></authors><title>A Localization Strategy Based on N-times Trilateral Centroid with Weight</title><categories>cs.NI</categories><comments>International Journal of Communication Systems, 2012</comments><msc-class>68M14</msc-class><acm-class>C.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localization based on received signal strength indication (RSSI) is a low
cost and low complexity technology, and it is widely applied in distance-based
localization of wireless sensor networks (WSNs). Error of existed localization
technologies is significant. This paper presents the N-times trilateral
centroid weighted localization algorithm (NTCWLA), which can reduce the error
considerably. Considering the instability of RSSI, we use the weighted average
of many RSSIs as current RSSI. To improve the accuracy we select a number of
(no less than three) reliable beacon nodes to increase the localization times.
Then we calculate the distances between reliable beacon nodes and the mobile
node using an empirical formula. The mobile node is located N times using the
trilateral centroid algorithm. Finally, we take the weighted average of the
filtered reference coordinates as the mobile node's coordinates. We conduct
experiments with the STM32W108 chip which supports IEEE 802.15.4. The results
show that the proposed algorithm performs better than the trilateral centroid
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6832</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6832</id><created>2013-12-19</created><authors><author><keyname>Feinberg</keyname><forenames>Eugene A.</forenames></author><author><keyname>Huang</keyname><forenames>Jefferson</forenames></author></authors><title>The Value Iteration Algorithm is Not Strongly Polynomial for Discounted
  Dynamic Programming</title><categories>cs.AI math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note provides a simple example demonstrating that, if exact computations
are allowed, the number of iterations required for the value iteration
algorithm to find an optimal policy for discounted dynamic programming problems
may grow arbitrarily quickly with the size of the problem. In particular, the
number of iterations can be exponential in the number of actions. Thus, unlike
policy iterations, the value iteration algorithm is not strongly polynomial for
discounted dynamic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6833</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6833</id><created>2013-12-24</created><authors><author><keyname>Liu</keyname><forenames>Haifeng</forenames></author><author><keyname>Xia</keyname><forenames>Feng</forenames></author><author><keyname>Yang</keyname><forenames>Zhuo</forenames></author><author><keyname>Cao</keyname><forenames>Yang</forenames></author></authors><title>An Energy-Efficient Localization Strategy for Smartphones</title><categories>cs.NI</categories><msc-class>68M14</msc-class><acm-class>C.2; C.4</acm-class><journal-ref>Computer Science and Information Systems, Vol.8, No.4, 1117-1128,
  2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, smartphones have become prevalent. Much attention is being
paid to developing and making use of mobile applications that require position
information. The Global Positioning System (GPS) is a very popular localization
technique used by these applications because of its high accuracy. However, GPS
incurs an unacceptable energy consumption, which severely limits the use of
smartphones and reduces the battery lifetime. Then an urgent requirement for
these applications is a localization strategy that not only provides enough
accurate position information to meet users' need but also consumes less
energy. In this paper, we present an energy-efficient localization strategy for
smartphone applications. On one hand, it can dynamically estimate the next
localization time point to avoid unnecessary localization operations. On the
other hand, it can also automatically select the energy-optimal localization
method. We evaluate the strategy through a series of simulations. Experimental
results show that it can significantly reduce the localization energy
consumption of smartphones while ensuring a good satisfaction degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6834</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6834</id><created>2013-12-15</created><authors><author><keyname>Sree</keyname><forenames>P. Kiran</forenames></author><author><keyname>Babu</keyname><forenames>I. Ramesh</forenames></author></authors><title>Face Detection from still and Video Images using Unsupervised Cellular
  Automata with K means clustering algorithm</title><categories>cs.CV</categories><comments>ICGST-GVIP Journal, ISSN: 1687-398X, Volume 8, Issue 2, July 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pattern recognition problem rely upon the features inherent in the pattern of
images. Face detection and recognition is one of the challenging research areas
in the field of computer vision. In this paper, we present a method to identify
skin pixels from still and video images using skin color. Face regions are
identified from this skin pixel region. Facial features such as eyes, nose and
mouth are then located. Faces are recognized from color images using an RBF
based neural network. Unsupervised Cellular Automata with K means clustering
algorithm is used to locate different facial elements. Orientation is corrected
by using eyes. Parameters like inter eye distance, nose length, mouth position,
Discrete Cosine Transform (DCT) coefficients etc. are computed and used for a
Radial Basis Function (RBF) based neural network. This approach reliably works
for face sequence with orientation in head, expressions etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6836</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6836</id><created>2013-12-24</created><authors><author><keyname>Sonia</keyname></author><author><keyname>Singhal</keyname><forenames>Archana</forenames></author><author><keyname>Banati</keyname><forenames>Hema</forenames></author></authors><title>Fuzzy Logic Approach for Threat Prioritization in Agile Security
  Framework using DREAD Model</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a qualitative system sound security practices must be a crucial part
throughout the entire software lifecycle. Furthermore, agile software
development has paved the way for overcoming the problems faced by developers
during traditional development process. In the given paper we are using an
Agile Security Framework that is compatible with practices of agile processes
and inherit in it the benefits of security engineering activities in the form
of risk assessment and threat prioritization. One of the most popular
techniques to deal with ever growing risks associated with security threats is
DREAD model. It is used for rating risk of threats identified in the abuser
stories. In this model threats needs to be defined by sharp cutoffs. However,
such precise distribution is not suitable for risk categorization as risks are
vague in nature and deals with high level of uncertainty. In view of these risk
factors, our paper proposes a novel fuzzy approach using DREAD model for
computing risk level that ensures better evaluation of imprecise concepts. Thus
it provides the capacity to include subjectivity and uncertainty during risk
ranking. A case study has been presented to illustrate and compare the proposed
approach with the existing one using Matlab.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6837</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6837</id><created>2013-12-24</created><authors><author><keyname>Xia</keyname><forenames>Feng</forenames></author><author><keyname>Vinel</keyname><forenames>Alexey</forenames></author><author><keyname>Gao</keyname><forenames>Ruixia</forenames></author><author><keyname>Wang</keyname><forenames>Linqiang</forenames></author><author><keyname>Qiu</keyname><forenames>Tie</forenames></author></authors><title>Evaluating IEEE 802.15.4 for Cyber-Physical Systems</title><categories>cs.NI</categories><msc-class>68M12</msc-class><acm-class>C.2</acm-class><journal-ref>EURASIP Journal on Wireless Communications and Networking, Volume
  2011, Article ID 596397, 2011</journal-ref><doi>10.1155/2011/596397</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With rapid advancements in sensing, networking, and computing technologies,
recent years have witnessed the emergence of cyber-physical systems (CPS) in a
broad range of application domains. CPS is a new class of engineered systems
that features the integration of computation, communications, and control. In
contrast to general-purpose computing systems, many cyber-physical applications
are safety-cricial. These applications impose considerable requirements on
quality of service (QoS) of the employed networking infrastruture. Since IEEE
802.15.4 has been widely considered as a suitable protocol for CPS over
wireless sensor and actuator networks, it is of vital importance to evaluate
its performance extensively. Serving for this purpose, this paper will analyze
the performance of IEEE 802.15.4 standard operating in different modes
respectively. Extensive simulations have been conducted to examine how network
QoS will be impacted by some critical parameters. The results are presented and
analyzed, which provide some useful insights for network parameter
configuration and optimization for CPS design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6838</identifier>
 <datestamp>2013-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6838</id><created>2013-12-24</created><authors><author><keyname>Farahat</keyname><forenames>Ahmed K.</forenames></author><author><keyname>Elgohary</keyname><forenames>Ahmed</forenames></author><author><keyname>Ghodsi</keyname><forenames>Ali</forenames></author><author><keyname>Kamel</keyname><forenames>Mohamed S.</forenames></author></authors><title>Greedy Column Subset Selection for Large-scale Data Sets</title><categories>cs.DS cs.LG</categories><comments>Under consideration for publication in Knowledge and Information
  Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's information systems, the availability of massive amounts of data
necessitates the development of fast and accurate algorithms to summarize these
data and represent them in a succinct format. One crucial problem in big data
analytics is the selection of representative instances from large and
massively-distributed data, which is formally known as the Column Subset
Selection (CSS) problem. The solution to this problem enables data analysts to
understand the insights of the data and explore its hidden structure. The
selected instances can also be used for data preprocessing tasks such as
learning a low-dimensional embedding of the data points or computing a low-rank
approximation of the corresponding matrix. This paper presents a fast and
accurate greedy algorithm for large-scale column subset selection. The
algorithm minimizes an objective function which measures the reconstruction
error of the data matrix based on the subset of selected columns. The paper
first presents a centralized greedy algorithm for column subset selection which
depends on a novel recursive formula for calculating the reconstruction error
of the data matrix. The paper then presents a MapReduce algorithm which selects
a few representative columns from a matrix whose columns are massively
distributed across several commodity machines. The algorithm first learns a
concise representation of all columns using random projection, and it then
solves a generalized column subset selection problem at each machine in which a
subset of columns are selected from the sub-matrix on that machine such that
the reconstruction error of the concise representation is minimized. The paper
demonstrates the effectiveness and efficiency of the proposed algorithm through
an empirical evaluation on benchmark data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6843</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6843</id><created>2013-12-24</created><updated>2014-08-03</updated><authors><author><keyname>Lev</keyname><forenames>Nir</forenames></author><author><keyname>Peled</keyname><forenames>Ron</forenames></author><author><keyname>Peres</keyname><forenames>Yuval</forenames></author></authors><title>Separating signal from noise</title><categories>math.PR cs.IT math.CA math.IT math.ST stat.TH</categories><comments>46 pages. Minor improvements to the exposition in new version</comments><msc-class>60G35, 62M20, 93E11, 94A12, 94A13</msc-class><doi>10.1112/plms/pdu057</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that a sequence of numbers $x_n$ (a `signal') is transmitted through
a noisy channel. The receiver observes a noisy version of the signal with
additive random fluctuations, $x_n + \xi_n$, where $\xi_n$ is a sequence of
independent standard Gaussian random variables. Suppose further that the signal
is known to come from some fixed space of possible signals. Is it possible to
fully recover the transmitted signal from its noisy version? Is it possible to
at least detect that a non-zero signal was transmitted?
  In this paper we consider the case in which signals are infinite sequences
and the recovery or detection are required to hold with probability one. We
provide conditions on the signal space for checking whether detection or
recovery are possible. We also analyze in detail several examples including
spaces of Fourier transforms of measures, spaces with fixed amplitudes and the
space of almost periodic functions. Many of our examples exhibit critical
phenomena, in which a sharp transition is made from a regime in which recovery
is possible to a regime in which even detection is impossible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6849</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6849</id><created>2013-12-24</created><updated>2015-03-30</updated><authors><author><keyname>Ager</keyname><forenames>Matthew</forenames></author><author><keyname>Cvetkovic</keyname><forenames>Zoran</forenames></author><author><keyname>Sollich</keyname><forenames>Peter</forenames></author></authors><title>Speech Recognition Front End Without Information Loss</title><categories>cs.CL cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech representation and modelling in high-dimensional spaces of acoustic
waveforms, or a linear transformation thereof, is investigated with the aim of
improving the robustness of automatic speech recognition to additive noise. The
motivation behind this approach is twofold: (i) the information in acoustic
waveforms that is usually removed in the process of extracting low-dimensional
features might aid robust recognition by virtue of structured redundancy
analogous to channel coding, (ii) linear feature domains allow for exact noise
adaptation, as opposed to representations that involve non-linear processing
which makes noise adaptation challenging. Thus, we develop a generative
framework for phoneme modelling in high-dimensional linear feature domains, and
use it in phoneme classification and recognition tasks. Results show that
classification and recognition in this framework perform better than analogous
PLP and MFCC classifiers below 18 dB SNR. A combination of the high-dimensional
and MFCC features at the likelihood level performs uniformly better than either
of the individual representations across all noise levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6872</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6872</id><created>2013-12-17</created><authors><author><keyname>Gogna</keyname><forenames>Anupriya</forenames></author><author><keyname>Shukla</keyname><forenames>Ankita</forenames></author><author><keyname>Majumdar</keyname><forenames>Angshul</forenames></author></authors><title>Matrix recovery using Split Bregman</title><categories>cs.NA cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of recovering a matrix, with inherent
low rank structure, from its lower dimensional projections. This problem is
frequently encountered in wide range of areas including pattern recognition,
wireless sensor networks, control systems, recommender systems, image/video
reconstruction etc. Both in theory and practice, the most optimal way to solve
the low rank matrix recovery problem is via nuclear norm minimization. In this
paper, we propose a Split Bregman algorithm for nuclear norm minimization. The
use of Bregman technique improves the convergence speed of our algorithm and
gives a higher success rate. Also, the accuracy of reconstruction is much
better even for cases where small number of linear measurements are available.
Our claim is supported by empirical results obtained using our algorithm and
its comparison to other existing methods for matrix recovery. The algorithms
are compared on the basis of NMSE, execution time and success rate for varying
ranks and sampling ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6875</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6875</id><created>2013-12-24</created><authors><author><keyname>Altug</keyname><forenames>Yucel</forenames></author><author><keyname>Wagner</keyname><forenames>Aaron B.</forenames></author></authors><title>Refinement of the random coding bound</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. Inform. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An improved pre-factor for the random coding bound is proved. Specifically,
for channels with critical rate not equal to capacity, if a regularity
condition is satisfied (resp. not satisfied), then for any $\epsilon &gt;0$ a
pre-factor of $O(N^{-\frac{1}{2}\left( 1 - \epsilon + \bar{\rho}^\ast_R
\right)})$ (resp. $O(N^{-\frac{1}{2}})$) is achievable for rates above the
critical rate, where $N$ and $R$ is the blocklength and rate, respectively. The
extra term $\bar{\rho}^\ast_R$ is related to the slope of the random coding
exponent. Further, the relation of these bounds with the authors' recent
refinement of the sphere-packing bound, as well as the pre-factor for the
random coding bound below the critical rate, is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6885</identifier>
 <datestamp>2013-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6885</id><created>2013-12-24</created><authors><author><keyname>Huval</keyname><forenames>Brody</forenames></author><author><keyname>Coates</keyname><forenames>Adam</forenames></author><author><keyname>Ng</keyname><forenames>Andrew</forenames></author></authors><title>Deep learning for class-generic object detection</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the use of deep neural networks for the novel task of class
generic object detection. We show that neural networks originally designed for
image recognition can be trained to detect objects within images, regardless of
their class, including objects for which no bounding box labels have been
provided. In addition, we show that bounding box labels yield a 1% performance
increase on the ImageNet recognition challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6911</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6911</id><created>2013-12-24</created><updated>2014-01-04</updated><authors><author><keyname>Zhou</keyname><forenames>Tianqing</forenames></author><author><keyname>Huang</keyname><forenames>Yongming</forenames></author><author><keyname>Yang</keyname><forenames>Luxi</forenames></author></authors><title>QoS-Aware User Association for Load Balancing in Heterogeneous Cellular
  Networks</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To solve the problem that the low capacity in hot-spots and coverage holes of
conventional cellular networks, the base stations (BSs) having lower transmit
power are deployed to form heterogeneous cellular networks (HetNets). However,
because of these introduced disparate power BSs, the user distributions among
them looked fairly unbalanced if an appropriate user association scheme hasn't
been provided. For effectively tackling this problem, we jointly consider the
load of each BS and user's achievable rate instead of only utilizing the latter
when designing an association algorithm, and formulate it as a network-wide
weighted utility maximization problem. Note that, the load mentioned above
relates to the amount of required subbands decided by actual rate requirements,
i.e., QoS, but the number of associated users, thus it can reflect user's
actual load level. As for the proposed problem, we give a maximum probability
(max-probability) algorithm by relaxing variables as well as a low-complexity
distributed algorithm with a near-optimal solution that provides a theoretical
performance guarantee. Experimental results show that, compared with the
association strategy advocated by Ye, our strategy has a speeder convergence
rate, a lower call blocking probability and a higher load balancing level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6916</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6916</id><created>2013-12-24</created><authors><author><keyname>Morimoto</keyname><forenames>Takuya</forenames></author><author><keyname>Kanazawa</keyname><forenames>Takafumi</forenames></author><author><keyname>Ushio</keyname><forenames>Toshimitsu</forenames></author></authors><title>Game Theoretic Approach to the Stabilization of Heterogeneous Multiagent
  Systems Using Subsidy</title><categories>cs.GT</categories><comments>6 pages, IEEE Conference on Decision and Control, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multiagent system consisting of selfish and heterogeneous
agents. Its behavior is modeled by multipopulation replicator dynamics, where
payoff functions of populations are different from each other. In general,
there exist several equilibrium points in the replicator dynamics. In order to
stabilize a desirable equilibrium point, we introduce a controller called a
government which controls the behaviors of agents by offering them subsidies.
In previous work, it is assumed that the government determines the subsidies
based on the populations the agents belong to. In general, however, the
government cannot identify the members of each population. In this paper, we
assume that the government observes the action of each agent and determines the
subsidies based on the observed action profile. Then, we model the controlled
behaviors of the agents using replicator dynamics with feedback. We derive a
stabilization condition of the target equilibrium point in the replicator
dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6918</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6918</id><created>2013-12-24</created><authors><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author><author><keyname>Yuan</keyname><forenames>Di</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author></authors><title>Data Offloading in Load Coupled Networks: A Utility Maximization
  Framework</title><categories>cs.IT cs.NI math.IT</categories><comments>12 pages, accepted for publication in IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a general framework for the problem of data offloading in a
heterogeneous wireless network, where some demand of cellular users is served
by a complementary network. The complementary network is either a small-cell
network that shares the same resources as the cellular network, or a WiFi
network that uses orthogonal resources. For a given demand served in a cellular
network, the load, or the level of resource usage, of each cell depends in a
non-linear manner on the load of other cells due to the mutual coupling of
interference seen by one another. With load coupling, we optimize the demand to
be served in the cellular or the complementary networks, so as to maximize a
utility function. We consider three representative utility functions that
balance, to varying degrees, the revenue from serving the users vs the user
fairness. We establish conditions for which the optimization problem has a
feasible solution and is convex, and hence tractable to numerical computations.
Finally, we propose a strategy with theoretical justification to constrain the
load to some maximum value, as required for practical implementation. Numerical
studies are conducted for both under-loaded and over-loaded networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6927</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6927</id><created>2013-12-25</created><updated>2014-08-09</updated><authors><author><keyname>Zhou</keyname><forenames>Jianqin</forenames></author><author><keyname>Liu</keyname><forenames>Wanquan</forenames></author><author><keyname>Wang</keyname><forenames>Xifeng</forenames></author></authors><title>Structure Analysis on the $k$-error Linear Complexity for $2^n$-periodic
  Binary Sequences</title><categories>cs.CR cs.IT math.IT</categories><comments>19 pages. arXiv admin note: substantial text overlap with
  arXiv:1309.1829, arXiv:1310.0132, arXiv:1108.5793, arXiv:1112.6047</comments><msc-class>94A55, 94A60, 11B50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, in order to characterize the critical error linear complexity
spectrum (CELCS) for $2^n$-periodic binary sequences, we first propose a
decomposition based on the cube theory. Based on the proposed $k$-error cube
decomposition, and the famous inclusion-exclusion principle, we obtain the
complete characterization of $i$th descent point (critical point) of the
k-error linear complexity for $i=2,3$. Second, by using the sieve method and
Games-Chan algorithm, we characterize the second descent point (critical point)
distribution of the $k$-error linear complexity for $2^n$-periodic binary
sequences. As a consequence, we obtain the complete counting functions on the
$k$-error linear complexity of $2^n$-periodic binary sequences as the second
descent point for $k=3,4$. This is the first time for the second and the third
descent points to be completely characterized. In fact, the proposed
constructive approach has the potential to be used for constructing
$2^n$-periodic binary sequences with the given linear complexity and $k$-error
linear complexity (or CELCS), which is a challenging problem to be deserved for
further investigation in future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6931</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6931</id><created>2013-12-25</created><authors><author><keyname>Zhao</keyname><forenames>Dawei</forenames></author><author><keyname>Li</keyname><forenames>Lixiang</forenames></author><author><keyname>Peng</keyname><forenames>Haipeng</forenames></author><author><keyname>Luo</keyname><forenames>Qun</forenames></author><author><keyname>Yang</keyname><forenames>Yixian</forenames></author></authors><title>Multiple routes transmitted epidemics on multiplex networks</title><categories>cs.SI physics.soc-ph</categories><comments>arXiv admin note: substantial text overlap with arXiv:1307.1834</comments><doi>10.1016/j.physleta.2014.01.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter investigates the multiple routes transmitted epidemic process on
multiplex networks. We propose detailed theoretical analysis that allows us to
accurately calculate the epidemic threshold and outbreak size. It is found that
the epidemic can spread across the multiplex network even if all the network
layers are well below their respective epidemic thresholds. Strong positive
degree-degree correlation of nodes in multiplex network could lead to a much
lower epidemic threshold and a relatively smaller outbreak size. However, the
average similarity of neighbors from different layers of nodes has no obvious
effect on the epidemic threshold and outbreak size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6934</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6934</id><created>2013-12-25</created><authors><author><keyname>Ferdousi</keyname><forenames>Arifa</forenames></author><author><keyname>Khan</keyname><forenames>Sadeque Reza</forenames></author></authors><title>Hardware and logic implementation of multiple alarm system for GSM BTS
  rooms</title><categories>cs.SY</categories><journal-ref>International Journal of Information Technology, Modeling and
  Computing(ijitmc) vol.1(4) (2013) 51-58</journal-ref><doi>10.5121/ijitmc.2013.1406</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular communication becomes the major mode of communication in present
century. With the development of this phase of communication the globalization
process is also in its peak of speed. The development of cellular communication
is largely depending on the improvement and stability of Base Transceiver
Station (BTS) room. So for the purpose of the development of cellular
communication a large numbered BTS rooms are installed throughout the world. To
ensure proper support from BTS rooms there must be a security system to avoid
any unnecessary vulnerability. Therefore multiple alarm system is designed to
secure the BTS rooms from any undesired circumstances. This system is designed
with a PIC Microcontroller as a main controller and a several sensors are
interfaced with it to provide high temperature alarm, smoke alarm, door alarm
and water alarm. All these alarms are interfaced with the alarm box in the BTS
room which provides the current status directly to Network Management Centre
(NMC) of a Global System for Mobile (GSM) communication network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6935</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6935</id><created>2013-12-25</created><authors><author><keyname>Malesevic</keyname><forenames>Branko</forenames></author><author><keyname>Obradovic</keyname><forenames>Ratko</forenames></author><author><keyname>Banjac</keyname><forenames>Bojan</forenames></author><author><keyname>Jovovic</keyname><forenames>Ivana</forenames></author><author><keyname>Makragic</keyname><forenames>Milica</forenames></author></authors><title>Application of polynomial texture mapping in process of digitalization
  of cultural heritage</title><categories>cs.GR cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present modern texture mapping techniques and several
applications of polynomial texture mapping in cultural heritage programs. We
also consider some well-known and some new methods for mathematical procedure
that is involved in generation of polynomial texture maps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6936</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6936</id><created>2013-12-25</created><authors><author><keyname>Ferdousi</keyname><forenames>Arifa</forenames></author><author><keyname>Enam</keyname><forenames>Farhana</forenames></author><author><keyname>Khan</keyname><forenames>Sadeque Reza</forenames></author></authors><title>The performance evaluation of IEEE 802.16 physical layer in the basis of
  bit error rate considering reference channel models</title><categories>cs.NI cs.IT math.IT</categories><comments>International Journal on Cybernetics &amp; Informatics (IJCI) Vol.2,
  No.4, August 2013</comments><doi>10.5121/ijci.2013.2403</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fixed Broadband Wireless Access is a promising technology which can offer
high speed data rate from transmitting end to customer end which can offer high
speed text, voice, and video data. IEEE 802.16 WirelessMAN is a standard that
specifies medium access control layer and a set of PHY layer to fixed and
mobile BWA in broad range of frequencies and it supports equipment
manufacturers due to its robust performance in multipath environment.
Consequently WiMAX forum has adopted this version to develop the network world
wide. In this paper the performance of IEEE 802.16 OFDM PHY Layer has been
investigated by using the simulation model in Matlab. The Stanford University
Interim (SUI) channel models are selected for the performance evaluation of
this standard. The Ideal Channel estimation is considered in this work and the
performance evaluation is observed in the basis of BER.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6939</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6939</id><created>2013-12-25</created><authors><author><keyname>Altahat</keyname><forenames>Zaid</forenames></author><author><keyname>Elrad</keyname><forenames>Tzilla</forenames></author><author><keyname>Tahat</keyname><forenames>Luay</forenames></author><author><keyname>Almasri</keyname><forenames>Nada</forenames></author></authors><title>Detection of Syntactic Aspect Interaction in UML State Diagrams Using
  Critical Pair Analysis in Graph Transformation</title><categories>cs.SE</categories><journal-ref>IJCSI Volume 10, Issue 5, September 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aspect Oriented Modeling separates crosscutting concerns by defining Aspects
and composition mechanisms at the model level. Composition of multiple Aspects
will most likely result in more than one Aspect matching the same join points.
Consequently, Aspects do not always interact in a predictable manner when woven
together. Intended interaction among aspects is designed by the system
designer. Unintended interaction (or interference) must be automatically
managed. When the woven aspect demonstrates a behavior that is different than
its autonomous behavior, then this is a potential interference. Interference
has been recently reported in Aspect Oriented Software Development (AOSD) by
the industry. Leaving this problem unsolved may result in erratic software
behavior and will hinder the adaptation of AOSD by the industry. This
identified problem is similar to a phenomenon that exists in graph
transformation systems where multiple Graph Transformation rules share some
conflicting elements, it is referred to as Critical Pair Analysis and it
provides an algebraic-based mechanism to detect and analyze the interaction of
the rules. In this paper we propose a framework to detect unintended Aspect
interaction at the model level. The proposed framework transforms Aspects
modeled in UML State Diagram to Graph Transformation Rules, and then it applies
Critical Pair Analysis to detect unintended interactions among aspects. This
will enable developers to specify only the order of precedence for intended
interaction among aspects without the need to manually investigate unintended
interactions for the combinations of every Aspect to every other Aspect in the
system. The proposed interaction detection solution is automated, modular, and
independent of the base model; which adds the advantage of not having to
re-evaluate the interaction each time the base model changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6945</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6945</id><created>2013-12-25</created><authors><author><keyname>Chen</keyname><forenames>Chunlin</forenames></author><author><keyname>Dong</keyname><forenames>Daoyi</forenames></author><author><keyname>Qi</keyname><forenames>Bo</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Rabitz</keyname><forenames>Herschel</forenames></author></authors><title>Quantum Ensemble Classification: A Sampling-based Learning Control
  Approach</title><categories>quant-ph cs.SY</categories><comments>32 pages, 15 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum ensemble classification has significant applications in
discrimination of atoms (or molecules), separation of isotopic molecules and
quantum information extraction. However, quantum mechanics forbids
deterministic discrimination among nonorthogonal states. The classification of
inhomogeneous quantum ensembles is very challenging since there exist
variations in the parameters characterizing the members within different
classes. In this paper, we recast quantum ensemble classification as a
supervised quantum learning problem. A systematic classification methodology is
presented by using a sampling-based learning control (SLC) approach for quantum
discrimination. The classification task is accomplished via simultaneously
steering members belonging to different classes to their corresponding target
states (e.g., mutually orthogonal states). Firstly a new discrimination method
is proposed for two similar quantum systems. Then an SLC method is presented
for quantum ensemble classification. Numerical results demonstrate the
effectiveness of the proposed approach for the binary classification of
two-level quantum ensembles and the multiclass classification of multilevel
quantum ensembles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6947</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6947</id><created>2013-12-25</created><updated>2016-03-08</updated><authors><author><keyname>Dasgupta</keyname><forenames>Sourish</forenames></author><author><keyname>Padia</keyname><forenames>Ankur</forenames></author><author><keyname>Shah</keyname><forenames>Kushal</forenames></author><author><keyname>Majumder</keyname><forenames>Prasenjit</forenames></author></authors><title>Formal Ontology Learning on Factual IS-A Corpus in English using
  Description Logics</title><categories>cs.CL cs.AI</categories><comments>This paper has been withdrawn by the author due to requirement of
  re-evaluation of results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ontology Learning (OL) is the computational task of generating a knowledge
base in the form of an ontology given an unstructured corpus whose content is
in natural language (NL). Several works can be found in this area most of which
are limited to statistical and lexico-syntactic pattern matching based
techniques Light-Weight OL. These techniques do not lead to very accurate
learning mostly because of several linguistic nuances in NL. Formal OL is an
alternative (less explored) methodology were deep linguistics analysis is made
using theory and tools found in computational linguistics to generate formal
axioms and definitions instead simply inducing a taxonomy. In this paper we
propose &quot;Description Logic (DL)&quot; based formal OL framework for learning factual
IS-A type sentences in English. We claim that semantic construction of IS-A
sentences is non trivial. Hence, we also claim that such sentences requires
special studies in the context of OL before any truly formal OL can be
proposed. We introduce a learner tool, called DLOL_IS-A, that generated such
ontologies in the owl format. We have adopted &quot;Gold Standard&quot; based OL
evaluation on IS-A rich WCL v.1.1 dataset and our own Community representative
IS-A dataset. We observed significant improvement of DLOL_IS-A when compared to
the light-weight OL tool Text2Onto and formal OL tool FRED.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6948</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6948</id><created>2013-12-25</created><authors><author><keyname>Dasgupta</keyname><forenames>Sourish</forenames></author><author><keyname>KaPatel</keyname><forenames>Rupali</forenames></author><author><keyname>Padia</keyname><forenames>Ankur</forenames></author><author><keyname>Shah</keyname><forenames>Kushal</forenames></author></authors><title>Description Logics based Formalization of Wh-Queries</title><categories>cs.CL cs.AI</categories><comments>Natural Language Query Processing, Representation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of Natural Language Query Formalization (NLQF) is to translate a
given user query in natural language (NL) into a formal language so that the
semantic interpretation has equivalence with the NL interpretation.
Formalization of NL queries enables logic based reasoning during information
retrieval, database query, question-answering, etc. Formalization also helps in
Web query normalization and indexing, query intent analysis, etc. In this paper
we are proposing a Description Logics based formal methodology for wh-query
intent (also called desire) identification and corresponding formal
translation. We evaluated the scalability of our proposed formalism using
Microsoft Encarta 98 query dataset and OWL-S TC v.4.0 dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6949</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6949</id><created>2013-12-25</created><authors><author><keyname>Wang</keyname><forenames>Taotao</forenames></author><author><keyname>Liew</keyname><forenames>Soung Chang</forenames></author></authors><title>Joint Phase Tracking and Channel Decoding for OFDM Physical-Layer
  Network Coding</title><categories>cs.IT math.IT</categories><comments>7 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of joint phase tracking and channel
decoding in OFDM based Physical-layer Network Coding (PNC) systems. OFDM
signaling can obviate the need for tight time synchronization among multiple
simultaneous transmissions in the uplink of PNC systems. However, OFDM PNC
systems are susceptible to phase drifts caused by residual carrier frequency
offsets (CFOs). In the traditional OFDM system in which a receiver receives
from only one transmitter, pilot tones are employed to aid phase tracking. In
OFDM PNC systems, multiple transmitters transmit to a receiver, and these pilot
tones must be shared among the multiple transmitters. This reduces the number
of pilots that can be used by each transmitting node. Phase tracking in OFDM
PNC is more challenging as a result. To overcome the degradation due to the
reduced number of per-node pilots, this work supplements the pilots with the
channel information contained in the data. In particular, we propose to solve
the problems of phase tracking and channel decoding jointly. Our solution
consists of the use of the expectation-maximization (EM) algorithm for phase
tracking and the use of the belief propagation (BP) algorithm for channel
decoding. The two problems are solved jointly through iterative processing
between the EM and BP algorithms. Simulations and real experiments based on
software-defined radio show that the proposed method can improve phase tracking
as well as channel decoding performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6954</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6954</id><created>2013-12-25</created><authors><author><keyname>Brzezinski</keyname><forenames>Michal</forenames></author></authors><title>Empirical modeling of the impact factor distribution</title><categories>cs.DL physics.soc-ph stat.AP</categories><comments>14 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The distribution of impact factors has been modeled in the recent informetric
literature using two-exponent law proposed by Mansilla et al. (2007). This
paper shows that two distributions widely-used in economics, namely the Dagum
and Singh-Maddala models, possess several advantages over the two-exponent
model. Compared to the latter, the former give as good as or slightly better
fit to data on impact factors in eight important scientific fields. In contrast
to the two-exponent model, both proposed distributions have closed-from
probability density functions and cumulative distribution functions, which
facilitates fitting these distributions to data and deriving their statistical
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6956</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6956</id><created>2013-12-25</created><authors><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author><author><keyname>Mohammed</keyname><forenames>Samer</forenames></author><author><keyname>Trabelsi</keyname><forenames>Dorra</forenames></author><author><keyname>Oukhellou</keyname><forenames>Latifa</forenames></author><author><keyname>Amirat</keyname><forenames>Yacine</forenames></author></authors><title>Joint segmentation of multivariate time series with hidden process
  regression for human activity recognition</title><categories>stat.ML cs.LG</categories><journal-ref>Neurocomputing, Volume 120, Pages 633-644, November 2013</journal-ref><doi>10.1016/j.neucom.2013.04.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of human activity recognition is central for understanding and
predicting the human behavior, in particular in a prospective of assistive
services to humans, such as health monitoring, well being, security, etc. There
is therefore a growing need to build accurate models which can take into
account the variability of the human activities over time (dynamic models)
rather than static ones which can have some limitations in such a dynamic
context. In this paper, the problem of activity recognition is analyzed through
the segmentation of the multidimensional time series of the acceleration data
measured in the 3-d space using body-worn accelerometers. The proposed model
for automatic temporal segmentation is a specific statistical latent process
model which assumes that the observed acceleration sequence is governed by
sequence of hidden (unobserved) activities. More specifically, the proposed
approach is based on a specific multiple regression model incorporating a
hidden discrete logistic process which governs the switching from one activity
to another over time. The model is learned in an unsupervised context by
maximizing the observed-data log-likelihood via a dedicated
expectation-maximization (EM) algorithm. We applied it on a real-world
automatic human activity recognition problem and its performance was assessed
by performing comparisons with alternative approaches, including well-known
supervised static classifiers and the standard hidden Markov model (HMM). The
obtained results are very encouraging and show that the proposed approach is
quite competitive even it works in an entirely unsupervised way and does not
requires a feature extraction preprocessing step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6962</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6962</id><created>2013-12-25</created><authors><author><keyname>Kamal</keyname><forenames>Ahmad</forenames></author></authors><title>Subjectivity Classification using Machine Learning Techniques for Mining
  Feature-Opinion Pairs from Web Opinion Sources</title><categories>cs.IR cs.CL cs.LG</categories><comments>10 pages, 2 Color Photographs, 1 Diagram, 14 Charts, 2 Graphs,
  International Journal of Computer Science Issues (IJCSI), Vol. 10, Issue 5,
  No 1, September 2013</comments><journal-ref>International Journal of Computer Science Issues (IJCSI), Volume
  10 Issue 5, 2013, pp 191-200</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to flourish of the Web 2.0, web opinion sources are rapidly emerging
containing precious information useful for both customers and manufactures.
Recently, feature based opinion mining techniques are gaining momentum in which
customer reviews are processed automatically for mining product features and
user opinions expressed over them. However, customer reviews may contain both
opinionated and factual sentences. Distillations of factual contents improve
mining performance by preventing noisy and irrelevant extraction. In this
paper, combination of both supervised machine learning and rule-based
approaches are proposed for mining feasible feature-opinion pairs from
subjective review sentences. In the first phase of the proposed approach, a
supervised machine learning technique is applied for classifying subjective and
objective sentences from customer reviews. In the next phase, a rule based
method is implemented which applies linguistic and semantic analysis of texts
to mine feasible feature-opinion pairs from subjective sentences retained after
the first phase. The effectiveness of the proposed methods is established
through experimentation over customer reviews on different electronic products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6965</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6965</id><created>2013-12-25</created><authors><author><keyname>Trabelsi</keyname><forenames>Dorra</forenames></author><author><keyname>Mohammed</keyname><forenames>Samer</forenames></author><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author><author><keyname>Oukhellou</keyname><forenames>Latifa</forenames></author><author><keyname>Amirat</keyname><forenames>Yacine</forenames></author></authors><title>An Unsupervised Approach for Automatic Activity Recognition based on
  Hidden Markov Model Regression</title><categories>stat.ML cs.CV cs.LG</categories><journal-ref>IEEE Transactions on Automation Science and Engineering, Volume:
  10, Issue: 3, July 2013, Pages: 829-835</journal-ref><doi>10.1109/TASE.2013.2256349</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using supervised machine learning approaches to recognize human activities
from on-body wearable accelerometers generally requires a large amount of
labelled data. When ground truth information is not available, too expensive,
time consuming or difficult to collect, one has to rely on unsupervised
approaches. This paper presents a new unsupervised approach for human activity
recognition from raw acceleration data measured using inertial wearable
sensors. The proposed method is based upon joint segmentation of
multidimensional time series using a Hidden Markov Model (HMM) in a multiple
regression context. The model is learned in an unsupervised framework using the
Expectation-Maximization (EM) algorithm where no activity labels are needed.
The proposed method takes into account the sequential appearance of the data.
It is therefore adapted for the temporal acceleration data to accurately detect
the activities. It allows both segmentation and classification of the human
activities. Experimental results are provided to demonstrate the efficiency of
the proposed approach with respect to standard supervised and unsupervised
classification approaches
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6966</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6966</id><created>2013-12-25</created><authors><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author><author><keyname>Glotin</keyname><forenames>Herv&#xe9;</forenames></author><author><keyname>Sam&#xe9;</keyname><forenames>Allou</forenames></author></authors><title>Model-based functional mixture discriminant analysis with hidden process
  regression for curve classification</title><categories>stat.ME cs.LG math.ST stat.ML stat.TH</categories><journal-ref>Neurocomputing, Volume 112, Pages 153-163, July 2013</journal-ref><doi>10.1016/j.neucom.2012.10.030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the modeling and the classification of functional
data presenting regime changes over time. We propose a new model-based
functional mixture discriminant analysis approach based on a specific hidden
process regression model that governs the regime changes over time. Our
approach is particularly adapted to handle the problem of complex-shaped
classes of curves, where each class is potentially composed of several
sub-classes, and to deal with the regime changes within each homogeneous
sub-class. The proposed model explicitly integrates the heterogeneity of each
class of curves via a mixture model formulation, and the regime changes within
each sub-class through a hidden logistic process. Each class of complex-shaped
curves is modeled by a finite number of homogeneous clusters, each of them
being decomposed into several regimes. The model parameters of each class are
learned by maximizing the observed-data log-likelihood by using a dedicated
expectation-maximization (EM) algorithm. Comparisons are performed with
alternative curve classification approaches, including functional linear
discriminant analysis and functional mixture discriminant analysis with
polynomial regression mixtures and spline regression mixtures. Results obtained
on simulated data and real data show that the proposed approach outperforms the
alternative approaches in terms of discrimination, and significantly improves
the curves approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6967</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6967</id><created>2013-12-25</created><authors><author><keyname>Sam&#xe9;</keyname><forenames>Allou</forenames></author><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author><author><keyname>Govaert</keyname><forenames>G&#xe9;rard</forenames></author><author><keyname>Aknin</keyname><forenames>Patrice</forenames></author></authors><title>Model-based clustering and segmentation of time series with changes in
  regime</title><categories>stat.ME cs.LG math.ST stat.ML stat.TH</categories><journal-ref>Advances in Data Analysis and Classification, December 2011,
  Volume 5, Issue 4, pp 301-321</journal-ref><doi>10.1007/s11634-011-0096-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixture model-based clustering, usually applied to multidimensional data, has
become a popular approach in many data analysis problems, both for its good
statistical properties and for the simplicity of implementation of the
Expectation-Maximization (EM) algorithm. Within the context of a railway
application, this paper introduces a novel mixture model for dealing with time
series that are subject to changes in regime. The proposed approach consists in
modeling each cluster by a regression model in which the polynomial
coefficients vary according to a discrete hidden process. In particular, this
approach makes use of logistic functions to model the (smooth or abrupt)
transitions between regimes. The model parameters are estimated by the maximum
likelihood method solved by an Expectation-Maximization algorithm. The proposed
approach can also be regarded as a clustering approach which operates by
finding groups of time series having common changes in regime. In addition to
providing a time series partition, it therefore provides a time series
segmentation. The problem of selecting the optimal numbers of clusters and
segments is solved by means of the Bayesian Information Criterion (BIC). The
proposed approach is shown to be efficient using a variety of simulated time
series and real-world time series of electrical power consumption from rail
switching operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6968</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6968</id><created>2013-12-25</created><authors><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author><author><keyname>Sam&#xe9;</keyname><forenames>Allou</forenames></author><author><keyname>Govaert</keyname><forenames>G&#xe9;rard</forenames></author><author><keyname>Aknin</keyname><forenames>Patrice</forenames></author></authors><title>A hidden process regression model for functional data description.
  Application to curve discrimination</title><categories>stat.ME cs.LG stat.ML</categories><journal-ref>Neurocomputing, Volume 73, Issues 7-9, March 2010, Pages 1210-1221</journal-ref><doi>10.1016/j.neucom.2009.12.023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach for functional data description is proposed in this paper. It
consists of a regression model with a discrete hidden logistic process which is
adapted for modeling curves with abrupt or smooth regime changes. The model
parameters are estimated in a maximum likelihood framework through a dedicated
Expectation Maximization (EM) algorithm. From the proposed generative model, a
curve discrimination rule is derived using the Maximum A Posteriori rule. The
proposed model is evaluated using simulated curves and real world curves
acquired during railway switch operations, by performing comparisons with the
piecewise regression approach in terms of curve modeling and classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6969</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6969</id><created>2013-12-25</created><authors><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author><author><keyname>Sam&#xe9;</keyname><forenames>Allou</forenames></author><author><keyname>Govaert</keyname><forenames>G&#xe9;rard</forenames></author><author><keyname>Aknin</keyname><forenames>Patrice</forenames></author></authors><title>Time series modeling by a regression approach based on a latent process</title><categories>stat.ME cs.LG math.ST stat.ML stat.TH</categories><journal-ref>Neural Networks 22(5-6): 593-602 (2009)</journal-ref><doi>10.1016/j.neunet.2009.06.040</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time series are used in many domains including finance, engineering,
economics and bioinformatics generally to represent the change of a measurement
over time. Modeling techniques may then be used to give a synthetic
representation of such data. A new approach for time series modeling is
proposed in this paper. It consists of a regression model incorporating a
discrete hidden logistic process allowing for activating smoothly or abruptly
different polynomial regression models. The model parameters are estimated by
the maximum likelihood method performed by a dedicated Expectation Maximization
(EM) algorithm. The M step of the EM algorithm uses a multi-class Iterative
Reweighted Least-Squares (IRLS) algorithm to estimate the hidden process
parameters. To evaluate the proposed approach, an experimental study on
simulated data and real world data was performed using two alternative
approaches: a heteroskedastic piecewise regression model using a global
optimization algorithm based on dynamic programming, and a Hidden Markov
Regression Model whose parameters are estimated by the Baum-Welch algorithm.
Finally, in the context of the remote monitoring of components of the French
railway infrastructure, and more particularly the switch mechanism, the
proposed approach has been applied to modeling and classifying time series
representing the condition measurements acquired during switch operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6974</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6974</id><created>2013-12-25</created><updated>2014-04-30</updated><authors><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author></authors><title>Piecewise regression mixture for simultaneous functional data clustering
  and optimal segmentation</title><categories>stat.ME cs.LG math.ST stat.ML stat.TH</categories><comments>preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel mixture model-based approach for simultaneous
clustering and optimal segmentation of functional data which are curves
presenting regime changes. The proposed model consists in a finite mixture of
piecewise polynomial regression models. Each piecewise polynomial regression
model is associated with a cluster, and within each cluster, each piecewise
polynomial component is associated with a regime (i.e., a segment). We derive
two approaches for learning the model parameters. The former is an estimation
approach and consists in maximizing the observed-data likelihood via a
dedicated expectation-maximization (EM) algorithm. A fuzzy partition of the
curves in K clusters is then obtained at convergence by maximizing the
posterior cluster probabilities. The latter however is a classification
approach and optimizes a specific classification likelihood criterion through a
dedicated classification expectation-maximization (CEM) algorithm. The optimal
curve segmentation is performed by using dynamic programming. In the
classification approach, both the curve clustering and the optimal segmentation
are performed simultaneously as the CEM learning proceeds. We show that the
classification approach is the probabilistic version that generalizes the
deterministic K-means-like algorithm proposed in H\'ebrail et al. (2010). The
proposed approach is evaluated using simulated curves and real-world curves.
Comparisons with alternatives including regression mixture models and the
K-means like algorithm for piecewise regression demonstrate the effectiveness
of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6978</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6978</id><created>2013-12-25</created><authors><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author><author><keyname>Sam&#xe9;</keyname><forenames>Allou</forenames></author><author><keyname>Govaert</keyname><forenames>G&#xe9;rard</forenames></author><author><keyname>Aknin</keyname><forenames>Patrice</forenames></author></authors><title>Mod\`ele \`a processus latent et algorithme EM pour la r\'egression non
  lin\'eaire</title><categories>math.ST cs.LG stat.ME stat.ML stat.TH</categories><journal-ref>Revue des Nouvelles Technologies de l'Information (RNTI),
  Statistique et nouvelles technologies de l'information (2011) 15-32</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A non linear regression approach which consists of a specific regression
model incorporating a latent process, allowing various polynomial regression
models to be activated preferentially and smoothly, is introduced in this
paper. The model parameters are estimated by maximum likelihood performed via a
dedicated expecation-maximization (EM) algorithm. An experimental study using
simulated and real data sets reveals good performances of the proposed
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6994</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6994</id><created>2013-12-25</created><authors><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author><author><keyname>Sam&#xe9;</keyname><forenames>Allou</forenames></author><author><keyname>Govaert</keyname><forenames>G&#xe9;rard</forenames></author><author><keyname>Aknin</keyname><forenames>Patrice</forenames></author></authors><title>A regression model with a hidden logistic process for signal
  parametrization</title><categories>stat.ME cs.LG stat.ML</categories><comments>In Proceedings of the XVIIth European Symposium on Artificial Neural
  Networks, Computational Intelligence and Machine Learning (ESANN), Pages
  503-508, 2009, Bruges, Belgium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach for signal parametrization, which consists of a specific
regression model incorporating a discrete hidden logistic process, is proposed.
The model parameters are estimated by the maximum likelihood method performed
by a dedicated Expectation Maximization (EM) algorithm. The parameters of the
hidden logistic process, in the inner loop of the EM algorithm, are estimated
using a multi-class Iterative Reweighted Least-Squares (IRLS) algorithm. An
experimental study using simulated and real data reveals good performances of
the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6995</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6995</id><created>2013-12-25</created><updated>2014-07-23</updated><authors><author><keyname>Bhattacharya</keyname><forenames>Sourav</forenames></author><author><keyname>Nurmi</keyname><forenames>Petteri</forenames></author><author><keyname>Hammerla</keyname><forenames>Nils</forenames></author><author><keyname>Pl&#xf6;tz</keyname><forenames>Thomas</forenames></author></authors><title>Towards Using Unlabeled Data in a Sparse-coding Framework for Human
  Activity Recognition</title><categories>cs.LG cs.AI stat.ML</categories><comments>18 pages, 12 figures, Pervasive and Mobile Computing, 2014</comments><doi>10.1016/j.pmcj.2014.05.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a sparse-coding framework for activity recognition in ubiquitous
and mobile computing that alleviates two fundamental problems of current
supervised learning approaches. (i) It automatically derives a compact, sparse
and meaningful feature representation of sensor data that does not rely on
prior expert knowledge and generalizes extremely well across domain boundaries.
(ii) It exploits unlabeled sample data for bootstrapping effective activity
recognizers, i.e., substantially reduces the amount of ground truth annotation
required for model estimation. Such unlabeled data is trivial to obtain, e.g.,
through contemporary smartphones carried by users as they go about their
everyday activities.
  Based on the self-taught learning paradigm we automatically derive an
over-complete set of basis vectors from unlabeled data that captures inherent
patterns present within activity data. Through projecting raw sensor data onto
the feature space defined by such over-complete sets of basis vectors effective
feature extraction is pursued. Given these learned feature representations,
classification backends are then trained using small amounts of labeled
training data.
  We study the new approach in detail using two datasets which differ in terms
of the recognition tasks and sensor modalities. Primarily we focus on
transportation mode analysis task, a popular task in mobile-phone based
sensing. The sparse-coding framework significantly outperforms the
state-of-the-art in supervised learning approaches. Furthermore, we demonstrate
the great practical potential of the new approach by successfully evaluating
its generalization capabilities across both domain and sensor modalities by
considering the popular Opportunity dataset. Our feature learning approach
outperforms state-of-the-art approaches to analyzing activities in daily
living.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.6996</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.6996</id><created>2013-12-25</created><authors><author><keyname>Karim</keyname><forenames>Muhammad Rezaul</forenames></author></authors><title>A New Approach to Constraint Weight Learning for Variable Ordering in
  CSPs</title><categories>cs.AI</categories><journal-ref>Proceedings of the IEEE Congress on Evolutionary Computation (CEC
  2014), pp. 2716-2723, Beijing, China, July 6-11, 2014</journal-ref><doi>10.1109/CEC.2014.6900262</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Constraint Satisfaction Problem (CSP) is a framework used for modeling and
solving constrained problems. Tree-search algorithms like backtracking try to
construct a solution to a CSP by selecting the variables of the problem one
after another. The order in which these algorithm select the variables
potentially have significant impact on the search performance. Various
heuristics have been proposed for choosing good variable ordering. Many
powerful variable ordering heuristics weigh the constraints first and then
utilize the weights for selecting good order of the variables. Constraint
weighting are basically employed to identify global bottlenecks in a CSP.
  In this paper, we propose a new approach for learning weights for the
constraints using competitive coevolutionary Genetic Algorithm (GA). Weights
learned by the coevolutionary GA later help to make better choices for the
first few variables in a search. In the competitive coevolutionary GA,
constraints and candidate solutions for a CSP evolve together through an
inverse fitness interaction process. We have conducted experiments on several
random, quasi-random and patterned instances to measure the efficiency of the
proposed approach. The results and analysis show that the proposed approach is
good at learning weights to distinguish the hard constraints for quasi-random
instances and forced satisfiable random instances generated with the Model RB.
For other type of instances, RNDI still seems to be the best approach as our
experiments show.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7001</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7001</id><created>2013-12-25</created><authors><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author><author><keyname>Sam&#xe9;</keyname><forenames>Allou</forenames></author><author><keyname>Govaert</keyname><forenames>G&#xe9;rard</forenames></author><author><keyname>Aknin</keyname><forenames>Patrice</forenames></author></authors><title>A regression model with a hidden logistic process for feature extraction
  from time series</title><categories>stat.ME cs.LG math.ST stat.ML stat.TH</categories><comments>In Proceedings of the International Joint Conference on Neural
  Networks (IJCNN), 2009, Atlanta, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach for feature extraction from time series is proposed in this
paper. This approach consists of a specific regression model incorporating a
discrete hidden logistic process. The model parameters are estimated by the
maximum likelihood method performed by a dedicated Expectation Maximization
(EM) algorithm. The parameters of the hidden logistic process, in the inner
loop of the EM algorithm, are estimated using a multi-class Iterative
Reweighted Least-Squares (IRLS) algorithm. A piecewise regression algorithm and
its iterative variant have also been considered for comparisons. An
experimental study using simulated and real data reveals good performances of
the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7003</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7003</id><created>2013-12-25</created><authors><author><keyname>Onanena</keyname><forenames>Ra&#xef;ssa</forenames></author><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author><author><keyname>Oukhellou</keyname><forenames>Latifa</forenames></author><author><keyname>Candusso</keyname><forenames>Denis</forenames></author><author><keyname>Aknin</keyname><forenames>Patrice</forenames></author><author><keyname>Hissel</keyname><forenames>Daniel</forenames></author></authors><title>Supervised learning of a regression model based on latent process.
  Application to the estimation of fuel cell life time</title><categories>stat.ML cs.LG stat.AP</categories><comments>In Proceeding of the 8th IEEE International Conference on Machine
  Learning and Applications (IEEE ICMLA'09), pages 632-637, 2009, Miami Beach,
  FL, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a pattern recognition approach aiming to estimate fuel
cell duration time from electrochemical impedance spectroscopy measurements. It
consists in first extracting features from both real and imaginary parts of the
impedance spectrum. A parametric model is considered in the case of the real
part, whereas regression model with latent variables is used in the latter
case. Then, a linear regression model using different subsets of extracted
features is used fo r the estimation of fuel cell time duration. The
performances of the proposed approach are evaluated on experimental data set to
show its feasibility. This could lead to interesting perspectives for
predictive maintenance policy of fuel cell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7006</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7006</id><created>2013-12-25</created><updated>2015-02-13</updated><authors><author><keyname>Chen</keyname><forenames>Yudong</forenames></author><author><keyname>Yi</keyname><forenames>Xinyang</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author></authors><title>A Convex Formulation for Mixed Regression with Two Components: Minimax
  Optimal Rates</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>Added results on minimax lower bounds, which match our upper bounds
  on recovery errors up to log factors. Appeared in the Conference on Learning
  Theory (COLT), 2014. (JMLR W&amp;CP 35 :560-604, 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the mixed regression problem with two components, under
adversarial and stochastic noise. We give a convex optimization formulation
that provably recovers the true solution, and provide upper bounds on the
recovery errors for both arbitrary noise and stochastic noise settings. We also
give matching minimax lower bounds (up to log factors), showing that under
certain assumptions, our algorithm is information-theoretically optimal. Our
results represent the first tractable algorithm guaranteeing successful
recovery with tight bounds on recovery errors and sample complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7007</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7007</id><created>2013-12-25</created><authors><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author><author><keyname>Glotin</keyname><forenames>Her&#xe9;</forenames></author><author><keyname>Rabouy</keyname><forenames>C&#xe9;line</forenames></author></authors><title>Functional Mixture Discriminant Analysis with hidden process regression
  for curve classification</title><categories>stat.ME cs.LG stat.ML</categories><comments>In Proceedings of the XXth European Symposium on Artificial Neural
  Networks, Computational Intelligence and Machine Learning (ESANN), Pages
  281-286, 2012, Bruges, Belgium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new mixture model-based discriminant analysis approach for
functional data using a specific hidden process regression model. The approach
allows for fitting flexible curve-models to each class of complex-shaped curves
presenting regime changes. The model parameters are learned by maximizing the
observed-data log-likelihood for each class by using a dedicated
expectation-maximization (EM) algorithm. Comparisons on simulated data with
alternative approaches show that the proposed approach provides better results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7013</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7013</id><created>2013-12-25</created><authors><author><keyname>Bahack</keyname><forenames>Lear</forenames></author></authors><title>Theoretical Bitcoin Attacks with less than Half of the Computational
  Power (draft)</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A widespread security claim of the Bitcoin system, presented in the original
Bitcoin white-paper, states that the security of the system is guaranteed as
long as there is no attacker in possession of half or more of the total
computational power used to maintain the system. This claim, however, is proved
based on theoretically flawed assumptions.
  In the paper we analyze two kinds of attacks based on two theoretical flaws:
the Block Discarding Attack and the Difficulty Raising Attack. We argue that
the current theoretical limit of attacker's fraction of total computational
power essential for the security of the system is in a sense not $\frac{1}{2}$
but a bit less than $\frac{1}{4}$, and outline proposals for protocol change
that can raise this limit to be as close to $\frac{1}{2}$ as we want.
  The basic idea of the Block Discarding Attack has been noted as early as
2010, and lately was independently though-of and analyzed by both author of
this paper and authors of a most recently pre-print published paper. We thus
focus on the major differences of our analysis, and try to explain the
unfortunate surprising coincidence. To the best of our knowledge, the second
attack is presented here for the first time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7014</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7014</id><created>2013-12-25</created><updated>2014-05-16</updated><authors><author><keyname>van Bevern</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Feldmann</keyname><forenames>Andreas Emil</forenames></author><author><keyname>Sorge</keyname><forenames>Manuel</forenames></author><author><keyname>Such&#xfd;</keyname><forenames>Ond&#x159;ej</forenames></author></authors><title>On the Parameterized Complexity of Computing Balanced Partitions in
  Graphs</title><categories>cs.DM cs.DS math.CO</categories><comments>This version of the article is to appear in Theory of Computing
  Systems</comments><msc-class>05C85</msc-class><acm-class>G.2.1; G.2.2; I.1.2; F.2.2</acm-class><journal-ref>Theory of Computing Systems 57(1):1-35, 2015</journal-ref><doi>10.1007/s00224-014-9557-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A balanced partition is a clustering of a graph into a given number of
equal-sized parts. For instance, the Bisection problem asks to remove at most k
edges in order to partition the vertices into two equal-sized parts. We prove
that Bisection is FPT for the distance to constant cliquewidth if we are given
the deletion set. This implies FPT algorithms for some well-studied parameters
such as cluster vertex deletion number and feedback vertex set. However, we
show that Bisection does not admit polynomial-size kernels for these
parameters.
  For the Vertex Bisection problem, vertices need to be removed in order to
obtain two equal-sized parts. We show that this problem is FPT for the number
of removed vertices k if the solution cuts the graph into a constant number c
of connected components. The latter condition is unavoidable, since we also
prove that Vertex Bisection is W[1]-hard w.r.t. (k,c).
  Our algorithms for finding bisections can easily be adapted to finding
partitions into d equal-sized parts, which entails additional running time
factors of n^{O(d)}. We show that a substantial speed-up is unlikely since the
corresponding task is W[1]-hard w.r.t. d, even on forests of maximum degree
two. We can, however, show that it is FPT for the vertex cover number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7018</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7018</id><created>2013-12-25</created><authors><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author><author><keyname>Glotin</keyname><forenames>Herv&#xe9;</forenames></author></authors><title>Mixture model-based functional discriminant analysis for curve
  classification</title><categories>stat.ME cs.LG stat.ML</categories><comments>In Proceedings of the 2012 International Joint Conference on Neural
  Networks (IJCNN), 2012, Pages: 1-8, Brisbane, Australia</comments><doi>10.1109/IJCNN.2012.6252818</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical approaches for Functional Data Analysis concern the paradigm for
which the individuals are functions or curves rather than finite dimensional
vectors. In this paper, we particularly focus on the modeling and the
classification of functional data which are temporal curves presenting regime
changes over time. More specifically, we propose a new mixture model-based
discriminant analysis approach for functional data using a specific hidden
process regression model. Our approach is particularly adapted to both handle
the problem of complex-shaped classes of curves, where each class is composed
of several sub-classes, and to deal with the regime changes within each
homogeneous sub-class. The model explicitly integrates the heterogeneity of
each class of curves via a mixture model formulation, and the regime changes
within each sub-class through a hidden logistic process. The approach allows
therefore for fitting flexible curve-models to each class of complex-shaped
curves presenting regime changes through an unsupervised learning scheme, to
automatically summarize it into a finite number of homogeneous clusters, each
of them is decomposed into several regimes. The model parameters are learned by
maximizing the observed-data log-likelihood for each class by using a dedicated
expectation-maximization (EM) algorithm. Comparisons on simulated data and real
data with alternative approaches, including functional linear discriminant
analysis and functional mixture discriminant analysis with polynomial
regression mixtures and spline regression mixtures, show that the proposed
approach provides better results regarding the discrimination results and
significantly improves the curves approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7022</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7022</id><created>2013-12-25</created><authors><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author></authors><title>Robust EM algorithm for model-based curve clustering</title><categories>stat.ME cs.LG stat.ML</categories><comments>In Proceedings of the 2013 International Joint Conference on Neural
  Networks (IJCNN), 2013, Dallas, TX, USA</comments><journal-ref>In Proceedings of the 2013 International Joint Conference on
  Neural Networks (IJCNN), 2013, Dallas, TX, USA</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based clustering approaches concern the paradigm of exploratory data
analysis relying on the finite mixture model to automatically find a latent
structure governing observed data. They are one of the most popular and
successful approaches in cluster analysis. The mixture density estimation is
generally performed by maximizing the observed-data log-likelihood by using the
expectation-maximization (EM) algorithm. However, it is well-known that the EM
algorithm initialization is crucial. In addition, the standard EM algorithm
requires the number of clusters to be known a priori. Some solutions have been
provided in [31, 12] for model-based clustering with Gaussian mixture models
for multivariate data. In this paper we focus on model-based curve clustering
approaches, when the data are curves rather than vectorial data, based on
regression mixtures. We propose a new robust EM algorithm for clustering
curves. We extend the model-based clustering approach presented in [31] for
Gaussian mixture models, to the case of curve clustering by regression
mixtures, including polynomial regression mixtures as well as spline or
B-spline regressions mixtures. Our approach both handles the problem of
initialization and the one of choosing the optimal number of clusters as the EM
learning proceeds, rather than in a two-fold scheme. This is achieved by
optimizing a penalized log-likelihood criterion. A simulation study confirms
the potential benefit of the proposed algorithm in terms of robustness
regarding initialization and funding the actual number of clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7024</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7024</id><created>2013-12-25</created><authors><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author><author><keyname>Sam&#xe9;</keyname><forenames>Allou</forenames></author><author><keyname>Aknin</keyname><forenames>Patrice</forenames></author><author><keyname>Govaert</keyname><forenames>G&#xe9;rard</forenames></author></authors><title>Model-based clustering with Hidden Markov Model regression for time
  series with regime changes</title><categories>stat.ML cs.LG stat.ME</categories><comments>In Proceedings of the 2011 International Joint Conference on Neural
  Networks (IJCNN), 2011, Pages 2814 - 2821, San Jose, California</comments><doi>10.1109/IJCNN.2011.6033590</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel model-based clustering approach for clustering
time series which present changes in regime. It consists of a mixture of
polynomial regressions governed by hidden Markov chains. The underlying hidden
process for each cluster activates successively several polynomial regimes
during time. The parameter estimation is performed by the maximum likelihood
method through a dedicated Expectation-Maximization (EM) algorithm. The
proposed approach is evaluated using simulated time series and real-world time
series issued from a railway diagnosis application. Comparisons with existing
approaches for time series clustering, including the stand EM for Gaussian
mixtures, $K$-means clustering, the standard mixture of regression models and
mixture of Hidden Markov Models, demonstrate the effectiveness of the proposed
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7034</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7034</id><created>2013-12-25</created><updated>2014-06-28</updated><authors><author><keyname>Fu</keyname><forenames>Fred</forenames></author><author><keyname>Abukhdeir</keyname><forenames>Nasser Mohieddin</forenames></author></authors><title>A Topologically-informed Hyperstreamline Seeding Method for Alignment
  Tensor Fields</title><categories>cs.GR cond-mat.soft</categories><comments>8 pages, 9 figures</comments><doi>10.1109/TVCG.2014.2363828</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A topologically-informed method is presented for seeding of hyperstreamlines
for visualization of alignment tensor fields. The method is inspired by and
applied to visualization of nematic liquid crystal (LC) reorientation dynamics
simulations. The method distributes hyperstreamlines along domain boundaries
and edges of a nearest-neighbor graph whose vertices are degenerate regions of
the alignment tensor field, which correspond to orientational defects in a
nematic LC domain. This is accomplished without iteration while conforming to a
user-specified spacing between hyperstreamlines and avoids possible failure
modes associated with hyperstreamline integration in the vicinity of
degeneracies of alignment (orientational defects). It is shown that the
presented seeding method enables automated hyperstreamline-based visualization
of a broad range of alignment tensor fields which enhances the ability of
researchers to interpret these fields and provides an alternative to using
glyph-based techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7035</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7035</id><created>2013-12-25</created><authors><author><keyname>Mousavi</keyname><forenames>Mohammad</forenames></author><author><keyname>Glynn</keyname><forenames>Peter W.</forenames></author></authors><title>Shape-constrained Estimation of Value Functions</title><categories>math.PR cs.CE math.OC stat.ML</categories><msc-class>93E20, 65C05, 60J22, 60J20, 91G60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fully nonparametric method to estimate the value function, via
simulation, in the context of expected infinite-horizon discounted rewards for
Markov chains. Estimating such value functions plays an important role in
approximate dynamic programming and applied probability in general. We
incorporate &quot;soft information&quot; into the estimation algorithm, such as knowledge
of convexity, monotonicity, or Lipchitz constants. In the presence of such
information, a nonparametric estimator for the value function can be computed
that is provably consistent as the simulated time horizon tends to infinity. As
an application, we implement our method on price tolling agreement contracts in
energy markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7036</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7036</id><created>2013-12-25</created><authors><author><keyname>Hu</keyname><forenames>Qingbo</forenames></author><author><keyname>Wang</keyname><forenames>Guan</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>Deriving Latent Social Impulses to Determine Longevous Videos</title><categories>cs.SI cs.MM</categories><comments>Accepted by WWW '14 as a poster paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online video websites receive huge amount of videos daily from users all
around the world. How to provide valuable recommendations to viewers is an
important task for both video websites and related third parties, such as
search engines. Previous work conducted numerous analysis on the view counts of
videos, which measure a video's value in terms of popularity. However, the
long-lasting value of an online video, namely longevity, is hidden behind the
history that a video accumulates its &quot;popularity&quot; through time. Generally
speaking, a longevous video tends to constantly draw society's attention. With
focus on one of the leading video websites, Youtube, this paper proposes a
scoring mechanism quantifying a video's longevity. Evaluating a video's
longevity can not only improve a video recommender system, but also help us to
discover videos having greater advertising value, as well as adjust a video
website's strategy of storing videos to shorten its responding time. In order
to accurately quantify longevity, we introduce the concept of latent social
impulses and how to use them measure a video's longevity. In order to derive
latent social impulses, we view the video website as a digital signal filter
and formulate the task as a convex minimization problem. The proposed longevity
computation is based on the derived social impulses. Unfortunately, the
required information to derive social impulses are not always public, which
makes a third party unable to directly evaluate every video's longevity. To
solve this problem, we formulate a semi-supervised learning task by using part
of videos having known longevity scores to predict the unknown longevity
scores. We propose a Gaussian Random Markov model with Loopy Belief Propagation
to solve this problem. The conducted experiments on Youtube demonstrate that
the proposed method significantly improves the prediction results comparing to
baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7039</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7039</id><created>2013-12-25</created><authors><author><keyname>Fan</keyname><forenames>Qibin</forenames></author><author><keyname>Jiao</keyname><forenames>Yuling</forenames></author><author><keyname>Lu</keyname><forenames>Xiliang</forenames></author></authors><title>A Primal Dual Active Set Algorithm with Continuation for Compressed
  Sensing</title><categories>cs.IT math.IT math.OC</categories><doi>10.1109/TSP.2014.2362880</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of compressed sensing relies essentially on the ability to
efficiently find an approximately sparse solution to an under-determined linear
system. In this paper, we developed an efficient algorithm for the sparsity
promoting $\ell_1$-regularized least squares problem by coupling the primal
dual active set strategy with a continuation technique (on the regularization
parameter). In the active set strategy, we first determine the active set from
primal and dual variables, and then update the primal and dual variables by
solving a low-dimensional least square problem on the active set, which makes
the algorithm very efficient. The continuation technique globalizes the
convergence of the algorithm, with provable global convergence under restricted
isometry property (RIP). Further, we adopt two alternative methods, i.e., a
modified discrepancy principle and a Bayesian information criterion, to choose
the regularization parameter. Numerical experiments indicate that our algorithm
is very competitive with state-of-the-art algorithms in terms of accuracy and
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7042</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7042</id><created>2013-12-25</created><authors><author><keyname>Kapoor</keyname><forenames>Sanjiv</forenames></author><author><keyname>Kaul</keyname><forenames>Hemanshu</forenames></author></authors><title>Approximating Quadratic 0-1 Programming via SOCP</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of approximating Quadratic O-1 Integer Programs with
bounded number of constraints and non-negative constraint matrix entries, which
we term as PIQP.
  We describe and analyze a randomized algorithm based on a program with
hyperbolic constraints (a Second-Order Cone Programming -SOCP- formulation)
that achieves an approximation ratio of $O(a_{max} \frac{n}{\beta(n)})$, where
$a_{max}$ is the maximum size of an entry in the constraint matrix and
$\beta(n) \leq \min_i{W_i} $, where $W_i$ are the constant terms that define
the constraint inequalities. We note that by appropriately choosing $\beta(n)$
the randomized algorithm, when combined with other algorithms that achieve good
approximations for smaller values of $ W_i$, allows better algorithms for the
complete range of $W_i$. This, together with a greedy algorithm, provides a
$O^*(a_{max} n^{1/2} )$ factor approximation, where $O^*$ hides logarithmic
terms. Our solution is achieved by a randomization of the optimal solution to
the relaxed version of the hyperbolic program. We show that this solution
provides the approximation bounds using concentration bounds provided by
Chernoff-Hoeffding and Kim-Vu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7050</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7050</id><created>2013-12-25</created><updated>2015-11-26</updated><authors><author><keyname>Lou</keyname><forenames>Youcheng</forenames></author><author><keyname>Hong</keyname><forenames>Yiguang</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Nash Equilibrium Computation in Subnetwork Zero-Sum Games with Switching
  Communications</title><categories>cs.SY cs.GT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate a distributed Nash equilibrium computation
problem for a time-varying multi-agent network consisting of two subnetworks,
where the two subnetworks share the same objective function. We first propose a
subgradient-based distributed algorithm with heterogeneous stepsizes to compute
a Nash equilibrium of a zero-sum game. We then prove that the proposed
algorithm can achieve a Nash equilibrium under uniformly jointly strongly
connected (UJSC) weight-balanced digraphs with homogenous stepsizes. Moreover,
we demonstrate that for weighted-unbalanced graphs a Nash equilibrium may not
be achieved with homogenous stepsizes unless certain conditions on the
objective function hold. We show that there always exist heterogeneous
stepsizes for the proposed algorithm to guarantee that a Nash equilibrium can
be achieved for UJSC digraphs. Finally, in two standard weight-unbalanced
cases, we verify the convergence to a Nash equilibrium by adaptively updating
the stepsizes along with the arc weights in the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7056</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7056</id><created>2013-12-26</created><authors><author><keyname>Zainalabidin</keyname><forenames>Izuddin</forenames></author><author><keyname>Halim</keyname><forenames>Izyan Izzati A</forenames></author><author><keyname>Fadzil</keyname><forenames>Faizal A</forenames></author></authors><title>Development of Display Ads Retrieval System to Match Publisher's
  Contents</title><categories>cs.CY cs.IR</categories><comments>14 pages, 3 figures, 1 table. arXiv admin note: text overlap with
  arXiv:1206.1754 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The technological transformation and automation of digital content delivery
has revolutionized the media industry. Advertising landscape is gradually
shifting its traditional media forms to the emergent of Internet advertising.
In this paper, the types of internet advertising to be discussed on are
contextual and sponsored search ads. These types of advertising have the
central challenge of finding the best match between a given context and a
suitable advertisement, through a principled method. Furthermore, there are
four main players that exist in the Internet advertising ecosystem: users,
advertisers, ad exchange and publishers. Hence, to find ways to counter the
central challenge, the paper addresses two objectives: how to successfully make
the best contextual ads selections to match to a web page content to ensure
that there is a valuable connection between the web page and the contextual
ads. All methods, discussions, conclusion and future recommendations are
presented as per sections. Hence, in order to prove the working mechanism of
matching contextual ads and web pages, web pages together with the ads matching
system are developed as a prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7062</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7062</id><created>2013-12-26</created><authors><author><keyname>Wijs</keyname><forenames>Anton</forenames><affiliation>Eindhoven University of Technology</affiliation></author><author><keyname>Bo&#x161;na&#x10d;ki</keyname><forenames>Dragan</forenames><affiliation>Eindhoven University of Technology</affiliation></author><author><keyname>Edelkamp</keyname><forenames>Stefan</forenames><affiliation>University of Bremen</affiliation></author></authors><title>Proceedings 2nd Workshop on GRAPH Inspection and Traversal Engineering</title><categories>cs.DS cs.LO cs.SE</categories><proxy>EPTCS</proxy><acm-class>D.2.4; E.1; F.2.2</acm-class><journal-ref>EPTCS 138, 2013</journal-ref><doi>10.4204/EPTCS.138</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These are the proceedings of the Second Workshop on GRAPH Inspection and
Traversal Engineering (GRAPHITE 2013), which took place on March 24, 2013 in
Rome, Italy, as a satellite event of the 16th European Joint Conferences on
Theory and Practice of Software (ETAPS 2013).
  The topic of the GRAPHITE workshop is graph analysis in all its forms in
computer science. Graphs are used to represent data in many application areas,
and they are subjected to various computational algorithms in order to acquire
the desired information. These graph algorithms tend to have common
characteristics, such as duplicate detection to guarantee their termination,
independent of their application domain. Over the past few years, it has been
shown that the scalability of such algorithms can be dramatically improved by
using, e.g., external memory, by exploiting parallel architectures, such as
clusters, multi-core CPUs, and graphics processing units, and by using
heuristics to guide the search. Novel techniques to further scale graph search
algorithms, and new applications of graph search are within the scope of this
workshop.
  Another topic of interest of the event is more related to the structural
properties of graphs: which kind of graph characteristics are relevant for a
particular application area, and how can these be measured? Finally, any novel
way of using graphs for a particular application area is on topic.
  The goal of this event is to gather scientists from different communities,
such as model checking, artificial intelligence planning, game playing, and
algorithm engineering, who do research on graph search algorithms, such that
awareness of each others' work is increased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7076</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7076</id><created>2013-12-26</created><updated>2014-03-25</updated><authors><author><keyname>Ioannidis</keyname><forenames>Stratis</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author><author><keyname>Yan</keyname><forenames>Jinyun</forenames></author></authors><title>A Consensus-Focused Group Recommender System</title><categories>cs.HC cs.IR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many cases, recommendations are consumed by groups of users rather than
individuals. In this paper, we present a system which recommends social events
to groups. The system helps groups to organize a joint activity and
collectively select which activity to perform among several possible options.
We also facilitate the consensus making, following the principle of group
consensus decision making. Our system allows users to asynchronously vote, add
and comment on alternatives. We observe social influence within groups through
post-recommendation feedback during the group decision making process. We
propose a decision cascading model and estimate such social influence, which
can be used to improve the performance of group recommendation. We conduct
experiments to measure the prediction performance of our model. The result
shows that the model achieves better results than that of independent decision
making model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7077</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7077</id><created>2013-12-26</created><updated>2014-10-03</updated><authors><author><keyname>Parikh</keyname><forenames>Ankur P.</forenames></author><author><keyname>Saluja</keyname><forenames>Avneesh</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Language Modeling with Power Low Rank Ensembles</title><categories>cs.CL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present power low rank ensembles (PLRE), a flexible framework for n-gram
language modeling where ensembles of low rank matrices and tensors are used to
obtain smoothed probability estimates of words in context. Our method can be
understood as a generalization of n-gram modeling to non-integer n, and
includes standard techniques such as absolute discounting and Kneser-Ney
smoothing as special cases. PLRE training is efficient and our approach
outperforms state-of-the-art modified Kneser Ney baselines in terms of
perplexity on large corpora as well as on BLEU score in a downstream machine
translation task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7085</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7085</id><created>2013-12-26</created><authors><author><keyname>Lu</keyname><forenames>Peng</forenames></author><author><keyname>Peng</keyname><forenames>Xujun</forenames></author><author><keyname>Zhu</keyname><forenames>Xinshan</forenames></author><author><keyname>Wang</keyname><forenames>Xiaojie</forenames></author></authors><title>Finding More Relevance: Propagating Similarity on Markov Random Field
  for Image Retrieval</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To effectively retrieve objects from large corpus with high accuracy is a
challenge task. In this paper, we propose a method that propagates visual
feature level similarities on a Markov random field (MRF) to obtain a high
level correspondence in image space for image pairs. The proposed
correspondence between image pair reflects not only the similarity of low-level
visual features but also the relations built through other images in the
database and it can be easily integrated into the existing
bag-of-visual-words(BoW) based systems to reduce the missing rate. We evaluate
our method on the standard Oxford-5K, Oxford-105K and Paris-6K dataset. The
experiment results show that the proposed method significantly improves the
retrieval accuracy on three datasets and exceeds the current state-of-the-art
retrieval performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7118</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7118</id><created>2013-12-26</created><authors><author><keyname>Jantawan</keyname><forenames>Bangsuk</forenames></author><author><keyname>Tsai</keyname><forenames>Cheng-Fa</forenames></author></authors><title>The Development of Educational Quality Administration: a Case of
  Technical College in Southern Thailand</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The purpose of this research were: to survey the needs of using the
information system for educational quality administration; to develop
Information System for Educational quality Administration (ISEs) in accordance
with quality assessment standard; to study the qualification of ISEs; and to
study satisfaction level of ISEs user. Subsequently, the tools of study have
been employed that there were the collection of 47 questionnaires and 5
interviews to specialist by responsible officers for Information center of
Technical colleges in Southern Thailand. The analysis of quantitative data has
employed descriptive statistics using mean and standard deviation as the tool
of measurement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7123</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7123</id><created>2013-12-26</created><authors><author><keyname>Jantawan</keyname><forenames>Bangsuk</forenames></author><author><keyname>Tsai</keyname><forenames>Cheng-Fa</forenames></author></authors><title>The Application of Data Mining to Build Classification Model for
  Predicting Graduate Employment</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Data mining has been applied in various areas because of its ability to
rapidly analyze vast amounts of data. This study is to build the Graduates
Employment Model using classification task in data mining, and to compare
several of data-mining approaches such as Bayesian method and the Tree method.
The Bayesian method includes 5 algorithms, including AODE, BayesNet, HNB,
NaviveBayes, WAODE. The Tree method includes 5 algorithms, including BFTree,
NBTree, REPTree, ID3, C4.5. The experiment uses a classification task in WEKA,
and we compare the results of each algorithm, where several classification
models were generated. To validate the generated model, the experiments were
conducted using real data collected from graduate profile at the Maejo
University in Thailand. The model is intended to be used for predicting whether
a graduate was employed, unemployed, or in an undetermined situation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7126</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7126</id><created>2013-12-26</created><authors><author><keyname>Moussa</keyname><forenames>Ali Cherif</forenames></author><author><keyname>Kamel</keyname><forenames>Faraoun Mohamed</forenames></author></authors><title>Link Quality and MAC-Overhead aware Predictive Preemptive Routing
  Protocol for Mobile Ad hoc Network</title><categories>cs.NI</categories><comments>10 pages, 19 figures. arXiv admin note: text overlap with
  arXiv:0907.5441 by other authors</comments><msc-class>68</msc-class><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 5, No 2, September 2013 ISSN (Print): 1694-0814 | ISSN (Online):
  1694-0784</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Ad Hoc networks, route failure may occur due to less received power,
mobility, congestion and node failures. Many approaches have been proposed in
literature to solve this problem, where a node predicts pre-emptively the route
failure that occurs with the less received power. However, this approach
encounters some difficulties, especially in scenario without mobility where
route failures may arise. In this paper, we propose an improvement of AODV
protocol called LO-PPAODV (Link Quality and MAC-Overhead aware Predictive
Preemptive AODV). This protocol is based on new metric combine more routing
metrics (Link Quality, MAC Overhead) between each node and one hop neighbor.
Also we propose a cross-layer networking mechanism to distinguish between both
situations, failures due to congestion or mobility, and consequently avoiding
unnecessary route repair process. The LO-PPAODV was implemented using NS-2. The
simulation results show that our approach improves the overall performance of
the network. It reduces the average end to end delay, the routing overhead, MAC
errors and route errors, and increases the packet delivery fraction of the
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7128</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7128</id><created>2013-12-26</created><authors><author><keyname>Nemiroff</keyname><forenames>Robert J.</forenames></author><author><keyname>Wilson</keyname><forenames>Teresa</forenames></author></authors><title>Searching the Internet for evidence of time travelers</title><categories>physics.pop-ph cs.CY</categories><comments>11 pages, comments welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time travel has captured the public imagination for much of the past century,
but little has been done to actually search for time travelers. Here, three
implementations of Internet searches for time travelers are described, all
seeking a prescient mention of information not previously available. The first
search covered prescient content placed on the Internet, highlighted by a
comprehensive search for specific terms in tweets on Twitter. The second search
examined prescient inquiries submitted to a search engine, highlighted by a
comprehensive search for specific search terms submitted to a popular astronomy
web site. The third search involved a request for a direct Internet
communication, either by email or tweet, pre-dating to the time of the inquiry.
Given practical verifiability concerns, only time travelers from the future
were investigated. No time travelers were discovered. Although these negative
results do not disprove time travel, given the great reach of the Internet,
this search is perhaps the most comprehensive to date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7135</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7135</id><created>2013-12-26</created><authors><author><keyname>Park</keyname><forenames>Seok-Hwan</forenames><affiliation>Shitz</affiliation></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Sahin</keyname><forenames>Onur</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Multihop Backhaul Compression for the Uplink of Cloud Radio Access
  Networks</title><categories>cs.IT math.IT</categories><comments>A shorter version of the paper has been submitted to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cloud radio access networks (C-RANs), the baseband processing of the radio
units (RUs) is migrated to remote control units (CUs). This is made possible by
a network of backhaul links that connects RUs and CUs and that carries
compressed baseband signals. While prior work has focused mostly on single-hop
backhaul networks, this paper investigates efficient backhaul compression
strategies for the uplink of C-RANs with a general multihop backhaul topology.
A baseline multiplex-and-forward (MF) scheme is first studied in which each RU
forwards the bit streams received from the connected RUs without any
processing. It is observed that this strategy may cause significant performance
degradation in the presence of a dense deployment of RUs with a well connected
backhaul network. To obviate this problem, a scheme is proposed in which each
RU decompresses the received bit streams and performs linear in-network
processing of the decompressed signals. For both the MF and the
decompress-process-and-recompress (DPR) backhaul schemes, the optimal design is
addressed with the aim of maximizing the sum-rate under the backhaul capacity
constraints. Recognizing the significant demands of the optimal solution of the
DPR scheme in terms of channel state information (CSI) at the RUs,
decentralized optimization algorithms are proposed under the assumption of
limited CSI at the RUs. Numerical results are provided to compare the
performance of the MF and DPR schemes, highlighting the potential advantage of
in-network processing and the impact of CSI limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7145</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7145</id><created>2013-12-26</created><updated>2014-09-19</updated><authors><author><keyname>Aminzare</keyname><forenames>Zahra</forenames></author><author><keyname>Sontag</keyname><forenames>Eduardo D.</forenames></author></authors><title>Some remarks on spatial uniformity of solutions of reaction-diffusion
  PDE's and a related synchronization problem for ODE's</title><categories>cs.SY math.AP</categories><comments>arXiv admin note: text overlap with arXiv:1208.0326</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we present a condition which guarantees spatial uniformity for
the asymptotic behavior of the solutions of a reaction-diffusion PDE with
Neumann boundary conditions in one dimension, using the Jacobian matrix of the
reaction term and the first Dirichlet eigenvalue of the Laplacian operator on
the given spatial domain. We also derive an analog of this PDE result for the
synchronization of a network of identical ODE models coupled by diffusion
terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7152</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7152</id><created>2013-12-26</created><authors><author><keyname>Freitas</keyname><forenames>Miguel</forenames></author></authors><title>twister - a P2P microblogging platform</title><categories>cs.NI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new microblogging architecture based on peer-to-peer
networks overlays. The proposed platform is comprised of three mostly
independent overlay networks. The first provides distributed user registration
and authentication and is based on the Bitcoin protocol. The second one is a
Distributed Hash Table (DHT) overlay network providing key/value storage for
user resources and tracker location for the third network. The last network is
a collection of possibly disjoint &quot;swarms&quot; of followers, based on the
Bittorrent protocol, which can be used for efficient near-instant notification
delivery to many users. By leveraging from existing and proven technologies,
twister provides a new microblogging platform offering security, scalability
and privacy features. A mechanism provides incentive for entities that
contribute processing time to run the user registration network, rewarding such
entities with the privilege of sending a single unsolicited (&quot;promoted&quot;)
message to the entire network. The number of unsolicited messages per day is
defined in order to not upset users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7165</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7165</id><created>2013-12-26</created><authors><author><keyname>Zitin</keyname><forenames>Ari</forenames></author><author><keyname>Gorowora</keyname><forenames>Alex</forenames></author><author><keyname>Squires</keyname><forenames>Shane</forenames></author><author><keyname>Herrera</keyname><forenames>Mark</forenames></author><author><keyname>Antonsen</keyname><forenames>Thomas M.</forenames></author><author><keyname>Girvan</keyname><forenames>Michelle</forenames></author><author><keyname>Ott</keyname><forenames>Edward</forenames></author></authors><title>Spatially embedded growing small-world networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks in nature are often formed within a spatial domain in a dynamical
manner, gaining links and nodes as they develop over time. We propose a class
of spatially-based growing network models and investigate the relationship
between the resulting statistical network properties and the dimension and
topology of the space in which the networks are embedded. In particular, we
consider models in which nodes are placed one by one in random locations in
space, with each such placement followed by configuration relaxation toward
uniform node density, and connection of the new node with spatially nearby
nodes. We find that such growth processes naturally result in networks with
small-world features, including a short characteristic path length and nonzero
clustering. These properties do not appear to depend strongly on the topology
of the embedding space, but do depend strongly on its dimension;
higher-dimensional spaces result in shorter path lengths but less clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7167</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7167</id><created>2013-12-26</created><authors><author><keyname>Kumar</keyname><forenames>Abhishek</forenames></author><author><keyname>Sindhwani</keyname><forenames>Vikas</forenames></author></authors><title>Near-separable Non-negative Matrix Factorization with $\ell_1$- and
  Bregman Loss Functions</title><categories>stat.ML cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a family of tractable NMF algorithms have been proposed under the
assumption that the data matrix satisfies a separability condition Donoho &amp;
Stodden (2003); Arora et al. (2012). Geometrically, this condition reformulates
the NMF problem as that of finding the extreme rays of the conical hull of a
finite set of vectors. In this paper, we develop several extensions of the
conical hull procedures of Kumar et al. (2013) for robust ($\ell_1$)
approximations and Bregman divergences. Our methods inherit all the advantages
of Kumar et al. (2013) including scalability and noise-tolerance. We show that
on foreground-background separation problems in computer vision, robust
near-separable NMFs match the performance of Robust PCA, considered state of
the art on these problems, with an order of magnitude faster training time. We
also demonstrate applications in exemplar selection settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7179</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7179</id><created>2013-12-26</created><authors><author><keyname>Songsiri</keyname><forenames>Patoomsiri</forenames></author><author><keyname>Phetkaew</keyname><forenames>Thimaporn</forenames></author><author><keyname>Ichise</keyname><forenames>Ryutaro</forenames></author><author><keyname>Kijsirikul</keyname><forenames>Boonserm</forenames></author></authors><title>Sub-Classifier Construction for Error Correcting Output Code Using
  Minimum Weight Perfect Matching</title><categories>cs.LG cs.IT math.IT</categories><comments>7 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-class classification is mandatory for real world problems and one of
promising techniques for multi-class classification is Error Correcting Output
Code. We propose a method for constructing the Error Correcting Output Code to
obtain the suitable combination of positive and negative classes encoded to
represent binary classifiers. The minimum weight perfect matching algorithm is
applied to find the optimal pairs of subset of classes by using the
generalization performance as a weighting criterion. Based on our method, each
subset of classes with positive and negative labels is appropriately combined
for learning the binary classifiers. Experimental results show that our
technique gives significantly higher performance compared to traditional
methods including the dense random code and the sparse random code both in
terms of accuracy and classification times. Moreover, our method requires
significantly smaller number of binary classifiers while maintaining accuracy
compared to the One-Versus-One.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7187</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7187</id><created>2013-12-26</created><authors><author><keyname>Saito</keyname><forenames>Hiroshi</forenames></author></authors><title>Analysis of Geometric Disaster Evaluation Model for Physical Networks</title><categories>cs.NI</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A geometric model of a physical network affected by a disaster is proposed
and analyzed using integral geometry (geometric probability). This analysis
provides a theoretical method of evaluating performance metrics, such as the
probability of maintaining connectivity, and a network design rule that can
make the network robust against disasters.
  The proposed model is of when the disaster area is much larger than the part
of the network in which we are interested. Performance metrics, such as the
probability of maintaining connectivity, are explicitly given by linear
functions of the perimeter length of convex hulls determined by physical
routes. The derived network design rule includes the following. (1) Reducing
the convex hull of the physical route reduces the expected number of nodes that
cannot connect to the destination. (2) The probability of maintaining the
connectivity of two nodes on a loop cannot be changed by changing the physical
route of that loop. (3) The effect of introducing a loop is identical to that
of a single physical route implemented by the straight-line route.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7191</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7191</id><created>2013-12-26</created><authors><author><keyname>Tang</keyname><forenames>Chunming</forenames></author><author><keyname>Qi</keyname><forenames>Yanfeng</forenames></author></authors><title>Special values of Kloosterman sums and binomial bent functions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $p\ge 7$, $q=p^m$. $K_q(a)=\sum_{x\in \mathbb{F}_{p^m}}
\zeta^{\mathrm{Tr}^m_1(x^{p^m-2}+ax)}$ is the Kloosterman sum of $a$ on
$\mathbb{F}_{p^m}$, where $\zeta=e^{\frac{2\pi\sqrt{-1}}{p}}$. The value
$1-\frac{2}{\zeta+\zeta^{-1}}$ of $K_q(a)$ and its conjugate have close
relationship with a class of binomial function with Dillon exponent. This paper
first presents some necessary conditions for $a$ such that
$K_q(a)=1-\frac{2}{\zeta+\zeta^{-1}}$. Further, we prove that if $p=11$, for
any $a$, $K_q(a)\neq 1-\frac{2}{\zeta+\zeta^{-1}}$. And for $p\ge 13$, if $a\in
\mathbb{F}_{p^s}$ and $s=\mathrm{gcd}(2,m)$, $K_q(a)\neq
1-\frac{2}{\zeta+\zeta^{-1}}$. In application, these results explains some
class of binomial regular bent functions does not exits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7198</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7198</id><created>2013-12-27</created><updated>2014-04-18</updated><authors><author><keyname>Yang</keyname><forenames>Hyun Jong</forenames></author><author><keyname>Shin</keyname><forenames>Won-Yong</forenames></author><author><keyname>Jung</keyname><forenames>Bang Chul</forenames></author><author><keyname>Suh</keyname><forenames>Changho</forenames></author><author><keyname>Paulraj</keyname><forenames>Arogyaswami</forenames></author></authors><title>Opportunistic Downlink Interference Alignment</title><categories>cs.IT math.IT</categories><comments>25 pages, 6 figures, Submitted to IEEE Transactions on Signal
  Processing. Part of this paper is to be presented at the IEEE International
  Conference on Acoustics, Speech and Signal Processing, Florence, Italy, May
  2014 and the IEEE International Symposium on Information Theory, Honolulu,
  HI, June/July 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an opportunistic downlink interference alignment
(ODIA) for interference-limited cellular downlink, which intelligently combines
user scheduling and downlink IA techniques. The proposed ODIA not only
efficiently reduces the effect of inter-cell interference from other-cell base
stations (BSs) but also eliminates intra-cell interference among spatial
streams in the same cell. We show that the minimum number of users required to
achieve a target degrees-of-freedom (DoF) can be fundamentally reduced, i.e.,
the fundamental user scaling law can be improved by using the ODIA, compared
with the existing downlink IA schemes. In addition, we adopt a limited feedback
strategy in the ODIA framework, and then analyze the required number of
feedback bits leading to the same performance as that of the ODIA assuming
perfect feedback. We also modify the original ODIA in order to further improve
sum-rate, which achieves the optimal multiuser diversity gain, i.e., $\log \log
N$, per spatial stream even in the presence of downlink inter-cell
interference, where $N$ denotes the number of users in a cell. Simulation
results show that the ODIA significantly outperforms existing interference
management techniques in terms of sum-rate in realistic cellular environments.
Note that the ODIA operates in a distributed and decoupled manner, while
requiring no information exchange among BSs and no iterative beamformer
optimization between BSs and users, thus leading to an easier implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7201</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7201</id><created>2013-12-27</created><authors><author><keyname>Gao</keyname><forenames>Juntao</forenames></author><author><keyname>Shen</keyname><forenames>Yulong</forenames></author><author><keyname>Jiang</keyname><forenames>Xiaohong</forenames></author><author><keyname>Takahashi</keyname><forenames>Osamu</forenames></author><author><keyname>Shiratori</keyname><forenames>Norio</forenames></author></authors><title>End-to-End Delay Modeling for Mobile Ad Hoc Networks: A
  Quasi-Birth-and-Death Approach</title><categories>cs.NI</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the fundamental end-to-end delay performance in mobile ad hoc
networks (MANETs) is of great importance for supporting Quality of Service
(QoS) guaranteed applications in such networks. While upper bounds and
approximations for end-to-end delay in MANETs have been developed in
literature, which usually introduce significant errors in delay analysis, the
modeling of exact end-to-end delay in MANETs remains a technical challenge.
This is partially due to the highly dynamical behaviors of MANETs, but also due
to the lack of an efficient theoretical framework to capture such dynamics.
This paper demonstrates the potential application of the powerful
Quasi-Birth-and-Death (QBD) theory in tackling the challenging issue of exact
end-to-end delay modeling in MANETs. We first apply the QBD theory to develop
an efficient theoretical framework for capturing the complex dynamics in
MANETs. We then show that with the help of this framework, closed form models
can be derived for the analysis of exact end-to-end delay and also per node
throughput capacity in MANETs. Simulation and numerical results are further
provided to illustrate the efficiency of these QBD theory-based models as well
as our theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7217</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7217</id><created>2013-12-27</created><authors><author><keyname>Agarwal</keyname><forenames>Archita</forenames></author><author><keyname>Chakaravarthy</keyname><forenames>Venkatesan T.</forenames></author><author><keyname>Choudhury</keyname><forenames>Anamitra R.</forenames></author><author><keyname>Roy</keyname><forenames>Sambuddha</forenames></author><author><keyname>Sabharwal</keyname><forenames>Yogish</forenames></author></authors><title>Distributed and Parallel Algorithms for Set Cover Problems with Small
  Neighborhood Covers</title><categories>cs.DS cs.DC</categories><comments>Full version of FSTTCS'13 paper</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a class of set cover problems that satisfy a special
property which we call the {\em small neighborhood cover} property. This class
encompasses several well-studied problems including vertex cover, interval
cover, bag interval cover and tree cover. We design unified distributed and
parallel algorithms that can handle any set cover problem falling under the
above framework and yield constant factor approximations. These algorithms run
in polylogarithmic communication rounds in the distributed setting and are in
NC, in the parallel setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7219</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7219</id><created>2013-12-27</created><updated>2016-01-28</updated><authors><author><keyname>Frosini</keyname><forenames>Patrizio</forenames></author><author><keyname>Jablonski</keyname><forenames>Grzegorz</forenames></author></authors><title>Combining persistent homology and invariance groups for shape comparison</title><categories>math.AT cs.CG cs.CV</categories><comments>33 pages, 12 figures, 1 table; corrected typos</comments><msc-class>55N35 (Primary), 22F99, 47H09, 54H15, 57S10, 68U05, 65D18
  (Secondary)</msc-class><acm-class>I.4.7; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications concerning the comparison of data expressed by
$\mathbb{R}^m$-valued functions defined on a topological space $X$, the
invariance with respect to a given group $G$ of self-homeomorphisms of $X$ is
required. While persistent homology is quite efficient in the topological and
qualitative comparison of this kind of data when the invariance group $G$ is
the group $\mathrm{Homeo}(X)$ of all self-homeomorphisms of $X$, this theory is
not tailored to manage the case in which $G$ is a proper subgroup of
$\mathrm{Homeo}(X)$, and its invariance appears too general for several tasks.
This paper proposes a way to adapt persistent homology in order to get
invariance just with respect to a given group of self-homeomorphisms of $X$.
The main idea consists in a dual approach, based on considering the set of all
$G$-invariant non-expanding operators defined on the space of the admissible
filtering functions on $X$. Some theoretical results concerning this approach
are proven and two experiments are presented. An experiment illustrates the
application of the proposed technique to compare 1D-signals, when the
invariance is expressed by the group of affinities, the group of
orientation-preserving affinities, the group of isometries, the group of
translations and the identity group. Another experiment shows how our technique
can be used for image comparison.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7222</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7222</id><created>2013-12-27</created><updated>2013-12-30</updated><authors><author><keyname>Chiu</keyname><forenames>Well Y.</forenames></author><author><keyname>Szegedy</keyname><forenames>Mario</forenames></author><author><keyname>Wang</keyname><forenames>Chengu</forenames></author><author><keyname>Xu</keyname><forenames>Yixin</forenames></author></authors><title>The Garden Hose Complexity for the Equality Function</title><categories>quant-ph cs.CC</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The garden hose complexity is a new communication complexity introduced by H.
Buhrman, S. Fehr, C. Schaffner and F. Speelman [BFSS13] to analyze
position-based cryptography protocols in the quantum setting. We focus on the
garden hose complexity of the equality function, and improve on the bounds of
O. Margalit and A. Matsliah[MM12] with the help of a new approach and of our
handmade simulated annealing based solver. We have also found beautiful
symmetries of the solutions that have lead us to develop the notion of garden
hose permutation groups. Then, exploiting this new concept, we get even
further, although several interesting open problems remain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7223</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7223</id><created>2013-12-27</created><authors><author><keyname>Gupta</keyname><forenames>Rashmi</forenames></author><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author><author><keyname>Mathur</keyname><forenames>Iti</forenames></author></authors><title>Quality Estimation of English-Hindi Outputs using Naive Bayes Classifier</title><categories>cs.CL</categories><comments>In Proceedings of 2013 International Conference on Advances in
  Computing, Communications and Informatics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an approach for estimating the quality of machine
translation system. There are various methods for estimating the quality of
output sentences, but in this paper we focus on Na\&quot;ive Bayes classifier to
build model using features which are extracted from the input sentences. These
features are used for finding the likelihood of each of the sentences of the
training data which are then further used for determining the scores of the
test data. On the basis of these scores we determine the class labels of the
test data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7243</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7243</id><created>2013-12-27</created><updated>2014-01-05</updated><authors><author><keyname>Jallu</keyname><forenames>Ramesh K.</forenames></author><author><keyname>Prasad</keyname><forenames>Prajwal R.</forenames></author><author><keyname>Das</keyname><forenames>Gautam K.</forenames></author></authors><title>Minimum Dominating Set for a Point Set in $\IR^2$</title><categories>cs.DS</categories><comments>14 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we consider the problem of computing minimum dominating set
for a given set $S$ of $n$ points in $\IR^2$. Here the objective is to find a
minimum cardinality subset $S'$ of $S$ such that the union of the unit radius
disks centered at the points in $S'$ covers all the points in $S$. We first
propose a simple 4-factor and 3-factor approximation algorithms in $O(n^6 \log
n)$ and $O(n^{11} \log n)$ time respectively improving time complexities by a
factor of $O(n^2)$ and $O(n^4)$ respectively over the best known result
available in the literature [M. De, G.K. Das, P. Carmi and S.C. Nandy, {\it
Approximation algorithms for a variant of discrete piercing set problem for
unit disk}, Int. J. of Comp. Geom. and Appl., to appear]. Finally, we propose a
very important shifting lemma, which is of independent interest and using this
lemma we propose a $\frac{5}{2}$-factor approximation algorithm and a PTAS for
the minimum dominating set problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7249</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7249</id><created>2013-12-27</created><authors><author><keyname>Reyes</keyname><forenames>Patricio</forenames></author><author><keyname>Silva</keyname><forenames>Alonso</forenames></author></authors><title>Maximum Coverage and Maximum Connected Covering in Social Networks with
  Partial Topology Information</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Viral marketing campaigns seek to recruit the most influential individuals to
cover the largest target audience. This can be modeled as the well-studied
maximum coverage problem. There is a related problem when the recruited nodes
are connected. It is called the maximum connected cover problem. This problem
ensures a strong coordination between the influential nodes which are the
backbone of the marketing campaign. In this work, we are interested on both of
these problems. Most of the related literature assumes knowledge about the
topology of the network. Even in that case, the problem is known to be NP-hard.
In this work, we propose heuristics to the maximum connected cover problem and
the maximum coverage problem with different knowledge levels about the topology
of the network. We quantify the difference between these heuristics and the
local and global greedy algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7253</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7253</id><created>2013-12-27</created><authors><author><keyname>Le</keyname><forenames>Van Bang</forenames></author><author><keyname>Pfender</keyname><forenames>Florian</forenames></author></authors><title>Complexity Results for Rainbow Matchings</title><categories>cs.DM math.CO</categories><comments>To appear in Theoretical Computer Science</comments><doi>10.1016/j.tcs.2013.12.013</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A rainbow matching in an edge-colored graph is a matching whose edges have
distinct colors. We address the complexity issue of the following problem,
\mrbm: Given an edge-colored graph $G$, how large is the largest rainbow
matching in $G$? We present several sharp contrasts in the complexity of this
problem.
  We show, among others, that
  * can be approximated by a polynomial algorithm with approximation ratio
$2/3-\eps$. * is APX-complete, even when restricted to properly edge-colored
linear forests without a $5$-vertex path, and is solvable in %time $O(m^{3/2})$
on edge-colored $m$-edge polynomial time for edge-colored forests without a
$4$-vertex path. * is APX-complete, even when restricted to properly
edge-colored trees without an $8$-vertex path, and is solvable in %time
$O(n^{7/2})$ on edge-colored $n$-vertex polynomial time for edge-colored trees
without a $7$-vertex path. * is APX-complete, even when restricted to properly
edge-colored paths.
  These results provide a dichotomy theorem for the complexity of the problem
on forests and trees in terms of forbidding paths. The latter is somewhat
surprising, since, to the best of our knowledge, no (unweighted) graph problem
prior to our result is known to be NP-hard for simple paths. We also address
the parameterized complexity of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7258</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7258</id><created>2013-12-27</created><updated>2014-03-17</updated><authors><author><keyname>Peel</keyname><forenames>Leto</forenames></author></authors><title>Active Discovery of Network Roles for Predicting the Classes of Network
  Nodes</title><categories>cs.LG cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nodes in real world networks often have class labels, or underlying
attributes, that are related to the way in which they connect to other nodes.
Sometimes this relationship is simple, for instance nodes of the same class are
may be more likely to be connected. In other cases, however, this is not true,
and the way that nodes link in a network exhibits a different, more complex
relationship to their attributes. Here, we consider networks in which we know
how the nodes are connected, but we do not know the class labels of the nodes
or how class labels relate to the network links. We wish to identify the best
subset of nodes to label in order to learn this relationship between node
attributes and network links. We can then use this discovered relationship to
accurately predict the class labels of the rest of the network nodes.
  We present a model that identifies groups of nodes with similar link
patterns, which we call network roles, using a generative blockmodel. The model
then predicts labels by learning the mapping from network roles to class labels
using a maximum margin classifier. We choose a subset of nodes to label
according to an iterative margin-based active learning strategy. By integrating
the discovery of network roles with the classifier optimisation, the active
learning process can adapt the network roles to better represent the network
for node classification. We demonstrate the model by exploring a selection of
real world networks, including a marine food web and a network of English
words. We show that, in contrast to other network classifiers, this model
achieves good classification accuracy for a range of networks with different
relationships between class labels and network links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7275</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7275</id><created>2013-12-10</created><updated>2015-04-13</updated><authors><author><keyname>Pfender</keyname><forenames>Michael</forenames></author></authors><title>Arithmetical Foundations - Recursion. Evaluation. Consistency</title><categories>math.LO cs.LO math.CT</categories><msc-class>03G30, 03B30, 03D75</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Primitive recursion, mu-recursion, universal object and universe theories,
complexity controlled iteration, code evaluation, soundness, decidability,
G\&quot;odel incompleteness theorems, inconsistency provability for set theory,
constructive consistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7279</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7279</id><created>2013-12-27</created><updated>2014-10-27</updated><authors><author><keyname>Schost</keyname><forenames>&#xc9;ric</forenames></author><author><keyname>Spaenlehauer</keyname><forenames>Pierre-Jean</forenames></author></authors><title>A Quadratically Convergent Algorithm for Structured Low-Rank
  Approximation</title><categories>cs.NA cs.SC math.NA</categories><comments>37 pages, Maple package available at
  http://www.pjspaenlehauer.net/data/software/NewtonSLRA_notes.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured Low-Rank Approximation is a problem arising in a wide range of
applications in Numerical Analysis and Engineering Sciences. Given an input
matrix $M$, the goal is to compute a matrix $M'$ of given rank $r$ in a linear
or affine subspace $E$ of matrices (usually encoding a specific structure) such
that the Frobenius distance $\lVert M-M'\rVert$ is small. We propose a
Newton-like iteration for solving this problem, whose main feature is that it
converges locally quadratically to such a matrix under mild transversality
assumptions between the manifold of matrices of rank $r$ and the linear/affine
subspace $E$. We also show that the distance between the limit of the iteration
and the optimal solution of the problem is quadratic in the distance between
the input matrix and the manifold of rank $r$ matrices in $E$. To illustrate
the applicability of this algorithm, we propose a Maple implementation and give
experimental results for several applicative problems that can be modeled by
Structured Low-Rank Approximation: univariate approximate GCDs (Sylvester
matrices), low-rank Matrix completion (coordinate spaces) and denoising
procedures (Hankel matrices). Experimental results give evidence that this
all-purpose algorithm is competitive with state-of-the-art numerical methods
dedicated to these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7284</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7284</id><created>2013-12-27</created><updated>2013-12-30</updated><authors><author><keyname>Avanzini</keyname><forenames>Martin</forenames></author><author><keyname>Eguchi</keyname><forenames>Naohi</forenames></author></authors><title>A New Term Rewriting Characterisation of ETIME functions</title><categories>cs.LO cs.CC</categories><comments>Technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adopting former term rewriting characterisations of polytime and
exponential-time computable functions, we introduce a new reduction order, the
Path Order for ETIME (POE* for short), that is sound and complete for ETIME
computable functions. The proposed reduction order for ETIME makes contrasts to
those related complexity classes clear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7292</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7292</id><created>2013-12-27</created><updated>2014-03-23</updated><authors><author><keyname>A.</keyname><forenames>Prashanth L.</forenames></author><author><keyname>Chatterjee</keyname><forenames>Abhranil</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>Two Timescale Convergent Q-learning for Sleep--Scheduling in Wireless
  Sensor Networks</title><categories>cs.SY cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider an intrusion detection application for Wireless
Sensor Networks (WSNs). We study the problem of scheduling the sleep times of
the individual sensors to maximize the network lifetime while keeping the
tracking error to a minimum. We formulate this problem as a
partially-observable Markov decision process (POMDP) with continuous
state-action spaces, in a manner similar to (Fuemmeler and Veeravalli [2008]).
However, unlike their formulation, we consider infinite horizon discounted and
average cost objectives as performance criteria. For each criterion, we propose
a convergent on-policy Q-learning algorithm that operates on two timescales,
while employing function approximation to handle the curse of dimensionality
associated with the underlying POMDP. Our proposed algorithm incorporates a
policy gradient update using a one-simulation simultaneous perturbation
stochastic approximation (SPSA) estimate on the faster timescale, while the
Q-value parameter (arising from a linear function approximation for the
Q-values) is updated in an on-policy temporal difference (TD) algorithm-like
fashion on the slower timescale. The feature selection scheme employed in each
of our algorithms manages the energy and tracking components in a manner that
assists the search for the optimal sleep-scheduling policy. For the sake of
comparison, in both discounted and average settings, we also develop a function
approximation analogue of the Q-learning algorithm. This algorithm, unlike the
two-timescale variant, does not possess theoretical convergence guarantees.
Finally, we also adapt our algorithms to include a stochastic iterative
estimation scheme for the intruder's mobility model. Our simulation results on
a 2-dimensional network setting suggest that our algorithms result in better
tracking accuracy at the cost of only a few additional sensors, in comparison
to a recent prior work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7296</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7296</id><created>2013-12-27</created><authors><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Kumar</keyname><forenames>Amit</forenames></author></authors><title>Online Steiner Tree with Deletions</title><categories>cs.DS</categories><comments>An extended abstract appears in the SODA 2014 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the online Steiner tree problem, the input is a set of vertices that
appear one-by-one, and we have to maintain a Steiner tree on the current set of
vertices. The cost of the tree is the total length of edges in the tree, and we
want this cost to be close to the cost of the optimal Steiner tree at all
points in time. If we are allowed to only add edges, a tight bound of
$\Theta(\log n)$ on the competitiveness is known. Recently it was shown that if
we can add one new edge and make one edge swap upon every vertex arrival, we
can maintain a constant-competitive tree online.
  But what if the set of vertices sees both additions and deletions? Again, we
would like to obtain a low-cost Steiner tree with as few edge changes as
possible. The original paper of Imase and Waxman had also considered this
model, and it gave a greedy algorithm that maintained a constant-competitive
tree online, and made at most $O(n^{3/2})$ edge changes for the first $n$
requests. In this paper give the following two results.
  Our first result is an online algorithm that maintains a Steiner tree only
under deletions: we start off with a set of vertices, and at each time one of
the vertices is removed from this set: our Steiner tree no longer has to span
this vertex. We give an algorithm that changes only a constant number of edges
upon each request, and maintains a constant-competitive tree at all times. Our
algorithm uses the primal-dual framework and a global charging argument to
carefully make these constant number of changes.
  We then study the natural greedy algorithm proposed by Imase and Waxman that
maintains a constant-competitive Steiner tree in the fully-dynamic model (where
each request either adds or deletes a vertex). Our second result shows that
this algorithm makes only a constant number of changes per request in an
amortized sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7302</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7302</id><created>2013-12-27</created><updated>2014-04-23</updated><authors><author><keyname>Jain</keyname><forenames>Arjun</forenames></author><author><keyname>Tompson</keyname><forenames>Jonathan</forenames></author><author><keyname>Andriluka</keyname><forenames>Mykhaylo</forenames></author><author><keyname>Taylor</keyname><forenames>Graham W.</forenames></author><author><keyname>Bregler</keyname><forenames>Christoph</forenames></author></authors><title>Learning Human Pose Estimation Features with Convolutional Networks</title><categories>cs.CV cs.LG cs.NE</categories><report-no>NYU-TR-2013-CS0999</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new architecture for human pose estimation using a
multi- layer convolutional network architecture and a modified learning
technique that learns low-level features and higher-level weak spatial models.
Unconstrained human pose estimation is one of the hardest problems in computer
vision, and our new architecture and learning schema shows significant
improvement over the current state-of-the-art results. The main contribution of
this paper is showing, for the first time, that a specific variation of deep
learning is able to outperform all existing traditional architectures on this
task. The paper also discusses several lessons learned while researching
alternatives, most notably, that it is possible to learn strong low-level
feature detectors on features that might even just cover a few pixels in the
image. Higher-level spatial models improve somewhat the overall result, but to
a much lesser extent then expected. Many researchers previously argued that the
kinematic structure and top-down information is crucial for this domain, but
with our purely bottom up, and weak spatial model, we could improve other more
complicated architectures that currently produce the best results. This mirrors
what many other researchers, like those in the speech recognition, object
recognition, and other domains have experienced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7305</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7305</id><created>2013-12-27</created><updated>2015-03-04</updated><authors><author><keyname>Brattka</keyname><forenames>Vasco</forenames></author><author><keyname>Gherardi</keyname><forenames>Guido</forenames></author><author><keyname>H&#xf6;lzl</keyname><forenames>Rupert</forenames></author></authors><title>Probabilistic Computability and Choice</title><categories>math.LO cs.CC cs.LO</categories><comments>Information and Computation (accepted for publication)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational power of randomized computations on infinite
objects, such as real numbers. In particular, we introduce the concept of a Las
Vegas computable multi-valued function, which is a function that can be
computed on a probabilistic Turing machine that receives a random binary
sequence as auxiliary input. The machine can take advantage of this random
sequence, but it always has to produce a correct result or to stop the
computation after finite time if the random advice is not successful. With
positive probability the random advice has to be successful. We characterize
the class of Las Vegas computable functions in the Weihrauch lattice with the
help of probabilistic choice principles and Weak Weak K\H{o}nig's Lemma. Among
other things we prove an Independent Choice Theorem that implies that Las Vegas
computable functions are closed under composition. In a case study we show that
Nash equilibria are Las Vegas computable, while zeros of continuous functions
with sign changes cannot be computed on Las Vegas machines. However, we show
that the latter problem admits randomized algorithms with weaker failure
recognition mechanisms. The last mentioned results can be interpreted such that
the Intermediate Value Theorem is reducible to the jump of Weak Weak
K\H{o}nig's Lemma, but not to Weak Weak K\H{o}nig's Lemma itself. These
examples also demonstrate that Las Vegas computable functions form a proper
superclass of the class of computable functions and a proper subclass of the
class of non-deterministically computable functions. We also study the impact
of specific lower bounds on the success probabilities, which leads to a strict
hierarchy of classes. In particular, the classical technique of probability
amplification fails for computations on infinite objects. We also investigate
the dependency on the underlying probability space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7306</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7306</id><created>2013-12-27</created><authors><author><keyname>Aditya</keyname><forenames>Satabdi</forenames></author><author><keyname>DasGupta</keyname><forenames>Bhaskar</forenames></author><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author></authors><title>Algorithmic Perspectives of Network Transitive Reduction Problems and
  their Applications to Synthesis and Analysis of Biological Networks</title><categories>cs.CC cs.DS q-bio.MN</categories><msc-class>68Q25, 68Q17, 68R10, 05C85, 68W25, 92C42, 92C40</msc-class><acm-class>E.1; F.2.2; G.2.1; J.3</acm-class><journal-ref>Biology, 3 (1), 1-21, 2014</journal-ref><doi>10.3390/biology3010001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this survey paper, we will present a number of core algorithmic questions
concerning several transitive reduction problems on network that have
applications in network synthesis and analysis involving cellular processes.
Our starting point will be the so-called minimum equivalent digraph problem, a
classic computational problem in combinatorial algorithms. We will subsequently
consider a few non-trivial extensions or generalizations of this problem
motivated by applications in systems biology. We will then discuss the
applications of these algorithmic methodologies in the context of three major
biological research questions: synthesizing and simplifying signal transduction
networks, analyzing disease networks, and measuring redundancy of biological
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7308</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7308</id><created>2013-12-27</created><authors><author><keyname>Jamieson</keyname><forenames>Kevin</forenames></author><author><keyname>Malloy</keyname><forenames>Matthew</forenames></author><author><keyname>Nowak</keyname><forenames>Robert</forenames></author><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author></authors><title>lil' UCB : An Optimal Exploration Algorithm for Multi-Armed Bandits</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proposes a novel upper confidence bound (UCB) procedure for
identifying the arm with the largest mean in a multi-armed bandit game in the
fixed confidence setting using a small number of total samples. The procedure
cannot be improved in the sense that the number of samples required to identify
the best arm is within a constant factor of a lower bound based on the law of
the iterated logarithm (LIL). Inspired by the LIL, we construct our confidence
bounds to explicitly account for the infinite time horizon of the algorithm. In
addition, by using a novel stopping time for the algorithm we avoid a union
bound over the arms that has been observed in other UCB-type algorithms. We
prove that the algorithm is optimal up to constants and also show through
simulations that it provides superior performance with respect to the
state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7326</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7326</id><created>2013-11-17</created><authors><author><keyname>Kamberaj</keyname><forenames>Hiqmet</forenames></author></authors><title>Replica Exchange using q-Gaussian Swarm Quantum Particle Intelligence
  Method</title><categories>cs.AI</categories><comments>10 pages, 5 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a newly developed Replica Exchange algorithm using q -Gaussian
Swarm Quantum Particle Optimization (REX@q-GSQPO) method for solving the
problem of finding the global optimum. The basis of the algorithm is to run
multiple copies of independent swarms at different values of q parameter. Based
on an energy criterion, chosen to satisfy the detailed balance, we are swapping
the particle coordinates of neighboring swarms at regular iteration intervals.
The swarm replicas with high q values are characterized by high diversity of
particles allowing escaping local minima faster, while the low q replicas,
characterized by low diversity of particles, are used to sample more
efficiently the local basins. We compare the new algorithm with the standard
Gaussian Swarm Quantum Particle Optimization (GSQPO) and q-Gaussian Swarm
Quantum Particle Optimization (q-GSQPO) algorithms, and we found that the new
algorithm is more robust in terms of the number of fitness function calls, and
more efficient in terms ability convergence to the global minimum. In
additional, we also provide a method of optimally allocating the swarm replicas
among different q values. Our algorithm is tested for three benchmark
functions, which are known to be multimodal problems, at different
dimensionalities. In addition, we considered a polyalanine peptide of 12
residues modeled using a G\=o coarse-graining potential energy function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7335</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7335</id><created>2013-12-20</created><updated>2014-02-16</updated><authors><author><keyname>K&#xe9;gl</keyname><forenames>Bal&#xe1;zs</forenames></author></authors><title>Correlation-based construction of neighborhood and edge features</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by an abstract notion of low-level edge detector filters, we
propose a simple method of unsupervised feature construction based on pairwise
statistics of features. In the first step, we construct neighborhoods of
features by regrouping features that correlate. Then we use these subsets as
filters to produce new neighborhood features. Next, we connect neighborhood
features that correlate, and construct edge features by subtracting the
correlated neighborhood features of each other. To validate the usefulness of
the constructed features, we ran AdaBoost.MH on four multi-class classification
problems. Our most significant result is a test error of 0.94% on MNIST with an
algorithm which is essentially free of any image-specific priors. On CIFAR-10
our method is suboptimal compared to today's best deep learning techniques,
nevertheless, we show that the proposed method outperforms not only boosting on
the raw pixels, but also boosting on Haar filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7345</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7345</id><created>2013-12-26</created><authors><author><keyname>Celebi</keyname><forenames>M. Emre</forenames></author><author><keyname>Wen</keyname><forenames>Quan</forenames></author><author><keyname>Hwang</keyname><forenames>Sae</forenames></author><author><keyname>Iyatomi</keyname><forenames>Hitoshi</forenames></author><author><keyname>Schaefer</keyname><forenames>Gerald</forenames></author></authors><title>Lesion Border Detection in Dermoscopy Images Using Ensembles of
  Thresholding Methods</title><categories>cs.CV</categories><comments>8 pages, 3 figures, 2 tables. arXiv admin note: substantial text
  overlap with arXiv:1009.1362</comments><acm-class>I.4.6</acm-class><journal-ref>Skin Research and Technology 19 (2013) e252--e258</journal-ref><doi>10.1111/j.1600-0846.2012.00636.x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dermoscopy is one of the major imaging modalities used in the diagnosis of
melanoma and other pigmented skin lesions. Due to the difficulty and
subjectivity of human interpretation, automated analysis of dermoscopy images
has become an important research area. Border detection is often the first step
in this analysis. In many cases, the lesion can be roughly separated from the
background skin using a thresholding method applied to the blue channel.
However, no single thresholding method appears to be robust enough to
successfully handle the wide variety of dermoscopy images encountered in
clinical practice. In this paper, we present an automated method for detecting
lesion borders in dermoscopy images using ensembles of thresholding methods.
Experiments on a difficult set of 90 images demonstrate that the proposed
method is robust, fast, and accurate when compared to nine state-of-the-art
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7352</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7352</id><created>2013-12-27</created><authors><author><keyname>Teuben</keyname><forenames>Peter</forenames></author><author><keyname>Allen</keyname><forenames>Alice</forenames></author><author><keyname>Berriman</keyname><forenames>Bruce</forenames></author><author><keyname>DuPrie</keyname><forenames>Kimberly</forenames></author><author><keyname>Hanisch</keyname><forenames>Robert J.</forenames></author><author><keyname>Mink</keyname><forenames>Jessica</forenames></author><author><keyname>Nemiroff</keyname><forenames>Robert</forenames></author><author><keyname>Shamir</keyname><forenames>Lior</forenames></author><author><keyname>Shortridge</keyname><forenames>Keith</forenames></author><author><keyname>Taylor</keyname><forenames>Mark</forenames></author><author><keyname>Wallin</keyname><forenames>John</forenames></author></authors><title>Ideas for Advancing Code Sharing (A Different Kind of Hack Day)</title><categories>astro-ph.IM cs.DL</categories><comments>To be published in Proceedings of ADASS XXIII. Links to notes from
  brainstorming sessions are available here: http://asterisk.apod.com/wp/?p=543</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How do we as a community encourage the reuse of software for telescope
operations, data processing, and calibration? How can we support making codes
used in research available for others to examine? Continuing the discussion
from last year Bring out your codes! BoF session, participants separated into
groups to brainstorm ideas to mitigate factors which inhibit code sharing and
nurture those which encourage code sharing. The BoF concluded with the sharing
of ideas that arose from the brainstorming sessions and a brief summary by the
moderator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7354</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7354</id><created>2013-12-22</created><authors><author><keyname>Mamun</keyname><forenames>Md. Selim Al</forenames></author><author><keyname>Hossain</keyname><forenames>Syed Monowar</forenames></author></authors><title>Design of Reversible Random Access Memory</title><categories>cs.ET cs.AR</categories><journal-ref>International Journal of Computer Applications,Volume 56 - Number
  15 , Year of Publication: 2012</journal-ref><doi>10.5120/8967-3182</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible logic has become immensely popular research area and its
applications have spread in various technologies for their low power
consumption. In this paper we proposed an efficient design of random access
memory using reversible logic. In the way of designing the reversible random
access memory we proposed a reversible decoder and a write enable reversible
master slave D flip-flop. All the reversible designs are superior in terms of
quantum cost, delay and garbage outputs compared to the designs existing in
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7355</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7355</id><created>2013-12-22</created><authors><author><keyname>Mamun</keyname><forenames>Md. Selim Al</forenames></author><author><keyname>Mondal</keyname><forenames>Pronab Kumar</forenames></author><author><keyname>Prodhan</keyname><forenames>Uzzal Kumar</forenames></author></authors><title>A Novel Approach for Designing Online Testable Reversible Circuits</title><categories>cs.ET cs.AR</categories><journal-ref>International Journal of Engineering Research and Development,
  Volume 5, Issue 2, PP. 39-44 Year 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible logic is gaining interest of many researchers due to its low power
dissipating characteristic. In this paper we proposed a new approach for
designing online testable reversible circuits. The resultant testable
reversible circuit can detect any single bit error whiles it is operating.
Appropriate theorems and lemmas are presented to clarify the proposed design.
The experimental results show that our design approach is superior in terms of
number of number of gates, garbage outputs and quantum cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7363</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7363</id><created>2013-12-27</created><authors><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>How beta-skeletons lose their edges</title><categories>cs.CG</categories><journal-ref>Adamatzky A. How {\beta}-skeletons lose their edges. Information
  Sciences 254 (2014) 213-224</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A {\beta}-skeleton is a proximity graphs with node neighbourhood defined by
continuous-valued parameter {\beta}. Two nodes in a {\beta}-skeleton are
connected by an edge if their lune-based neighbourhood contains no other nodes.
With increase of {\beta} some edges a skeleton are disappear. We study how a
number of edges in {\beta}-skeleton depends on {\beta}. We speculate how this
dependence can be used to discriminate between random and non-random planar
sets. We also analyse stability of {\beta}-skeletons and their sensitivity to
perturbations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7366</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7366</id><created>2013-12-27</created><updated>2014-05-14</updated><authors><author><keyname>Chan</keyname><forenames>Stanley H.</forenames></author><author><keyname>Zickler</keyname><forenames>Todd</forenames></author><author><keyname>Lu</keyname><forenames>Yue M.</forenames></author></authors><title>Monte Carlo non local means: Random sampling for large-scale image
  filtering</title><categories>cs.CV stat.CO</categories><comments>submitted for publication</comments><doi>10.1109/TIP.2014.2327813</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a randomized version of the non-local means (NLM) algorithm for
large-scale image filtering. The new algorithm, called Monte Carlo non-local
means (MCNLM), speeds up the classical NLM by computing a small subset of image
patch distances, which are randomly selected according to a designed sampling
pattern. We make two contributions. First, we analyze the performance of the
MCNLM algorithm and show that, for large images or large external image
databases, the random outcomes of MCNLM are tightly concentrated around the
deterministic full NLM result. In particular, our error probability bounds show
that, at any given sampling ratio, the probability for MCNLM to have a large
deviation from the original NLM solution decays exponentially as the size of
the image or database grows. Second, we derive explicit formulas for optimal
sampling patterns that minimize the error probability bound by exploiting
partial knowledge of the pairwise similarity weights. Numerical experiments
show that MCNLM is competitive with other state-of-the-art fast NLM algorithms
for single-image denoising. When applied to denoising images using an external
database containing ten billion patches, MCNLM returns a randomized solution
that is within 0.2 dB of the full NLM solution while reducing the runtime by
three orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7373</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7373</id><created>2013-12-27</created><updated>2014-01-20</updated><authors><author><keyname>Milani</keyname><forenames>Mostafa</forenames></author><author><keyname>Bertossi</keyname><forenames>Leopoldo</forenames></author><author><keyname>Ariyan</keyname><forenames>Sina</forenames></author></authors><title>Extending Contexts with Ontologies for Multidimensional Data Quality
  Assessment</title><categories>cs.DB</categories><comments>To appear in Proc. 5th International Workshop on Data Engineering
  meets the Semantic Web (DESWeb). In conjunction with ICDE 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data quality and data cleaning are context dependent activities. Starting
from this observation, in previous work a context model for the assessment of
the quality of a database instance was proposed. In that framework, the context
takes the form of a possibly virtual database or data integration system into
which a database instance under quality assessment is mapped, for additional
analysis and processing, enabling quality assessment. In this work we extend
contexts with dimensions, and by doing so, we make possible a multidimensional
assessment of data quality assessment. Multidimensional contexts are
represented as ontologies written in Datalog+-. We use this language for
representing dimensional constraints, and dimensional rules, and also for doing
query answering based on dimensional navigation, which becomes an important
auxiliary activity in the assessment of data. We show ideas and mechanisms by
means of examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7377</identifier>
 <datestamp>2014-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7377</id><created>2013-12-27</created><updated>2014-08-20</updated><authors><author><keyname>Li</keyname><forenames>Zhongkui</forenames></author><author><keyname>Wen</keyname><forenames>Guanghui</forenames></author><author><keyname>Duan</keyname><forenames>Zhisheng</forenames></author><author><keyname>Ren</keyname><forenames>Wei</forenames></author></authors><title>Designing Fully Distributed Consensus Protocols for Linear Multi-agent
  Systems with Directed Graphs</title><categories>math.OC cs.SY</categories><comments>16 page, 3 figures. To appear in IEEE Transactions on Automatic
  Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the distributed consensus protocol design problem for
multi-agent systems with general linear dynamics and directed communication
graphs. Existing works usually design consensus protocols using the smallest
real part of the nonzero eigenvalues of the Laplacian matrix associated with
the communication graph, which however is global information. In this paper,
based on only the agent dynamics and the relative states of neighboring agents,
a distributed adaptive consensus protocol is designed to achieve
leader-follower consensus for any communication graph containing a directed
spanning tree with the leader as the root node. The proposed adaptive protocol
is independent of any global information of the communication graph and thereby
is fully distributed. Extensions to the case with multiple leaders are further
studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7381</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7381</id><created>2013-12-27</created><updated>2014-04-16</updated><authors><author><keyname>Giraldo</keyname><forenames>Luis G. Sanchez</forenames></author><author><keyname>Principe</keyname><forenames>Jose C.</forenames></author></authors><title>Rate-Distortion Auto-Encoders</title><categories>cs.LG</categories><comments>Submission International Conference on Learning Representations 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A rekindled the interest in auto-encoder algorithms has been spurred by
recent work on deep learning. Current efforts have been directed towards
effective training of auto-encoder architectures with a large number of coding
units. Here, we propose a learning algorithm for auto-encoders based on a
rate-distortion objective that minimizes the mutual information between the
inputs and the outputs of the auto-encoder subject to a fidelity constraint.
The goal is to learn a representation that is minimally committed to the input
data, but that is rich enough to reconstruct the inputs up to certain level of
distortion. Minimizing the mutual information acts as a regularization term
whereas the fidelity constraint can be understood as a risk functional in the
conventional statistical learning setting. The proposed algorithm uses a
recently introduced measure of entropy based on infinitely divisible matrices
that avoids the plug in estimation of densities. Experiments using
over-complete bases show that the rate-distortion auto-encoders can learn a
regularized input-output mapping in an implicit manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7412</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7412</id><created>2013-12-28</created><updated>2014-04-28</updated><authors><author><keyname>Besselink</keyname><forenames>Bart</forenames></author><author><keyname>Sandberg</keyname><forenames>Henrik</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Model reduction of networked passive systems through clustering</title><categories>cs.SY math.DS</categories><comments>7 pages, 2 figures; minor changes in the final version, as accepted
  for publication at the 13th European Control Conference, Strasbourg, France</comments><msc-class>93A15 (Primary) 93B05, 93B07, 93C05 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a model reduction procedure for a network of interconnected
identical passive subsystems is presented. Here, rather than performing model
reduction on the subsystems, adjacent subsystems are clustered, leading to a
reduced-order networked system that allows for a convenient physical
interpretation. The identification of the subsystems to be clustered is
performed through controllability and observability analysis of an associated
edge system and it is shown that the property of synchronization (i.e., the
convergence of trajectories of the subsystems to each other) is preserved
during reduction. The results are illustrated by means of an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7414</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7414</id><created>2013-12-28</created><authors><author><keyname>Hajebi</keyname><forenames>Kiana</forenames></author><author><keyname>Zhang</keyname><forenames>Hong</forenames></author></authors><title>Stopping Rules for Bag-of-Words Image Search and Its Application in
  Appearance-Based Localization</title><categories>cs.CV cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a technique to improve the search efficiency of the bag-of-words
(BoW) method for image retrieval. We introduce a notion of difficulty for the
image matching problems and propose methods that reduce the amount of
computations required for the feature vector-quantization task in BoW by
exploiting the fact that easier queries need less computational resources.
Measuring the difficulty of a query and stopping the search accordingly is
formulated as a stopping problem. We introduce stopping rules that terminate
the image search depending on the difficulty of each query, thereby
significantly reducing the computational cost. Our experimental results show
the effectiveness of our approach when it is applied to appearance-based
localization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7422</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7422</id><created>2013-12-28</created><authors><author><keyname>Fink</keyname><forenames>Michael</forenames></author><author><keyname>Lierler</keyname><forenames>Yuliya</forenames></author></authors><title>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers presented at the sixth workshop on Answer Set
Programming and Other Computing Paradigms (ASPOCP 2013) held on August 25th,
2013 in Istanbul, co-located with the 29th International Conference on Logic
Programming (ICLP 2013). It thus continues a series of previous events
co-located with ICLP, aiming at facilitating the discussion about crossing the
boundaries of current ASP techniques in theory, solving, and applications, in
combination with or inspired by other computing paradigms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7425</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7425</id><created>2013-12-28</created><updated>2014-06-05</updated><authors><author><keyname>Elder</keyname><forenames>Murray</forenames></author><author><keyname>Taback</keyname><forenames>Jennifer</forenames></author></authors><title>$\mathcal C$-graph automatic groups</title><categories>math.GR cs.FL</categories><comments>26 pages, 3 figures. To appear in Journal of Algebra</comments><msc-class>20F65, 68Q45</msc-class><doi>10.1016/j.jalgebra.2014.04.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the notion of a graph automatic group introduced by
Kharlampovich, Khoussainov and Miasnikov (arXiv:1107.3645) by replacing the
regular languages in their definition with more powerful language classes. For
a fixed language class $\mathcal C$, we call the resulting groups $\mathcal
C$-graph automatic. We prove that the class of $\mathcal C$-graph automatic
groups is closed under change of generating set, direct and free product for
certain classes $\mathcal C$. We show that for quasi-realtime counter-graph
automatic groups where normal forms have length that is linear in the geodesic
length, there is an algorithm to compute normal forms (and therefore solve the
word problem) in polynomial time. The class of quasi-realtime counter-graph
automatic groups includes all Baumslag-Solitar groups, and the free group of
countably infinite rank. Context-sensitive-graph automatic groups are shown to
be a very large class, which encompasses, for example, groups with unsolvable
conjugacy problem, the Grigorchuk group, and Thompson's groups $F,T$ and $V$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7430</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7430</id><created>2013-12-28</created><authors><author><keyname>Prashanth</keyname><forenames>L. A.</forenames></author><author><keyname>Prasad</keyname><forenames>H. L.</forenames></author><author><keyname>Desai</keyname><forenames>Nirmit</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author><author><keyname>Dasgupta</keyname><forenames>Gargi</forenames></author></authors><title>Simultaneous Perturbation Methods for Adaptive Labor Staffing in Service
  Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service systems are labor intensive due to the large variation in the tasks
required to address service requests from multiple customers. Aligning the
staffing levels to the forecasted workloads adaptively in such systems is
nontrivial because of a large number of parameters and operational variations
leading to a huge search space. A challenging problem here is to optimize the
staffing while maintaining the system in steady-state and compliant to
aggregate service level agreement (SLA) constraints. Further, because these
parameters change on a weekly basis, the optimization should not take longer
than a few hours. We formulate this problem as a constrained Markov cost
process parameterized by the (discrete) staffing levels. We propose novel
simultaneous perturbation stochastic approximation (SPSA) based SASOC (Staff
Allocation using Stochastic Optimization with Constraints) algorithms for
solving the above problem. The algorithms include both first order as well as
second order methods and incorporate SPSA based gradient estimates in the
primal, with dual ascent for the Lagrange multipliers. Both the algorithms that
we propose are online, incremental and easy to implement. Further, they involve
a certain generalized smooth projection operator, which is essential to project
the continuous-valued worker parameter tuned by SASOC algorithms onto the
discrete set. We validated our algorithms on five real-life service systems and
compared them with a state-of-the-art optimization tool-kit OptQuest. Being 25
times faster than OptQuest, our algorithms are particularly suitable for
adaptive labor staffing. Also, we observe that our algorithms guarantee
convergence and find better solutions than OptQuest in many cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7432</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7432</id><created>2013-12-28</created><authors><author><keyname>Grama</keyname><forenames>Cristina-Nicol</forenames></author><author><keyname>Baggio</keyname><forenames>Rodolfo</forenames></author></authors><title>A network analysis of Sibiu County, Romania</title><categories>physics.soc-ph cs.SI</categories><comments>Research note, 6 pages, 2 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network science methods have proved to be able to provide useful insights
from both a theoretical and a practical point of view in that they can better
inform governance policies in complex dynamic environments. The tourism
research community has provided an increasing number of works that analyse
destinations from a network science perspective. However, most of the studies
refer to relatively small samples of actors and linkages. With this note we
provide a full network study, although at a preliminary stage, that reports a
complete analysis of a Romanian destination (Sibiu). Our intention is to
increase the set of similar studies with the aim of supporting the
investigations in structural and dynamical characteristics of tourism
destinations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7436</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7436</id><created>2013-12-28</created><authors><author><keyname>Ritter</keyname><forenames>Daniel</forenames></author></authors><title>Advanced Data Processing in the Business Network System</title><categories>cs.OH</categories><comments>5 pages, 2nd International Conference on Knowledge Discovery (ICKD),
  Copenhagen, 2013. Proceedings in the International Journal of Machine
  Learning and Computing (IJMLC)</comments><report-no>Article 300</report-no><journal-ref>International Journal of Machine Learning and Computing, Volume 3,
  Number 2, 2013</journal-ref><doi>10.7763/IJMLC</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The discovery, representation and reconstruction of Business Networks (BN)
from Network Mining (NM) raw data is a difficult problem for enterprises. This
is due to huge amounts of e.g. complex business processes within and across
enterprise boundaries, heterogeneous technology stacks, and fragmented data. To
remain competitive, visibility into the enterprise and partner networks on
different, interrelated abstraction levels is desirable.
  We show the query and data processing capabilities of a novel data discovery,
mining and network inference system, called Business Network System (BNS) that
reconstructs the BN--integration and business process networks - from raw data,
hidden in the enterprises' landscapes. The paper covers both the foundation and
the key data processing characteristics features of BNS, including its
underlying technologies, its overall system architecture, and data provenance
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7441</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7441</id><created>2013-12-28</created><authors><author><keyname>Ren</keyname><forenames>Hai-Peng</forenames></author><author><keyname>Zhao</keyname><forenames>Yang</forenames></author></authors><title>A Novel Carrier Waveform Inter-Displacement Modulation Method in
  Underwater Communication Channel</title><categories>cs.OH</categories><comments>8 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the main way of underwater wireless communication, underwater acoustic
communication is one of the focuses of ocean research. Compared with the free
space wireless communication channel, the underwater acoustic channel suffers
from more severe multipath effect, the less available bandwidth and the even
complex noise. The underwater acoustic channel is one of the most complicated
wireless communication channels. To achieve a reliable underwater acoustic
communication, Phase Shift Keying (PSK) modulation and Passive Time Reversal
Mirror (PTRM) equalization are considered to be a suitable scheme. However, due
to the serious distortion of the received signal caused by the channel, this
scheme suffers from a high Bit Error Rate (BER) under the condition of the low
Signal to Noise Ratio (SNR). To solve this problem, we proposes a Carrier
Waveform Inter-Displacement (CWID) modulation method based on the Linear
Frequency Modulation (LFM) PSK and PTRM scheme. The new communication scheme
reduces BER by increasing the difference from the carrier waveform for
different symbols. Simulation results show the effectiveness and superiority of
the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7442</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7442</id><created>2013-12-28</created><authors><author><keyname>Hamodi</keyname><forenames>Jamil</forenames></author><author><keyname>Salah</keyname><forenames>Khaled</forenames></author><author><keyname>Thool</keyname><forenames>Ravindra</forenames></author></authors><title>Evaluating the Performance of IPTV over Fixed WiMAX</title><categories>cs.MM cs.NI</categories><comments>9 Pages, 9 Figures. arXiv admin note: substantial text overlap with
  other internet sources by other authors</comments><journal-ref>International Journal of Computer Applications 84(6):35-43,
  December 2013. Published by Foundation of Computer Science, New York, USA</journal-ref><doi>10.5120/14582-2812</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  IEEE specifies different modulation techniques for WiMAX; namely, BPSK, QPSK,
16 QAM and 64 QAM. This paper studies the performance of Internet Protocol
Television (IPTV) over Fixed WiMAX system considering different combinations of
digital modulation. The performance is studied taking into account a number of
key system parameters which include the variation in the video coding,
path-loss, scheduling service classes different rated codes in FEC channel
coding. The performance study was conducted using OPNET simulation. The
performance is studied in terms of packet lost, packet jitter delay, end-to-end
delay, and network throughput. Simulation results show that higher order
modulation and coding schemes (namely, 16 QAM and 64 QAM) yield better
performance than that of QPSK.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7444</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7444</id><created>2013-12-28</created><authors><author><keyname>Chowdhury</keyname><forenames>Mohammad Jabed Morshed</forenames></author><author><keyname>Chakraborty</keyname><forenames>Narayan Ranjan</forenames></author></authors><title>CAPTCHA Based on Human Cognitive Factor</title><categories>cs.HC cs.CY</categories><comments>International Journal of Advanced Computer Science and Applications,
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A CAPTCHA (Completely Automated Public Turing test to tell Computers and
Humans Apart) is an automatic security mechanism used to determine whether the
user is a human or a malicious computer program. It is a program that generates
and grades tests that are human solvable, but intends to be beyond the
capabilities of current computer programs. CAPTCHA should be designed to be
very easy for humans but very hard for machines. Unfortunately, the existing
CAPTCHA systems while trying to maximize the difficulty for automated programs
to pass tests by increasing distortion or noise have consequently, made it also
very difficult for potential users. To address the issue, this paper addresses
an alternative form of CAPTCHA that provides a variety of questions from
mathematical, logical and general problems which only human can understand and
answer correctly in a given time. The proposed framework supports diversity in
choosing the questions to be answered and a user-friendly framework to the
users. A user-study is also conducted to judge the performance of the developed
system with different background. The study shows the efficacy of the
implemented system with a good level of user satisfaction over traditional
CAPTCHA available today.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7445</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7445</id><created>2013-12-28</created><authors><author><keyname>Zhao</keyname><forenames>Yu</forenames></author><author><keyname>Duan</keyname><forenames>Zhisheng</forenames></author><author><keyname>Li</keyname><forenames>Zhongkui</forenames></author></authors><title>Distributed average tracking for multiple reference signals with general
  linear dynamics</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical note studies the distributed average tracking problem for
multiple time-varying signals with general linear dynamics, whose reference
inputs are nonzero and not available to any agent in the network. In
distributed fashion, a pair of continuous algorithms with, respectively, static
and adaptive coupling strengths are designed. Based on the boundary layer
concept, the proposed continuous algorithm with static coupling strengths can
asymptotically track the average of the multiple reference signals without
chattering phenomenon. Furthermore, for the case of algorithms with adaptive
coupling strengths, the average tracking errors are uniformly ultimately
bounded and exponentially converge to a small adjustable bounded set. Finally,
a simulation example is presented to show the validity of the theoretical
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7446</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7446</id><created>2013-12-28</created><updated>2014-07-21</updated><authors><author><keyname>Huang</keyname><forenames>Sheng</forenames></author><author><keyname>Yang</keyname><forenames>Dan</forenames></author><author><keyname>Zhang</keyname><forenames>Haopeng</forenames></author><author><keyname>Huangfu</keyname><forenames>Luwen</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaohong</forenames></author></authors><title>Shape Primitive Histogram: A Novel Low-Level Face Representation for
  Face Recognition</title><categories>cs.CV</categories><comments>second version, two columns and 11 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We further exploit the representational power of Haar wavelet and present a
novel low-level face representation named Shape Primitives Histogram (SPH) for
face recognition. Since human faces exist abundant shape features, we address
the face representation issue from the perspective of the shape feature
extraction. In our approach, we divide faces into a number of tiny shape
fragments and reduce these shape fragments to several uniform atomic shape
patterns called Shape Primitives. A convolution with Haar Wavelet templates is
applied to each shape fragment to identify its belonging shape primitive. After
that, we do a histogram statistic of shape primitives in each spatial local
image patch for incorporating the spatial information. Finally, each face is
represented as a feature vector via concatenating all the local histograms of
shape primitives. Four popular face databases, namely ORL, AR, Yale-B and LFW-a
databases, are employed to evaluate SPH and experimentally study the choices of
the parameters. Extensive experimental results demonstrate that the proposed
approach outperform the state-of-the-arts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7447</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7447</id><created>2013-12-28</created><authors><author><keyname>Li</keyname><forenames>Zhongkui</forenames></author><author><keyname>Duan</keyname><forenames>Zhisheng</forenames></author><author><keyname>Ren</keyname><forenames>Wei</forenames></author><author><keyname>Feng</keyname><forenames>Gang</forenames></author></authors><title>Containment Control of Linear Multi-Agent Systems with Multiple Leaders
  of Bounded Inputs Using Distributed Continuous Controllers</title><categories>cs.SY math.OC</categories><comments>16 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1312.7379</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the containment control problem for multi-agent systems
with general linear dynamics and multiple leaders whose control inputs are
possibly nonzero and time varying. Based on the relative states of neighboring
agents, a distributed static continuous controller is designed, under which the
containment error is uniformly ultimately bounded and the upper bound of the
containment error can be made arbitrarily small, if the subgraph associated
with the followers is undirected and for each follower there exists at least
one leader that has a directed path to that follower. It is noted that the
design of the static controller requires the knowledge of the eigenvalues of
the Laplacian matrix and the upper bounds of the leaders' control inputs. In
order to remove these requirements, a distributed adaptive continuous
controller is further proposed, which can be designed and implemented by each
follower in a fully distributed fashion. Extensions to the case where only
local output information is available are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7463</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7463</id><created>2013-12-28</created><authors><author><keyname>Audhkhasi</keyname><forenames>Kartik</forenames></author><author><keyname>Sethy</keyname><forenames>Abhinav</forenames></author><author><keyname>Ramabhadran</keyname><forenames>Bhuvana</forenames></author><author><keyname>Narayanan</keyname><forenames>Shrikanth S.</forenames></author></authors><title>Generalized Ambiguity Decomposition for Understanding Ensemble Diversity</title><categories>stat.ML cs.CV cs.LG</categories><comments>32 pages, 10 figures</comments><acm-class>I.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diversity or complementarity of experts in ensemble pattern recognition and
information processing systems is widely-observed by researchers to be crucial
for achieving performance improvement upon fusion. Understanding this link
between ensemble diversity and fusion performance is thus an important research
question. However, prior works have theoretically characterized ensemble
diversity and have linked it with ensemble performance in very restricted
settings. We present a generalized ambiguity decomposition (GAD) theorem as a
broad framework for answering these questions. The GAD theorem applies to a
generic convex ensemble of experts for any arbitrary twice-differentiable loss
function. It shows that the ensemble performance approximately decomposes into
a difference of the average expert performance and the diversity of the
ensemble. It thus provides a theoretical explanation for the
empirically-observed benefit of fusing outputs from diverse classifiers and
regressors. It also provides a loss function-dependent, ensemble-dependent, and
data-dependent definition of diversity. We present extensions of this
decomposition to common regression and classification loss functions, and
report a simulation-based analysis of the diversity term and the accuracy of
the decomposition. We finally present experiments on standard pattern
recognition data sets which indicate the accuracy of the decomposition for
real-world classification and regression problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7468</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7468</id><created>2013-12-28</created><updated>2013-12-30</updated><authors><author><keyname>Balaji</keyname><forenames>Nikhil</forenames></author><author><keyname>Datta</keyname><forenames>Samir</forenames></author></authors><title>Tree-width and Logspace: Determinants and Counting Euler Tours</title><categories>cs.CC cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the recent result of [EJT10] showing that MSO properties are
Logspace computable on graphs of bounded tree-width, we consider the complexity
of computing the determinant of the adjacency matrix of a bounded tree-width
graph and prove that it is L-complete. It is important to notice that the
determinant is neither an MSO-property nor counts the number of solutions of an
MSO-predicate. We extend this technique to count the number of spanning
arborescences and directed Euler tours in bounded tree-width digraphs, and
further to counting the number of spanning trees and the number of Euler tours
in undirected graphs, all in L. Notice that undirected Euler tours are not
known to be MSO-expressible and the corresponding counting problem is in fact
#P-hard for general graphs. Counting undirected Euler tours in bounded
tree-width graphs was not known to be polynomial time computable till very
recently Chebolu et al [CCM13] gave a polynomial time algorithm for this
problem (concurrently and independently of this work). Finally, we also show
some linear algebraic extensions of the determinant algorithm to show how to
compute the charcteristic polynomial and trace of the powers of a bounded
tree-width graph in L.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7469</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7469</id><created>2013-12-28</created><updated>2014-02-08</updated><authors><author><keyname>Huang</keyname><forenames>Sheng</forenames></author><author><keyname>Yang</keyname><forenames>Dan</forenames></author><author><keyname>Yang</keyname><forenames>Dong</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author></authors><title>Collaborative Discriminant Locality Preserving Projections With its
  Application to Face Recognition</title><categories>cs.CV</categories><comments>second version</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a novel Discriminant Locality Preserving Projections (DLPP)
algorithm named Collaborative Discriminant Locality Preserving Projection
(CDLPP). In our algorithm, the discriminating power of DLPP are further
exploited from two aspects. On the one hand, the global optimum of class
scattering is guaranteed via using the between-class scatter matrix to replace
the original denominator of DLPP. On the other hand, motivated by collaborative
representation, an $L_2$-norm constraint is imposed to the projections to
discover the collaborations of dimensions in the sample space. We apply our
algorithm to face recognition. Three popular face databases, namely AR, ORL and
LFW-A, are employed for evaluating the performance of CDLPP. Extensive
experimental results demonstrate that CDLPP significantly improves the
discriminating power of DLPP and outperforms the state-of-the-arts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7477</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7477</id><created>2013-12-28</created><authors><author><keyname>Wang</keyname><forenames>Han</forenames></author></authors><title>Covering with Excess One: Seeing the Topology</title><categories>math.GN cs.CG cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have initiated the study of topology of the space of coverings on grid
domains. The space has the following constraint: while all the covering agents
can move freely (we allow overlapping) on the domain, their union must cover
the whole domain. A minimal number $N$ of the covering agents is required for a
successful covering of the domain. In this paper, we demonstrate beautiful
topological structures of this space on grid domains in 2D with $N+1$
coverings, the topology of the space has the homotopy type of $1$ dimensional
complex, regardless of the domain shape. We also present the Euler
characteristic formula which connects the topology of the space with that of
the domain itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7482</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7482</id><created>2013-12-28</created><updated>2015-04-24</updated><authors><author><keyname>Dillon</keyname><forenames>Keith</forenames></author><author><keyname>Fainman</keyname><forenames>Yeshaiahu</forenames></author></authors><title>Element-wise uniqueness, prior knowledge, and data-dependent resolution</title><categories>math.OC cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Techniques for finding regularized solutions to underdetermined linear
systems can be viewed as imposing prior knowledge on the unknown vector. The
success of modern techniques, which can impose priors such as sparsity and
non-negativity, is the result of advances in optimization algorithms to solve
problems which lack closed-form solutions. Techniques for characterization and
analysis of the system to determined when information is recoverable, however,
still typically rely on closed-form solution techniques such as singular value
decomposition or a filter cutoff, for example. In this letter we pose
optimization approaches to broaden the approach to system characterization. We
start by deriving conditions for when each unknown element of a system admits a
unique solution, subject to a broad class of types of prior knowledge. With
this approach we can pose a convex optimization problem to find &quot;how unique&quot;
each element of the solution is, which may be viewed as a generalization of
resolution to incorporate prior knowledge. We find that the result varies with
the unknown vector itself, i.e. is data-dependent, such as when the sparsity of
the solution improves the chance it can be uniquely reconstructed. The approach
can be used to analyze systems on a case-by-case basis, estimate the amount of
important information present in the data, and quantitatively understand the
degree to which the regularized solution may be trusted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7485</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7485</id><created>2013-12-28</created><authors><author><keyname>Bareinboim</keyname><forenames>Elias</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>A General Algorithm for Deciding Transportability of Experimental
  Results</title><categories>cs.AI stat.ME stat.ML</categories><journal-ref>Journal of Causal Inference, 2013; 1(1): 107-134</journal-ref><doi>10.1515/jci-2012-0004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalizing empirical findings to new environments, settings, or populations
is essential in most scientific explorations. This article treats a particular
problem of generalizability, called &quot;transportability&quot;, defined as a license to
transfer information learned in experimental studies to a different population,
on which only observational studies can be conducted. Given a set of
assumptions concerning commonalities and differences between the two
populations, Pearl and Bareinboim (2011) derived sufficient conditions that
permit such transfer to take place. This article summarizes their findings and
supplements them with an effective procedure for deciding when and how
transportability is feasible. It establishes a necessary and sufficient
condition for deciding when causal effects in the target population are
estimable from both the statistical information available and the causal
information transferred from the experiments. The article further provides a
complete algorithm for computing the transport formula, that is, a way of
combining observational and experimental information to synthesize bias-free
estimate of the desired causal relation. Finally, the article examines the
differences between transportability and other variants of generalizability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7499</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7499</id><created>2013-12-28</created><authors><author><keyname>Boutsidis</keyname><forenames>Christos</forenames></author><author><keyname>Magdon-Ismail</keyname><forenames>Malik</forenames></author></authors><title>A note on sparse least-squares regression</title><categories>cs.DS</categories><comments>Information Processing Letters, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compute a \emph{sparse} solution to the classical least-squares problem
$\min_x||A x -b||,$ where $A$ is an arbitrary matrix. We describe a novel
algorithm for this sparse least-squares problem. The algorithm operates as
follows: first, it selects columns from $A$, and then solves a least-squares
problem only with the selected columns. The column selection algorithm that we
use is known to perform well for the well studied column subset selection
problem. The contribution of this article is to show that it gives favorable
results for sparse least-squares as well. Specifically, we prove that the
solution vector obtained by our algorithm is close to the solution vector
obtained via what is known as the &quot;SVD-truncated regularization approach&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7511</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7511</id><created>2013-12-29</created><authors><author><keyname>Shinde</keyname><forenames>Shraddha S.</forenames></author><author><keyname>Khedkar</keyname><forenames>Prof. Anagha P.</forenames></author></authors><title>A Novel Scheme for Generating Secure Face Templates Using BDA</title><categories>cs.CV cs.CR</categories><comments>07 pages,IJASCSE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In identity management system, frequently used biometric recognition system
needs awareness towards issue of protecting biometric template as far as more
reliable solution is apprehensive. In sight of this biometric template
protection algorithm should gratify the basic requirements viz. security,
discriminability and cancelability. As no single template protection method is
capable of satisfying these requirements, a novel scheme for face template
generation and protection is proposed. The novel scheme is proposed to provide
security and accuracy in new user enrolment and authentication process. This
novel scheme takes advantage of both the hybrid approach and the binary
discriminant analysis algorithm. This algorithm is designed on the basis of
random projection, binary discriminant analysis and fuzzy commitment scheme.
Publicly available benchmark face databases (FERET, FRGC, CMU-PIE) and other
datasets are used for evaluation. The proposed novel scheme enhances the
discriminability and recognition accuracy in terms of matching score of the
face images for each stage and provides high security against potential attacks
namely brute force and smart attacks. In this paper, we discuss results viz.
averages matching score, computation time and security for hybrid approach and
novel approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7513</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7513</id><created>2013-12-29</created><updated>2015-05-22</updated><authors><author><keyname>Cohen</keyname><forenames>Kobi</forenames></author><author><keyname>Leshem</keyname><forenames>Amir</forenames></author></authors><title>Distributed Game Theoretic Optimization and Management of Multichannel
  ALOHA Networks</title><categories>cs.NI cs.GT cs.IT math.IT</categories><comments>34 pages, 6 figures, accepted for publication in the IEEE/ACM
  Transactions on Networking, part of this work was presented at IEEE CAMSAP
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of distributed rate maximization in multi-channel ALOHA networks
is considered. First, we study the problem of constrained distributed rate
maximization, where user rates are subject to total transmission probability
constraints. We propose a best-response algorithm, where each user updates its
strategy to increase its rate according to the channel state information and
the current channel utilization. We prove the convergence of the algorithm to a
Nash equilibrium in both homogeneous and heterogeneous networks using the
theory of potential games. The performance of the best-response dynamic is
analyzed and compared to a simple transmission scheme, where users transmit
over the channel with the highest collision-free utility. Then, we consider the
case where users are not restricted by transmission probability constraints.
Distributed rate maximization under uncertainty is considered to achieve both
efficiency and fairness among users. We propose a distributed scheme where
users adjust their transmission probability to maximize their rates according
to the current network state, while maintaining the desired load on the
channels. We show that our approach plays an important role in achieving the
Nash bargaining solution among users. Sequential and parallel algorithms are
proposed to achieve the target solution in a distributed manner. The
efficiencies of the algorithms are demonstrated through both theoretical and
simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7520</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7520</id><created>2013-12-29</created><updated>2014-01-06</updated><authors><author><keyname>Imran</keyname><forenames>Muhammad</forenames></author></authors><title>An Effective End-User Development Approach Through Domain-Specific
  Mashups for Research Impact Evaluation</title><categories>cs.DL cs.HC</categories><comments>This PhD dissertation consists of 206 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last decade, there has been growing interest in the assessment of
the performance of researchers, research groups, universities and even
countries. The assessment of productivity is an instrument to select and
promote personnel, assign research grants and measure the results of research
projects. One particular assessment approach is bibliometrics i.e., the
quantitative analysis of scientific publications through citation and content
analysis. However, there is little consensus today on how research evaluation
should be performed, and it is commonly acknowledged that the quantitative
metrics available today are largely unsatisfactory. A number of different
scientific data sources available on the Web (e.g., DBLP, Google Scholar) that
are used for such analysis purposes. Taking data from these diverse sources,
performing the analysis and visualizing results in different ways is not a
trivial and straight forward task. Moreover, people involved in such evaluation
processes are not always IT experts and hence not capable to crawl data
sources, merge them and compute the needed evaluation procedures. The recent
emergence of mashup tools has refueled research on end-user development, i.e.,
on enabling end-users without programming skills to produce their own
applications. We believe that the heart of the problem is that it is
impractical to design tools that are generic enough to cover a wide range of
application domains, powerful enough to enable the specification of non-trivial
logic, and simple enough to be actually accessible to non-programmers. This
thesis presents a novel approach for an effective end-user development,
specifically for non-programmers. That is, we introduce a domain-specific
approach to mashups that &quot;speaks the language of users&quot;., i.e., that is aware
of the terminology, concepts, rules, and conventions (the domain) the user is
comfortable with.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7522</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7522</id><created>2013-12-29</created><authors><author><keyname>Bacso</keyname><forenames>Gabor</forenames></author><author><keyname>Borowiecki</keyname><forenames>Piotr</forenames></author><author><keyname>Hujter</keyname><forenames>Mihaly</forenames></author><author><keyname>Tuza</keyname><forenames>Zsolt</forenames></author></authors><title>Minimum order of graphs with given coloring parameters</title><categories>cs.DM math.CO</categories><comments>23 pages, 6 figures</comments><msc-class>05C15, 05C75, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A complete $k$-coloring of a graph $G=(V,E)$ is an assignment
$\varphi:V\to\{1,\ldots,k\}$ of colors to the vertices such that no two
vertices of the same color are adjacent, and the union of any two color classes
contains at least one edge. Three extensively investigated graph invariants
related to complete colorings are the minimum and maximum number of colors in a
complete coloring (chromatic number $\chi(G)$ and achromatic number $\psi(G)$,
respectively), and the Grundy number $\Gamma(G)$ defined as the largest $k$
admitting a complete coloring $\varphi$ with exactly $k$ colors such that every
vertex $v\in V$ of color $\varphi(v)$ has a neighbor of color $i$ for all $1\le
i&lt;\varphi(v)$. The inequality chain $\chi(G)\le \Gamma(G)\le \psi(G)$ obviously
holds for all graphs $G$. A triple $(f,g,h)$ of positive integers at least 2 is
called realizable if there exists a connected graph $G$ with $\chi(G)=f$,
$\Gamma(G)=g$, and $\psi(G)=h$. Chartrand et al. (A note on graphs with
prescribed complete coloring numbers, J. Combin. Math. Combin. Comput. LXXIII
(2010) 77-84) found the list of realizable triples. In this paper we determine
the minimum number of vertices in a connected graph with chromatic number $f$,
Grundy number $g$, and achromatic number $h$, for all realizable triples
$(f,g,h)$ of integers. Furthermore, for $f=g=3$ we describe the (two) extremal
graphs for each $h \geq 6$. For $h=4$ and $5$, there are more extremal graphs,
their description is contained as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7523</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7523</id><created>2013-12-29</created><authors><author><keyname>Bartocci</keyname><forenames>Ezio</forenames></author><author><keyname>Bortolussi</keyname><forenames>Luca</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Guido</forenames></author></authors><title>Learning Temporal Logical Properties Discriminating ECG models of
  Cardiac Arrhytmias</title><categories>cs.LO cs.CV q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach to learn the formulae characterising the emergent
behaviour of a dynamical system from system observations. At a high level, the
approach starts by devising a statistical dynamical model of the system which
optimally fits the observations. We then propose general optimisation
strategies for selecting high support formulae (under the learnt model of the
system) either within a discrete set of formulae of bounded complexity, or a
parametric family of formulae. We illustrate and apply the methodology on an
in-depth case study of characterising cardiac malfunction from
electro-cardiogram data, where our approach enables us to quantitatively
determine the diagnostic power of a formula in discriminating between different
cardiac conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7542</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7542</id><created>2013-12-29</created><authors><author><keyname>Ritter</keyname><forenames>Daniel</forenames></author></authors><title>Towards Connected Enterprises: The Business Network System</title><categories>cs.DB</categories><comments>10 pages, 15. GI-Fachtagung Datenbanksysteme f\&quot;ur Business,
  Technologie und Web (BTW): Data Management in the Cloud (DMC), Magdeburg,
  2013. arXiv admin note: text overlap with arXiv:1312.7436</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The discovery, representation and reconstruction of Business Networks (BN)
from Network Mining (NM) raw data is a difficult problem for enterprises. This
is due to huge amounts of complex business processes within and across
enterprise boundaries, heterogeneous technology stacks, and fragmented data. To
remain competitive, visibility into the enterprise and partner networks on
different, interrelated abstraction levels is desirable. We present a novel
data discovery, mining and network inference system, called Business Network
System (BNS), that reconstructs the BN--integration and business process
networks--from raw data, hidden in the enterprises' landscapes. BNS provides a
new, declarative foundation for gathering information, defining a network
model, inferring the network and check its conformance to the real-world
&quot;as-is&quot; network. The paper covers both the foundation and the key features of
BNS, including its underlying technologies, its overall system architecture,
and its most interesting capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7551</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7551</id><created>2013-12-29</created><updated>2015-11-22</updated><authors><author><keyname>Feldmann</keyname><forenames>Michel</forenames></author></authors><title>Information-theoretic interpretation of quantum formalism</title><categories>cs.IT math.IT</categories><comments>Revised version, 60 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an information-theoretic interpretation of quantum formalism based
on a Bayesian framework and free of any additional axiom or principle. Quantum
information is merely construed as a technique of statistical estimation for
analyzing a logical system subject to classical constraints, regardless of the
specific variables used. The problem is initially formulated in a standard
Boolean algebra involving a particular set of working variables. Statistical
estimation is to express the truth table in terms of likelihood probability
instead of the variables themselves. The constraints are thus converted into a
Bayesian prior. This method leads to solving a linear programming problem in a
real-valued probability space. The complete set of alternative Boolean
variables is introduced afterwards by transcribing the probability space into a
Hilbert space, thanks to Gleason's theorem. This allows to completely recover
standard quantum information and provides an information-theoretic rationale to
its technical rules. The model offers a natural answer to the major puzzles
that base quantum mechanics: Why is the theory linear? Why is the theory
probabilistic? Where does the Hilbert space come from? Also, most of the
paradoxes, such as entanglement, contextuality, nonsignaling correlation,
measurement problem, etc., find a quite trivial explanation, while the concept
of information conveyed by a wave vector is clarified. We conclude that quantum
information, although dramatically expanding the scope of classical
information, is not different from the information itself and is therefore a
universal tool of reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7555</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7555</id><created>2013-12-29</created><updated>2014-09-27</updated><authors><author><keyname>Wagner</keyname><forenames>Zsolt Adam</forenames></author></authors><title>Cops and Robbers on diameter two graphs</title><categories>math.CO cs.DM</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short paper we study the game of Cops and Robbers, played on the
vertices of some fixed graph $G$ of order $n$. The minimum number of cops
required to capture a robber is called the cop number of $G$. We show that the
cop number of graphs of diameter 2 is at most $\sqrt{2n}$, improving a recent
result of Lu and Peng by a constant factor. We conjecture that this bound is
still not optimal, and obtain some partial results towards the optimal bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7557</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7557</id><created>2013-12-29</created><authors><author><keyname>Fazli</keyname><forenames>Saeid</forenames></author><author><keyname>Samadi</keyname><forenames>Sevin</forenames></author></authors><title>A Novel Retinal Vessel Segmentation Based On Histogram Transformation
  Using 2-D Morlet Wavelet and Supervised Classification</title><categories>cs.CV</categories><comments>International Journal of Advanced Studies in Computer Science and
  Engineering (IJASCSE)December 2013. arXiv admin note: text overlap with
  arXiv:cs/0510001 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The appearance and structure of blood vessels in retinal images have an
important role in diagnosis of diseases. This paper proposes a method for
automatic retinal vessel segmentation. In this work, a novel preprocessing
based on local histogram equalization is used to enhance the original image
then pixels are classified as vessel and non-vessel using a classifier. For
this classification, special feature vectors are organized based on responses
to Morlet wavelet. Morlet wavelet is a continues transform which has the
ability to filter existing noises after preprocessing. Bayesian classifier is
used and Gaussian mixture model (GMM) is its likelihood function. The
probability distributions are approximated according to training set of manual
that has been segmented by a specialist. After this, morphological transforms
are used in different directions to make the existing discontinuities uniform
on the DRIVE database, it achieves the accuracy about 0.9571 which shows that
it is an accurate method among the available ones for retinal vessel
segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7560</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7560</id><created>2013-12-29</created><authors><author><keyname>Dhawan</keyname><forenames>Amiraj</forenames></author><author><keyname>Honrao</keyname><forenames>Vipul</forenames></author></authors><title>Implementation of Hand Detection based Techniques for Human Computer
  Interaction</title><categories>cs.CV cs.HC</categories><journal-ref>International Journal of Computer Applications, Volume 72 No 17,
  June 2013</journal-ref><doi>10.5120/12632-9151 10.5120/12632-9151 10.5120/12632-9151</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computer industry is developing at a fast pace. With this development
almost all of the fields under computers have advanced in the past couple of
decades. But the same technology is being used for human computer interaction
that was used in 1970s. Even today the same type of keyboard and mouse is used
for interacting with computer systems. With the recent boom in the mobile
segment touchscreens have become popular for interaction with cell phones. But
these touchscreens are rarely used on traditional systems. This paper tries to
introduce methods for human computer interaction using the users hand which can
be used both on traditional computer platforms as well as cell phones. The
methods explain how the users detected hand can be used as input for
applications and also explain applications that can take advantage of this type
of interaction mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7563</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7563</id><created>2013-12-29</created><authors><author><keyname>Levit</keyname><forenames>Vadim E.</forenames></author><author><keyname>Tankus</keyname><forenames>David</forenames></author></authors><title>Weighted Well-Covered Claw-Free Graphs</title><categories>cs.DM math.CO</categories><comments>14 pages, 1 figure</comments><msc-class>05C69 (Primary) 05C85 (Secondary)</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph G is well-covered if all its maximal independent sets are of the same
cardinality. Assume that a weight function w is defined on its vertices. Then G
is w-well-covered if all maximal independent sets are of the same weight. For
every graph G, the set of weight functions w such that G is w-well-covered is a
vector space. Given an input claw-free graph G, we present an O(n^6)algortihm,
whose input is a claw-free graph G, and output is the vector space of weight
functions w, for which G is w-well-covered. A graph G is equimatchable if all
its maximal matchings are of the same cardinality. Assume that a weight
function w is defined on the edges of G. Then G is w-equimatchable if all its
maximal matchings are of the same weight. For every graph G, the set of weight
functions w such that G is w-equimatchable is a vector space. We present an
O(m*n^4 + n^5*log(n)) algorithm which receives an input graph G, and outputs
the vector space of weight functions w such that G is w-equimatchable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7567</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7567</id><created>2013-12-29</created><authors><author><keyname>Genovese</keyname><forenames>Christopher</forenames></author><author><keyname>Perone-Pacifico</keyname><forenames>Marco</forenames></author><author><keyname>Verdinelli</keyname><forenames>Isabella</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Nonparametric Inference For Density Modes</title><categories>stat.ME cs.LG</categories><msc-class>62G07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive nonparametric confidence intervals for the eigenvalues of the
Hessian at modes of a density estimate. This provides information about the
strength and shape of modes and can also be used as a significance test. We use
a data-splitting approach in which potential modes are identified using the
first half of the data and inference is done with the second half of the data.
To get valid confidence sets for the eigenvalues, we use a bootstrap based on
an elementary-symmetric-polynomial (ESP) transformation. This leads to valid
bootstrap confidence sets regardless of any multiplicities in the eigenvalues.
We also suggest a new method for bandwidth selection, namely, choosing the
bandwidth to maximize the number of significant modes. We show by example that
this method works well. Even when the true distribution is singular, and hence
does not have a density, (in which case cross validation chooses a zero
bandwidth), our method chooses a reasonable bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7570</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7570</id><created>2013-12-29</created><authors><author><keyname>Mathe</keyname><forenames>Stefan</forenames></author><author><keyname>Sminchisescu</keyname><forenames>Cristian</forenames></author></authors><title>Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for
  Visual Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systems based on bag-of-words models from image features collected at maxima
of sparse interest point operators have been used successfully for both
computer visual object and action recognition tasks. While the sparse,
interest-point based approach to recognition is not inconsistent with visual
processing in biological systems that operate in `saccade and fixate' regimes,
the methodology and emphasis in the human and the computer vision communities
remains sharply distinct. Here, we make three contributions aiming to bridge
this gap. First, we complement existing state-of-the art large scale dynamic
computer vision annotated datasets like Hollywood-2 and UCF Sports with human
eye movements collected under the ecological constraints of the visual action
recognition task. To our knowledge these are the first large human eye tracking
datasets to be collected and made publicly available for video,
vision.imar.ro/eyetracking (497,107 frames, each viewed by 16 subjects), unique
in terms of their (a) large scale and computer vision relevance, (b) dynamic,
video stimuli, (c) task control, as opposed to free-viewing. Second, we
introduce novel sequential consistency and alignment measures, which underline
the remarkable stability of patterns of visual search among subjects. Third, we
leverage the significant amount of collected data in order to pursue studies
and build automatic, end-to-end trainable computer vision systems based on
human eye movements. Our studies not only shed light on the differences between
computer vision spatio-temporal interest point image sampling strategies and
the human fixations, as well as their impact for visual recognition
performance, but also demonstrate that human fixations can be accurately
predicted, and when used in an end-to-end automatic system, leveraging some of
the advanced computer vision practice, can lead to state of the art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7572</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7572</id><created>2013-12-29</created><authors><author><keyname>Kchir</keyname><forenames>Selma</forenames></author><author><keyname>Ziadi</keyname><forenames>Tewfik</forenames></author><author><keyname>Ziane</keyname><forenames>Mikal</forenames></author><author><keyname>Stinckwich</keyname><forenames>Serge</forenames></author></authors><title>A Top-Down Approach to Managing Variability in Robotics Algorithms</title><categories>cs.RO</categories><comments>6 pages, 5 figures, Presented at DSLRob 2013 (arXiv:cs/1312.5952)</comments><report-no>DSLRob/2013/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the defining features of the field of robotics is its breadth and
heterogeneity. Unfortunately, despite the availability of several robotics
middleware services, robotics software still fails to smoothly handle at least
two kinds of variability: algorithmic variability and lower-level variability.
The consequence is that implementations of algorithms are hard to understand
and impacted by changes to lower-level details such as the choice or
configuration of sensors or actuators. Moreover, when several algorithms or
algorithmic variants are available it is difficult to compare and combine them.
  In order to alleviate these problems we propose a top-down approach to
express and implement robotics algorithms and families of algorithms so that
they are both less dependent on lower-level details and easier to understand
and combine. This approach goes top-down from the algorithms and shields them
from lower-level details by introducing very high level abstractions atop the
intermediate abstractions of robotics middleware. This approach is illustrated
on 7 variants of the Bug family that were implemented using both laser and
infra-red sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7573</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7573</id><created>2013-12-29</created><authors><author><keyname>Fazli</keyname><forenames>Saeid</forenames></author><author><keyname>Nadirkhanlou</keyname><forenames>Parisa</forenames></author></authors><title>A Novel Method for Automatic Segmentation of Brain Tumors in MRI Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The brain tumor segmentation on MRI images is a very difficult and important
task which is used in surgical and medical planning and assessments. If experts
do the segmentation manually with their own medical knowledge, it will be
time-consuming. Therefore, researchers propose methods and systems which can do
the segmentation automatically and without any interference. In this article,
an unsupervised automatic method for brain tumor segmentation on MRI images is
presented. In this method, at first in the pre-processing level, the extra
parts which are outside the skull and don't have any helpful information are
removed and then anisotropic diffusion filter with 8-connected neighborhood is
applied to the MRI images to remove noise. By applying the fast bounding
box(FBB) algorithm, the tumor area is displayed on the MRI image with a
bounding box and the central part is selected as sample points for training of
a One Class SVM classifier. A database is also provided by the Zanjan MRI
Center. The MRI images are related to 10 patients who have brain tumor. 100
T2-weighted MRI images are used in this study. Experimental results show the
high precision and dependability of the proposed algorithm. The results are
also highly helpful for specialists and radiologists to easily estimate the
size and position of a tumor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7580</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7580</id><created>2013-12-29</created><updated>2015-04-20</updated><authors><author><keyname>Chen</keyname><forenames>Jianshu</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>On the Learning Behavior of Adaptive Networks - Part II: Performance
  Analysis</title><categories>cs.MA cs.SY math.OC</categories><comments>to appear in IEEE Transactions on Information Theory, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Part I of this work examined the mean-square stability and convergence of the
learning process of distributed strategies over graphs. The results identified
conditions on the network topology, utilities, and data in order to ensure
stability; the results also identified three distinct stages in the learning
behavior of multi-agent networks related to transient phases I and II and the
steady-state phase. This Part II examines the steady-state phase of distributed
learning by networked agents. Apart from characterizing the performance of the
individual agents, it is shown that the network induces a useful equalization
effect across all agents. In this way, the performance of noisier agents is
enhanced to the same level as the performance of agents with less noisy data.
It is further shown that in the small step-size regime, each agent in the
network is able to achieve the same performance level as that of a centralized
strategy corresponding to a fully connected network. The results in this part
reveal explicitly which aspects of the network topology and operation influence
performance and provide important insights into the design of effective
mechanisms for the processing and diffusion of information over networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7581</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7581</id><created>2013-12-29</created><updated>2015-04-20</updated><authors><author><keyname>Chen</keyname><forenames>Jianshu</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>On the Learning Behavior of Adaptive Networks - Part I: Transient
  Analysis</title><categories>cs.MA cs.SY math.OC</categories><comments>to appear in IEEE Transactions on Information Theory, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work carries out a detailed transient analysis of the learning behavior
of multi-agent networks, and reveals interesting results about the learning
abilities of distributed strategies. Among other results, the analysis reveals
how combination policies influence the learning process of networked agents,
and how these policies can steer the convergence point towards any of many
possible Pareto optimal solutions. The results also establish that the learning
process of an adaptive network undergoes three (rather than two) well-defined
stages of evolution with distinctive convergence rates during the first two
stages, while attaining a finite mean-square-error (MSE) level in the last
stage. The analysis reveals what aspects of the network topology influence
performance directly and suggests design procedures that can optimize
performance by adjusting the relevant topology parameters. Interestingly, it is
further shown that, in the adaptation regime, each agent in a sparsely
connected network is able to achieve the same performance level as that of a
centralized stochastic-gradient strategy even for left-stochastic combination
strategies. These results lead to a deeper understanding and useful insights on
the convergence behavior of coupled distributed learners. The results also lead
to effective design mechanisms to help diffuse information more thoroughly over
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7595</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7595</id><created>2013-12-29</created><authors><author><keyname>P&#xf3;sfai</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>H&#xf6;vel</keyname><forenames>Philipp</forenames></author></authors><title>Phase transition in the controllability of temporal networks</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The control of complex systems is an ongoing challenge of complexity
research. Recent advances using concepts of structural control deduce a wide
range of control related properties from the network representation of complex
systems. Here, we examine the controllability of complex systems for which the
timescale of the dynamics we control and the timescale of changes in the
network are comparable. We provide both analytical and computational tools to
study controllability based on temporal network characteristics. We apply these
results to investigate the controllable subnetwork using a single input,
present analytical results for a generic class of temporal network models, and
preform measurements using data collected from a real system. Depending upon
the density of the interactions compared to the timescale of the dynamics, we
witness a phase transition describing the sudden emergence of a giant
controllable subspace spanning a finite fraction of the network. We also study
the role of temporal patterns and network topology in real data making use of
various randomization procedures, finding that the overall activity and the
degree distribution of the underlying network are the main features influencing
controllability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7602</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7602</id><created>2013-12-29</created><updated>2015-07-08</updated><authors><author><keyname>Huynh</keyname><forenames>Vu Anh</forenames></author><author><keyname>Kogan</keyname><forenames>Leonid</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author></authors><title>A Martingale Approach and Time-Consistent Sampling-based Algorithms for
  Risk Management in Stochastic Optimal Control</title><categories>cs.SY cs.RO math.DS math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a class of stochastic optimal control problems
with risk constraints that are expressed as bounded probabilities of failure
for particular initial states. We present here a martingale approach that
diffuses a risk constraint into a martingale to construct time-consistent
control policies. The martingale stands for the level of risk tolerance over
time. By augmenting the system dynamics with the controlled martingale, the
original risk-constrained problem is transformed into a stochastic target
problem. We extend the incremental Markov Decision Process (iMDP) algorithm to
approximate arbitrarily well an optimal feedback policy of the original problem
by sampling in the augmented state space and computing proper boundary
conditions for the reformulated problem. We show that the algorithm is both
probabilistically sound and asymptotically optimal. The performance of the
proposed algorithm is demonstrated on motion planning and control problems
subject to bounded probability of collision in uncertain cluttered
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7603</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7603</id><created>2013-12-29</created><updated>2014-04-28</updated><authors><author><keyname>Bundala</keyname><forenames>Daniel</forenames></author><author><keyname>Ouaknine</keyname><forenames>Jo&#xeb;l</forenames></author></authors><title>On the Complexity of Temporal-Logic Path Checking</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a formula in a temporal logic such as LTL or MTL, a fundamental problem
is the complexity of evaluating the formula on a given finite word. For LTL,
the complexity of this task was recently shown to be in NC. In this paper, we
present an NC algorithm for MTL, a quantitative (or metric) extension of LTL,
and give an NCC algorithm for UTL, the unary fragment of LTL. At the time of
writing, MTL is the most expressive logic with an NC path-checking algorithm,
and UTL is the most expressive fragment of LTL with a more efficient
path-checking algorithm than for full LTL (subject to standard
complexity-theoretic assumptions). We then establish a connection between LTL
path checking and planar circuits, which we exploit to show that any further
progress in determining the precise complexity of LTL path checking would
immediately entail more efficient evaluation algorithms than are known for a
certain class of planar circuits. The connection further implies that the
complexity of LTL path checking depends on the Boolean connectives allowed:
adding Boolean exclusive or yields a temporal logic with P-complete
path-checking problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7605</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7605</id><created>2013-12-29</created><authors><author><keyname>Martin</keyname><forenames>Barnaby</forenames></author><author><keyname>Stacho</keyname><forenames>Juraj</forenames></author></authors><title>Constraint Satisfaction with Counting Quantifiers 2</title><categories>cs.LO cs.CC cs.DM math.CO</categories><msc-class>03C80, 03C13, 05C15, 05C57</msc-class><acm-class>F.4.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study constraint satisfaction problems (CSPs) in the presence of counting
quantifiers $\exists^{\geq j}$, asserting the existence of $j$ distinct
witnesses for the variable in question. As a continuation of our previous (CSR
2012) paper, we focus on the complexity of undirected graph templates. As our
main contribution, we settle the two principal open questions proposed in (CSR
2012). Firstly, we complete the classification of clique templates by proving a
full trichotomy for all possible combinations of counting quantifiers and
clique sizes, placing each case either in P, NP-complete or Pspace-complete.
This involves resolution of the cases in which we have the single quantifier
$\exists^{\geq j}$ on the clique $K_{2j}$. Secondly, we confirm a conjecture
from (CSR 2012), which proposes a full dichotomy for $\exists$ and
$\exists^{\geq 2}$ on all finite undirected graphs. The main thrust of this
second result is the solution of the complexity for the infinite path which we
prove is a polynomial-time solvable problem. By adapting the algorithm for the
infinite path we are then able to solve the problem for finite paths, and then
trees and forests. Thus as a corollary to this work, combining with the other
cases from (CSR 2012), we obtain a full dichotomy for $\exists$ and
$\exists^{\geq 2}$ quantifiers on finite graphs, each such problem being either
in P or NP-hard. Finally, we persevere with the work of (CSR 2012) in exploring
cases in which there is dichotomy between P and Pspace-complete, in contrast
with situations in which the intermediate NP-completeness may appear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7606</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7606</id><created>2013-12-29</created><updated>2014-11-05</updated><authors><author><keyname>Macua</keyname><forenames>Sergio Valcarcel</forenames></author><author><keyname>Chen</keyname><forenames>Jianshu</forenames></author><author><keyname>Zazo</keyname><forenames>Santiago</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Distributed Policy Evaluation Under Multiple Behavior Strategies</title><categories>cs.MA cs.AI cs.DC cs.LG</categories><comments>36 pages, 4 figures, accepted for publication on IEEE Transactions on
  Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply diffusion strategies to develop a fully-distributed cooperative
reinforcement learning algorithm in which agents in a network communicate only
with their immediate neighbors to improve predictions about their environment.
The algorithm can also be applied to off-policy learning, meaning that the
agents can predict the response to a behavior different from the actual
policies they are following. The proposed distributed strategy is efficient,
with linear complexity in both computation time and memory footprint. We
provide a mean-square-error performance analysis and establish convergence
under constant step-size updates, which endow the network with continuous
learning capabilities. The results show a clear gain from cooperation: when the
individual agents can estimate the solution, cooperation increases stability
and reduces bias and variance of the prediction error; but, more importantly,
the network is able to approach the optimal solution even when none of the
individual agents can (e.g., when the individual behavior policies restrict
each agent to sample a small portion of the state space).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7615</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7615</id><created>2013-12-29</created><authors><author><keyname>Majster-Cederbaum</keyname><forenames>Mila</forenames><affiliation>University Mannheim</affiliation></author><author><keyname>Semmelrock</keyname><forenames>Nils</forenames><affiliation>University Mannheim</affiliation></author></authors><title>Reachability in Cooperating Systems with Architectural Constraints is
  PSPACE-Complete</title><categories>cs.CC</categories><comments>In Proceedings GRAPHITE 2013, arXiv:1312.7062</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 138, 2013, pp. 1-11</journal-ref><doi>10.4204/EPTCS.138.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reachability problem in cooperating systems is known to be
PSPACE-complete. We show here that this problem remains PSPACE-complete when we
restrict the communication structure between the subsystems in various ways.
For this purpose we introduce two basic and incomparable subclasses of
cooperating systems that occur often in practice and provide respective
reductions. The subclasses we consider consist of cooperating systems the
communication structure of which forms a line respectively a star.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7626</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7626</id><created>2013-12-29</created><authors><author><keyname>Abu-Khzam</keyname><forenames>Faisal N.</forenames></author><author><keyname>Daudjee</keyname><forenames>Khuzaima</forenames></author><author><keyname>Mouawad</keyname><forenames>Amer E.</forenames></author><author><keyname>Nishimura</keyname><forenames>Naomi</forenames></author></authors><title>An Easy-to-use Scalable Framework for Parallel Recursive Backtracking</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supercomputers are equipped with an increasingly large number of cores to use
computational power as a way of solving problems that are otherwise
intractable. Unfortunately, getting serial algorithms to run in parallel to
take advantage of these computational resources remains a challenge for several
application domains. Many parallel algorithms can scale to only hundreds of
cores. The limiting factors of such algorithms are usually communication
overhead and poor load balancing. Solving NP-hard graph problems to optimality
using exact algorithms is an example of an area in which there has so far been
limited success in obtaining large scale parallelism. Many of these algorithms
use recursive backtracking as their core solution paradigm. In this paper, we
propose a lightweight, easy-to-use, scalable framework for transforming almost
any recursive backtracking algorithm into a parallel one. Our framework incurs
minimal communication overhead and guarantees a load-balancing strategy that is
implicit, i.e., does not require any problem-specific knowledge. The key idea
behind this framework is the use of an indexed search tree approach that is
oblivious to the problem being solved. We test our framework with parallel
implementations of algorithms for the well-known Vertex Cover and Dominating
Set problems. On sufficiently hard instances, experimental results show linear
speedups for thousands of cores, reducing running times from days to just a few
minutes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7630</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7630</id><created>2013-12-30</created><authors><author><keyname>Krishnamurthy</keyname><forenames>Vikram</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Interactive Sensing in Social Networks</title><categories>cs.SI math.OC physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents models and algorithms for interactive sensing in social
networks where individuals act as sensors and the information exchange between
individuals is exploited to optimize sensing. Social learning is used to model
the interaction between individuals that aim to estimate an underlying state of
nature. In this context the following questions are addressed: How can
self-interested agents that interact via social learning achieve a tradeoff
between individual privacy and reputation of the social group? How can
protocols be designed to prevent data incest in online reputation blogs where
individuals make recommendations? How can sensing by individuals that interact
with each other be used by a global decision maker to detect changes in the
underlying state of nature? When individual agents possess limited sensing,
computation and communication capabilities, can a network of agents achieve
sophisticated global behavior? Social and game theoretic learning are natural
settings for addressing these questions. This article presents an overview,
insights and discussion of social learning models in the context of data incest
propagation, change detection and coordination of decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7637</identifier>
 <datestamp>2014-01-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7637</id><created>2013-12-30</created><authors><author><keyname>Premjith</keyname><forenames>B.</forenames></author><author><keyname>Kumar</keyname><forenames>S. Sachin</forenames></author><author><keyname>Manikkoth</keyname><forenames>Akhil</forenames></author><author><keyname>Bijeesh</keyname><forenames>T V</forenames></author><author><keyname>Soman</keyname><forenames>K P</forenames></author></authors><title>Insight into Primal Augmented Lagrangian Multilplier Method</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a simplified form of Primal Augmented Lagrange Multiplier
algorithm. We intend to fill the gap in the steps involved in the mathematical
derivations of the algorithm so that an insight into the algorithm is made. The
experiment is focused to show the reconstruction done using this algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7642</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7642</id><created>2013-12-30</created><authors><author><keyname>Drescher</keyname><forenames>Lukas</forenames></author><author><keyname>Fawzi</keyname><forenames>Omar</forenames></author></authors><title>On simultaneous min-entropy smoothing</title><categories>quant-ph cs.IT math.IT</categories><comments>5 pages</comments><journal-ref>Proceedings of ISIT 2013, pages 161 - 165</journal-ref><doi>10.1109/ISIT.2013.6620208</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of network information theory, one often needs a multiparty
probability distribution to be typical in several ways simultaneously. When
considering quantum states instead of classical ones, it is in general
difficult to prove the existence of a state that is jointly typical. Such a
difficulty was recently emphasized and conjectures on the existence of such
states were formulated. In this paper, we consider a one-shot multiparty
typicality conjecture. The question can then be stated easily: is it possible
to smooth the largest eigenvalues of all the marginals of a multipartite state
{\rho} simultaneously while staying close to {\rho}? We prove the answer is yes
whenever the marginals of the state commute. In the general quantum case, we
prove that simultaneous smoothing is possible if the number of parties is two
or more generally if the marginals to optimize satisfy some non-overlap
property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7645</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7645</id><created>2013-12-30</created><authors><author><keyname>Fehnker</keyname><forenames>Ansgar</forenames></author><author><keyname>van Glabbeek</keyname><forenames>Rob</forenames></author><author><keyname>H&#xf6;fner</keyname><forenames>Peter</forenames></author><author><keyname>McIver</keyname><forenames>Annabelle</forenames></author><author><keyname>Portmann</keyname><forenames>Marius</forenames></author><author><keyname>Tan</keyname><forenames>Wee Lum</forenames></author></authors><title>A Process Algebra for Wireless Mesh Networks used for Modelling,
  Verifying and Analysing AODV</title><categories>cs.NI cs.LO</categories><report-no>Technical Report 5513, NICTA, 2013</report-no><acm-class>C.2.2; F.3.1; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose AWN (Algebra for Wireless Networks), a process algebra tailored to
the modelling of Mobile Ad hoc Network (MANET) and Wireless Mesh Network (WMN)
protocols. It combines novel treatments of local broadcast, conditional unicast
and data structures.
  In this framework we present a rigorous analysis of the Ad hoc On-Demand
Distance Vector (AODV) protocol, a popular routing protocol designed for MANETs
and WMNs, and one of the four protocols currently standardised by the IETF
MANET working group.
  We give a complete and unambiguous specification of this protocol, thereby
formalising the RFC of AODV, the de facto standard specification, given in
English prose. In doing so, we had to make non-evident assumptions to resolve
ambiguities occurring in that specification. Our formalisation models the exact
details of the core functionality of AODV, such as route maintenance and error
handling, and only omits timing aspects.
  The process algebra allows us to formalise and (dis)prove crucial properties
of mesh network routing protocols such as loop freedom and packet delivery. We
are the first to provide a detailed proof of loop freedom of AODV. In contrast
to evaluations using simulation or model checking, our proof is generic and
holds for any possible network scenario in terms of network topology, node
mobility, etc. Due to ambiguities and contradictions the RFC specification
allows several interpretations; we show for more than 5000 of them whether they
are loop free or not, thereby demonstrating how the reasoning and proofs can
relatively easily be adapted to protocol variants.
  Using our formal and unambiguous specification, we find shortcomings of AODV
that affect performance, e.g. the establishment of non-optimal routes, and some
routes not being found at all. We formalise improvements in the same process
algebra; carrying over the proofs is again easy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7646</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7646</id><created>2013-12-30</created><authors><author><keyname>Brown</keyname><forenames>Winton</forenames></author><author><keyname>Fawzi</keyname><forenames>Omar</forenames></author></authors><title>Short random circuits define good quantum error correcting codes</title><categories>quant-ph cs.IT math.IT</categories><comments>5 pages</comments><acm-class>E.4; F.1.1</acm-class><journal-ref>Proceedings of ISIT 2013, pages 346 - 350</journal-ref><doi>10.1109/ISIT.2013.6620245</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the encoding complexity for quantum error correcting codes with
large rate and distance. We prove that random Clifford circuits with $O(n
\log^2 n)$ gates can be used to encode $k$ qubits in $n$ qubits with a distance
$d$ provided $\frac{k}{n} &lt; 1 - \frac{d}{n} \log_2 3 - h(\frac{d}{n})$. In
addition, we prove that such circuits typically have a depth of $O( \log^3 n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7650</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7650</id><created>2013-12-30</created><updated>2014-01-02</updated><authors><author><keyname>Liu</keyname><forenames>Xiaodong</forenames></author><author><keyname>Li</keyname><forenames>Yuan</forenames></author><author><keyname>Kan</keyname><forenames>Haibin</forenames></author></authors><title>On the Minimum Decoding Delay of Balanced Complex Orthogonal Design</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Information Theory, Volume:61, Issue: 1
  (2014) pg 696-699</journal-ref><doi>10.1109/TIT.2014.2368554</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex orthogonal design (COD) with parameter $[p, n, k]$ is a combinatorial
design used in space-time block codes (STBCs). For STBC, $n$ is the number of
antennas, $k/p$ is the rate, and $p$ is the decoding delay. A class of rate
$1/2$ COD called balanced complex orthogonal design (BCOD) has been proposed by
Adams et al., and they constructed BCODs with rate $k/p = 1/2$ and decoding
delay $p = 2^m$ for $n=2m$. Furthermore, they prove that the constructions have
optimal decoding delay when $m$ is congruent to $1$, $2$, or $3$ module $4$.
They conjecture that for the case $m \equiv 0 \pmod 4$, $2^m$ is also a lower
bound of $p$. In this paper, we prove this conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7651</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7651</id><created>2013-12-30</created><updated>2015-05-14</updated><authors><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author><author><keyname>Ho</keyname><forenames>Qirong</forenames></author><author><keyname>Dai</keyname><forenames>Wei</forenames></author><author><keyname>Kim</keyname><forenames>Jin Kyu</forenames></author><author><keyname>Wei</keyname><forenames>Jinliang</forenames></author><author><keyname>Lee</keyname><forenames>Seunghak</forenames></author><author><keyname>Zheng</keyname><forenames>Xun</forenames></author><author><keyname>Xie</keyname><forenames>Pengtao</forenames></author><author><keyname>Kumar</keyname><forenames>Abhimanu</forenames></author><author><keyname>Yu</keyname><forenames>Yaoliang</forenames></author></authors><title>Petuum: A New Platform for Distributed Machine Learning on Big Data</title><categories>stat.ML cs.LG cs.SY</categories><comments>15 pages, 10 figures, final version in KDD 2015 under the same title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What is a systematic way to efficiently apply a wide spectrum of advanced ML
programs to industrial scale problems, using Big Models (up to 100s of billions
of parameters) on Big Data (up to terabytes or petabytes)? Modern
parallelization strategies employ fine-grained operations and scheduling beyond
the classic bulk-synchronous processing paradigm popularized by MapReduce, or
even specialized graph-based execution that relies on graph representations of
ML programs. The variety of approaches tends to pull systems and algorithms
design in different directions, and it remains difficult to find a universal
platform applicable to a wide range of ML programs at scale. We propose a
general-purpose framework that systematically addresses data- and
model-parallel challenges in large-scale ML, by observing that many ML programs
are fundamentally optimization-centric and admit error-tolerant,
iterative-convergent algorithmic solutions. This presents unique opportunities
for an integrative system design, such as bounded-error network synchronization
and dynamic scheduling based on ML program structure. We demonstrate the
efficacy of these system designs versus well-known implementations of modern ML
algorithms, allowing ML programs to run in much less time and at considerably
larger model sizes, even on modestly-sized compute clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7658</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7658</id><created>2013-12-30</created><authors><author><keyname>Bernstein</keyname><forenames>Andrey</forenames></author><author><keyname>Shimkin</keyname><forenames>Nahum</forenames></author></authors><title>Response-Based Approachability and its Application to Generalized
  No-Regret Algorithms</title><categories>cs.LG cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approachability theory, introduced by Blackwell (1956), provides fundamental
results on repeated games with vector-valued payoffs, and has been usefully
applied since in the theory of learning in games and to learning algorithms in
the online adversarial setup. Given a repeated game with vector payoffs, a
target set $S$ is approachable by a certain player (the agent) if he can ensure
that the average payoff vector converges to that set no matter what his
adversary opponent does. Blackwell provided two equivalent sets of conditions
for a convex set to be approachable. The first (primary) condition is a
geometric separation condition, while the second (dual) condition requires that
the set be {\em non-excludable}, namely that for every mixed action of the
opponent there exists a mixed action of the agent (a {\em response}) such that
the resulting payoff vector belongs to $S$. Existing approachability algorithms
rely on the primal condition and essentially require to compute at each stage a
projection direction from a given point to $S$. In this paper, we introduce an
approachability algorithm that relies on Blackwell's {\em dual} condition.
Thus, rather than projection, the algorithm relies on computation of the
response to a certain action of the opponent at each stage. The utility of the
proposed algorithm is demonstrated by applying it to certain generalizations of
the classical regret minimization problem, which include regret minimization
with side constraints and regret minimization for global cost functions. In
these problems, computation of the required projections is generally complex
but a response is readily obtainable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7660</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7660</id><created>2013-12-30</created><authors><author><keyname>Akhtar</keyname><forenames>Md. Amir Khusru</forenames></author><author><keyname>Sahoo</keyname><forenames>G.</forenames></author></authors><title>Humanistic approach in mobile ad hoc network: HAMANET</title><categories>cs.NI cs.CY</categories><comments>16 pages, 6 figures, 8 tables. arXiv admin note: substantial text
  overlap with arXiv:1311.3172</comments><journal-ref>Third International Conference on Computer Science &amp; Information
  Technology (CCSIT 2013), Bangalore, India. CS &amp; IT - CSCP, Vol. 3, No. 6, pp.
  1-12, Feb 2013</journal-ref><doi>10.5121/csit.2013.3601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human society is a complex and most organized networks, in which many
communities have different cultural livelihood. The creation/formation of one
or more communities within a society and the way of associations can be mapped
to MANET. By involving human characteristics and behavior, surely it would pave
a new way, for further development. In this paper we have presented a new
approach called &quot;HAMANET&quot; which is not only robust and secure but it certainly
meets the challenges of MANET (such as name resolution, address allocation and
authentication). Our object oriented design defines a service in terms of Arts,
Culture, and Machine. The 'Art' is the smallest unit of work (defined as an
interface), the 'Culture' is the integration/assembling of one or more Arts
(defined as a class) and finally the 'Machine' which is an instance of a
Culture that defines a service. The grouping of the communicable Machines of
the same Culture forms a 'Community'. We have used the term 'Society' for MANET
consisting of one or more communities and modeled using humanistic approach. We
have compared our design with GloMoSim and proposed the implementation of file
transfer service using the said approach. Our approach gives better results in
terms of implementation of the basic services, security, reliability,
throughput, extensibility, scalability etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7670</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7670</id><created>2013-12-30</created><authors><author><keyname>Kelemen</keyname><forenames>Z&#xe1;dor D&#xe1;niel</forenames></author><author><keyname>Balla</keyname><forenames>Katalin</forenames></author><author><keyname>Trienekens</keyname><forenames>Jos</forenames></author><author><keyname>Kusters</keyname><forenames>Rob</forenames></author></authors><title>Towards supporting simultaneous use of process-based quality approaches</title><categories>cs.SE</categories><journal-ref>In Proceedings of 9th International Carpathian Control Conference
  pp291-295 Sinaia Romania 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe the first steps of a PhD programme, having the goal
to develop a common meta-model for different software quality approaches and
methods. We focus on presenting the structure of quality approaches emphasizing
the similarities amongst them. Understanding the structure of quality
approaches helps supporting organizations in using multiple quality approaches
and methods in the same time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7685</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7685</id><created>2013-12-30</created><authors><author><keyname>Naparstek</keyname><forenames>Oshri</forenames><affiliation>Student Member, IEEE</affiliation></author><author><keyname>Leshem</keyname><forenames>Amir</forenames><affiliation>Senior Member, IEEE</affiliation></author></authors><title>Fully distributed optimal channel assignment for open spectrum access</title><categories>cs.DC cs.IT cs.NI math.IT math.OC</categories><doi>10.1109/TSP.2013.2285512</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of fully distributed assignment of users
to sub-bands such that the sum-rate of the system is maximized. We introduce a
modified auction algorithm that can be applied in a fully distributed way using
an opportunistic CSMA assignment scheme and is $\epsilon$ optimal. We analyze
the expected time complexity of the algorithm and suggest a variant to the
algorithm that has lower expected complexity. We then show that in the case of
i.i.d Rayleigh channels a simple greedy scheme is asymptotically optimal as
$\SNR$ increases or as the number of users is increased to infinity. We
conclude by providing simulated results of the suggested algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7688</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7688</id><created>2013-12-30</created><authors><author><keyname>Kovalchuk</keyname><forenames>Sergey V.</forenames></author><author><keyname>Smirnov</keyname><forenames>Pavel A.</forenames></author><author><keyname>Knyazkov</keyname><forenames>Konstantin V.</forenames></author><author><keyname>Zagarskikh</keyname><forenames>Alexander S.</forenames></author><author><keyname>Boukhanovsky</keyname><forenames>Alexander V.</forenames></author></authors><title>Knowledge-based Expressive Technologies within Cloud Computing
  Environments</title><categories>cs.SE cs.DC</categories><comments>Proceedings of the 8th International Conference on Intelligent
  Systems and Knowledge Engineering (ISKE2013). 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Presented paper describes the development of comprehensive approach for
knowledge processing within e-Sceince tasks. Considering the task solving
within a simulation-driven approach a set of knowledge-based procedures for
task definition and composite application processing can be identified. This
procedures could be supported by the use of domain-specific knowledge being
formalized and used for automation purpose. Within this work the developed
conceptual and technological knowledge-based toolbox for complex
multidisciplinary task solv-ing support is proposed. Using CLAVIRE cloud
computing environment as a core platform a set of interconnected expressive
technologies were developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7695</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7695</id><created>2013-12-30</created><updated>2014-07-09</updated><authors><author><keyname>Yang</keyname><forenames>Zai</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author><author><keyname>Zhang</keyname><forenames>Cishen</forenames></author></authors><title>A discretization-free sparse and parametric approach for linear array
  signal processing</title><categories>cs.IT math.IT</categories><comments>15 pages, double column, 8 figures, 1 table. To appear in IEEE
  Transactions on Signal Processing, accepted in July 2014</comments><doi>10.1109/TSP.2014.2339792</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direction of arrival (DOA) estimation in array processing using
uniform/sparse linear arrays is concerned in this paper. While sparse methods
via approximate parameter discretization have been popular in the past decade,
the discretization may cause problems, e.g., modeling error and increased
computations due to dense sampling. In this paper, an exact discretization-free
method, named as sparse and parametric approach (SPA), is proposed for uniform
and sparse linear arrays. SPA carries out parameter estimation in the
continuous range based on well-established covariance fitting criteria and
convex optimization. It guarantees to produce a sparse parameter estimate
without discretization required by existing sparse methods. Theoretical
analysis shows that the SPA parameter estimator is a large-snapshot realization
of the maximum likelihood estimator and is statistically consistent (in the
number of snapshots) under uncorrelated sources. Other merits of SPA include
improved resolution, applicability to arbitrary number of snapshots, robustness
to correlation of the sources and no requirement of user-parameters. Numerical
simulations are carried out to verify our analysis and demonstrate advantages
of SPA compared to existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7699</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7699</id><created>2013-12-30</created><updated>2014-01-23</updated><authors><author><keyname>Bodirsky</keyname><forenames>Manuel</forenames></author><author><keyname>Pinsker</keyname><forenames>Michael</forenames></author><author><keyname>Pongr&#xe1;cz</keyname><forenames>Andr&#xe1;s</forenames></author></authors><title>Reconstructing the topology of clones</title><categories>math.LO cs.LO math.RA</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Function clones are sets of functions on a fixed domain that are closed under
composition and contain the projections. They carry a natural algebraic
structure, provided by the laws of composition which hold in them, as well as a
natural topological structure, provided by the topology of pointwise
convergence, under which composition of functions becomes continuous. Inspired
by recent results indicating the importance of the topological ego of function
clones even for originally algebraic problems, we study questions of the
following type: In which situations does the algebraic structure of a function
clone determine its topological structure? We pay particular attention to
function clones which contain an oligomorphic permutation group, and discuss
applications of this situation in model theory and theoretical computer
science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7710</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7710</id><created>2013-12-30</created><authors><author><keyname>Weinmann</keyname><forenames>Andreas</forenames></author><author><keyname>Demaret</keyname><forenames>Laurent</forenames></author><author><keyname>Storath</keyname><forenames>Martin</forenames></author></authors><title>Total variation regularization for manifold-valued data</title><categories>math.OC cs.CV physics.med-ph</categories><msc-class>65K05, 65K10, 68U10, 94A08</msc-class><doi>10.1137/130951075</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider total variation minimization for manifold valued data. We propose
a cyclic proximal point algorithm and a parallel proximal point algorithm to
minimize TV functionals with $\ell^p$-type data terms in the manifold case.
These algorithms are based on iterative geodesic averaging which makes them
easily applicable to a large class of data manifolds. As an application, we
consider denoising images which take their values in a manifold. We apply our
algorithms to diffusion tensor images, interferometric SAR images as well as
sphere and cylinder valued images. For the class of Cartan-Hadamard manifolds
(which includes the data space in diffusion tensor imaging) we show the
convergence of the proposed TV minimizing algorithms to a global minimizer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7715</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7715</id><created>2013-12-30</created><updated>2014-07-31</updated><authors><author><keyname>Banica</keyname><forenames>Dan</forenames></author><author><keyname>Sminchisescu</keyname><forenames>Cristian</forenames></author></authors><title>Constrained Parametric Proposals and Pooling Methods for Semantic
  Segmentation in RGB-D Images</title><categories>cs.CV</categories><acm-class>I.2.10; I.4.6; I.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the problem of semantic segmentation based on RGB-D data, with
emphasis on analyzing cluttered indoor scenes containing many instances from
many visual categories. Our approach is based on a parametric figure-ground
intensity and depth-constrained proposal process that generates spatial layout
hypotheses at multiple locations and scales in the image followed by a
sequential inference algorithm that integrates the proposals into a complete
scene estimate. Our contributions can be summarized as proposing the following:
(1) a generalization of parametric max flow figure-ground proposal methodology
to take advantage of intensity and depth information, in order to
systematically and efficiently generate the breakpoints of an underlying
spatial model in polynomial time, (2) new region description methods based on
second-order pooling over multiple features constructed using both intensity
and depth channels, (3) an inference procedure that can resolve conflicts in
overlapping spatial partitions, and handles scenes with a large number of
objects category instances, of very different scales, (4) extensive evaluation
of the impact of depth, as well as the effectiveness of a large number of
descriptors, both pre-designed and automatically obtained using deep learning,
in a difficult RGB-D semantic segmentation problem with 92 classes. We report
state of the art results in the challenging NYU Depth v2 dataset, extended for
RMRC 2013 Indoor Segmentation Challenge, where currently the proposed model
ranks first, with an average score of 24.61% and a number of 39 classes won.
Moreover, we show that by combining second-order and deep learning features,
over 15% relative accuracy improvements can be additionally achieved. In a
scene classification benchmark, our methodology further improves the state of
the art by 24%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7724</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7724</id><created>2013-12-30</created><updated>2014-10-07</updated><authors><author><keyname>Lamperski</keyname><forenames>Andrew</forenames></author><author><keyname>Doyle</keyname><forenames>John C.</forenames></author></authors><title>The H2 Control Problem for Quadratically Invariant Systems with Delays</title><categories>cs.SY math.OC</categories><comments>Draft submitted to IEEE Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives a new solution to the output feedback H2 problem for
quadratically invariant communication delay patterns. A characterization of all
stabilizing controllers satisfying the delay constraints is given and the
decentralized H2 problem is cast as a convex model matching problem. The main
result shows that the model matching problem can be reduced to a
finite-dimensional quadratic program. A recursive state-space method for
computing the optimal controller based on vectorization is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7740</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7740</id><created>2013-12-30</created><authors><author><keyname>Mortezapour</keyname><forenames>Reza</forenames></author><author><keyname>Afzali</keyname><forenames>Mehdi</forenames></author></authors><title>Assessment of Customer Credit through Combined Clustering of Artificial
  Neural Networks, Genetics Algorithm and Bayesian Probabilities</title><categories>cs.AI</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, with respect to the increasing growth of demand to get credit from the
customers of banks and finance and credit institutions, using an effective and
efficient method to decrease the risk of non-repayment of credit given is very
necessary. Assessment of customers' credit is one of the most important and the
most essential duties of banks and institutions, and if an error occurs in this
field, it would leads to the great losses for banks and institutions. Thus,
using the predicting computer systems has been significantly progressed in
recent decades. The data that are provided to the credit institutions' managers
help them to make a straight decision for giving the credit or not-giving it.
In this paper, we will assess the customer credit through a combined
classification using artificial neural networks, genetics algorithm and
Bayesian probabilities simultaneously, and the results obtained from three
methods mentioned above would be used to achieve an appropriate and final
result. We use the K_folds cross validation test in order to assess the method
and finally, we compare the proposed method with the methods such as
Clustering-Launched Classification (CLC), Support Vector Machine (SVM) as well
as GA+SVM where the genetics algorithm has been used to improve them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7742</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7742</id><created>2013-12-27</created><authors><author><keyname>Masucci</keyname><forenames>Antonia Maria</forenames></author><author><keyname>Silva</keyname><forenames>Alonso</forenames></author></authors><title>Information Spreading on Almost Torus Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epidemic modeling has been extensively used in the last years in the field of
telecommunications and computer networks. We consider the popular
Susceptible-Infected-Susceptible spreading model as the metric for information
spreading. In this work, we analyze information spreading on a particular class
of networks denoted almost torus networks and over the lattice which can be
considered as the limit when the torus length goes to infinity. Almost torus
networks consist on the torus network topology where some nodes or edges have
been removed. We find explicit expressions for the characteristic polynomial of
these graphs and tight lower bounds for its computation. These expressions
allow us to estimate their spectral radius and thus how the information spreads
on these networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7758</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7758</id><created>2013-12-30</created><authors><author><keyname>Dubslaff</keyname><forenames>Clemens</forenames></author><author><keyname>Kl&#xfc;ppelholz</keyname><forenames>Sascha</forenames></author><author><keyname>Baier</keyname><forenames>Christel</forenames></author></authors><title>Probabilistic Model Checking for Energy Analysis in Software Product
  Lines</title><categories>cs.SE cs.LO</categories><comments>14 pages, 11 figures</comments><acm-class>B.2.2; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a software product line (SPL), a collection of software products is
defined by their commonalities in terms of features rather than explicitly
specifying all products one-by-one. Several verification techniques were
adapted to establish temporal properties of SPLs. Symbolic and family-based
model checking have been proven to be successful for tackling the combinatorial
blow-up arising when reasoning about several feature combinations. However,
most formal verification approaches for SPLs presented in the literature focus
on the static SPLs, where the features of a product are fixed and cannot be
changed during runtime. This is in contrast to dynamic SPLs, allowing to adapt
feature combinations of a product dynamically after deployment. The main
contribution of the paper is a compositional modeling framework for dynamic
SPLs, which supports probabilistic and nondeterministic choices and allows for
quantitative analysis. We specify the feature changes during runtime within an
automata-based coordination component, enabling to reason over strategies how
to trigger dynamic feature changes for optimizing various quantitative
objectives, e.g., energy or monetary costs and reliability. For our framework
there is a natural and conceptually simple translation into the input language
of the prominent probabilistic model checker PRISM. This facilitates the
application of PRISM's powerful symbolic engine to the operational behavior of
dynamic SPLs and their family-based analysis against various quantitative
queries. We demonstrate feasibility of our approach by a case study issuing an
energy-aware bonding network device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7793</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7793</id><created>2013-12-30</created><authors><author><keyname>Tan</keyname><forenames>Zhao</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Nehorai</keyname><forenames>Arye</forenames></author></authors><title>Direction of Arrival Estimation Using Co-prime Arrays: A Super
  Resolution Viewpoint</title><categories>cs.IT math.IT</categories><comments>Submitted on December 17th, 2013</comments><doi>10.1109/TSP.2014.2354316</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of direction of arrival (DOA) estimation using a
newly proposed structure of non-uniform linear arrays, referred to as co-prime
arrays, in this paper. By exploiting the second order statistical information
of the received signals, co-prime arrays exhibit O(MN) degrees of freedom with
only M + N sensors. A sparsity based recovery method is proposed to fully
utilize these degrees of freedom. Unlike traditional sparse recovery methods,
the proposed method is based on the developing theory of super resolution,
which considers a continuous range of possible sources instead of discretizing
this range into a discrete grid. With this approach, off-grid effects inherited
in traditional sparse recovery can be neglected, thus improving the accuracy of
DOA estimation. In this paper we show that in the noiseless case one can
theoretically detect up to M N sources with only 2M + N sensors. The noise 2
statistics of co-prime arrays are also analyzed to demonstrate the robustness
of the proposed optimization scheme. A source number detection method is
presented based on the spectrum reconstructed from the sparse method. By
extensive numerical examples, we show the superiority of the proposed method in
terms of DOA estimation accuracy, degrees of freedom, and resolution ability
compared with previous methods, such as MUSIC with spatial smoothing and the
discrete sparse recovery method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7794</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7794</id><created>2013-12-30</created><updated>2014-11-24</updated><authors><author><keyname>Gr&#xf6;chenig</keyname><forenames>Karlheinz</forenames></author><author><keyname>Romero</keyname><forenames>Jos&#xe9; Luis</forenames></author><author><keyname>Unnikrishnan</keyname><forenames>Jayakrishnan</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>On Minimal Trajectories for Mobile Sampling of Bandlimited Fields</title><categories>cs.IT math.CA math.IT</categories><comments>28 pages, 8 figures</comments><msc-class>94A20, 94A12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the design of sampling trajectories for stable sampling and the
reconstruction of bandlimited spatial fields using mobile sensors. The spectrum
is assumed to be a symmetric convex set. As a performance metric we use the
path density of the set of sampling trajectories that is defined as the total
distance traveled by the moving sensors per unit spatial volume of the spatial
region being monitored. Focussing first on parallel lines, we identify the set
of parallel lines with minimal path density that contains a set of stable
sampling for fields bandlimited to a known set. We then show that the problem
becomes ill-posed when the optimization is performed over all trajectories by
demonstrating a feasible trajectory set with arbitrarily low path density.
However, the problem becomes well-posed if we explicitly specify the stability
margins. We demonstrate this by obtaining a non-trivial lower bound on the path
density of an arbitrary set of trajectories that contain a sampling set with
explicitly specified stability bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7815</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7815</id><created>2013-12-30</created><updated>2014-08-08</updated><authors><author><keyname>Gallego</keyname><forenames>Guillermo</forenames></author><author><keyname>Berj&#xf3;n</keyname><forenames>Daniel</forenames></author><author><keyname>Garc&#xed;a</keyname><forenames>Narciso</forenames></author></authors><title>Optimal polygonal L1 linearization and fast interpolation of nonlinear
  systems</title><categories>math.OC cs.SY math.NA</categories><comments>10 pages, 9 figures, 1 table</comments><doi>10.1109/TCSI.2014.2327313</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of complex nonlinear systems is often carried out using simpler
piecewise linear representations of them. A principled and practical technique
is proposed to linearize and evaluate arbitrary continuous nonlinear functions
using polygonal (continuous piecewise linear) models under the L1 norm. A
thorough error analysis is developed to guide an optimal design of two kinds of
polygonal approximations in the asymptotic case of a large budget of evaluation
subintervals N. The method allows the user to obtain the level of linearization
(N) for a target approximation error and vice versa. It is suitable for, but
not limited to, an efficient implementation in modern Graphics Processing Units
(GPUs), allowing real-time performance of computationally demanding
applications. The quality and efficiency of the technique has been measured in
detail on two nonlinear functions that are widely used in many areas of
scientific computing and are expensive to evaluate
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7820</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7820</id><created>2013-12-30</created><updated>2014-06-26</updated><authors><author><keyname>Berth&#xe9;</keyname><forenames>Val&#xe9;rie</forenames></author><author><keyname>Jamet</keyname><forenames>Damien</forenames></author><author><keyname>Jolivet</keyname><forenames>Timo</forenames></author><author><keyname>Proven&#xe7;al</keyname><forenames>Xavier</forenames></author></authors><title>Critical connectedness of thin arithmetical discrete planes</title><categories>cs.DM math.CO</categories><comments>18 pages, v2 includes several corrections and is a long version of
  the DGCI extended abstract</comments><journal-ref>DGCI 2013, conference proceedings LNCS 7749, 107-118</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An arithmetical discrete plane is said to have critical connecting thickness
if its thickness is equal to the infimum of the set of values that preserve its
$2$-connectedness. This infimum thickness can be computed thanks to the fully
subtractive algorithm. This multidimensional continued fraction algorithm
consists, in its linear form, in subtracting the smallest entry to the other
ones. We provide a characterization of the discrete planes with critical
thickness that have zero intercept and that are $2$-connected. Our tools rely
on the notion of dual substitution which is a geometric version of the usual
notion of substitution acting on words. We associate with the fully subtractive
algorithm a set of substitutions whose incidence matrix is provided by the
matrices of the algorithm, and prove that their geometric counterparts generate
arithmetic discrete planes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7829</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7829</id><created>2013-12-30</created><updated>2014-07-07</updated><authors><author><keyname>Jolivet</keyname><forenames>Timo</forenames></author><author><keyname>Loridant</keyname><forenames>Beno&#xee;t</forenames></author><author><keyname>Luo</keyname><forenames>Jun</forenames></author></authors><title>Rauzy fractals with countable fundamental group</title><categories>math.DS cs.DM math.CO</categories><comments>14 pages, v3 includes some corrections to match the published version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that every free group of finite rank can be realized as the
fundamental group of a planar Rauzy fractal associated with a 4-letter
unimodular cubic Pisot substitution. This characterizes all countable
fundamental groups for planar Rauzy fractals. We give an explicit construction
relying on two operations on substitutions: symbolic splittings and
conjugations by free group automorphisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7832</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7832</id><created>2013-12-30</created><updated>2014-05-19</updated><authors><author><keyname>Fu</keyname><forenames>Li</forenames></author></authors><title>Defining implication relation for classical logic</title><categories>math.LO cs.LO</categories><comments>10 pages, 1 figure, PDFLaTeX; typos corrected, &quot;truth function(s)&quot;
  corrected to &quot;valuation function(s)&quot; in Definition 3.1.1 and Definition 3.1.2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simple and useful classical logic is unfortunately defective with its
problematic definition of material implication. This paper presents an
implication relation defined by a simple equation to replace the traditional
material implication in classical logic. Common &quot;paradoxes&quot; of material
implication are avoided while simplicity and usefulness of the system are
reserved with this implication relation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7839</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7839</id><created>2013-12-30</created><updated>2014-07-15</updated><authors><author><keyname>Collin</keyname><forenames>Jussi</forenames></author><author><keyname>Kirkko-Jaakkola</keyname><forenames>Martti</forenames></author><author><keyname>Takala</keyname><forenames>Jarmo</forenames></author></authors><title>Effect of Carouseling on Angular Rate Sensor Error Processes</title><categories>cs.OH</categories><comments>This is a post-print version of a paper published in the IEEE
  Transactions on Instrumentation and Measurement (2014)</comments><doi>10.1109/TIM.2014.2335921</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Carouseling is an efficient method to mitigate the measurement errors of
inertial sensors, particularly MEMS gyroscopes. In this article, the effect of
carouseling on the most significant stochastic error processes of a MEMS
gyroscope, i.e., additive bias, white noise, 1/f noise, and rate random walk,
is investigated. Variance propagation equations for these processes under
averaging and carouseling are defined. Furthermore, a novel approach to
generating 1/f noise is presented. The experimental results show that
carouseling reduces the contributions of additive bias, 1/f noise, and rate
random walk significantly in comparison with plain averaging, which can be
utilized to improve the accuracy of dead reckoning systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7844</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7844</id><created>2013-12-30</created><authors><author><keyname>Wong</keyname><forenames>Felix Ming Fai</forenames></author><author><keyname>Joe-Wong</keyname><forenames>Carlee</forenames></author><author><keyname>Ha</keyname><forenames>Sangtae</forenames></author><author><keyname>Liu</keyname><forenames>Zhenming</forenames></author><author><keyname>Chiang</keyname><forenames>Mung</forenames></author></authors><title>Mind Your Own Bandwidth: An Edge Solution to Peak-hour Broadband
  Congestion</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by recent increases in network traffic, we propose a decentralized
network edge-based solution to peak-hour broadband congestion that incentivizes
users to moderate their bandwidth demands to their actual needs. Our solution
is centered on smart home gateways that allocate bandwidth in a two-level
hierarchy: first, a gateway purchases guaranteed bandwidth from the Internet
Service Provider (ISP) with virtual credits. It then self-limits its bandwidth
usage and distributes the bandwidth among its apps and devices according to
their relative priorities. To this end, we design a credit allocation and
redistribution mechanism for the first level, and implement our gateways on
commodity wireless routers for the second level. We demonstrate our system's
effectiveness and practicality with theoretical analysis, simulations and
experiments on real traffic. Compared to a baseline equal sharing algorithm,
our solution significantly improves users' overall satisfaction and yields a
fair allocation of bandwidth across users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7847</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7847</id><created>2013-12-30</created><updated>2015-02-05</updated><authors><author><keyname>Tsiligkaridis</keyname><forenames>Theodoros</forenames></author><author><keyname>Sadler</keyname><forenames>Brian M.</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>On Decentralized Estimation with Active Queries</title><categories>cs.MA cs.IT cs.SY math.IT</categories><comments>22 pages, to appear in IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of decentralized 20 questions with noise for multiple
players/agents under the minimum entropy criterion in the setting of stochastic
search over a parameter space, with application to target localization. We
propose decentralized extensions of the active query-based stochastic search
strategy that combines elements from the 20 questions approach and social
learning. We prove convergence to correct consensus on the value of the
parameter. This framework provides a flexible and tractable mathematical model
for decentralized parameter estimation systems based on active querying. We
illustrate the effectiveness and robustness of the proposed decentralized
collaborative 20 questions algorithm for random network topologies with
information sharing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7852</identifier>
 <datestamp>2014-01-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7852</id><created>2013-12-30</created><authors><author><keyname>Erdbrink</keyname><forenames>C. D.</forenames></author><author><keyname>Krzhizhanovskaya</keyname><forenames>V. V.</forenames></author><author><keyname>Sloot</keyname><forenames>P. M. A.</forenames></author></authors><title>Evolutionary Design of Numerical Methods: Generating Finite Difference
  and Integration Schemes by Differential Evolution</title><categories>cs.NE cs.NA</categories><comments>19 pages, 7 figures, 10 tables, 4 appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical and new numerical schemes are generated using evolutionary
computing. Differential Evolution is used to find the coefficients of finite
difference approximations of function derivatives, and of single and multi-step
integration methods. The coefficients are reverse engineered based on samples
from a target function and its derivative used for training. The Runge-Kutta
schemes are trained using the order condition equations. An appealing feature
of the evolutionary method is the low number of model parameters. The
population size, termination criterion and number of training points are
determined in a sensitivity analysis. Computational results show good agreement
between evolved and analytical coefficients. In particular, a new fifth-order
Runge-Kutta scheme is computed which adheres to the order conditions with a sum
of absolute errors of order 10^-14. Execution of the evolved schemes proved the
intended orders of accuracy. The outcome of this study is valuable for future
developments in the design of complex numerical methods that are out of reach
by conventional means.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7853</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7853</id><created>2013-12-30</created><updated>2014-05-13</updated><authors><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Communication Efficient Distributed Optimization using an Approximate
  Newton-type Method</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel Newton-type method for distributed optimization, which is
particularly well suited for stochastic optimization and learning problems. For
quadratic objectives, the method enjoys a linear rate of convergence which
provably \emph{improves} with the data size, requiring an essentially constant
number of iterations under reasonable assumptions. We provide theoretical and
empirical evidence of the advantages of our method compared to other
approaches, such as one-shot parameter averaging and ADMM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7862</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7862</id><created>2013-12-30</created><authors><author><keyname>Sidoravicius</keyname><forenames>Vladas</forenames></author><author><keyname>Stauffer</keyname><forenames>Alexandre</forenames></author></authors><title>Phase transition for finite-speed detection among moving particles</title><categories>math.PR cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the model where particles are initially distributed on
$\mathbb{Z}^d, \, d\geq 2$, according to a Poisson point process of intensity
$\lambda&gt;0$, and are moving in continuous time as independent simple symmetric
random walks. We study the escape versus detection problem, in which the
target, initially placed at the origin of $\mathbb{Z}^d, \, d\geq 2$, and
changing its location on the lattice in time according to some rule, is said to
be detected if at some finite time its position coincides with the position of
a particle. We consider the case where the target can move with speed at most
1, according to any continuous function and can adapt its motion based on the
location of the particles. We show that there exists sufficiently small
$\lambda_* &gt; 0$, so that if the initial density of particles $\lambda &lt;
\lambda_*$, then the target can avoid detection forever.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.7869</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.7869</id><created>2013-12-30</created><updated>2013-12-31</updated><authors><author><keyname>Wei</keyname><forenames>Jinliang</forenames></author><author><keyname>Dai</keyname><forenames>Wei</forenames></author><author><keyname>Kumar</keyname><forenames>Abhimanu</forenames></author><author><keyname>Zheng</keyname><forenames>Xun</forenames></author><author><keyname>Ho</keyname><forenames>Qirong</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Consistent Bounded-Asynchronous Parameter Servers for Distributed ML</title><categories>stat.ML cs.DC cs.LG</categories><comments>Corrected Title</comments><report-no>CMU-ML-13-115</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In distributed ML applications, shared parameters are usually replicated
among computing nodes to minimize network overhead. Therefore, proper
consistency model must be carefully chosen to ensure algorithm's correctness
and provide high throughput. Existing consistency models used in
general-purpose databases and modern distributed ML systems are either too
loose to guarantee correctness of the ML algorithms or too strict and thus fail
to fully exploit the computing power of the underlying distributed system.
  Many ML algorithms fall into the category of \emph{iterative convergent
algorithms} which start from a randomly chosen initial point and converge to
optima by repeating iteratively a set of procedures. We've found that many such
algorithms are to a bounded amount of inconsistency and still converge
correctly. This property allows distributed ML to relax strict consistency
models to improve system performance while theoretically guarantees algorithmic
correctness. In this paper, we present several relaxed consistency models for
asynchronous parallel computation and theoretically prove their algorithmic
correctness. The proposed consistency models are implemented in a distributed
parameter server and evaluated in the context of a popular ML application:
topic modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0001</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0001</id><created>2013-12-27</created><updated>2015-01-30</updated><authors><author><keyname>Bertschinger</keyname><forenames>Nils</forenames></author><author><keyname>Wolpert</keyname><forenames>David H.</forenames></author><author><keyname>Olbrich</keyname><forenames>Eckehard</forenames></author><author><keyname>Jost</keyname><forenames>Juergen</forenames></author></authors><title>Value of information in noncooperative games</title><categories>cs.GT</categories><comments>Shortened and simplified text, in particular no knowledge of
  differential geometry is assumed any more. Accordingly the title has been
  changed from &quot;Information Geometry of noncooperative games&quot; to &quot;Value of
  information in noncooperative games&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In some games, additional information hurts a player, e.g., in games with
first-mover advantage, the second-mover is hurt by seeing the first-mover's
move. What properties of a game determine whether it has such negative &quot;value
of information&quot; for a particular player? Can a game have negative value of
information for all players? To answer such questions, we generalize the
definition of marginal utility of a good to define the marginal utility of a
parameter vector specifying a game. So rather than analyze the global structure
of the relationship between a game's parameter vector and player behavior, as
in previous work, we focus on the local structure of that relationship. This
allows us to prove that generically, every game can have negative marginal
value of information, unless one imposes a priori constraints on allowed
changes to the game's parameter vector. We demonstrate these and related
results numerically, and discuss their implications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0034</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0034</id><created>2013-12-30</created><authors><author><keyname>Khan</keyname><forenames>Atta ur Rehman</forenames></author><author><keyname>Othman</keyname><forenames>Mazliza</forenames></author><author><keyname>Khan</keyname><forenames>Abdul Nasir</forenames></author></authors><title>A Novel Application Licensing Framework for Mobile Cloud Environment</title><categories>cs.CY cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile cloud computing is a new technology that enhances smartphone
applications capabilities in terms of performance, energy efficiency, and
execution support. These features are achieved via computation offloading
technique that is supported by specialized mobile cloud application development
models. However, the cloud-enabled applications are prone to application piracy
issue for which the traditional licensing frameworks are of no use. Therefore,
a new licensing framework is required to control application piracy in mobile
cloud environment. This paper presents a preliminary design of a novel
application licensing framework for mobile cloud environment that restricts
execution of applications on unauthenticated smartphones and cloud resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0042</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0042</id><created>2013-12-30</created><updated>2014-01-04</updated><authors><author><keyname>Andoni</keyname><forenames>Alexandr</forenames></author><author><keyname>Nikolov</keyname><forenames>Aleksandar</forenames></author><author><keyname>Onak</keyname><forenames>Krzysztof</forenames></author><author><keyname>Yaroslavtsev</keyname><forenames>Grigory</forenames></author></authors><title>Parallel Algorithms for Geometric Graph Problems</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give algorithms for geometric graph problems in the modern parallel models
inspired by MapReduce. For example, for the Minimum Spanning Tree (MST) problem
over a set of points in the two-dimensional space, our algorithm computes a
$(1+\epsilon)$-approximate MST. Our algorithms work in a constant number of
rounds of communication, while using total space and communication proportional
to the size of the data (linear space and near linear time algorithms). In
contrast, for general graphs, achieving the same result for MST (or even
connectivity) remains a challenging open problem, despite drawing significant
attention in recent years.
  We develop a general algorithmic framework that, besides MST, also applies to
Earth-Mover Distance (EMD) and the transportation cost problem. Our algorithmic
framework has implications beyond the MapReduce model. For example it yields a
new algorithm for computing EMD cost in the plane in near-linear time,
$n^{1+o_\epsilon(1)}$. We note that while recently Sharathkumar and Agarwal
developed a near-linear time algorithm for $(1+\epsilon)$-approximating EMD,
our algorithm is fundamentally different, and, for example, also solves the
transportation (cost) problem, raised as an open question in their work.
Furthermore, our algorithm immediately gives a $(1+\epsilon)$-approximation
algorithm with $n^{\delta}$ space in the streaming-with-sorting model with
$1/\delta^{O(1)}$ passes. As such, it is tempting to conjecture that the
parallel models may also constitute a concrete playground in the quest for
efficient algorithms for EMD (and other similar problems) in the vanilla
streaming model, a well-known open problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0044</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0044</id><created>2013-12-30</created><authors><author><keyname>Weller</keyname><forenames>Adrian</forenames></author><author><keyname>Jebara</keyname><forenames>Tony</forenames></author></authors><title>Approximating the Bethe partition function</title><categories>cs.LG</categories><report-no>cucs-031-13</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When belief propagation (BP) converges, it does so to a stationary point of
the Bethe free energy $F$, and is often strikingly accurate. However, it may
converge only to a local optimum or may not converge at all. An algorithm was
recently introduced for attractive binary pairwise MRFs which is guaranteed to
return an $\epsilon$-approximation to the global minimum of $F$ in polynomial
time provided the maximum degree $\Delta=O(\log n)$, where $n$ is the number of
variables. Here we significantly improve this algorithm and derive several
results including a new approach based on analyzing first derivatives of $F$,
which leads to performance that is typically far superior and yields a fully
polynomial-time approximation scheme (FPTAS) for attractive models without any
degree restriction. Further, the method applies to general (non-attractive)
models, though with no polynomial time guarantee in this case, leading to the
important result that approximating $\log$ of the Bethe partition function,
$\log Z_B=-\min F$, for a general model to additive $\epsilon$-accuracy may be
reduced to a discrete MAP inference problem. We explore an application to
predicting equipment failure on an urban power network and demonstrate that the
Bethe approximation can perform well even when BP fails to converge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0050</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0050</id><created>2013-12-30</created><updated>2014-04-02</updated><authors><author><keyname>D'yachkov</keyname><forenames>Arkady</forenames></author><author><keyname>Vorobyev</keyname><forenames>Ilya</forenames></author><author><keyname>Polianskii</keyname><forenames>Nikita</forenames></author><author><keyname>Shchukin</keyname><forenames>Vladislav</forenames></author></authors><title>Bounds on the rate of superimposed codes</title><categories>cs.IT math.IT math.PR</categories><comments>32 pages, 3 tables</comments><doi>10.1134/S0032946014010037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A binary code is called a superimposed cover-free $(s,\ell)$-code if the code
is identified by the incidence matrix of a family of finite sets in which no
intersection of $\ell$ sets is covered by the union of $s$ others. A binary
code is called a superimposed list-decoding $s_L$-code if the code is
identified by the incidence matrix of a family of finite sets in which the
union of any $s$ sets can cover not more than $L-1$ other sets of the family.
For $L=\ell=1$, both of the definitions coincide and the corresponding binary
code is called a superimposed $s$-code. Our aim is to obtain new lower and
upper bounds on the rate of given codes. The most interesting result is a lower
bound on the rate of superimposed cover-free $(s,\ell)$-code based on the
ensemble of constant-weight binary codes. If parameter $\ell\ge1$ is fixed and
$s\to\infty$, then the ratio of this lower bound to the best known upper bound
converges to the limit $2\,e^{-2}=0,271$. For the classical case $\ell=1$ and
$s\ge2$, the given Statement means that our recurrent upper bound on the rate
of superimposed $s$-codes obtained in 1982 is attained to within a constant
factor $a$, $0,271\le a\le1$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0052</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0052</id><created>2013-12-30</created><authors><author><keyname>Firner</keyname><forenames>Bernhard</forenames></author><author><keyname>Sugrim</keyname><forenames>Shridatt</forenames></author><author><keyname>Yang</keyname><forenames>Yulong</forenames></author><author><keyname>Lindqvist</keyname><forenames>Janne</forenames></author></authors><title>Elastic Pathing: Your Speed is Enough to Track You</title><categories>cs.CR</categories><report-no>Technical Report - Rutgers University - WINLAB - TR-429</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today people increasingly have the opportunity to opt-in to &quot;usage-based&quot;
automotive insurance programs for reducing insurance premiums. In these
programs, participants install devices in their vehicles that monitor their
driving behavior, which raises some privacy concerns. Some devices collect
fine-grained speed data to monitor driving habits. Companies that use these
devices claim that their approach is privacy-preserving because speedometer
measurements do not have physical locations. However, we show that with
knowledge of the user's home location, as the insurance companies have, speed
data is sufficient to discover driving routes and destinations when trip data
is collected over a period of weeks. To demonstrate the real-world
applicability of our approach we applied our algorithm, elastic pathing, to
data collected over hundreds of driving trips occurring over several months.
With this data and our approach, we were able to predict trip destinations to
within 250 meters of ground truth in 10% of the traces and within 500 meters in
20% of the traces. This result, combined with the amount of speed data that is
being collected by insurance companies, constitutes a substantial breach of
privacy because a person's regular driving pattern can be deduced with repeated
examples of the same paths with just a few weeks of monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0061</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0061</id><created>2013-12-30</created><updated>2015-02-04</updated><authors><author><keyname>Yu</keyname><forenames>Changtao</forenames></author></authors><title>On dually flat general $(\alpha,\beta)$-metrics</title><categories>math.DG cs.IT math.IT</categories><comments>10 pages,no figures</comments><msc-class>53B40, 53C60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, the dual flatness, which is connected with Statistics and
Information geometry, of general $(\alpha,\beta)$-metrics (a new class of
Finsler metrics) is studied. A nice characterization for such metrics to be
dually flat under some suitable conditions is provided and all the solutions
are completely determined. By using an original kind of metrical deformations,
many non-trivial explicit examples are constructed. Moreover, the relationship
of dual flatness and projective flatness of such metrics is shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0063</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0063</id><created>2013-12-30</created><updated>2014-09-20</updated><authors><author><keyname>Geneson</keyname><forenames>Jesse</forenames></author></authors><title>Improved bounds on maximum sets of letters in sequences with forbidden
  alternations</title><categories>math.CO cs.DM</categories><comments>10 pages</comments><msc-class>05D99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $A_{s,k}(m)$ be the maximum number of distinct letters in any sequence
which can be partitioned into $m$ contiguous blocks of pairwise distinct
letters, has at least $k$ occurrences of every letter, and has no subsequence
forming an alternation of length $s$. Nivasch (2010) proved that $A_{5,
2d+1}(m) = \theta( m \alpha_{d}(m))$ for all fixed $d \geq 2$. We show that
$A_{s+1, s}(m) = \binom{m- \lceil \frac{s}{2} \rceil}{\lfloor \frac{s}{2}
\rfloor}$ for all $s \geq 2$, $A_{5, 6}(m) = \theta(m \log \log m)$, and $A_{5,
2d+2}(m) = \theta(m \alpha_{d}(m))$ for all fixed $d \geq 3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0069</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0069</id><created>2013-12-30</created><updated>2014-01-03</updated><authors><author><keyname>Martinenghi</keyname><forenames>Davide</forenames></author></authors><title>Determining Relevant Relations for Datalog Queries under Access
  Limitations is Undecidable</title><categories>cs.DB</categories><comments>2 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Access limitations are restrictions in the way in which the tuples of a
relation can be accessed. Under access limitations, query answering becomes
more complex than in the traditional case, with no guarantee that the answer
tuples that can be extracted (aka maximal answer) are all those that would be
found without access limitations (aka complete answer). The field of query
answering under access limitations has been broadly investigated in the past.
Attention has been devoted to the problem of determining relations that are
relevant for a query, i.e., those (possibly off-query) relations that might
need to be accessed in order to find all tuples in the maximal answer. In this
short paper, we show that relevance is undecidable for Datalog queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0077</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0077</id><created>2013-12-30</created><authors><author><keyname>Kamthe</keyname><forenames>Sanket</forenames></author><author><keyname>Peters</keyname><forenames>Jan</forenames></author><author><keyname>Deisenroth</keyname><forenames>Marc P</forenames></author></authors><title>Multi-modal filtering for non-linear estimation</title><categories>cs.SY</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Multi-modal densities appear frequently in time series and practical
applications. However, they cannot be represented by common state estimators,
such as the Extended Kalman Filter (EKF) and the Unscented Kalman Filter (UKF),
which additionally suffer from the fact that uncertainty is often not captured
sufficiently well, which can result in incoherent and divergent tracking
performance. In this paper, we address these issues by devising a non-linear
filtering algorithm where densities are represented by Gaussian mixture models,
whose parameters are estimated in closed form. The resulting method exhibits a
superior performance on typical benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0085</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0085</id><created>2013-12-30</created><authors><author><keyname>Lee</keyname><forenames>Yin Tat</forenames></author></authors><title>Probabilistic Spectral Sparsification In Sublinear Time</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a variant of spectral sparsification, called
probabilistic $(\varepsilon,\delta)$-spectral sparsification. Roughly speaking,
it preserves the cut value of any cut $(S,S^{c})$ with an $1\pm\varepsilon$
multiplicative error and a $\delta\left|S\right|$ additive error. We show how
to produce a probabilistic $(\varepsilon,\delta)$-spectral sparsifier with
$O(n\log n/\varepsilon^{2})$ edges in time $\tilde{O}(n/\varepsilon^{2}\delta)$
time for unweighted undirected graph. This gives fastest known sub-linear time
algorithms for different cut problems on unweighted undirected graph such as
  - An $\tilde{O}(n/OPT+n^{3/2+t})$ time $O(\sqrt{\log n/t})$-approximation
algorithm for the sparsest cut problem and the balanced separator problem.
  - A $n^{1+o(1)}/\varepsilon^{4}$ time approximation minimum s-t cut algorithm
with an $\varepsilon n$ additive error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0092</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0092</id><created>2013-12-30</created><authors><author><keyname>Shinde</keyname><forenames>Shraddha S.</forenames></author><author><keyname>Khedkar</keyname><forenames>Prof. Anagha P.</forenames></author></authors><title>A Novel Approach For Generating Face Template Using Bda</title><categories>cs.CV</categories><comments>11 pages, ITCSE 2013 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In identity management system, commonly used biometric recognition system
needs attention towards issue of biometric template protection as far as more
reliable solution is concerned. In view of this biometric template protection
algorithm should satisfy security, discriminability and cancelability. As no
single template protection method is capable of satisfying the basic
requirements, a novel technique for face template generation and protection is
proposed. The novel approach is proposed to provide security and accuracy in
new user enrollment as well as authentication process. This novel technique
takes advantage of both the hybrid approach and the binary discriminant
analysis algorithm. This algorithm is designed on the basis of random
projection, binary discriminant analysis and fuzzy commitment scheme. Three
publicly available benchmark face databases are used for evaluation. The
proposed novel technique enhances the discriminability and recognition accuracy
by 80% in terms of matching score of the face images and provides high
security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0102</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0102</id><created>2013-12-31</created><updated>2014-12-28</updated><authors><author><keyname>Zamani</keyname><forenames>Mahdi</forenames></author><author><keyname>Movahedi</keyname><forenames>Mahnush</forenames></author><author><keyname>Ebadzadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Pedram</keyname><forenames>Hossein</forenames></author></authors><title>A DDoS-Aware IDS Model Based on Danger Theory and Mobile Agents</title><categories>cs.DC cs.AI cs.CR cs.MA</categories><comments>10 pages, 3 figure</comments><acm-class>C.2.1; C.2.2; I.2.11; C.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an artificial immune model for intrusion detection in distributed
systems based on a relatively recent theory in immunology called Danger theory.
Based on Danger theory, immune response in natural systems is a result of
sensing corruption as well as sensing unknown substances. In contrast,
traditional self-nonself discrimination theory states that immune response is
only initiated by sensing nonself (unknown) patterns. Danger theory solves many
problems that could only be partially explained by the traditional model.
Although the traditional model is simpler, such problems result in high false
positive rates in immune-inspired intrusion detection systems. We believe using
danger theory in a multi-agent environment that computationally emulates the
behavior of natural immune systems is effective in reducing false positive
rates. We first describe a simplified scenario of immune response in natural
systems based on danger theory and then, convert it to a computational model as
a network protocol. In our protocol, we define several immune signals and model
cell signaling via message passing between agents that emulate cells. Most
messages include application-specific patterns that must be meaningfully
extracted from various system properties. We show how to model these messages
in practice by performing a case study on the problem of detecting distributed
denial-of-service attacks in wireless sensor networks. We conduct a set of
systematic experiments to find a set of performance metrics that can accurately
distinguish malicious patterns. The results indicate that the system can be
efficiently used to detect malicious patterns with a high level of accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0104</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0104</id><created>2013-12-31</created><authors><author><keyname>Bao</keyname><forenames>Yukun</forenames></author><author><keyname>Xiong</keyname><forenames>Tao</forenames></author><author><keyname>Hu</keyname><forenames>Zhongyi</forenames></author></authors><title>PSO-MISMO Modeling Strategy for Multi-Step-Ahead Time Series Prediction</title><categories>cs.AI cs.LG cs.NE stat.ML</categories><comments>14 pages. IEEE Transactions on Cybernetics. 2013</comments><doi>10.1109/TCYB.2013.2265084</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-step-ahead time series prediction is one of the most challenging
research topics in the field of time series modeling and prediction, and is
continually under research. Recently, the multiple-input several
multiple-outputs (MISMO) modeling strategy has been proposed as a promising
alternative for multi-step-ahead time series prediction, exhibiting advantages
compared with the two currently dominating strategies, the iterated and the
direct strategies. Built on the established MISMO strategy, this study proposes
a particle swarm optimization (PSO)-based MISMO modeling strategy, which is
capable of determining the number of sub-models in a self-adaptive mode, with
varying prediction horizons. Rather than deriving crisp divides with equal-size
s prediction horizons from the established MISMO, the proposed PSO-MISMO
strategy, implemented with neural networks, employs a heuristic to create
flexible divides with varying sizes of prediction horizons and to generate
corresponding sub-models, providing considerable flexibility in model
construction, which has been validated with simulated and real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0108</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0108</id><created>2013-12-31</created><updated>2014-04-28</updated><authors><author><keyname>Banerjee</keyname><forenames>Sandip</forenames></author><author><keyname>Banik</keyname><forenames>Aritra</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Bhargab B.</forenames></author><author><keyname>Bishnu</keyname><forenames>Arijit</forenames></author><author><keyname>Chatterjee</keyname><forenames>Soumyottam</forenames></author></authors><title>On Packing Almost Half of a Square with Anchored Rectangles: A
  Constructive Approach</title><categories>cs.CG</categories><comments>This paper has been withdrawn as a bug has been discovered in the
  proof of claim 5 of the paper entitled &quot;An Existential Proof of the
  Conjecture on Packing Anchored Rectangles&quot; and this result has been used here
  also</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the following geometric puzzle whose origin was
traced to Allan Freedman \cite{croft91,tutte69} in the 1960s by Dumitrescu and
T{\'o}th \cite{adriancasaba2011}. The puzzle has been popularized of late by
Peter Winkler \cite{Winkler2007}. Let $P_{n}$ be a set of $n$ points, including
the origin, in the unit square $U = [0,1]^2$. The problem is to construct $n$
axis-parallel and mutually disjoint rectangles inside $U$ such that the
bottom-left corner of each rectangle coincides with a point in $P_{n}$ and the
total area covered by the rectangles is maximized. We would term the above
rectangles as \emph{anchored rectangles}. The longstanding conjecture has been
that at least half of $U$ can be covered when anchored rectangles are properly
placed. Dumitrescu and T{\'o}th \cite{Dumitrescu2012} have shown a construction
method that can cover at least $0.09121$, i.e., roughly $9\%$ of the area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0113</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0113</id><created>2013-12-31</created><updated>2014-06-17</updated><authors><author><keyname>Wang</keyname><forenames>Shaofan</forenames></author><author><keyname>Kong</keyname><forenames>Dehui</forenames></author><author><keyname>Xue</keyname><forenames>Juan</forenames></author><author><keyname>Zhu</keyname><forenames>Weijia</forenames></author><author><keyname>Xu</keyname><forenames>Min</forenames></author><author><keyname>Yin</keyname><forenames>Baocai</forenames></author><author><keyname>Roth</keyname><forenames>Hubert</forenames></author></authors><title>Connectivity-preserving Geometry Images</title><categories>cs.GR</categories><comments>15 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose connectivity-preserving geometry images (CGIMs), which map a
three-dimensional mesh onto a rectangular regular array of an image, such that
the reconstructed mesh produces no sampling errors, but merely round-off
errors. We obtain a V-matrix with respect to the original mesh, whose elements
are vertices of the mesh, which intrinsically preserves the vertex-set and the
connectivity of the original mesh in the sense of allowing round-off errors. We
generate a CGIM array by using the Cartesian coordinates of corresponding
vertices of the V-matrix. To reconstruct a mesh, we obtain a vertex-set and an
edge-set by collecting all the elements with different pixels, and all
different pairwise adjacent elements from the CGIM array respectively. Compared
with traditional geometry images, CGIMs achieve minimum reconstruction errors
with an efficient parametrization-free algorithm via elementary permutation
techniques. We apply CGIMs to lossy compression of meshes, and the experimental
results show that CGIMs perform well in reconstruction precision and detail
preservation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0114</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0114</id><created>2013-12-31</created><authors><author><keyname>Melazzi</keyname><forenames>Nicola Blefari</forenames></author><author><keyname>Detti</keyname><forenames>Andrea</forenames></author><author><keyname>Arumaithurai</keyname><forenames>Mayutan</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>K. K.</forenames></author></authors><title>Internames: a name-to-name principle for the future Internet</title><categories>cs.NI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Internames, an architectural framework in which names are used to
identify all entities involved in communication: contents, users, devices,
logical as well as physical points involved in the communication, and services.
By not having a static binding between the name of a communication entity and
its current location, we allow entities to be mobile, enable them to be reached
by any of a number of basic communication primitives, enable communication to
span networks with different technologies and allow for disconnected operation.
Furthermore, with the ability to communicate between names, the communication
path can be dynamically bound to any of a number of end-points, and the
end-points themselves could change as needed. A key benefit of our architecture
is its ability to accommodate gradual migration from the current IP
infrastructure to a future that may be a ubiquitous Information Centric
Network. Basic building blocks of Internames are: i) a name-based Application
Programming Interface; ii) a separation of identifiers (names) and locators;
iii) a powerful Name Resolution Service (NRS) that dynamically maps names to
locators, as a function of time/location/context/service; iv) a built-in
capacity of evolution, allowing a transparent migration from current networks
and the ability to include as particular cases current specific architectures.
To achieve this vision, shared by many other researchers, we exploit and expand
on Information Centric Networking principles, extending ICN functionality
beyond content retrieval, easing send-to-name and push services, and allowing
to use names also to route data in the return path. A key role in this
architecture is played by the NRS, which allows for the co-existence of
multiple network &quot;realms&quot;, including current IP and non-IP networks, glued
together by a name-to-name overarching communication primitive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0116</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0116</id><created>2013-12-31</created><authors><author><keyname>Govindaraj</keyname><forenames>Dinesh</forenames></author><author><keyname>Sankaran</keyname><forenames>Raman</forenames></author><author><keyname>Menon</keyname><forenames>Sreedal</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Chiranjib</forenames></author></authors><title>Controlled Sparsity Kernel Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple Kernel Learning(MKL) on Support Vector Machines(SVMs) has been a
popular front of research in recent times due to its success in application
problems like Object Categorization. This success is due to the fact that MKL
has the ability to choose from a variety of feature kernels to identify the
optimal kernel combination. But the initial formulation of MKL was only able to
select the best of the features and misses out many other informative kernels
presented. To overcome this, the Lp norm based formulation was proposed by
Kloft et. al. This formulation is capable of choosing a non-sparse set of
kernels through a control parameter p. Unfortunately, the parameter p does not
have a direct meaning to the number of kernels selected. We have observed that
stricter control over the number of kernels selected gives us an edge over
these techniques in terms of accuracy of classification and also helps us to
fine tune the algorithms to the time requirements at hand. In this work, we
propose a Controlled Sparsity Kernel Learning (CSKL) formulation that can
strictly control the number of kernels which we wish to select. The CSKL
formulation introduces a parameter t which directly corresponds to the number
of kernels selected. It is important to note that a search in t space is finite
and fast as compared to p. We have also provided an efficient Reduced Gradient
Descent based algorithm to solve the CSKL formulation, which is proven to
converge. Through our experiments on the Caltech101 Object Categorization
dataset, we have also shown that one can achieve better accuracies than the
previous formulations through the right choice of t.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0118</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0118</id><created>2013-12-31</created><authors><author><keyname>Ranganath</keyname><forenames>Rajesh</forenames></author><author><keyname>Gerrish</keyname><forenames>Sean</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author></authors><title>Black Box Variational Inference</title><categories>stat.ML cs.LG stat.CO stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variational inference has become a widely used method to approximate
posteriors in complex latent variables models. However, deriving a variational
inference algorithm generally requires significant model-specific analysis, and
these efforts can hinder and deter us from quickly developing and exploring a
variety of models for a problem at hand. In this paper, we present a &quot;black
box&quot; variational inference algorithm, one that can be quickly applied to many
models with little additional derivation. Our method is based on a stochastic
optimization of the variational objective where the noisy gradient is computed
from Monte Carlo samples from the variational distribution. We develop a number
of methods to reduce the variance of the gradient, always maintaining the
criterion that we want to avoid difficult model-based derivations. We evaluate
our method against the corresponding black box sampling based methods. We find
that our method reaches better predictive likelihoods much faster than sampling
methods. Finally, we demonstrate that Black Box Variational Inference lets us
easily explore a wide space of models by quickly constructing and evaluating
several models of longitudinal healthcare data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0119</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0119</id><created>2013-12-31</created><authors><author><keyname>Naparstek</keyname><forenames>Oshri</forenames></author><author><keyname>Leshem</keyname><forenames>Amir</forenames></author></authors><title>Expected time complexity of the auction algorithm and the push relabel
  algorithm for maximal bipartite matching on random graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze the expected time complexity of the auction
algorithm for the matching problem on random bipartite graphs. We prove that
the expected time complexity of the auction algorithm for bipartite matching is
$O\left(\frac{N\log^2(N)}{\log\left(Np\right)}\right)$ on sequential machines.
This is equivalent to other augmenting path algorithms such as the HK
algorithm. Furthermore, we show that the algorithm can be implemented on
parallel machines with $O(\log(N))$ processors and shared memory with an
expected time complexity of $O(N\log(N))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0120</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0120</id><created>2013-12-31</created><authors><author><keyname>Ge</keyname><forenames>Cunjing</forenames></author><author><keyname>Ma</keyname><forenames>Feifei</forenames></author><author><keyname>Zhang</keyname><forenames>Jian</forenames></author></authors><title>A Fast and Practical Method to Estimate Volumes of Convex Polytopes</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The volume is an important attribute of a convex body. In general, it is
quite difficult to calculate the exact volume. But in many cases, it suffices
to have an approximate value. Volume estimation methods for convex bodies have
been extensively studied in theory, however, there is still a lack of practical
implementations of such methods. In this paper, we present an efficient method
which is based on the Multiphase Monte-Carlo algorithm to estimate volumes of
convex polytopes. It uses the coordinate directions hit-and-run method, and
employs a technique of reutilizing sample points. The experiments show that our
method can efficiently handle instances with dozens of dimensions with high
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0131</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0131</id><created>2013-12-31</created><authors><author><keyname>Bhute</keyname><forenames>Avinash N</forenames></author><author><keyname>Meshram</keyname><forenames>B B</forenames></author></authors><title>System Analysis And Design For Multimedia Retrieval Systems</title><categories>cs.IR cs.CV cs.MM</categories><comments>20 pages, 12 Figures. arXiv admin note: substantial text overlap with
  arXiv:1211.4683</comments><journal-ref>The International Journal of Multimedia &amp; Its Applications (IJMA)
  Vol.5, No.6, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the extensive use of information technology and the recent
developments in multimedia systems, the amount of multimedia data available to
users has increased exponentially. Video is an example of multimedia data as it
contains several kinds of data such as text, image, meta-data, visual and
audio. Content based video retrieval is an approach for facilitating the
searching and browsing of large multimedia collections over WWW. In order to
create an effective video retrieval system, visual perception must be taken
into account. We conjectured that a technique which employs multiple features
for indexing and retrieval would be more effective in the discrimination and
search tasks of videos. In order to validate this, content based indexing and
retrieval systems were implemented using color histogram, Texture feature
(GLCM), edge density and motion..
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0137</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0137</id><created>2013-12-31</created><authors><author><keyname>Bhattacharya</keyname><forenames>Uttam</forenames></author><author><keyname>Rahut</keyname><forenames>Amit Kumar</forenames></author><author><keyname>De</keyname><forenames>Sujoy</forenames></author></authors><title>Audit Maturity Model</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today it is crucial for organizations to pay even greater attention on
quality management as the importance of this function in achieving ultimate
business objectives is increasingly becoming clearer. Importance of the Quality
Management Function in achieving basic need by ensuring compliance with
Capability Maturity Model Integrated or International Organization for
Standardization is a basic demand from business nowadays. However, Quality
Management Function and its processes need to be made much more mature to
prevent delivery outages and to achieve business excellence through their
review and auditing capability. Many organizations now face challenges in
determining the maturity of the Quality Management group along with the service
offered by them and the right way to elevate the maturity of the same. The
objective of this whitepaper is to propose a new model, the Audit Maturity
Model which will provide organizations with a measure of their maturity in
quality management in the perspective of auditing, along with recommendations
for preventing delivery outage, and identifying risk to achieve business
excellence. This will enable organizations to assess Quality Management
maturity higher than basic hygiene and will also help them to identify gaps and
to take corrective actions for achieving higher maturity levels. Hence the
objective is to envisage a new auditing model as a part of organisation quality
management function which can be a guide for them to achieve higher level of
maturity and ultimately help to achieve delivery and business excellence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0157</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0157</id><created>2013-12-31</created><updated>2014-06-18</updated><authors><author><keyname>Brzozowski</keyname><forenames>Janusz</forenames></author><author><keyname>Szyku&#x142;a</keyname><forenames>Marek</forenames></author></authors><title>Large Aperiodic Semigroups</title><categories>cs.FL</categories><comments>22 pages, 1 figure, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The syntactic complexity of a regular language is the size of its syntactic
semigroup. This semigroup is isomorphic to the transition semigroup of the
minimal deterministic finite automaton accepting the language, that is, to the
semigroup generated by transformations induced by non-empty words on the set of
states of the automaton. In this paper we search for the largest syntactic
semigroup of a star-free language having $n$ left quotients; equivalently, we
look for the largest transition semigroup of an aperiodic finite automaton with
$n$ states.
  We introduce two new aperiodic transition semigroups. The first is generated
by transformations that change only one state; we call such transformations and
resulting semigroups unitary. In particular, we study complete unitary
semigroups which have a special structure, and we show that each maximal
unitary semigroup is complete. For $n \ge 4$ there exists a complete unitary
semigroup that is larger than any aperiodic semigroup known to date.
  We then present even larger aperiodic semigroups, generated by
transformations that map a non-empty subset of states to a single state; we
call such transformations and semigroups semiconstant. In particular, we
examine semiconstant tree semigroups which have a structure based on full
binary trees. The semiconstant tree semigroups are at present the best
candidates for largest aperiodic semigroups.
  We also prove that $2^n-1$ is an upper bound on the state complexity of
reversal of star-free languages, and resolve an open problem about a special
case of state complexity of concatenation of star-free languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0159</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0159</id><created>2013-12-31</created><authors><author><keyname>Zibulevsky</keyname><forenames>Michael</forenames></author></authors><title>Speeding-Up Convergence via Sequential Subspace Optimization: Current
  State and Future Directions</title><categories>cs.NA cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is an overview paper written in style of research proposal. In recent
years we introduced a general framework for large-scale unconstrained
optimization -- Sequential Subspace Optimization (SESOP) and demonstrated its
usefulness for sparsity-based signal/image denoising, deconvolution,
compressive sensing, computed tomography, diffraction imaging, support vector
machines. We explored its combination with Parallel Coordinate Descent and
Separable Surrogate Function methods, obtaining state of the art results in
above-mentioned areas. There are several methods, that are faster than plain
SESOP under specific conditions: Trust region Newton method - for problems with
easily invertible Hessian matrix; Truncated Newton method - when fast
multiplication by Hessian is available; Stochastic optimization methods - for
problems with large stochastic-type data; Multigrid methods - for problems with
nested multilevel structure. Each of these methods can be further improved by
merge with SESOP. One can also accelerate Augmented Lagrangian method for
constrained optimization problems and Alternating Direction Method of
Multipliers for problems with separable objective function and non-separable
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0163</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0163</id><created>2013-12-31</created><authors><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>Radoszewski</keyname><forenames>Jakub</forenames></author><author><keyname>Rytter</keyname><forenames>Wojciech</forenames></author><author><keyname>Pissis</keyname><forenames>Solon P.</forenames></author><author><keyname>Wale&#x144;</keyname><forenames>Tomasz</forenames></author></authors><title>Fast Algorithm for Partial Covers in Words</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A factor $u$ of a word $w$ is a cover of $w$ if every position in $w$ lies
within some occurrence of $u$ in $w$. A word $w$ covered by $u$ thus
generalizes the idea of a repetition, that is, a word composed of exact
concatenations of $u$. In this article we introduce a new notion of
$\alpha$-partial cover, which can be viewed as a relaxed variant of cover, that
is, a factor covering at least $\alpha$ positions in $w$. We develop a data
structure of $O(n)$ size (where $n=|w|$) that can be constructed in $O(n\log
n)$ time which we apply to compute all shortest $\alpha$-partial covers for a
given $\alpha$. We also employ it for an $O(n\log n)$-time algorithm computing
a shortest $\alpha$-partial cover for each $\alpha=1,2,\ldots,n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0166</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0166</id><created>2013-12-31</created><authors><author><keyname>James</keyname><forenames>A. P.</forenames></author><author><keyname>Dasarathy</keyname><forenames>B. V.</forenames></author></authors><title>Medical Image Fusion: A survey of the state of the art</title><categories>cs.CV cs.AI physics.med-ph</categories><comments>Information Fusion, 2014</comments><doi>10.1016/j.inffus.2013.12.002</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Medical image fusion is the process of registering and combining multiple
images from single or multiple imaging modalities to improve the imaging
quality and reduce randomness and redundancy in order to increase the clinical
applicability of medical images for diagnosis and assessment of medical
problems. Multi-modal medical image fusion algorithms and devices have shown
notable achievements in improving clinical accuracy of decisions based on
medical images. This review article provides a factual listing of methods and
summarizes the broad scientific challenges faced in the field of medical image
fusion. We characterize the medical image fusion research based on (1) the
widely used image fusion methods, (2) imaging modalities, and (3) imaging of
organs that are under study. This review concludes that even though there
exists several open ended technological and scientific challenges, the fusion
of medical images has proved to be useful for advancing the clinical
reliability of using medical imaging for medical diagnostics and analysis, and
is a scientific discipline that has the potential to significantly grow in the
coming years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0174</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0174</id><created>2013-12-31</created><updated>2014-06-15</updated><authors><author><keyname>Bhattiprolu</keyname><forenames>Vijay V. S. P.</forenames></author><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author></authors><title>Separating a Voronoi Diagram via Local Search</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $\mathsf{P}$ of $n$ points in $\mathbb{R}^d$, we show how to
insert a set $\mathsf{X}$ of $O( n^{1-1/d} )$ additional points, such that
$\mathsf{P}$ can be broken into two sets $\mathsf{P}_1$ and $\mathsf{P}_2$, of
roughly equal size, such that in the Voronoi diagram $\mathcal{V}( \mathsf{P}
\cup \mathsf{X} )$, the cells of $\mathsf{P}_1$ do not touch the cells of
$\mathsf{P}_2$; that is, $\mathsf{X}$ separates $\mathsf{P}_1$ from
$\mathsf{P}_2$ in the Voronoi diagram.
  Given such a partition $(\mathsf{P}_1,\mathsf{P}_2)$ of $\mathsf{P}$, we
present approximation algorithms to compute the minimum size separator
realizing this partition.
  Finally, we present a simple local search algorithm that is a PTAS for
geometric hitting set of fat objects (which can also be used to approximate the
optimal Voronoi partition).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0180</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0180</id><created>2013-12-31</created><authors><author><keyname>N'Guyen</keyname><forenames>Steve</forenames><affiliation>ISIR, LPPA</affiliation></author><author><keyname>Moulin-Frier</keyname><forenames>Cl&#xe9;ment</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, GIPSA-lab</affiliation></author><author><keyname>Droulez</keyname><forenames>Jacques</forenames><affiliation>LPPA</affiliation></author></authors><title>Decision Making under Uncertainty: A Quasimetric Approach</title><categories>cs.AI math.OC</categories><proxy>ccsd</proxy><journal-ref>PLoS ONE 8, 12 (2013) e83411</journal-ref><doi>10.1371/journal.pone.0083411</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new approach for solving a class of discrete decision making
problems under uncertainty with positive cost. This issue concerns multiple and
diverse fields such as engineering, economics, artificial intelligence,
cognitive science and many others. Basically, an agent has to choose a single
or series of actions from a set of options, without knowing for sure their
consequences. Schematically, two main approaches have been followed: either the
agent learns which option is the correct one to choose in a given situation by
trial and error, or the agent already has some knowledge on the possible
consequences of his decisions; this knowledge being generally expressed as a
conditional probability distribution. In the latter case, several optimal or
suboptimal methods have been proposed to exploit this uncertain knowledge in
various contexts. In this work, we propose following a different approach,
based on the geometric intuition of distance. More precisely, we define a goal
independent quasimetric structure on the state space, taking into account both
cost function and transition probability. We then compare precision and
computation time with classical approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0189</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0189</id><created>2013-12-31</created><authors><author><keyname>Chillara</keyname><forenames>Suryajith</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Partha</forenames></author></authors><title>On the Limits of Depth Reduction at Depth 3 Over Small Finite Fields</title><categories>cs.CC</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recently, Gupta et.al. [GKKS2013] proved that over Q any $n^{O(1)}$-variate
and $n$-degree polynomial in VP can also be computed by a depth three
$\Sigma\Pi\Sigma$ circuit of size $2^{O(\sqrt{n}\log^{3/2}n)}$. Over fixed-size
finite fields, Grigoriev and Karpinski proved that any $\Sigma\Pi\Sigma$
circuit that computes $Det_n$ (or $Perm_n$) must be of size $2^{\Omega(n)}$
[GK1998]. In this paper, we prove that over fixed-size finite fields, any
$\Sigma\Pi\Sigma$ circuit for computing the iterated matrix multiplication
polynomial of $n$ generic matrices of size $n\times n$, must be of size
$2^{\Omega(n\log n)}$. The importance of this result is that over fixed-size
fields there is no depth reduction technique that can be used to compute all
the $n^{O(1)}$-variate and $n$-degree polynomials in VP by depth 3 circuits of
size $2^{o(n\log n)}$. The result [GK1998] can only rule out such a possibility
for depth 3 circuits of size $2^{o(n)}$.
  We also give an example of an explicit polynomial ($NW_{n,\epsilon}(X)$) in
VNP (not known to be in VP), for which any $\Sigma\Pi\Sigma$ circuit computing
it (over fixed-size fields) must be of size $2^{\Omega(n\log n)}$. The
polynomial we consider is constructed from the combinatorial design. An
interesting feature of this result is that we get the first examples of two
polynomials (one in VP and one in VNP) such that they have provably stronger
circuit size lower bounds than Permanent in a reasonably strong model of
computation.
  Next, we prove that any depth 4
$\Sigma\Pi^{[O(\sqrt{n})]}\Sigma\Pi^{[\sqrt{n}]}$ circuit computing
$NW_{n,\epsilon}(X)$ (over any field) must be of size $2^{\Omega(\sqrt{n}\log
n)}$. To the best of our knowledge, the polynomial $NW_{n,\epsilon}(X)$ is the
first example of an explicit polynomial in VNP such that it requires
$2^{\Omega(\sqrt{n}\log n)}$ size depth four circuits, but no known matching
upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0190</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0190</id><created>2013-12-31</created><authors><author><keyname>Adimurthi</keyname><forenames>Adi</forenames><affiliation>TIFR-CAM</affiliation></author><author><keyname>Gowda</keyname><forenames>G. D. Veerappa</forenames><affiliation>TIFR-CAM</affiliation></author><author><keyname>Jaffr&#xe9;</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>Inria Paris-Rocquencourt</affiliation></author></authors><title>The DFLU flux for systems of conservation laws</title><categories>cs.NA</categories><comments>This paper is published in the Journal of Computational and Applied
  Mathematics 247 (2013) 102-123. arXiv admin note: substantial text overlap
  with arXiv:0908.0320</comments><proxy>ccsd</proxy><report-no>RR-8442</report-no><journal-ref>N&amp;deg; RR-8442 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The DFLU numerical flux was introduced in order to solve hyperbolic scalar
conservation laws with a flux function discontinuous in space. We show how this
flux can be used to solve certain class of systems of conservation laws such as
systems modeling polymer flooding in oil reservoir engineering. Furthermore,
these results are extended to the case where the flux function is discontinuous
in the space variable. Such a situation arises for example while dealing with
oil reservoirs which are heterogeneous. Numerical experiments are presented to
illustrate the efficiency of this new scheme compared to other standard schemes
like upstream mobility, Lax-Friedrichs and Force schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0191</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0191</id><created>2013-12-31</created><authors><author><keyname>Cornu</keyname><forenames>Benoit</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Seinturier</keyname><forenames>Lionel</forenames><affiliation>INRIA Lille - Nord Europe, LIFL, IUF</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Reasoning and Improving on Software Resilience against Unanticipated
  Exceptions</title><categories>cs.SE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In software, there are the errors anticipated at specification and design
time, those encountered at development and testing time, and those that happen
in production mode yet never anticipated. In this paper, we aim at reasoning on
the ability of software to correctly handle unanticipated exceptions. We
propose an algorithm, called short-circuit testing, which injects exceptions
during test suite execution so as to simulate unanticipated errors. This
algorithm collects data that is used as input for verifying two formal
exception contracts that capture two resilience properties. Our evaluation on 9
test suites, with 78% line coverage in average, analyzes 241 executed catch
blocks, shows that 101 of them expose resilience properties and that 84 can be
transformed to be more resilient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0193</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0193</id><created>2013-12-31</created><authors><author><keyname>Knabner</keyname><forenames>Peter</forenames><affiliation>Inria Paris-Rocquencourt</affiliation></author><author><keyname>Roberts</keyname><forenames>Jean</forenames><affiliation>Inria Paris-Rocquencourt</affiliation></author></authors><title>Mathematical analysis of a discrete fracture model coupling Darcy flow
  in the matrix with Darcy-Forchheimer flow in the fracture</title><categories>cs.NA</categories><proxy>ccsd</proxy><report-no>RR-8443</report-no><journal-ref>N&amp;deg; RR-8443 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a model for flow in a porous medium with a fracture in which the
flow in the fracture is governed by the Darcy-Forchheimer law while that in the
surrounding matrix is governed by Darcy's law. We give an appropriate mixed,
variational formulation and show existence and uniqueness of the solution. To
show existence we give an analogous formulation for the model in which the
Darcy-Forchheimer law is the governing equation throughout the domain. We show
existence and uniqueness of the solution and show that the solution for the
model with Darcy's law in the matrix is the weak limit of solutions of the
model with the Darcy-Forchheimer law in the entire domain when the Forchheimer
coefficient in the matrix tends toward zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0200</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0200</id><created>2013-12-31</created><updated>2014-09-03</updated><authors><author><keyname>Medhi</keyname><forenames>Nabajyoti</forenames></author><author><keyname>Pal</keyname><forenames>Manjish</forenames></author></authors><title>Sixsoid: A new paradigm for $k$-coverage in 3D Wireless Sensor Networks</title><categories>cs.NI</categories><acm-class>C.2; F.2.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coverage in 3D wireless sensor network (WSN) is always a very critical issue
to deal with. Coming up with good coverage models implies more energy efficient
networks. $K$-coverage is one model that ensures that every point in a given 3D
Field of Interest (FoI) is guaranteed to be covered by $k$ sensors. When it
comes to 3D, coming up with a deployment of sensors that gurantees $k$-coverage
becomes much more complicated than in 2D. The basic idea is to come up with a
geometrical shape that is guaranteed to be $k$-covered by taking a specific
arrangement of sensors, and then fill the FoI will non-overlapping copies of
this shape. In this work, we propose a new shape for the 3D scenario which we
call a \textbf{Devilsoid}. Prior to this work, the shape which was proposed for
coverage in 3D was the so called \textbf{Reuleaux Tetrahedron}. Our
construction is motivated from a construction that can be applied to the 2D
version of the problem \cite{MS} in which it imples better guarantees over the
\textbf{Reuleaux Triangle}. Our contribution in this paper is twofold, firstly
we show how Devilsoid gurantees more coverage volume over Reuleaux Tetrahedron,
secondly we show how Devilsoid also guarantees simpler and more pragmatic
deployment strategy for 3D wireless sensor networks. In this paper, we show the
constuction of Devilsoid, calculate its volume and discuss its effect on the
$k$-coverage in WSN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0201</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0201</id><created>2013-12-31</created><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author><author><keyname>Zhang</keyname><forenames>Cun-Hui</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Sparse Recovery with Very Sparse Compressed Counting</title><categories>stat.ME cs.DS cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing (sparse signal recovery) often encounters nonnegative data
(e.g., images). Recently we developed the methodology of using (dense)
Compressed Counting for recovering nonnegative K-sparse signals. In this paper,
we adopt very sparse Compressed Counting for nonnegative signal recovery. Our
design matrix is sampled from a maximally-skewed p-stable distribution (0&lt;p&lt;1),
and we sparsify the design matrix so that on average (1-g)-fraction of the
entries become zero. The idea is related to very sparse stable random
projections (Li et al 2006 and Li 2007), the prior work for estimating summary
statistics of the data.
  In our theoretical analysis, we show that, when p-&gt;0, it suffices to use M=
K/(1-exp(-gK) log N measurements, so that all coordinates can be recovered in
one scan of the coordinates. If g = 1 (i.e., dense design), then M = K log N.
If g= 1/K or 2/K (i.e., very sparse design), then M = 1.58K log N or M = 1.16K
log N. This means the design matrix can be indeed very sparse at only a minor
inflation of the sample complexity.
  Interestingly, as p-&gt;1, the required number of measurements is essentially M
= 2.7K log N, provided g= 1/K. It turns out that this result is a general
worst-case bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0202</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0202</id><created>2013-12-31</created><authors><author><keyname>Lamperski</keyname><forenames>Andrew</forenames></author><author><keyname>Cowan</keyname><forenames>Noah J.</forenames></author></authors><title>Optimal Control with Noisy Time</title><categories>math.OC cs.SY</categories><comments>Submitted to IEEE TAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines stochastic optimal control problems in which the state is
perfectly known, but the controller's measure of time is a stochastic process
derived from a strictly increasing L\'evy process. We provide dynamic
programming results for continuous-time finite-horizon control and specialize
these results to solve a noisy-time variant of the linear quadratic regulator
problem and a portfolio optimization problem with random trade activity rates.
For the linear quadratic case, the optimal controller is linear and can be
computed from a generalization of the classical Riccati differential equation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0207</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0207</id><created>2013-12-31</created><updated>2014-01-06</updated><authors><author><keyname>Wilkerson</keyname><forenames>Galen</forenames></author><author><keyname>Khalili</keyname><forenames>Ramin</forenames></author><author><keyname>Schmid</keyname><forenames>Stefan</forenames></author></authors><title>Urban Mobility Scaling: Lessons from `Little Data'</title><categories>physics.soc-ph cs.CY cs.SI physics.data-an stat.AP</categories><comments>6 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent mobility scaling research, using new data sources, often relies on
aggregated data alone. Hence, these studies face difficulties characterizing
the influence of factors such as transportation mode on mobility patterns. This
paper attempts to complement this research by looking at a category-rich
mobility data set. In order to shed light on the impact of categories, as a
case study, we use conventionally collected German mobility data. In contrast
to `check-in'-based data, our results are not biased by Euclidean distance
approximations. In our analysis, we show that aggregation can hide crucial
differences between trip length distributions, when subdivided by categories.
For example, we see that on an urban scale (0 to ~15 km), walking, versus
driving, exhibits a highly different scaling exponent, thus universality class.
Moreover, mode share and trip length are responsive to day-of-week and
time-of-day. For example, in Germany, although driving is relatively less
frequent on Sundays than on Wednesdays, trips seem to be longer. In addition,
our work may shed new light on the debate between distance-based and
intervening-opportunity mechanisms affecting mobility patterns, since mode may
be chosen both according to trip length and urban form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0209</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0209</id><created>2013-12-31</created><updated>2015-12-19</updated><authors><author><keyname>Liu</keyname><forenames>Chang</forenames></author><author><keyname>Sitaraman</keyname><forenames>Ramesh K.</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Go-With-The-Winner: Client-Side Server Selection for Content Delivery</title><categories>cs.NI</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content delivery networks deliver much of the web and video content in the
world by deploying a large distributed network of servers. We model and analyze
a simple paradigm for client-side server selection that is commonly used in
practice where each user independently measures the performance of a set of
candidate servers and selects the one that performs the best. For web (resp.,
video) delivery, we propose and analyze a simple algorithm where each user
randomly chooses two or more candidate servers and selects the server that
provided the best hitrate (resp., bitrate). We prove that the algorithm
converges quickly to an optimal state where all users receive the best hitrate
(resp., bitrate), with high probability. We also show that if each user chose
just one random server instead of two, some users receive a hitrate (resp.,
bitrate) that tends to zero. We simulate our algorithm and evaluate its
performance with varying choices of parameters, system load, and content
popularity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0214</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0214</id><created>2013-12-31</created><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Sultan</keyname><forenames>Ahmed</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author></authors><title>Band Allocation for Cognitive Radios with Buffered Primary and Secondary
  Users</title><categories>cs.NI cs.IT math.IT</categories><comments>Accepted in WCNC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study band allocation of $\mathcal{M}_s$ buffered secondary
users (SUs) to $\mathcal{M}_p$ orthogonal primary licensed bands, where each
primary band is assigned to one primary user (PU). Each SU is assigned to one
of the available primary bands with a certain probability designed to satisfy
some specified quality of service (QoS) requirements for the SUs. In the
proposed system, only one SU is assigned to a particular band. The optimization
problem used to obtain the stability region's envelope (closure) is shown to be
a linear program. We compare the stability region of the proposed system with
that of a system where each SU chooses a band randomly with some assignment
probability. We also compare with a fixed (deterministic) assignment system,
where only one SU is assigned to one of the primary bands all the time. We
prove the advantage of the proposed system over the other systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0223</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0223</id><created>2013-12-31</created><authors><author><keyname>Redlich</keyname><forenames>Amanda</forenames></author></authors><title>Unbalanced Allocations</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the unbalanced allocation of $m$ balls into $n$ bins by a
randomized algorithm using the &quot;power of two choices&quot;. For each ball, we select
a set of bins at random, then place the ball in the fullest bin within the set.
Applications of this generic algorithm range from cost minimization to
condensed matter physics. In this paper, we analyze the distribution of the bin
loads produced by this algorithm, considering, for example, largest and
smallest loads, loads of subsets of the bins, and the likelihood of bins having
equal loads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0224</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0224</id><created>2013-12-31</created><authors><author><keyname>Lindzey</keyname><forenames>Nathan</forenames></author><author><keyname>McConnell</keyname><forenames>Ross M.</forenames></author></authors><title>Linear-Time Algorithms for Finding Tucker Submatrices and
  Lekkerkerker-Boland Subgraphs</title><categories>cs.DM cs.DS math.CO</categories><comments>A preliminary version of this work appeared in WG13: 39th
  International Workshop on Graph-Theoretic Concepts in Computer Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lekkerkerker and Boland characterized the minimal forbidden induced subgraphs
for the class of interval graphs. We give a linear-time algorithm to find one
in any graph that is not an interval graph. Tucker characterized the minimal
forbidden submatrices of binary matrices that do not have the consecutive-ones
property. We give a linear-time algorithm to find one in any binary matrix that
does not have the consecutive-ones property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0245</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0245</id><created>2013-12-31</created><authors><author><keyname>Gath</keyname><forenames>S. J</forenames></author><author><keyname>Kulkarni</keyname><forenames>R. V</forenames></author></authors><title>A Review: Expert System for Diagnosis of Myocardial Infarction</title><categories>cs.AI</categories><comments>7 pages. arXiv admin note: text overlap with arXiv:1006.4544 by other
  authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A computer Program Capable of performing at a human-expert level in a narrow
problem domain area is called an expert system. Management of uncertainty is an
intrinsically important issue in the design of expert systems because much of
the information in the knowledge base of a typical expert system is imprecise,
incomplete or not totally reliable. In this paper, the author present s the
review of past work that has been carried out by various researchers based on
development of expert systems for the diagnosis of cardiac disease
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0247</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0247</id><created>2013-12-31</created><updated>2014-07-12</updated><authors><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author><author><keyname>Gupta</keyname><forenames>Pramod</forenames></author></authors><title>Robust Hierarchical Clustering</title><categories>cs.LG cs.DS</categories><comments>37 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most widely used techniques for data clustering is agglomerative
clustering. Such algorithms have been long used across many different fields
ranging from computational biology to social sciences to computer vision in
part because their output is easy to interpret. Unfortunately, it is well
known, however, that many of the classic agglomerative clustering algorithms
are not robust to noise. In this paper we propose and analyze a new robust
algorithm for bottom-up agglomerative clustering. We show that our algorithm
can be used to cluster accurately in cases where the data satisfies a number of
natural properties and where the traditional agglomerative algorithms fail. We
also show how to adapt our algorithm to the inductive setting where our given
data is only a small random sample of the entire data set. Experimental
evaluations on synthetic and real world data sets show that our algorithm
achieves better performance than other hierarchical algorithms in the presence
of noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0248</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0248</id><created>2013-12-31</created><updated>2014-01-02</updated><authors><author><keyname>Latkin</keyname><forenames>Evgeny</forenames></author></authors><title>Twofold fast summation</title><categories>cs.NA</categories><comments>All used tests and testing results available at author's Web site:
  https://sites.google.com/site/yevgenylatkin/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Debugging accumulation of floating-point errors is hard; ideally, computer
should track it automatically. Here we consider twofold approximation of an
exact real with value + error pair of floating-point numbers. Normally, value +
error sum is more accurate than value alone, so error can estimate deviation
between value and its exact target. Fast summation algorithm, that provides
twofold sum of x[1]+...+x[N] or dot product x[1]*y[1]+...+x[N]*y[N], can be
same fast as direct summation sometimes if leveraging processor underused
potential. This way, we can hit three goals: improve precision, track
inaccuracy, and do this with little if any loss in performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0255</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0255</id><created>2014-01-01</created><authors><author><keyname>Govindaraj</keyname><forenames>Dinesh</forenames></author><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Vishwanathan</keyname><forenames>S. V. N.</forenames></author></authors><title>Modeling Attractiveness and Multiple Clicks in Sponsored Search Results</title><categories>cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Click models are an important tool for leveraging user feedback, and are used
by commercial search engines for surfacing relevant search results. However,
existing click models are lacking in two aspects. First, they do not share
information across search results when computing attractiveness. Second, they
assume that users interact with the search results sequentially. Based on our
analysis of the click logs of a commercial search engine, we observe that the
sequential scan assumption does not always hold, especially for sponsored
search results. To overcome the above two limitations, we propose a new click
model. Our key insight is that sharing information across search results helps
in identifying important words or key-phrases which can then be used to
accurately compute attractiveness of a search result. Furthermore, we argue
that the click probability of a position as well as its attractiveness changes
during a user session and depends on the user's past click experience. Our
model seamlessly incorporates the effect of externalities (quality of other
search results displayed in response to a user query), user fatigue, as well as
pre and post-click relevance of a sponsored search result. We propose an
efficient one-pass inference scheme and empirically evaluate the performance of
our model via extensive experiments using the click logs of a large commercial
search engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0260</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0260</id><created>2014-01-01</created><updated>2014-06-16</updated><authors><author><keyname>Nardelli</keyname><forenames>Pedro H. J.</forenames></author><author><keyname>Rubido</keyname><forenames>Nicolas</forenames></author><author><keyname>Wang</keyname><forenames>Chengwei</forenames></author><author><keyname>Baptista</keyname><forenames>Murilo S.</forenames></author><author><keyname>Pomalaza-Raez</keyname><forenames>Carlos</forenames></author><author><keyname>Cardieri</keyname><forenames>Paulo</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>Models for the modern power grid</title><categories>cs.SY</categories><comments>Submitted to EPJ-ST Power Grids, May 2015</comments><doi>10.1140/epjst/e2014-02219-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article reviews different kinds of models for the electric power grid
that can be used to understand the modern power system, the smart grid. From
the physical network to abstract energy markets, we identify in the literature
different aspects that co-determine the spatio-temporal multilayer dynamics of
power system. We start our review by showing how the generation, transmission
and distribution characteristics of the traditional power grids are already
subject to complex behaviour appearing as a result of the the interplay between
dynamics of the nodes and topology, namely synchronisation and cascade effects.
When dealing with smart grids, the system complexity increases even more: on
top of the physical network of power lines and controllable sources of
electricity, the modernisation brings information networks, renewable
intermittent generation, market liberalisation, prosumers, among other aspects.
In this case, we forecast a dynamical co-evolution of the smart grid and other
kind of networked systems that cannot be understood isolated. This review
compiles recent results that model electric power grids as complex systems,
going beyond pure technological aspects. From this perspective, we then
indicate possible ways to incorporate the diverse co-evolving systems into the
smart grid model using, for example, network theory and multi-agent simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0279</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0279</id><created>2014-01-01</created><authors><author><keyname>Dimitrov</keyname><forenames>Darko</forenames></author></authors><title>On structural properties of trees with minimal atom-bond connectivity
  index</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The {\em atom-bond connectivity (ABC) index} is a degree-based molecular
descriptor, that found chemical applications. It is well known that among all
connected graphs, the graphs with minimal ABC index are trees. A complete
characterization of trees with minimal $ABC$ index is still an open problem. In
this paper, we present new structural properties of trees with minimal ABC
index. Our main results reveal that trees with minimal ABC index do not contain
so-called {\em $B_k$-branches}, with $k \geq 5$, and that they do not have more
than four $B_4$-branches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0282</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0282</id><created>2014-01-01</created><authors><author><keyname>Nourjou</keyname><forenames>Reza</forenames></author><author><keyname>Hatayama</keyname><forenames>Michinori</forenames></author><author><keyname>Smith</keyname><forenames>Stephen F.</forenames></author><author><keyname>Sadeghi</keyname><forenames>Atabak</forenames></author><author><keyname>Szekely</keyname><forenames>Pedro</forenames></author></authors><title>Design of a GIS-based Assistant Software Agent for the Incident
  Commander to Coordinate Emergency Response Operations</title><categories>cs.MA cs.AI</categories><comments>3 pages, 1 figure, In Workshop on Robots and Sensors integration in
  future rescue INformation system (ROSIN' 13). In Conjunction of the IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS' 13), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Problem: This paper addresses the design of an intelligent software system
for the IC (incident commander) of a team in order to coordinate actions of
agents (field units or robots) in the domain of emergency/crisis response
operations. Objective: This paper proposes GICoordinator. It is a GIS-based
assistant software agent that assists and collaborates with the human planner
in strategic planning and macro tasks assignment for centralized multi-agent
coordination. Method: Our approach to design GICoordinator was to: analyze the
problem, design a complete data model, design an architecture of GICoordinator,
specify required capabilities of human and system in coordination problem
solving, specify development tools, and deploy. Result: The result was an
architecture/design of GICoordinator that contains system requirements.
Findings: GICoordinator efficiently integrates geoinformatics with artifice
intelligent techniques in order to provide a spatial intelligent coordinator
system for an IC to efficiently coordinate and control agents by making
macro/strategic decisions. Results define a framework for future works to
develop this system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0294</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0294</id><created>2014-01-01</created><authors><author><keyname>Levit</keyname><forenames>Vadim E.</forenames></author><author><keyname>Tankus</keyname><forenames>David</forenames></author></authors><title>Complexity results on w-well-covered graphs</title><categories>cs.DM math.CO</categories><comments>17 pages, 2 figures, 1 table. arXiv admin note: text overlap with
  arXiv:1312.7563</comments><msc-class>05C69 (Primary) 05C85 (Secondary)</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph G is well-covered if all its maximal independent sets are of the same
cardinality. Assume that a weight function w is defined on its vertices. Then G
is w-well-covered if all maximal independent sets are of the same weight.
  For every graph G, the set of weight functions w such that G is
w-well-covered is a vector space, denoted WCW(G). Let B be a complete bipartite
induced subgraph of G on vertex sets of bipartition B_X and B_Y. Then B is
generating if there exists an independent set S such that S \cup B_X and S \cup
B_Y are both maximal independent sets of G. A relating edge is a generating
subgraph in the restricted case that B = K_{1,1}.
  Deciding whether an input graph G is well-covered is co-NP-complete.
Therefore finding WCW(G) is co-NP-hard. Deciding whether an edge is relating is
co-NP-complete. Therefore, deciding whether a subgraph is generating is
co-NP-complete as well.
  In this article we discuss the connections among these problems, provide
proofs for NP-completeness for several restricted cases, and present polynomial
characterizations for some other cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0304</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0304</id><created>2014-01-01</created><updated>2014-10-22</updated><authors><author><keyname>Mendelson</keyname><forenames>Shahar</forenames></author></authors><title>Learning without Concentration</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We obtain sharp bounds on the performance of Empirical Risk Minimization
performed in a convex class and with respect to the squared loss, without
assuming that class members and the target are bounded functions or have
rapidly decaying tails.
  Rather than resorting to a concentration-based argument, the method used here
relies on a `small-ball' assumption and thus holds for classes consisting of
heavy-tailed functions and for heavy-tailed targets.
  The resulting estimates scale correctly with the `noise level' of the
problem, and when applied to the classical, bounded scenario, always improve
the known bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0323</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0323</id><created>2014-01-01</created><updated>2014-01-02</updated><authors><author><keyname>Wang</keyname><forenames>Tian</forenames></author><author><keyname>Krim</keyname><forenames>Hamid</forenames></author><author><keyname>Viniotis</keyname><forenames>Yannis</forenames></author></authors><title>Analysis and Control of Beliefs in Social Networks</title><categories>cs.SI physics.soc-ph</categories><doi>10.1109/TSP.2014.2352591</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of how beliefs diffuse among
members of social networks. We propose an information flow model (IFM) of
belief that captures how interactions among members affect the diffusion and
eventual convergence of a belief. The IFM model includes a generalized Markov
Graph (GMG) model as a social network model, which reveals that the diffusion
of beliefs depends heavily on two characteristics of the social network
characteristics, namely degree centralities and clustering coefficients. We
apply the IFM to both converged belief estimation and belief control strategy
optimization. The model is compared with an IFM including the Barabasi-Albert
model, and is evaluated via experiments with published real social network
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0336</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0336</id><created>2014-01-01</created><authors><author><keyname>Cummings</keyname><forenames>Rachel</forenames></author><author><keyname>Echenique</keyname><forenames>Federico</forenames></author><author><keyname>Wierman</keyname><forenames>Adam</forenames></author></authors><title>The Empirical Implications of Privacy-Aware Choice</title><categories>cs.GT</categories><msc-class>91</msc-class><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper initiates the study of the testable implications of choice data in
settings where agents have privacy preferences. We adapt the standard
conceptualization of consumer choice theory to a situation where the consumer
is aware of, and has preferences over, the information revealed by her choices.
The main message of the paper is that little can be inferred about consumers'
preferences once we introduce the possibility that the consumer has concerns
about privacy. This holds even when consumers' privacy preferences are assumed
to be monotonic and separable. This motivates the consideration of stronger
assumptions and, to that end, we introduce an additive model for privacy
preferences that does have testable implications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0340</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0340</id><created>2014-01-01</created><updated>2014-04-27</updated><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author></authors><title>Optimal Random Access and Random Spectrum Sensing for an Energy
  Harvesting Cognitive Radio with and without Primary Feedback Leveraging</title><categories>cs.IT cs.NI math.IT</categories><comments>ACCEPTED in EAI Endorsed Transactions on Cognitive Communications.
  arXiv admin note: substantial text overlap with arXiv:1208.5659</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a secondary user (SU) with energy harvesting capability. We
design access schemes for the SU which incorporate random spectrum sensing and
random access, and which make use of the primary automatic repeat request (ARQ)
feedback. We study two problem-formulations. In the first problem-formulation,
we characterize the stability region of the proposed schemes. The sensing and
access probabilities are obtained such that the secondary throughput is
maximized under the constraints that both the primary and secondary queues are
stable. Whereas in the second problem-formulation, the sensing and access
probabilities are obtained such that the secondary throughput is maximized
under the stability of the primary queue and that the primary queueing delay is
kept lower than a specified value needed to guarantee a certain quality of
service (QoS) for the primary user (PU). We consider spectrum sensing errors
and assume multipacket reception (MPR) capabilities. Numerical results show the
enhanced performance of our proposed systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0347</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0347</id><created>2014-01-01</created><authors><author><keyname>Li</keyname><forenames>Peng</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Distributed Iterative Detection Based on Reduced Message Passing for
  Networked MIMO Cellular Systems</title><categories>cs.IT math.IT</categories><comments>9 pages, 6 figures. IEEE Transactions on Vehicular Technology, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers base station cooperation (BSC) strategies for the uplink
of a multi-user multi-cell high frequency reuse scenario where distributed
iterative detection (DID) schemes with soft/hard interference cancellation
algorithms are studied. The conventional distributed detection scheme exchanges
{soft symbol estimates} with all cooperating BSs. Since a large amount of
information needs to be shared via the backhaul, the exchange of hard bit
information is preferred, however a performance degradation is experienced. In
this paper, we consider a reduced message passing (RMP) technique in which each
BS generates a detection list with the probabilities for the desired symbol
that are sorted according to the calculated probability. The network then
selects the best {detection candidates} from the lists and conveys the index of
the constellation symbols (instead of double-precision values) among the
cooperating cells. The proposed DID-RMP achieves an inter-cell-interference
(ICI) suppression with low backhaul traffic overhead compared with {the
conventional soft bit exchange} and outperforms the previously reported
hard/soft information exchange algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0348</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0348</id><created>2014-01-01</created><updated>2014-02-12</updated><authors><author><keyname>Bitansky</keyname><forenames>Nir</forenames></author><author><keyname>Canetti</keyname><forenames>Ran</forenames></author><author><keyname>Cohn</keyname><forenames>Henry</forenames></author><author><keyname>Goldwasser</keyname><forenames>Shafi</forenames></author><author><keyname>Kalai</keyname><forenames>Yael Tauman</forenames></author><author><keyname>Paneth</keyname><forenames>Omer</forenames></author><author><keyname>Rosen</keyname><forenames>Alon</forenames></author></authors><title>The impossibility of obfuscation with auxiliary input or a universal
  simulator</title><categories>cs.CR</categories><comments>19 pages</comments><proxy>Henry Cohn</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that the existence of general indistinguishability
obfuscators conjectured in a few recent works implies, somewhat
counterintuitively, strong impossibility results for virtual black box
obfuscation. In particular, we show that indistinguishability obfuscation for
all circuits implies:
  * The impossibility of average-case virtual black box obfuscation with
auxiliary input for any circuit family with super-polynomial pseudo-entropy.
Such circuit families include all pseudo-random function families, and all
families of encryption algorithms and randomized digital signatures that
generate their required coin flips pseudo-randomly. Impossibility holds even
when the auxiliary input depends only on the public circuit family, and not the
specific circuit in the family being obfuscated.
  * The impossibility of average-case virtual black box obfuscation with a
universal simulator (with or without any auxiliary input) for any circuit
family with super-polynomial pseudo-entropy.
  These bounds significantly strengthen the impossibility results of Goldwasser
and Kalai (STOC 2005).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0355</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0355</id><created>2014-01-01</created><updated>2014-04-13</updated><authors><author><keyname>Fan</keyname><forenames>Liya</forenames></author><author><keyname>Gao</keyname><forenames>Bo</forenames></author><author><keyname>Sun</keyname><forenames>Xi</forenames></author><author><keyname>Zhang</keyname><forenames>Fa</forenames></author><author><keyname>Liu</keyname><forenames>Zhiyong</forenames></author></authors><title>Improving the Load Balance of MapReduce Operations based on the Key
  Distribution of Pairs</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Load balance is important for MapReduce to reduce job duration, increase
parallel efficiency, etc. Previous work focuses on coarse-grained scheduling.
This study concerns fine-grained scheduling on MapReduce operations. Each
operation represents one invocation of the Map or Reduce function. Scheduling
MapReduce operations is difficult due to highly screwed operation loads, no
support to collect workload statistics, and high complexity of the scheduling
problem. So current implementations adopt simple strategies, leading to poor
load balance. To address these difficulties, we design an algorithm to schedule
operations based on the key distribution of intermediate pairs. The algorithm
involves a sub-program for selecting operations for task slots, and we name it
the Balanced Subset Sum (BSS) problem. We discuss properties of BSS and design
exact and approximation algorithms for it. To transparently incorporate these
algorithms into MapReduce, we design a communication mechanism to collect
statistics, and a pipeline within Reduce tasks to increase resource
utilization. To the best of our knowledge, this is the first work on scheduling
MapReduce workload at this fine-grained level. Experiments on PUMA [T+12]
benchmarks show consistent performance improvement. The job duration can be
reduced by up to 37%, compared with standard MapReduce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0359</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0359</id><created>2014-01-01</created><updated>2015-10-27</updated><authors><author><keyname>Hescott</keyname><forenames>Benjamin</forenames></author><author><keyname>Malchik</keyname><forenames>Caleb</forenames></author><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author></authors><title>Tight Bounds for Active Self-Assembly Using an Insertion Primitive</title><categories>cs.FL</categories><comments>To appear in Algorithmica. An abstract (12-page) version of this
  paper appeared in the proceedings of ESA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove two tight bounds on the behavior of a model of self-assembling
particles introduced by Dabby and Chen (SODA 2013), called insertion systems,
where monomers insert themselves into the middle of a growing linear polymer.
First, we prove that the expressive power of these systems is equal to
context-free grammars, answering a question posed by Dabby and Chen. Second, we
prove that systems of $k$ monomer types can deterministically construct
polymers of length $n = 2^{\Theta(k^{3/2})}$ in $O(\log^{5/3}(n))$ expected
time, and that this is optimal in both the number of monomer types and expected
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0362</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0362</id><created>2014-01-01</created><updated>2015-07-13</updated><authors><author><keyname>Peng</keyname><forenames>Hao</forenames></author><author><keyname>Qi</keyname><forenames>Yuan</forenames></author></authors><title>EigenGP: Gaussian Process Models with Adaptive Eigenfunctions</title><categories>cs.LG</categories><comments>Accepted by IJCAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian processes (GPs) provide a nonparametric representation of functions.
However, classical GP inference suffers from high computational cost for big
data. In this paper, we propose a new Bayesian approach, EigenGP, that learns
both basis dictionary elements--eigenfunctions of a GP prior--and prior
precisions in a sparse finite model. It is well known that, among all
orthogonal basis functions, eigenfunctions can provide the most compact
representation. Unlike other sparse Bayesian finite models where the basis
function has a fixed form, our eigenfunctions live in a reproducing kernel
Hilbert space as a finite linear combination of kernel functions. We learn the
dictionary elements--eigenfunctions--and the prior precisions over these
elements as well as all the other hyperparameters from data by maximizing the
model marginal likelihood. We explore computational linear algebra to simplify
the gradient computation significantly. Our experimental results demonstrate
improved predictive performance of EigenGP over alternative sparse GP methods
as well as relevance vector machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0366</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0366</id><created>2014-01-01</created><updated>2014-01-10</updated><authors><author><keyname>Viswanathan</keyname><forenames>Vaisagh</forenames></author><author><keyname>Lee</keyname><forenames>Chong Eu</forenames></author><author><keyname>Lees</keyname><forenames>Michael Harold</forenames></author><author><keyname>Cheong</keyname><forenames>Siew Ann</forenames></author><author><keyname>Sloot</keyname><forenames>Peter M. A.</forenames></author></authors><title>Quantitative Comparison Between Crowd Models for Evacuation Planning and
  Evaluation</title><categories>cs.MA cs.CY</categories><comments>12 pages, 25 figures, accepted in EPJ B</comments><acm-class>I.2.11; I.6.4; I.6.5; I.6.6; J.2</acm-class><doi>10.1140/epjb/e2014-40699-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowd simulation is rapidly becoming a standard tool for evacuation planning
and evaluation. However, the many crowd models in the literature are
structurally different, and few have been rigorously calibrated against
real-world egress data, especially in emergency situations. In this paper we
describe a procedure to quantitatively compare different crowd models or
between models and real-world data. We simulated three models: (1) the lattice
gas model, (2) the social force model, and (3) the RVO2 model, and obtained the
distributions of six observables: (1) evacuation time, (2) zoned evacuation
time, (3) passage density, (4) total distance traveled, (5) inconvenience, and
(6) flow rate. We then used the DISTATIS procedure to compute the compromise
matrix of statistical distances between the three models. Projecting the three
models onto the first two principal components of the compromise matrix, we
find the lattice gas and RVO2 models are similar in terms of the evacuation
time, passage density, and flow rates, whereas the social force and RVO2 models
are similar in terms of the total distance traveled. Most importantly, we find
that the zoned evacuation times of the three models to be very different from
each other. Thus we propose to use this variable, if it can be measured, as the
key test between different models, and also between models and the real world.
Finally, we compared the model flow rates against the flow rate of an emergency
evacuation during the May 2008 Sichuan earthquake, and found the social force
model agrees best with this real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0376</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0376</id><created>2014-01-02</created><authors><author><keyname>Zhang</keyname><forenames>Chao</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Fan</keyname><forenames>Wei</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>Generalization Bounds for Representative Domain Adaptation</title><categories>cs.LG stat.ML</categories><comments>arXiv admin note: substantial text overlap with arXiv:1304.1574</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel framework to analyze the theoretical
properties of the learning process for a representative type of domain
adaptation, which combines data from multiple sources and one target (or
briefly called representative domain adaptation). In particular, we use the
integral probability metric to measure the difference between the distributions
of two domains and meanwhile compare it with the H-divergence and the
discrepancy distance. We develop the Hoeffding-type, the Bennett-type and the
McDiarmid-type deviation inequalities for multiple domains respectively, and
then present the symmetrization inequality for representative domain
adaptation. Next, we use the derived inequalities to obtain the Hoeffding-type
and the Bennett-type generalization bounds respectively, both of which are
based on the uniform entropy number. Moreover, we present the generalization
bounds based on the Rademacher complexity. Finally, we analyze the asymptotic
convergence and the rate of convergence of the learning process for
representative domain adaptation. We discuss the factors that affect the
asymptotic behavior of the learning process and the numerical experiments
support our theoretical findings as well. Meanwhile, we give a comparison with
the existing results of domain adaptation and the classical results under the
same-distribution assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0379</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0379</id><created>2014-01-02</created><authors><author><keyname>Grygiel</keyname><forenames>Katarzyna</forenames><affiliation>LIP</affiliation></author><author><keyname>Lescanne</keyname><forenames>Pierre</forenames><affiliation>LIP</affiliation></author></authors><title>Counting Terms in the Binary Lambda Calculus</title><categories>cs.LO cs.DM cs.DS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a paper entitled Binary lambda calculus and combinatory logic, John Tromp
presents a simple way of encoding lambda calculus terms as binary sequences. In
what follows, we study the numbers of binary strings of a given size that
represent lambda terms and derive results from their generating functions,
especially that the number of terms of size n grows roughly like 1.963447954^n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0395</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0395</id><created>2014-01-02</created><authors><author><keyname>Kodinariya</keyname><forenames>Trupti M.</forenames></author></authors><title>Hybrid Approach to Face Recognition System using Principle component and
  Independent component with score based fusion process</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid approach has a special status among Face Recognition Systems as they
combine different recognition approaches in an either serial or parallel to
overcome the shortcomings of individual methods. This paper explores the area
of Hybrid Face Recognition using score based strategy as a combiner/fusion
process. In proposed approach, the recognition system operates in two modes:
training and classification. Training mode involves normalization of the face
images (training set), extracting appropriate features using Principle
Component Analysis (PCA) and Independent Component Analysis (ICA). The
extracted features are then trained in parallel using Back-propagation neural
networks (BPNNs) to partition the feature space in to different face classes.
In classification mode, the trained PCA BPNN and ICA BPNN are fed with new face
image(s). The score based strategy which works as a combiner is applied to the
results of both PCA BPNN and ICA BPNN to classify given new face image(s)
according to face classes obtained during the training mode. The proposed
approach has been tested on ORL and other face databases; the experimented
results show that the proposed system has higher accuracy than face recognition
systems using single feature extractor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0396</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0396</id><created>2014-01-02</created><authors><author><keyname>Piotr&#xf3;w</keyname><forenames>Marek</forenames></author></authors><title>Faster 3-Periodic Merging Networks</title><categories>cs.DS cs.DC</categories><msc-class>68Q05, 68Q25</msc-class><acm-class>F.1.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of merging two sorted sequences on a comparator
network that is used repeatedly, that is, if the output is not sorted, the
network is applied again using the output as input. The challenging task is to
construct such networks of small depth. The first constructions of merging
networks with a constant period were given by Kuty{\l}owski, Lory\'s and
Oesterdikhoff. They have given $3$-periodic network that merges two sorted
sequences of $N$ numbers in time $12\log N$ and a similar network of period $4$
that works in $5.67\log N$. We present a new family of such networks that are
based on Canfield and Williamson periodic sorter. Our $3$-periodic merging
networks work in time upper-bounded by $6\log N$. The construction can be
easily generalized to larger constant periods with decreasing running time, for
example, to $4$-periodic ones that work in time upper-bounded by $4\log N$.
Moreover, to obtain the facts we have introduced a new proof technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0400</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0400</id><created>2014-01-02</created><updated>2015-05-03</updated><authors><author><keyname>Renault</keyname><forenames>Gabriel</forenames></author><author><keyname>Schmidt</keyname><forenames>Simon</forenames></author></authors><title>On the Complexity of the Mis\`ere Version of Three Games Played on
  Graphs</title><categories>cs.DM math.CO</categories><msc-class>05C57</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the complexity of finding a winning strategy for the mis\`ere
version of three games played on graphs : two variants of the game
$\text{NimG}$, introduced by Stockmann in 2004 and the game $\text{Vertex
Geography}$ on both directed and undirected graphs. We show that on general
graphs those three games are $\text{PSPACE}$-Hard or Complete. For one
$\text{PSPACE}$-Hard variant of $\text{NimG}$, we find an algorithm to compute
an effective winning strategy in time $\mathcal{O}(\sqrt{|V(G)|}.|E(G)|)$ when
$G$ is a bipartite graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0412</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0412</id><created>2014-01-02</created><updated>2014-01-22</updated><authors><author><keyname>Tan</keyname><forenames>Fei</forenames></author><author><keyname>Wu</keyname><forenames>Jiajing</forenames></author><author><keyname>Xia</keyname><forenames>Yongxiang</forenames></author><author><keyname>Tse</keyname><forenames>Chi K.</forenames></author></authors><title>Traffic congestion in interconnected complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages</comments><journal-ref>Phys. Rev. E 89, 062813 (2014)</journal-ref><doi>10.1103/PhysRevE.89.062813</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffic congestion in isolated complex networks has been investigated
extensively over the last decade. Coupled network models have recently been
developed to facilitate further understanding of real complex systems. Analysis
of traffic congestion in coupled complex networks, however, is still relatively
unexplored. In this paper, we try to explore the effect of interconnections on
traffic congestion in interconnected BA scale-free networks. We find that
assortative coupling can alleviate traffic congestion more readily than
disassortative and random coupling when the node processing capacity is
allocated based on node usage probability. Furthermore, the optimal coupling
probability can be found for assortative coupling. However, three types of
coupling preferences achieve similar traffic performance if all nodes share the
same processing capacity. We analyze interconnected Internet AS-level graphs of
South Korea and Japan and obtain similar results. Some practical suggestions
are presented to optimize such real-world interconnected networks accordingly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0417</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0417</id><created>2014-01-02</created><updated>2014-05-28</updated><authors><author><keyname>Boutsidis</keyname><forenames>Christos</forenames></author><author><keyname>Magdon-Ismail</keyname><forenames>Malik</forenames></author></authors><title>Faster SVD-Truncated Least-Squares Regression</title><categories>cs.DS math.NA</categories><comments>2014 IEEE International Symposium on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a fast algorithm for computing the &quot;SVD-truncated&quot; regularized
solution to the least-squares problem: $ \min_{\x} \TNorm{\matA \x - \b}. $ Let
$\matA_k$ of rank $k$ be the best rank $k$ matrix computed via the SVD of
$\matA$. Then, the SVD-truncated regularized solution is: $ \x_k =
\pinv{\matA}_k \b. $ If $\matA$ is $m \times n$, then, it takes $O(m n
\min\{m,n\})$ time to compute $\x_k $ using the SVD of \math{\matA}. We give an
approximation algorithm for \math{\x_k} which constructs a rank-\math{k}
approximation $\tilde{\matA}_{k}$ and computes $ \tilde{\x}_{k} =
\pinv{\tilde\matA}_{k} \b$ in roughly $O(\nnz(\matA) k \log n)$ time. Our
algorithm uses a randomized variant of the subspace iteration. We show that,
with high probability: $ \TNorm{\matA \tilde{\x}_{k} - \b} \approx \TNorm{\matA
\x_k - \b}$ and $\TNorm{\x_k - \tilde\x_k} \approx 0. $
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0430</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0430</id><created>2014-01-02</created><updated>2014-09-26</updated><authors><author><keyname>Siriteanu</keyname><forenames>Constantin</forenames></author><author><keyname>Takemura</keyname><forenames>Akimichi</forenames></author><author><keyname>Kuriki</keyname><forenames>Satoshi</forenames></author><author><keyname>Richards</keyname><forenames>Donald St. P.</forenames></author><author><keyname>Shin</keyname><forenames>Hyundong</forenames></author></authors><title>Schur Complement Based Analysis of MIMO Zero-Forcing for Rician Fading</title><categories>cs.IT math.IT</categories><comments>32 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For multiple-input/multiple-output (MIMO) spatial multiplexing with
zero-forcing detection (ZF), signal-to-noise ratio (SNR) analysis for Rician
fading involves the cumbersome noncentral-Wishart distribution (NCWD) of the
transmit sample-correlation (Gramian) matrix. An \textsl{approximation} with a
\textsl{virtual} CWD previously yielded for the ZF SNR an approximate (virtual)
Gamma distribution. However, analytical conditions qualifying the accuracy of
the SNR-distribution approximation were unknown. Therefore, we have been
attempting to exactly characterize ZF SNR for Rician fading. Our previous
attempts succeeded only for the sole Rician-fading stream under
Rician--Rayleigh fading, by writing it as scalar Schur complement (SC) in the
Gramian. Herein, we pursue a more general, matrix-SC-based analysis to
characterize SNRs when several streams may undergo Rician fading. On one hand,
for full-Rician fading, the SC distribution is found to be exactly a CWD if and
only if a channel-mean--correlation \textsl{condition} holds. Interestingly,
this CWD then coincides with the \textsl{virtual} CWD ensuing from the
\textsl{approximation}. Thus, under the \textsl{condition}, the actual and
virtual SNR-distributions coincide. On the other hand, for Rician--Rayleigh
fading, the matrix-SC distribution is characterized in terms of determinant of
matrix with elementary-function entries, which also yields a new
characterization of the ZF SNR. Average error probability results validate our
analysis vs.~simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0432</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0432</id><created>2014-01-02</created><updated>2014-04-14</updated><authors><author><keyname>Narayanaswamy</keyname><forenames>N. S.</forenames></author><author><keyname>Ramakrishna</keyname><forenames>G.</forenames></author></authors><title>On Minimum Average Stretch Spanning Trees in Polygonal 2-trees</title><categories>cs.DS</categories><comments>17 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A spanning tree of an unweighted graph is a minimum average stretch spanning
tree if it minimizes the ratio of sum of the distances in the tree between the
end vertices of the graph edges and the number of graph edges. We consider the
problem of computing a minimum average stretch spanning tree in polygonal
2-trees, a super class of 2-connected outerplanar graphs. For a polygonal
2-tree on $n$ vertices, we present an algorithm to compute a minimum average
stretch spanning tree in $O(n \log n)$ time. This algorithm also finds a
minimum fundamental cycle basis in polygonal 2-trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0437</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0437</id><created>2014-01-02</created><authors><author><keyname>Gul</keyname><forenames>Omer Melih</forenames></author><author><keyname>Uysal-Biyikoglu</keyname><forenames>Elif</forenames></author></authors><title>UROP: A Simple, Near-Optimal Scheduling Policy for Energy Harvesting
  Sensors</title><categories>cs.IT math.IT</categories><comments>32 pages, 10 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a single-hop wireless network where a central node (or
fusion center, FC) collects data from a set of m energy harvesting (EH) nodes
(e.g. nodes of a wireless sensor network). In each time slot, k of m nodes can
be scheduled by the FC for transmission over k orthogonal channels. FC has no
knowledge about EH processes and current battery states of nodes; however, it
knows outcomes of previous transmission attempts. The objective is to find a
low complexity scheduling policy that maximizes total throughput of the data
backlogged system using the harvested energy, for all types (uniform,
non-uniform, independent, correlated (i.e. Markovian), etc.) EH processes.
Energy is assumed to be stored losslessly in the nodes batteries, up to a
storage capacity (the infinite capacity case is also considered.) The problem
is treated in finite and infinite problem horizons. A low-complexity policy,
UROP (Uniformizing Random Ordered Policy) is proposed, whose near optimality is
shown. Numerical examples indicate that under a reasonable-sized battery
capacity, UROP uses the arriving energy with almost perfect efficiency. As the
problem is a restless multi-armed bandit (RMAB) problem with an average reward
criterion, UROP may have a wider application area than communication networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0443</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0443</id><created>2014-01-02</created><authors><author><keyname>Ashok</keyname><forenames>Pradeesha</forenames></author><author><keyname>Rajgopal</keyname><forenames>Ninad</forenames></author><author><keyname>Govindarajan</keyname><forenames>Sathish</forenames></author></authors><title>Selection Lemmas for various geometric objects</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Selection lemmas are classical results in discrete geometry that have been
well studied and have applications in many geometric problems like weak epsilon
nets and slimming Delaunay triangulations. Selection lemma type results
typically show that there exists a point that is contained in many objects that
are induced (spanned) by an underlying point set.
  In the first selection lemma, we consider the set of all the objects induced
(spanned) by a point set $P$. This question has been widely explored for
simplices in $\mathbb{R}^d$, with tight bounds in $\mathbb{R}^2$. In our paper,
we prove first selection lemma for other classes of geometric objects. We also
consider the strong variant of this problem where we add the constraint that
the piercing point comes from $P$. We prove an exact result on the strong and
the weak variant of the first selection lemma for axis-parallel rectangles,
special subclasses of axis-parallel rectangles like quadrants and slabs, disks
(for centrally symmetric point sets). We also show non-trivial bounds on the
first selection lemma for axis-parallel boxes and hyperspheres in
$\mathbb{R}^d$.
  In the second selection lemma, we consider an arbitrary $m$ sized subset of
the set of all objects induced by $P$. We study this problem for axis-parallel
rectangles and show that there exists an point in the plane that is contained
in $\frac{m^3}{24n^4}$ rectangles. This is an improvement over the previous
bound by Smorodinsky and Sharir when $m$ is almost quadratic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0445</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0445</id><created>2014-01-02</created><updated>2014-02-06</updated><authors><author><keyname>Anantharaman</keyname><forenames>Siva</forenames><affiliation>LIFO, Universite d'Orleans</affiliation></author><author><keyname>Bouchard</keyname><forenames>Christopher</forenames><affiliation>University at Albany - SUNY</affiliation></author><author><keyname>Narendran</keyname><forenames>Paliath</forenames><affiliation>University at Albany - SUNY</affiliation></author><author><keyname>Rusinowitch</keyname><forenames>Micha&#xeb;l</forenames><affiliation>Loria-INRIA Grand Est, Nancy</affiliation></author></authors><title>Unification modulo a 2-sorted Equational theory for Cipher-Decipher
  Block Chaining</title><categories>cs.LO</categories><comments>26 pages</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (February
  9, 2014) lmcs:808</journal-ref><doi>10.2168/LMCS-10(1:5)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate unification problems related to the Cipher Block Chaining
(CBC) mode of encryption. We first model chaining in terms of a simple,
convergent, rewrite system over a signature with two disjoint sorts: list and
element. By interpreting a particular symbol of this signature suitably, the
rewrite system can model several practical situations of interest. An inference
procedure is presented for deciding the unification problem modulo this rewrite
system. The procedure is modular in the following sense: any given problem is
handled by a system of `list-inferences', and the set of equations thus derived
between the element-terms of the problem is then handed over to any
(`black-box') procedure which is complete for solving these element-equations.
An example of application of this unification procedure is given, as attack
detection on a Needham-Schroeder like protocol, employing the CBC encryption
mode based on the associative-commutative (AC) operator XOR. The 2-sorted
convergent rewrite system is then extended into one that fully captures a block
chaining encryption-decryption mode at an abstract level, using no AC-symbols;
and unification modulo this extended system is also shown to be decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0447</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0447</id><created>2014-01-02</created><authors><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author><author><keyname>Salnikov</keyname><forenames>Vsevolod</forenames></author><author><keyname>Rosvall</keyname><forenames>Martin</forenames></author></authors><title>Effect of Memory on the Dynamics of Random Walks on Networks</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages; 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pathways of diffusion observed in real-world systems often require stochastic
processes going beyond first-order Markov models, as implicitly assumed in
network theory. In this work, we focus on second-order Markov models, and
derive an analytical expression for the effect of memory on the spectral gap
and thus, equivalently, on the characteristic time needed for the stochastic
process to asymptotically reach equilibrium. Perturbation analysis shows that
standard first-order Markov models can either overestimate or underestimate the
diffusion rate of flows across the modular structure of a system captured by a
second-order Markov network. We test the theoretical predictions on a toy
example and on numerical data, and discuss their implications for network
theory, in particular in the case of temporal or multiplex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0458</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0458</id><created>2014-01-02</created><updated>2014-07-01</updated><authors><author><keyname>Nettleton</keyname><forenames>David F.</forenames></author><author><keyname>Torra</keyname><forenames>Vicenc</forenames></author><author><keyname>Dries</keyname><forenames>Anton</forenames></author></authors><title>The effect of constraints on information loss and risk for clustering
  and modification based graph anonymization methods</title><categories>cs.CR</categories><comments>21 pages, 6 figures, 7 tables</comments><msc-class>05C85</msc-class><acm-class>E.1; G.2.2; I.2.8; K.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a novel approach for anonymizing Online Social
Network graphs which can be used in conjunction with existing perturbation
approaches such as clustering and modification. The main insight of this paper
is that by imposing additional constraints on which nodes can be selected we
can reduce the information loss with respect to key structural metrics, while
maintaining an acceptable risk. We present and evaluate two constraints,
'local1' and 'local2' which select the most similar subgraphs within the same
community while excluding some key structural nodes. To this end, we introduce
a novel distance metric based on local subgraph characteristics and which is
calibrated using an isomorphism matcher. Empirical testing is conducted with
three real OSN datasets, six information loss measures, five adversary queries
as risk measures, and different levels of k-anonymity. The results show that
overall, the methods with constraints give the best results for information
loss and risk of disclosure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0463</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0463</id><created>2014-01-02</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Sampaio-Neto</keyname><forenames>Raimundo</forenames></author></authors><title>Sparsity-Aware Adaptive Algorithms Based on Alternating Optimization
  with Shrinkage</title><categories>cs.SY</categories><comments>10 pages, 3 figures. IEEE Signal Processing Letters, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter proposes a novel sparsity-aware adaptive filtering scheme and
algorithms based on an alternating optimization strategy with shrinkage. The
proposed scheme employs a two-stage structure that consists of an alternating
optimization of a diagonally-structured matrix that speeds up the convergence
and an adaptive filter with a shrinkage function that forces the coefficients
with small magnitudes to zero. We devise alternating optimization least-mean
square (LMS) algorithms for the proposed scheme and analyze its mean-square
error. Simulations for a system identification application show that the
proposed scheme and algorithms outperform in convergence and tracking existing
sparsity-aware algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0468</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0468</id><created>2014-01-02</created><authors><author><keyname>Iglesias-Ham</keyname><forenames>Mabel</forenames></author><author><keyname>Kerber</keyname><forenames>Michael</forenames></author><author><keyname>Uhler</keyname><forenames>Caroline</forenames></author></authors><title>Sphere Packing with Limited Overlap</title><categories>cs.CG</categories><comments>12 pages, 3 figures, submitted to SOCG 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical sphere packing problem asks for the best (infinite) arrangement
of non-overlapping unit balls which cover as much space as possible. We define
a generalized version of the problem, where we allow each ball a limited amount
of overlap with other balls. We study two natural choices of overlap measures
and obtain the optimal lattice packings in a parameterized family of lattices
which contains the FCC, BCC, and integer lattice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0480</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0480</id><created>2014-01-02</created><authors><author><keyname>Correa</keyname><forenames>Denzil</forenames></author><author><keyname>Sureka</keyname><forenames>Ashish</forenames></author></authors><title>Chaff from the Wheat : Characterization and Modeling of Deleted
  Questions on Stack Overflow</title><categories>cs.IR cs.SI</categories><comments>11 pages, Pre-print</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Stack Overflow is the most popular CQA for programmers on the web with 2.05M
users, 5.1M questions and 9.4M answers. Stack Overflow has explicit, detailed
guidelines on how to post questions and an ebullient moderation community.
Despite these precise communications and safeguards, questions posted on Stack
Overflow can be extremely off topic or very poor in quality. Such questions can
be deleted from Stack Overflow at the discretion of experienced community
members and moderators. We present the first study of deleted questions on
Stack Overflow. We divide our study into two parts (i) Characterization of
deleted questions over approx. 5 years (2008-2013) of data, (ii) Prediction of
deletion at the time of question creation. Our characterization study reveals
multiple insights on question deletion phenomena. We observe a significant
increase in the number of deleted questions over time. We find that it takes
substantial time to vote a question to be deleted but once voted, the community
takes swift action. We also see that question authors delete their questions to
salvage reputation points. We notice some instances of accidental deletion of
good quality questions but such questions are voted back to be undeleted
quickly. We discover a pyramidal structure of question quality on Stack
Overflow and find that deleted questions lie at the bottom (lowest quality) of
the pyramid. We also build a predictive model to detect the deletion of
question at the creation time. We experiment with 47 features based on User
Profile, Community Generated, Question Content and Syntactic style and report
an accuracy of 66%. Our feature analysis reveals that all four categories of
features are important for the prediction task. Our findings reveal important
suggestions for content quality maintenance on community based question
answering websites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0486</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0486</id><created>2014-01-02</created><authors><author><keyname>Tagougui</keyname><forenames>Najiba</forenames></author><author><keyname>Boubaker</keyname><forenames>Houcine</forenames></author><author><keyname>Kherallah</keyname><forenames>Monji</forenames></author><author><keyname>ALIMI</keyname><forenames>Adel M.</forenames></author></authors><title>A Hybrid NN/HMM Modeling Technique for Online Arabic Handwriting
  Recognition</title><categories>cs.CV</categories><journal-ref>International Journal of Computational Linguistics Research Volume
  4 Number 3 September 2013 pp. 107-118</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose a hybrid NN/HMM model for online Arabic handwriting
recognition. The proposed system is based on Hidden Markov Models (HMMs) and
Multi Layer Perceptron Neural Networks (MLPNNs). The input signal is segmented
to continuous strokes called segments based on the Beta-Elliptical strategy by
inspecting the extremum points of the curvilinear velocity profile. A neural
network trained with segment level contextual information is used to extract
class character probabilities. The output of this network is decoded by HMMs to
provide character level recognition. In evaluations on the ADAB database, we
achieved 96.4% character recognition accuracy that is statistically
significantly important in comparison with character recognition accuracies
obtained from state-of-the-art online Arabic systems.8
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0494</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0494</id><created>2014-01-02</created><authors><author><keyname>Benali-Sougui</keyname><forenames>Ines</forenames></author><author><keyname>Sassi-Hidri</keyname><forenames>Minyar</forenames></author><author><keyname>Grissa-Touzi</keyname><forenames>Amel</forenames></author></authors><title>Flexible SQLf query based on fuzzy linguistic summaries</title><categories>cs.DB</categories><journal-ref>International Conference on Control, Engineering &amp; Information
  Technology (CEIT), Proceedings Engineering &amp; Technology, Vol. 1, pp. 175-180,
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data is often partially known, vague or ambiguous in many real world
applications. To deal with such imprecise information, fuzziness is introduced
in the classical model. SQLf is one of the practical language to deal with
flexible fuzzy querying in Fuzzy DataBases (FDB). However, with a huge amount
of fuzzy data, the necessity to work with synthetic views became a challenge
for many DB community researchers. The present work deals with Flexible SQLf
query based on fuzzy linguistic summaries. We use the fuzzy summaries produced
by our Fuzzy-SaintEtiq approach. It provides a description of objects depending
on the fuzzy linguistic labels specified as selection criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0496</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0496</id><created>2014-01-02</created><updated>2014-03-24</updated><authors><author><keyname>Karafyllis</keyname><forenames>Iasson</forenames></author><author><keyname>Papageorgiou</keyname><forenames>Markos</forenames></author></authors><title>Global Stability Results for Traffic Networks</title><categories>math.OC cs.SY math.DS</categories><comments>22 pages, 1 figure, to be submitted for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides sufficient conditions for global asymptotic stability and
global exponential stability, which can be applied to nonlinear, large-scale,
uncertain discrete-time systems. The conditions are derived by means of vector
Lyapunov functions. The obtained results are applied to traffic networks for
the derivation of sufficient conditions of global exponential stability of the
uncongested equilibrium point of the network. Specific results and algorithms
are provided for freeway models. Various examples illustrate the applicability
of the obtained results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0503</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0503</id><created>2013-12-30</created><authors><author><keyname>Kelemen</keyname><forenames>Z&#xe1;dor D&#xe1;niel</forenames></author></authors><title>Process Based Unification for Multi-Model Software Process Improvement</title><categories>cs.SE</categories><comments>PhD Thesis</comments><journal-ref>ISBN: 978-90-386-3313-8, 2013, Technische Universiteit Eindhoven</journal-ref><doi>10.6100/IR741509</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of differences among quality approaches exist and there can be
various situations in which the usage of multiple approaches is required, e.g.
to strengthen a particular process with multiple quality approaches or to reach
certification of the compliance to a number of standards. First of all it has
to be decided which approaches have potential for the organization. In many
cases one approach does not contain enough information for process
implementation. Consequently, the organization may need to use several
approaches and the decision has to be made how the chosen approaches can be
used simultaneously. This area is called Multi-model Software Process
Improvement (MSPI). The simultaneous usage of multiple quality approaches is
called the multi-model problem. In this dissertation we propose a solution for
the multi-model problem which we call the Process Based Unification (PBU)
framework. The PBU framework consists of the PBU concept, a PBU process and the
PBU result. We call PBU concept the mapping of quality approaches to a unified
process. The PBU concept is operationalized by a PBU process. The PBU result
includes the resulting unified process and the mapping of quality approaches to
the unified process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0504</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0504</id><created>2013-12-28</created><authors><author><keyname>Huynh</keyname><forenames>Chien</forenames></author><author><keyname>Huynh</keyname><forenames>Phuong</forenames></author></authors><title>Student-based Collaborative Network for Delivering Information of
  Natural Disasters and Climate Adaptation</title><categories>cs.CY</categories><comments>4 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The student generation nowadays is considered as the Net Generation who grow
up in the age of Internet and mobile technology and prefer bringing
technological application into their life. The Learning Resource Center, an
academic information center, takes the initiative in developing a collaborative
network with the participation of students for delivering information of
natural disasters and climate adaptation to the people who live in the
vulnerable areas of Thua Thien Hue province. The network is aimed at helping
local people mitigating vulnerabilities and adapting to the globally climate
change for a better life.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0509</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0509</id><created>2013-12-20</created><updated>2014-03-07</updated><authors><author><keyname>Dauphin</keyname><forenames>Yann N.</forenames></author><author><keyname>Tur</keyname><forenames>Gokhan</forenames></author><author><keyname>Hakkani-Tur</keyname><forenames>Dilek</forenames></author><author><keyname>Heck</keyname><forenames>Larry</forenames></author></authors><title>Zero-Shot Learning for Semantic Utterance Classification</title><categories>cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose a novel zero-shot learning method for semantic utterance
classification (SUC). It learns a classifier $f: X \to Y$ for problems where
none of the semantic categories $Y$ are present in the training set. The
framework uncovers the link between categories and utterances using a semantic
space. We show that this semantic space can be learned by deep neural networks
trained on large amounts of search engine query log data. More precisely, we
propose a novel method that can learn discriminative semantic features without
supervision. It uses the zero-shot learning framework to guide the learning of
the semantic features. We demonstrate the effectiveness of the zero-shot
semantic learning algorithm on the SUC dataset collected by (Tur, 2012).
Furthermore, we achieve state-of-the-art results by combining the semantic
features with a supervised method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0514</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0514</id><created>2014-01-02</created><updated>2014-06-20</updated><authors><author><keyname>Maddison</keyname><forenames>Chris J.</forenames></author><author><keyname>Tarlow</keyname><forenames>Daniel</forenames></author></authors><title>Structured Generative Models of Natural Source Code</title><categories>cs.PL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of building generative models of natural source code
(NSC); that is, source code written and understood by humans. Our primary
contribution is to describe a family of generative models for NSC that have
three key properties: First, they incorporate both sequential and hierarchical
structure. Second, we learn a distributed representation of source code
elements. Finally, they integrate closely with a compiler, which allows
leveraging compiler logic and abstractions when building structure into the
model. We also develop an extension that includes more complex structure,
refining how the model generates identifier tokens based on what variables are
currently in scope. Our models can be learned efficiently, and we show
empirically that including appropriate structure greatly improves the models,
measured by the probability of generating test programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0523</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0523</id><created>2014-01-02</created><authors><author><keyname>Jebari</keyname><forenames>Khalid</forenames></author><author><keyname>Madiafi</keyname><forenames>Mohammed</forenames></author><author><keyname>Moujahid</keyname><forenames>Abdelaziz El</forenames></author></authors><title>Solving Poisson Equation by Genetic Algorithms</title><categories>cs.NE</categories><journal-ref>International Journal of Computer Applications Volume 83, No 5,
  December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with a method for solving Poisson Equation (PE) based on
genetic algorithms and grammatical evolution. The method forms generations of
solutions expressed in an analytical form. Several examples of PE are tested
and in most cases the exact solution is recovered. But, when the solution
cannot be expressed in an analytical form, our method produces a satisfactory
solution with a good level of accuracy
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0534</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0534</id><created>2013-12-31</created><authors><author><keyname>Benhima</keyname><forenames>Mounire</forenames></author><author><keyname>Reilly</keyname><forenames>John P.</forenames></author><author><keyname>Naamane</keyname><forenames>Zaineb</forenames></author><author><keyname>Kharbat</keyname><forenames>Meriam</forenames></author><author><keyname>Kabbaj</keyname><forenames>Mohammed Issam</forenames></author><author><keyname>Esqalli</keyname><forenames>Oussama</forenames></author></authors><title>Design and implementation of the Customer Experience Data Mart in the
  Telecommunication Industry: Application Order-To-Payment end to end process</title><categories>cs.OH</categories><comments>25 pages, 24 figures, 17 tables, IJCSI</comments><journal-ref>IJCSI Journal, Volume 10, Issue 3, May 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Facing the new market challenges, service providers are looking for solutions
to improve three major business areas namely the Customer Experience, The
Operational Efficiency and Revenue and Margin. To meet the business requiements
related to these areas, service providers are going through three major
transformation programs namely the Business Support Systems transformation
program for Customer related aspects, the Operations Support System
transformation program for mainly service Fulfillment and Assurance and
Resource, Fulfillment and Assurance, and Time To Market Transformation program
for Products ans Services development and management. These transformations are
about making a transition from a current situation with all its views to a
desired one. The information view transformation is about reorganizing and
reengineering the existing information to be used for the day to day activities
and reporting to support decision making. For reporting purpose, service
providers have to invest in Business Intelligence solutions. For which the main
purpose is to provide the right information in a timely manner to efficiently
support the decision making. One of the key BI challenges is to model an
information structure where to host all the information coming from multiple
sources. The purpose of this paper is to suggest a step by step methodology to
design a Telco Data Mart, one of the fundamental BI components.
Order-To-Payment, an end to end customer process, will be used as an
application for this methodology. Our methodology consists on bringing together
the concepts of business intelligence and the telecom business frameworks
developed by the TM Forum: the Business Process Framework, the Information
Framework, and the Business Metrics. The advantage of this solution is its
ability to adapt to any telecom enterprise architecture since it's built around
the business standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0543</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0543</id><created>2014-01-02</created><authors><author><keyname>Fu</keyname><forenames>Amy</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Beyond the Min-Cut Bound: Deterministic Network Coding for Asynchronous
  Multirate Broadcast</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a single hop broadcast packet erasure network, we demonstrate that it is
possible to provide multirate packet delivery outside of what is given by the
network min-cut. This is achieved by using a deterministic non-block-based
network coding scheme, which allows us to sidestep some of the limitations put
in place by the block coding model used to determine the network capacity.
  Under the network coding scheme we outline, the sender is able to transmit
network coded packets above the channel rate of some receivers, while ensuring
that they still experience nonzero delivery rates. Interestingly, in this
generalised form of asynchronous network coded broadcast, receivers are not
required to obtain knowledge of all packets transmitted so far. Instead, causal
feedback from the receivers about packet erasures is used by the sender to
determine a network coded transmission that will allow at least one, but often
multiple receivers, to deliver their next needed packet.
  Although the analysis of deterministic coding schemes is generally a
difficult problem, by making some approximations we are able to obtain
tractable estimates of the receivers' delivery rates, which are shown to match
reasonably well with simulation. Using these estimates, we design a fairness
algorithm that allocates the sender's resources so all receivers will
experience fair delivery rate performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0546</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0546</id><created>2014-01-02</created><authors><author><keyname>Sohail</keyname><forenames>Muhammad Saqib</forenames></author><author><keyname>Saeed</keyname><forenames>Muhammad Omer Bin</forenames></author><author><keyname>Rizvi</keyname><forenames>Syed Zeeshan</forenames></author><author><keyname>Shoaib</keyname><forenames>Mobien</forenames></author><author><keyname>Sheikh</keyname><forenames>Asrar Ul Haq</forenames></author></authors><title>Low-Complexity Particle Swarm Optimization for Time-Critical
  Applications</title><categories>cs.NE</categories><comments>24 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particle swam optimization (PSO) is a popular stochastic optimization method
that has found wide applications in diverse fields. However, PSO suffers from
high computational complexity and slow convergence speed. High computational
complexity hinders its use in applications that have limited power resources
while slow convergence speed makes it unsuitable for time critical
applications. In this paper, we propose two techniques to overcome these
limitations. The first technique reduces the computational complexity of PSO
while the second technique speeds up its convergence. These techniques can be
applied, either separately or in conjunction, to any existing PSO variant. The
proposed techniques are robust to the number of dimensions of the optimization
problem. Simulation results are presented for the proposed techniques applied
to the standard PSO as well as to several PSO variants. The results show that
the use of both these techniques in conjunction results in a reduction in the
number of computations required as well as faster convergence speed while
maintaining an acceptable error performance for time-critical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0561</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0561</id><created>2014-01-02</created><authors><author><keyname>Sherman</keyname><forenames>Michael</forenames></author><author><keyname>Clark</keyname><forenames>Gradeigh</forenames></author><author><keyname>Yang</keyname><forenames>Yulong</forenames></author><author><keyname>Sugrim</keyname><forenames>Shridatt</forenames></author><author><keyname>Modig</keyname><forenames>Arttu</forenames></author><author><keyname>Lindqvist</keyname><forenames>Janne</forenames></author><author><keyname>Oulasvirta</keyname><forenames>Antti</forenames></author><author><keyname>Roos</keyname><forenames>Teemu</forenames></author></authors><title>User-Generated Free-Form Gestures for Authentication: Security and
  Memorability</title><categories>cs.CR cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the security and memorability of free-form multitouch
gestures for mobile authentication. Towards this end, we collected a dataset
with a generate-test-retest paradigm where participants (N=63) generated
free-form gestures, repeated them, and were later retested for memory. Half of
the participants decided to generate one-finger gestures, and the other half
generated multi-finger gestures. Although there has been recent work on
template-based gestures, there are yet no metrics to analyze security of either
template or free-form gestures. For example, entropy-based metrics used for
text-based passwords are not suitable for capturing the security and
memorability of free-form gestures. Hence, we modify a recently proposed metric
for analyzing information capacity of continuous full-body movements for this
purpose. Our metric computed estimated mutual information in repeated sets of
gestures. Surprisingly, one-finger gestures had higher average mutual
information. Gestures with many hard angles and turns had the highest mutual
information. The best-remembered gestures included signatures and simple
angular shapes. We also implemented a multitouch recognizer to evaluate the
practicality of free-form gestures in a real authentication system and how they
perform against shoulder surfing attacks. We conclude the paper with strategies
for generating secure and memorable free-form gestures, which present a robust
method for mobile authentication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0564</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0564</id><created>2014-01-02</created><authors><author><keyname>Aguirre</keyname><forenames>Nazareno</forenames><affiliation>Universidad Nacional de R&#xed;o Cuarto</affiliation></author><author><keyname>Ribeiro</keyname><forenames>Leila</forenames><affiliation>Universidade Federal do Rio Grande do Sul</affiliation></author></authors><title>Proceedings First Latin American Workshop on Formal Methods</title><categories>cs.SE cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 139, 2014</journal-ref><doi>10.4204/EPTCS.139</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal approaches to software development are techniques that aim at
developing quality software by employing notations, analysis processes, etc.,
based on mathematical grounds. Although traditionally they aim at increasing
software correctness, formal techniques have been applied to various other
aspects of software quality. Moreover, while originally formal methods employed
complex &quot;heavyweight&quot; mechanisms for analysis (often manual or semi automated),
there has been progress towards embracing &quot;lightweight&quot;, many times fully
automated, analysis techniques, that broaden the adoption of formal methods in
various software engineering contexts.
  The Latin American Workshop on Formal Methods brings together researchers
working in formal methods, and related areas such as automated analysis. In
particular, the workshop provides a venue for Latin American researchers
working in these areas, to promote their interaction and collaboration.
  The workshop was held in August as a satellite event of CONCUR 2013. It took
place in Buenos Aires, Argentina's capital and largest city, and one of the
most interesting cultural places in South America.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0569</identifier>
 <datestamp>2014-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0569</id><created>2014-01-02</created><updated>2014-01-08</updated><authors><author><keyname>Doan</keyname><forenames>Son</forenames></author><author><keyname>Conway</keyname><forenames>Mike</forenames></author><author><keyname>Phuong</keyname><forenames>Tu Minh</forenames></author><author><keyname>Ohno-Machado</keyname><forenames>Lucila</forenames></author></authors><title>Natural Language Processing in Biomedicine: A Unified System
  Architecture Overview</title><categories>cs.CL</categories><comments>25 pages, 5 figures, book chapter in Clinical Bioinformatics, 2014,
  edited by Ronand Trent</comments><doi>10.1007/978-1-4939-0847-9_16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In modern electronic medical records (EMR) much of the clinically important
data - signs and symptoms, symptom severity, disease status, etc. - are not
provided in structured data fields, but rather are encoded in clinician
generated narrative text. Natural language processing (NLP) provides a means of
&quot;unlocking&quot; this important data source for applications in clinical decision
support, quality assurance, and public health. This chapter provides an
overview of representative NLP systems in biomedicine based on a unified
architectural view. A general architecture in an NLP system consists of two
main components: background knowledge that includes biomedical knowledge
resources and a framework that integrates NLP tools to process text. Systems
differ in both components, which we will review briefly. Additionally,
challenges facing current research efforts in biomedical NLP include the
paucity of large, publicly available annotated corpora, although initiatives
that facilitate data sharing, system evaluation, and collaborative work between
researchers in clinical NLP are starting to emerge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0578</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0578</id><created>2014-01-02</created><authors><author><keyname>Chang</keyname><forenames>Ling-Hua</forenames></author><author><keyname>Wu</keyname><forenames>Jwo-Yuh</forenames></author></authors><title>An Improved RIP-Based Performance Guarantee for Sparse Signal Recovery
  via Orthogonal Matching Pursuit</title><categories>cs.IT math.IT</categories><comments>38 pages,4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sufficient condition reported very recently for perfect recovery of a
K-sparse vector via orthogonal matching pursuit (OMP) in K iterations is that
the restricted isometry constant of the sensing matrix satisfies
delta_K+1&lt;1/(sqrt(delta_K+1)+1). By exploiting an approximate orthogonality
condition characterized via the achievable angles between two orthogonal sparse
vectors upon compression, this paper shows that the upper bound on delta can be
further relaxed to delta_K+1&lt;(sqrt(1+4*delta_K+1)-1)/(2K).This result thus
narrows the gap between the so far best known bound and the ultimate
performance guarantee delta_K+1&lt;1/(sqrt(delta_K+1)) that is conjectured by Dai
and Milenkovic in 2009. The proposed approximate orthogonality condition is
also exploited to derive less restricted sufficient conditions for signal
reconstruction in several compressive sensing problems, including signal
recovery via OMP in a noisy environment, compressive domain interference
cancellation, and support identification via the subspace pursuit algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0579</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0579</id><created>2014-01-02</created><authors><author><keyname>Arora</keyname><forenames>Sanjeev</forenames></author><author><keyname>Bhaskara</keyname><forenames>Aditya</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Ma</keyname><forenames>Tengyu</forenames></author></authors><title>More Algorithms for Provable Dictionary Learning</title><categories>cs.DS cs.LG stat.ML</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In dictionary learning, also known as sparse coding, the algorithm is given
samples of the form $y = Ax$ where $x\in \mathbb{R}^m$ is an unknown random
sparse vector and $A$ is an unknown dictionary matrix in $\mathbb{R}^{n\times
m}$ (usually $m &gt; n$, which is the overcomplete case). The goal is to learn $A$
and $x$. This problem has been studied in neuroscience, machine learning,
visions, and image processing. In practice it is solved by heuristic algorithms
and provable algorithms seemed hard to find. Recently, provable algorithms were
found that work if the unknown feature vector $x$ is $\sqrt{n}$-sparse or even
sparser. Spielman et al. \cite{DBLP:journals/jmlr/SpielmanWW12} did this for
dictionaries where $m=n$; Arora et al. \cite{AGM} gave an algorithm for
overcomplete ($m &gt;n$) and incoherent matrices $A$; and Agarwal et al.
\cite{DBLP:journals/corr/AgarwalAN13} handled a similar case but with weaker
guarantees.
  This raised the problem of designing provable algorithms that allow sparsity
$\gg \sqrt{n}$ in the hidden vector $x$. The current paper designs algorithms
that allow sparsity up to $n/poly(\log n)$. It works for a class of matrices
where features are individually recoverable, a new notion identified in this
paper that may motivate further work.
  The algorithm runs in quasipolynomial time because they use limited
enumeration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0583</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0583</id><created>2014-01-02</created><authors><author><keyname>Warnell</keyname><forenames>Garrett</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Sourabh</forenames></author><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Adaptive-Rate Compressive Sensing Using Side Information</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide two novel adaptive-rate compressive sensing (CS) strategies for
sparse, time-varying signals using side information. Our first method utilizes
extra cross-validation measurements, and the second one exploits extra
low-resolution measurements. Unlike the majority of current CS techniques, we
do not assume that we know an upper bound on the number of significant
coefficients that comprise the images in the video sequence. Instead, we use
the side information to predict the number of significant coefficients in the
signal at the next time instant. For each image in the video sequence, our
techniques specify a fixed number of spatially-multiplexed CS measurements to
acquire, and adjust this quantity from image to image. Our strategies are
developed in the specific context of background subtraction for surveillance
video, and we experimentally validate the proposed methods on real video
sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0585</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0585</id><created>2014-01-02</created><authors><author><keyname>Sandholm</keyname><forenames>Thomas</forenames></author><author><keyname>Lee</keyname><forenames>Dongman</forenames></author><author><keyname>Tegelund</keyname><forenames>Bjorn</forenames></author><author><keyname>Han</keyname><forenames>Seonyeong</forenames></author><author><keyname>Shin</keyname><forenames>Byoungheon</forenames></author><author><keyname>Kim</keyname><forenames>Byoungoh</forenames></author></authors><title>CloudFridge: A Testbed for Smart Fridge Interactions</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a testbed for exploring novel smart refrigerator interactions, and
identify three key adoption-limiting interaction shortcomings of
state-of-the-art smart fridges: lack of 1) user experience focus, 2)
low-intrusion object recognition and 2) automatic item position detection. Our
testbed system addresses these limitations by a combination of sensors,
software filters, architectural components and a RESTful API to track
interaction events in real-time, and retrieve current state and historical data
to learn patterns and recommend user actions. We evaluate the accuracy and
overhead of our system in a realistic interaction flow. The accuracy was
measured to 83-88% and the overhead compared to a representative
state-of-the-art barcode scanner improved by 27%. We also showcase two
applications built on top of our testbed, one for finding expired items and
ingredients of dishes; and one to monitor your health. The pattern that these
applications have in common is that they cast the interaction as an
item-recommendation problem triggered when the user takes something out. Our
testbed could help reveal further user-experience centric interaction patterns
and new classes of applications for smart fridges that inherently, by relying
on our testbed primitives, mitigate the issues with existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0591</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0591</id><created>2014-01-03</created><authors><author><keyname>Kumar</keyname><forenames>Puneet</forenames></author><author><keyname>Kumar</keyname><forenames>Dharminder</forenames></author><author><keyname>Kumar</keyname><forenames>Narendra</forenames></author></authors><title>ICT in Local Self Governance: A Study of Rural India</title><categories>cs.CY</categories><comments>6 pages, 4 figures, Published with International Journal of Computer
  Applications (IJCA)</comments><journal-ref>International Journal of Computer Applications 83(6):31-36,
  December 2013. Published by Foundation of Computer Science, New York, USA</journal-ref><doi>10.5120/14453-2714</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of local self-governance is not new as it has its roots in
ancient time even before the era of Mauryan emperors. This paper depicts the
journey of local self-governance from antediluvian time to 21st century.
Further, in the current scenario Information and Communication Technology (ICT)
has emerged as a successful tool for dissemination of various e-governance
services and in this regard the Government of India has formulated NeGP with
adequate service delivery mechanism. With the inculcation of ICT, various
applications were designed by central as well as state governments which lead
towards strengthening of PRIs for rural reform. This paper also throws some
light on necessity of ICT in self-governance along with some case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0598</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0598</id><created>2014-01-03</created><authors><author><keyname>Wu</keyname><forenames>Wu</forenames></author><author><keyname>Hu</keyname><forenames>Jiulin</forenames></author><author><keyname>Huang</keyname><forenames>Xiaofang</forenames></author><author><keyname>Chen</keyname><forenames>Huijie</forenames></author><author><keyname>Sun</keyname><forenames>Bo</forenames></author></authors><title>Flight trajectory recreation and playback system of aerial mission based
  on ossimplanet</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recreation of flight trajectory is important among research areas. The design
of a flight trajectory recreation and playback system is presented in this
paper. Rather than transferring the flight data to diagram, graph and table,
flight data is visualized on the 3D global of ossimPlanet. ossimPlanet is an
open-source 3D global geo-spatial viewer and the system realization is based on
analysis it. Users are allowed to choose their interested flight of aerial
mission. The aerial photographs and corresponding configuration files in which
flight data is included would be read in. And the flight statuses would be
stored. The flight trajectory is then recreated. Users can view the photographs
and flight trajectory marks on the correct positions of 3D global. The scene
along flight trajectory is also simulated at the plane's eye point. This paper
provides a more intuitive way for recreation of flight trajectory. The cost is
decreased remarkably and security is ensured by secondary development on
open-source platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0608</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0608</id><created>2014-01-03</created><authors><author><keyname>Sheharyar</keyname><forenames>Ali</forenames></author><author><keyname>Bouhali</keyname><forenames>Othmane</forenames></author></authors><title>A Framework for Creating a Distributed Rendering Environment on the
  Compute Clusters</title><categories>cs.DC cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the deployment of existing render farm manager in a
typical compute cluster environment such as a university. Usually, both a
render farm and a compute cluster use different queue managers and assume total
control over the physical resources. But, taking out the physical resources
from an existing compute cluster in a university-like environment whose primary
use of the cluster is to run numerical simulations may not be possible. It can
potentially reduce the overall resource utilization in a situation where
compute tasks are more than rendering tasks. Moreover, it can increase the
system administration cost. In this paper, a framework has been proposed that
creates a dynamic distributed rendering environment on top of the compute
clusters using existing render farm managers without requiring the physical
separation of the resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0615</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0615</id><created>2014-01-03</created><authors><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author></authors><title>Message Encoding for Spread and Orbit Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE International Symposium on Information Theory 2014</comments><doi>10.1109/ISIT.2014.6875303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spread codes and orbit codes are special families of constant dimension
subspace codes. These codes have been well-studied for their error correction
capability and transmission rate, but the question of how to encode messages
has not been investigated. In this work we show how the message space can be
chosen for a given code and how message en- and decoding can be done.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0625</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0625</id><created>2014-01-03</created><authors><author><keyname>Lewenstein</keyname><forenames>Moshe</forenames></author><author><keyname>Nekrich</keyname><forenames>Yakov</forenames></author><author><keyname>Vitter</keyname><forenames>Jeffrey Scott</forenames></author></authors><title>Space-Efficient String Indexing for Wildcard Pattern Matching</title><categories>cs.DS</categories><comments>15 pages, extended version of the STACS paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe compressed indexes that support pattern matching
queries for strings with wildcards. For a constant size alphabet our data
structure uses $O(n\log^{\varepsilon}n)$ bits for any $\varepsilon&gt;0$ and
reports all $\mathrm{occ}$ occurrences of a wildcard string in $O(m+\sigma^g
\cdot\mu(n) + \mathrm{occ})$ time, where $\mu(n)=o(\log\log\log n)$, $\sigma$
is the alphabet size, $m$ is the number of alphabet symbols and $g$ is the
number of wildcard symbols in the query string. We also present an $O(n)$-bit
index with $O((m+\sigma^g+\mathrm{occ})\log^{\varepsilon}n)$ query time and an
$O(n(\log\log n)^2)$-bit index with $O((m+\sigma^g+\mathrm{occ})\log\log n)$
query time. These are the first non-trivial data structures for this problem
that need $o(n\log n)$ bits of space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0629</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0629</id><created>2014-01-03</created><updated>2014-03-28</updated><authors><author><keyname>Doerfel</keyname><forenames>Stephan</forenames></author><author><keyname>Zoller</keyname><forenames>Daniel</forenames></author><author><keyname>Singer</keyname><forenames>Philipp</forenames></author><author><keyname>Niebler</keyname><forenames>Thomas</forenames></author><author><keyname>Hotho</keyname><forenames>Andreas</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author></authors><title>Of course we share! Testing Assumptions about Social Tagging Systems</title><categories>cs.IR cs.DL cs.SI</categories><acm-class>H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social tagging systems have established themselves as an important part in
today's web and have attracted the interest from our research community in a
variety of investigations. The overall vision of our community is that simply
through interactions with the system, i.e., through tagging and sharing of
resources, users would contribute to building useful semantic structures as
well as resource indexes using uncontrolled vocabulary not only due to the
easy-to-use mechanics. Henceforth, a variety of assumptions about social
tagging systems have emerged, yet testing them has been difficult due to the
absence of suitable data. In this work we thoroughly investigate three
available assumptions - e.g., is a tagging system really social? - by examining
live log data gathered from the real-world public social tagging system
BibSonomy. Our empirical results indicate that while some of these assumptions
hold to a certain extent, other assumptions need to be reflected and viewed in
a very critical light. Our observations have implications for the design of
future search and other algorithms to better reflect the actual user behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0640</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0640</id><created>2014-01-03</created><authors><author><keyname>El-Ghannam</keyname><forenames>Fatma</forenames></author><author><keyname>El-Shishtawy</keyname><forenames>Tarek</forenames></author></authors><title>Multi-Topic Multi-Document Summarizer</title><categories>cs.CL</categories><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 5, No 6, December 2013</journal-ref><doi>10.5121/ijcsit</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current multi-document summarization systems can successfully extract summary
sentences, however with many limitations including: low coverage, inaccurate
extraction to important sentences, redundancy and poor coherence among the
selected sentences. The present study introduces a new concept of centroid
approach and reports new techniques for extracting summary sentences for
multi-document. In both techniques keyphrases are used to weigh sentences and
documents. The first summarization technique (Sen-Rich) prefers maximum
richness sentences. While the second (Doc-Rich), prefers sentences from
centroid document. To demonstrate the new summarization system application to
extract summaries of Arabic documents we performed two experiments. First, we
applied Rouge measure to compare the new techniques among systems presented at
TAC2011. The results show that Sen-Rich outperformed all systems in ROUGE-S.
Second, the system was applied to summarize multi-topic documents. Using human
evaluators, the results show that Doc-Rich is the superior, where summary
sentences characterized by extra coverage and more cohesion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0645</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0645</id><created>2014-01-03</created><updated>2015-11-13</updated><authors><author><keyname>Bradford</keyname><forenames>Russell</forenames></author><author><keyname>Davenport</keyname><forenames>James H.</forenames></author><author><keyname>England</keyname><forenames>Matthew</forenames></author><author><keyname>McCallum</keyname><forenames>Scott</forenames></author><author><keyname>Wilson</keyname><forenames>David</forenames></author></authors><title>Truth Table Invariant Cylindrical Algebraic Decomposition</title><categories>cs.SC</categories><comments>40 pages</comments><msc-class>68W30, 03C10</msc-class><acm-class>I.1.2</acm-class><journal-ref>Journal of Symbolic Computation 76, pp. 1-35, 2016</journal-ref><doi>10.1016/j.jsc.2015.11.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When using cylindrical algebraic decomposition (CAD) to solve a problem with
respect to a set of polynomials, it is likely not the signs of those
polynomials that are of paramount importance but rather the truth values of
certain quantifier free formulae involving them. This observation motivates our
article and definition of a Truth Table Invariant CAD (TTICAD).
  In ISSAC 2013 the current authors presented an algorithm that can efficiently
and directly construct a TTICAD for a list of formulae in which each has an
equational constraint. This was achieved by generalising McCallum's theory of
reduced projection operators. In this paper we present an extended version of
our theory which can be applied to an arbitrary list of formulae, achieving
savings if at least one has an equational constraint. We also explain how the
theory of reduced projection operators can allow for further improvements to
the lifting phase of CAD algorithms, even in the context of a single equational
constraint.
  The algorithm is implemented fully in Maple and we present both promising
results from experimentation and a complexity analysis showing the benefits of
our contributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0647</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0647</id><created>2014-01-03</created><updated>2014-04-24</updated><authors><author><keyname>Wilson</keyname><forenames>D. J.</forenames></author><author><keyname>Bradford</keyname><forenames>R. J.</forenames></author><author><keyname>Davenport</keyname><forenames>J. H.</forenames></author><author><keyname>England</keyname><forenames>M.</forenames></author></authors><title>Cylindrical Algebraic Sub-Decompositions</title><categories>cs.SC math.AG</categories><comments>26 pages</comments><msc-class>68W30</msc-class><acm-class>I.1.2</acm-class><journal-ref>Mathematics in Computer Science: Volume 8, Issue 2, pages 263-288,
  Springer, 2014</journal-ref><doi>10.1007/s11786-014-0191-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cylindrical algebraic decompositions (CADs) are a key tool in real algebraic
geometry, used primarily for eliminating quantifiers over the reals and
studying semi-algebraic sets. In this paper we introduce cylindrical algebraic
sub-decompositions (sub-CADs), which are subsets of CADs containing all the
information needed to specify a solution for a given problem.
  We define two new types of sub-CAD: variety sub-CADs which are those cells in
a CAD lying on a designated variety; and layered sub-CADs which have only those
cells of dimension higher than a specified value. We present algorithms to
produce these and describe how the two approaches may be combined with each
other and the recent theory of truth-table invariant CAD.
  We give a complexity analysis showing that these techniques can offer
substantial theoretical savings, which is supported by experimentation using an
implementation in Maple.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0648</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0648</id><created>2014-01-03</created><authors><author><keyname>Mummert</keyname><forenames>Carl</forenames></author><author><keyname>Saadaoui</keyname><forenames>Alaeddine</forenames></author><author><keyname>Sovine</keyname><forenames>Sean</forenames></author></authors><title>The modal logic of Reverse Mathematics</title><categories>math.LO cs.LO</categories><msc-class>03B30 (Primary) 03B45 (Secondary)</msc-class><journal-ref>Archive for Mathematical Logic May 2015, Volume 54, Issue 3-4, pp
  425-437</journal-ref><doi>10.1007/s00153-015-0417-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The implication relationship between subsystems in Reverse Mathematics has an
underlying logic, which can be used to deduce certain new Reverse Mathematics
results from existing ones in a routine way. We use techniques of modal logic
to formalize the logic of Reverse Mathematics into a system that we name
s-logic. We argue that s-logic captures precisely the &quot;logical&quot; content of the
implication and nonimplication relations between subsystems in Reverse
Mathematics. We present a sound, complete, decidable, and compact tableau-style
deductive system for s-logic, and explore in detail two fragments that are
particularly relevant to Reverse Mathematics practice and automated theorem
proving of Reverse Mathematics results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0655</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0655</id><created>2014-01-03</created><updated>2014-02-01</updated><authors><author><keyname>Sims</keyname><forenames>Owen</forenames></author><author><keyname>Gilles</keyname><forenames>Robert P.</forenames></author></authors><title>Critical Nodes In Directed Networks</title><categories>cs.SI physics.soc-ph</categories><comments>28 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Critical nodes or &quot;middlemen&quot; have an essential place in both social and
economic networks when considering the flow of information and trade. This
paper extends the concept of critical nodes to directed networks. We identify
strong and weak middlemen. Node contestability is introduced as a form of
competition in networks; a duality between uncontested intermediaries and
middlemen is established. The brokerage power of middlemen is formally
expressed and a general algorithm is constructed to measure the brokerage power
of each node from the networks adjacency matrix. Augmentations of the brokerage
power measure are discussed to encapsulate relevant centrality measures. We use
these concepts to identify and measure middlemen in two empirical
socio-economic networks, the elite marriage network of Renaissance Florence and
Krackhardt's advice network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0660</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0660</id><created>2014-01-03</created><authors><author><keyname>Mery</keyname><forenames>Bruno</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Moot</keyname><forenames>Richard</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Retor&#xe9;</keyname><forenames>Christian</forenames><affiliation>LaBRI</affiliation></author></authors><title>Plurals: individuals and sets in a richly typed semantics</title><categories>cs.CL</categories><proxy>ccsd</proxy><journal-ref>LENSL'10 - 10th Workshop on Logic and Engineering of Natural
  Semantics of Language, Japanese Symposium for Artifitial Intelligence,
  International Society for AI - 2013 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We developed a type-theoretical framework for natural lan- guage semantics
that, in addition to the usual Montagovian treatment of compositional
semantics, includes a treatment of some phenomena of lex- ical semantic:
coercions, meaning, transfers, (in)felicitous co-predication. In this setting
we see how the various readings of plurals (collective, dis- tributive,
coverings,...) can be modelled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0670</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0670</id><created>2014-01-03</created><authors><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Zhang</keyname><forenames>Qinwei</forenames></author><author><keyname>Yuan</keyname><forenames>Jing</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author></authors><title>MRF denoising with compressed sensing and adaptive filtering</title><categories>cs.IT math.IT</categories><comments>4 pages,5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently proposed Magnetic Resonance Fingerprinting (MRF) technique can
simultaneously estimate multiple parameters through dictionary matching. It has
promising potentials in a wide range of applications. However, MRF introduces
errors due to undersampling during the data acquisition process and the limit
of dictionary resolution. In this paper, we investigate the error source of MRF
and propose the technologies of improving the quality of MRF with compressed
sensing, error prediction by decision trees, and adaptive filtering.
Experimental results support our observations and show significant improvement
of the proposed technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0684</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0684</id><created>2014-01-03</created><authors><author><keyname>Bekos</keyname><forenames>Michael A.</forenames></author><author><keyname>Gronemann</keyname><forenames>Martin</forenames></author><author><keyname>Raftopoulou</keyname><forenames>Chrysanthi N.</forenames></author></authors><title>Two-Page Book Embeddings of 4-Planar Graphs</title><categories>cs.DM cs.CC math.CO</categories><comments>21 pages, 16 Figures. A shorter version is to appear at STACS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Back in the Eighties, Heath showed that every 3-planar graph is
subhamiltonian and asked whether this result can be extended to a class of
graphs of degree greater than three. In this paper we affirmatively answer this
question for the class of 4-planar graphs. Our contribution consists of two
algorithms: The first one is limited to triconnected graphs, but runs in linear
time and uses existing methods for computing hamiltonian cycles in planar
graphs. The second one, which solves the general case of the problem, is a
quadratic-time algorithm based on the book-embedding viewpoint of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0689</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0689</id><created>2014-01-02</created><updated>2015-06-05</updated><authors><author><keyname>Roy</keyname><forenames>Ankush</forenames></author><author><keyname>Halder</keyname><forenames>Biswajit</forenames></author><author><keyname>Garain</keyname><forenames>Utpal</forenames></author><author><keyname>Doermann</keyname><forenames>David S.</forenames></author></authors><title>Machine Assisted Authentication of Paper Currency: an Experiment on
  Indian Banknotes</title><categories>cs.CV</categories><comments>There were several errors in the experimental section which we are
  looking into</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic authentication of paper money has been targeted. Indian bank notes
are taken as reference to show how a system can be developed for discriminating
fake notes from genuine ones. Image processing and pattern recognition
techniques are used to design the overall approach. The ability of the embedded
security aspects is thoroughly analysed for detecting fake currencies. Real
forensic samples are involved in the experiment that shows a high precision
machine can be developed for authentication of paper money. The system
performance is reported in terms of both accuracy and processing speed.
Comparison with human subjects namely forensic experts and bank staffs clearly
shows its applicability for mass checking of currency notes in the real world.
The analysis of security features to protect counterfeiting highlights some
facts that should be taken care of in future designing of currency notes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0694</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0694</id><created>2014-01-03</created><authors><author><keyname>Placzek</keyname><forenames>Bartlomiej</forenames></author><author><keyname>Bernas</keyname><forenames>Marcin</forenames></author></authors><title>Uncertainty-based information extraction in wireless sensor networks for
  control applications</title><categories>cs.NI</categories><comments>19 pages, 10 figures. Ad Hoc Networks 2013</comments><journal-ref>Ad Hoc Networks 14C (2014) pp. 106-117</journal-ref><doi>10.1016/j.adhoc.2013.11.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Design of control applications over wireless sensor networks (WSNs) is a
challenging issue due to the bandwidth-limited communication medium, energy
constraints and real-time data delivery requirements. This paper introduces a
new information extraction method for WSN-based control applications, which
reduces the number of required data transmissions to save energy and avoid data
congestion. According to the proposed approach, control applications recognize
when new data readings have to be collected and determine sensor nodes that
have to be activated on the basis of uncertainty analysis. Processing of the
selectively collected input data is based on definition of information granules
that describe state of the controlled system as well as performance of
particular control decisions. This method was implemented for object tracking
in WSNs. The task is to control movement of a mobile sink, which has to reach a
target in the shortest possible time. Extensive simulation experiments were
performed to compare performance of the proposed approach against
state-of-the-art methods. Results of the experiments show that the presented
information extraction method allows for substantial reduction in the amount of
transmitted data with no significant negative effect on tracking performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0699</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0699</id><created>2014-01-03</created><updated>2014-04-28</updated><authors><author><keyname>Makarychev</keyname><forenames>Konstantin</forenames></author><author><keyname>Makarychev</keyname><forenames>Yury</forenames></author></authors><title>Nonuniform Graph Partitioning with Unrelated Weights</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a bi-criteria approximation algorithm for the Minimum Nonuniform
Partitioning problem, recently introduced by Krauthgamer, Naor, Schwartz and
Talwar (2014). In this problem, we are given a graph $G=(V,E)$ on $n$ vertices
and $k$ numbers $\rho_1,\dots, \rho_k$. The goal is to partition the graph into
$k$ disjoint sets $P_1,\dots, P_k$ satisfying $|P_i|\leq \rho_i n$ so as to
minimize the number of edges cut by the partition. Our algorithm has an
approximation ratio of $O(\sqrt{\log n \log k})$ for general graphs, and an
$O(1)$ approximation for graphs with excluded minors. This is an improvement
upon the $O(\log n)$ algorithm of Krauthgamer, Naor, Schwartz and Talwar
(2014). Our approximation ratio matches the best known ratio for the Minimum
(Uniform) $k$-Partitioning problem.
  We extend our results to the case of &quot;unrelated weights&quot; and to the case of
&quot;unrelated $d$-dimensional weights&quot;. In the former case, different vertices may
have different weights and the weight of a vertex may depend on the set $P_i$
the vertex is assigned to. In the latter case, each vertex $u$ has a
$d$-dimensional weight $r(u,i) = (r_1(u,i), \dots, r_d(u,i))$ if $u$ is
assigned to $P_i$. Each set $P_i$ has a $d$-dimensional capacity $c(i) =
(c_1(i),\dots, c_d(i))$. The goal is to find a partition such that $\sum_{u\in
{P_i}} r(u,i) \leq c(i)$ coordinate-wise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0702</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0702</id><created>2014-01-03</created><updated>2015-09-19</updated><authors><author><keyname>Cafaro</keyname><forenames>Massimo</forenames></author><author><keyname>Pulimeno</keyname><forenames>Marco</forenames></author><author><keyname>Tempesta</keyname><forenames>Piergiulio</forenames></author></authors><title>A Parallel Space Saving Algorithm For Frequent Items and the Hurwitz
  zeta distribution</title><categories>cs.DS</categories><comments>Accepted for publication. To appear in Information Sciences,
  Elsevier. http://www.sciencedirect.com/science/article/pii/S002002551500657X</comments><journal-ref>Information Sciences, Elsevier, Volume 329, 2016, pp. 1 - 19,
  ISSN: 0020-0255</journal-ref><doi>10.1016/j.ins.2015.09.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a message-passing based parallel version of the Space Saving
algorithm designed to solve the $k$--majority problem. The algorithm determines
in parallel frequent items, i.e., those whose frequency is greater than a given
threshold, and is therefore useful for iceberg queries and many other different
contexts. We apply our algorithm to the detection of frequent items in both
real and synthetic datasets whose probability distribution functions are a
Hurwitz and a Zipf distribution respectively. Also, we compare its parallel
performances and accuracy against a parallel algorithm recently proposed for
merging summaries derived by the Space Saving or Frequent algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0704</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0704</id><created>2014-01-03</created><updated>2014-06-26</updated><authors><author><keyname>Berth&#xe9;</keyname><forenames>Val&#xe9;rie</forenames></author><author><keyname>Bourdon</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author><author><keyname>Jolivet</keyname><forenames>Timo</forenames></author><author><keyname>Siegel</keyname><forenames>Anne</forenames></author></authors><title>A combinatorial approach to products of Pisot substitutions</title><categories>math.DS cs.DM math.CO</categories><comments>32 pages, v2 with many corrections and improvements</comments><doi>10.1017/etds.2014.141</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a generic algorithmic framework to prove pure discrete spectrum for
the substitutive symbolic dynamical systems associated with some infinite
families of Pisot substitutions. We focus on the families obtained as finite
products of the three-letter substitutions associated with the multidimensional
continued fraction algorithms of Brun and Jacobi-Perron.
  Our tools consist in a reformulation of some combinatorial criteria
(coincidence conditions), in terms of properties of discrete plane generation
using multidimensional (dual) substitutions. We also deduce some topological
and dynamical properties of the Rauzy fractals, of the underlying symbolic
dynamical systems, as well as some number-theoretical properties of the
associated Pisot numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0705</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0705</id><created>2014-01-03</created><updated>2014-06-26</updated><authors><author><keyname>Jolivet</keyname><forenames>Timo</forenames></author><author><keyname>Kari</keyname><forenames>Jarkko</forenames></author></authors><title>Undecidable properties of self-affine sets and multi-tape automata</title><categories>cs.FL math.CO math.DS</categories><comments>10 pages, v2 includes some corrections to match the published version</comments><journal-ref>MFCS 2014, conference proccedings LNCS 8634, 352-364</journal-ref><doi>10.1007/978-3-662-44522-8_30</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the decidability of the topological properties of some objects
coming from fractal geometry. We prove that having empty interior is
undecidable for the sets defined by two-dimensional graph-directed iterated
function systems. These results are obtained by studying a particular class of
self-affine sets associated with multi-tape automata. We first establish the
undecidability of some language-theoretical properties of such automata, which
then translate into undecidability results about their associated self-affine
sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0708</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0708</id><created>2014-01-03</created><authors><author><keyname>Rama</keyname><forenames>Taraka</forenames></author><author><keyname>Kolachina</keyname><forenames>Sudheer</forenames></author><author><keyname>B</keyname><forenames>Lakshmi Bai</forenames></author></authors><title>Quantitative methods for Phylogenetic Inference in Historical
  Linguistics: An experimental case study of South Central Dravidian</title><categories>cs.CL cs.AI</categories><journal-ref>Indian Linguistics, Volume 70, 2009</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we examine the usefulness of two classes of algorithms Distance
Methods, Discrete Character Methods (Felsenstein and Felsenstein 2003) widely
used in genetics, for predicting the family relationships among a set of
related languages and therefore, diachronic language change. Applying these
algorithms to the data on the numbers of shared cognates- with-change and
changed as well as unchanged cognates for a group of six languages belonging to
a Dravidian language sub-family given in Krishnamurti et al. (1983), we
observed that the resultant phylogenetic trees are largely in agreement with
the linguistic family tree constructed using the comparative method of
reconstruction with only a few minor differences. Furthermore, we studied these
minor differences and found that they were cases of genuine ambiguity even for
a well-trained historical linguist. We evaluated the trees obtained through our
experiments using a well-defined criterion and report the results here. We
finally conclude that quantitative methods like the ones we examined are quite
useful in predicting family relationships among languages. In addition, we
conclude that a modest degree of confidence attached to the intuition that
there could indeed exist a parallelism between the processes of linguistic and
genetic change is not totally misplaced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0711</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0711</id><created>2014-01-03</created><updated>2014-03-21</updated><authors><author><keyname>Chattopadhyay</keyname><forenames>Ishanu</forenames></author><author><keyname>Lipson</keyname><forenames>Hod</forenames></author></authors><title>Computing Entropy Rate Of Symbol Sources &amp; A Distribution-free Limit
  Theorem</title><categories>cs.IT cs.LG math.IT math.PR stat.CO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entropy rate of sequential data-streams naturally quantifies the complexity
of the generative process. Thus entropy rate fluctuations could be used as a
tool to recognize dynamical perturbations in signal sources, and could
potentially be carried out without explicit background noise characterization.
However, state of the art algorithms to estimate the entropy rate have markedly
slow convergence; making such entropic approaches non-viable in practice. We
present here a fundamentally new approach to estimate entropy rates, which is
demonstrated to converge significantly faster in terms of input data lengths,
and is shown to be effective in diverse applications ranging from the
estimation of the entropy rate of English texts to the estimation of complexity
of chaotic dynamical systems. Additionally, the convergence rate of entropy
estimates do not follow from any standard limit theorem, and reported
algorithms fail to provide any confidence bounds on the computed values.
Exploiting a connection to the theory of probabilistic automata, we establish a
convergence rate of $O(\log \vert s \vert/\sqrt[3]{\vert s \vert})$ as a
function of the input length $\vert s \vert$, which then yields explicit
uncertainty estimates, as well as required data lengths to satisfy
pre-specified confidence bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0730</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0730</id><created>2014-01-03</created><updated>2014-11-02</updated><authors><author><keyname>Iscen</keyname><forenames>Ahmet</forenames></author><author><keyname>Armagan</keyname><forenames>Anil</forenames></author><author><keyname>Duygulu</keyname><forenames>Pinar</forenames></author></authors><title>What is usual in unusual videos? Trajectory snippet histograms for
  discovering unusualness</title><categories>cs.CV</categories><journal-ref>Computer Vision and Pattern Recognition Workshops (CVPRW), 2014
  IEEE Conference on</journal-ref><doi>10.1109/CVPRW.2014.123</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unusual events are important as being possible indicators of undesired
consequences. Moreover, unusualness in everyday life activities may also be
amusing to watch as proven by the popularity of such videos shared in social
media. Discovery of unusual events in videos is generally attacked as a problem
of finding usual patterns, and then separating the ones that do not resemble to
those. In this study, we address the problem from the other side, and try to
answer what type of patterns are shared among unusual videos that make them
resemble to each other regardless of the ongoing event. With this challenging
problem at hand, we propose a novel descriptor to encode the rapid motions in
videos utilizing densely extracted trajectories. The proposed descriptor, which
is referred to as trajectory snipped histograms, is used to distinguish unusual
videos from usual videos, and further exploited to discover snapshots in which
unusualness happen. Experiments on domain specific people falling videos and
unrestricted funny videos show the effectiveness of our method in capturing
unusualness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0733</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0733</id><created>2014-01-03</created><updated>2014-10-29</updated><authors><author><keyname>Iscen</keyname><forenames>Ahmet</forenames></author><author><keyname>Golge</keyname><forenames>Eren</forenames></author><author><keyname>Sarac</keyname><forenames>Ilker</forenames></author><author><keyname>Duygulu</keyname><forenames>Pinar</forenames></author></authors><title>ConceptVision: A Flexible Scene Classification Framework</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce ConceptVision, a method that aims for high accuracy in
categorizing large number of scenes, while keeping the model relatively simpler
and efficient for scalability. The proposed method combines the advantages of
both low-level representations and high-level semantic categories, and
eliminates the distinctions between different levels through the definition of
concepts. The proposed framework encodes the perspectives brought through
different concepts by considering them in concept groups. Different
perspectives are ensembled for the final decision. Extensive experiments are
carried out on benchmark datasets to test the effects of different concepts,
and methods used to ensemble. Comparisons with state-of-the-art studies show
that we can achieve better results with incorporation of concepts in different
levels with different perspectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0734</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0734</id><created>2014-01-03</created><authors><author><keyname>Asteris</keyname><forenames>Megasthenis</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author></authors><title>Repairable Fountain Codes</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Journal on Selected Areas in Communications, Issue
  on Communication Methodologies for Next-Generation Storage Systems 2013, 11
  pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new family of Fountain codes that are systematic and also have
sparse parities. Given an input of $k$ symbols, our codes produce an unbounded
number of output symbols, generating each parity independently by linearly
combining a logarithmic number of randomly selected input symbols. The
construction guarantees that for any $\epsilon&gt;0$ accessing a random subset of
$(1+\epsilon)k$ encoded symbols, asymptotically suffices to recover the $k$
input symbols with high probability.
  Our codes have the additional benefit of logarithmic locality: a single lost
symbol can be repaired by accessing a subset of $O(\log k)$ of the remaining
encoded symbols. This is a desired property for distributed storage systems
where symbols are spread over a network of storage nodes. Beyond recovery upon
loss, local reconstruction provides an efficient alternative for reading
symbols that cannot be accessed directly. In our code, a logarithmic number of
disjoint local groups is associated with each systematic symbol, allowing
multiple parallel reads.
  Our main mathematical contribution involves analyzing the rank of sparse
random matrices with specific structure over finite fields. We rely on
establishing that a new family of sparse random bipartite graphs have perfect
matchings with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0742</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0742</id><created>2014-01-03</created><authors><author><keyname>Chattopadhyay</keyname><forenames>Ishanu</forenames></author><author><keyname>Lipson</keyname><forenames>Hod</forenames></author></authors><title>Data Smashing</title><categories>cs.LG cs.AI cs.CE cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Investigation of the underlying physics or biology from empirical data
requires a quantifiable notion of similarity - when do two observed data sets
indicate nearly identical generating processes, and when they do not. The
discriminating characteristics to look for in data is often determined by
heuristics designed by experts, $e.g.$, distinct shapes of &quot;folded&quot; lightcurves
may be used as &quot;features&quot; to classify variable stars, while determination of
pathological brain states might require a Fourier analysis of brainwave
activity. Finding good features is non-trivial. Here, we propose a universal
solution to this problem: we delineate a principle for quantifying similarity
between sources of arbitrary data streams, without a priori knowledge, features
or training. We uncover an algebraic structure on a space of symbolic models
for quantized data, and show that such stochastic generators may be added and
uniquely inverted; and that a model and its inverse always sum to the generator
of flat white noise. Therefore, every data stream has an anti-stream: data
generated by the inverse model. Similarity between two streams, then, is the
degree to which one, when summed to the other's anti-stream, mutually
annihilates all statistical structure to noise. We call this data smashing. We
present diverse applications, including disambiguation of brainwaves pertaining
to epileptic seizures, detection of anomalous cardiac rhythms, and
classification of astronomical objects from raw photometry. In our examples,
the data smashing principle, without access to any domain knowledge, meets or
exceeds the performance of specialized algorithms tuned by domain experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0750</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0750</id><created>2014-01-03</created><updated>2014-08-23</updated><authors><author><keyname>Qi</keyname><forenames>Junjian</forenames></author><author><keyname>Sun</keyname><forenames>Kai</forenames></author><author><keyname>Mei</keyname><forenames>Shengwei</forenames></author></authors><title>An Interaction Model for Simulation and Mitigation of Cascading Failures</title><categories>cs.SY physics.soc-ph</categories><comments>Accepted by IEEE Transactions on Power Systems</comments><doi>10.1109/TPWRS.2014.2337284</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the interactions between component failures are quantified and
the interaction matrix and interaction network are obtained. The quantified
interactions can capture the general propagation patterns of the cascades from
utilities or simulation, thus helping to better understand how cascading
failures propagate and to identify key links and key components that are
crucial for cascading failure propagation. By utilizing these interactions a
high-level probabilistic model called interaction model is proposed to study
the influence of interactions on cascading failure risk and to support online
decision-making. It is much more time efficient to first quantify the
interactions between component failures with fewer original cascades from a
more detailed cascading failure model and then perform the interaction model
simulation than it is to directly simulate a large number of cascades with a
more detailed model. Interaction-based mitigation measures are suggested to
mitigate cascading failure risk by weakening key links, which can be achieved
in real systems by wide area protection such as blocking of some specific
protective relays. The proposed interaction quantifying method and interaction
model are validated with line outage data generated by the AC OPA cascading
simulations on the IEEE 118-bus system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0758</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0758</id><created>2014-01-03</created><authors><author><keyname>Snook</keyname><forenames>Aaron</forenames></author><author><keyname>Schoenebeck</keyname><forenames>Grant</forenames></author><author><keyname>Codenotti</keyname><forenames>Paolo</forenames></author></authors><title>Graph Isomorphism and the Lasserre Hierarchy</title><categories>cs.CC math.CO</categories><comments>22 pages, 3 figures, submitted to CCC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show lower bounds for a certain large class of algorithms
solving the Graph Isomorphism problem, even on expander graph instances.
Spielman [25] shows an algorithm for isomorphism of strongly regular expander
graphs that runs in time exp(O(n^(1/3)) (this bound was recently improved to
expf O(n^(1/5) [5]). It has since been an open question to remove the
requirement that the graph be strongly regular. Recent algorithmic results show
that for many problems the Lasserre hierarchy works surprisingly well when the
underlying graph has expansion properties. Moreover, recent work of Atserias
and Maneva [3] shows that k rounds of the Lasserre hierarchy is a
generalization of the k-dimensional Weisfeiler-Lehman algorithm for Graph
Isomorphism. These two facts combined make the Lasserre hierarchy a good
candidate for solving graph isomorphism on expander graphs. Our main result
rules out this promising direction by showing that even Omega(n) rounds of the
Lasserre semide?nite program hierarchy fail to solve the Graph Isomorphism
problem even on expander graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0763</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0763</id><created>2014-01-03</created><authors><author><keyname>Mittal</keyname><forenames>Sparsh</forenames></author></authors><title>A Study of Successive Over-relaxation Method Parallelization Over Modern
  HPC Languages</title><categories>cs.DC cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Successive over-relaxation (SOR) is a computationally intensive, yet
extremely important iterative solver for solving linear systems. Due to recent
trends of exponential growth in amount of data generated and increasing problem
sizes, serial platforms have proved to be insufficient in providing the
required computational power. In this paper, we present parallel
implementations of red-black SOR method using three modern programming
languages namely Chapel, D and Go. We employ SOR method for solving 2D
steady-state heat conduction problem. We discuss the optimizations incorporated
and the features of these languages which are crucial for improving the program
performance. Experiments have been performed using 2, 4, and 8 threads and
performance results are compared with serial execution. The analysis of results
provides important insights into working of SOR method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0764</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0764</id><created>2014-01-03</created><authors><author><keyname>Li</keyname><forenames>Xi</forenames></author><author><keyname>Hu</keyname><forenames>Weiming</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Dick</keyname><forenames>Anthony</forenames></author><author><keyname>Zhang</keyname><forenames>Zhongfei</forenames></author></authors><title>Context-Aware Hypergraph Construction for Robust Spectral Clustering</title><categories>cs.CV cs.LG</categories><comments>10 pages. Appearing in IEEE TRANSACTIONS ON KNOWLEDGE AND DATA
  ENGINEERING: http://doi.ieeecomputersociety.org/10.1109/TKDE.2013.126</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral clustering is a powerful tool for unsupervised data analysis. In
this paper, we propose a context-aware hypergraph similarity measure (CAHSM),
which leads to robust spectral clustering in the case of noisy data. We
construct three types of hypergraph---the pairwise hypergraph, the
k-nearest-neighbor (kNN) hypergraph, and the high-order over-clustering
hypergraph. The pairwise hypergraph captures the pairwise similarity of data
points; the kNN hypergraph captures the neighborhood of each point; and the
clustering hypergraph encodes high-order contexts within the dataset. By
combining the affinity information from these three hypergraphs, the CAHSM
algorithm is able to explore the intrinsic topological information of the
dataset. Therefore, data clustering using CAHSM tends to be more robust.
Considering the intra-cluster compactness and the inter-cluster separability of
vertices, we further design a discriminative hypergraph partitioning criterion
(DHPC). Using both CAHSM and DHPC, a robust spectral clustering algorithm is
developed. Theoretical analysis and experimental evaluation demonstrate the
effectiveness and robustness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0765</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0765</id><created>2014-01-03</created><authors><author><keyname>Mittal</keyname><forenames>Sparsh</forenames></author></authors><title>A Survey of Techniques For Improving Energy Efficiency in Embedded
  Computing Systems</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent technological advances have greatly improved the performance and
features of embedded systems. With the number of just mobile devices now
reaching nearly equal to the population of earth, embedded systems have truly
become ubiquitous. These trends, however, have also made the task of managing
their power consumption extremely challenging. In recent years, several
techniques have been proposed to address this issue. In this paper, we survey
the techniques for managing power consumption of embedded systems. We discuss
the need of power management and provide a classification of the techniques on
several important parameters to highlight their similarities and differences.
This paper is intended to help the researchers and application-developers in
gaining insights into the working of power management techniques and designing
even more efficient high-performance embedded systems of tomorrow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0767</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0767</id><created>2014-01-03</created><authors><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Liu</keyname><forenames>Fayao</forenames></author></authors><title>From Kernel Machines to Ensemble Learning</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensemble methods such as boosting combine multiple learners to obtain better
prediction than could be obtained from any individual learner. Here we propose
a principled framework for directly constructing ensemble learning methods from
kernel methods. Unlike previous studies showing the equivalence between
boosting and support vector machines (SVMs), which needs a translation
procedure, we show that it is possible to design boosting-like procedure to
solve the SVM optimization problems.
  In other words, it is possible to design ensemble methods directly from SVM
without any middle procedure.
  This finding not only enables us to design new ensemble learning methods
directly from kernel methods, but also makes it possible to take advantage of
those highly-optimized fast linear SVM solvers for ensemble learning.
  We exemplify this framework for designing binary ensemble learning as well as
a new multi-class ensemble learning methods.
  Experimental results demonstrate the flexibility and usefulness of the
proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0778</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0778</id><created>2014-01-04</created><authors><author><keyname>Shen</keyname><forenames>Hua-Wei</forenames></author><author><keyname>Wang</keyname><forenames>Dashun</forenames></author><author><keyname>Song</keyname><forenames>Chaoming</forenames></author><author><keyname>Barab&#xe1;si</keyname><forenames>Albert-L&#xe1;szl&#xf3;</forenames></author></authors><title>Modeling and Predicting Popularity Dynamics via Reinforced Poisson
  Processes</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages, 5 figure; 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An ability to predict the popularity dynamics of individual items within a
complex evolving system has important implications in an array of areas. Here
we propose a generative probabilistic framework using a reinforced Poisson
process to model explicitly the process through which individual items gain
their popularity. This model distinguishes itself from existing models via its
capability of modeling the arrival process of popularity and its remarkable
power at predicting the popularity of individual items. It possesses the
flexibility of applying Bayesian treatment to further improve the predictive
power using a conjugate prior. Extensive experiments on a longitudinal citation
dataset demonstrate that this model consistently outperforms existing
popularity prediction methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0781</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0781</id><created>2014-01-04</created><authors><author><keyname>Zheng</keyname><forenames>Zizhan</forenames></author><author><keyname>Lu</keyname><forenames>Zhixue</forenames></author><author><keyname>Sinha</keyname><forenames>Prasun</forenames></author><author><keyname>Kumar</keyname><forenames>Santosh</forenames></author></authors><title>Ensuring Predictable Contact Opportunity for Scalable Vehicular Internet
  Access On the Go</title><categories>cs.NI</categories><comments>Technical report - this work was submitted to IEEE/ACM Transactions
  on Networking, June 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With increasing popularity of media enabled hand-helds and their integration
with the in-vehicle entertainment systems, the need for high data-rate services
for mobile users on the go is evident. This ever-increasing demand of data is
constantly surpassing what cellular networks can economically support.
Large-scale Wireless LANs (WLANs) can provide such a service, but they are
expensive to deploy and maintain. Open WLAN access-points, on the other hand,
need no new deployments, but can offer only opportunistic services, lacking any
performance guarantees. In contrast, a carefully planned sparse deployment of
roadside WiFi provides an economically scalable infrastructure with quality of
service assurance to mobile users. In this paper, we present a new metric,
called Contact Opportunity, to closely model the quality of data service that a
mobile user might experience when driving through the system. We then present
efficient deployment algorithms for minimizing the cost for ensuring a required
level of contact opportunity. We further extend this concept and the deployment
techniques to a more intuitive metric -- the average throughput -- by taking
various dynamic elements into account. Simulations over a real road network and
experimental results show that our approach achieves significantly better cost
vs. throughput tradeoff in both the worst case and average case compared with
some commonly used deployment algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0794</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0794</id><created>2014-01-04</created><authors><author><keyname>Rama</keyname><forenames>Taraka</forenames></author><author><keyname>Borin</keyname><forenames>Lars</forenames></author></authors><title>Properties of phoneme N -grams across the world's language families</title><categories>cs.CL stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we investigate the properties of phoneme N-grams across half
of the world's languages. We investigate if the sizes of three different N-gram
distributions of the world's language families obey a power law. Further, the
N-gram distributions of language families parallel the sizes of the families,
which seem to obey a power law distribution. The correlation between N-gram
distributions and language family sizes improves with increasing values of N.
We applied statistical tests, originally given by physicists, to test the
hypothesis of power law fit to twelve different datasets. The study also raises
some new questions about the use of N-gram distributions in linguistic
research, which we answer by running a statistical test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0798</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0798</id><created>2014-01-04</created><updated>2014-07-10</updated><authors><author><keyname>Vabishchevich</keyname><forenames>Petr</forenames></author><author><keyname>Zakharov</keyname><forenames>Petr</forenames></author></authors><title>Domain decomposition methods with overlapping subdomains for
  time-dependent problems</title><categories>cs.NA</categories><comments>8 pages</comments><msc-class>65N06, 65M06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain decomposition (DD) methods for solving time-dependent problems can be
classified by (i) the method of domain decomposition used, (ii) the choice of
decomposition operators (exchange of boundary conditions), and (iii) the
splitting scheme employed. To construct homogeneous numerical algorithms,
overlapping subdomain methods are preferable. Domain decomposition is
associated with the corresponding additive representation of the problem
operator. To solve time-dependent problems with the DD splitting, different
operator-splitting schemes are used. Various variants of decomposition
operators differ by distinct types of data exchanges on interfaces. They ensure
the convergence of the approximate solution in various spaces of grid
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0799</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0799</id><created>2014-01-04</created><authors><author><keyname>Kretz</keyname><forenames>Tobias</forenames></author><author><keyname>Lehmann</keyname><forenames>Karsten</forenames></author><author><keyname>Hofs&#xe4;&#xdf;</keyname><forenames>Ingmar</forenames></author></authors><title>User Equilibrium Route Assignment for Microscopic Pedestrian Simulation</title><categories>physics.soc-ph cs.CE cs.MA cs.RO physics.comp-ph</categories><journal-ref>Advances in Complex Systems 17(2) pp. 1450010 (2014)</journal-ref><doi>10.1142/S0219525914500106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the simulation of pedestrians a method is introduced to find routing
alternatives from any origin position to a given destination area in a given
geometry composed of walking areas and obstacles. The method includes a
parameter which sets a threshold for the approximate minimum size of obstacles
to generate routing alternatives. The resulting data structure for navigation
is constructed such that it does not introduce artifacts to the movement of
simulated pedestrians and that locally pedestrians prefer to walk on the
shortest path. The generated set of routes can be used with iterating static or
dynamic assignment methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0802</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0802</id><created>2014-01-04</created><authors><author><keyname>Voskoglou</keyname><forenames>Michael Gr.</forenames></author></authors><title>A stochastic model for Case-Based Reasoning</title><categories>cs.AI math.PR</categories><comments>7 pages, 2 figures</comments><msc-class>Primary: 68T20, Secondary: 60J20</msc-class><journal-ref>Journal of Mathematical Modelling and Application, 1(3), 33-39,
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Case-Bsed Reasoning (CBR) is a recent theory for problem-solving and learning
in computers and people.Broadly construed it is the process of solving new
problems based on the solution of similar past problems. In the present paper
we introduce an absorbing Markov chain on the main steps of the CBR process.In
this way we succeed in obtaining the probabilities for the above process to be
in a certain step at a certain phase of the solution of the corresponding
problem, and a measure for the efficiency of a CBR system. Examples are given
to illustrate our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0818</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0818</id><created>2014-01-04</created><authors><author><keyname>Huo</keyname><forenames>Qiang</forenames></author><author><keyname>Liu</keyname><forenames>Tianxi</forenames></author><author><keyname>Sun</keyname><forenames>Shaohui</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Jiao</keyname><forenames>Bingli</forenames></author></authors><title>Selective Combining for Hybrid Cooperative Networks</title><categories>cs.IT math.IT</categories><comments>27 pages, 8 figures, IET Communications, 2014</comments><journal-ref>IET Communication, Vol. 8, Iss. 4, pp. 471--482, 2014</journal-ref><doi>10.1049/iet-com.2013.0323</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we consider the selective combining in hybrid cooperative
networks (SCHCNs scheme) with one source node, one destination node and $N$
relay nodes. In the SCHCN scheme, each relay first adaptively chooses between
amplify-and-forward protocol and decode-and-forward protocol on a per frame
basis by examining the error-detecting code result, and $N_c$ ($1\leq N_c \leq
N$) relays will be selected to forward their received signals to the
destination. We first develop a signal-to-noise ratio (SNR) threshold-based
frame error rate (FER) approximation model. Then, the theoretical FER
expressions for the SCHCN scheme are derived by utilizing the proposed SNR
threshold-based FER approximation model. The analytical FER expressions are
validated through simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0821</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0821</id><created>2014-01-04</created><authors><author><keyname>Pradhan</keyname><forenames>Rajkumar</forenames></author><author><keyname>Pal</keyname><forenames>Madhumangal</forenames></author></authors><title>Intuitionistic Fuzzy Linear Transformations</title><categories>cs.DM</categories><comments>12 pages,24 references</comments><msc-class>08A72, 15B15</msc-class><journal-ref>Annals of Pure and Applied Mathematics, vol. 1, no. 1, (2012)
  57-68</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discussed about the intuitionistic fuzzy linear
transformations (IFLT) and shown that the set of all linear transformations
L(V) defined over an intuitionistic fuzzy vector space V does not form an
vector space. Here we determine the unique intuitionistic fuzzy matrix
associated with an intuitionistic fuzzy linear transformation with respect to
an ordered standard basis for an intuitionistic fuzzy vector space. We
introduced the concept of the inverse of an IFLT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0823</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0823</id><created>2014-01-04</created><authors><author><keyname>Rashmanlou</keyname><forenames>Hossein</forenames></author><author><keyname>Pal</keyname><forenames>Madhumangal</forenames></author></authors><title>Antipodal Interval-Valued Fuzzy Graphs</title><categories>cs.DM</categories><comments>24 pages; 9 figures; 32 referenes</comments><journal-ref>International Journal of Applications of Fuzzy Sets and Artificial
  Intelligence (ISSN 2241-1240), Vol. 3 ( 2013), 107-130</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concepts of graph theory have applications in many areas of computer science
including data mining, image segmentation, clustering, image capturing,
networks, etc . An interval-valued fuzzy set is a generalization of the notion
of a fuzzy set. Interval-valued fuzzy models give more precision, flexibility
and compatibility to the system as compared to the fuzzy models. In this paper,
we introduce the concept of antipodal interval - valued fuzzy graph and self
median interval-valued fuzzy graph of the given interval-valued fuzzy graph. We
investigate isomorphism properties of antipodal interval - valued fuzzy graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0827</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0827</id><created>2014-01-04</created><authors><author><keyname>Derouiche</keyname><forenames>K. I. A.</forenames></author></authors><title>Interaction entre math\'ematique et informatique Libre/Open Source par
  le logiciel math\'ematique</title><categories>cs.MS cs.CY math.HO</categories><comments>11 pages, written in French, In Proceedings of the S\'eminaire
  National sur la didactique des Math\'ematiques, 25-26 Novembre Tebessa,
  Alg\'erie (SNDM'13). 2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This article focuses on the application of model development and opening the
source code available and implemented by the Free Software and Open Source
FLOSS to the instructional and teaching has both mathematics and computer by
the read-write(R/W) of mathematical software, including the most famous cases
are numerical and symbolic computation. The article analysis the development of
the mathematical model of Free/Open Source(math FLOSS) software has proven its
importance in the area of research in mathematics and computer science .
However, although their actual use, is very readable in higher education
courses. We discuss the feasibility of this model to the characteristics of the
domain, actors, interaction they have and the communities they form during the
development of the software. Finally, we propose a mathematical example of
Free/Open Source(Math FlOSS) software as analysis device .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0836</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0836</id><created>2014-01-04</created><authors><author><keyname>Petrosyan</keyname><forenames>Petros A.</forenames></author></authors><title>Sequential edge-coloring on the subset of vertices of almost regular
  graphs</title><categories>math.CO cs.DM</categories><comments>4 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Let $G$ be a graph and $R\subseteq V(G)$. A proper edge-coloring of a graph
$G$ with colors $1,\ldots,t$ is called an $R$-sequential $t$-coloring if the
edges incident to each vertex $v\in R$ are colored by the colors
$1,\ldots,d_{G}(v)$, where $d_{G}(v)$ is the degree of the vertex $v$ in $G$.
In this note, we show that if $G$ is a graph with $\Delta(G)-\delta(G)\leq 1$
and $\chi^{\prime}(G)=\Delta(G)=r$ ($r\geq 3$), then $G$ has an $R$-sequential
$r$-coloring with $\vert R\vert \geq
\left\lceil\frac{(r-1)n_{r}+n}{r}\right\rceil$, where $n=\vert V(G)\vert$ and
$n_{r}=\vert\{v\in V(G):d_{G}(v)=r\}\vert$. As a corollary, we obtain the
following result: if $G$ is a graph with $\Delta(G)-\delta(G)\leq 1$ and
$\chi^{\prime}(G)=\Delta(G)=r$ ($r\geq 3$), then $\Sigma^{\prime}(G)\leq
\left\lfloor\frac {2n_{r}(2r-1)+n(r-1)(r^{2}+2r-2)}{4r}\right\rfloor$, where
$\Sigma^{\prime}(G)$ is the edge-chromatic sum of $G$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0839</identifier>
 <datestamp>2014-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0839</id><created>2014-01-04</created><updated>2014-08-15</updated><authors><author><keyname>Javarone</keyname><forenames>Marco Alberto</forenames></author></authors><title>Social Influences in Opinion Dynamics: the Role of Conformity</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>22 pages, 12 figures, appears in Physica A: Statistical Mechanics and
  its Applications (volume 414) 2014</comments><doi>10.1016/j.physa.2014.07.018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the effects of social influences in opinion dynamics. In particular,
we define a simple model, based on the majority rule voting, in order to
consider the role of conformity. Conformity is a central issue in social
psychology as it represents one of people's behaviors that emerges as a result
of their interactions. The proposed model represents agents, arranged in a
network and provided with an individual behavior, that change opinion in
function of those of their neighbors. In particular, agents can behave as
conformists or as nonconformists. In the former case, agents change opinion in
accordance with the majority of their social circle (i.e., their neighbors); in
the latter case, they do the opposite, i.e., they take the minority opinion.
Moreover, we investigate the nonconformity both on a global and on a local
perspective, i.e., in relation to the whole population and to the social circle
of each nonconformist agent, respectively. We perform a computational study of
the proposed model, with the aim to observe if and how the conformity affects
the related outcomes. Moreover, we want to investigate whether it is possible
to achieve some kind of equilibrium, or of order, during the evolution of the
system. Results highlight that the amount of nonconformist agents in the
population plays a central role in these dynamics. In particular, conformist
agents play the role of stabilizers in fully-connected networks, whereas the
opposite happens in complex networks. Furthermore, by analyzing complex
topologies of the agent network, we found that in the presence of radical
nonconformist agents the topology of the system has a prominent role; otherwise
it does not matter since we observed that a conformist behavior is almost
always more convenient. Finally, we analyze the results of the model by
considering that agents can change also their behavior over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0843</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0843</id><created>2014-01-04</created><authors><author><keyname>Scott</keyname><forenames>Warren R.</forenames></author><author><keyname>Powell</keyname><forenames>Warren B.</forenames></author><author><keyname>Moazehi</keyname><forenames>Somayeh</forenames></author></authors><title>Least Squares Policy Iteration with Instrumental Variables vs. Direct
  Policy Search: Comparison Against Optimal Benchmarks Using Energy Storage</title><categories>math.OC cs.LG</categories><comments>37 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies approximate policy iteration (API) methods which use
least-squares Bellman error minimization for policy evaluation. We address
several of its enhancements, namely, Bellman error minimization using
instrumental variables, least-squares projected Bellman error minimization, and
projected Bellman error minimization using instrumental variables. We prove
that for a general discrete-time stochastic control problem, Bellman error
minimization using instrumental variables is equivalent to both variants of
projected Bellman error minimization. An alternative to these API methods is
direct policy search based on knowledge gradient. The practical performance of
these three approximate dynamic programming methods are then investigated in
the context of an application in energy storage, integrated with an
intermittent wind energy supply to fully serve a stochastic time-varying
electricity demand. We create a library of test problems using real-world data
and apply value iteration to find their optimal policies. These benchmarks are
then used to compare the developed policies. Our analysis indicates that API
with instrumental variables Bellman error minimization prominently outperforms
API with least-squares Bellman error minimization. However, these approaches
underperform our direct policy search implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0852</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0852</id><created>2014-01-04</created><updated>2015-01-04</updated><authors><author><keyname>Aragam</keyname><forenames>Bryon</forenames></author><author><keyname>Zhou</keyname><forenames>Qing</forenames></author></authors><title>Concave Penalized Estimation of Sparse Gaussian Bayesian Networks</title><categories>stat.ME cs.LG stat.ML</categories><comments>57 pages</comments><journal-ref>Journal of Machine Learning Research 16(Nov):2273-2328, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a penalized likelihood estimation framework to estimate the
structure of Gaussian Bayesian networks from observational data. In contrast to
recent methods which accelerate the learning problem by restricting the search
space, our main contribution is a fast algorithm for score-based structure
learning which does not restrict the search space in any way and works on
high-dimensional datasets with thousands of variables. Our use of concave
regularization, as opposed to the more popular $\ell_0$ (e.g. BIC) penalty, is
new. Moreover, we provide theoretical guarantees which generalize existing
asymptotic results when the underlying distribution is Gaussian. Most notably,
our framework does not require the existence of a so-called faithful DAG
representation, and as a result the theory must handle the inherent
nonidentifiability of the estimation problem in a novel way. Finally, as a
matter of independent interest, we provide a comprehensive comparison of our
approach to several standard structure learning methods using open-source
packages developed for the R language. Based on these experiments, we show that
our algorithm is significantly faster than other competing methods while
obtaining higher sensitivity with comparable false discovery rates for
high-dimensional data. In particular, the total runtime for our method to
generate a solution path of 20 estimates for DAGs with 8000 nodes is around one
hour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0855</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0855</id><created>2014-01-04</created><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Andrepoulos</keyname><forenames>Yiannis</forenames></author><author><keyname>Xiao</keyname><forenames>Yuanzhang</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Non-stationary Resource Allocation Policies for Delay-constrained Video
  Streaming: Application to Video over Internet-of-Things-enabled Networks</title><categories>cs.NI cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the high bandwidth requirements and stringent delay constraints of
multi-user wireless video transmission applications, ensuring that all video
senders have sufficient transmission opportunities to use before their delay
deadlines expire is a longstanding research problem. We propose a novel
solution that addresses this problem without assuming detailed packet-level
knowledge, which is unavailable at resource allocation time. Instead, we
translate the transmission delay deadlines of each sender's video packets into
a monotonically-decreasing weight distribution within the considered time
horizon. Higher weights are assigned to the slots that have higher probability
for deadline-abiding delivery. Given the sets of weights of the senders' video
streams, we propose the low-complexity Delay-Aware Resource Allocation (DARA)
approach to compute the optimal slot allocation policy that maximizes the
deadline-abiding delivery of all senders. A unique characteristic of the DARA
approach is that it yields a non-stationary slot allocation policy that depends
on the allocation of previous slots. We prove that the DARA approach is optimal
for weight distributions that are exponentially decreasing in time. We further
implement our framework for real-time video streaming in wireless personal area
networks that are gaining significant traction within the new
Internet-of-Things (IoT) paradigm. For multiple surveillance videos encoded
with H.264/AVC and streamed via the 6tisch framework that simulates the
IoT-oriented IEEE 802.15.4e TSCH medium access control, our solution is shown
to be the only one that ensures all video bitstreams are delivered with
acceptable quality in a deadline-abiding manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0858</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0858</id><created>2014-01-04</created><authors><author><keyname>Seksaria</keyname><forenames>Videh</forenames></author></authors><title>Multimodal Optimization by Sparkling Squid Populations</title><categories>cs.NE</categories><comments>19 pages, 4 figues</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The swarm intelligence of animals is a natural paradigm to apply to
optimization problems. Ant colony, bee colony, firefly and bat algorithms are
amongst those that have been demonstrated to efficiently to optimize complex
constraints. This paper proposes the new Sparkling Squid Algorithm (SSA) for
multimodal optimization, inspired by the intelligent swarm behavior of its
namesake. After an introduction, formulation and discussion of its
implementation, it will be compared to other popular metaheuristics. Finally,
applications to well - known problems such as image registration and the
traveling salesperson problem will be discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0864</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0864</id><created>2014-01-04</created><authors><author><keyname>Fan</keyname><forenames>Mingming</forenames></author><author><keyname>Khademi</keyname><forenames>Maryam</forenames></author></authors><title>Predicting a Business Star in Yelp from Its Reviews Text Alone</title><categories>cs.IR</categories><comments>5 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Yelp online reviews are invaluable source of information for users to choose
where to visit or what to eat among numerous available options. But due to
overwhelming number of reviews, it is almost impossible for users to go through
all reviews and find the information they are looking for. To provide a
business overview, one solution is to give the business a 1-5 star(s). This
rating can be subjective and biased toward users personality. In this paper, we
predict a business rating based on user-generated reviews texts alone. This not
only provides an overview of plentiful long review texts but also cancels out
subjectivity. Selecting the restaurant category from Yelp Dataset Challenge, we
use a combination of three feature generation methods as well as four machine
learning models to find the best prediction result. Our approach is to create
bag of words from the top frequent words in all raw text reviews, or top
frequent words/adjectives from results of Part-of-Speech analysis. Our results
show Root Mean Square Error (RMSE) of 0.6 for the combination of Linear
Regression with either of the top frequent words from raw data or top frequent
adjectives after Part-of-Speech (POS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0869</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0869</id><created>2014-01-05</created><updated>2015-11-24</updated><authors><author><keyname>Lu</keyname><forenames>Zhaosong</forenames></author><author><keyname>Zhang</keyname><forenames>Yong</forenames></author></authors><title>Schatten-$p$ Quasi-Norm Regularized Matrix Optimization via Iterative
  Reweighted Singular Value Minimization</title><categories>math.OC cs.LG math.NA stat.CO stat.ML</categories><comments>25 pages, 6 figures, and 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study general Schatten-$p$ quasi-norm (SPQN) regularized
matrix minimization problems. In particular, we first introduce a class of
first-order stationary points for them, and show that the first-order
stationary points introduced in [11] for an SPQN regularized $vector$
minimization problem are equivalent to those of an SPQN regularized $matrix$
minimization reformulation. We also show that any local minimizer of the SPQN
regularized matrix minimization problems must be a first-order stationary
point. Moreover, we derive lower bounds for nonzero singular values of the
first-order stationary points and hence also of the local minimizers of the
SPQN regularized matrix minimization problems. The iterative reweighted
singular value minimization (IRSVM) methods are then proposed to solve these
problems, whose subproblems are shown to have a closed-form solution. In
contrast to the analogous methods for the SPQN regularized $vector$
minimization problems, the convergence analysis of these methods is
significantly more challenging. We develop a novel approach to establishing the
convergence of these methods, which makes use of the expression of a specific
solution of their subproblems and avoids the intricate issue of finding the
explicit expression for the Clarke subdifferential of the objective of their
subproblems. In particular, we show that any accumulation point of the sequence
generated by the IRSVM methods is a first-order stationary point of the
problems. Our computational results demonstrate that the IRSVM methods
generally outperform some recently developed state-of-the-art methods in terms
of solution quality and/or speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0870</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0870</id><created>2014-01-05</created><authors><author><keyname>Aroquiaraj</keyname><forenames>I. Laurence</forenames></author><author><keyname>Thangavel</keyname><forenames>K.</forenames></author></authors><title>Pectoral Muscles Suppression in Digital Mammograms using Hybridization
  of Soft Computing Methods</title><categories>cs.CV cs.CE</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Breast region segmentation is an essential prerequisite in computerized
analysis of mammograms. It aims at separating the breast tissue from the
background of the mammogram and it includes two independent segmentations. The
first segments the background region which usually contains annotations, labels
and frames from the whole breast region, while the second removes the pectoral
muscle portion (present in Medio Lateral Oblique (MLO) views) from the rest of
the breast tissue. In this paper we propose hybridization of Connected
Component Labeling (CCL), Fuzzy, and Straight line methods. Our proposed
methods worked good for separating pectoral region. After removal pectoral
muscle from the mammogram, further processing is confined to the breast region
alone. To demonstrate the validity of our segmentation algorithm, it is
extensively tested using over 322 mammographic images from the Mammographic
Image Analysis Society (MIAS) database. The segmentation results were evaluated
using a Mean Absolute Error (MAE), Hausdroff Distance (HD), Probabilistic Rand
Index (PRI), Local Consistency Error (LCE) and Tanimoto Coefficient (TC). The
hybridization of fuzzy with straight line method is given more than 96% of the
curve segmentations to be adequate or better. In addition a comparison with
similar approaches from the state of the art has been given, obtaining slightly
improved results. Experimental results demonstrate the effectiveness of the
proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0872</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0872</id><created>2014-01-05</created><updated>2014-12-16</updated><authors><author><keyname>Ziniel</keyname><forenames>Justin</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author><author><keyname>Sederberg</keyname><forenames>Per</forenames></author></authors><title>Binary Linear Classification and Feature Selection via Generalized
  Approximate Message Passing</title><categories>cs.IT math.IT stat.ML</categories><doi>10.1109/TSP.2015.2407311</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the problem of binary linear classification and feature selection, we
propose algorithmic approaches to classifier design based on the generalized
approximate message passing (GAMP) algorithm, recently proposed in the context
of compressive sensing. We are particularly motivated by problems where the
number of features greatly exceeds the number of training examples, but where
only a few features suffice for accurate classification. We show that
sum-product GAMP can be used to (approximately) minimize the classification
error rate and max-sum GAMP can be used to minimize a wide variety of
regularized loss functions. Furthermore, we describe an
expectation-maximization (EM)-based scheme to learn the associated model
parameters online, as an alternative to cross-validation, and we show that
GAMP's state-evolution framework can be used to accurately predict the
misclassification rate. Finally, we present a detailed numerical study to
confirm the accuracy, speed, and flexibility afforded by our GAMP-based
approaches to binary linear classification and feature selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0875</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0875</id><created>2014-01-05</created><authors><author><keyname>Sahoo</keyname><forenames>Anoop J.</forenames></author><author><keyname>Akhtar</keyname><forenames>Md. Amir Khusru</forenames></author></authors><title>Determining the Possibilities and Certainties in Network Participation
  for MANETS</title><categories>cs.NI</categories><comments>10 Pages. International Journal of Computer Engineering and
  Applications,2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mobile ad hoc network is a self organized cooperative network that works
without any permanent infrastructure. This infrastructure less design makes it
complex compared to other wireless networks. Lot of attacks and misbehavior
obstruct the growth and implementation. The majority of attacks and misbehavior
can be handled by existing protocols. But these protocols reduce the total
strength of nodes in a network because they isolate nodes from network
participation having lesser reputation value. To cope with this problem we have
presented the Possibility and Certainty model. This model uses reputation value
to determine the possibilities and certainties in network participation. The
proposed model classifies nodes into three classes such as certain or HIGH
grade possible or MED grade and not possible or LOW grade. Choosing HIGH grade
nodes in network activities improves the Packet Delivery Ratio which enhances
the throughput of the MANET. On the other hand when node strength is poor we
choose MED grade nodes for network activities. Thus the proposed model allows
communication in the worst scenario with the possibility of success. It
protects a network from misbehavior by isolating LOW grade nodes from routing
paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0877</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0877</id><created>2014-01-05</created><authors><author><keyname>Unnikrishnan</keyname><forenames>K. G.</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Space-Time Coded Spatial Modulated Physical Layer Network Coding for
  Two-Way Relaying</title><categories>cs.IT math.IT</categories><comments>9 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the spatial modulation approach, where only one transmit antenna is
active at a time, we propose two transmission schemes for two-way relay channel
using physical layer network coding with space time coding using Coordinate
Interleaved Orthogonal Designs (CIOD's). It is shown that using two
uncorrelated transmit antennas at the nodes, but using only one RF transmit
chain and space-time coding across these antennas can give a better performance
without using any extra resources and without increasing the hardware
implementation cost and complexity. In the first transmission scheme, two
antennas are used only at the relay, Adaptive Network Coding (ANC) is employed
at the relay and the relay transmits a CIOD Space Time Block Code (STBC). This
gives a better performance compared to an existing ANC scheme for two-way relay
channel which uses one antenna each at all the three nodes. It is shown that
for this scheme at high SNR the average end-to-end symbol error probability
(SEP) is upper bounded by twice the SEP of a point-to-point fading channel. In
the second transmission scheme, two transmit antennas are used at all the three
nodes, CIOD STBC's are transmitted in multiple access and broadcast phases.
This scheme provides a diversity order of two for the average end-to-end SEP
with an increased decoding complexity of $\mathcal{O}(M^3)$ for an arbitrary
signal set and $\mathcal{O}(M^2\sqrt{M})$ for square QAM signal set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0880</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0880</id><created>2014-01-05</created><authors><author><keyname>Chen</keyname><forenames>Ning</forenames></author><author><keyname>Gravin</keyname><forenames>Nick</forenames></author><author><keyname>Lu</keyname><forenames>Pinyan</forenames></author></authors><title>Optimal Competitive Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the design of truthful auctions for selling identical items in
unlimited supply (e.g., digital goods) to n unit demand buyers. This classic
problem stands out from profit-maximizing auction design literature as it
requires no probabilistic assumptions on buyers' valuations and employs the
framework of competitive analysis. Our objective is to optimize the worst-case
performance of an auction, measured by the ratio between a given benchmark and
revenue generated by the auction.
  We establish a sufficient and necessary condition that characterizes
competitive ratios for all monotone benchmarks. The characterization identifies
the worst-case distribution of instances and reveals intrinsic relations
between competitive ratios and benchmarks in the competitive analysis. With the
characterization at hand, we show optimal competitive auctions for two natural
benchmarks.
  The most well-studied benchmark $\mathcal{F}^{(2)}(\cdot)$ measures the
envy-free optimal revenue where at least two buyers win. Goldberg et al. [13]
showed a sequence of lower bounds on the competitive ratio for each number of
buyers n. They conjectured that all these bounds are tight. We show that
optimal competitive auctions match these bounds. Thus, we confirm the
conjecture and settle a central open problem in the design of digital goods
auctions. As one more application we examine another economically meaningful
benchmark, which measures the optimal revenue across all limited-supply Vickrey
auctions. We identify the optimal competitive ratios to be
$(\frac{n}{n-1})^{n-1}-1$ for each number of buyers n, that is $e-1$ as $n$
approaches infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0882</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0882</id><created>2014-01-05</created><updated>2014-01-08</updated><authors><author><keyname>Alama</keyname><forenames>Jesse</forenames></author></authors><title>Exploring Steinitz-Rademacher polyhedra: A challenge for automated
  reasoning tools</title><categories>math.LO cs.DM cs.LO</categories><comments>5 pages. Presented at IWIL 2010 (International Workshop on the
  Implementation of Logics), Yogyakarta, Indonesia</comments><msc-class>03-04, 05B20, 51M20</msc-class><acm-class>F.4.1</acm-class><journal-ref>Proceedings of the Eighth International Workshop on the
  Implementation of Logics, 2010, pp. 14-18</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note reports on some experiments, using a handful of standard automated
reasoning tools, for exploring Steinitz-Rademacher polyhedra, which are models
of a certain first-order theory of incidence structures. This theory and its
models, even simple ones, presents significant, geometrically fascinating
challenges for automated reasoning tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0885</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0885</id><created>2014-01-05</created><updated>2014-02-11</updated><authors><author><keyname>Berardi</keyname><forenames>Stefano</forenames><affiliation>University of Turin</affiliation></author><author><keyname>de'Liguoro</keyname><forenames>Ugo</forenames><affiliation>University of Turin</affiliation></author></authors><title>Knowledge Spaces and the Completeness of Learning Strategies</title><categories>cs.LO math.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (February
  12, 2014) lmcs:729</journal-ref><doi>10.2168/LMCS-10(1:9)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a theory of learning aimed to formalize some ideas underlying
Coquand's game semantics and Krivine's realizability of classical logic. We
introduce a notion of knowledge state together with a new topology, capturing
finite positive and negative information that guides a learning strategy. We
use a leading example to illustrate how non-constructive proofs lead to
continuous and effective learning strategies over knowledge spaces, and prove
that our learning semantics is sound and complete w.r.t. classical truth, as it
is the case for Coquand's and Krivine's approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0886</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0886</id><created>2014-01-05</created><authors><author><keyname>Najashi</keyname><forenames>Barau Gafai</forenames></author><author><keyname>Wenjiang</keyname><forenames>Feng</forenames></author><author><keyname>Almustapha</keyname><forenames>Mohammed Dikko</forenames></author></authors><title>Spectrum Hole Prediction Based On Historical Data: A Neural Network
  Approach</title><categories>cs.NE</categories><comments>8 pages,9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of cognitive radio pioneered by Mitola promises to change the
future of wireless communication especially in the area of spectrum management.
Currently, the command and control strategy employed in spectrum assignment is
too rigid and needs to be reviewed. Recent studies have shown that assigned
spectrum is underutilized spectrally and temporally. Cognitive radio provides a
viable solution whereby licensed users can share the spectrum with unlicensed
users opportunistically without causing interference. Unlicensed users must be
able to sense weather the channel is busy or idle, failure to do so will lead
to interference to the licensed user. In this paper, a neural network based
prediction model for predicting the channel status using historical data
obtained during a spectrum occupancy measurement is presented. Genetic
algorithm is combined with LM BP for increasing the probability of obtaining
the best weights thus optimizing the network. The results obtained indicate
high prediction accuracy over all bands considered
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0887</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0887</id><created>2014-01-05</created><authors><author><keyname>Thanou</keyname><forenames>Dorina</forenames></author><author><keyname>Shuman</keyname><forenames>David I</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Learning parametric dictionaries for graph signals</title><categories>cs.LG cs.SI stat.ML</categories><doi>10.1109/TSP.2014.2332441</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In sparse signal representation, the choice of a dictionary often involves a
tradeoff between two desirable properties -- the ability to adapt to specific
signal data and a fast implementation of the dictionary. To sparsely represent
signals residing on weighted graphs, an additional design challenge is to
incorporate the intrinsic geometric structure of the irregular data domain into
the atoms of the dictionary. In this work, we propose a parametric dictionary
learning algorithm to design data-adapted, structured dictionaries that
sparsely represent graph signals. In particular, we model graph signals as
combinations of overlapping local patterns. We impose the constraint that each
dictionary is a concatenation of subdictionaries, with each subdictionary being
a polynomial of the graph Laplacian matrix, representing a single pattern
translated to different areas of the graph. The learning algorithm adapts the
patterns to a training set of graph signals. Experimental results on both
synthetic and real datasets demonstrate that the dictionaries learned by the
proposed algorithm are competitive with and often better than unstructured
dictionaries learned by state-of-the-art numerical learning algorithms in terms
of sparse approximation of graph signals. In contrast to the unstructured
dictionaries, however, the dictionaries learned by the proposed algorithm
feature localized atoms and can be implemented in a computationally efficient
manner in signal processing tasks such as compression, denoising, and
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0889</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0889</id><created>2014-01-05</created><updated>2014-01-07</updated><authors><author><keyname>Guo</keyname><forenames>Yue</forenames></author><author><keyname>Shen</keyname><forenames>Xuelian</forenames></author><author><keyname>Zhu</keyname><forenames>Zhanfeng</forenames></author></authors><title>Research on the mobile robots intelligent path planning based on ant
  colony algorithm application in manufacturing logistics</title><categories>cs.RO</categories><comments>17 pages,7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development of robotics and artificial intelligence field
unceasingly thorough, path planning as an important field of robot calculation
has been widespread concern. This paper analyzes the current development of
robot and path planning algorithm and focuses on the advantages and
disadvantages of the traditional intelligent path planning as well as the path
planning. The problem of mobile robot path planning is studied by using ant
colony algorithm, and it also provides some solving methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0892</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0892</id><created>2014-01-05</created><updated>2014-11-06</updated><authors><author><keyname>Weinberger</keyname><forenames>Nir</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Optimum Trade-offs Between the Error Exponent and the Excess-Rate
  Exponent of Variable-Rate Slepian-Wolf Coding</title><categories>cs.IT math.IT</categories><comments>Extended version of paper submitted to the IEEE Trans. on Information
  Theory. Presented in part in ISIT2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the optimal trade-off between the error exponent and the
excess-rate exponent for variable-rate Slepian-Wolf codes. In particular, we
first derive upper (converse) bounds on the optimal error and excess-rate
exponents, and then lower (achievable) bounds, via a simple class of
variable-rate codes which assign the same rate to all source blocks of the same
type class. Then, using the exponent bounds, we derive bounds on the optimal
rate functions, namely, the minimal rate assigned to each type class, needed in
order to achieve a given target error exponent. The resulting excess-rate
exponent is then evaluated. Iterative algorithms are provided for the
computation of both bounds on the optimal rate functions and their excess-rate
exponents. The resulting Slepian-Wolf codes bridge between the two extremes of
fixed-rate coding, which has minimal error exponent and maximal excess-rate
exponent, and average-rate coding, which has maximal error exponent and minimal
excess-rate exponent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0898</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0898</id><created>2014-01-05</created><authors><author><keyname>Singh</keyname><forenames>Vijendra</forenames></author><author><keyname>Pathak</keyname><forenames>Shivani</forenames></author></authors><title>Feature Selection Using Classifier in High Dimensional Data</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection is frequently used as a pre-processing step to machine
learning. It is a process of choosing a subset of original features so that the
feature space is optimally reduced according to a certain evaluation criterion.
The central objective of this paper is to reduce the dimension of the data by
finding a small set of important features which can give good classification
performance. We have applied filter and wrapper approach with different
classifiers QDA and LDA respectively. A widely-used filter method is used for
bioinformatics data i.e. a univariate criterion separately on each feature,
assuming that there is no interaction between features and then applied
Sequential Feature Selection method. Experimental results show that filter
approach gives better performance in respect of Misclassification Error Rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0906</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0906</id><created>2014-01-05</created><authors><author><keyname>Clarke</keyname><forenames>P.</forenames></author></authors><title>A Search Procedure for Cyclic Subsets</title><categories>cs.DS math.CO</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a polynomial time algorithm for finding the set of all cyclic
subsets in a graph is presented. The concept of cyclic subsets has already been
introduced in an earlier paper. The algorithm finds cyclic subsets in a graph G
by conjoining building block subsets of length three in V(G). We prove the
correctness of this algorithm and present an asymptotic time complexity
analysis of the algorithm's performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0912</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0912</id><created>2014-01-05</created><updated>2014-08-23</updated><authors><author><keyname>Mahadev</keyname><forenames>Urmila</forenames><affiliation>UC Berkeley</affiliation></author><author><keyname>de Wolf</keyname><forenames>Ronald</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Rational approximations and quantum algorithms with postselection</title><categories>quant-ph cs.CC</categories><comments>v2: 12 pages LaTeX, to appear in Quantum Information and Computation.
  Compared to version 1, the writing has been improved but the results are
  unchanged</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the close connection between rational functions that approximate a
given Boolean function, and quantum algorithms that compute the same function
using postselection. We show that the minimal degree of the former equals (up
to a factor of 2) the minimal query complexity of the latter. We give optimal
(up to constant factors) quantum algorithms with postselection for the Majority
function, slightly improving upon an earlier algorithm of Aaronson. Finally we
show how Newman's classic theorem about low-degree rational approximation of
the absolute-value function follows from these algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0918</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0918</id><created>2014-01-05</created><authors><author><keyname>Sznajd-Weron</keyname><forenames>Katarzyna</forenames></author><author><keyname>Suszczynski</keyname><forenames>Karol Michal</forenames></author></authors><title>Nonlinear q-voter model with deadlocks on the Watts-Strogatz graph</title><categories>physics.soc-ph cs.SI</categories><doi>10.1088/1742-5468/2014/07/P07018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the nonlinear $q$-voter model with deadlocks on a Watts-Strogats
graph. Using Monte Carlo simulations, we obtain so called exit probability and
exit time. We determine how network properties, such as randomness or density
of links influence exit properties of a model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0921</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0921</id><created>2014-01-05</created><authors><author><keyname>Burghardt</keyname><forenames>Jochen</forenames></author></authors><title>Maintaining partial sums in logarithmic time</title><categories>cs.DS</categories><comments>8 pages, 3 figues. Full version of an article in the &quot;Nordic Journal
  of Computing&quot;, including Hoare-style correctness proofs of algorithms</comments><msc-class>68P05</msc-class><acm-class>E.1</acm-class><journal-ref>Nordic Journal of Computing, Vol.8, No.4, p.473-474, 2001</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a data structure that allows to maintain in logarithmic time all
partial sums of elements of a linear array during incremental changes of
element's values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0922</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0922</id><created>2014-01-05</created><authors><author><keyname>Majooni</keyname><forenames>Azam</forenames></author><author><keyname>Masood</keyname><forenames>Mona</forenames></author><author><keyname>Akhavan</keyname><forenames>Amir</forenames></author></authors><title>A survey on the importance of visualization and social collaboration in
  academic digital libraries</title><categories>cs.DL</categories><comments>14 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From more than half a century ago indexing scientific articles has been
studied intensively to provide a more efficient data retrieval and to conserve
researchers invaluable time. In the last two decades with the emergence of the
World Wide Web and the rapid growth in the number of scientific documents
online many academic databases and search engines were launched with almost
similar structure in order to reduce the difficulty in finding, relating and
sorting of the existing scientific documents published online. The dramatic
increase of the scientific documents in the last few years makes it necessary
that the retrieved information by the search engines be analyzed and more
organized and interpretable representation be displayed to the users.
Information visualization is a great way for exploration of large and complex
data sets, therefore it can be a natural candidate for the purpose of
generating more comprehensible search results for the citation and academic
databases. In this survey the usage pattern of the participants and their
demands and ideas for the existence of other beneficial methods for literature
review has been questioned and the results are quantitatively analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0926</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0926</id><created>2014-01-05</created><authors><author><keyname>Park</keyname><forenames>Shinkyu</forenames></author><author><keyname>Martins</keyname><forenames>Nuno C.</forenames></author></authors><title>A Class of LTI Distributed Observers for LTI Plants: Necessary and
  Sufficient Conditions for Stabilizability</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider that an autonomous linear time-invariant (LTI) plant is given and
that a network of LTI observers assesses its output vector. The dissemination
of information within the network is dictated by a pre-specified directed graph
in which each vertex represents an observer. Each observer computes its own
state estimate using only the portion of the output vector accessible to it and
the state estimates of other observers that are transmitted to it by its
neighbors, according to the graph. This paper proposes an update rule that is a
natural generalization of consensus, and for which we determine necessary and
sufficient conditions for the existence of parameters for the update rule that
lead to asymptotic omniscience of the state of the plant at all observers. The
conditions reduce to certain detectability requirements that imply that if
omniscience is not possible under the proposed scheme then it is not viable
under any other scheme that is subject to the same communication graph,
including nonlinear and time-varying ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0936</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0936</id><created>2014-01-05</created><updated>2015-08-14</updated><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author></authors><title>Linear time construction of compressed text indices in compact space</title><categories>cs.DS</categories><comments>Expanded version of a paper appeared in proceedings of STOC 2014
  conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the compressed suffix array and the compressed suffix tree for a
string of length $n$ over an integer alphabet of size $\sigma\leq n$ can both
be built in $O(n)$ (randomized) time using only $O(n\log\sigma)$ bits of
working space. The previously fastest construction algorithms that used
$O(n\log\sigma)$ bits of space took times $O(n\log\log\sigma)$ and
$O(n\log^{\epsilon}n)$ respectively (where $\epsilon$ is any positive constant
smaller than $1$). In the passing, we show that the Burrows-Wheeler transform
of a string of length $n$ over an alphabet of size $\sigma$ can be built in
deterministic $O(n)$ time and space $O(n\log\sigma)$. We also show that within
the same time and space, we can carry many sequence analysis tasks and
construct some variants of the compressed suffix array and compressed suffix
tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0943</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0943</id><created>2014-01-05</created><authors><author><keyname>Akanbi</keyname><forenames>Adeyinka K</forenames></author></authors><title>LB2CO: A Semantic Ontology Framework for B2C eCommerce Transaction on
  the Internet</title><categories>cs.CY cs.AI</categories><comments>9 Pages, 7 figures, Research Paper</comments><journal-ref>International Journal of Research in Computer Science, 4 (1): pp.
  1-9, January 2014</journal-ref><doi>10.7815/ijorcs.41.2014.075</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Business ontology can enhance the successful development of complex
enterprise system; this is being achieved through knowledge sharing and the
ease of communication between every entity in the domain. Through human
semantic interaction with the web resources, machines to interpret the data
published in a machine interpretable form under web. However, the theoretical
practice of business ontology in eCommerce domain is quite a few especially in
the section of electronic transaction, and the various techniques used to
obtain efficient communication across spheres are error prone and are not
always guaranteed to be efficient in obtaining desired result due to poor
semantic integration between entities. To overcome the poor semantic
integration this research focuses on proposed ontology called LB2CO, which
combines the framework of IDEF5 &amp; SNAP as an analysis tool, for automated
recommendation of product and services and create effective ontological
framework for B2C transaction &amp; communication across different business domains
that facilitates the interoperability &amp; integration of B2C transactions over
the web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0968</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0968</id><created>2014-01-05</created><authors><author><keyname>Casta&#xf1;o</keyname><forenames>Rodrigo</forenames><affiliation>Departamento de Computaci&#xf3;n. FCEyN. UBA</affiliation></author><author><keyname>Galeotti</keyname><forenames>Juan Pablo</forenames><affiliation>Saarland University</affiliation></author><author><keyname>Garbervetsky</keyname><forenames>Diego</forenames><affiliation>Departamento de Computaci&#xf3;n. FCEyN. UBA</affiliation></author><author><keyname>Tapicer</keyname><forenames>Jonathan</forenames><affiliation>Departamento de Computaci&#xf3;n. FCEyN. UBA</affiliation></author><author><keyname>Zoppi</keyname><forenames>Edgardo</forenames><affiliation>Departamento de Computaci&#xf3;n. FCEyN. UBA</affiliation></author></authors><title>On Verifying Resource Contracts using Code Contracts</title><categories>cs.SE cs.PL</categories><comments>In Proceedings LAFM 2013, arXiv:1401.0564</comments><proxy>EPTCS</proxy><acm-class>D.2.4</acm-class><journal-ref>EPTCS 139, 2014, pp. 1-15</journal-ref><doi>10.4204/EPTCS.139.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an approach to check resource consumption contracts
using an off-the-shelf static analyzer.
  We propose a set of annotations to support resource usage specifications, in
particular, dynamic memory consumption constraints. Since dynamic memory may be
recycled by a memory manager, the consumption of this resource is not monotone.
The specification language can express both memory consumption and lifetime
properties in a modular fashion.
  We develop a proof-of-concept implementation by extending Code Contracts'
specification language. To verify the correctness of these annotations we rely
on the Code Contracts static verifier and a points-to analysis. We also briefly
discuss possible extensions of our approach to deal with non-linear
expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0969</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0969</id><created>2014-01-05</created><authors><author><keyname>Castro</keyname><forenames>Pablo F.</forenames><affiliation>Universidad Nacional de Rio Cuarto - CONICET</affiliation></author><author><keyname>Maibaum</keyname><forenames>Thomas S. E.</forenames><affiliation>McMaster University</affiliation></author></authors><title>Automated Reasoning over Deontic Action Logics with Finite Vocabularies</title><categories>cs.LO</categories><comments>In Proceedings LAFM 2013, arXiv:1401.0564</comments><proxy>EPTCS</proxy><acm-class>Logic in Computer Science</acm-class><journal-ref>EPTCS 139, 2014, pp. 16-30</journal-ref><doi>10.4204/EPTCS.139.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate further the tableaux system for a deontic action
logic we presented in previous work. This tableaux system uses atoms (of a
given boolean algebra of action terms) as labels of formulae, this allows us to
embrace parallel execution of actions and action complement, two action
operators that may present difficulties in their treatment. One of the
restrictions of this logic is that it uses vocabularies with a finite number of
actions. In this article we prove that this restriction does not affect the
coherence of the deduction system; in other words, we prove that the system is
complete with respect to language extension. We also study the computational
complexity of this extended deductive framework and we prove that the
complexity of this system is in PSPACE, which is an improvement with respect to
related systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0970</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0970</id><created>2014-01-05</created><authors><author><keyname>Cassano</keyname><forenames>Valentin</forenames><affiliation>McMaster University</affiliation></author><author><keyname>Maibaum</keyname><forenames>Thomas S. E.</forenames><affiliation>McMaster University</affiliation></author></authors><title>Actions and Events in Concurrent Systems Design</title><categories>cs.SE cs.LO</categories><comments>In Proceedings LAFM 2013, arXiv:1401.0564</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 139, 2014, pp. 31-45</journal-ref><doi>10.4204/EPTCS.139.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, having in mind the construction of concurrent systems from
components, we discuss the difference between actions and events. For this
discussion, we propose an(other) architecture description language in which
actions and events are made explicit in the description of a component and a
system. Our work builds from the ideas set forth by the categorical approach to
the construction of software based systems from components advocated by Goguen
and Burstall, in the context of institutions, and by Fiadeiro and Maibaum, in
the context of temporal logic. In this context, we formalize a notion of a
component as an element of an indexed category and we elicit a notion of a
morphism between components as morphisms of this category. Moreover, we
elaborate on how this formalization captures, in a convenient manner, the
underlying structure of a component and the basic interaction mechanisms for
putting components together. Further, we advance some ideas on how certain
matters related to the openness and the compositionality of a component/system
may be described in terms of classes of morphisms, thus potentially supporting
a compositional rely/guarantee reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0971</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0971</id><created>2014-01-05</created><authors><author><keyname>Regis</keyname><forenames>Germ&#xe1;n</forenames><affiliation>UNRC - Argentina</affiliation></author><author><keyname>Villar</keyname><forenames>Fernando</forenames><affiliation>UNRC - Argentina</affiliation></author><author><keyname>Ricci</keyname><forenames>Nicol&#xe1;s</forenames><affiliation>UNRC - Argentina</affiliation></author></authors><title>Fluent Logic Workflow Analyser: A Tool for The Verification of Workflow
  Properties</title><categories>cs.SE</categories><comments>In Proceedings LAFM 2013, arXiv:1401.0564</comments><proxy>EPTCS</proxy><acm-class>H.4.1; D.2.4; F.3.1</acm-class><journal-ref>EPTCS 139, 2014, pp. 46-52</journal-ref><doi>10.4204/EPTCS.139.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the design and implementation, as well as a use
case, of a tool for workflow analysis. The tool provides an assistant for the
specification of properties of a workflow model. The specification language for
property description is Fluent Linear Time Temporal Logic. Fluents provide an
adequate flexibility for capturing properties of workflows. Both the model and
the properties are encoded, in an automated way, as Labelled Transition
Systems, and the analysis is reduced to model checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0972</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0972</id><created>2014-01-05</created><authors><author><keyname>Medeiros</keyname><forenames>Val&#xe9;rio</forenames><suffix>Jr.</suffix><affiliation>Federal Institute of Education, Science and Technology of Rio Grande do Norte</affiliation></author><author><keyname>D&#xe9;harbe</keyname><forenames>David</forenames><affiliation>Federal University of Rio Grande do Norte</affiliation></author></authors><title>BEval: A Plug-in to Extend Atelier B with Current Verification
  Technologies</title><categories>cs.SE cs.LO</categories><comments>In Proceedings LAFM 2013, arXiv:1401.0564</comments><proxy>EPTCS</proxy><acm-class>D.2.4</acm-class><journal-ref>EPTCS 139, 2014, pp. 53-58</journal-ref><doi>10.4204/EPTCS.139.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents BEval, an extension of Atelier B to improve automation in
the verification activities in the B method or Event-B. It combines a tool for
managing and verifying software projects (Atelier B) and a model
checker/animator (ProB) so that the verification conditions generated in the
former are evaluated with the latter. In our experiments, the two main
verification strategies (manual and automatic) showed significant improvement
as ProB's evaluator proves complementary to Atelier B built-in provers. We
conducted experiments with the B model of a micro-controller instruction set;
several verification conditions, that we were not able to discharge
automatically or manually with AtelierB's provers, were automatically verified
using BEval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0973</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0973</id><created>2014-01-05</created><authors><author><keyname>Bendersky</keyname><forenames>Pablo</forenames><affiliation>Departamento de Computaci&#xf3;n, FCEyN, UBA Buenos Aires, Argentina</affiliation></author><author><keyname>Galeotti</keyname><forenames>Juan Pablo</forenames><affiliation>Saarland University Saarbr&#xfc;cken, Germany</affiliation></author><author><keyname>Garbervetsky</keyname><forenames>Diego</forenames><affiliation>Departamento de Computaci&#xf3;n, FCEyN, UBA Buenos Aires, Argentina</affiliation></author></authors><title>The DynAlloy Visualizer</title><categories>cs.SE cs.LO</categories><comments>In Proceedings LAFM 2013, arXiv:1401.0564</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 139, 2014, pp. 59-64</journal-ref><doi>10.4204/EPTCS.139.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an extension to the DynAlloy tool to navigate DynAlloy
counterexamples: the DynAlloy Visualizer. The user interface mimics the
functionality of a programming language debugger. Without this tool, a DynAlloy
user is forced to deal with the internals of the Alloy intermediate
representation in order to debug a flaw in her model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0974</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0974</id><created>2014-01-05</created><authors><author><keyname>Gim&#xe9;nez</keyname><forenames>Manuel</forenames><affiliation>DC - FCEN - UBA</affiliation></author><author><keyname>Moscato</keyname><forenames>Mariano M.</forenames><affiliation>DC - FCEN - UBA</affiliation></author><author><keyname>Pombo</keyname><forenames>Carlos G. Lopez</forenames><affiliation>DC - FCEN - UBA</affiliation></author><author><keyname>Frias</keyname><forenames>Marcelo F.</forenames><affiliation>ITBA - CONICET</affiliation></author></authors><title>HeteroGenius: A Framework for Hybrid Analysis of Heterogeneous Software
  Specifications</title><categories>cs.SE cs.LO</categories><comments>In Proceedings LAFM 2013, arXiv:1401.0564</comments><proxy>EPTCS</proxy><acm-class>D.2.4; F.3.1</acm-class><journal-ref>EPTCS 139, 2014, pp. 65-70</journal-ref><doi>10.4204/EPTCS.139.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, software artifacts are ubiquitous in our lives being an essential
part of home appliances, cars, cell phones, and even in more critical
activities like aeronautics and health sciences. In this context software
failures may produce enormous losses, either economical or, in the worst case,
in human lives. Software analysis is an area in software engineering concerned
with the application of diverse techniques in order to prove the absence of
errors in software pieces. In many cases different analysis techniques are
applied by following specific methodological combinations that ensure better
results. These interactions between tools are usually carried out at the user
level and it is not supported by the tools. In this work we present
HeteroGenius, a framework conceived to develop tools that allow users to
perform hybrid analysis of heterogeneous software specifications.
  HeteroGenius was designed prioritising the possibility of adding new
specification languages and analysis tools and enabling a synergic relation of
the techniques under a graphical interface satisfying several well-known
usability enhancement criteria. As a case-study we implemented the
functionality of Dynamite on top of HeteroGenius.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0975</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0975</id><created>2014-01-05</created><authors><author><keyname>Scilingo</keyname><forenames>Gast&#xf3;n</forenames><affiliation>UNRC - Argentina</affiliation></author><author><keyname>Novaira</keyname><forenames>Mar&#xed;a Marta</forenames><affiliation>UNRC - Argentina</affiliation></author><author><keyname>Degiovanni</keyname><forenames>Renzo</forenames><affiliation>CONICET, UNRC - Argentina</affiliation></author></authors><title>Analyzing Behavioural Scenarios over Tabular Specifications Using Model
  Checking</title><categories>cs.SE cs.SY</categories><comments>In Proceedings LAFM 2013, arXiv:1401.0564</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.1</acm-class><journal-ref>EPTCS 139, 2014, pp. 71-76</journal-ref><doi>10.4204/EPTCS.139.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tabular notations, in particular SCR specifications, have proved to be a
useful means for formally describing complex requirements. The SCR method
offers a powerful family of analysis tools, known as the SCR Toolset, but its
availability is restricted by the Naval Research Laboratory of the USA. This
toolset applies different kinds of analysis considering the whole set of
behaviours associated with a requirements specification. In this paper we
present a tool for describing and analyzing SCR requirements descriptions, that
complements the SCR Toolset in two aspects. First, its use is not limited by
any institution, and resorts to a standard model checking tool for analysis;
and second, it allows to concentrate the analysis to particular sets of
behaviours (subsets of the whole specifications), that correspond to particular
scenarios explicitly mentioned in the specification. We take an operational
notation that allows the engineer to describe behavioural &quot;scenarios&quot; by means
of programs, and provide a translation into Promela to perform the analysis via
Spin, an efficient off-the-shelf model checker freely available. In addition,
we apply the SCR method to a Pacemaker system and we use its tabular
specification as a running example of this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0976</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0976</id><created>2014-01-05</created><authors><author><keyname>Saxena</keyname><forenames>Nitin</forenames></author></authors><title>Progress on Polynomial Identity Testing - II</title><categories>cs.CC cs.DM math.AC math.AG</categories><comments>17 pages, 1 figure, survey</comments><msc-class>68Q25, 68W30, 12Y05, 13P25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey the area of algebraic complexity theory; with the focus being on
the problem of polynomial identity testing (PIT). We discuss the key ideas that
have gone into the results of the last few years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0978</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0978</id><created>2014-01-05</created><updated>2014-10-08</updated><authors><author><keyname>Griffith</keyname><forenames>Virgil</forenames></author></authors><title>A Principled Infotheoretic \phi-like Measure</title><categories>cs.IT math.IT</categories><comments>18 pages; 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrated information theory is a mathematical, quantifiable theory of
conscious experience. The linchpin of this theory, the $\phi$ measure,
quantifies a system's irreducibility to disjoint parts. Purely as a measure of
irreducibility, we pinpoint three concerns about $\phi$ and propose a revised
measure, $\psi$, which addresses them. Our measure $\psi$ is rigorously
grounded in Partial Information Decomposition and is faster to compute than
$\phi$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0987</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0987</id><created>2014-01-06</created><authors><author><keyname>Jin</keyname><forenames>Chi</forenames></author><author><keyname>Wang</keyname><forenames>Ziteng</forenames></author><author><keyname>Huang</keyname><forenames>Junliang</forenames></author><author><keyname>Zhong</keyname><forenames>Yiqiao</forenames></author><author><keyname>Wang</keyname><forenames>Liwei</forenames></author></authors><title>Differentially Private Data Releasing for Smooth Queries with Synthetic
  Database Output</title><categories>cs.DB stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider accurately answering smooth queries while preserving differential
privacy. A query is said to be $K$-smooth if it is specified by a function
defined on $[-1,1]^d$ whose partial derivatives up to order $K$ are all
bounded. We develop an $\epsilon$-differentially private mechanism for the
class of $K$-smooth queries. The major advantage of the algorithm is that it
outputs a synthetic database. In real applications, a synthetic database output
is appealing. Our mechanism achieves an accuracy of $O
(n^{-\frac{K}{2d+K}}/\epsilon )$, and runs in polynomial time. We also
generalize the mechanism to preserve $(\epsilon, \delta)$-differential privacy
with slightly improved accuracy. Extensive experiments on benchmark datasets
demonstrate that the mechanisms have good accuracy and are efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0994</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0994</id><created>2014-01-06</created><authors><author><keyname>Cai</keyname><forenames>Chunxiao</forenames></author><author><keyname>Cai</keyname><forenames>Yueming</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author><author><keyname>Yang</keyname><forenames>Weiwei</forenames></author><author><keyname>Yang</keyname><forenames>Wendong</forenames></author></authors><title>When Does Relay Transmission Give a More Secure Connection in Wireless
  Ad Hoc Networks?</title><categories>cs.IT cs.CR math.IT</categories><comments>Accepted for publication in IEEE Transactions On Information
  Forensics and Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relay transmission can enhance coverage and throughput, while it can be
vulnerable to eavesdropping attacks due to the additional transmission of the
source message at the relay. Thus, whether or not one should use relay
transmission for secure communication is an interesting and important problem.
In this paper, we consider the transmission of a confidential message from a
source to a destination in a decentralized wireless network in the presence of
randomly distributed eavesdroppers. The source-destination pair can be
potentially assisted by randomly distributed relays. For an arbitrary relay, we
derive exact expressions of secure connection probability for both colluding
and non-colluding eavesdroppers. We further obtain lower bound expressions on
the secure connection probability, which are accurate when the eavesdropper
density is small. By utilizing these lower bound expressions, we propose a
relay selection strategy to improve the secure connection probability. By
analytically comparing the secure connection probability for direct
transmission and relay transmission, we address the important problem of
whether or not to relay and discuss the condition for relay transmission in
terms of the relay density and source-destination distance. These analytical
results are accurate in the small eavesdropper density regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0997</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0997</id><created>2014-01-06</created><authors><author><keyname>Vucinic</keyname><forenames>Malisa</forenames><affiliation>LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Tourancheau</keyname><forenames>Bernard</forenames><affiliation>LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Duda</keyname><forenames>Andrzej</forenames><affiliation>LIG Laboratoire d'Informatique de Grenoble</affiliation></author></authors><title>Performance Comparison of the RPL and LOADng Routing Protocols in a Home
  Automation Scenario</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>Wireless Communications and Networking Conference (WCNC), 2013
  IEEE (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RPL, the routing protocol proposed by IETF for IPv6/6LoWPAN Low Power and
Lossy Networks has significant complexity. Another protocol called LOADng, a
lightweight variant of AODV, emerges as an alternative solution. In this paper,
we compare the performance of the two protocols in a Home Automation scenario
with heterogenous traffic patterns including a mix of multipoint-to-point and
point-to-multipoint routes in realistic dense non-uniform network topologies.
We use Contiki OS and Cooja simulator to evaluate the behavior of the
ContikiRPL implementation and a basic non-optimized implementation of LOADng.
Unlike previous studies, our results show that RPL provides shorter delays,
less control overhead, and requires less memory than LOADng. Nevertheless,
enhancing LOADng with more efficient flooding and a better route storage
algorithm may improve its performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.0998</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.0998</id><created>2014-01-06</created><authors><author><keyname>Allali</keyname><forenames>Lisa</forenames><affiliation>LIX</affiliation></author><author><keyname>Hermant</keyname><forenames>Olivier</forenames><affiliation>CRI, INRIA Paris-Rocquencourt</affiliation></author></authors><title>Semantic A-translation and Super-consistency entail Classical Cut
  Elimination</title><categories>cs.LO math.LO</categories><proxy>ccsd</proxy><journal-ref>LPAR 19 - 19th Conference on Logic for Programming, Artificial
  Intelligence, and Reasoning - 2013 8312 (2013) 407-422</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that if a theory R defined by a rewrite system is super-consistent,
the classical sequent calculus modulo R enjoys the cut elimination property,
which was an open question. For such theories it was already known that proofs
strongly normalize in natural deduction modulo R, and that cut elimination
holds in the intuitionistic sequent calculus modulo R. We first define a
syntactic and a semantic version of Friedman's A-translation, showing that it
preserves the structure of pseudo-Heyting algebra, our semantic framework. Then
we relate the interpretation of a theory in the A-translated algebra and its
A-translation in the original algebra. This allows to show the stability of the
super-consistency criterion and the cut elimination theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1003</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1003</id><created>2014-01-06</created><authors><author><keyname>Rao</keyname><forenames>Nanditha P.</forenames></author><author><keyname>Sarik</keyname><forenames>Shahbaz</forenames></author><author><keyname>Desai</keyname><forenames>Madhav P.</forenames></author></authors><title>On the likelihood of multiple bit upsets in logic circuits</title><categories>cs.AR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Soft errors have a significant impact on the circuit reliability at nanoscale
technologies. At the architectural level, soft errors are commonly modeled by a
probabilistic bit-flip model. In developing such abstract fault models, an
important issue to consider is the likelihood of multiple bit errors caused by
particle strikes. This likelihood has been studied to a great extent in
memories, but has not been understood to the same extent in logic circuits. In
this paper, we attempt to quantify the likelihood that a single transient event
can cause multiple bit errors in logic circuits consisting of combinational
gates and flip-flops. In particular, we calculate the conditional probability
of multiple bit-flips given that a single bit flips as a result of the
transient. To calculate this conditional probability, we use a Monte Carlo
technique in which samples are generated using detailed post-layout circuit
simulations. Our experiments on the ISCAS'85 benchmarks and a few other
circuits indicate that, this conditional probability is quite significant and
can be as high as 0.31. Thus we conclude that multiple bit-flips must
necessarily be considered in order to obtain a realistic architectural fault
model for soft errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1011</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1011</id><created>2014-01-06</created><authors><author><keyname>Zhu</keyname><forenames>Guangxu</forenames></author><author><keyname>Zhong</keyname><forenames>Caijun</forenames></author><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Outage Probability of Dual-Hop Multiple Antenna AF Systems with Linear
  Processing in the Presence of Co-Channel Interference</title><categories>cs.IT math.IT</categories><comments>14 pages TWC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a dual-hop amplify-and-forward (AF) relaying system
where the relay is equipped with multiple antennas, while the source and the
destination are equipped with a single antenna. Assuming that the relay is
subjected to co-channel interference (CCI) and additive white Gaussian noise
(AWGN) while the destination is corrupted by AWGN only, we propose three
heuristic relay precoding schemes to combat the CCI, namely, 1) Maximum ratio
combining/maximal ratio transmission (MRC/MRT), 2) Zero-forcing/MRT (ZF/MRT),
3) Minimum mean-square error/MRT (MMSE/MRT). We derive new exact outage
expressions as well as simple high signal-to-noise ratio (SNR) outage
approximations for all three schemes. Our findings suggest that both the
MRC/MRT and the MMSE/MRT schemes achieve a full diversity of N, while the
ZF/MRT scheme achieves a diversity order of N-M, where N is the number of relay
antennas and M is the number of interferers. In addition, we show that the
MMSE/MRT scheme always achieves the best outage performance, and the ZF/MRT
scheme outperforms the MRC/MRT scheme in the low SNR regime, while becomes
inferior to the MRC/MRT scheme in the high SNR regime. Finally, in the large N
regime, we show that both the ZF/MRT and MMSE/MRT schemes are capable of
completely eliminating the CCI, while perfect interference cancelation is not
possible with the MRC/MRT scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1016</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1016</id><created>2014-01-06</created><updated>2014-05-13</updated><authors><author><keyname>Sen</keyname><forenames>Pinar</forenames></author><author><keyname>Yilmaz</keyname><forenames>Ali Ozgur</forenames></author></authors><title>Factor Graph Based LMMSE Filtering for Colored Gaussian Processes</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a low complexity, graph based linear minimum mean square error
(LMMSE) filter in which the non-white characteristics of a random process are
taken into account. Our method corresponds to block LMMSE filtering, and has
the advantage of complexity linearly increasing with the block length and the
ease of incorporating the a priori information of the input signals whenever
possible. The proposed method can be used with any random process with a known
autocorrelation function with the help of an approximation to an autoregressive
(AR) process. We show through extensive simulations that our method performs
very close to the optimal block LMMSE filtering for Gaussian input signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1024</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1024</id><created>2014-01-06</created><authors><author><keyname>Hoos</keyname><forenames>Holger</forenames></author><author><keyname>Kaminski</keyname><forenames>Roland</forenames></author><author><keyname>Lindauer</keyname><forenames>Marius</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author></authors><title>Solver Scheduling via Answer Set Programming</title><categories>cs.AI cs.LO</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although Boolean Constraint Technology has made tremendous progress over the
last decade, the efficacy of state-of-the-art solvers is known to vary
considerably across different types of problem instances and is known to depend
strongly on algorithm parameters. This problem was addressed by means of a
simple, yet effective approach using handmade, uniform and unordered schedules
of multiple solvers in ppfolio, which showed very impressive performance in the
2011 SAT Competition. Inspired by this, we take advantage of the modeling and
solving capacities of Answer Set Programming (ASP) to automatically determine
more refined, that is, non-uniform and ordered solver schedules from existing
benchmarking data. We begin by formulating the determination of such schedules
as multi-criteria optimization problems and provide corresponding ASP
encodings. The resulting encodings are easily customizable for different
settings and the computation of optimum schedules can mostly be done in the
blink of an eye, even when dealing with large runtime data sets stemming from
many solvers on hundreds to thousands of instances. Also, the fact that our
approach can be customized easily enabled us to swiftly adapt it to generate
parallel schedules for multi-processor machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1031</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1031</id><created>2014-01-06</created><authors><author><keyname>Jamil</keyname><forenames>Noreen</forenames></author></authors><title>Constraint Solvers for User Interface Layout</title><categories>cs.HC cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraints have played an important role in the construction of GUIs, where
they are mainly used to define the layout of the widgets. Resizing behavior is
very important in GUIs because areas have domain specific parameters such as
form the resizing of windows. If linear objective function is used and window
is resized then error is not distributed equally. To distribute the error
equally, a quadratic objective function is introduced. Different algorithms are
widely used for solving linear constraints and quadratic problems in a variety
of different scientific areas. The linear relxation, Kaczmarz, direct and
linear programming methods are common methods for solving linear constraints
for GUI layout. The interior point and active set methods are most commonly
used techniques to solve quadratic programming problems. Current constraint
solvers designed for GUI layout do not use interior point methods for solving a
quadratic objective function subject to linear equality and inequality
constraints. In this paper, performance aspects and the convergence speed of
interior point and active set methods are compared along with one most commonly
used linear programming method when they are implemented for graphical user
interface layout. The performance and convergence of the proposed algorithms
are evaluated empirically using randomly generated UI layout specifications of
various sizes. The results show that the interior point algorithms perform
significantly better than the Simplex method and QOCA-solver, which uses the
active set method implementation for solving quadratic optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1032</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1032</id><created>2014-01-06</created><authors><author><keyname>Moussaid</keyname><forenames>Mehdi</forenames></author></authors><title>Opinion Formation and the Collective Dynamics of Risk Perception</title><categories>physics.soc-ph cs.SI nlin.AO</categories><journal-ref>PLoS ONE 8(12): e84592</journal-ref><doi>10.1371/journal.pone.0084592</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The formation of collective opinion is a complex phenomenon that results from
the combined effects of mass media exposure and social influence between
individuals. The present work introduces a model of opinion formation
specifically designed to address risk judgments, such as attitudes towards
climate change, terrorist threats, or children vaccination. The model assumes
that people collect risk information from the media environment and exchange
them locally with other individuals. Even though individuals are initially
exposed to the same sample of information, the model predicts the emergence of
opinion polarization and clustering. In particular, numerical simulations
highlight two crucial factors that determine the collective outcome: the
propensity of individuals to search for independent information, and the
strength of social influence. This work provides a quantitative framework to
anticipate and manage how the public responds to a given risk, and could help
understanding the systemic amplification of fears and worries, or the
underestimation of real dangers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1043</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1043</id><created>2014-01-06</created><updated>2014-10-11</updated><authors><author><keyname>Ibrahim</keyname><forenames>A.</forenames></author><author><keyname>Sastry</keyname><forenames>Shivakumar</forenames></author><author><keyname>Sastry</keyname><forenames>P. S.</forenames></author></authors><title>Discovering Compressing Serial Episodes from Event Sequences</title><categories>cs.DB</categories><comments>27 pages 3 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most pattern mining methods output a very large number of frequent patterns
and isolating a small but relevant subset is a challenging problem of current
interest in frequent pattern mining. In this paper we consider discovery of a
small set of relevant frequent episodes from data sequences. We make use of the
Minimum Description Length principle to formulate the problem of selecting a
subset of episodes. Using an interesting class of serial episodes with
inter-event constraints and a novel encoding scheme for data using such
episodes, we present algorithms for discovering small set of episodes that
achieve good data compression. Using an example of the data streams obtained
from distributed sensors in a composable coupled conveyor system, we show that
our method is very effective in unearthing highly relevant episodes and that
our scheme also achieves good data compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1053</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1053</id><created>2014-01-06</created><updated>2014-04-22</updated><authors><author><keyname>Ahrens</keyname><forenames>Benedikt</forenames></author><author><keyname>Spadotti</keyname><forenames>R&#xe9;gis</forenames></author></authors><title>Terminal semantics for codata types in intensional Martin-L\&quot;of type
  theory</title><categories>cs.LO math.CT</categories><comments>14 pages, ancillary files contain formalized proof in the proof
  assistant Coq; v2: 20 pages, title and abstract changed, give a terminal
  semantics for streams as well as for matrices, Coq proof files updated
  accordingly</comments><msc-class>68Q65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the notions of relative comonad and comodule over a
relative comonad, and use these notions to give a terminal coalgebra semantics
for the coinductive type families of streams and of infinite triangular
matrices, respectively, in intensional Martin-L\&quot;of type theory. Our results
are mechanized in the proof assistant Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1059</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1059</id><created>2014-01-06</created><updated>2014-09-01</updated><authors><author><keyname>Grover</keyname><forenames>Pulkit</forenames></author></authors><title>&quot;Information-Friction&quot; and its implications on minimum energy required
  for communication</title><categories>cs.IT cs.CC math-ph math.IT math.MP</categories><comments>Accepted in IEEE Trans. Information Theory; preliminary version
  presented at ISIT '13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Just as there are frictional losses associated with moving masses on a
surface, what if there were frictional losses associated with moving
information on a substrate? Indeed, many modes of communication suffer from
such frictional losses. We propose to model these losses as proportional to
&quot;bit-meters,&quot; i.e., the product of mass of information (i.e., the number of
bits) and the distance of information transport. We use this &quot;information-
friction&quot; model to understand fundamental energy requirements on encoding and
decoding in communication circuitry. First, for communication across a binary
input AWGN channel, we arrive at fundamental limits on bit-meters (and thus
energy consumption) for decoding implementations that have a predetermined
input-independent length of messages. For encoding, we relax the fixed-length
assumption and derive bounds for flexible-message- length implementations.
Using these lower bounds we show that the total (transmit + encoding +
decoding) energy-per-bit must diverge to infinity as the target error
probability is lowered to zero. Further, the closer the communication rate is
maintained to the channel capacity (as the target error-probability is lowered
to zero), the faster the required decoding energy diverges to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1061</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1061</id><created>2014-01-06</created><updated>2014-04-15</updated><authors><author><keyname>Verwer</keyname><forenames>Sicco</forenames></author><author><keyname>Zhang</keyname><forenames>Yingqian</forenames></author><author><keyname>Ye</keyname><forenames>Qing Chuan</forenames></author></authors><title>Learning optimization models in the presence of unknown relations</title><categories>cs.AI cs.GT</categories><comments>37 pages. Working paper</comments><acm-class>F.5.3; K.3; K.4</acm-class><doi>10.1016/j.artint.2015.05.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a sequential auction with multiple bidding agents, it is highly
challenging to determine the ordering of the items to sell in order to maximize
the revenue due to the fact that the autonomy and private information of the
agents heavily influence the outcome of the auction.
  The main contribution of this paper is two-fold. First, we demonstrate how to
apply machine learning techniques to solve the optimal ordering problem in
sequential auctions. We learn regression models from historical auctions, which
are subsequently used to predict the expected value of orderings for new
auctions. Given the learned models, we propose two types of optimization
methods: a black-box best-first search approach, and a novel white-box approach
that maps learned models to integer linear programs (ILP) which can then be
solved by any ILP-solver. Although the studied auction design problem is hard,
our proposed optimization methods obtain good orderings with high revenues.
  Our second main contribution is the insight that the internal structure of
regression models can be efficiently evaluated inside an ILP solver for
optimization purposes. To this end, we provide efficient encodings of
regression trees and linear regression models as ILP constraints. This new way
of using learned models for optimization is promising. As the experimental
results show, it significantly outperforms the black-box best-first search in
nearly all settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1065</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1065</id><created>2014-01-06</created><updated>2014-11-05</updated><authors><author><keyname>Ghari</keyname><forenames>Meghdad</forenames></author></authors><title>Labeled Sequent Calculus and Countermodel Construction for Justification
  Logics</title><categories>math.LO cs.LO</categories><comments>71 pages</comments><msc-class>03B45, 03B60, 03F05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Justification logics are modal-like logics that provide a framework for
reasoning about justifications. This paper introduces labeled sequent calculi
for justification logics, as well as for hybrid modal-justification logics.
Using the method due to Sara Negri, we internalize the Kripke-style semantics
of justification logics, known as Fitting models, within the syntax of the
sequent calculus to produce labeled sequent calculus. We show that our labeled
sequent calculi enjoy a weak subformula property, all of the rules are
invertible and the structural rules (weakening and contraction) and cut are
admissible. Finally soundness and completeness are established, and termination
of proof search for some of the labeled systems are shown. We describe a
procedure, for some of the labeled systems, which produces a derivation for
valid sequents and a countermodel for non-valid sequents. We also show a model
correspondence for justification logics in the context of labeled sequent
calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1085</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1085</id><created>2014-01-06</created><updated>2014-06-30</updated><authors><author><keyname>Alewijnse</keyname><forenames>Sander P. A.</forenames></author><author><keyname>Bouts</keyname><forenames>Quirijn W.</forenames></author><author><keyname>Brink</keyname><forenames>Alex P. ten</forenames></author><author><keyname>Buchin</keyname><forenames>Kevin</forenames></author></authors><title>Distribution-Sensitive Construction of the Greedy Spanner</title><categories>cs.CG cs.DS</categories><comments>16 pages,22 figures. Full version of the ESA 2014 publication with
  the same title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The greedy spanner is the highest quality geometric spanner (in e.g. edge
count and weight, both in theory and practice) known to be computable in
polynomial time. Unfortunately, all known algorithms for computing it take
Omega(n^2) time, limiting its applicability on large data sets.
  We observe that for many point sets, the greedy spanner has many `short'
edges that can be determined locally and usually quickly, and few or no `long'
edges that can usually be determined quickly using local information and the
well-separated pair decomposition. We give experimental results showing large
to massive performance increases over the state-of-the-art on nearly all tests
and real-life data sets. On the theoretical side we prove a near-linear
expected time bound on uniform point sets and a near-quadratic worst-case
bound.
  Our bound for point sets drawn uniformly and independently at random in a
square follows from a local characterization of t-spanners we give on such
point sets: we give a geometric property that holds with high probability on
such point sets. This property implies that if an edge set on these points has
t-paths between pairs of points `close' to each other, then it has t-paths
between all pairs of points.
  This characterization gives a O(n log^2 n log^2 log n) expected time bound on
our greedy spanner algorithm, making it the first subquadratic time algorithm
for this problem on any interesting class of points. We also use this
characterization to give a O((n + |E|) log^2 n log log n) expected time
algorithm on uniformly distributed points that determines if E is a t-spanner,
making it the first subquadratic time algorithm for this problem that does not
make assumptions on E.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1086</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1086</id><created>2014-01-06</created><authors><author><keyname>Shakarian</keyname><forenames>Paulo</forenames></author><author><keyname>Lei</keyname><forenames>Hansheng</forenames></author><author><keyname>Lindelauf</keyname><forenames>Roy</forenames></author></authors><title>Power Grid Defense Against Malicious Cascading Failure</title><categories>cs.CR cs.MA physics.soc-ph</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  An adversary looking to disrupt a power grid may look to target certain
substations and sources of power generation to initiate a cascading failure
that maximizes the number of customers without electricity. This is
particularly an important concern when the enemy has the capability to launch
cyber-attacks as practical concerns (i.e. avoiding disruption of service,
presence of legacy systems, etc.) may hinder security. Hence, a defender can
harden the security posture at certain power stations but may lack the time and
resources to do this for the entire power grid. We model a power grid as a
graph and introduce the cascading failure game in which both the defender and
attacker choose a subset of power stations such as to minimize (maximize) the
number of consumers having access to producers of power. We formalize problems
for identifying both mixed and deterministic strategies for both players, prove
complexity results under a variety of different scenarios, identify tractable
cases, and develop algorithms for these problems. We also perform an
experimental evaluation of the model and game on a real-world power grid
network. Empirically, we noted that the game favors the attacker as he benefits
more from increased resources than the defender. Further, the minimax defense
produces roughly the same expected payoff as an easy-to-compute deterministic
load based (DLB) defense when played against a minimax attack strategy.
However, DLB performs more poorly than minimax defense when faced with the
attacker's best response to DLB. This is likely due to the presence of low-load
yet high-payoff nodes, which we also found in our empirical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1100</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1100</id><created>2014-01-03</created><updated>2014-02-03</updated><authors><author><keyname>Jaffe</keyname><forenames>Klaus</forenames></author><author><keyname>Florez</keyname><forenames>Astrid</forenames></author><author><keyname>Gomes</keyname><forenames>Cristina M</forenames></author><author><keyname>Rodriguez</keyname><forenames>Daniel</forenames></author><author><keyname>Achury</keyname><forenames>Carla</forenames></author></authors><title>On the biological and cultural evolution of shame: Using internet search
  tools to weight values in many cultures</title><categories>cs.CY</categories><comments>Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shame has clear biological roots and its precise form of expression affects
social cohesion and cultural characteristics. Here we explore the relative
importance between shame and guilt by using Google Translate to produce
translation for the words shame, guilt, pain, embarrassment and fear to the 64
languages covered. We also explore the meanings of these concepts among the
Yanomami, a horticulturist hunter-gatherer tribe in the Orinoquia. Results show
that societies previously described as 'guilt societies' have more words for
guilt than for shame, but the large majority, including the societies
previously described as 'shame societies', have more words for shame than for
guilt. Results are consistent with evolutionary models of shame which predict a
wide scatter in the relative importance between guilt and shame, suggesting
that cultural evolution of shame has continued the work of biological
evolution, and that neither provides a strong adaptive advantage to either
shame or guilt. We propose that the study of shame will improve our
understanding of the interaction between biological and cultural evolution in
the evolution of cognition and emotions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1106</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1106</id><created>2014-01-06</created><updated>2014-07-06</updated><authors><author><keyname>Krahmer</keyname><forenames>Felix</forenames></author><author><keyname>Rauhut</keyname><forenames>Holger</forenames></author></authors><title>Structured random measurements in signal processing</title><categories>cs.IT math.IT</categories><comments>22 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing and its extensions have recently triggered interest in
randomized signal acquisition. A key finding is that random measurements
provide sparse signal reconstruction guarantees for efficient and stable
algorithms with a minimal number of samples. While this was first shown for
(unstructured) Gaussian random measurement matrices, applications require
certain structure of the measurements leading to structured random measurement
matrices. Near optimal recovery guarantees for such structured measurements
have been developed over the past years in a variety of contexts. This article
surveys the theory in three scenarios: compressed sensing (sparse recovery),
low rank matrix recovery, and phaseless estimation. The random measurement
matrices to be considered include random partial Fourier matrices, partial
random circulant matrices (subsampled convolutions), matrix completion, and
phase estimation from magnitudes of Fourier type measurements. The article
concludes with a brief discussion of the mathematical techniques for the
analysis of such structured random measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1117</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1117</id><created>2014-01-06</created><updated>2014-01-11</updated><authors><author><keyname>Mukherjee</keyname><forenames>Manuj</forenames></author><author><keyname>Kashyap</keyname><forenames>Navin</forenames></author></authors><title>On the Communication Complexity of Secret Key Generation in the
  Multiterminal Source Model</title><categories>cs.IT math.IT</categories><comments>A 5-page version of this manuscript will be submitted to the 2014
  IEEE International Symposium on Information Theory (ISIT 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication complexity refers to the minimum rate of public communication
required for generating a maximal-rate secret key (SK) in the multiterminal
source model of Csiszar and Narayan. Tyagi recently characterized this
communication complexity for a two-terminal system. We extend the ideas in
Tyagi's work to derive a lower bound on communication complexity in the general
multiterminal setting. In the important special case of the complete graph
pairwise independent network (PIN) model, our bound allows us to determine the
exact linear communication complexity, i.e., the communication complexity when
the communication and SK are restricted to be linear functions of the
randomness available at the terminals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1123</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1123</id><created>2014-01-06</created><authors><author><keyname>Galichet</keyname><forenames>Nicolas</forenames><affiliation>LRI, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Sebag</keyname><forenames>Mich&#xe8;le</forenames><affiliation>LRI, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Teytaud</keyname><forenames>Olivier</forenames><affiliation>LRI, INRIA Saclay - Ile de France</affiliation></author></authors><title>Exploration vs Exploitation vs Safety: Risk-averse Multi-Armed Bandits</title><categories>cs.LG</categories><comments>16 pages</comments><proxy>ccsd</proxy><journal-ref>Asian Conference on Machine Learning 2013, Canberra : Australia
  (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by applications in energy management, this paper presents the
Multi-Armed Risk-Aware Bandit (MARAB) algorithm. With the goal of limiting the
exploration of risky arms, MARAB takes as arm quality its conditional value at
risk. When the user-supplied risk level goes to 0, the arm quality tends toward
the essential infimum of the arm distribution density, and MARAB tends toward
the MIN multi-armed bandit algorithm, aimed at the arm with maximal minimal
value. As a first contribution, this paper presents a theoretical analysis of
the MIN algorithm under mild assumptions, establishing its robustness
comparatively to UCB. The analysis is supported by extensive experimental
validation of MIN and MARAB compared to UCB and state-of-art risk-aware MAB
algorithms on artificial and real-world problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1124</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1124</id><created>2014-01-06</created><updated>2014-05-10</updated><authors><author><keyname>Chen</keyname><forenames>Yu</forenames></author><author><keyname>Xie</keyname><forenames>Weicheng</forenames></author><author><keyname>Zou</keyname><forenames>Xiufen</forenames></author></authors><title>A binary differential evolution algorithm learning from explored
  solutions</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although real-coded differential evolution (DE) algorithms can perform well
on continuous optimization problems (CoOPs), it is still a challenging task to
design an efficient binary-coded DE algorithm. Inspired by the learning
mechanism of particle swarm optimization (PSO) algorithms, we propose a binary
learning differential evolution (BLDE) algorithm that can efficiently locate
the global optimal solutions by learning from the last population. Then, we
theoretically prove the global convergence of BLDE, and compare it with some
existing binary-coded evolutionary algorithms (EAs) via numerical experiments.
Numerical results show that BLDE is competitive to the compared EAs, and
meanwhile, further study is performed via the change curves of a renewal metric
and a refinement metric to investigate why BLDE cannot outperform some compared
EAs for several selected benchmark problems. Finally, we employ BLDE solving
the unit commitment problem (UCP) in power systems to show its applicability in
practical problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1125</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1125</id><created>2014-01-06</created><authors><author><keyname>Anderson</keyname><forenames>Matthew</forenames></author><author><keyname>Dawar</keyname><forenames>Anuj</forenames></author></authors><title>On Symmetric Circuits and Fixed-Point Logics</title><categories>cs.CC cs.LO</categories><comments>22 pages. Full version of a paper to appear in STACS 2014</comments><acm-class>F.1.3; F.4.1; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study properties of relational structures such as graphs that are decided
by families of Boolean circuits. Circuits that decide such properties are
necessarily invariant to permutations of the elements of the input structures.
We focus on families of circuits that are symmetric, i.e., circuits whose
invariance is witnessed by automorphisms of the circuit induced by the
permutation of the input structure. We show that the expressive power of such
families is closely tied to definability in logic. In particular, we show that
the queries defined on structures by uniform families of symmetric Boolean
circuits with majority gates are exactly those definable in fixed-point logic
with counting. This shows that inexpressibility results in the latter logic
lead to lower bounds against polynomial-size families of symmetric circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1137</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1137</id><created>2014-01-06</created><updated>2015-03-27</updated><authors><author><keyname>Caron</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Fox</keyname><forenames>Emily B.</forenames></author></authors><title>Sparse graphs using exchangeable random measures</title><categories>stat.ME cs.SI math.ST stat.ML stat.TH</categories><comments>New title. Extended version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical network modeling has focused on representing the graph as a
discrete structure, namely the adjacency matrix, and considering the
exchangeability of this array. In such cases, the Aldous-Hoover representation
theorem (Aldous, 1981;Hoover, 1979} applies and informs us that the graph is
necessarily either dense or empty. In this paper, we instead consider
representing the graph as a measure on $\mathbb{R}_+^2$. For the associated
definition of exchangeability in this continuous space, we rely on the
Kallenberg representation theorem (Kallenberg, 2005). We show that for certain
choices of such exchangeable random measures underlying our graph construction,
our network process is sparse with power-law degree distribution. In
particular, we build on the framework of completely random measures (CRMs) and
use the theory associated with such processes to derive important network
properties, such as an urn representation for our analysis and network
simulation. Our theoretical results are explored empirically and compared to
common network models. We then present a Hamiltonian Monte Carlo algorithm for
efficient exploration of the posterior distribution and demonstrate that we are
able to recover graphs ranging from dense to sparse--and perform associated
tests--based on our flexible CRM-based formulation. We explore network
properties in a range of real datasets, including Facebook social circles, a
political blogosphere, protein networks, citation networks, and world wide web
networks, including networks with hundreds of thousands of nodes and millions
of edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1138</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1138</id><created>2014-01-06</created><authors><author><keyname>Ispas</keyname><forenames>Adrian</forenames></author><author><keyname>Schneider</keyname><forenames>Christian</forenames></author><author><keyname>Ascheid</keyname><forenames>Gerd</forenames></author><author><keyname>Thom&#xe4;</keyname><forenames>Reiner</forenames></author></authors><title>Analysis of the Local Quasi-Stationarity of Measured Dual-Polarized MIMO
  Channels</title><categories>cs.IT math.IT</categories><doi>10.1109/TVT.2014.2358942</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is common practice in wireless communications to assume strict or
wide-sense stationarity of the wireless channel in time and frequency. While
this approximation has some physical justification, it is only valid inside
certain time-frequency regions. This paper presents an elaborate
characterization of the non-stationarity of wireless dual-polarized channels in
time. The evaluation is based on urban macrocell measurements performed at 2.53
GHz. In order to define local quasi-stationarity (LQS) regions, i.e., regions
in which the change of certain channel statistics is deemed insignificant, we
resort to the performance degradation of selected algorithms specific to
channel estimation and beamforming. Additionally, we compare our results to
commonly used measures in the literature. We find that the polarization, the
antenna spacing, and the opening angle of the antennas into the propagation
channel can strongly influence the non-stationarity of the observed channel.
The obtained LQS regions can be of significant size, i.e., several meters, and
thus the reuse of channel statistics over large distances is meaningful (in an
average sense) for certain algorithms. Furthermore, we conclude that, from a
system perspective, a proper non-stationarity analysis should be based on the
considered algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1139</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1139</id><created>2014-01-06</created><authors><author><keyname>Polonsky</keyname><forenames>Andrew</forenames></author></authors><title>Extensionality of lambda-*</title><categories>cs.LO</categories><comments>25 pages</comments><msc-class>03F50</msc-class><acm-class>F.4.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We prove an extensionality theorem for the &quot;type-in-type&quot; dependent type
theory with Sigma-types. We suggest that the extensional equality type be
identified with the logical equivalence relation on the free term model of type
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1140</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1140</id><created>2014-01-06</created><updated>2014-01-07</updated><authors><author><keyname>Bacher</keyname><forenames>Axel</forenames></author><author><keyname>Bodini</keyname><forenames>Olivier</forenames></author><author><keyname>Jacquot</keyname><forenames>Alice</forenames></author></authors><title>Efficient random sampling of binary and unary-binary trees via holonomic
  equations</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new uniform random sampler for binary trees with $n$ internal
nodes consuming $2n + \Theta(\log(n)^2)$ random bits on average. This makes it
quasi-optimal and out-performs the classical Remy algorithm. We also present a
sampler for unary-binary trees with $n$ nodes taking $\Theta(n)$ random bits on
average. Both are the first linear-time algorithms to be optimal up to a
constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1148</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1148</id><created>2014-01-06</created><updated>2015-02-19</updated><authors><author><keyname>Polonsky</keyname><forenames>Andrew</forenames></author></authors><title>Internalization of extensional equality</title><categories>cs.LO</categories><comments>31 pages</comments><msc-class>03B15</msc-class><acm-class>F.4.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We give a type system in which the universe of types is closed by reflection
into it of the logical relation defined externally by induction on the
structure of types. This contribution is placed in the context of the search
for a natural, syntactic construction of the extensional equality type (Tait
[1995], Altenkirch [1999], Coquand [2011], Licata and Harper [2012], Martin-Lof
[2013]). The system is presented as an extension of lambda-*, the terminal pure
type system in which the universe of all types is a type. The universe
inconsistency is then removed by the usual method of stratification into
levels. We give a set-theoretic model for the stratified system. We conjecture
that Strong Normalization holds as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1152</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1152</id><created>2014-01-06</created><updated>2015-01-09</updated><authors><author><keyname>Bene&#x161;</keyname><forenames>Michal</forenames></author><author><keyname>&#x160;tefan</keyname><forenames>Radek</forenames></author></authors><title>Hygro-thermo-mechanical analysis of spalling in concrete walls at high
  temperatures as a moving boundary problem</title><categories>cs.CE</categories><doi>10.1016/j.ijheatmasstransfer.2015.01.050</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mathematical model allowing coupled hygro-thermo-mechanical analysis of
spalling in concrete walls at high temperatures by means of the moving boundary
problem is presented. A simplified mechanical approach to account for effects
of thermal stresses and pore pressure build-up on spalling is incorporated into
the model. The numerical algorithm based on finite element discretization in
space and the semi-implicit method for discretization in time is presented. The
validity of the developed model is carefully examined by a comparison between
experimental tests performed by Kalifa et al. (2000) and Mindeguia (2009) on
concrete prismatic specimens under unidirectional heating of temperature of 600
${\deg}$C and ISO 834 fire curve and the results obtained from the numerical
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1154</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1154</id><created>2014-01-06</created><updated>2014-01-07</updated><authors><author><keyname>Ferrari</keyname><forenames>Franco</forenames></author><author><keyname>Zhao</keyname><forenames>Yani</forenames></author></authors><title>Monte Carlo Computation of the Vassiliev knot invariant of degree 2 in
  the integral representation</title><categories>math.NA cs.NA</categories><comments>35 pages, 13 figures, LaTeX + RevTeX 4.1, a mistake in the name of
  one of the authors has been corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In mathematics there is a wide class of knot invariants that may be expressed
in the form of multiple line integrals computed along the trajectory C
describing the spatial conformation of the knot. In this work it is addressed
the problem of evaluating invariants of this kind in the case in which the knot
is discrete, i.e. its trajectory is constructed by joining together a set of
segments of constant length. Such discrete knots appear almost everywhere in
numerical simulations of systems containing one dimensional ring-shaped
objects. Examples are polymers, the vortex lines in fluids and superfluids like
helium and other quantum liquids. Formally, the trajectory of a discrete knot
is a piecewise smooth curve characterized by sharp corners at the joints
between contiguous segments. The presence of these corners spoils the
topological invariance of the knot invariants considered here and prevents the
correct evaluation of their values. To solve this problem, a smoothing
procedure is presented, which eliminates the sharp corners and transforms the
original path C into a curve that is everywhere differentiable. The procedure
is quite general and can be applied to any discrete knot defined off or on
lattice. This smoothing algorithm is applied to the computation of the
Vassiliev knot invariant of degree 2 denoted here with the symbol r(C). This is
the simplest knot invariant that admits a definition in terms of multiple line
integrals. For a fast derivation of r(C), it is used a Monte Carlo integration
technique. It is shown that, after the smoothing, the values of r(C) may be
evaluated with an arbitrary precision. Several algorithms for the fast
computation of the Vassiliev knot invariant of degree 2 are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1158</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1158</id><created>2014-01-06</created><authors><author><keyname>Roth</keyname><forenames>Benjamin</forenames></author><author><keyname>Barth</keyname><forenames>Tassilo</forenames></author><author><keyname>Wiegand</keyname><forenames>Michael</forenames></author><author><keyname>Singh</keyname><forenames>Mittul</forenames></author><author><keyname>Klakow</keyname><forenames>Dietrich</forenames></author></authors><title>Effective Slot Filling Based on Shallow Distant Supervision Methods</title><categories>cs.CL</categories><comments>to be published in: Proceedings of the Sixth Text Analysis Conference
  (TAC 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spoken Language Systems at Saarland University (LSV) participated this year
with 5 runs at the TAC KBP English slot filling track. Effective algorithms for
all parts of the pipeline, from document retrieval to relation prediction and
response post-processing, are bundled in a modular end-to-end relation
extraction system called RelationFactory. The main run solely focuses on
shallow techniques and achieved significant improvements over LSV's last year's
system, while using the same training data and patterns. Improvements mainly
have been obtained by a feature representation focusing on surface skip n-grams
and improved scoring for extracted distant supervision patterns. Important
factors for effective extraction are the training and tuning scheme for distant
supervision classifiers, and the query expansion by a translation model based
on Wikipedia links. In the TAC KBP 2013 English Slotfilling evaluation, the
submitted main run of the LSV RelationFactory system achieved the top-ranked
F1-score of 37.3%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1163</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1163</id><created>2014-01-06</created><updated>2014-01-08</updated><authors><author><keyname>Baccelli</keyname><forenames>Emmanuel</forenames></author><author><keyname>Juraschek</keyname><forenames>Felix</forenames></author><author><keyname>Hahm</keyname><forenames>Oliver</forenames></author><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author><author><keyname>Will</keyname><forenames>Heiko</forenames></author><author><keyname>W&#xe4;hlisch</keyname><forenames>Matthias</forenames></author></authors><title>Proceedings of the 3rd MANIAC Challenge, Berlin, Germany, July 27 - 28,
  2013</title><categories>cs.NI</categories><acm-class>C.2.1; C.2.2; C.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the 3rd MANIAC Challenge, which was held in
Berlin, Germany, July 27 - 28, 2013.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1171</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1171</id><created>2014-01-06</created><updated>2014-06-22</updated><authors><author><keyname>Yu</keyname><forenames>Zhenhua</forenames></author><author><keyname>Redfern</keyname><forenames>Arthur J.</forenames></author><author><keyname>Zhou</keyname><forenames>G. Tong</forenames></author></authors><title>Using Delta-Sigma Modulators in Visible Light OFDM Systems</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Wireless and Optical Communication Conference (WOCC
  2014), Newark, New Jersey, May 2014</comments><doi>10.1109/WOCC.2014.6839942</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visible light communications (VLC) are motivated by the radio-frequency (RF)
spectrum crunch and fast-growing solid-state lighting technology. VLC relies on
white light emitting diodes (LEDs) to provide communication and illumination
simultaneously. Simple two-level on-off keying (OOK) and pulse-position
modulation (PPM) are supported in IEEE standard due to their compatibility with
existing constant current LED drivers, but their low spectral efficiency have
limited the achievable data rates of VLC. Orthogonal frequency division
multiplexing (OFDM) has been applied to VLC due to its high spectral efficiency
and ability to combat inter-symbol-interference (ISI). However, VLC-OFDM
inherits the disadvantage of high peak-to-average power ratio (PAPR) from
RF-OFDM. Besides, the continuous magnitude of OFDM signals requires complicated
mixed-signal digital-to-analog converter (DAC) and modification of LED drivers.
We propose the use of delta-sigma modulators in visible light OFDM systems to
convert continuous magnitude OFDM symbols into LED driver signals. The proposed
system has the communication theory advantages of OFDM along with the practical
analog and optical advantages of simple two level driver signals. Simulation
results are provided to illustrate the proposed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1174</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1174</id><created>2014-01-06</created><authors><author><keyname>Zakerzadeh</keyname><forenames>Hessam</forenames></author><author><keyname>Aggrawal</keyname><forenames>Charu C.</forenames></author><author><keyname>Barker</keyname><forenames>Ken</forenames></author></authors><title>Towards Breaking the Curse of Dimensionality for High-Dimensional
  Privacy: An Extended Version</title><categories>cs.DB</categories><comments>13 pages, An extended version of the paper accepted in 2014 SIAM
  international conference on Data Mining</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The curse of dimensionality has remained a challenge for a wide variety of
algorithms in data mining, clustering, classification and privacy. Recently, it
was shown that an increasing dimensionality makes the data resistant to
effective privacy. The theoretical results seem to suggest that the
dimensionality curse is a fundamental barrier to privacy preservation. However,
in practice, we show that some of the common properties of real data can be
leveraged in order to greatly ameliorate the negative effects of the curse of
dimensionality. In real data sets, many dimensions contain high levels of
inter-attribute correlations. Such correlations enable the use of a process
known as vertical fragmentation in order to decompose the data into vertical
subsets of smaller dimensionality. An information-theoretic criterion of mutual
information is used in the vertical decomposition process. This allows the use
of an anonymization process, which is based on combining results from multiple
independent fragments. We present a general approach which can be applied to
the k-anonymity, l-diversity, and t-closeness models. In the presence of
inter-attribute correlations, such an approach continues to be much more robust
in higher dimensionality, without losing accuracy. We present experimental
results illustrating the effectiveness of the approach. This approach is
resilient enough to prevent identity, attribute, and membership disclosure
attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1190</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1190</id><created>2014-01-06</created><authors><author><keyname>Bhowmick</keyname><forenames>Souvik</forenames></author><author><keyname>Banerjee</keyname><forenames>Purnendu</forenames></author></authors><title>Bangla Text Recognition from Video Sequence: A New Focus</title><categories>cs.CV</categories><journal-ref>NATIONAL CONFERENCE ON COMPUTING AND SYSTEMS (NaCCS), pp.
  62-67,2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extraction and recognition of Bangla text from video frame images is
challenging due to complex color background, low-resolution etc. In this paper,
we propose an algorithm for extraction and recognition of Bangla text form such
video frames with complex background. Here, a two-step approach has been
proposed. First, the text line is segmented into words using information based
on line contours. First order gradient value of the text blocks are used to
find the word gap. Next, a local binarization technique is applied on each word
and text line is reconstructed using those words. Secondly, this binarized text
block is sent to OCR for recognition purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1191</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1191</id><created>2013-11-07</created><authors><author><keyname>Chen</keyname><forenames>Zichong</forenames></author><author><keyname>Ranieri</keyname><forenames>Juri</forenames></author><author><keyname>Zhang</keyname><forenames>Runwei</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>DASS: Distributed Adaptive Sparse Sensing</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks are often designed to perform two tasks: sensing a
physical field and transmitting the data to end-users. A crucial aspect of the
design of a WSN is the minimization of the overall energy consumption. Previous
researchers aim at optimizing the energy spent for the communication, while
mostly ignoring the energy cost due to sensing. Recently, it has been shown
that considering the sensing energy cost can be beneficial for further
improving the overall energy efficiency. More precisely, sparse sensing
techniques were proposed to reduce the amount of collected samples and recover
the missing data by using data statistics. While the majority of these
techniques use fixed or random sampling patterns, we propose to adaptively
learn the signal model from the measurements and use the model to schedule when
and where to sample the physical field. The proposed method requires minimal
on-board computation, no inter-node communications and still achieves appealing
reconstruction performance. With experiments on real-world datasets, we
demonstrate significant improvements over both traditional sensing schemes and
the state-of-the-art sparse sensing schemes, particularly when the measured
data is characterized by a strong intra-sensor (temporal) or inter-sensors
(spatial) correlation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1203</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1203</id><created>2014-01-05</created><updated>2014-06-09</updated><authors><author><keyname>Liu</keyname><forenames>Zhiyang</forenames></author><author><keyname>Dai</keyname><forenames>Lin</forenames></author></authors><title>A Comparative Study of Downlink MIMO Cellular Networks with Co-located
  and Distributed Base-Station Antennas</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><msc-class>94A15</msc-class><doi>10.1109/TWC.2014.2355833</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the common belief that substantial capacity gains can be achieved by
using more antennas at the base-station (BS) side in cellular networks, the
effect of BS antenna topology on the capacity scaling behavior is little
understood. In this paper, we present a comparative study on the ergodic
capacity of a downlink single-user multiple-input-multiple-output (MIMO) system
where BS antennas are either co-located at the center or grouped into uniformly
distributed antenna clusters in a circular cell. By assuming that the number of
BS antennas and the number of user antennas go to infinity with a fixed ratio
$L\gg 1$, the asymptotic analysis reveals that the average per-antenna
capacities in both cases logarithmically increase with $L$, but in the orders
of $\log_2 L$ and $\tfrac{\alpha}{2}\log_2 L$, for the co-located and
distributed BS antenna layouts, respectively, where $\alpha&gt;2$ denotes the
path-loss factor. The analysis is further extended to the multi-user case where
a 1-tier (7-cell) MIMO cellular network with $K\gg 1$ uniformly distributed
users in each cell is considered. By assuming that the number of BS antennas
and the number of user antennas go to infinity with a fixed ratio $L\gg K$, an
asymptotic analysis is presented on the downlink rate performance with block
diagonalization (BD) adopted at each BS. It is shown that the average
per-antenna rates with the co-located and distributed BS antenna layouts scale
in the orders of $\log_2 \tfrac{L}{K}$ and $\log_2
\frac{(L-K+1)^{\alpha/2}}{K}$, respectively. The rate performance of MIMO
cellular networks with small cells is also discussed, which highlights the
importance of employing a large number of distributed BS antennas for the
next-generation cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1206</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1206</id><created>2014-01-06</created><authors><author><keyname>Liu</keyname><forenames>Ming</forenames><affiliation>IETR</affiliation></author><author><keyname>H&#xe9;lard</keyname><forenames>Maryline</forenames><affiliation>IETR</affiliation></author><author><keyname>H&#xe9;lard</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>IETR</affiliation></author><author><keyname>Crussi&#xe8;re</keyname><forenames>Matthieu</forenames><affiliation>IETR</affiliation></author></authors><title>A Fast Decodable Full-Rate STBC with High Coding Gain for 4x2 MIMO
  Systems</title><categories>cs.IT math.IT</categories><comments>2013 IEEE 24th International Symposium on Personal Indoor and Mobile
  Radio Communications (PIMRC), London : United Kingdom (2013)</comments><proxy>ccsd</proxy><doi>10.1109/PIMRC.2013.6666222</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a new fast-decodable space-time block code (STBC) is proposed.
The code is full-rate and full-diversity for 4x2 multiple-input multiple-output
(MIMO) transmission. Due to the unique structure of the codeword, the proposed
code requires a much lower computational complexity to provide
maximum-likelihood (ML) decoding performance. It is shown that the ML decoding
complexity is only O(M^{4.5}) when M-ary square QAM constellation is used.
Finally, the proposed code has highest minimum determinant among the
fast-decodable STBCs known in the literature. Simulation results prove that the
proposed code provides the best bit error rate (BER) performance among the
state-of-the-art STBCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1225</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1225</id><created>2014-01-06</created><authors><author><keyname>Bosek</keyname><forenames>Bart&#x142;omiej</forenames></author><author><keyname>Felsner</keyname><forenames>Stefan</forenames></author><author><keyname>Knauer</keyname><forenames>Kolja</forenames></author><author><keyname>Matecki</keyname><forenames>Grzegorz</forenames></author></authors><title>On the Duality of Semiantichains and Unichain Coverings</title><categories>math.CO cs.DM</categories><comments>10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a min-max relation conjectured by Saks and West: For any two posets
$P$ and $Q$ the size of a maximum semiantichain and the size of a minimum
unichain covering in the product $P\times Q$ are equal. For positive we state
conditions on $P$ and $Q$ that imply the min-max relation. Based on these
conditions we identify some new families of posets where the conjecture holds
and get easy proofs for several instances where the conjecture had been
verified before. However, we also have examples showing that in general the
min-max relation is false, i.e., we disprove the Saks-West conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1232</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1232</id><created>2014-01-06</created><authors><author><keyname>Santana</keyname><forenames>Yeray Cachon</forenames></author></authors><title>A Cryptographic Scheme Of Mellin Transform</title><categories>cs.CR</categories><comments>Cryptography using Mellin's transform</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper it has been developed an algorithm for cryptography, using the
Mellin's transform. Cryptography is very important to protect data to ensure
that two people, using an insecure channel, may communicate in a secure way. In
the present age, ensure the communications will essential to shared data that
have to be protected. The original message is a plain text, and the encrypted
form as cipher text. The cipher text message contains all the information of
the plain text, but is cannot be read from a human without a key and a method
to decrypt it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1236</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1236</id><created>2014-01-06</created><authors><author><keyname>Gomez</keyname><forenames>Sergio</forenames></author><author><keyname>Fernandez</keyname><forenames>Alberto</forenames></author><author><keyname>Granell</keyname><forenames>Clara</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author></authors><title>Structural patterns in complex systems using multidendrograms</title><categories>physics.data-an cs.IR cs.SI physics.comp-ph physics.soc-ph</categories><journal-ref>Entropy 15 (2013) 5464-5474</journal-ref><doi>10.3390/e15125464</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex systems are usually represented as an intricate set of relations
between their components forming a complex graph or network. The understanding
of their functioning and emergent properties are strongly related to their
structural properties. The finding of structural patterns is of utmost
importance to reduce the problem of understanding the structure-function
relationships. Here we propose the analysis of similarity measures between
nodes using hierarchical clustering methods. The discrete nature of the
networks usually leads to a small set of different similarity values, making
standard hierarchical clustering algorithms ambiguous. We propose the use of
&quot;multidendrograms&quot;, an algorithm that computes agglomerative hierarchical
clusterings implementing a variable-group technique that solves the
non-uniqueness problem found in the standard pair-group algorithm. This problem
arises when there are more than two clusters separated by the same maximum
similarity (or minimum distance) during the agglomerative process. Forcing
binary trees in this case means breaking ties in some way, thus giving rise to
different output clusterings depending on the criterion used. Multidendrograms
solves this problem grouping more than two clusters at the same time when ties
occur.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1239</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1239</id><created>2014-01-06</created><authors><author><keyname>Asadi</keyname><forenames>Behzad</forenames></author><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>The Capacity of Three-Receiver AWGN Broadcast Channels with Receiver
  Message Side Information</title><categories>cs.IT math.IT</categories><comments>This is an extended version of the same-titled paper submitted to
  IEEE International Symposium on Information Theory (ISIT) 2014</comments><journal-ref>Proceedings of the 2014 IEEE International Symposium on
  Information Theory (ISIT 2014), Honolulu, USA, pp. 2899-2903, June 29-July 4,
  2014</journal-ref><doi>10.1109/ISIT.2014.6875364</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the capacity region of three-receiver AWGN broadcast
channels where the receivers (i) have private-message requests and (ii) know
the messages requested by some other receivers as side information. We classify
these channels based on their side information into eight groups, and construct
different transmission schemes for the groups. For six groups, we characterize
the capacity region, and show that it improves both the best known inner and
outer bounds. For the remaining two groups, we improve the best known inner
bound by using side information during channel decoding at the receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1247</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1247</id><created>2014-01-06</created><updated>2014-04-22</updated><authors><author><keyname>Niepert</keyname><forenames>Mathias</forenames></author><author><keyname>Broeck</keyname><forenames>Guy Van den</forenames></author></authors><title>Tractability through Exchangeability: A New Perspective on Efficient
  Probabilistic Inference</title><categories>cs.AI</categories><comments>In Proceedings of the 28th AAAI Conference on Artificial Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exchangeability is a central notion in statistics and probability theory. The
assumption that an infinite sequence of data points is exchangeable is at the
core of Bayesian statistics. However, finite exchangeability as a statistical
property that renders probabilistic inference tractable is less
well-understood. We develop a theory of finite exchangeability and its relation
to tractable probabilistic inference. The theory is complementary to that of
independence and conditional independence. We show that tractable inference in
probabilistic models with high treewidth and millions of variables can be
understood using the notion of finite (partial) exchangeability. We also show
that existing lifted inference algorithms implicitly utilize a combination of
conditional independence and partial exchangeability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1257</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1257</id><created>2014-01-06</created><updated>2014-09-18</updated><authors><author><keyname>Nematzadeh</keyname><forenames>Azadeh</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author><author><keyname>Ahn</keyname><forenames>Yong-Yeol</forenames></author></authors><title>Optimal network modularity for information diffusion</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 10 figures</comments><journal-ref>Phys. Rev. Lett. 113, 088701 (2014)</journal-ref><doi>10.1103/PhysRevLett.113.088701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the impact of community structure on information diffusion
with the linear threshold model. Our results demonstrate that modular structure
may have counter-intuitive effects on information diffusion when social
reinforcement is present. We show that strong communities can facilitate global
diffusion by enhancing local, intra-community spreading. Using both analytic
approaches and numerical simulations, we demonstrate the existence of an
optimal network modularity, where global diffusion require the minimal number
of early adopters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1258</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1258</id><created>2014-01-06</created><authors><author><keyname>Habak</keyname><forenames>Karim</forenames></author><author><keyname>Harras</keyname><forenames>Khaled A.</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author></authors><title>OSCAR: A Collaborative Bandwidth Aggregation System</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exponential increase in mobile data demand, coupled with growing user
expectation to be connected in all places at all times, have introduced novel
challenges for researchers to address. Fortunately, the wide spread deployment
of various network technologies and the increased adoption of multi-interface
enabled devices have enabled researchers to develop solutions for those
challenges. Such solutions aim to exploit available interfaces on such devices
in both solitary and collaborative forms. These solutions, however, have faced
a steep deployment barrier.
  In this paper, we present OSCAR, a multi-objective, incentive-based,
collaborative, and deployable bandwidth aggregation system. We present the
OSCAR architecture that does not introduce any intermediate hardware nor
require changes to current applications or legacy servers. The OSCAR
architecture is designed to automatically estimate the system's context,
dynamically schedule various connections and/or packets to different
interfaces, be backwards compatible with the current Internet architecture, and
provide the user with incentives for collaboration. We also formulate the OSCAR
scheduler as a multi-objective, multi-modal scheduler that maximizes system
throughput while minimizing energy consumption or financial cost. We evaluate
OSCAR via implementation on Linux, as well as via simulation, and compare our
results to the current optimal achievable throughput, cost, and energy
consumption. Our evaluation shows that, in the throughput maximization mode, we
provide up to 150% enhancement in throughput compared to current operating
systems, without any changes to legacy servers. Moreover, this performance gain
further increases with the availability of connection resume-supporting, or
OSCAR-enabled servers, reaching the maximum achievable upper-bound throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1274</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1274</id><created>2014-01-07</created><authors><author><keyname>Gao</keyname><forenames>Liang</forenames></author><author><keyname>Song</keyname><forenames>Chaoming</forenames></author><author><keyname>Gao</keyname><forenames>Ziyou</forenames></author><author><keyname>Barab&#xe1;si</keyname><forenames>Albert-L&#xe1;szl&#xf3;</forenames></author><author><keyname>Bagrow</keyname><forenames>James P.</forenames></author><author><keyname>Wang</keyname><forenames>Dashun</forenames></author></authors><title>Quantifying Information Flow During Emergencies</title><categories>physics.soc-ph cs.SI</categories><comments>Under review in Scientific Reports</comments><journal-ref>Scientific Reports 4, 3997 2014</journal-ref><doi>10.1038/srep03997</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recent advances on human dynamics have focused on the normal patterns of
human activities, with the quantitative understanding of human behavior under
extreme events remaining a crucial missing chapter. This has a wide array of
potential applications, ranging from emergency response and detection to
traffic control and management. Previous studies have shown that human
communications are both temporally and spatially localized following the onset
of emergencies, indicating that social propagation is a primary means to
propagate situational awareness. We study real anomalous events using
country-wide mobile phone data, finding that information flow during
emergencies is dominated by repeated communications. We further demonstrate
that the observed communication patterns cannot be explained by inherent
reciprocity in social networks, and are universal across different
demographics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1289</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1289</id><created>2014-01-07</created><authors><author><keyname>Heidrich</keyname><forenames>Jens</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Implementing Software Project Control Centers: An Architectural View</title><categories>cs.SE</categories><comments>14 pages. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F978-3-540-89403-2_25</comments><journal-ref>Software Process and Product Measurement, volume 5338 of Lecture
  Notes in Computer Science, pages 302-315. Springer Berlin Heidelberg, 2008</journal-ref><doi>10.1007/978-3-540-89403-2_25</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Setting up effective and efficient mechanisms for controlling software and
system development projects is still challenging in industrial practice. On the
one hand, necessary prerequisites such as established development processes,
understanding of cause-effect relationships on relevant indicators, and
sufficient sustainability of measurement programs are often missing. On the
other hand, there are more fundamental methodological deficits related to the
controlling process itself and to appropriate tool support. Additional
activities that would guarantee the usefulness, completeness, and precision of
the result- ing controlling data are widely missing. This article presents a
conceptual architecture for so-called Software Project Control Centers (SPCC)
that addresses these challenges. The architecture includes mechanisms for
getting sufficiently precise and complete data and supporting the information
needs of different stakeholders. In addition, an implementation of this
architecture, the so-called Specula Project Support Environment, is sketched,
and results from evaluating this implementation in industrial settings are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1290</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1290</id><created>2014-01-07</created><authors><author><keyname>Pantelis</keyname><forenames>Garry</forenames></author></authors><title>Program Verification of Numerical Computation</title><categories>cs.MS cs.SE</categories><msc-class>03Fxx</msc-class><acm-class>F.4.1; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These notes outline a formal method for program verification of numerical
computation. It forms the basis of the software package VPC in its initial
phase of development. Much of the style of presentation is in the form of notes
that outline the definitions and rules upon which VPC is based. The initial
motivation of this project was to address some practical issues of computation,
especially of numerically intensive programs that are commonplace in computer
models. The project evolved into a wider area for program construction as
proofs leading to a model of inference in a more general sense. Some basic
results of machine arithmetic are derived as a demonstration of VPC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1294</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1294</id><created>2014-01-07</created><updated>2014-09-03</updated><authors><author><keyname>Shokri-Ghadikolaei</keyname><forenames>Hossein</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author></authors><title>Analysis and Optimization of Random Sensing Order in Cognitive Radio
  Networks</title><categories>cs.IT cs.PF math.IT math.PR</categories><comments>16 pages, 12 figures, 7 tables, accepted in Journal of Selected Areas
  in Communications (J-SAC) CR series and will be published in Apr'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing an efficient spectrum access policy enables cognitive radios to
dramatically increase spectrum utilization while ensuring predetermined quality
of service levels for primary users. In this paper, modeling, performance
analysis, and optimization of a distributed secondary network with random
sensing order policy are studied. Specifically, the secondary users create a
random order of available channels upon primary users return, and then find
optimal transmission and handoff opportunities in a distributed manner. By a
Markov chain analysis, the average throughputs of the secondary users and
average interference level among the secondary and primary users are
investigated. A maximization of the secondary network performance in terms of
the throughput while keeping under control the average interference is
proposed. It is shown that despite of traditional view, non-zero false alarm in
the channel sensing can increase channel utilization, especially in a dense
secondary network where the contention is too high. Then, two simple and
practical adaptive algorithms are established to optimize the network. The
second algorithm follows the variations of the wireless channels in
non-stationary conditions and outperforms even static brute force optimization,
while demanding few computations. The convergence of the distributed algorithms
are theoretically investigated based on the analytical performance indicators
established by the Markov chain analysis. Finally, numerical results validate
the analytical derivations and demonstrate the efficiency of the proposed
schemes. It is concluded that fully distributed sensing order algorithms can
lead to substantial performance improvements in cognitive radio networks
without the need of centralized management or message passing among the users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1302</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1302</id><created>2014-01-07</created><authors><author><keyname>Roy</keyname><forenames>Senjuti Basu</forenames></author><author><keyname>Lykourentzou</keyname><forenames>Ioanna</forenames></author><author><keyname>Thirumuruganathan</keyname><forenames>Saravanan</forenames></author><author><keyname>Amer-Yahia</keyname><forenames>Sihem</forenames></author><author><keyname>Das</keyname><forenames>Gautam</forenames></author></authors><title>Optimization in Knowledge-Intensive Crowdsourcing</title><categories>cs.DB cs.SI</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present SmartCrowd, a framework for optimizing collaborative
knowledge-intensive crowdsourcing. SmartCrowd distinguishes itself by
accounting for human factors in the process of assigning tasks to workers.
Human factors designate workers' expertise in different skills, their expected
minimum wage, and their availability. In SmartCrowd, we formulate task
assignment as an optimization problem, and rely on pre-indexing workers and
maintaining the indexes adaptively, in such a way that the task assignment
process gets optimized both qualitatively, and computation time-wise. We
present rigorous theoretical analyses of the optimization problem and propose
optimal and approximation algorithms. We finally perform extensive performance
and quality experiments using real and synthetic data to demonstrate that
adaptive indexing in SmartCrowd is necessary to achieve efficient high quality
task assignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1307</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1307</id><created>2014-01-07</created><authors><author><keyname>Xiong</keyname><forenames>Jiping</forenames></author><author><keyname>Tang</keyname><forenames>Qinghua</forenames></author><author><keyname>Zhao</keyname><forenames>Jian</forenames></author></authors><title>1-bit Compressive Data Gathering for Wireless Sensor Networks</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Compressive sensing (CS) has been widely used for the data gathering in
wireless sensor networks for the purpose of reducing the communication overhead
recent years. In this paper, we first show that with simple modification, 1-bit
compressive sensing can also been used for the data gathering in wireless
sensor networks to further reduce the communication overhead. We also propose a
novel blind 1-bit CS reconstruction algorithm which outperforms other state of
the art blind 1-bit CS reconstruction algorithms. Experimental results on real
sensor datasets demonstrate the efficiency of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1308</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1308</id><created>2014-01-07</created><authors><author><keyname>Kretz</keyname><forenames>Tobias</forenames></author><author><keyname>Lehmann</keyname><forenames>Karsten</forenames></author><author><keyname>Hofs&#xe4;&#xdf;</keyname><forenames>Ingmar</forenames></author><author><keyname>Leonhardt</keyname><forenames>Axel</forenames></author></authors><title>Dynamic Assignment in Microsimulations of Pedestrians</title><categories>cs.CE cs.MA physics.soc-ph</categories><comments>Contribution to 93rd Annual Meeting of the Transportation Research
  Board (TRB) 2014. Contribution no. 14-0941</comments><journal-ref>in Annual Meeting of the Transportation Research Board, 14-0941
  (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A generic method for dynamic assignment used with microsimulation of
pedestrian dynamics is introduced. As pedestrians - unlike vehicles - do not
move on a network, but on areas they in principle can choose among an infinite
number of routes. To apply assignment algorithms one has to select for each OD
pair a finite (realistically a small) number of relevant representatives from
these routes. This geometric task is the main focus of this contribution. The
main task is to find for an OD pair the relevant routes to be used with common
assignment methods. The method is demonstrated for one single OD pair and
exemplified with an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1313</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1313</id><created>2014-01-07</created><authors><author><keyname>Mitra</keyname><forenames>Sayan</forenames></author></authors><title>Proving Abstractions of Dynamical Systems through Numerical Simulations</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key question that arises in rigorous analysis of cyberphysical systems
under attack involves establishing whether or not the attacked system deviates
significantly from the ideal allowed behavior. This is the problem of deciding
whether or not the ideal system is an abstraction of the attacked system. A
quantitative variation of this question can capture how much the attacked
system deviates from the ideal. Thus, algorithms for deciding abstraction
relations can help measure the effect of attacks on cyberphysical systems and
to develop attack detection strategies. In this paper, we present a decision
procedure for proving that one nonlinear dynamical system is a quantitative
abstraction of another. Directly computing the reach sets of these nonlinear
systems are undecidable in general and reach set over-approximations do not
give a direct way for proving abstraction. Our procedure uses (possibly
inaccurate) numerical simulations and a model annotation to compute tight
approximations of the observable behaviors of the system and then uses these
approximations to decide on abstraction. We show that the procedure is sound
and that it is guaranteed to terminate under reasonable robustness assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1318</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1318</id><created>2014-01-07</created><authors><author><keyname>Sekhar</keyname><forenames>Vorugunti Chandra</forenames></author><author><keyname>Sarvabhatla</keyname><forenames>Mrudula</forenames></author></authors><title>A Robust Biometric-Based Three-factor Remote User Authentication Scheme</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid development of Internet of Things (IoT) technology, which is an
inter connection of networks through an insecure public channel i.e. Internet
demands for authenticating the remote user trying to access the secure network
resources. In 2013, Ankita et al. proposed an improved three factor remote user
authentication scheme. In this poster we will show that Ankita et al scheme is
vulnerable to known session specific temporary information attack, on
successfully performing the attack, the adversary can perform all other major
cryptographic attacks. As a part of our contribution, we will propose an
improved scheme which is resistance to all major cryptographic attacks and
overcomes the defects in Ankita et al. scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1326</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1326</id><created>2014-01-07</created><authors><author><keyname>Kl&#xe4;s</keyname><forenames>Michael</forenames></author><author><keyname>Elberzhager</keyname><forenames>Frank</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Hartjes</keyname><forenames>Klaus</forenames></author><author><keyname>von Graevemeyer</keyname><forenames>Olaf</forenames></author></authors><title>Transparent Combination of Expert and Measurement Data for Defect
  Prediction: An Industrial Case Study</title><categories>cs.SE</categories><comments>10 pages. The final publication is available at
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=6062145</comments><journal-ref>Proceedings of the 32nd ACM/IEEE International Conference on
  Software Engineering (ICSE '10), pages 119-128, New York, NY, USA, 2010</journal-ref><doi>10.1145/1810295.1810313</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Defining strategies on how to perform quality assurance (QA) and how to
control such activities is a challenging task for organizations developing or
maintaining software and software-intensive systems. Planning and adjusting QA
activities could benefit from accurate estimations of the expected defect
content of relevant artifacts and the effectiveness of important quality
assurance activities. Combining expert opinion with commonly available
measurement data in a hybrid way promises to overcome the weaknesses of purely
data-driven or purely expert-based estimation methods. This article presents a
case study of the hybrid estimation method HyDEEP for estimating defect content
and QA effectiveness in the telecommunication domain. The specific focus of
this case study is the use of the method for gaining quantitative predictions.
This aspect has not been empirically analyzed in previous work. Among other
things, the results show that for defect content estimation, the method
performs significantly better statistically than purely data-based methods,
with a relative error of 0.3 on average (MMRE).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1330</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1330</id><created>2014-01-07</created><authors><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author><author><keyname>Tautenhahn</keyname><forenames>Nikolas</forenames></author></authors><title>Classes of Complete Simple Games that are All Weighted</title><categories>math.CO cs.GT</categories><comments>14 pages, to be presented and published at ICORES 2014:
  http://www.icores.org/</comments><msc-class>91B12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Important decisions are likely made by groups of agents. Thus group decision
making is very common in practice. Very transparent group aggregating rules are
given by weighted voting, where each agent is assigned a weight. Here a
proposal is accepted if the sum of the weights of the supporting agents meets
or exceeds a given quota. We study a more general class of binary voting
systems -- complete simple games -- and propose an algorithm to determine which
sub classes, parameterized by the agent's type composition, are weighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1331</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1331</id><created>2014-01-07</created><authors><author><keyname>Garcia-Morchon</keyname><forenames>Oscar</forenames></author><author><keyname>Rietman</keyname><forenames>Ronald</forenames></author><author><keyname>Shparlinski</keyname><forenames>Igor E.</forenames></author><author><keyname>Tolhuizen</keyname><forenames>Ludo</forenames></author></authors><title>Interpolation and Approximation of Polynomials in Finite Fields over a
  Short Interval from Noisy Values</title><categories>math.NT cs.CR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by a recently introduced HIMMO key distribution scheme, we consider
a modification of the noisy polynomial interpolation problem of recovering an
unknown polynomial $f(X) \in Z[X]$ from approximate values of the residues of
$f(t)$ modulo a prime $p$ at polynomially many points $t$ taken from a short
interval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1333</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1333</id><created>2014-01-07</created><authors><author><keyname>Oancea</keyname><forenames>Bogdan</forenames></author><author><keyname>Ciucu</keyname><forenames>&#x15e;Tefan Cristian</forenames></author></authors><title>Time series forecasting using neural networks</title><categories>cs.NE</categories><comments>Proceedings of the CKS 2013 International Conference</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Recent studies have shown the classification and prediction power of the
Neural Networks. It has been demonstrated that a NN can approximate any
continuous function. Neural networks have been successfully used for
forecasting of financial data series. The classical methods used for time
series prediction like Box-Jenkins or ARIMA assumes that there is a linear
relationship between inputs and outputs. Neural Networks have the advantage
that can approximate nonlinear functions. In this paper we compared the
performances of different feed forward and recurrent neural networks and
training algorithms for predicting the exchange rate EUR/RON and USD/RON. We
used data series with daily exchange rates starting from 2005 until 2013.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1346</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1346</id><created>2014-01-07</created><authors><author><keyname>Xi</keyname><forenames>Feng</forenames></author><author><keyname>Chen</keyname><forenames>Shengyao</forenames></author><author><keyname>Liu</keyname><forenames>Zhong</forenames></author></authors><title>Quadrature Compressive Sampling for Radar Signals</title><categories>cs.IT math.IT</categories><comments>16 pages, 15 figures submitted to IEEE Trans on Signal Processing</comments><doi>10.1109/TSP.2014.2315168</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quadrature sampling has been widely applied in coherent radar systems to
extract in-phase and quadrature (I and Q) components in the received radar
signal. However, the sampling is inefficient because the received signal
contains only a small number of significant target signals. This paper
incorporates the compressive sampling (CS) theory into the design of the
quadrature sampling system, and develops a quadrature compressive sampling
(QuadCS) system to acquire the I and Q components with low sampling rate. The
QuadCS system first randomly projects the received signal into a compressive
bandpass signal and then utilizes the quadrature sampling to output compressive
I and Q components. The compressive outputs are used to reconstruct the I and Q
components. To understand the system performance, we establish the frequency
domain representation of the QuadCS system. With the waveform-matched
dictionary, we prove that the QuadCS system satisfies the restricted isometry
property with overwhelming probability. For K target signals in the observation
interval T, simulations show that the QuadCS requires just O(Klog(BT/K))
samples to stably reconstruct the signal, where B is the signal bandwidth. The
reconstructed signal-to-noise ratio decreases by 3dB for every octave increase
in the target number K and increases by 3dB for every octave increase in the
compressive bandwidth. Theoretical analyses and simulations verify that the
proposed QuadCS is a valid system to acquire the I and Q components in the
received radar signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1347</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1347</id><created>2014-01-07</created><authors><author><keyname>Chilipirea</keyname><forenames>Cristian</forenames></author><author><keyname>Petre</keyname><forenames>Andreea-Cristina</forenames></author><author><keyname>Dobre</keyname><forenames>Ciprian</forenames></author></authors><title>MANIAC Challenge: The Wolf-pack strategy</title><categories>cs.NI</categories><comments>Published in: E. Baccelli, F. Juraschek, O. Hahm, T. C. Schmidt, H.
  Will, M. W\&quot;ahlisch (Eds.), Proc. of 3rd MANIAC Challenge, Berlin, Germany,
  July 27 - 28, 2013, arXiv:1401.1163, Jan. 2014</comments><report-no>MANIAC/2013/03</report-no><acm-class>C.2.1; C.2.2; C.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MANIAC Challenge raises a problem of game theory, different players
strategies intertwine and the success of any player is dependent on the actions
of all players in the system. A truly fair scenario is when all the strategies
are identical, all the nodes co-operate and they all equally share the rewards
and risks that come with every transfer. A successful strategy is one that
tries to diverge from the equilibrium to maximize its own gains and it manages
to do so. We propose the wolf-pack strategy. Unlike standard game-theory based
strategies, our strategy does not penalize the nodes that diverge from fairness
or from equilibrium, as we believe most nodes will do so in an attempt to get
an advantage over the other nodes. The wolf-pack strategy will try to always
find the most successful node or nodes and penalize them. We believe that just
like in nature, a number of small predators can take down the bigger, more
profitable ones. Furthermore during the Challenge we test two different
strategies that provide completely opposite results. These offer a clear
picture of what the best strategy is and the problems of the current system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1349</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1349</id><created>2014-01-07</created><authors><author><keyname>Supeene</keyname><forenames>Isaac</forenames></author><author><keyname>Udugama</keyname><forenames>Asanga</forenames></author><author><keyname>Steinr&#xfc;cken</keyname><forenames>Ren&#xe9;</forenames></author></authors><title>Selfishness as a Virtue in Mobile Ad Hoc Networks</title><categories>cs.NI</categories><comments>Published in: E. Baccelli, F. Juraschek, O. Hahm, T. C. Schmidt, H.
  Will, M. W\&quot;ahlisch (Eds.), Proc. of 3rd MANIAC Challenge, Berlin, Germany,
  July 27 - 28, 2013, arXiv:1401.1163, Jan. 2014</comments><report-no>MANIAC/2013/05</report-no><acm-class>C.2.1; C.2.2; C.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using Mobile Ad Hoc Networks as a way to break the dependency of mobile
communications on permanent infrastructure has been an active area of research
for several years, but only more recently has consideration been given to the
economics of a distributed mobile service. In particular, the topic of handling
nodes that act selfishly or maliciously has not yet been explored in detail. In
this paper, we discuss the SAVMAN strategy for mobile offloading under the
rules of the 2013 MANIAC Challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1376</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1376</id><created>2014-01-07</created><authors><author><keyname>Buchmann</keyname><forenames>Thomas</forenames></author><author><keyname>Baumgartl</keyname><forenames>Johannes</forenames></author><author><keyname>Henrich</keyname><forenames>Dominik</forenames></author><author><keyname>Westfechtel</keyname><forenames>Bernhard</forenames></author></authors><title>Towards A Domain-specific Language For Pick-And-Place Applications</title><categories>cs.RO</categories><comments>Presented at DSLRob 2013 (arXiv:1312.5952)</comments><report-no>DSLRob/2013/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programming robots is a complicated and time-consuming task. A robot is
essentially a real-time, distributed embedded system. Often, control and
communication paths within the system are tightly coupled to the actual
physical configuration of the robot. Thus, programming a robot is a very
challenging task for domain experts who do not have a dedicated background in
robotics. In this paper we present an approach towards a domain specific
language, which is intended to reduce the efforts and the complexity which is
required when developing robotic applications. Furthermore we apply a software
product line approach to realize a configurable code generator which produces
C++ code which can either be run on real robots or on a robot simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1381</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1381</id><created>2014-01-07</created><authors><author><keyname>Liu</keyname><forenames>Ming</forenames><affiliation>IETR</affiliation></author><author><keyname>H&#xe9;lard</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>IETR</affiliation></author><author><keyname>Crussi&#xe8;re</keyname><forenames>Matthieu</forenames><affiliation>IETR</affiliation></author><author><keyname>H&#xe9;lard</keyname><forenames>Maryline</forenames><affiliation>IETR</affiliation></author></authors><title>Reduced-complexity maximum-likelihood decoding for 3D MIMO code</title><categories>cs.IT math.IT</categories><comments>IEEE Wireless Communications and Networking Conference (WCNC 2013),
  Shanghai : China (2013)</comments><proxy>ccsd</proxy><doi>10.1109/WCNC.2013.6555219</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 3D MIMO code is a robust and efficient space-time coding scheme for the
distributed MIMO broadcasting. However, it suffers from the high computational
complexity if the optimal maximum-likelihood (ML) decoding is used. In this
paper we first investigate the unique properties of the 3D MIMO code and
consequently propose a simplified decoding algorithm without sacrificing the ML
optimality. Analysis shows that the decoding complexity is reduced from O(M^8)
to O(M^{4.5}) in quasi-static channels when M-ary square QAM constellation is
used. Moreover, we propose an efficient implementation of the simplified ML
decoder which achieves a much lower decoding time delay compared to the
classical sphere decoder with Schnorr-Euchner enumeration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1393</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1393</id><created>2014-01-07</created><updated>2015-03-30</updated><authors><author><keyname>Tsuiki</keyname><forenames>Hideki</forenames><affiliation>Graduate School of Human and Environmental Studies, Kyoto University</affiliation></author><author><keyname>Tsukamoyo</keyname><forenames>Yasuyuki</forenames><affiliation>Graduate School of Human and Environmental Studies, Kyoto University</affiliation></author></authors><title>Domain Representations Induced by Dyadic Subbases</title><categories>math.GN cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (March 31,
  2015) lmcs:1175</journal-ref><doi>10.2168/LMCS-11(1:17)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study domain representations induced by dyadic subbases and show that a
proper dyadic subbase S of a second-countable regular space X induces an
embedding of X in the set of minimal limit elements of a subdomain D of
{0,1,&#xe2;&#x8a;&#xa5;}&#xcf;&#x89;. In particular, if X is compact, then X is a retract of the set of
limit elements of D.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1399</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1399</id><created>2014-01-07</created><authors><author><keyname>Dvorak</keyname><forenames>Zdenek</forenames></author><author><keyname>Thomas</keyname><forenames>Robin</forenames></author></authors><title>List-coloring apex-minor-free graphs</title><categories>cs.DM math.CO</categories><comments>28 pages, 2 figures</comments><msc-class>05C15 (Primary) 05C85, 68Q17 (Secondary)</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph H is t-apex if H-X is planar for some set X\subset V(H) of size t.
For any integer t&gt;=0 and a fixed t-apex graph H, we give a polynomial-time
algorithm to decide whether a (t+3)-connected H-minor-free graph is colorable
from a given assignment of lists of size t+4. The connectivity requirement is
the best possible in the sense that for every t&gt;=1, there exists a t-apex graph
H such that testing (t+4)-colorability of (t+2)-connected H-minor-free graphs
is NP-complete. Similarly, the size of the lists cannot be decreased (unless
P=NP), since for every t&gt;=1, testing (t+3)-list-colorability of (t+3)-connected
K_{t+4}-minor-free graphs is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1406</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1406</id><created>2014-01-06</created><updated>2014-02-22</updated><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Zhan</keyname><forenames>Jianfeng</forenames></author><author><keyname>Luo</keyname><forenames>Chunjie</forenames></author><author><keyname>Zhu</keyname><forenames>Yuqing</forenames></author><author><keyname>Yang</keyname><forenames>Qiang</forenames></author><author><keyname>He</keyname><forenames>Yongqiang</forenames></author><author><keyname>Gao</keyname><forenames>Wanling</forenames></author><author><keyname>Jia</keyname><forenames>Zhen</forenames></author><author><keyname>Shi</keyname><forenames>Yingjie</forenames></author><author><keyname>Zhang</keyname><forenames>Shujie</forenames></author><author><keyname>Zheng</keyname><forenames>Chen</forenames></author><author><keyname>Lu</keyname><forenames>Gang</forenames></author><author><keyname>Zhan</keyname><forenames>Kent</forenames></author><author><keyname>Li</keyname><forenames>Xiaona</forenames></author><author><keyname>Qiu</keyname><forenames>Bizhu</forenames></author></authors><title>BigDataBench: a Big Data Benchmark Suite from Internet Services</title><categories>cs.DB</categories><comments>12 pages, 6 figures, The 20th IEEE International Symposium On High
  Performance Computer Architecture (HPCA-2014), February 15-19, 2014, Orlando,
  Florida, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As architecture, systems, and data management communities pay greater
attention to innovative big data systems and architectures, the pressure of
benchmarking and evaluating these systems rises. Considering the broad use of
big data systems, big data benchmarks must include diversity of data and
workloads. Most of the state-of-the-art big data benchmarking efforts target
evaluating specific types of applications or system software stacks, and hence
they are not qualified for serving the purposes mentioned above. This paper
presents our joint research efforts on this issue with several industrial
partners. Our big data benchmark suite BigDataBench not only covers broad
application scenarios, but also includes diverse and representative data sets.
BigDataBench is publicly available from http://prof.ict.ac.cn/BigDataBench .
Also, we comprehensively characterize 19 big data workloads included in
BigDataBench with varying data inputs. On a typical state-of-practice
processor, Intel Xeon E5645, we have the following observations: First, in
comparison with the traditional benchmarks: including PARSEC, HPCC, and
SPECCPU, big data applications have very low operation intensity; Second, the
volume of data input has non-negligible impact on micro-architecture
characteristics, which may impose challenges for simulation-based big data
architecture research; Last but not least, corroborating the observations in
CloudSuite and DCBench (which use smaller data inputs), we find that the
numbers of L1 instruction cache misses per 1000 instructions of the big data
applications are higher than in the traditional benchmarks; also, we find that
L3 caches are effective for the big data applications, corroborating the
observation in DCBench.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1424</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1424</id><created>2014-01-07</created><authors><author><keyname>Kalejaiye</keyname><forenames>Gabriel B. T.</forenames></author><author><keyname>Rondina</keyname><forenames>Jo&#xe3;o A. S. R.</forenames></author><author><keyname>Albuquerque</keyname><forenames>Leonardo V. V. L.</forenames></author><author><keyname>Pereira</keyname><forenames>Ta&#xed;s L.</forenames></author><author><keyname>Campos</keyname><forenames>Luiz F. O.</forenames></author><author><keyname>Melo</keyname><forenames>Raphael A. S.</forenames></author><author><keyname>Mascarenhas</keyname><forenames>Daniel S.</forenames></author><author><keyname>Carvalho</keyname><forenames>Marcelo M.</forenames></author></authors><title>Mobile Offloading in Wireless Ad Hoc Networks</title><categories>cs.NI</categories><comments>Published in: E. Baccelli, F. Juraschek, O. Hahm, T. C. Schmidt, H.
  Will, M. W\&quot;ahlisch (Eds.), Proc. of 3rd MANIAC Challenge, Berlin, Germany,
  July 27 - 28, 2013, arXiv:1401.1163, Jan. 2014</comments><report-no>MANIAC/2013/04</report-no><acm-class>C.2.1; C.2.2; C.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a strategy that was designed, implemented, and presented
at the {\it Mobile Ad Hoc Networking Interoperability and Cooperation (MANIAC)
Challenge 2013}. The theme of the MANIAC Challenge 2013 was ``Mobile Data
Offloading,'' and consisted on developing and comparatively evaluating
strategies to offload infrastructure access points via customer ad hoc
forwarding using handheld devices (e.g., tablets and smartphones). According to
the challenge rules, a hop-by-hop bidding contest (or ``auction'') should
decide the path of each data packet towards its destination. Consequently, each
team should rely on other teams' willingness to forward packets for them in
order to get their traffic across the network. In this application scenario,
the incentive for customers to join the ad hoc network is discounted monthly
fees, while operators should benefit from decreased infrastructure costs.
Following the rules of MANIAC Challenge 2013, this paper proposes a strategy
that is based on the concept of how ``tight'' a node is to successfully deliver
a packet to its destination within a given deadline. This ``tightness'' idea
relies on a shortest-path analysis of the underlying network graph, and it is
used to define three sub-strategies that specify a) how to participate in an
auction; b) how to announce an auction; and c) how to decide who wins the
announced auction. The proposed strategy seeks to minimize network resource
utilization and to promote cooperative behavior among participant nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1428</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1428</id><created>2014-01-07</created><authors><author><keyname>Ferrari</keyname><forenames>Alan</forenames></author><author><keyname>Gallucci</keyname><forenames>Dario</forenames></author></authors><title>B-Maniac</title><categories>cs.NI</categories><comments>Published in: E. Baccelli, F. Juraschek, O. Hahm, T. C. Schmidt, H.
  Will, M. W\&quot;ahlisch (Eds.), Proc. of 3rd MANIAC Challenge, Berlin, Germany,
  July 27 - 28, 2013, arXiv:1401.1163, Jan. 2014</comments><report-no>MANIAC/2013/01</report-no><acm-class>C.2.1; C.2.2; C.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Data Offloading permits users to use cheap communication media,
whenever feasible, for delivering their personal data instead of using the
infrastructure which is more expensive. Having good procedures to assess and
compare the different possibilities a device has to send his data is crucial.
In the following paper, we propose an evaluating approach that takes into
consideration the topology changes history in order to provide an efficient way
to calculate the quality of a specific medium.
  In particular \textit{Bayesian Networks} are used as basis to provide our
solution to the Maniac Challenge 2013. Bayesian Networks is a statistical model
for the generation of inferences starting with very little information. In the
next sections we will give a definition of the Bayesian Networks focusing on
the simplest and computationally efficient of the Bayesian Networks versions,
named \textit{\naive Bayes}. Finally, we will show our method for the challenge
to evaluate the medium and the bid strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1434</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1434</id><created>2014-01-07</created><authors><author><keyname>K&#xf6;nig</keyname><forenames>Stefan</forenames></author></authors><title>Computational Aspects of the Hausdorff Distance in Unbounded Dimension</title><categories>cs.CG cs.CC math.MG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of determining the Hausdorff distance
of two polytopes given in halfspace- or vertex-presentation in arbitrary
dimension. Subsequently, a matching problem is investigated where a convex body
is allowed to be homothetically transformed in order to minimize its Hausdorff
distance to another one. For this problem, we characterize optimal solutions,
deduce a Helly-type theorem and give polynomial time (approximation) algorithms
for polytopes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1448</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1448</id><created>2014-01-07</created><updated>2014-05-08</updated><authors><author><keyname>Kuperberg</keyname><forenames>Denis</forenames><affiliation>LIAFA, Universit&#xe9; Paris Diderot</affiliation></author></authors><title>Linear Temporal Logic for Regular Cost Functions</title><categories>cs.LO cs.FL</categories><comments>37 pages, 13 figures, accepted to LMCS</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (February
  4, 2014) lmcs:1222</journal-ref><doi>10.2168/LMCS-10(1:4)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regular cost functions have been introduced recently as an extension to the
notion of regular languages with counting capabilities, which retains strong
closure, equivalence, and decidability properties. The specificity of cost
functions is that exact values are not considered, but only estimated. In this
paper, we define an extension of Linear Temporal Logic (LTL) over finite words
to describe cost functions. We give an explicit translation from this new logic
to two dual form of cost automata, and we show that the natural decision
problems for this logic are PSPACE-complete, as it is the case in the classical
setting. We then algebraically characterize the expressive power of this logic,
using a new syntactic congruence for cost functions introduced in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1450</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1450</id><created>2014-01-07</created><authors><author><keyname>L</keyname><forenames>Diego Fernando C. Carri&#xf3;n</forenames></author></authors><title>A Recursive Algorithmic Approach to the Finding of Permutations for the
  Combination of Any Two Sets</title><categories>cs.DS</categories><comments>12 pages, 4 figures, originally submitted to the July 2013 Brigham
  Young University-Idaho Research and Creative Works Conference</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper I present a conjecture for a recursive algorithm that finds
each permutation of combining two sets of objects (AKA the Shuffle Product).
This algorithm provides an efficient way to navigate this problem, as each
atomic operation yields a permutation of the union. The permutations of the
union of the two sets are represented as binary integers which are then
manipulated mathematically to find the next permutation. The routes taken to
find each of the permutations then form a series of associations or adjacencies
which can be represented in a tree graph which appears to possess some
properties of a fractal.
  This algorithm was discovered while attempting to identify every possible
end-state of a Tic-Tac-Toe (Naughts and Crosses) board. It was found to be a
viable and efficient solution to the problem, and now---in its more generalized
state---it is my belief that it may find applications among a wide range of
theoretical and applied sciences.
  I hypothesize that, due to the fractal-like nature of the tree it traverses,
this algorithm sheds light on a more generic principle of combinatorics and as
such could be further generalized to perhaps be applied to the union of any
number of sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1451</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1451</id><created>2014-01-07</created><authors><author><keyname>Kandpal</keyname><forenames>Kalpana</forenames></author><author><keyname>Singhal</keyname><forenames>Anjali</forenames></author></authors><title>Smart Grid Demand Monitoring Model</title><categories>cs.OH</categories><comments>3 pages, 1 figure, International Journal</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper is in related to the demand genrated by the consumer for a time
for the power which is being viewed by taking some measures to solve the demand
need.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1456</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1456</id><created>2014-01-07</created><updated>2014-11-09</updated><authors><author><keyname>Karkali</keyname><forenames>Margarita</forenames></author><author><keyname>Rousseau</keyname><forenames>Francois</forenames></author><author><keyname>Ntoulas</keyname><forenames>Alexandros</forenames></author><author><keyname>Vazirgiannis</keyname><forenames>Michalis</forenames></author></authors><title>Using temporal IDF for efficient novelty detection in text streams</title><categories>cs.IR</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Novelty detection in text streams is a challenging task that emerges in quite
a few different scenarios, ranging from email thread filtering to RSS news feed
recommendation on a smartphone. An efficient novelty detection algorithm can
save the user a great deal of time and resources when browsing through relevant
yet usually previously-seen content. Most of the recent research on detection
of novel documents in text streams has been building upon either geometric
distances or distributional similarities, with the former typically performing
better but being much slower due to the need of comparing an incoming document
with all the previously-seen ones. In this paper, we propose a new approach to
novelty detection in text streams. We describe a resource-aware mechanism that
is able to handle massive text streams such as the ones present today thanks to
the burst of social media and the emergence of the Web as the main source of
information. We capitalize on the historical Inverse Document Frequency (IDF)
that was known for capturing well term specificity and we show that it can be
used successfully at the document level as a measure of document novelty. This
enables us to avoid similarity comparisons with previous documents in the text
stream, thus scaling better and leading to faster execution times. Moreover, as
the collection of documents evolves over time, we use a temporal variant of IDF
not only to maintain an efficient representation of what has already been seen
but also to decay the document frequencies as the time goes by. We evaluate the
performance of the proposed approach on a real-world news articles dataset
created for this task. The results show that the proposed method outperforms
all of the baselines while managing to operate efficiently in terms of time
complexity and memory usage, which are of great importance in a mobile setting
scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1458</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1458</id><created>2014-01-07</created><updated>2014-04-10</updated><authors><author><keyname>Eom</keyname><forenames>Young-Ho</forenames></author><author><keyname>Jo</keyname><forenames>Hang-Hyun</forenames></author></authors><title>Generalized friendship paradox in complex networks: The case of
  scientific collaboration</title><categories>cs.SI physics.data-an physics.soc-ph</categories><comments>Published in Scientific Reports. 9 pages, 3 figures</comments><journal-ref>Scientific Reports 4, 4603 (2014)</journal-ref><doi>10.1038/srep04603</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The friendship paradox states that your friends have on average more friends
than you have. Does the paradox &quot;hold&quot; for other individual characteristics
like income or happiness? To address this question, we generalize the
friendship paradox for arbitrary node characteristics in complex networks. By
analyzing two coauthorship networks of Physical Review journals and Google
Scholar profiles, we find that the generalized friendship paradox (GFP) holds
at the individual and network levels for various characteristics, including the
number of coauthors, the number of citations, and the number of publications.
The origin of the GFP is shown to be rooted in positive correlations between
degree and characteristics. As a fruitful application of the GFP, we suggest
effective and efficient sampling methods for identifying high characteristic
nodes in large-scale networks. Our study on the GFP can shed lights on
understanding the interplay between network structure and node characteristics
in complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1460</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1460</id><created>2014-01-07</created><updated>2014-06-23</updated><authors><author><keyname>Grabmayer</keyname><forenames>Clemens</forenames></author><author><keyname>Rochel</keyname><forenames>Jan</forenames></author></authors><title>Maximal Sharing in the Lambda Calculus with letrec</title><categories>cs.PL</categories><comments>18 pages, plus 19 pages appendix</comments><acm-class>F.3.3; D.1.1</acm-class><doi>10.1145/2628136.2628148</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Increasing sharing in programs is desirable to compactify the code, and to
avoid duplication of reduction work at run-time, thereby speeding up execution.
We show how a maximal degree of sharing can be obtained for programs expressed
as terms in the lambda calculus with letrec. We introduce a notion of `maximal
compactness' for lambda-letrec-terms among all terms with the same infinite
unfolding. Instead of defined purely syntactically, this notion is based on a
graph semantics. lambda-letrec-terms are interpreted as first-order term graphs
so that unfolding equivalence between terms is preserved and reflected through
bisimilarity of the term graph interpretations. Compactness of the term graphs
can then be compared via functional bisimulation.
  We describe practical and efficient methods for the following two problems:
transforming a lambda-letrec-term into a maximally compact form; and deciding
whether two lambda-letrec-terms are unfolding-equivalent. The transformation of
a lambda-letrec-term $L$ into maximally compact form $L_0$ proceeds in three
steps:
  (i) translate L into its term graph $G = [[ L ]]$; (ii) compute the maximally
shared form of $G$ as its bisimulation collapse $G_0$; (iii) read back a
lambda-letrec-term $L_0$ from the term graph $G_0$ with the property $[[ L_0 ]]
= G_0$. This guarantees that $L_0$ and $L$ have the same unfolding, and that
$L_0$ exhibits maximal sharing.
  The procedure for deciding whether two given lambda-letrec-terms $L_1$ and
$L_2$ are unfolding-equivalent computes their term graph interpretations $[[
L_1 ]]$ and $[[ L_2 ]]$, and checks whether these term graphs are bisimilar.
  For illustration, we also provide a readily usable implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1465</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1465</id><created>2014-01-07</created><authors><author><keyname>Balduzzi</keyname><forenames>David</forenames></author></authors><title>Cortical prediction markets</title><categories>cs.AI cs.GT cs.LG cs.MA q-bio.NC</categories><comments>To appear, AAMAS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate cortical learning from the perspective of mechanism design.
First, we show that discretizing standard models of neurons and synaptic
plasticity leads to rational agents maximizing simple scoring rules. Second,
our main result is that the scoring rules are proper, implying that neurons
faithfully encode expected utilities in their synaptic weights and encode
high-scoring outcomes in their spikes. Third, with this foundation in hand, we
propose a biologically plausible mechanism whereby neurons backpropagate
incentives which allows them to optimize their usefulness to the rest of
cortex. Finally, experiments show that networks that backpropagate incentives
can learn simple tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1467</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1467</id><created>2014-01-07</created><authors><author><keyname>Andreev</keyname><forenames>Mikhail</forenames></author><author><keyname>Kumok</keyname><forenames>Akim</forenames></author></authors><title>The sum $2^{\mathit{KA}(x)-\mathit{KP}(x)}$ over all prefixes $x$ of
  some binary sequence can be infinite</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two quantities that measure complexity of binary strings:
$\mathit{KA}(x)$ is defined as the minus logarithm of continuous a priori
probability on the binary tree, and $\mathit{KP}(x)$ denotes prefix complexity
of a binary string $x$. In this paper we answer a question posed by Joseph
Miller and prove that there exists an infinite binary sequence $\omega$ such
that the sum of $2^{\mathit{KA}(x)-\mathit{KP}(x)}$ over all prefixes $x$ of
$\omega$ is infinite. Such a sequence can be chosen among characteristic
sequences of computably enumerable sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1472</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1472</id><created>2014-01-07</created><updated>2014-10-29</updated><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Kumar</keyname><forenames>Nirman</forenames></author></authors><title>Robust Proximity Search for Balls using Sublinear Space</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of n disjoint balls b1, . . ., bn in IRd, we provide a data
structure, of near linear size, that can answer (1 \pm \epsilon)-approximate
kth-nearest neighbor queries in O(log n + 1/\epsilon^d) time, where k and
\epsilon are provided at query time. If k and \epsilon are provided in advance,
we provide a data structure to answer such queries, that requires (roughly)
O(n/k) space; that is, the data structure has sublinear space requirement if k
is sufficiently large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1475</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1475</id><created>2014-01-07</created><authors><author><keyname>Shakarian</keyname><forenames>Paulo</forenames></author><author><keyname>Simari</keyname><forenames>Gerardo I.</forenames></author><author><keyname>Falappa</keyname><forenames>Marcelo A.</forenames></author></authors><title>Belief Revision in Structured Probabilistic Argumentation</title><categories>cs.LO cs.AI</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In real-world applications, knowledge bases consisting of all the information
at hand for a specific domain, along with the current state of affairs, are
bound to contain contradictory data coming from different sources, as well as
data with varying degrees of uncertainty attached. Likewise, an important
aspect of the effort associated with maintaining knowledge bases is deciding
what information is no longer useful; pieces of information (such as
intelligence reports) may be outdated, may come from sources that have recently
been discovered to be of low quality, or abundant evidence may be available
that contradicts them. In this paper, we propose a probabilistic structured
argumentation framework that arises from the extension of Presumptive
Defeasible Logic Programming (PreDeLP) with probabilistic models, and argue
that this formalism is capable of addressing the basic issues of handling
contradictory and uncertain data. Then, to address the last issue, we focus on
the study of non-prioritized belief revision operations over probabilistic
PreDeLP programs. We propose a set of rationality postulates -- based on
well-known ones developed for classical knowledge bases -- that characterize
how such operations should behave, and study a class of operators along with
theoretical relationships with the proposed postulates, including a
representation theorem stating the equivalence between this class and the class
of operators characterized by the postulates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1477</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1477</id><created>2014-01-07</created><updated>2015-03-19</updated><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Raichel</keyname><forenames>Benjamin</forenames></author></authors><title>On the Complexity of Randomly Weighted Voronoi Diagrams</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide an $O(n \mathrm{polylog} n)$ bound on the expected
complexity of the randomly weighted Voronoi diagram of a set of $n$ sites in
the plane, where the sites can be either points, interior-disjoint convex sets,
or other more general objects. Here the randomness is on the weight of the
sites, not their location. This compares favorably with the worst case
complexity of these diagrams, which is quadratic. As a consequence we get an
alternative proof to that of Agarwal etal [AHKS13] of the near linear
complexity of the union of randomly expanded disjoint segments or convex sets
(with an improved bound on the latter). The technique we develop is elegant and
should be applicable to other problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1479</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1479</id><created>2014-01-07</created><authors><author><keyname>Garnaev</keyname><forenames>Andrey</forenames></author><author><keyname>Hayel</keyname><forenames>Yezekael</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author></authors><title>Multilevel Pricing Schemes in a Deregulated Wireless Network Market</title><categories>cs.GT</categories><comments>This is the last draft version of the paper. Revised version of the
  paper accepted by ValueTools 2013 can be found in Proceedings of the 7th
  International Conference on Performance Evaluation Methodologies and Tools
  (ValueTools '13), December 10-12, 2013, Turin, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Typically the cost of a product, a good or a service has many components.
Those components come from different complex steps in the supply chain of the
product from sourcing to distribution. This economic point of view also takes
place in the determination of goods and services in wireless networks. Indeed,
before transmitting customer data, a network operator has to lease some
frequency range from a spectrum owner and also has to establish agreements with
electricity suppliers. The goal of this paper is to compare two pricing
schemes, namely a power-based and a flat rate, and give a possible explanation
why flat rate pricing schemes are more common than power based pricing ones in
a deregulated wireless market. We suggest a hierarchical game-theoretical model
of a three level supply chain: the end users, the service provider and the
spectrum owner. The end users intend to transmit data on a wireless network.
The amount of traffic sent by the end users depends on the available frequency
bandwidth as well as the price they have to pay for their transmission. A
natural question arises for the service provider: how to design an efficient
pricing scheme in order to maximize his profit. Moreover he has to take into
account the lease charge he has to pay to the spectrum owner and how many
frequency bandwidth to rent. The spectrum owner itself also looks for
maximizing its profit and has to determine the lease price to the service
provider. The equilibrium at each level of our supply chain model are
established and several properties are investigated. In particular, in the case
of a power-based pricing scheme, the service provider and the spectrum owner
tend to share the gross provider profit. Whereas, considering the flat rate
pricing scheme, if the end users are going to exploit the network intensively,
then the tariffs of the suppliers (spectrum owner and service provider)
explode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1480</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1480</id><created>2014-01-07</created><authors><author><keyname>Carmon</keyname><forenames>Yair</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author></authors><title>Lower Bounds and Approximations for the Information Rate of the ISI
  Channel</title><categories>cs.IT math.IT</categories><comments>21 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the discrete-time intersymbol interference (ISI) channel model,
with additive Gaussian noise and fixed i.i.d. inputs. In this setting, we
investigate the expression put forth by Shamai and Laroia as a conjectured
lower bound for the input-output mutual information after application of a
MMSE-DFE receiver. A low-SNR expansion is used to prove that the conjectured
bound does not hold under general conditions, and to characterize inputs for
which it is particularly ill-suited. One such input is used to construct a
counterexample, indicating that the Shamai-Laroia expression does not always
bound even the achievable rate of the channel, thus excluding a natural
relaxation of the original conjectured bound. However, this relaxed bound is
then shown to hold for any finite entropy input and ISI channel, when the SNR
is sufficiently high. Finally, new simple bounds for the achievable rate are
proven, and compared to other known bounds. Information-Estimation relations
and estimation-theoretic bounds play a key role in establishing our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1486</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1486</id><created>2014-01-07</created><authors><author><keyname>Ismaili</keyname><forenames>Imdad Ali</forenames></author><author><keyname>Bhatti</keyname><forenames>Zeeshan</forenames></author><author><keyname>Shah</keyname><forenames>Azhar Ali</forenames></author></authors><title>Design &amp; Development of the Graphical User Interface for Sindhi Language</title><categories>cs.HC cs.CL</categories><journal-ref>Mehran University Research Journal of Engineering &amp; Technology,
  Volume 30, No. 4, October 2011 [ISSN 0254-7821]</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the design and implementation of a Unicode-based GUISL
(Graphical User Interface for Sindhi Language). The idea is to provide a
software platform to the people of Sindh as well as Sindhi diasporas living
across the globe to make use of computing for basic tasks such as editing,
composition, formatting, and printing of documents in Sindhi by using GUISL.
The implementation of the GUISL has been done in the Java technology to make
the system platform independent. The paper describes several design issues of
Sindhi GUI in the context of existing software tools and technologies and
explains how mapping and concatenation techniques have been employed to achieve
the cursive shape of Sindhi script.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1488</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1488</id><created>2014-01-07</created><authors><author><keyname>Bhatti</keyname><forenames>Zeeshan</forenames></author><author><keyname>Shah</keyname><forenames>Asadullah</forenames></author><author><keyname>Shahidi</keyname><forenames>Farruh</forenames></author><author><keyname>Karbasi</keyname><forenames>Mostafa</forenames></author></authors><title>Forward and Inverse Kinematics Seamless Matching Using Jacobian</title><categories>cs.GR</categories><journal-ref>Sindh University Research Journal (SURJ) Volume 45 (2), 8/2013,
  pp:387-392, Sindh University Press</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the problem of matching Forward Kinematics (FK) motion of a 3
Dimensional (3D) joint chain to the Inverse Kinematics (IK) movement and vice
versa has been addressed. The problem lies at the heart of animating a 3D
character having controller and manipulator based rig for animation within any
3D modeling and animation software. The seamless matching has been achieved
through the use of pseudo-inverse of Jacobian Matrix. The Jacobian Matrix is
used to determine the rotation values of each joint of character body part such
as arms, between the inverse kinematics and forward kinematics motion. Then
moving the corresponding kinematic joint system to the desired place,
automatically eliminating the jumping or popping effect which would reduce the
complexity of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1489</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1489</id><created>2014-01-07</created><authors><author><keyname>Komar</keyname><forenames>John</forenames></author><author><keyname>H&#xe9;rault</keyname><forenames>Romain</forenames></author><author><keyname>Seifert</keyname><forenames>Ludovic</forenames></author></authors><title>Key point selection and clustering of swimmer coordination through
  Sparse Fisher-EM</title><categories>stat.ML cs.CV cs.LG physics.data-an stat.AP</categories><comments>Presented at ECML/PKDD 2013 Workshop on Machine Learning and Data
  Mining for Sports Analytics (MLSA2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To answer the existence of optimal swimmer learning/teaching strategies, this
work introduces a two-level clustering in order to analyze temporal dynamics of
motor learning in breaststroke swimming. Each level have been performed through
Sparse Fisher-EM, a unsupervised framework which can be applied efficiently on
large and correlated datasets. The induced sparsity selects key points of the
coordination phase without any prior knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1513</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1513</id><created>2014-01-06</created><authors><author><keyname>Seddik</keyname><forenames>Karim G.</forenames></author></authors><title>On the Stability of Random Multiple Access with Feedback Exploitation
  and Queue Priority</title><categories>cs.IT cs.NI cs.PF math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we study the stability of two interacting queues under random
multiple access in which the queues leverage the feedback information. We
derive the stability region under random multiple access where one of the two
queues exploits the feedback information and backs off under negative
acknowledgement (NACK) and the other, higher priority, queue will access the
channel with probability one. We characterize the stability region of this
feedback-based random access protocol and prove that this derived stability
region encloses the stability region of the conventional random access (RA)
scheme that does not exploit the feedback information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1526</identifier>
 <datestamp>2014-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1526</id><created>2014-01-07</created><updated>2014-08-05</updated><authors><author><keyname>Swanson</keyname><forenames>Colleen M.</forenames></author><author><keyname>Stinson</keyname><forenames>Douglas R.</forenames></author></authors><title>Additional Constructions to Solve the Generalized Russian Cards Problem
  using Combinatorial Designs</title><categories>cs.CR math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the generalized Russian cards problem, we have a card deck $X$ of $n$
cards and three participants, Alice, Bob, and Cathy, dealt $a$, $b$, and $c$
cards, respectively. Once the cards are dealt, Alice and Bob wish to privately
communicate their hands to each other via public announcements, without the
advantage of a shared secret or public key infrastructure. Cathy should remain
ignorant of all but her own cards after Alice and Bob have made their
announcements. Notions for Cathy's ignorance in the literature range from Cathy
not learning the fate of any individual card with certainty (weak $1$-security)
to not gaining any probabilistic advantage in guessing the fate of some set of
$\delta$ cards (perfect $\delta$-security). As we demonstrate, the generalized
Russian cards problem has close ties to the field of combinatorial designs, on
which we rely heavily, particularly for perfect security notions. Our main
result establishes an equivalence between perfectly $\delta$-secure strategies
and $(c+\delta)$-designs on $n$ points with block size $a$, when announcements
are chosen uniformly at random from the set of possible announcements. We also
provide construction methods and example solutions, including a construction
that yields perfect $1$-security against Cathy when $c=2$. We leverage a known
combinatorial design to construct a strategy with $a=8$, $b=13$, and $c=3$ that
is perfectly $2$-secure. Finally, we consider a variant of the problem that
yields solutions that are easy to construct and optimal with respect to both
the number of announcements and level of security achieved. Moreover, this is
the first method obtaining weak $\delta$-security that allows Alice to hold an
arbitrary number of cards and Cathy to hold a set of $c = \lfloor
\frac{a-\delta}{2} \rfloor$ cards. Alternatively, the construction yields
solutions for arbitrary $\delta$, $c$ and any $a \geq \delta + 2c$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1533</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1533</id><created>2014-01-07</created><updated>2014-04-18</updated><authors><author><keyname>Pantano</keyname><forenames>Devis</forenames></author></authors><title>Proposta di nuovi strumenti per comprendere come funziona la cognizione
  (Novel tools to understand how cognition works)</title><categories>cs.AI</categories><comments>In Italian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I think that the main reason why we do not understand the general principles
of how knowledge works (and probably also the reason why we have not yet
designed and built efficient machines capable of artificial intelligence), is
not the excessive complexity of cognitive phenomena, but the lack of the
conceptual and methodological tools to properly address the problem. It is like
trying to build up Physics without the concept of number, or to understand the
origin of species without including the mechanism of natural selection. In this
paper I propose some new conceptual and methodological tools, which seem to
offer a real opportunity to understand the logic of cognitive processes. I
propose a new method to properly treat the concepts of structure and schema,
and to perform on them operations of structural analysis. These operations
allow to move straightforwardly from concrete to more abstract representations.
With these tools I will suggest a definition for the concept of rule, of
regularity and of emergent phenomena. From the analysis of some important
aspects of the rules, I suggest to distinguish them in operational and
associative rules. I propose that associative rules assume a dominant role in
cognition. I also propose a definition for the concept of problem. At the end I
will briefly illustrate a possible general model for cognitive systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1541</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1541</id><created>2014-01-07</created><authors><author><keyname>Stewart</keyname><forenames>Lorna</forenames></author><author><keyname>Valenzano</keyname><forenames>Richard Anthony</forenames></author></authors><title>On polygon numbers of circle graphs and distance hereditary graphs</title><categories>cs.DM math.CO</categories><comments>34 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Circle graphs are intersection graphs of chords in a circle and $k$-polygon
graphs are the intersection graphs of chords in a convex $k$-sided polygon
where each chord has its endpoints on distinct sides. Every $k$-polygon graph
is a circle graph and every circle graph is a $k$-polygon graph for some $k$.
The polygon number $\psi(G)$ of a circle graph $G$ is the minimum $k$ such that
$G$ is a $k$-polygon graph and the polygon number of a circle representation is
the minimum number of corners that must be added to the circle to produce a
polygon representation. Given a circle graph $G$ and an integer $k$,
determining whether $\psi(G) \le k$ is NP-complete, while the problem is
solvable in polynomial time for fixed $k$, and the polygon number of a circle
representation can be computed in polynomial time.
  In this paper, we give bounds on $\psi(G)$ when $G$ is an arbitrary circle
graph -- upper bounds in terms of the independence number, the clique cover
number, and the number of vertices of $G$, and a lower bound in terms of the
asteroidal number of $G$ -- and show that $\psi(G)$ is equal to the asteroidal
number of $G$ when $G$ is a connected nonclique distance hereditary graph. We
then use our results to develop characterizations of distance hereditary
permutation graphs. Finally, we describe linear time algorithms for finding the
polygon number of a circle representation and for finding the asteroidal number
of a distance hereditary graph, improvements over previously known algorithms
for those problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1545</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1545</id><created>2014-01-07</created><authors><author><keyname>Ugrinovskii</keyname><forenames>V.</forenames></author><author><keyname>Fridman</keyname><forenames>E.</forenames></author></authors><title>A Round-Robin Protocol for Distributed Estimation with $H_\infty$
  Consensus</title><categories>math.OC cs.SY</categories><comments>A version of this paper has been presented at the 52nd IEEE CDC,
  Florence, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper considers a distributed robust estimation problem over a network
with directed topology involving continuous time observers. While measurements
are available to the observers continuously, the nodes interact according to a
Round-Robin rule, at discrete time instances. The results of the paper are
sufficient conditions which guarantee a suboptimal $H_\infty$ level of
consensus between observers with sampled interconnections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1549</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1549</id><created>2014-01-07</created><updated>2014-06-28</updated><authors><author><keyname>Wen</keyname><forenames>Zheng</forenames></author><author><keyname>O'Neill</keyname><forenames>Daniel</forenames></author><author><keyname>Maei</keyname><forenames>Hamid Reza</forenames></author></authors><title>Optimal Demand Response Using Device Based Reinforcement Learning</title><categories>cs.LG cs.AI cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Demand response (DR) for residential and small commercial buildings is
estimated to account for as much as 65% of the total energy savings potential
of DR, and previous work shows that a fully automated Energy Management System
(EMS) is a necessary prerequisite to DR in these areas. In this paper, we
propose a novel EMS formulation for DR problems in these sectors. Specifically,
we formulate a fully automated EMS's rescheduling problem as a reinforcement
learning (RL) problem, and argue that this RL problem can be approximately
solved by decomposing it over device clusters. Compared with existing
formulations, our new formulation (1) does not require explicitly modeling the
user's dissatisfaction on job rescheduling, (2) enables the EMS to
self-initiate jobs, (3) allows the user to initiate more flexible requests and
(4) has a computational complexity linear in the number of devices. We also
demonstrate the simulation results of applying Q-learning, one of the most
popular and classical RL algorithms, to a representative example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1551</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1551</id><created>2014-01-07</created><updated>2014-10-06</updated><authors><author><keyname>Checco</keyname><forenames>Alessandro</forenames></author><author><keyname>Lancia</keyname><forenames>Carlo</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author></authors><title>Using Crowdsourcing for Local Topology Discovery in Wireless Networks</title><categories>cs.NI cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce the idea of estimating local topology in wireless
networks by means of crowdsourced user reports. In this approach each user
periodically reports to the serving basestation information about the set of
neighbouring basestations observed by the user. We show that, by mapping the
local topological structure of the network onto states of increasing knowledge,
a crisp mathematical framework can be obtained, which allows in turn for the
use of a variety of user mobility models. Using a simplified mobility model we
show how obtain useful upper bounds on the expected time for a basestation to
gain full knowledge of its local neighbourhood, answering the fundamental
question about which classes of network deployments can effectively benefit
from a crowdsourcing approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1558</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1558</id><created>2014-01-07</created><authors><author><keyname>Fan</keyname><forenames>Zhitao</forenames></author><author><keyname>Guan</keyname><forenames>Feng</forenames></author><author><keyname>Wu</keyname><forenames>Chunlin</forenames></author><author><keyname>Yan</keyname><forenames>Ming</forenames></author></authors><title>The Continuity of Images by Transmission Imaging Revisited</title><categories>math.DG cs.CV math.NA</categories><comments>23 pages, 8 figures</comments><msc-class>92C55, 90C90, 68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmission imaging, as an important imaging technique widely used in
astronomy, medical diagnosis, and biology science, has been shown in [49] quite
different from reflection imaging used in our everyday life. Understanding the
structures of images (the prior information) is important for designing,
testing, and choosing image processing methods, and good image processing
methods are helpful for further uses of the image data, e.g., increasing the
accuracy of the object reconstruction methods in transmission imaging
applications. In reflection imaging, the images are usually modeled as
discontinuous functions and even piecewise constant functions. In transmission
imaging, it was shown very recently in [49] that almost all images are
continuous functions. However, the author in [49] considered only the case of
parallel beam geometry and used some too strong assumptions in the proof, which
exclude some common cases such as cylindrical objects. In this paper, we
consider more general beam geometries and simplify the assumptions by using
totally different techniques. In particular, we will prove that almost all
images in transmission imaging with both parallel and divergent beam geometries
(two most typical beam geometries) are continuous functions, under much weaker
assumptions than those in [49], which admit almost all practical cases.
Besides, taking into accounts our analysis, we compare two image processing
methods for Poisson noise (which is the most significant noise in transmission
imaging) removal. Numerical experiments will be provided to demonstrate our
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1559</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1559</id><created>2014-01-07</created><authors><author><keyname>Babaioff</keyname><forenames>Moshe</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author><author><keyname>Leme</keyname><forenames>Renato Paes</forenames></author></authors><title>Price Competition in Online Combinatorial Markets</title><categories>cs.GT</categories><comments>accept to WWW'14 (23rd International World Wide Web Conference)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a single buyer with a combinatorial preference that would like to
purchase related products and services from different vendors, where each
vendor supplies exactly one product. We study the general case where subsets of
products can be substitutes as well as complementary and analyze the game that
is induced on the vendors, where a vendor's strategy is the price that he asks
for his product. This model generalizes both Bertrand competition (where
vendors are perfect substitutes) and Nash bargaining (where they are perfect
complements), and captures a wide variety of scenarios that can appear in
complex crowd sourcing or in automatic pricing of related products.
  We study the equilibria of such games and show that a pure efficient
equilibrium always exists. In the case of submodular buyer preferences we fully
characterize the set of pure Nash equilibria, essentially showing uniqueness.
For the even more restricted &quot;substitutes&quot; buyer preferences we also prove
uniqueness over {\em mixed} equilibria. Finally we begin the exploration of
natural generalizations of our setting such as when services have costs, when
there are multiple buyers or uncertainty about the the buyer's valuation, and
when a single vendor supplies multiple products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1560</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1560</id><created>2014-01-07</created><authors><author><keyname>Xiong</keyname><forenames>Tao</forenames></author><author><keyname>Bao</keyname><forenames>Yukun</forenames></author><author><keyname>Hu</keyname><forenames>Zhongyi</forenames></author></authors><title>Beyond One-Step-Ahead Forecasting: Evaluation of Alternative
  Multi-Step-Ahead Forecasting Models for Crude Oil Prices</title><categories>cs.LG cs.AI</categories><comments>32 pages</comments><journal-ref>Energy Economics. 40, 2013: 405-415</journal-ref><doi>10.1016/j.eneco.2013.07.028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An accurate prediction of crude oil prices over long future horizons is
challenging and of great interest to governments, enterprises, and investors.
This paper proposes a revised hybrid model built upon empirical mode
decomposition (EMD) based on the feed-forward neural network (FNN) modeling
framework incorporating the slope-based method (SBM), which is capable of
capturing the complex dynamic of crude oil prices. Three commonly used
multi-step-ahead prediction strategies proposed in the literature, including
iterated strategy, direct strategy, and MIMO (multiple-input multiple-output)
strategy, are examined and compared, and practical considerations for the
selection of a prediction strategy for multi-step-ahead forecasting relating to
crude oil prices are identified. The weekly data from the WTI (West Texas
Intermediate) crude oil spot price are used to compare the performance of the
alternative models under the EMD-SBM-FNN modeling framework with selected
counterparts. The quantitative and comprehensive assessments are performed on
the basis of prediction accuracy and computational cost. The results obtained
in this study indicate that the proposed EMD-SBM-FNN model using the MIMO
strategy is the best in terms of prediction accuracy with accredited
computational load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1573</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1573</id><created>2014-01-07</created><authors><author><keyname>Deng</keyname><forenames>Xuegong</forenames></author></authors><title>The security deposit for finitely repeated Prisoner's dilemma</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under the assumption of complete rationality, Nash equilibrium is the only
reasonable strategy (set) of the finitely repeated prisoner's dilemma. In fact,
some strategies only slightly deviate from the so-called rationality, and the
corresponding payoff may much better than that of Nash equilibrium. This
article points out, even under the rational assumptions, the players have
reason to seek a mutually beneficial agreement (Pareto dominated compare to
Nash equilibrium) and a weak and optional constraints, so that the agreement
can be successfully implemented. If the constraint does not harm the interests
of the participants, or the adversely affects of the constraint are negligible,
then the finitely repeated prisoner's dilemma becomes a bargaining problem
issues on the strategy sequences and the problem to seek the constraints. The
quantification of the constraints, the so-called security deposit in this
paper, is nearly a concept of distance from an agreement (a strategy set) to
the complete rationality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1577</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1577</id><created>2014-01-07</created><authors><author><keyname>Quan</keyname><forenames>Quan</forenames></author><author><keyname>Jiang</keyname><forenames>Lu</forenames></author><author><keyname>Cai</keyname><forenames>Kai-Yuan</forenames></author></authors><title>Discrete-Time Output-Feedback Robust Repetitive Control for a Class of
  Nonlinear Systems by Additive State Decomposition</title><categories>cs.SY</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The discrete-time robust repetitive control (RC, or repetitive controller,
also designated RC) problem for nonlinear systems is both challenging and
practical. This paper proposes a discrete-time output-feedback RC design for a
class of systems subject to measurable nonlinearities to track reference
robustly with respect to the period variation. The design relies on additive
state decomposition, by which the output-feedback RC problem is decomposed into
an output-feedback RC problem for a linear time-invariant system and a
state-feedback stabilization problem for a nonlinear system. Thanks to the
decomposition, existing controller design methods in both the frequency domain
and time domain can be employed to make the robustness and discretization for a
nonlinear system tractable. To demonstrate the effectiveness, an illustrative
example is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1580</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1580</id><created>2014-01-07</created><authors><author><keyname>Quan</keyname><forenames>Quan</forenames></author><author><keyname>Cai</keyname><forenames>Kai-Yuan</forenames></author></authors><title>A New Causal Ideal Internal Dynamics Generator</title><categories>cs.SY</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of ideal internal dynamics (IID) generators, namely solving IID,
is a fundamental problem, which is a key step to handle the nonminimum-phase
output tracking problem. In this paper, for a class of unstable matrix
differential equations, a new causal dynamic IID generator is proposed, whose
parameters are partly chosen via H_2/H_inf optimization. Compared with existing
similar generators, it is applicable to matrix differential equations with
singular system matrices and is easily extended to slowly time-varying matrix
differential equations without extra computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1605</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1605</id><created>2014-01-08</created><updated>2014-04-14</updated><authors><author><keyname>Hensman</keyname><forenames>James</forenames></author><author><keyname>Rattray</keyname><forenames>Magnus</forenames></author><author><keyname>Lawrence</keyname><forenames>Neil D.</forenames></author></authors><title>Fast nonparametric clustering of structured time-series</title><categories>cs.LG cs.CV stat.ML</categories><comments>Accepted for publication in special edition of TPAMI on Bayesian
  Nonparametrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this publication, we combine two Bayesian non-parametric models: the
Gaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP
model is to introduce a variation on the GP prior which enables us to model
structured time-series data, i.e. data containing groups where we wish to model
inter- and intra-group variability. Our innovation in the DP model is an
implementation of a new fast collapsed variational inference procedure which
enables us to optimize our variationala pproximation significantly faster than
standard VB approaches. In a biological time series application we show how our
model better captures salient features of the data, leading to better
consistency with existing biological classifications, while the associated
inference algorithm provides a twofold speed-up over EM-based variational
inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1626</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1626</id><created>2014-01-08</created><updated>2015-09-30</updated><authors><author><keyname>Paolini</keyname><forenames>Enrico</forenames></author><author><keyname>Liva</keyname><forenames>Gianluigi</forenames></author><author><keyname>Chiani</keyname><forenames>Marco</forenames></author></authors><title>Coded Slotted ALOHA: A Graph-Based Method for Uncoordinated Multiple
  Access</title><categories>cs.IT math.IT</categories><comments>The final version to appear in IEEE Trans. Inf. Theory. 18 pages, 10
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a random access scheme is introduced which relies on the
combination of packet erasure correcting codes and successive interference
cancellation (SIC). The scheme is named coded slotted ALOHA. A bipartite graph
representation of the SIC process, resembling iterative decoding of generalized
low-density parity-check codes over the erasure channel, is exploited to
optimize the selection probabilities of the component erasure correcting codes
via density evolution analysis. The capacity (in packets per slot) of the
scheme is then analyzed in the context of the collision channel without
feedback. Moreover, a capacity bound is developed and component code
distributions tightly approaching the bound are derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1632</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1632</id><created>2014-01-08</created><authors><author><keyname>Vega-Fuentes</keyname><forenames>E.</forenames></author><author><keyname>Rosario</keyname><forenames>S. Leon-del</forenames></author><author><keyname>Cerezo-Sanchez</keyname><forenames>J. M.</forenames></author><author><keyname>Vega-Martinez</keyname><forenames>A.</forenames></author></authors><title>Fuzzy Inference System for VOLT/VAR control in distribution substations
  in isolated power systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a fuzzy inference system for voltage/reactive power
control in distribution substations. The purpose is go forward to automation
distribution and its implementation in isolated power systems where control
capabilities are limited and it is common using the same applications as in
continental power systems. This means that lot of functionalities do not apply
and computational burden generates high response times. A fuzzy controller,
with logic guidelines embedded based upon heuristic rules resulting from
operators at dispatch control center past experience, has been designed.
Working as an on-line tool, it has been tested under real conditions and it has
managed the operation during a whole day in a distribution substation. Within
the limits of control capabilities of the system, the controller maintained
successfully an acceptable voltage profile, power factor values over 0,98 and
it has ostensibly improved the performance given by an optimal power flow based
automation system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1669</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1669</id><created>2014-01-08</created><authors><author><keyname>Wolff</keyname><forenames>J. Gerard</forenames></author></authors><title>Smart machines and the SP theory of intelligence</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1306.3890</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These notes describe how the &quot;SP theory of intelligence&quot;, and its embodiment
in the &quot;SP machine&quot;, may help to realise cognitive computing, as described in
the book &quot;Smart Machines&quot;. In the SP system, information compression and a
concept of &quot;multiple alignment&quot; are centre stage. The system is designed to
integrate such things as unsupervised learning, pattern recognition,
probabilistic reasoning, and more. It may help to overcome the problem of
variety in big data, it may serve in pattern recognition and in the
unsupervised learning of structure in data, and it may facilitate the
management and transmission of big data. There is potential, via information
compression, for substantial gains in computational efficiency, especially in
the use of energy. The SP system may help to realise data-centric computing,
perhaps via a development of Hebb's concept of a &quot;cell assembly&quot;, or via the
use of light or DNA for the processing of information. It has potential in the
management of errors and uncertainty in data, in medical diagnosis, in
processing streams of data, and in promoting adaptability in robots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1671</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1671</id><created>2014-01-08</created><authors><author><keyname>Naparstek</keyname><forenames>Oshri</forenames><affiliation>Senior Member, IEEE</affiliation></author><author><keyname>Leshem</keyname><forenames>Amir</forenames><affiliation>Senior Member, IEEE</affiliation></author><author><keyname>Jorswieck</keyname><forenames>Eduard</forenames><affiliation>Senior Member, IEEE</affiliation></author></authors><title>Distributed medium access control for energy efficient transmission in
  cognitive radios</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last few years the issue of energy-efficient network design has gained
increasingly more importance, both in academia and in industry. Information and
communication technologies (ICT) represent about 2% of the entire world's
energy consumption, and the situation is likely to reach a point where ICT
equipments in large cities will require more energy than is actually available.
In this paper we develop and investigate methods for energy efficiency
optimization for wireless networks with cognitive units. We show that the
problem can be solved without explicit message passing using a modified
distributed auction algorithm. Then we introduce a fast converging algorithm
that approximates the solution to the maximal energy efficiency problem. We
state conditions under which the fast algorithm is asymptotically optimal in
terms of the number of users. Finally, we provide simulated examples of the
methods developed in this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1686</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1686</id><created>2014-01-08</created><authors><author><keyname>Kretz</keyname><forenames>Tobias</forenames></author><author><keyname>Lehmann</keyname><forenames>Karsten</forenames></author><author><keyname>Hofs&#xe4;&#xdf;</keyname><forenames>Ingmar</forenames></author></authors><title>Pedestrian Route Choice by Iterated Equilibrium Search</title><categories>cs.MA cs.CE nlin.AO physics.soc-ph</categories><comments>contribution to proceedings of Traffic and Granular Flow 2013 (TGF13)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In vehicular traffic planning it is a long standing problem how to assign
demand such on the available model of a road network that an equilibrium with
regard to travel time or generalized costs is realized. For pedestrian traffic
this question can be asked as well. However, as the infrastructure of
pedestrian dynamics is not a network (a graph), but two-dimensional, there is
in principle an infinitely large set of routes. As a consequence none of the
iterating assignment methods developed for road traffic can be applied for
pedestrians. In this contribution a method to overcome this problem is briefly
summarized and applied with an example geometry which as a result is enhanced
with routes with intermediate destination areas of certain shape. The enhanced
geometry is used in some exemplary assignment calculations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1690</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1690</id><created>2014-01-08</created><authors><author><keyname>Andreyev</keyname><forenames>Sergey</forenames></author></authors><title>Tendencies, Dead-ends, and Promising Ways. From Interface Ideas to New
  Programs</title><categories>cs.HC</categories><comments>5 pages</comments><acm-class>H.5.2; D.2.2; H.1.2; I.3.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mechanism of communication between users and devices is called interface.
From time to time changes in interface significantly improve our work with
computers even without any serious changes in programs themselves. Main ideas
in PCs interface were introduced many years ago and since then there are no
significant changes, while new devices show promising ways by using direct
manipulation of screen objects. Users' direct action with all the screen
objects of our ordinary PCs turns standard screens into touch screens of very
high resolution and not only changes the interface of familiar programs but
creates the new type of programs: user-driven applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1693</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1693</id><created>2014-01-08</created><authors><author><keyname>Hofmann</keyname><forenames>Martin</forenames></author><author><keyname>Ruess</keyname><forenames>Harald</forenames></author></authors><title>Certification for mu-calculus with winning strategies</title><categories>cs.LO</categories><comments>Presented at the VeriSure workshop associated with CAV'13 in St.
  Petersburg in July 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define memory-efficient certificates for $\mu$-calculus model checking
problems based on the well-known correspondence of the $\mu$-calculus model
checking with winning certain parity games. Winning strategies can
independently checked, in low polynomial time, by observing that there is no
reachable strongly connected component in the graph of the parity game whose
largest priority is odd. Winning strategies are computed by fixpoint iteration
following the naive semantics of $\mu$-calculus. We instrument the usual
fixpoint iteration of $\mu$-calculus model checking so that it produces
evidence in the form of a winning strategy; these winning strategies can be
computed in polynomial time in $|S|$ and in space $O(|S|^2 |{\phi}|^2)$, where
$|S|$ is the size of the state space and $|\phi|$ the length of the formula
$\phi$\@. The main technical contribution here is the notion and algebra of
partial winning strategies. On the technical level our work can be seen as a
new, simpler, and immediate constructive proof of the correspondence between
$\mu$-calculus and parity games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1711</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1711</id><created>2014-01-08</created><updated>2014-07-07</updated><authors><author><keyname>Kolte</keyname><forenames>Ritesh</forenames></author><author><keyname>Niesen</keyname><forenames>Urs</forenames></author><author><keyname>Gupta</keyname><forenames>Piyush</forenames></author></authors><title>Energy-Efficient Communication over the Unsynchronized Gaussian Diamond
  Network</title><categories>cs.IT math.IT</categories><comments>20 pages, 4 figures, submitted to IEEE Transactions on Information
  Theory, presented at IEEE ISIT 2014</comments><journal-ref>IEEE Transactions on Information Theory, vol. 60, pp. 7719 - 7731,
  December 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication networks are often designed and analyzed assuming tight
synchronization among nodes. However, in applications that require
communication in the energy-efficient regime of low signal-to-noise ratios,
establishing tight synchronization among nodes in the network can result in a
significant energy overhead. Motivated by a recent result showing that
near-optimal energy efficiency can be achieved over the AWGN channel without
requiring tight synchronization, we consider the question of whether the
potential gains of cooperative communication can be achieved in the absence of
synchronization. We focus on the symmetric Gaussian diamond network and
establish that cooperative-communication gains are indeed feasible even with
unsynchronized nodes. More precisely, we show that the capacity per unit energy
of the unsynchronized symmetric Gaussian diamond network is within a constant
factor of the capacity per unit energy of the corresponding synchronized
network. To this end, we propose a distributed relaying scheme that does not
require tight synchronization but nevertheless achieves most of the energy
gains of coherent combining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1714</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1714</id><created>2014-01-08</created><authors><author><keyname>Stefanovic</keyname><forenames>Cedomir</forenames></author><author><keyname>Momoda</keyname><forenames>Miyu</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Exploiting Capture Effect in Frameless ALOHA for Massive Wireless Random
  Access</title><categories>cs.IT math.IT</categories><comments>Accepted for presentation at IEEE WCNC'14 Track 2 (MAC and
  Cross-Layer Design)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analogies between successive interference cancellation (SIC) in slotted
ALOHA framework and iterative belief-propagation erasure-decoding, established
recently, enabled the application of the erasure-coding theory and tools to
design random access schemes. This approach leads to throughput substantially
higher than the one offered by the traditional slotted ALOHA. In the simplest
setting, SIC progresses when a successful decoding occurs for a single user
transmission. In this paper we consider a more general setting of a channel
with capture and explore how such physical model affects the design of the
coded random access protocol. Specifically, we assess the impact of capture
effect in Rayleigh fading scenario on the design of SIC-enabled slotted ALOHA
schemes. We provide analytical treatment of frameless ALOHA, which is a special
case of SIC-enabled ALOHA scheme. We demonstrate both through analytical and
simulation results that the capture effect can be very beneficial in terms of
achieved throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1723</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1723</id><created>2014-01-08</created><updated>2014-05-16</updated><authors><author><keyname>Gudmundsdottir</keyname><forenames>Helga</forenames></author><author><keyname>&#xc1;sgeirsson</keyname><forenames>Eyj&#xf3;lfur I</forenames></author><author><keyname>Bodlaender</keyname><forenames>Marijke H. L.</forenames></author><author><keyname>Foley</keyname><forenames>Joseph T.</forenames></author><author><keyname>Halld&#xf3;rsson</keyname><forenames>Magn&#xfa;s M.</forenames></author><author><keyname>Vigfusson</keyname><forenames>Ymir</forenames></author></authors><title>Wireless Scheduling Algorithms in Complex Environments</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient spectrum use in wireless sensor networks through spatial reuse
requires effective models of packet reception at the physical layer in the
presence of interference. Despite recent progress in analytic and simulations
research into worst-case behavior from interference effects, these efforts
generally assume geometric path loss and isotropic transmission, assumptions
which have not been borne out in experiments.
  Our paper aims to provide a methodology for grounding theoretical results
into wireless interference in experimental reality. We develop a new framework
for wireless algorithms in which distance-based path loss is replaced by an
arbitrary gain matrix, typically obtained by measurements of received signal
strength (RSS). Gain matrices allow for the modeling of complex environments,
e.g., with obstacles and walls. We experimentally evaluate the framework in two
indoors testbeds with 20 and 60 motes, and confirm superior predictive
performance in packet reception rate for a gain matrix model over a geometric
distance-based model.
  At the heart of our approach is a new parameter $\zeta$ called metricity
which indicates how close the gain matrix is to a distance metric, effectively
measuring the complexity of the environment. A powerful theoretical feature of
this parameter is that all known SINR scheduling algorithms that work in
general metric spaces carry over to arbitrary gain matrices and achieve
equivalent performance guarantees in terms of $\zeta$ as previously obtained in
terms of the path loss constant. Our experiments confirm the sensitivity of
$\zeta$ to the nature of the environment. Finally, we show analytically and
empirically how multiple channels can be leveraged to improve metricity and
thereby performance. We believe our contributions will facilitate experimental
validation for recent advances in algorithms for physical wireless interference
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1732</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1732</id><created>2014-01-08</created><authors><author><keyname>Sordoni</keyname><forenames>Alessandro</forenames></author><author><keyname>Nie</keyname><forenames>Jian-Yun</forenames></author></authors><title>Looking at Vector Space and Language Models for IR using Density
  Matrices</title><categories>cs.IR</categories><comments>In Proceedings of Quantum Interaction 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we conduct a joint analysis of both Vector Space and Language
Models for IR using the mathematical framework of Quantum Theory. We shed light
on how both models allocate the space of density matrices. A density matrix is
shown to be a general representational tool capable of leveraging capabilities
of both VSM and LM representations thus paving the way for a new generation of
retrieval models. We analyze the possible implications suggested by our
findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1742</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1742</id><created>2014-01-08</created><authors><author><keyname>Bhute</keyname><forenames>Avinash N</forenames></author><author><keyname>Meshram</keyname><forenames>B. B.</forenames></author></authors><title>Content Based Image Indexing and Retrieval</title><categories>cs.CV cs.GR cs.IR cs.MM</categories><comments>12 pages</comments><journal-ref>IJGIP 2013 Vol 3 issue 4</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the efficient content based image retrieval systems
which employ the color, texture and shape information of images to facilitate
the retrieval process. For efficient feature extraction, we extract the color,
texture and shape feature of images automatically using edge detection which is
widely used in signal processing and image compression. For facilitated the
speedy retrieval we are implements the antipole-tree algorithm for indexing the
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1752</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1752</id><created>2014-01-06</created><authors><author><keyname>Jamil</keyname><forenames>Noreen</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Johannes</forenames></author><author><keyname>Lutteroth</keyname><forenames>Christof</forenames></author><author><keyname>Weber</keyname><forenames>Gerald</forenames></author></authors><title>Speeding up SOR Solvers for Constraint-based GUIs with a Warm-Start
  Strategy</title><categories>cs.HC cs.AI cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many computer programs have graphical user interfaces (GUIs), which need good
layout to make efficient use of the available screen real estate. Most GUIs do
not have a fixed layout, but are resizable and able to adapt themselves.
Constraints are a powerful tool for specifying adaptable GUI layouts: they are
used to specify a layout in a general form, and a constraint solver is used to
find a satisfying concrete layout, e.g.\ for a specific GUI size. The
constraint solver has to calculate a new layout every time a GUI is resized or
changed, so it needs to be efficient to ensure a good user experience. One
approach for constraint solvers is based on the Gauss-Seidel algorithm and
successive over-relaxation (SOR).
  Our observation is that a solution after resizing or changing is similar in
structure to a previous solution. Thus, our hypothesis is that we can increase
the computational performance of an SOR-based constraint solver if we reuse the
solution of a previous layout to warm-start the solving of a new layout. In
this paper we report on experiments to test this hypothesis experimentally for
three common use cases: big-step resizing, small-step resizing and constraint
change. In our experiments, we measured the solving time for randomly generated
GUI layout specifications of various sizes. For all three cases we found that
the performance is improved if an existing solution is used as a starting
solution for a new layout.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1753</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1753</id><created>2014-01-08</created><updated>2014-01-13</updated><authors><author><keyname>Sadhukhan</keyname><forenames>Sounak</forenames></author><author><keyname>Sarma</keyname><forenames>Samar Sen</forenames></author></authors><title>A Solution of Degree Constrained Spanning Tree Using Hybrid GA</title><categories>cs.NE cs.DS</categories><comments>This paper has been withdrawn by the author due to some crucial
  errors</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In real life, it is always an urge to reach our goal in minimum effort i.e.,
it should have a minimum constrained path. The path may be shortest route in
practical life, either physical or electronic medium. The scenario is to
represents the ambiance as a graph and to find a spanning tree with custom
design criteria. Here, we have chosen a minimum degree spanning tree, which can
be generated in real time with minimum turnaround time. The problem is
NP-complete in nature [1, 2]. The solution approach, in general, is
approximate. We have used a heuristic approach, namely hybrid genetic algorithm
(GA), with motivated criteria of encoded data structures of graph. We compare
the experimental result with the existing approximate algorithm and the result
is so encouraging that we are interested to use it in our future applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1757</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1757</id><created>2014-01-08</created><updated>2014-06-27</updated><authors><author><keyname>Tucker</keyname><forenames>Mark</forenames></author><author><keyname>Bull</keyname><forenames>J. Mark</forenames></author></authors><title>An efficient algorithm for the calculation of reserves for non-unit
  linked life policies</title><categories>q-fin.CP cs.CE</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The underlying stochastic nature of the requirements for the Solvency II
regulations has introduced significant challenges if the required calculations
are to be performed correctly, without resorting to excessive approximations,
within practical timescales. It is generally acknowledged by practising
actuaries within UK life offices that it is currently impossible to correctly
fulfil the requirements imposed by Solvency II using existing computational
techniques based on commercially available valuation packages. Our work has
already shown that it is possible to perform profitability calculations at a
far higher rate than is achievable using commercial packages. One of the key
factors in achieving these gains is to calculate reserves using recurrence
relations that scale linearly with the number of time steps. Here, we present a
general vector recurrence relation which can be used for a wide range of
non-unit linked policies that are covered by Solvency II; such contracts
include annuities, term assurances, and endowments. Our results suggest that by
using an optimised parallel implementation of this algorithm, on an affordable
hardware platform, it is possible to perform the `brute force' approach to
demonstrating solvency in a realistic timescale (of the order of a few hours).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1760</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1760</id><created>2014-01-08</created><updated>2014-02-03</updated><authors><author><keyname>Sinha</keyname><forenames>Abhinav</forenames></author><author><keyname>Anastasopoulos</keyname><forenames>Achilleas</forenames></author></authors><title>Generalized Proportional Allocation Mechanism Design for Unicast Service
  on the Internet</title><categories>cs.GT</categories><comments>arXiv admin note: text overlap with arXiv:1307.2569</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report we construct two mechanisms that fully implement social
welfare maximising allocation in Nash equilibria for the case of a single
infinitely divisible good subject to multiple inequality constraints. The first
mechanism achieves weak budget balance, while the second is an extension of the
first, and achieves strong budget balance. One important application of this
mechanism is unicast service on the Internet where a network operator wishes to
allocate rates among strategic users in such a way that maximise overall user
satisfaction while respecting capacity constraints on every link in the
network. The emphasis of this work is on full implementation, which means that
all Nash equilibria of the induced game result in the optimal allocations of
the centralized allocation problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1763</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1763</id><created>2014-01-08</created><updated>2014-01-26</updated><authors><author><keyname>Braverman</keyname><forenames>Vladimir</forenames></author><author><keyname>Katzman</keyname><forenames>Jonathan</forenames></author><author><keyname>Seidell</keyname><forenames>Charles</forenames></author><author><keyname>Vorsanger</keyname><forenames>Gregory</forenames></author></authors><title>Approximating Large Frequency Moments with $O(n^{1-2/k})$ Bits</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of approximating frequency moments in
the streaming model. Given a stream $D = \{p_1,p_2,\dots,p_m\}$ of numbers from
$\{1,\dots, n\}$, a frequency of $i$ is defined as $f_i = |\{j: p_j = i\}|$.
The $k$-th \emph{frequency moment} of $D$ is defined as $F_k = \sum_{i=1}^n
f_i^k$.
  In this paper we give an upper bound on the space required to find a $k$-th
frequency moment of $O(n^{1-2/k})$ bits that matches, up to a constant factor,
the lower bound of Woodruff and Zhang (STOC 12) for constant $\epsilon$ and
constant $k$. Our algorithm makes a single pass over the stream and works for
any constant $k &gt; 3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1766</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1766</id><created>2014-01-08</created><updated>2015-08-31</updated><authors><author><keyname>Wang</keyname><forenames>James Z.</forenames></author><author><keyname>Zhang</keyname><forenames>Yuanyuan</forenames></author><author><keyname>Dong</keyname><forenames>Liang</forenames></author><author><keyname>Li</keyname><forenames>Lin</forenames></author><author><keyname>Srimani</keyname><forenames>Pradip K</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>G-Bean: an ontology-graph based web tool for biomedical literature
  retrieval</title><categories>cs.IR</categories><comments>This paper has been withdrawn by the author due to errors in figure 1</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Currently, most people use PubMed to search the MEDLINE database, an
important bibliographical information source for life science and biomedical
information. However, PubMed has some drawbacks that make it difficult to find
relevant publications pertaining to users' individual intentions, especially
for non-expert users. To ameliorate the disadvantages of PubMed, we developed
G-Bean, a graph based biomedical search engine, to search biomedical articles
in MEDLINE database more efficiently.G-Bean addresses PubMed's limitations with
three innovations: parallel document index creation,ontology-graph based query
expansion, and retrieval and re-ranking of documents based on user's search
intention.Performance evaluation with 106 OHSUMED benchmark queries shows that
G-Bean returns more relevant results than PubMed does when using these queries
to search the MEDLINE database. PubMed could not even return any search result
for some OHSUMED queries because it failed to form the appropriate Boolean
query statement automatically from the natural language query strings. G-Bean
is available at http://bioinformatics.clemson.edu/G-Bean/index.php.G-Bean
addresses PubMed's limitations with ontology-graph based query expansion,
automatic document indexing, and user search intention discovery. It shows
significant advantages in finding relevant articles from the MEDLINE database
to meet the information need of the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1770</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1770</id><created>2014-01-08</created><authors><author><keyname>Leconte</keyname><forenames>Mathieu</forenames></author><author><keyname>Lelarge</keyname><forenames>Marc</forenames></author><author><keyname>Massouli&#xe9;</keyname><forenames>Laurent</forenames></author></authors><title>Adaptive Replication in Distributed Content Delivery Networks</title><categories>cs.NI</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of content replication in large distributed content
delivery networks, composed of a data center assisted by many small servers
with limited capabilities and located at the edge of the network. The objective
is to optimize the placement of contents on the servers to offload as much as
possible the data center. We model the system constituted by the small servers
as a loss network, each loss corresponding to a request to the data center.
Based on large system / storage behavior, we obtain an asymptotic formula for
the optimal replication of contents and propose adaptive schemes related to
those encountered in cache networks but reacting here to loss events, and
faster algorithms generating virtual events at higher rate while keeping the
same target replication. We show through simulations that our adaptive schemes
outperform significantly standard replication strategies both in terms of loss
rates and adaptation speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1771</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1771</id><created>2014-01-08</created><authors><author><keyname>Xiang</keyname><forenames>Yang</forenames></author></authors><title>Simple linear algorithms for mining graph cores</title><categories>cs.DS cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Batagelj and Zaversnik proposed a linear algorithm for the well-known
$k$-core decomposition problem. However, when $k$-cores are desired for a given
$k$, we find that a simple linear algorithm requiring no sorting works for
mining $k$-cores. In addition, this algorithm can be extended to mine $(k_1,
k_2,\ldots, k_p)$-cores from $p$-partite graphs in linear time, and this mining
approach can be efficiently implemented in a distributed computing environment
with a lower message complexity bound in comparison with the best known method
of distributed $k$-core decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1773</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1773</id><created>2014-01-08</created><updated>2015-05-07</updated><authors><author><keyname>Elsheikh</keyname><forenames>Mustafa</forenames></author><author><keyname>Giesbrecht</keyname><forenames>Mark</forenames></author></authors><title>Relating $p$-adic eigenvalues and the local Smith normal form</title><categories>math.RA cs.SC math.PR</categories><comments>To appear in Linear Algebra and Its Applications</comments><msc-class>15A36, 15A18, 15A21</msc-class><doi>10.1016/j.laa.2015.05.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditions are established under which the $p$-adic valuations of the
invariant factors (diagonal entries of the Smith form) of an integer matrix are
equal to the $p$-adic valuations of the eigenvalues. It is then shown that this
correspondence is the typical case for &quot;most&quot; matrices; precise density bounds
are given for when the property holds, as well as easy transformations to this
typical case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1778</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1778</id><created>2014-01-08</created><authors><author><keyname>Jagadeesh</keyname><forenames>Vignesh</forenames></author><author><keyname>Piramuthu</keyname><forenames>Robinson</forenames></author><author><keyname>Bhardwaj</keyname><forenames>Anurag</forenames></author><author><keyname>Di</keyname><forenames>Wei</forenames></author><author><keyname>Sundaresan</keyname><forenames>Neel</forenames></author></authors><title>Large Scale Visual Recommendations From Street Fashion Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a completely automated large scale visual recommendation system
for fashion. Our focus is to efficiently harness the availability of large
quantities of online fashion images and their rich meta-data. Specifically, we
propose four data driven models in the form of Complementary Nearest Neighbor
Consensus, Gaussian Mixture Models, Texture Agnostic Retrieval and Markov Chain
LDA for solving this problem. We analyze relative merits and pitfalls of these
algorithms through extensive experimentation on a large-scale data set and
baseline them against existing ideas from color science. We also illustrate key
fashion insights learned through these experiments and show how they can be
employed to design better recommendation systems. Finally, we also outline a
large-scale annotated data set of fashion images (Fashion-136K) that can be
exploited for future vision research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1783</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1783</id><created>2014-01-08</created><authors><author><keyname>Sen</keyname><forenames>Arunabha</forenames></author><author><keyname>Mazumder</keyname><forenames>Anisha</forenames></author><author><keyname>Banerjee</keyname><forenames>Joydeep</forenames></author><author><keyname>Das</keyname><forenames>Arun</forenames></author><author><keyname>Compton</keyname><forenames>Randy</forenames></author></authors><title>Identification of $\cal K$ Most Vulnerable Nodes in Multi-layered
  Network Using a New Model of Interdependency</title><categories>cs.NI</categories><comments>6 pages, 4 figures, submitted to NetSciCom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The critical infrastructures of the nation including the power grid and the
communication network are highly interdependent. Recognizing the need for a
deeper understanding of the interdependency in a multi-layered network,
significant efforts have been made by the research community in the last few
years to achieve this goal. Accordingly a number of models have been proposed
and analyzed. Unfortunately, most of the models are over simplified and, as
such, they fail to capture the complex interdependency that exists between
entities of the power grid and the communication networks involving a
combination of conjunctive and disjunctive relations. To overcome the
limitations of existing models, we propose a new model that is able to capture
such complex interdependency relations. Utilizing this model, we provide
techniques to identify the $\cal K$ most vulnerable nodes of an interdependent
network. We show that the problem can be solved in polynomial time in some
special cases, whereas for some others, the problem is NP-complete. We
establish that this problem is equivalent to computation of a {\em fixed point}
of a multilayered network system and we provide a technique for its computation
utilizing Integer Linear Programming. Finally, we evaluate the efficacy of our
technique using real data collected from the power grid and the communication
network that span the Maricopa County of Arizona.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1803</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1803</id><created>2014-01-08</created><authors><author><keyname>Lauly</keyname><forenames>Stanislas</forenames></author><author><keyname>Boulanger</keyname><forenames>Alex</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author></authors><title>Learning Multilingual Word Representations using a Bag-of-Words
  Autoencoder</title><categories>cs.CL cs.LG stat.ML</categories><comments>This workshop paper was accepted on Octoble 30 2013 at the NIPS 2013
  workshop on deep learning
  (https://sites.google.com/site/deeplearningworkshopnips2013/accepted-papers)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on learning multilingual word representations usually relies on
the use of word-level alignements (e.g. infered with the help of GIZA++)
between translated sentences, in order to align the word embeddings in
different languages. In this workshop paper, we investigate an autoencoder
model for learning multilingual word representations that does without such
word-level alignements. The autoencoder is trained to reconstruct the
bag-of-word representation of given sentence from an encoded representation
extracted from its translation. We evaluate our approach on a multilingual
document classification task, where labeled data is available only for one
language (e.g. English) while classification must be performed in a different
language (e.g. French). In our experiments, we observe that our method compares
favorably with a previously proposed method that exploits word-level alignments
to learn word representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1842</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1842</id><created>2014-01-08</created><authors><author><keyname>Liu</keyname><forenames>Jason Gejie</forenames></author><author><keyname>Aeron</keyname><forenames>Shuchin</forenames></author></authors><title>Robust Large Scale Non-negative Matrix Factorization using Proximal
  Point Algorithm</title><categories>stat.ML cs.IT cs.LG cs.NA math.IT</categories><comments>Appeared in IEEE GlobalSIP, 2013, TX, Austin</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robust algorithm for non-negative matrix factorization (NMF) is presented
in this paper with the purpose of dealing with large-scale data, where the
separability assumption is satisfied. In particular, we modify the Linear
Programming (LP) algorithm of [9] by introducing a reduced set of constraints
for exact NMF. In contrast to the previous approaches, the proposed algorithm
does not require the knowledge of factorization rank (extreme rays [3] or
topics [7]). Furthermore, motivated by a similar problem arising in the context
of metabolic network analysis [13], we consider an entirely different regime
where the number of extreme rays or topics can be much larger than the
dimension of the data vectors. The performance of the algorithm for different
synthetic data sets are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1849</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1849</id><created>2014-01-08</created><updated>2015-03-24</updated><authors><author><keyname>Bauer</keyname><forenames>Matthew Steven</forenames><affiliation>University of Illinois at Urbana-Champaign</affiliation></author></authors><title>The Computational Complexity of Propositional Cirquent Calculus</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (March 25,
  2015) lmcs:1127</journal-ref><doi>10.2168/LMCS-11(1:12)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Introduced in 2006 by Japaridze, cirquent calculus is a refinement of sequent
calculus. The advent of cirquent calculus arose from the need for a deductive
system with a more explicit ability to reason about resources. Unlike the more
traditional proof-theoretic approaches that manipulate tree-like objects
(formulas, sequents, etc.), cirquent calculus is based on circuit-style
structures called cirquents, in which different &quot;peer&quot; (sibling, cousin, etc.)
substructures may share components. It is this resource sharing mechanism to
which cirquent calculus owes its novelty (and its virtues). From its inception,
cirquent calculus has been paired with an abstract resource semantics. This
semantics allows for reasoning about the interaction between a resource
provider and a resource user, where resources are understood in the their most
general and intuitive sense. Interpreting resources in a more restricted
computational sense has made cirquent calculus instrumental in axiomatizing
various fundamental fragments of Computability Logic, a formal theory of
(interactive) computability. The so-called &quot;classical&quot; rules of cirquent
calculus, in the absence of the particularly troublesome contraction rule,
produce a sound and complete system CL5 for Computability Logic. In this paper,
we investigate the computational complexity of CL5, showing it is
$\Sigma_2^p$-complete. We also show that CL5 without the duplication rule has
polynomial size proofs and is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1861</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1861</id><created>2014-01-08</created><authors><author><keyname>Breuer</keyname><forenames>Peter T.</forenames></author><author><keyname>Bowen</keyname><forenames>Jonathan P.</forenames></author></authors><title>Empirical Patterns in Google Scholar Citation Counts</title><categories>cs.DL</categories><comments>6 pages, 8 figures, submitted to Cyberpatterns 2014</comments><msc-class>62P99, 01A90</msc-class><acm-class>I.5.1; I.7.5</acm-class><doi>10.1109/SOSE.2014.55</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scholarly impact may be metricized using an author's total number of
citations as a stand-in for real worth, but this measure varies in
applicability between disciplines. The detail of the number of citations per
publication is nowadays mapped in much more detail on the Web, exposing certain
empirical patterns. This paper explores those patterns, using the citation data
from Google Scholar for a number of authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1872</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1872</id><created>2014-01-08</created><authors><author><keyname>Beame</keyname><forenames>Paul</forenames></author><author><keyname>Koutris</keyname><forenames>Paraschos</forenames></author><author><keyname>Suciu</keyname><forenames>Dan</forenames></author></authors><title>Skew in Parallel Query Processing</title><categories>cs.DB cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of computing a conjunctive query q in parallel, using p
of servers, on a large database. We consider algorithms with one round of
communication, and study the complexity of the communication. We are especially
interested in the case where the data is skewed, which is a major challenge for
scalable parallel query processing. We establish a tight connection between the
fractional edge packings of the query and the amount of communication, in two
cases. First, in the case when the only statistics on the database are the
cardinalities of the input relations, and the data is skew-free, we provide
matching upper and lower bounds (up to a poly log p factor) expressed in terms
of fractional edge packings of the query q. Second, in the case when the
relations are skewed and the heavy hitters and their frequencies are known, we
provide upper and lower bounds (up to a poly log p factor) expressed in terms
of packings of residual queries obtained by specializing the query to a heavy
hitter. All our lower bounds are expressed in the strongest form, as number of
bits needed to be communicated between processors with unlimited computational
power. Our results generalizes some prior results on uniform databases (where
each relation is a matching) [4], and other lower bounds for the MapReduce
model [1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1874</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1874</id><created>2014-01-08</created><authors><author><keyname>Perera</keyname><forenames>Sirani M.</forenames></author><author><keyname>Bonik</keyname><forenames>Grigory</forenames></author><author><keyname>Olshevsky</keyname><forenames>Vadim</forenames></author></authors><title>A Fast Algorithm for the Inversion of Quasiseparable Vandermonde-like
  Matrices</title><categories>math.NA cs.SC</categories><msc-class>15A09, 15B05, 65Y04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The results on Vandermonde-like matrices were introduced as a generalization
of polynomial Vandermonde matrices, and the displacement structure of these
matrices was used to derive an inversion formula. In this paper we first
present a fast Gaussian elimination algorithm for the polynomial
Vandermonde-like matrices. Later we use the said algorithm to derive fast
inversion algorithms for quasiseparable, semiseparable and well-free
Vandermonde-like matrices having $\mathcal{O}(n^2)$ complexity. To do so we
identify structures of displacement operators in terms of generators and the
recurrence relations(2-term and 3-term) between the columns of the basis
transformation matrices for quasiseparable, semiseparable and well-free
polynomials. Finally we present an $\mathcal{O}(n^2)$ algorithm to compute the
inversion of quasiseparable Vandermonde-like matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1876</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1876</id><created>2014-01-08</created><authors><author><keyname>Bose</keyname><forenames>Subhonmesh</forenames></author><author><keyname>Low</keyname><forenames>Steven H.</forenames></author><author><keyname>Teeraratkul</keyname><forenames>Thanchanok</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Equivalent relaxations of optimal power flow</title><categories>cs.SY</categories><comments>12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several convex relaxations of the optimal power flow (OPF) problem have
recently been developed using both bus injection models and branch flow models.
In this paper, we prove relations among three convex relaxations: a
semidefinite relaxation that computes a full matrix, a chordal relaxation based
on a chordal extension of the network graph, and a second-order cone relaxation
that computes the smallest partial matrix. We prove a bijection between the
feasible sets of the OPF in the bus injection model and the branch flow model,
establishing the equivalence of these two models and their second-order cone
relaxations. Our results imply that, for radial networks, all these relaxations
are equivalent and one should always solve the second-order cone relaxation.
For mesh networks, the semidefinite relaxation is tighter than the second-order
cone relaxation but requires a heavier computational effort, and the chordal
relaxation strikes a good balance. Simulations are used to illustrate these
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1880</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1880</id><created>2014-01-08</created><updated>2015-03-25</updated><authors><author><keyname>Liebman</keyname><forenames>Elad</forenames></author><author><keyname>Saar-Tsechansky</keyname><forenames>Maytal</forenames></author><author><keyname>Stone</keyname><forenames>Peter</forenames></author></authors><title>DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation</title><categories>cs.LG</categories><comments>-Updated to the most recent and completed version (to be presented at
  AAMAS 2015) -Updated author list. in Autonomous Agents and Multiagent Systems
  (AAMAS) 2015, Istanbul, Turkey, May 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, there has been growing focus on the study of automated
recommender systems. Music recommendation systems serve as a prominent domain
for such works, both from an academic and a commercial perspective. A
fundamental aspect of music perception is that music is experienced in temporal
context and in sequence. In this work we present DJ-MC, a novel
reinforcement-learning framework for music recommendation that does not
recommend songs individually but rather song sequences, or playlists, based on
a model of preferences for both songs and song transitions. The model is
learned online and is uniquely adapted for each listener. To reduce exploration
time, DJ-MC exploits user feedback to initialize a model, which it subsequently
updates by reinforcement. We evaluate our framework with human participants
using both real song and playlist data. Our results indicate that DJ-MC's
ability to recommend sequences of songs provides a significant improvement over
more straightforward approaches, which do not take transitions into account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1882</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1882</id><created>2014-01-08</created><authors><author><keyname>Sun</keyname><forenames>Yuli</forenames></author><author><keyname>Tao</keyname><forenames>Jinxu</forenames></author></authors><title>Image reconstruction from few views by L0-norm optimization</title><categories>cs.IT cs.CV math.IT</categories><comments>11 pages,5 figures, 1 table</comments><doi>10.1088/1674-1056/23/7/078703</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The L1-norm of the gradient-magnitude images (GMI), which is the well-known
total variation (TV) model, is widely used as regularization in the few views
CT reconstruction. As the L1-norm TV regularization is tending to uniformly
penalize the image gradient and the low-contrast structures are sometimes over
smoothed, we proposed a new algorithm based on the L0-norm of the GMI to deal
with the few views problem. To rise to the challenges introduced by the L0-norm
DGT, the algorithm uses a pseudo-inverse transform of DGT and adapts an
iterative hard thresholding (IHT) algorithm, whose convergence and effective
efficiency have been theoretically proven. The simulation indicates that the
algorithm proposed in this paper can obviously improve the reconstruction
quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1887</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1887</id><created>2014-01-08</created><authors><author><keyname>Li</keyname><forenames>Shuxing</forenames></author><author><keyname>Feng</keyname><forenames>Tao</forenames></author><author><keyname>Ge</keyname><forenames>Gennian</forenames></author></authors><title>On the Weight Distribution of Cyclic Codes with Niho Exponents</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been intensive research on the weight distributions of
cyclic codes. In this paper, we compute the weight distributions of three
classes of cyclic codes with Niho exponents. More specifically, we obtain two
classes of binary three-weight and four-weight cyclic codes and a class of
nonbinary four-weight cyclic codes. The weight distributions follow from the
determination of value distributions of certain exponential sums. Several
examples are presented to show that some of our codes are optimal and some have
the best known parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1888</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1888</id><created>2014-01-08</created><updated>2016-02-21</updated><authors><author><keyname>Wang</keyname><forenames>Li-Xin</forenames></author></authors><title>Dynamical Models of Stock Prices Based on Technical Trading Rules Part
  I: The Models</title><categories>q-fin.TR cs.CE q-fin.ST</categories><journal-ref>IEEE Trans. on Fuzzy Systems, Vol. 23, No. 4, pp. 787-801, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we use fuzzy systems theory to convert the technical trading
rules commonly used by stock practitioners into excess demand functions which
are then used to drive the price dynamics. The technical trading rules are
recorded in natural languages where fuzzy words and vague expressions abound.
In Part I of this paper, we will show the details of how to transform the
technical trading heuristics into nonlinear dynamic equations. First, we define
fuzzy sets to represent the fuzzy terms in the technical trading rules; second,
we translate each technical trading heuristic into a group of fuzzy IF-THEN
rules; third, we combine the fuzzy IF-THEN rules in a group into a fuzzy
system; and finally, the linear combination of these fuzzy systems is used as
the excess demand function in the price dynamic equation. We transform a wide
variety of technical trading rules into fuzzy systems, including moving average
rules, support and resistance rules, trend line rules, big buyer, big seller
and manipulator rules, band and stop rules, and volume and relative strength
rules. Simulation results show that the price dynamics driven by these
technical trading rules are complex and chaotic, and some common phenomena in
real stock prices such as jumps, trending and self-fulfilling appear naturally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1891</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1891</id><created>2014-01-08</created><updated>2016-02-21</updated><authors><author><keyname>Wang</keyname><forenames>Li-Xin</forenames></author></authors><title>Dynamical Models of Stock Prices Based on Technical Trading Rules Part
  II: Analysis of the Models</title><categories>q-fin.TR cs.CE q-fin.ST</categories><journal-ref>IEEE Trans. on Fuzzy Systems, Vol. 23, No. 4, pp. 1127-1141, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Part II of this paper, we concentrate our analysis on the price dynamical
model with the moving average rules developed in Part I of this paper. By
decomposing the excessive demand function, we reveal that it is the interplay
between trend-following and contrarian actions that generates the price chaos,
and give parameter ranges for the price series to change from divergence to
chaos and to oscillation. We prove that the price dynamical model has an
infinite number of equilibrium points but all these equilibrium points are
unstable. We demonstrate the short-term predictability of the return volatility
and derive the detailed formula of the Lyapunov exponent as function of the
model parameters. We show that although the price is chaotic, the volatility
converges to some constant very quickly at the rate of the Lyapunov exponent.
We extract the formula relating the converged volatility to the model
parameters based on Monte-Carlo simulations. We explore the circumstances under
which the returns show independency and illustrate in details how the
independency index changes with the model parameters. Finally, we plot the
strange attractor and return distribution of the chaotic price model to
illustrate the complex structure and fat-tailed distribution of the returns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1892</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1892</id><created>2014-01-08</created><updated>2016-02-21</updated><authors><author><keyname>Wang</keyname><forenames>Li-Xin</forenames></author></authors><title>Dynamical Models of Stock Prices Based on Technical Trading Rules Part
  III: Application to Hong Kong Stocks</title><categories>q-fin.TR cs.CE q-fin.ST</categories><journal-ref>IEEE Trans. on Fuzzy Systems, Vol. 23, No. 5, pp. 1680-1697, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Part III of this study, we apply the price dynamical model with big buyers
and big sellers developed in Part I of this paper to the daily closing prices
of the top 20 banking and real estate stocks listed in the Hong Kong Stock
Exchange. The basic idea is to estimate the strength parameters of the big
buyers and the big sellers in the model and make buy/sell decisions based on
these parameter estimates. We propose two trading strategies: (i)
Follow-the-Big-Buyer which buys when big buyer begins to appear and there is no
sign of big sellers, holds the stock as long as the big buyer is still there,
and sells the stock once the big buyer disappears; and (ii) Ride-the-Mood which
buys as soon as the big buyer strength begins to surpass the big seller
strength, and sells the stock once the opposite happens. Based on the testing
over 245 two-year intervals uniformly distributed across the seven years from
03-July-2007 to 02-July-2014 which includes a variety of scenarios, the net
profits would increase 67% or 120% on average if an investor switched from the
benchmark Buy-and-Hold strategy to the Follow-the-Big-Buyer or Ride-the-Mood
strategies during this period, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1895</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1895</id><created>2014-01-09</created><authors><author><keyname>Shahbaba</keyname><forenames>Mahdi</forenames></author><author><keyname>Beheshti</keyname><forenames>Soosan</forenames></author></authors><title>Efficient unimodality test in clustering by signature testing</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a new unimodality test with application in hierarchical
clustering methods. The proposed method denoted by signature test (Sigtest),
transforms the data based on its statistics. The transformed data has much
smaller variation compared to the original data and can be evaluated in a
simple proposed unimodality test. Compared with the existing unimodality tests,
Sigtest is more accurate in detecting the overlapped clusters and has a much
less computational complexity. Simulation results demonstrate the efficiency of
this statistic test for both real and synthetic data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1903</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1903</id><created>2014-01-09</created><authors><author><keyname>Shim</keyname><forenames>Young-Chul</forenames></author></authors><title>Distributed Cloud Computing Environment Enhanced with Capabilities for
  Wide-Area Migration and Replication of Virtual Machines</title><categories>cs.DC</categories><comments>13 pages</comments><journal-ref>International Journal of Computer Science &amp; Information
  Technology, Vol. 5, No. 6, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a network application is implmented as a virtual machine on a cloud and
is used by a large number of users, the location of the virtual machine should
be selected carefully so that the response time experienced by users is
minimized. As the user population moves and/or increases, the virtual machine
may need to be migrated to a new location or replicated on many locations over
a wide-area network. Virtual machine migration and replication have been
studied extensively but in most cases are limited within a subnetwork to be
able to maintain service continuity. In this paper we introduce a distributed
cloud computing environment which facilitates the migration and replication of
a virtual machine over a wide area network. The mechanism is provided by an
overlay network of smart routers, each of which connects a cooperating data
center to the Internet. The proposed approach is analyzed and compared with
related works
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1905</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1905</id><created>2014-01-09</created><authors><author><keyname>Corus</keyname><forenames>Dogan</forenames></author><author><keyname>Lehre</keyname><forenames>Per Kristian</forenames></author><author><keyname>Neumann</keyname><forenames>Frank</forenames></author><author><keyname>Pourhassan</keyname><forenames>Mojgan</forenames></author></authors><title>A Parameterized Complexity Analysis of Bi-level Optimisation with
  Evolutionary Algorithms</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bi-level optimisation problems have gained increasing interest in the field
of combinatorial optimisation in recent years. With this paper, we start the
runtime analysis of evolutionary algorithms for bi-level optimisation problems.
We examine two NP-hard problems, the generalised minimum spanning tree problem
(GMST), and the generalised travelling salesman problem (GTSP) in the context
of parameterised complexity.
  For the generalised minimum spanning tree problem, we analyse the two
approaches presented by Hu and Raidl (2012) with respect to the number of
clusters that distinguish each other by the chosen representation of possible
solutions. Our results show that a (1+1) EA working with the spanning nodes
representation is not a fixed-parameter evolutionary algorithm for the problem,
whereas the global structure representation enables to solve the problem in
fixed-parameter time. We present hard instances for each approach and show that
the two approaches are highly complementary by proving that they solve each
other's hard instances very efficiently.
  For the generalised travelling salesman problem, we analyse the problem with
respect to the number of clusters in the problem instance. Our results show
that a (1+1) EA working with the global structure representation is a
fixed-parameter evolutionary algorithm for the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1906</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1906</id><created>2014-01-09</created><authors><author><keyname>Liggesmeyer</keyname><forenames>Peter</forenames></author><author><keyname>Heidrich</keyname><forenames>Jens</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Kalckl&#xf6;sch</keyname><forenames>Robert</forenames></author><author><keyname>Barthel</keyname><forenames>Henning</forenames></author><author><keyname>Zeckzer</keyname><forenames>Dirk</forenames></author></authors><title>Visualization of Software and Systems as Support Mechanism for
  Integrated Software Project Control</title><categories>cs.SE</categories><comments>10 pages. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F978-3-642-02574-7_94</comments><report-no>Human-Computer Interaction: New Trends, volume 5610 of Lecture Notes
  in Computer Science, pages 846-855. Springer Berlin Heidelberg, 2009</report-no><doi>10.1007/978-3-642-02574-7_94</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many software development organizations still lack support for obtaining
intellectual control over their software development processes and for
determining the performance of their processes and the quality of the produced
products. Systematic support for detecting and reacting to critical process and
product states in order to achieve planned goals is usually missing. One means
to institutionalize measurement on the basis of explicit models is the
development and establishment of a so-called Software Project Control Center
(SPCC) for systematic quality assurance and management support. An SPCC is
comparable to a control room, which is a well known term in the mechanical
production domain. One crucial task of an SPCC is the systematic visualization
of measurement data in order to provide context-, purpose-, and role-oriented
information for all stakeholders (e.g., project managers, quality assurance
managers, developers) during the execution of a software development project.
The article will present an overview of SPCC concepts, a concrete instantiation
that supports goal-oriented data visualization, as well as examples and
experiences from practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1907</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1907</id><created>2014-01-09</created><authors><author><keyname>Chowdhury</keyname><forenames>Md. Ibrahim</forenames></author><author><keyname>Iqbal</keyname><forenames>Mohammad</forenames></author><author><keyname>Sultana</keyname><forenames>Naznin</forenames></author><author><keyname>Rahman</keyname><forenames>Faisal</forenames></author></authors><title>Analyzing an Analytical Solution Model for Simultaneous Mobility</title><categories>cs.NI</categories><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  5, December 2013, pp 111-124</journal-ref><doi>10.5121/ijwmn.2013.5609</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Current mobility models for simultaneous mobility have their convolution in
designing simultaneous movement where mobile nodes (MNs) travel randomly from
the two adjacent cells at the same time and also have their complexity in the
measurement of the occurrences of simultaneous handover. Simultaneous mobility
problem incurs when two of the MNs start handover approximately at the same
time. As Simultaneous mobility is different for the other mobility pattern,
generally occurs less number of times in real time; we analyze that a
simplified simultaneous mobility model can be considered by taking only
symmetric positions of MNs with random steps. In addition to that, we simulated
the model using mSCTP and compare the simulation results in different scenarios
with customized cell ranges. The analytical results shows that with the bigger
the cell sizes, simultaneous handover with random steps occurrences become lees
and for the sequential mobility (where initial positions of MNs is
predetermined) with random steps, simultaneous handover is more frequent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1910</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1910</id><created>2014-01-09</created><authors><author><keyname>Kowalczyk</keyname><forenames>Martin</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Katahira</keyname><forenames>Masafumi</forenames></author><author><keyname>Kaneko</keyname><forenames>Tatsuya</forenames></author><author><keyname>Miyamoto</keyname><forenames>Yuko</forenames></author><author><keyname>Koishi</keyname><forenames>Yumi</forenames></author></authors><title>Aligning Software-related Strategies in Multi-Organizational Settings</title><categories>cs.SE</categories><comments>14 pages. The final publication is available at
  http://www.shaker.de/de/content/catalogue/index.asp?
  lang=de&amp;ID=8&amp;ISBN=978-3-8322-9618-6</comments><journal-ref>Proceedings of the International Conference on Software Process
  and Product Measurement (IWSM/MetriKon/ Mensura 2010), pages 261-274,
  Stuttgart, Germany, November 10-12 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aligning the activities of an organization with its business goals is a
challenging task that is critical for success. Alignment in a
multi-organizational setting requires the integration of different internal or
external organizational units. The anticipated benefits of multi-organizational
alignment consist of clarified contributions and increased transparency of the
involved organizational units. The GQM+Strategies approach provides mechanisms
for explicitly linking goals and strategies within an organization and is based
on goal-oriented measurement. This paper presents the process and first-hand
experience of applying GQM+Strategies in a multi-organizational setting from
the aerospace industry. Additionally, the resulting GQM+Strategies grid is
sketched and selected parts are discussed. Finally, the results are reflected
on and an overview of future work is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1913</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1913</id><created>2014-01-09</created><authors><author><keyname>Trendowicz</keyname><forenames>Adam</forenames></author><author><keyname>Kl&#xe4;s</keyname><forenames>Michael</forenames></author><author><keyname>Lampasona</keyname><forenames>Constanza</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>K&#xf6;rner</keyname><forenames>Christian</forenames></author><author><keyname>Saft</keyname><forenames>Matthias</forenames></author></authors><title>Model-based Product Quality Evaluation with Multi-Criteria Decision
  Analysis</title><categories>cs.SE</categories><comments>18 pages. The final publication is available at
  http://www.shaker.de/de/content/catalogue/index.asp?
  lang=de&amp;ID=8&amp;ISBN=978-3-8322-9618-6</comments><journal-ref>Proceedings of the International Conference on Software Process
  and Product Measurement (IWSM/MetriKon/Mensura 2010), pages 3-20, Stuttgart,
  Germany, November 10-12 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to develop or evolve software or software-based systems/services
with defined and guaranteed quality in a predictable way is becoming
increasingly important. Essential - though not exclusive - prerequisites for
this are the ability to model the relevant quality properties appropriately and
the capability to perform reliable quality evaluations. Existing approaches for
integrated quality modeling and evaluation are typically either narrowly
focused or too generic and have proprietary ways for modeling and evaluating
quality. This article sketches an ap- proach for modeling and evaluating
quality properties in a uniform way, without losing the ability to build
sufficiently detailed customized models for specific quality properties. The
focus of this article is on the description of a multi-criteria aggregation
mechanism that can be used for the evaluation. In addition, the underlying
quality meta-model, an example application scenario, related work, initial
application results, and an outlook on future research are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1916</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1916</id><created>2014-01-09</created><authors><author><keyname>Xiong</keyname><forenames>Tao</forenames></author><author><keyname>Bao</keyname><forenames>Yukun</forenames></author><author><keyname>Hu</keyname><forenames>Zhongyi</forenames></author></authors><title>Multiple-output support vector regression with a firefly algorithm for
  interval-valued stock price index forecasting</title><categories>cs.CE cs.LG q-fin.ST</categories><comments>33 pages</comments><journal-ref>Knowledge-based Systems. 55, 2013:87-100</journal-ref><doi>10.1016/j.knosys.2013.10.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Highly accurate interval forecasting of a stock price index is fundamental to
successfully making a profit when making investment decisions, by providing a
range of values rather than a point estimate. In this study, we investigate the
possibility of forecasting an interval-valued stock price index series over
short and long horizons using multi-output support vector regression (MSVR).
Furthermore, this study proposes a firefly algorithm (FA)-based approach, built
on the established MSVR, for determining the parameters of MSVR (abbreviated as
FA-MSVR). Three globally traded broad market indices are used to compare the
performance of the proposed FA-MSVR method with selected counterparts. The
quantitative and comprehensive assessments are performed on the basis of
statistical criteria, economic criteria, and computational cost. In terms of
statistical criteria, we compare the out-of-sample forecasting using
goodness-of-forecast measures and testing approaches. In terms of economic
criteria, we assess the relative forecast performance with a simple trading
strategy. The results obtained in this study indicate that the proposed FA-MSVR
method is a promising alternative for forecasting interval-valued financial
time series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1918</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1918</id><created>2014-01-09</created><authors><author><keyname>Delgado</keyname><forenames>Jose Dario Luis</forenames></author><author><keyname>Santiago</keyname><forenames>Jesus Maximo Ramirez</forenames></author></authors><title>Key Performance Indicators for QOS Assessment in TETRA Networks</title><categories>cs.NI</categories><comments>18 pages, International Journal of Mobile Network Communications &amp;
  Telematics (IJMNCT) Vol.3, No.6, December 2013</comments><doi>10.5121/ijmnct.2013.3601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Key Performance Indicators (KPIs) are widely used by GSM and UMTS carriers
with the aim of evaluating the network performance and the Quality of Service
(QoS) delivered to users. TETRA networks are basically designed to provide
telecommunication services to Public Safety &amp; Security (PSS) organizations,
thus the compliance of the QoS levels required by these clients is usually
critical. Despite that, the use of KPIs to assess the network performance and
the QoS achieved in TETRA systems is not very common. This paper not only
states the need of monitoring and evaluating these parameters, but also
introduces a set of KPIs which is considered necessary and sufficient in order
to allow TETRA operators to be aware of whether provided services meet the QoS
requirements established by end users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1919</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1919</id><created>2014-01-09</created><authors><author><keyname>Huang</keyname><forenames>Silu</forenames></author><author><keyname>Cheng</keyname><forenames>James</forenames></author><author><keyname>Wu</keyname><forenames>Huanhuan</forenames></author></authors><title>Temporal Graph Traversals: Definitions, Algorithms, and Applications</title><categories>cs.DS cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A temporal graph is a graph in which connections between vertices are active
at specific times, and such temporal information leads to completely new
patterns and knowledge that are not present in a non-temporal graph. In this
paper, we study traversal problems in a temporal graph. Graph traversals, such
as DFS and BFS, are basic operations for processing and studying a graph. While
both DFS and BFS are well-known simple concepts, it is non-trivial to adopt the
same notions from a non-temporal graph to a temporal graph. We analyze the
difficulties of defining temporal graph traversals and propose new definitions
of DFS and BFS for a temporal graph. We investigate the properties of temporal
DFS and BFS, and propose efficient algorithms with optimal complexity. In
particular, we also study important applications of temporal DFS and BFS. We
verify the efficiency and importance of our graph traversal algorithms in real
world temporal graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1926</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1926</id><created>2014-01-09</created><authors><author><keyname>Bao</keyname><forenames>Yukun</forenames></author><author><keyname>Hu</keyname><forenames>Zhongyi</forenames></author><author><keyname>Xiong</keyname><forenames>Tao</forenames></author></authors><title>A PSO and Pattern Search based Memetic Algorithm for SVMs Parameters
  Optimization</title><categories>cs.LG cs.AI cs.NE stat.ML</categories><comments>27 pages. Neurocomputing, 2013</comments><doi>10.1016/j.neucom.2013.01.027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Addressing the issue of SVMs parameters optimization, this study proposes an
efficient memetic algorithm based on Particle Swarm Optimization algorithm
(PSO) and Pattern Search (PS). In the proposed memetic algorithm, PSO is
responsible for exploration of the search space and the detection of the
potential regions with optimum solutions, while pattern search (PS) is used to
produce an effective exploitation on the potential regions obtained by PSO.
Moreover, a novel probabilistic selection strategy is proposed to select the
appropriate individuals among the current population to undergo local
refinement, keeping a well balance between exploration and exploitation.
Experimental results confirm that the local refinement with PS and our proposed
selection strategy are effective, and finally demonstrate effectiveness and
robustness of the proposed PSO-PS based MA for SVMs parameters optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1942</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1942</id><created>2014-01-09</created><updated>2014-06-18</updated><authors><author><keyname>Sinha</keyname><forenames>Ankur</forenames></author><author><keyname>Malo</keyname><forenames>Pekka</forenames></author><author><keyname>Deb</keyname><forenames>Kalyanmoy</forenames></author></authors><title>Test Problem Construction for Single-Objective Bilevel Optimization</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a procedure for designing controlled test problems
for single-objective bilevel optimization. The construction procedure is
flexible and allows its user to control the different complexities that are to
be included in the test problems independently of each other. In addition to
properties that control the difficulty in convergence, the procedure also
allows the user to introduce difficulties caused by interaction of the two
levels. As a companion to the test problem construction framework, the paper
presents a standard test suite of twelve problems, which includes eight
unconstrained and four constrained problems. Most of the problems are scalable
in terms of variables and constraints. To provide baseline results, we have
solved the proposed test problems using a nested bilevel evolutionary
algorithm. The results can be used for comparison, while evaluating the
performance of any other bilevel optimization algorithm. The codes related to
the paper may be accessed from the website \url{http://bilevel.org}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1943</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1943</id><created>2014-01-09</created><updated>2014-12-03</updated><authors><author><keyname>Morsi</keyname><forenames>Rania</forenames></author><author><keyname>Michalopoulos</keyname><forenames>Diomidis S.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Multi-user Scheduling Schemes for Simultaneous Wireless Information and
  Power Transfer Over Fading Channels</title><categories>cs.IT math.IT</categories><comments>30 pages, 5 figures, 6 tables, accepted for publication in IEEE
  Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study downlink multi-user scheduling for a time-slotted
system with simultaneous wireless information and power transfer. In
particular, in each time slot, a single user is scheduled to receive
information, while the remaining users opportunistically harvest the ambient
radio frequency energy. We devise novel online scheduling schemes in which the
tradeoff between the users' ergodic rates and their average amount of harvested
energy can be controlled. In particular, we modify the well-known maximum
signal-to-noise ratio (SNR) and maximum normalized-SNR (N-SNR) schedulers by
scheduling the user whose SNR/N-SNR has a certain ascending order (selection
order) rather than the maximum one. We refer to these new schemes as
order-based SNR/N-SNR scheduling and show that the lower the selection order,
the higher the average amount of harvested energy in the system at the expense
of a reduced ergodic sum rate. The order-based N-SNR scheduling scheme provides
proportional fairness among the users in terms of both the ergodic achievable
rate and the average harvested energy. Furthermore, we propose an order-based
equal throughput (ET) fair scheduler, which schedules the user having the
minimum moving average throughput out of the users whose N-SNR orders fall into
a given set of allowed orders. We show that this scheme provides the users with
proportionally fair average harvested energies. In this context, we also derive
feasibility conditions for achieving ET with the order-based ET scheduler.
Using the theory of order statistics, the average per-user harvested energy and
ergodic achievable rate of all proposed scheduling schemes are analyzed and
obtained in closed form for independent and non-identically distributed
Rayleigh, Ricean, Nakagami-m, and Weibull fading channels. Our closed-form
analytical results are corroborated by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1944</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1944</id><created>2014-01-09</created><updated>2014-02-16</updated><authors><author><keyname>Stefanatos</keyname><forenames>Stelios</forenames></author><author><keyname>Alexiou</keyname><forenames>Angeliki</forenames></author></authors><title>Exploiting Frequency and Spatial Dimensions in Small Cell Wireless
  Networks</title><categories>cs.IT math.IT</categories><comments>IEEE WCNC '14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the efficiency of spatial and frequency dimensions in
serving multiple users in the downlink of a small cell wireless network with
randomly deployed access points. For this purpose, the stochastic geometry
framework is incorporated, taking into account the user distribution within
each cell and the effect of sharing the available system resources to multiple
users. An analysis of performance in terms of signal-to-interference-ratio and
achieved user rate is provided that holds under the class of non-cooperative
multiple access schemes. In order to obtain concrete results, two simple
instances of multiple access schemes are considered. It is shown that
performance depends critically on both the availability of frequency and/or
spatial dimensions as well as the way they are employed. In particular,
increasing the number of available frequency dimensions alone is beneficial for
users experiencing large interference, whereas increasing spatial dimensions
without employing frequency dimensions degrades performance. However, best
performance is achieved when both dimensions are combined in serving the users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1946</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1946</id><created>2014-01-09</created><authors><author><keyname>Arold</keyname><forenames>Oliver</forenames></author><author><keyname>Ettl</keyname><forenames>Svenja</forenames></author><author><keyname>Willomitzer</keyname><forenames>Florian</forenames></author><author><keyname>H&#xe4;usler</keyname><forenames>Gerd</forenames></author></authors><title>Hand-guided 3D surface acquisition by combining simple light sectioning
  with real-time algorithms</title><categories>physics.optics cs.CV</categories><comments>19 pages, 22 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Precise 3D measurements of rigid surfaces are desired in many fields of
application like quality control or surgery. Often, views from all around the
object have to be acquired for a full 3D description of the object surface. We
present a sensor principle called &quot;Flying Triangulation&quot; which avoids an
elaborate &quot;stop-and-go&quot; procedure. It combines a low-cost classical
light-section sensor with an algorithmic pipeline. A hand-guided sensor
captures a continuous movie of 3D views while being moved around the object.
The views are automatically aligned and the acquired 3D model is displayed in
real time. In contrast to most existing sensors no bandwidth is wasted for
spatial or temporal encoding of the projected lines. Nor is an expensive color
camera necessary for 3D acquisition. The achievable measurement uncertainty and
lateral resolution of the generated 3D data is merely limited by physics. An
alternating projection of vertical and horizontal lines guarantees the
existence of corresponding points in successive 3D views. This enables a
precise registration without surface interpolation. For registration, a variant
of the iterative closest point algorithm - adapted to the specific nature of
our 3D views - is introduced. Furthermore, data reduction and smoothing without
losing lateral resolution as well as the acquisition and mapping of a color
texture is presented. The precision and applicability of the sensor is
demonstrated by simulation and measurement results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1974</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1974</id><created>2014-01-09</created><updated>2014-01-28</updated><authors><author><keyname>Nguyen</keyname><forenames>Vu</forenames></author><author><keyname>Phung</keyname><forenames>Dinh</forenames></author><author><keyname>Nguyen</keyname><forenames>XuanLong</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author><author><keyname>Bui</keyname><forenames>Hung Hai</forenames></author></authors><title>Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts</title><categories>cs.LG stat.ML</categories><comments>Full version of ICML 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Bayesian nonparametric framework for multilevel clustering which
utilizes group-level context information to simultaneously discover
low-dimensional structures of the group contents and partitions groups into
clusters. Using the Dirichlet process as the building block, our model
constructs a product base-measure with a nested structure to accommodate
content and context observations at multiple levels. The proposed model
possesses properties that link the nested Dirichlet processes (nDP) and the
Dirichlet process mixture models (DPM) in an interesting way: integrating out
all contents results in the DPM over contexts, whereas integrating out
group-specific contexts results in the nDP mixture over content variables. We
provide a Polya-urn view of the model and an efficient collapsed Gibbs
inference procedure. Extensive experiments on real-world datasets demonstrate
the advantage of utilizing context information via our model in both text and
image domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1977</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1977</id><created>2014-01-09</created><authors><author><keyname>Addis</keyname><forenames>Bernardetta</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Carello</keyname><forenames>Giuliana</forenames><affiliation>DEIB</affiliation></author><author><keyname>Capone</keyname><forenames>Antonio</forenames><affiliation>DEIB</affiliation></author><author><keyname>Gianoli</keyname><forenames>Luca G.</forenames><affiliation>DEIB</affiliation></author><author><keyname>Sans&#xf2;</keyname><forenames>Brunilde</forenames></author></authors><title>Robust Energy Management for Green and Survivable IP Networks</title><categories>cs.NI cs.SY</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the growing necessity to make Internet greener, it is worth pointing
out that energy-aware strategies to minimize network energy consumption must
not undermine the normal network operation. In particular, two very important
issues that may limit the application of green networking techniques concern,
respectively, network survivability, i.e. the network capability to react to
device failures, and robustness to traffic variations. We propose novel
modelling techniques to minimize the daily energy consumption of IP networks,
while explicitly guaranteeing, in addition to typical QoS requirements, both
network survivability and robustness to traffic variations. The impact of such
limitations on final network consumption is exhaustively investigated. Daily
traffic variations are modelled by dividing a single day into multiple time
intervals (multi-period problem), and network consumption is reduced by putting
to sleep idle line cards and chassis. To preserve network resiliency we
consider two different protection schemes, i.e. dedicated and shared
protection, according to which a backup path is assigned to each demand and a
certain amount of spare capacity has to be available on each link. Robustness
to traffic variations is provided by means of a specific modelling framework
that allows to tune the conservatism degree of the solutions and to take into
account load variations of different magnitude. Furthermore, we impose some
inter-period constraints necessary to guarantee network stability and preserve
the device lifetime. Both exact and heuristic methods are proposed.
Experimentations carried out with realistic networks operated with flow-based
routing protocols (i.e. MPLS) show that significant savings, up to 30%, can be
achieved also when both survivability and robustness are fully guaranteed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1982</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1982</id><created>2014-01-09</created><authors><author><keyname>Maleh</keyname><forenames>Y.</forenames></author><author><keyname>Ezzati</keyname><forenames>A.</forenames></author></authors><title>A review of security attacks and Intrusion Detection Schemes in Wireless
  Sensor Networks</title><categories>cs.CR cs.NI</categories><comments>12 pages with 9 figures</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  5, No. 6, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) are currently used in different industrial
and consumer applications, such as earth monitoring, health related
applications, natural disaster prevention, and many other areas. Security is
one of the major aspects of Wireless sensor networks due to the resource
limitations of sensor nodes. However, these networks are facing several threats
that affect their functioning and their life. In this paper we present security
attacks in wireless sensor networks, and we focus on a review and analysis of
the recent Intrusion Detection schemes in WSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1990</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1990</id><created>2014-01-09</created><authors><author><keyname>Prates</keyname><forenames>R. F.</forenames></author><author><keyname>C&#xe1;mara-Ch&#xe1;vez</keyname><forenames>G.</forenames></author><author><keyname>Schwartz</keyname><forenames>William R.</forenames></author><author><keyname>Menotti</keyname><forenames>D.</forenames></author></authors><title>Brazilian License Plate Detection Using Histogram of Oriented Gradients
  and Sliding Windows</title><categories>cs.CV</categories><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 5, No 6, pp. 39-52, December 2013</journal-ref><doi>10.5121/ijcsit.2013.5603</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the increasingly need for automatic traffic monitoring, vehicle
license plate detection is of high interest to perform automatic toll
collection, traffic law enforcement, parking lot access control, among others.
In this paper, a sliding window approach based on Histogram of Oriented
Gradients (HOG) features is used for Brazilian license plate detection. This
approach consists in scanning the whole image in a multiscale fashion such that
the license plate is located precisely. The main contribution of this work
consists in a deep study of the best setup for HOG descriptors on the detection
of Brazilian license plates, in which HOG have never been applied before. We
also demonstrate the reliability of this method ensured by a recall higher than
98% (with a precision higher than 78%) in a publicly available data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.1996</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.1996</id><created>2014-01-09</created><updated>2014-04-24</updated><authors><author><keyname>Righi</keyname><forenames>Simone</forenames></author><author><keyname>Tak&#xe1;cs</keyname><forenames>K&#xe1;roly</forenames></author></authors><title>Emotional Strategies as Catalysts for Cooperation in Signed Networks</title><categories>physics.soc-ph cs.SI</categories><comments>24 pages, Accepted for publication in Advances in Complex Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evolution of unconditional cooperation is one of the fundamental problems
in science. A new solution is proposed to solve this puzzle. We treat this
issue with an evolutionary model in which agents play the Prisoner's Dilemma on
signed networks. The topology is allowed to co-evolve with relational signs as
well as with agent strategies. We introduce a strategy that is conditional on
the emotional content embedded in network signs. We show that this strategy
acts as a catalyst and creates favorable conditions for the spread of
unconditional cooperation. In line with the literature, we found evidence that
the evolution of cooperation most likely occurs in networks with relatively
high chances of rewiring and with low likelihood of strategy adoption. While a
low likelihood of rewiring enhances cooperation, a very high likelihood seems
to limit its diffusion. Furthermore, unlike in non-signed networks, cooperation
becomes more prevalent in denser topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2000</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2000</id><created>2014-01-09</created><authors><author><keyname>Dolfi</keyname><forenames>M.</forenames></author><author><keyname>Gukelberger</keyname><forenames>J.</forenames></author><author><keyname>Hehn</keyname><forenames>A.</forenames></author><author><keyname>Imri&#x161;ka</keyname><forenames>J.</forenames></author><author><keyname>Pakrouski</keyname><forenames>K.</forenames></author><author><keyname>R&#xf8;nnow</keyname><forenames>T. F.</forenames></author><author><keyname>Troyer</keyname><forenames>M.</forenames></author><author><keyname>Zintchenko</keyname><forenames>I.</forenames></author><author><keyname>Chirigati</keyname><forenames>F.</forenames></author><author><keyname>Freire</keyname><forenames>J.</forenames></author><author><keyname>Shasha</keyname><forenames>D.</forenames></author></authors><title>A model project for reproducible papers: critical temperature for the
  Ising model on a square lattice</title><categories>cs.CE cond-mat.stat-mech physics.comp-ph</categories><comments>Authors are listed in alphabetical order by institution and name. 5
  pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a simple, yet typical simulation in statistical
physics, consisting of large scale Monte Carlo simulations followed by an
involved statistical analysis of the results. The purpose is to provide an
example publication to explore tools for writing reproducible papers. The
simulation estimates the critical temperature where the Ising model on the
square lattice becomes magnetic to be Tc /J = 2.26934(6) using a finite size
scaling analysis of the crossing points of Binder cumulants. We provide a
virtual machine which can be used to reproduce all figures and results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2001</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2001</id><created>2014-01-09</created><authors><author><keyname>Mayer</keyname><forenames>R. V.</forenames></author></authors><title>A study of Monte-Carlo method in a teachers' training institute</title><categories>cs.CY</categories><comments>6 pages, 4 figures,on russian. New educational technologies in higher
  education institution, Yekaterinburg, 2013, pp. 1 - 7</comments><msc-class>68U20</msc-class><acm-class>I.6.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of Monte-Carlo method study at computer simulation lessons in a
Teachers' Training Institute is reviewed in the article. The suggested
technique envisages the simulation modelling of various stochastic processes.
They include transmission of information via a communication link, oscillation
of a pendulum in an air stream, deflection of alpha particles by Au atoms,
formation of a percolating cluster, etc. (New educational technologies in
higher education institution: the collection of reports of the tenth
international scientific and methodical conference. Yekaterinburg, UGTU UPI,
2013).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2008</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2008</id><created>2014-01-09</created><authors><author><keyname>Jeyabharathi</keyname><forenames>C.</forenames></author><author><keyname>Pethalakshmi</keyname><forenames>A.</forenames></author></authors><title>New Approaches with Chord in Efficient P2P Grid Resource Discovery</title><categories>cs.DC</categories><comments>14 pages,9 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grid computing is a type of distributed computing which allows sharing of
computer resources through Internet. It not only allows us to share files but
also most of the software and hardware resources. An efficient resource
discovery mechanism is the fundamental requirements for grid computing systems,
as it supports resource management and scheduling of applications. Among
various discovery mechanisms,Peer-to-Peer (P2P) technology witnessed rapid
development and the key component for this success is efficient lookup
applications of P2P. Chord is a P2P structural model widely used as a routing
protocol to find resources in grid environment. Plenty of ideas are implemented
by researchers to improve the lookup performance of chord protocol in Grid
environment. In this paper, we discuss the recent researches made on Chord
Structured P2P protocol and present our proposed methods in which we use the
address of Recently Visited Node (RVN) and fuzzy technique to easily locate the
grid resources by reducing message complexity and time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2010</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2010</id><created>2014-01-09</created><updated>2016-01-21</updated><authors><author><keyname>Giraudo</keyname><forenames>Samuele</forenames></author><author><keyname>Luque</keyname><forenames>Jean-Gabriel</forenames></author><author><keyname>Mignot</keyname><forenames>Ludovic</forenames></author><author><keyname>Nicart</keyname><forenames>Florent</forenames></author></authors><title>Operads, quasiorders, and regular languages</title><categories>cs.FL math.CO</categories><comments>32 pages</comments><msc-class>68Q70, 68Q45, 18D50</msc-class><journal-ref>Advances in Applied Mathematics, 75, 56--93, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the construction of multitildes in the aim to provide
multitilde operators for regular languages. We show that the underliying
algebraic structure involves the action of some operads. An operad is an
algebraic structure that mimics the composition of the functions. The involved
operads are described in terms of combinatorial objects. These operads are
obtained from more primitive objects, namely precompositions, whose algebraic
counter-parts are investigated. One of these operads acts faithfully on
languages in the sense that two different operators act in two different ways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2011</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2011</id><created>2014-01-09</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Kets</keyname><forenames>Willemien</forenames></author></authors><title>A logic for reasoning about ambiguity</title><categories>cs.AI cs.GT cs.LO</categories><comments>Some of the material in this paper appeared in preliminary form in
  &quot;Ambiguous langage and differences of belief&quot; (see arXiv:1203.0699)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard models of multi-agent modal logic do not capture the fact that
information is often \emph{ambiguous}, and may be interpreted in different ways
by different agents. We propose a framework that can model this, and consider
different semantics that capture different assumptions about the agents'
beliefs regarding whether or not there is ambiguity. We examine the expressive
power of logics of ambiguity compared to logics that cannot model ambiguity,
with respect to the different semantics that we propose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2018</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2018</id><created>2014-01-09</created><updated>2014-06-03</updated><authors><author><keyname>Kong</keyname><forenames>Shoubin</forenames></author><author><keyname>Mei</keyname><forenames>Qiaozhu</forenames></author><author><keyname>Feng</keyname><forenames>Ling</forenames></author><author><keyname>Zhao</keyname><forenames>Zhe</forenames></author><author><keyname>Ye</keyname><forenames>Fei</forenames></author></authors><title>On the Real-time Prediction Problems of Bursting Hashtags in Twitter</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hundreds of thousands of hashtags are generated every day on Twitter. Only a
few become bursting topics. Among the few, only some can be predicted in
real-time. In this paper, we take the initiative to conduct a systematic study
of a series of challenging real-time prediction problems of bursting hashtags.
Which hashtags will become bursting? If they do, when will the burst happen?
How long will they remain active? And how soon will they fade away? Based on
empirical analysis of real data from Twitter, we provide insightful statistics
to answer these questions, which span over the entire lifecycles of hashtags.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2038</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2038</id><created>2014-01-09</created><authors><author><keyname>Bamberger</keyname><forenames>Johanna</forenames></author><author><keyname>Ge&#xdf;ler</keyname><forenames>Anna-Lena</forenames></author><author><keyname>Heitzelmann</keyname><forenames>Peter</forenames></author><author><keyname>Korn</keyname><forenames>Sara</forenames></author><author><keyname>Kahlmeyer</keyname><forenames>Rene</forenames></author><author><keyname>Lu</keyname><forenames>Xue Hao</forenames></author><author><keyname>Sang</keyname><forenames>Qi Hao</forenames></author><author><keyname>Wang</keyname><forenames>Zhi Jie</forenames></author><author><keyname>Yuan</keyname><forenames>Guan Zong</forenames></author><author><keyname>Gau&#xdf;</keyname><forenames>Michael</forenames></author><author><keyname>Kretz</keyname><forenames>Tobias</forenames></author></authors><title>Crowd Research at School: Crossing Flows</title><categories>physics.soc-ph cs.MA</categories><comments>contribution to proceedings of Traffic and Granular Flow 2013 held in
  J\&quot;ulich, Germany</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has become widely known that when two flows of pedestrians cross stripes
emerge spontaneously by which the pedestrians of the two walking directions
manage to pass each other in an orderly manner. In this work, we report about
the results of an experiment on crossing flows which has been carried out at a
German school. These results include that previously reported high flow volumes
on the crossing area can be confirmed. The empirical results are furthermore
compared to the results of a simulation model which succesfully could be
calibrated to catch the specific properties of the population of participants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2039</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2039</id><created>2014-01-09</created><authors><author><keyname>Barthou</keyname><forenames>Denis</forenames></author><author><keyname>Brand-Foissac</keyname><forenames>Olivier</forenames></author><author><keyname>Dolbeau</keyname><forenames>Romain</forenames></author><author><keyname>Grosdidier</keyname><forenames>Gilbert</forenames></author><author><keyname>Eisenbeis</keyname><forenames>Christina</forenames></author><author><keyname>Kruse</keyname><forenames>Michael</forenames></author><author><keyname>Pene</keyname><forenames>Olivier</forenames></author><author><keyname>Petrov</keyname><forenames>Konstantin</forenames></author><author><keyname>Tadonki</keyname><forenames>Claude</forenames></author></authors><title>Automated Code Generation for Lattice Quantum Chromodynamics and beyond</title><categories>hep-lat cs.DC cs.PL</categories><report-no>LPT-Orsay-13-142, hal-00926513</report-no><doi>10.1088/1742-6596/510/1/012005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present here our ongoing work on a Domain Specific Language which aims to
simplify Monte-Carlo simulations and measurements in the domain of Lattice
Quantum Chromodynamics. The tool-chain, called Qiral, is used to produce
high-performance OpenMP C code from LaTeX sources. We discuss conceptual issues
and details of implementation and optimization. The comparison of the
performance of the generated code to the well-established simulation software
is also made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2051</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2051</id><created>2014-01-09</created><authors><author><keyname>Agunbiade</keyname><forenames>Olusanya Y.</forenames></author><author><keyname>Zuva</keyname><forenames>Tranos</forenames></author><author><keyname>Johnson</keyname><forenames>Awosejo O.</forenames></author><author><keyname>Zuva</keyname><forenames>Keneilwe</forenames></author></authors><title>Enhancement performance of road recognition system of autonomous robots
  in shadow scenario</title><categories>cs.RO cs.CV</categories><comments>Signal &amp; Image Processing : An International Journal (SIPIJ) Vol.4,
  No.6, December 2013</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Road region recognition is a main feature that is gaining increasing
attention from intellectuals because it helps autonomous vehicle to achieve a
successful navigation without accident. However, different techniques based on
camera sensor have been used by various researchers and outstanding results
have been achieved. Despite their success, environmental noise like shadow
leads to inaccurate recognition of road region which eventually leads to
accident for autonomous vehicle. In this research, we conducted an
investigation on shadow and its effects, optimized the road region recognition
system of autonomous vehicle by introducing an algorithm capable of detecting
and eliminating the effects of shadow. The experimental performance of our
system was tested and compared using the following schemes: Total Positive Rate
(TPR), False Negative Rate (FNR), Total Negative Rate (TNR), Error Rate (ERR)
and False Positive Rate (FPR). The performance result of the system improved on
road recognition in shadow scenario and this advancement has added tremendously
to successful navigation approaches for autonomous vehicle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2056</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2056</id><created>2014-01-09</created><authors><author><keyname>Ramaswamy</keyname><forenames>Vanaja</forenames></author><author><keyname>Sivarasu</keyname><forenames>Abinaya</forenames></author><author><keyname>Sridharan</keyname><forenames>Bharghavi</forenames></author><author><keyname>Venkatesh</keyname><forenames>Hamsalekha</forenames></author></authors><title>A Bi-Scheduler Algorithm for Frame Aggregation in IEEE 802.11n</title><categories>cs.NI</categories><comments>9 pages,6 figures, International Journal of Wireless &amp; Mobile
  Networks (IJWMN) Vol. 5, No. 6</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  5, No. 6, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IEEE 802.11n mainly aims to provide high throughput, reliability and good
security over its other previous standards. The performance of 802.11n is very
effective on the saturated traffic through the use of frame aggregation. But
this frame aggregation will not effectively function in all scenarios. The main
objective of this paper is to improve the throughput of the wireless LAN
through effective frame aggregation using scheduler mechanism. The Bi-Scheduler
algorithm proposed in this article aims to segregate frames based on their
access categories. The outer scheduler separates delay sensitive applications
from the incoming burst of multi-part data and also decides whether to apply
Aggregated - MAC Service Data Unit (A-MSDU) aggregation technique or to send
the data without any aggregation. The inner scheduler schedules the remaining
(delay-insensitive, background and best-effort) packets using Aggregated-MAC
Protocol Data unit (A-MPDU) aggregation technique
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2058</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2058</id><created>2014-01-09</created><authors><author><keyname>Puri</keyname><forenames>Rachit</forenames></author></authors><title>Gesture recognition based mouse events</title><categories>cs.CV</categories><comments>9 pages, IJCSIT</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents the maneuver of mouse pointer and performs various mouse
operations such as left click, right click, double click, drag etc using
gestures recognition technique. Recognizing gestures is a complex task which
involves many aspects such as motion modeling, motion analysis, pattern
recognition and machine learning. Keeping all the essential factors in mind a
system has been created which recognizes the movement of fingers and various
patterns formed by them. Color caps have been used for fingers to distinguish
it from the background color such as skin color. Thus recognizing the gestures
various mouse events have been performed. The application has been created on
MATLAB environment with operating system as windows 7.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2065</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2065</id><created>2014-01-09</created><updated>2014-06-29</updated><authors><author><keyname>Hermelin</keyname><forenames>Danny</forenames></author><author><keyname>Landau</keyname><forenames>Gad M.</forenames></author><author><keyname>Rabinovich</keyname><forenames>Yuri</forenames></author><author><keyname>Weimann</keyname><forenames>Oren</forenames></author></authors><title>Binary Jumbled Pattern Matching via All-Pairs Shortest Paths</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In binary jumbled pattern matching we wish to preprocess a binary string $S$
in order to answer queries $(i,j)$ which ask for a substring of $S$ that is of
size $i$ and has exactly $j$ 1-bits. The problem naturally generalizes to
node-labeled trees and graphs by replacing &quot;substring&quot; with &quot;connected
subgraph&quot;.
  In this paper, we give an ${n^2}/{2^{\Omega(\log n/\log \log n)^{1/2}}}$ time
solution for both strings and trees. This odd-looking time complexity improves
the state of the art $O(n^2/\log^2 n)$ solutions by more than any
poly-logarithmic factor. It originates from the recent seminal algorithm of
Williams for min-plus matrix multiplication. We obtain the result by giving a
black box reduction from trees to strings. This is then combined with a
reduction from strings to min-plus matrix multiplications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2071</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2071</id><created>2014-01-09</created><authors><author><keyname>Hougardy</keyname><forenames>Stefan</forenames></author><author><keyname>Wilde</keyname><forenames>Mirko</forenames></author></authors><title>On the Nearest Neighbor Rule for the Metric Traveling Salesman Problem</title><categories>cs.DS cs.DM math.CO</categories><msc-class>90C27 90C59 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a very simple family of traveling salesman instances with $n$
cities where the nearest neighbor rule may produce a tour that is $\Theta(\log
n)$ times longer than an optimum solution. Our family works for the graphic,
the euclidean, and the rectilinear traveling salesman problem at the same time.
It improves the so far best known lower bound in the euclidean case and proves
for the first time a lower bound in the rectilinear case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2086</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2086</id><created>2014-01-08</created><updated>2015-07-02</updated><authors><author><keyname>Prasad</keyname><forenames>H. L</forenames></author><author><keyname>Prashanth</keyname><forenames>L. A.</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>Actor-Critic Algorithms for Learning Nash Equilibria in N-player
  General-Sum Games</title><categories>cs.GT cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding stationary Nash equilibria (NE) in a
finite discounted general-sum stochastic game. We first generalize a non-linear
optimization problem from Filar and Vrieze [2004] to a $N$-player setting and
break down this problem into simpler sub-problems that ensure there is no
Bellman error for a given state and an agent. We then provide a
characterization of solution points of these sub-problems that correspond to
Nash equilibria of the underlying game and for this purpose, we derive a set of
necessary and sufficient SG-SP (Stochastic Game - Sub-Problem) conditions.
Using these conditions, we develop two actor-critic algorithms: OFF-SGSP
(model-based) and ON-SGSP (model-free). Both algorithms use a critic that
estimates the value function for a fixed policy and an actor that performs
descent in the policy space using a descent direction that avoids local minima.
We establish that both algorithms converge, in self-play, to the equilibria of
a certain ordinary differential equation (ODE), whose stable limit points
coincide with stationary NE of the underlying general-sum stochastic game. On a
single state non-generic game (see Hart and Mas-Colell [2005]) as well as on a
synthetic two-player game setup with $810,000$ states, we establish that
ON-SGSP consistently outperforms NashQ ([Hu and Wellman, 2003] and FFQ
[Littman, 2001] algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2101</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2101</id><created>2014-01-09</created><authors><author><keyname>Carro</keyname><forenames>Massimo</forenames></author></authors><title>NoSQL Databases</title><categories>cs.DB</categories><comments>57 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this document, I present the main notions of NoSQL databases and compare
four selected products (Riak, MongoDB, Cassandra, Neo4J) according to their
capabilities with respect to consistency, availability, and partition
tolerance, as well as performance. I also propose a few criteria for selecting
the right tool for the right situation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2107</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2107</id><created>2014-01-09</created><authors><author><keyname>Vidakovic</keyname><forenames>Dragan</forenames></author><author><keyname>Parezanovic</keyname><forenames>Dusko</forenames></author><author><keyname>Vucetic</keyname><forenames>Zoran</forenames></author></authors><title>Minimizing the Time of Detection of Large (Probably) Prime Numbers</title><categories>cs.CR</categories><comments>11 pages</comments><acm-class>D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the experimental results that more clearly than any
theory suggest an answer to the question: when in detection of large (probably)
prime numbers to apply, a very resource demanding, Miller-Rabin algorithm. Or,
to put it another way, when the dividing by first several tens of prime numbers
should be replaced by primality testing? As an innovation, the procedure above
will be supplemented by considering the use of the well-known Goldbach's
conjecture in the solving of this and some other important questions about the
RSA cryptosystem, always guided by the motto &quot;do not harm&quot; - neither the
security nor the time spent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2113</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2113</id><created>2014-01-09</created><authors><author><keyname>Negi</keyname><forenames>Rohit</forenames></author><author><keyname>Prabhu</keyname><forenames>Vinay Uday</forenames></author><author><keyname>Rodrigues</keyname><forenames>Miguel</forenames></author></authors><title>Latent Sentiment Detection in Online Social Networks: A
  Communications-oriented View</title><categories>cs.SI</categories><comments>13 pages, 6 figures, Submitted to ICC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of latent sentiment detection in
Online Social Networks such as Twitter. We demonstrate the benefits of using
the underlying social network as an Ising prior to perform network aided
sentiment detection. We show that the use of the underlying network results in
substantially lower detection error rates compared to strictly features-based
detection. In doing so, we introduce a novel communications-oriented framework
for characterizing the probability of error, based on information-theoretic
analysis. We study the variation of the calculated error exponent for several
stylized network topologies such as the complete network, the star network and
the closed-chain network, and show the importance of the network structure in
determining detection performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2118</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2118</id><created>2014-01-09</created><authors><author><keyname>Frolov</keyname><forenames>Alexey</forenames></author><author><keyname>Rybin</keyname><forenames>Pavel</forenames></author><author><keyname>Zyablov</keyname><forenames>Victor</forenames></author></authors><title>On the Capacity of the Multiuser Vector Adder Channel</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, submitted to IEEE ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the capacity of the $Q$-frequency $S$-user vector adder
channel (channel with intensity information) introduced by Chang and Wolf. Both
coordinated and uncoordinated types of transmission are considered. Asymptotic
(under the conditions $Q \to \infty$, $S = \gamma Q$ and $0 &lt; \gamma &lt; \infty$)
upper and lower bounds on the relative (per subchannel) capacity are derived.
The lower bound for the coordinated case is shown to increase when $\gamma$
grows. At the same time the relative capacity for the uncoordinated case is
upper bounded by a constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2119</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2119</id><created>2014-01-09</created><updated>2014-06-09</updated><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author></authors><title>Maximum Throughput for a Cognitive Radio Multi-Antenna User with
  Multiple Primary Users</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a cognitive radio scenario involving a single cognitive
transmitter equipped with $\mathcal{K}$ antennas sharing the spectrum with
$\mathcal{M}$ primary users (PUs) transmitting over orthogonal bands. Each
terminal has a queue to store its incoming traffic. We propose a novel protocol
where the cognitive user transmits its packet over a channel formed by the
aggregate of the inactive primary bands. We study the impact of the number of
PUs, sensing errors, and the number of antennas on the maximum secondary stable
throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2120</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2120</id><created>2014-01-09</created><authors><author><keyname>Frolov</keyname><forenames>Alexey</forenames></author><author><keyname>Rybin</keyname><forenames>Pavel</forenames></author></authors><title>Upper Bounds on the Minimum Distance of Quasi-Cyclic LDPC codes
  Revisited</title><categories>cs.IT math.IT</categories><comments>4 pages, 1 figure, submitted to IEEE ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two upper bounds on the minimum distance of type-1 quasi-cyclic low-density
parity-check (QC LDPC) codes are derived. The necessary condition is given for
the minimum code distance of such codes to grow linearly with the code length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2121</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2121</id><created>2014-01-09</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Carlos Pedro</forenames></author></authors><title>Emotional Responses in Artificial Agent-Based Systems: Reflexivity and
  Adaptation in Artificial Life</title><categories>cs.AI nlin.AO</categories><msc-class>68T01, 68T05, 68T42</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current work addresses a virtual environment with self-replicating agents
whose decisions are based on a form of &quot;somatic computation&quot; (soma - body) in
which basic emotional responses, taken in parallelism to actual living
organisms, are introduced as a way to provide the agents with greater reflexive
abilities. The work provides a contribution to the field of Artificial
Intelligence (AI) and Artificial Life (ALife) in connection to a
neurobiology-based cognitive framework for artificial systems and virtual
environments' simulations. The performance of the agents capable of emotional
responses is compared with that of self-replicating automata, and the
implications of research on emotions and AI, in connection to both virtual
agents as well as robots, is addressed regarding possible future directions and
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2125</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2125</id><created>2014-01-09</created><updated>2015-01-29</updated><authors><author><keyname>Tranquilli</keyname><forenames>Paul</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>Exponential-Krylov methods for ordinary differential equations</title><categories>cs.NA math.NA</categories><report-no>Computational Science Laboratory CSLTR-01/2014</report-no><journal-ref>Journal of Computational Physics. Volume 278, Pages 31 -- 46, 2014</journal-ref><doi>10.1016/j.jcp.2014.08.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a new class of exponential-type integrators where all the
matrix exponentiations are performed in a single Krylov space of low dimension.
The new family, called Lightly Implicit Krylov-Exponential (LIKE), is well
suited for solving large scale systems of ODEs or semi-discrete PDEs. The time
discretization and the Krylov space approximation are treated as a single
computational process, and the Krylov space properties are an integral part of
the new LIKE order condition theory developed herein. Consequently, LIKE
methods require a small number of basis vectors determined solely by the
temporal order of accuracy. The subspace size is independent of the ODE under
consideration, and there is no need to monitor the errors in linear system
solutions at each stage. Numerical results illustrate the favorable properties
of new family of methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2127</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2127</id><created>2014-01-09</created><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>van Renssen</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Upper Bounds on the Spanning Ratio of Constrained Theta-Graphs</title><categories>cs.CG</categories><comments>Full version of a paper accepted to the 11th Latin American Symposium
  on Theoretical Informatics (LATIN 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present tight upper and lower bounds on the spanning ratio of a large
family of constrained $\theta$-graphs. We show that constrained $\theta$-graphs
with $4k + 2$ ($k \geq 1$ and integer) cones have a tight spanning ratio of $1
+ 2 \sin(\theta/2)$, where $\theta$ is $2 \pi / (4k + 2)$. We also present
improved upper bounds on the spanning ratio of the other families of
constrained $\theta$-graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2134</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2134</id><created>2014-01-09</created><authors><author><keyname>Goodman</keyname><forenames>Alyssa</forenames></author><author><keyname>Pepe</keyname><forenames>Alberto</forenames></author><author><keyname>Blocker</keyname><forenames>Alexander W.</forenames></author><author><keyname>Borgman</keyname><forenames>Christine L.</forenames></author><author><keyname>Cranmer</keyname><forenames>Kyle</forenames></author><author><keyname>Crosas</keyname><forenames>Merc&#xe8;</forenames></author><author><keyname>Di Stefano</keyname><forenames>Rosanne</forenames></author><author><keyname>Gil</keyname><forenames>Yolanda</forenames></author><author><keyname>Groth</keyname><forenames>Paul</forenames></author><author><keyname>Hedstrom</keyname><forenames>Margaret</forenames></author><author><keyname>Hogg</keyname><forenames>David W.</forenames></author><author><keyname>Kashyap</keyname><forenames>Vinay</forenames></author><author><keyname>Mahabal</keyname><forenames>Ashish</forenames></author><author><keyname>Siemiginowska</keyname><forenames>Aneta</forenames></author><author><keyname>Slavkovic</keyname><forenames>Aleksandra</forenames></author></authors><title>10 Simple Rules for the Care and Feeding of Scientific Data</title><categories>cs.DL astro-ph.IM cs.CY</categories><comments>Accepted in PLOS Computational Biology. This paper was written
  collaboratively, on the web, in the open, using Authorea. The living version
  of this article, which includes sources and history, is available at
  http://www.authorea.com/3410/</comments><doi>10.1371/journal.pcbi.1003542</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article offers a short guide to the steps scientists can take to ensure
that their data and associated analyses continue to be of value and to be
recognized. In just the past few years, hundreds of scholarly papers and
reports have been written on questions of data sharing, data provenance,
research reproducibility, licensing, attribution, privacy, and more, but our
goal here is not to review that literature. Instead, we present a short guide
intended for researchers who want to know why it is important to &quot;care for and
feed&quot; data, with some practical advice on how to do that.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2139</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2139</id><created>2014-01-09</created><authors><author><keyname>Ravetti</keyname><forenames>Mart&#xed;n G&#xf3;mez</forenames></author><author><keyname>Carpi</keyname><forenames>Laura C.</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Bruna Amin</forenames></author><author><keyname>Frery</keyname><forenames>Alejandro C.</forenames></author><author><keyname>Rosso</keyname><forenames>Osvaldo A.</forenames></author></authors><title>Distinguishing noise from chaos: objective versus subjective criteria
  using Horizontal Visibility Graph</title><categories>stat.ML cs.IT math.IT nlin.CD</categories><comments>Submitted to PLOS One</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recently proposed methodology called the Horizontal Visibility Graph (HVG)
[Luque {\it et al.}, Phys. Rev. E., 80, 046103 (2009)] that constitutes a
geometrical simplification of the well known Visibility Graph algorithm [Lacasa
{\it et al.\/}, Proc. Natl. Sci. U.S.A. 105, 4972 (2008)], has been used to
study the distinction between deterministic and stochastic components in time
series [L. Lacasa and R. Toral, Phys. Rev. E., 82, 036120 (2010)].
Specifically, the authors propose that the node degree distribution of these
processes follows an exponential functional of the form $P(\kappa)\sim
\exp(-\lambda~\kappa)$, in which $\kappa$ is the node degree and $\lambda$ is a
positive parameter able to distinguish between deterministic (chaotic) and
stochastic (uncorrelated and correlated) dynamics. In this work, we investigate
the characteristics of the node degree distributions constructed by using HVG,
for time series corresponding to $28$ chaotic maps and $3$ different stochastic
processes. We thoroughly study the methodology proposed by Lacasa and Toral
finding several cases for which their hypothesis is not valid. We propose a
methodology that uses the HVG together with Information Theory quantifiers. An
extensive and careful analysis of the node degree distributions obtained by
applying HVG allow us to conclude that the Fisher-Shannon information plane is
a remarkable tool able to graphically represent the different nature,
deterministic or stochastic, of the systems under study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2153</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2153</id><created>2014-01-09</created><updated>2014-01-22</updated><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author><author><keyname>Vijayalakshmi</keyname><forenames>V. J.</forenames></author><author><keyname>Jeyakumar</keyname><forenames>P.</forenames></author></authors><title>Ontology - Based Dynamic Business Process Customization</title><categories>cs.AI</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interaction between business models is used in consumer centric manner
instead of using a producer centric approach for customizing the business
process in cloud environment. The knowledge based human semantic web is used
for customizing the business process It introduces the Human Semantic Web as a
conceptual interface, providing human-understandable semantics on top of the
ordinary Semantic Web, which provides machine-readable semantics based on RDF
in this mismatching is a major problem. To overcome this following technique
automatic customization detection is an automated process of detecting possible
elements or variables of a business process that needto be especially treated
in order to suit the requirement of the other process. To the business
processto be customized as the primary business process and those that it
collaborates with as secondary business process or SBP Automatic customization
enactment is an automated process of taking actions to perform the
customization on the PBP according to the detected customization spots and the
automatic reasoning on the customization conceptualization knowledge framework.
The process of customizing businessprocesses by composite the web pages by
using web service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2165</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2165</id><created>2014-01-09</created><authors><author><keyname>Roos</keyname><forenames>Stefanie</forenames></author><author><keyname>Strufe</keyname><forenames>Thorsten</forenames></author></authors><title>NextBestOnce: Achieving Polylog Routing despite Non-greedy Embeddings</title><categories>cs.DS cs.SI</categories><comments>23 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social Overlays suffer from high message delivery delays due to insufficient
routing strategies. Limiting connections to device pairs that are owned by
individuals with a mutual trust relationship in real life, they form topologies
restricted to a subgraph of the social network of their users. While
centralized, highly successful social networking services entail a complete
privacy loss of their users, Social Overlays at higher performance represent an
ideal private and censorship-resistant communication substrate for the same
purpose.
  Routing in such restricted topologies is facilitated by embedding the social
graph into a metric space. Decentralized routing algorithms have up to date
mainly been analyzed under the assumption of a perfect lattice structure.
However, currently deployed embedding algorithms for privacy-preserving Social
Overlays cannot achieve a sufficiently accurate embedding and hence
conventional routing algorithms fail. Developing Social Overlays with
acceptable performance hence requires better models and enhanced algorithms,
which guarantee convergence in the presence of local optima with regard to the
distance to the target.
  We suggest a model for Social Overlays that includes inaccurate embeddings
and arbitrary degree distributions. We further propose NextBestOnce, a routing
algorithm that can achieve polylog routing length despite local optima. We
provide analytical bounds on the performance of NextBestOnce assuming a
scale-free degree distribution, and furthermore show that its performance can
be improved by more than a constant factor when including Neighbor-of-Neighbor
information in the routing decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2169</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2169</id><created>2014-01-09</created><authors><author><keyname>Karzand</keyname><forenames>Mina</forenames></author><author><keyname>Zheng</keyname><forenames>Lizhong</forenames></author></authors><title>Achievability of Nonlinear Degrees of Freedom in Correlatively Changing
  Fading Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach toward the noncoherent communications over the time varying
fading channels is presented. In this approach, the relationship between the
input signal space and the output signal space of a correlatively changing
fading channel is shown to be a nonlinear mapping between manifolds of
different dimensions. Studying this mapping, it is shown that using nonlinear
decoding algorithms for single input-multiple output (SIMO) and multiple input
multiple output (MIMO) systems, extra numbers of degrees of freedom (DOF) are
available. We call them the nonlinear degrees of freedom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2175</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2175</id><created>2014-01-09</created><authors><author><keyname>Cormode</keyname><forenames>Graham</forenames></author><author><keyname>Jowhari</keyname><forenames>Hossein</forenames></author></authors><title>A Second Look at Counting Triangles in Graph Streams</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present improved results on the problem of counting
triangles in edge streamed graphs. For graphs with $m$ edges and at least $T$
triangles, we show that an extra look over the stream yields a two-pass
streaming algorithm that uses $O(\frac{m}{\epsilon^{4.5}\sqrt{T}})$ space and
outputs a $(1+\epsilon)$ approximation of the number of triangles in the graph.
This improves upon the two-pass streaming tester of Braverman, Ostrovsky and
Vilenchik, ICALP 2013, which distinguishes between triangle-free graphs and
graphs with at least $T$ triangle using $O(\frac{m}{T^{1/3}})$ space. Also, in
terms of dependence on $T$, we show that more passes would not lead to a better
space bound. In other words, we prove there is no constant pass streaming
algorithm that distinguishes between triangle-free graphs from graphs with at
least $T$ triangles using $O(\frac{m}{T^{1/2+\rho}})$ space for any constant
$\rho \ge 0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2181</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2181</id><created>2014-01-04</created><authors><author><keyname>Gao</keyname><forenames>Cai</forenames></author><author><keyname>Yan</keyname><forenames>Chao</forenames></author><author><keyname>Wei</keyname><forenames>Daijun</forenames></author><author><keyname>Hu</keyname><forenames>Yong</forenames></author><author><keyname>Mahadevan</keyname><forenames>Sankaran</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>A biologically inspired model for transshipment problem</title><categories>cs.SY math.OC</categories><comments>4 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transshipment problem is one of the basic operational research problems. In
this paper, our first work is to develop a biologically inspired mathematical
model for a dynamical system, which is first used to solve minimum cost flow
problem. It has lower computational complexity than Physarum Solver. Second, we
apply the proposed model to solve the traditional transshipment problem.
Compared with the conditional methods, experiment results show the provided
model is simple, effective as well as handling problem in a continuous manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2184</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2184</id><created>2014-01-08</created><authors><author><keyname>Moalic</keyname><forenames>Laurent</forenames><affiliation>IRTES - SET</affiliation></author><author><keyname>Gondran</keyname><forenames>Alexandre</forenames><affiliation>MAIAA</affiliation></author></authors><title>Variations on Memetic Algorithms for Graph Coloring Problems</title><categories>cs.AI cs.NE math.OC</categories><comments>24 pages, 7 figures</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph vertices coloring with a given number of colors is a famous and
much-studied NP-complete problem. The best methods to solve this problem are
hybrid algorithms such as memetic algorithms [Galinier99, Lu10, Wu12] or
quantum annealing [Titiloye11a, Titiloye11b, Titiloye12]. Those hybrid
algorithms use a powerful local search inside a population-based algorithm. The
balance between intensification and diversification is essential for those
metaheuristics but difficult to archieve. Customizing metaheuristics takes long
time and is one of the main weak points of these approaches. This paper studies
the impact of the increase and the decrease of diversification in one of the
most effective algorithms known: the Hybrid Evolutionary Algorithm (HEA) from
Galinier and Hao [Galinier99]. We then propose a modification of this memetic
algorithm in order to work with a population of only two individuals. This new
algorithm more effectively manages the correct 'dose' of diversification to add
into the included local search - TabuCol [Hertz87] in the case of the HEA. It
has produced several good results for the well-known DIMACS benchmark graphs,
such as 47-colorings for DSJC500.5, 82-colorings for DSJC1000.5, 222-colorings
for DSJC1000.9 and 81-colorings for flat1000\_76\_0, which have so far only
been produced by quantum annealing [Titiloye12] in 2012 with massive
multi-CPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2185</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2185</id><created>2014-01-09</created><authors><author><keyname>Xiao</keyname><forenames>Yuanzhang</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Foresighted Demand Side Management</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a smart grid with an independent system operator (ISO), and
distributed aggregators who have energy storage and purchase energy from the
ISO to serve its customers. All the entities in the system are foresighted:
each aggregator seeks to minimize its own long-term payments for energy
purchase and operational costs of energy storage by deciding how much energy to
buy from the ISO, and the ISO seeks to minimize the long-term total cost of the
system (e.g. energy generation costs and the aggregators' costs) by dispatching
the energy production among the generators. The decision making of the entities
is complicated for two reasons. First, the information is decentralized: the
ISO does not know the aggregators' states (i.e. their energy consumption
requests from customers and the amount of energy in their storage), and each
aggregator does not know the other aggregators' states or the ISO's state (i.e.
the energy generation costs and the status of the transmission lines). Second,
the coupling among the aggregators is unknown to them. Specifically, each
aggregator's energy purchase affects the price, and hence the payments of the
other aggregators. However, none of them knows how its decision influences the
price because the price is determined by the ISO based on its state. We propose
a design framework in which the ISO provides each aggregator with a conjectured
future price, and each aggregator distributively minimizes its own long-term
cost based on its conjectured price as well as its local information. The
proposed framework can achieve the social optimum despite being decentralized
and involving complex coupling among the various entities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2187</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2187</id><created>2014-01-09</created><authors><author><keyname>Long</keyname><forenames>James T.</forenames></author><author><keyname>Stanley</keyname><forenames>Lee J.</forenames></author></authors><title>A Busy Beaver Problem for Infinite-Time Turing Machines</title><categories>math.LO cs.CC cs.LO</categories><msc-class>03D99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note introduces a generalization to the setting of infinite-time
computation of the busy beaver problem from classical computability theory, and
proves some results concerning the growth rate of an associated function. In
our view, these results indicate that the generalization is both natural and
promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2195</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2195</id><created>2014-01-09</created><authors><author><keyname>Chang</keyname><forenames>Ching-Lueh</forenames></author></authors><title>A lower bound for metric 1-median selection</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of finding a point in an n-point metric space with the
minimum average distance to all points. We show that this problem has no
deterministic $o(n^2)$-query $(4-\Omega(1))$-approximation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2198</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2198</id><created>2014-01-09</created><authors><author><keyname>Paya</keyname><forenames>Ashkan</forenames></author><author><keyname>Marinescu</keyname><forenames>Dan C.</forenames></author></authors><title>Energy-aware Load Balancing Policies for the Cloud Ecosystem</title><categories>cs.DC</categories><comments>10 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The energy consumption of computer and communication systems does not scale
linearly with the workload. A system uses a significant amount of energy even
when idle or lightly loaded. A widely reported solution to resource management
in large data centers is to concentrate the load on a subset of servers and,
whenever possible, switch the rest of the servers to one of the possible sleep
states. We propose a reformulation of the traditional concept of load balancing
aiming to optimize the energy consumption of a large-scale system: {\it
distribute the workload evenly to the smallest set of servers operating at an
optimal energy level, while observing QoS constraints, such as the response
time.} Our model applies to clustered systems; the model also requires that the
demand for system resources to increase at a bounded rate in each reallocation
interval. In this paper we report the VM migration costs for application
scaling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2200</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2200</id><created>2014-01-09</created><authors><author><keyname>Grammatico</keyname><forenames>Sergio</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaojing</forenames></author><author><keyname>Margellos</keyname><forenames>Kostas</forenames></author><author><keyname>Goulart</keyname><forenames>Paul</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>A scenario approach for non-convex control design</title><categories>cs.SY</categories><comments>Submitted to IEEE Transactions on Automatic Control</comments><doi>10.1109/TAC.2015.2433591</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Randomized optimization is an established tool for control design with
modulated robustness. While for uncertain convex programs there exist
randomized approaches with efficient sampling, this is not the case for
non-convex problems. Approaches based on statistical learning theory are
applicable to non-convex problems, but they usually are conservative in terms
of performance and require high sample complexity to achieve the desired
probabilistic guarantees. In this paper, we derive a novel scenario approach
for a wide class of random non-convex programs, with a sample complexity
similar to that of uncertain convex programs and with probabilistic guarantees
that hold not only for the optimal solution of the scenario program, but for
all feasible solutions inside a set of a-priori chosen complexity. We also
address measure-theoretic issues for uncertain convex and non-convex programs.
Among the family of non-convex control- design problems that can be addressed
via randomization, we apply our scenario approach to randomized Model
Predictive Control for chance-constrained nonlinear control-affine systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2205</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2205</id><created>2014-01-09</created><updated>2015-02-07</updated><authors><author><keyname>Berthet</keyname><forenames>Quentin</forenames></author></authors><title>Optimal Testing for Planted Satisfiability Problems</title><categories>math.ST cs.CC math.PR stat.TH</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We study the problem of detecting planted solutions in a random
satisfiability formula. Adopting the formalism of hypothesis testing in
statistical analysis, we describe the minimax optimal rates of detection. Our
analysis relies on the study of the number of satisfying assignments, for which
we prove new results. We also address algorithmic issues, and give a
computationally efficient test with optimal statistical performance. This
result is compared to an average-case hypothesis on the hardness of refuting
satisfiability of random formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2209</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2209</id><created>2014-01-09</created><authors><author><keyname>Huang</keyname><forenames>Te-Yuan</forenames></author><author><keyname>Johari</keyname><forenames>Ramesh</forenames></author><author><keyname>McKeown</keyname><forenames>Nick</forenames></author><author><keyname>Trunnell</keyname><forenames>Matthew</forenames></author><author><keyname>Watson</keyname><forenames>Mark</forenames></author></authors><title>Using the Buffer to Avoid Rebuffers: Evidence from a Large Video
  Streaming Service</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To provide a better streaming experience, video clients today select their
video rates by observing and estimating the available capacity. Recent work has
shown that capacity estimation is fraught with difficulties because of complex
interactions between the ABR control loop, HTTP server performance and TCP
congestion control. Estimation-based rate selection algorithms can lead to
unnecessary rebuffering events and suboptimal video quality. This paper argues
that we should do away with estimating network capacity, and instead directly
observe and control the playback buffer--which is the state variable we are
most interested in controlling. We present a class of &quot;buffer-based&quot; rate
selection algorithms that reduce the rebuffering rate while allowing us to
control the delivered video quality. We implemented our algorithms inside the
Netflix video client and ran a series of experiments spanning millions of
Netflix users around the world. Our results show that by doing away with
estimating network capacity and instead focusing on buffer occupancy, we can
reduce rebuffer rates by 20% while holding video rate constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2220</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2220</id><created>2014-01-09</created><authors><author><keyname>Kaddoum</keyname><forenames>Georges</forenames></author><author><keyname>Giard</keyname><forenames>Pascal</forenames></author></authors><title>Analog Network Coding for Multi-User Spread-Spectrum Communication
  Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures, to appear at IEEE WCNC'14</comments><doi>10.1109/WCNC.2014.6952033</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents another look at an analog network coding scheme for
multi-user spread-spectrum communication systems. Our proposed system combines
coding and cooperation between a relay and users to boost the throughput and to
exploit interference. To this end, each pair of users, $\mathcal{A}$ and
$\mathcal{B}$, that communicate with each other via a relay $\mathcal{R}$
shares the same spreading code. The relay has two roles, it synchronizes
network transmissions and it broadcasts the combined signals received from
users. From user $\mathcal{B}$'s point of view, the signal is decoded, and
then, the data transmitted by user $\mathcal{A}$ is recovered by subtracting
user $\mathcal{B}$'s own data. We derive the analytical performance of this
system for an additive white Gaussian noise channel with the presence of
multi-user interference, and we confirm its accuracy by simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2224</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2224</id><created>2014-01-09</created><authors><author><keyname>Goudarzi</keyname><forenames>Alireza</forenames></author><author><keyname>Banda</keyname><forenames>Peter</forenames></author><author><keyname>Lakin</keyname><forenames>Matthew R.</forenames></author><author><keyname>Teuscher</keyname><forenames>Christof</forenames></author><author><keyname>Stefanovic</keyname><forenames>Darko</forenames></author></authors><title>A Comparative Study of Reservoir Computing for Temporal Signal
  Processing</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reservoir computing (RC) is a novel approach to time series prediction using
recurrent neural networks. In RC, an input signal perturbs the intrinsic
dynamics of a medium called a reservoir. A readout layer is then trained to
reconstruct a target output from the reservoir's state. The multitude of RC
architectures and evaluation metrics poses a challenge to both practitioners
and theorists who study the task-solving performance and computational power of
RC. In addition, in contrast to traditional computation models, the reservoir
is a dynamical system in which computation and memory are inseparable, and
therefore hard to analyze. Here, we compare echo state networks (ESN), a
popular RC architecture, with tapped-delay lines (DL) and nonlinear
autoregressive exogenous (NARX) networks, which we use to model systems with
limited computation and limited memory respectively. We compare the performance
of the three systems while computing three common benchmark time series:
H{\'e}non Map, NARMA10, and NARMA20. We find that the role of the reservoir in
the reservoir computing paradigm goes beyond providing a memory of the past
inputs. The DL and the NARX network have higher memorization capability, but
fall short of the generalization power of the ESN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2228</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2228</id><created>2014-01-09</created><authors><author><keyname>Huang</keyname><forenames>Yu-Chih</forenames></author><author><keyname>Narayanan</keyname><forenames>Krishna R.</forenames></author><author><keyname>Tunali</keyname><forenames>Nihat Engin</forenames></author></authors><title>Multistage Compute-and-Forward with Multilevel Lattice Codes Based on
  Product Constructions</title><categories>cs.IT math.IT</categories><comments>45 pages, 22 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel construction of lattices is proposed. This construction can be
thought of as Construction A with codes that can be represented as the
Cartesian product of $L$ linear codes over
$\mathbb{F}_{p_1},\ldots,\mathbb{F}_{p_L}$, respectively; hence, is referred to
as the product construction. The existence of a sequence of such lattices that
are good for quantization and Poltyrev-good under multistage decoding is shown.
This family of lattices is then used to generate a sequence of nested lattice
codes which allows one to achieve the same computation rate of Nazer and
Gastpar for compute-and-forward under multistage decoding, which is referred to
as lattice-based multistage compute-and-forward.
  Motivated by the proposed lattice codes, two families of signal
constellations are then proposed for the separation-based compute-and-forward
framework proposed by Tunali \textit{et al.} together with a multilevel
coding/multistage decoding scheme tailored specifically for these
constellations. This scheme is termed separation-based multistage
compute-and-forward and is shown having a complexity of the channel coding
dominated by the greatest common divisor of the constellation size (may not be
a prime number) instead of the constellation size itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2229</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2229</id><created>2014-01-09</created><authors><author><keyname>Jensi</keyname><forenames>R.</forenames></author><author><keyname>Jiji</keyname><forenames>Dr. G. Wiselin</forenames></author></authors><title>A Survey on optimization approaches to text document clustering</title><categories>cs.IR</categories><comments>14 pages</comments><journal-ref>International Journal on Computational Sciences &amp; Applications
  (IJCSA) Vol.3, No.6, December 2013</journal-ref><doi>10.5121/ijcsa.2013.3604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text Document Clustering is one of the fastest growing research areas because
of availability of huge amount of information in an electronic form. There are
several number of techniques launched for clustering documents in such a way
that documents within a cluster have high intra-similarity and low
inter-similarity to other clusters. Many document clustering algorithms provide
localized search in effectively navigating, summarizing, and organizing
information. A global optimal solution can be obtained by applying high-speed
and high-quality optimization algorithms. The optimization technique performs a
globalized search in the entire solution space. In this paper, a brief survey
on optimization approaches to text document clustering is turned out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2230</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2230</id><created>2014-01-10</created><authors><author><keyname>Bhattacharya</keyname><forenames>P. P.</forenames></author><author><keyname>Sarkar</keyname><forenames>Ananya</forenames></author><author><keyname>IndranilSarkar</keyname></author><author><keyname>Chatterjee</keyname><forenames>Subhajit</forenames></author></authors><title>An ANN Based Call Handoff Management Scheme for Mobile Cellular Network</title><categories>cs.NI</categories><comments>11 pages. arXiv admin note: text overlap with arXiv:1004.1794 by
  other authors</comments><doi>10.5121/ijwmn.2013.5610</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Handoff decisions are usually signal strength based because of simplicity and
effectiveness. Apart from the conventional techniques, such as threshold and
hysteresis based schemes, recently many artificial intelligent techniques such
as Fuzzy Logic, Artificial Neural Network (ANN) etc. are also used for taking
handoff decision. In this paper, an Artificial Neural Network based handoff
algorithm is proposed and its performance is studied. We have used ANN here for
taking fast and accurate handoff decision. In our proposed handoff algorithm,
Backpropagation Neural Network model is used.The advantages of Back propagation
method are its simplicity and reasonable speed. The algorithm is designed,
tested and found to give optimum results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2232</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2232</id><created>2014-01-10</created><authors><author><keyname>Laisema</keyname><forenames>Sitthichai</forenames></author><author><keyname>Wannapiroon</keyname><forenames>Panita</forenames></author></authors><title>Collaborative learning model with virtual team in ubiquitous learning
  environment using creative problem solving process</title><categories>cs.CY</categories><comments>14 pages, 1 figures</comments><journal-ref>International Journal on Integrating Technology in Education
  (IJITE) Vol.2, No.4, December 2013,1-14</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The purposes of this research study were: 1) to develop a Collaborative
Learning Model with Virtual Team in u-Learning Environment using Creative
Problem-solving Process(U-CCPS Model); 2) to evaluate a U-CCPS Model. The
research procedures were divided into two phases. The first phase was to
develop U-CCPS Model, and the second phase was to evaluate U-CCPS Model. The
sample group in this study consisted of five experts using purposive sampling.
Data were analyzed by arithmetic mean and standard deviation. The research
findings were as follows: The U-CCPS learning Model consisted of five
components as follows: 1) Input factors, 2) Process, 3) Control, 4) Output and
5) Feedback. The input factors consisted of four components as followed: 1)
Objectives of U-CCPS Model, 2) Roles of Instructors, 3) Roles of learners and
4) Design of learning media. The process consisted of two components as
followed: 1) Preparation before learning, and 2) Instructional management
process. The experts agree that a U-CCPS Model was highest suitability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2234</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2234</id><created>2014-01-10</created><authors><author><keyname>Phumeechanya</keyname><forenames>Noppadon</forenames></author><author><keyname>Wannapiroon</keyname><forenames>Panita</forenames></author></authors><title>Ubiquitous Scaffold Learning Environment Using Problem-based Learning to
  Enhance Problem-solving Skills and Context Awareness</title><categories>cs.CY</categories><comments>11 page, 2 figures</comments><journal-ref>International Journal on Integrating Technology in Education
  (IJITE) Vol.2, No.4, December 2013, 23-33</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The purpose of this research is to 1) design of an Ubiquitous Scaffold
Learning Environment Using Problem-based Learning model to enhance
problem-solving skills and context awareness, and 2) evaluate the developed
model. The research procedures divide into two phases. The first phase is to
design of Ubiquitous Scaffold Learning Environment Using Problem-based Learning
model, and the second phase is to evaluate the developed model. The sample
group in this study consists of five experts selected by purposive sampling
method. Data were analyzed by arithmetic mean and standard deviation. The
research findings are as follows: 1. The developed model consist of three
components is 1) principles of ubiquitous learning environment (ULE),
problem-based learning with scaffolding in ULE, problem solving skill and
context awareness 2) objectives of the model are to enhance problem solving
skill and context awareness and 3) Process of the instructional model 2. The
experts agree Ubiquitous Scaffold Learning Environment Using Problem-based
Learning model model is high level of appropriateness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2248</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2248</id><created>2014-01-10</created><updated>2015-01-02</updated><authors><author><keyname>Hardy</keyname><forenames>Yorick</forenames></author><author><keyname>Steeb</keyname><forenames>Willi-Hans</forenames></author></authors><title>Boolean Functions, Quantum Gates, Hamilton Operators, Spin Systems and
  Computer Algebra</title><categories>cs.MS quant-ph</categories><comments>title extended, construction of spin system added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the construction of quantum gates (unitary operators) from
boolean functions and give a number of applications. Both non-reversible and
reversible boolean functions are considered. The construction of the Hamilton
operator for a quantum gate is also described with the Hamilton operator
expressed as spin system. Computer algebra implementations are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2250</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2250</id><created>2014-01-10</created><authors><author><keyname>Uddin</keyname><forenames>Md. Palash</forenames></author><author><keyname>Ahmed</keyname><forenames>Ashfaque</forenames></author><author><keyname>Hossain</keyname><forenames>Md. Delowar</forenames></author><author><keyname>Afjal</keyname><forenames>Masud Ibn</forenames></author><author><keyname>Siddiquee</keyname><forenames>Shah Md. Tanvir</forenames></author></authors><title>High speed data retrieval from national data center (ndc) reducing time
  and ignoring spelling error in search key based on double metaphone algorithm</title><categories>cs.DB</categories><comments>Page: 23-32, International Journal of Computer Science, Engineering
  and Applications (IJCSEA) Vol.2, No.6, December 2013,
  http://airccse.org/journal/ijcsea/index.html</comments><doi>10.5121/ijcsea.2013.3601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast and efficient data management is one of the demanding technologies of
todays aspect. This paper proposes a system which makes the working procedures
of present manual system of storing and retrieving huge citizens information of
Bangladesh automated and increases its effectiveness. The implemented search
methodology is user friendly and efficient enough for high speed data retrieval
ignoring spelling error in the input keywords used for searching a particular
citizen. The main concern in this research is minimizing the total searching
time for a given keyword. This can be done if we can pre-establish the idea of
getting the data belonging to the searching keyword. The primary and secondary
key-code generated by the Double Metaphone Algorithm for each word is used to
establish that idea about the word. This algorithm is used for creating the map
of the original database, through which the keyword is matched against the
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2254</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2254</id><created>2014-01-10</created><updated>2015-11-17</updated><authors><author><keyname>Williams</keyname><forenames>Richard</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>Sampling Issues in Bibliometric Analysis</title><categories>stat.AP cs.DL physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bibliometricians face several issues when drawing and analyzing samples of
citation records for their research. Drawing samples that are too small may
make it difficult or impossible for studies to achieve their goals, while
drawing samples that are too large may drain resources that could be better
used for other purposes. This paper considers three common situations and
offers advice for dealing with each. First, an entire population of records is
available for an institution. We argue that, even though all records have been
collected, the use of inferential statistics, significance testing, and
confidence intervals is both common and desirable. Second, because of limited
resources or other factors, a sample of records needs to be drawn. We
demonstrate how power analyses can be used to determine in advance how large
the sample needs to be to achieve the study's goals. Third, the sample size may
already be determined, either because the data have already been collected or
because resources are limited. We show how power analyses can again be used to
determine how large effects need to be in order to find effects that are
statistically significant. Such information can then help bibliometricians to
develop reasonable expectations as to what their analysis can accomplish. While
we focus on issues of interest to bibliometricians, our recommendations and
procedures can easily be adapted for other fields of study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2258</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2258</id><created>2014-01-10</created><authors><author><keyname>Roth</keyname><forenames>Benjamin</forenames></author></authors><title>Assessing Wikipedia-Based Cross-Language Retrieval Models</title><categories>cs.IR cs.CL</categories><comments>74 pages; MSc thesis at Saarland University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work compares concept models for cross-language retrieval: First, we
adapt probabilistic Latent Semantic Analysis (pLSA) for multilingual documents.
Experiments with different weighting schemes show that a weighting method
favoring documents of similar length in both language sides gives best results.
Considering that both monolingual and multilingual Latent Dirichlet Allocation
(LDA) behave alike when applied for such documents, we use a training corpus
built on Wikipedia where all documents are length-normalized and obtain
improvements over previously reported scores for LDA. Another focus of our work
is on model combination. For this end we include Explicit Semantic Analysis
(ESA) in the experiments. We observe that ESA is not competitive with LDA in a
query based retrieval task on CLEF 2000 data. The combination of machine
translation with concept models increased performance by 21.1% map in
comparison to machine translation alone. Machine translation relies on parallel
corpora, which may not be available for many language pairs. We further explore
how much cross-lingual information can be carried over by a specific
information source in Wikipedia, namely linked text. The best results are
obtained using a language modeling approach, entirely without information from
parallel corpora. The need for smoothing raises interesting questions on
soundness and efficiency. Link models capture only a certain kind of
information and suggest weighting schemes to emphasize particular words. For a
combined model, another interesting question is therefore how to integrate
different weighting schemes. Using a very simple combination scheme, we obtain
results that compare favorably to previously reported results on the CLEF 2000
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2288</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2288</id><created>2014-01-10</created><updated>2014-02-02</updated><authors><author><keyname>Aggarwal</keyname><forenames>Hemant Kumar</forenames></author><author><keyname>Majumdar</keyname><forenames>Angshul</forenames></author></authors><title>Extension of Sparse Randomized Kaczmarz Algorithm for Multiple
  Measurement Vectors</title><categories>cs.NA cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Kaczmarz algorithm is popular for iteratively solving an overdetermined
system of linear equations. The traditional Kaczmarz algorithm can approximate
the solution in few sweeps through the equations but a randomized version of
the Kaczmarz algorithm was shown to converge exponentially and independent of
number of equations. Recently an algorithm for finding sparse solution to a
linear system of equations has been proposed based on weighted randomized
Kaczmarz algorithm. These algorithms solves single measurement vector problem;
however there are applications were multiple-measurements are available. In
this work, the objective is to solve a multiple measurement vector problem with
common sparse support by modifying the randomized Kaczmarz algorithm. We have
also modeled the problem of face recognition from video as the multiple
measurement vector problem and solved using our proposed technique. We have
compared the proposed algorithm with state-of-art spectral projected gradient
algorithm for multiple measurement vectors on both real and synthetic datasets.
The Monte Carlo simulations confirms that our proposed algorithm have better
recovery and convergence rate than the MMV version of spectral projected
gradient algorithm under fairness constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2290</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2290</id><created>2014-01-10</created><updated>2015-01-09</updated><authors><author><keyname>Gundert</keyname><forenames>Anna</forenames></author><author><keyname>Szedl&#xe1;k</keyname><forenames>May</forenames></author></authors><title>Higher Dimensional Discrete Cheeger Inequalities</title><categories>math.CO cs.DM math.SP</categories><comments>14 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For graphs there exists a strong connection between spectral and
combinatorial expansion properties. This is expressed, e.g., by the discrete
Cheeger inequality, the lower bound of which states that $\lambda(G) \leq
h(G)$, where $\lambda(G)$ is the second smallest eigenvalue of the Laplacian of
a graph $G$ and $h(G)$ is the Cheeger constant measuring the edge expansion of
$G$. We are interested in generalizations of expansion properties to finite
simplicial complexes of higher dimension (or uniform hypergraphs).
  Whereas higher dimensional Laplacians were introduced already in 1945 by
Eckmann, the generalization of edge expansion to simplicial complexes is not
straightforward. Recently, a topologically motivated notion analogous to edge
expansion that is based on $\mathbb{Z}_2$-cohomology was introduced by Gromov
and independently by Linial, Meshulam and Wallach. It is known that for this
generalization there is no higher dimensional analogue of the lower bound of
the Cheeger inequality. A different, combinatorially motivated generalization
of the Cheeger constant, denoted by $h(X)$, was studied by Parzanchevski,
Rosenthal and Tessler. They showed that indeed $\lambda(X) \leq h(X)$, where
$\lambda(X)$ is the smallest non-trivial eigenvalue of the ($(k-1)$-dimensional
upper) Laplacian, for the case of $k$-dimensional simplicial complexes $X$ with
complete $(k-1)$-skeleton.
  Whether this inequality also holds for $k$-dimensional complexes with
non-complete $(k-1)$-skeleton has been an open question. We give two proofs of
the inequality for arbitrary complexes. The proofs differ strongly in the
methods and structures employed, and each allows for a different kind of
additional strengthening of the original result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2294</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2294</id><created>2014-01-10</created><updated>2014-07-23</updated><authors><author><keyname>T&#xf6;rm&#xe4;</keyname><forenames>Ilkka</forenames></author></authors><title>Quantifier Extensions of Multidimensional Sofic Shifts</title><categories>math.DS cs.FL</categories><comments>15 pages, 3 figures. Submitted to Proceedings of the American
  Mathematical Society</comments><msc-class>37B50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a pair of simple combinatorial operations on subshifts, called
existential and universal extensions, and study their basic properties. We
prove that the existential extension of a sofic shift by another sofic shift is
always sofic, and the same holds for the universal extension in one dimension.
However, we also show by a construction that universal extensions of
two-dimensional sofic shifts may not be sofic, even if the subshift we extend
by is very simple.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2304</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2304</id><created>2014-01-10</created><authors><author><keyname>Hummelsheim</keyname><forenames>Stefan</forenames></author></authors><title>Lasso and equivalent quadratic penalized models</title><categories>stat.ML cs.LG</categories><comments>7 pages with 2 figures</comments><msc-class>62J07 (primary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The least absolute shrinkage and selection operator (lasso) and ridge
regression produce usually different estimates although input, loss function
and parameterization of the penalty are identical. In this paper we look for
ridge and lasso models with identical solution set.
  It turns out, that the lasso model with shrink vector $\lambda$ and a
quadratic penalized model with shrink matrix as outer product of $\lambda$ with
itself are equivalent, in the sense that they have equal solutions. To achieve
this, we have to restrict the estimates to be positive. This doesn't limit the
area of application since we can easily decompose every estimate in a positive
and negative part. The resulting problem can be solved with a non negative
least square algorithm.
  Beside this quadratic penalized model, an augmented regression model with
positive bounded estimates is developed which is also equivalent to the lasso
model, but is probably faster to solve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2327</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2327</id><created>2014-01-10</created><authors><author><keyname>Najeebullah</keyname><forenames>Kamran</forenames></author><author><keyname>Khan</keyname><forenames>Kifayat Ullah</forenames></author><author><keyname>Nawaz</keyname><forenames>Waqas</forenames></author><author><keyname>Lee</keyname><forenames>Young-Koo</forenames></author></authors><title>BPP: Large Graph Storage for Efficient Disk Based Processing</title><categories>cs.DS cs.DB</categories><comments>5 pages, Published in ICCA, 2013</comments><journal-ref>Advanced Science and Technology Letters Vol.30 (ICCA 2013),
  pp.117-121</journal-ref><doi>10.14257/astl.2013.30.25</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Processing very large graphs like social networks, biological and chemical
compounds is a challenging task. Distributed graph processing systems process
the billion-scale graphs efficiently but incur overheads of efficient
partitioning and distribution of the graph over a cluster of nodes. Distributed
processing also requires cluster management and fault tolerance. In order to
overcome these problems GraphChi was proposed recently. GraphChi significantly
outperformed all the representative distributed processing frameworks. Still,
we observe that GraphChi incurs some serious degradation in performance due to
1) high number of non-sequential I/Os for processing every chunk of graph; and
2) lack of true parallelism to process the graph. In this paper we propose a
simple yet powerful engine BiShard Parallel Processor (BPP) to efficiently
process billions-scale graphs on a single PC. We extend the storage structure
proposed by GraphChi and introduce a new processing model called BiShard
Parallel (BP). BP enables full CPU parallelism for processing the graph and
significantly reduces the number of non-sequential I/Os required to process
every chunk of the graph. Our experiments on real large graphs show that our
solution significantly outperforms GraphChi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2342</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2342</id><created>2014-01-10</created><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Park</keyname><forenames>Han Woo</forenames></author></authors><title>Can Synergy in Triple-Helix Relations be Quantified? A Review of the
  Development of the Triple-Helix Indicator</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Triple-Helix arrangements of bi- and trilateral relations can be considered
as adaptive eco-systems. During the last decade, we have further developed a
Triple-Helix indicator of synergy as reduction of uncertainty in niches that
can be shaped among three or more distributions. Reduction of uncertainty can
be generated in correlations among distributions of relations, but this
(next-order) effect can be counterbalanced by uncertainty generated in the
relations. We first explain the indicator, and then review possible results
when this indicator is applied to (i) co-author networks of academic,
industrial, and governmental authors and (ii) synergies in the distributions of
firms over geographical addresses, technological classes, and industrial-size
classes for a number of nations. Co-variation is then considered as a measure
of relationship. The balance between globalizing and localizing dynamics can be
quantified. Too much synergy locally can also be considered as lock-in.
Tendencies are different for the globalizing knowledge dynamics versus locally
retaining wealth from knowledge in industrial innovations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2369</identifier>
 <datestamp>2014-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2369</id><created>2014-01-10</created><authors><author><keyname>Tall</keyname><forenames>Abdoulaye</forenames></author><author><keyname>Altman</keyname><forenames>Zwi</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author></authors><title>Self Organizing strategies for enhanced ICIC (eICIC)</title><categories>cs.NI</categories><comments>Submitted to WiOpt 2014</comments><doi>10.1109/WIOPT.2014.6850315</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small cells have been identified as an effective solution for coping with the
important traffic increase that is expected in the coming years. But this
solution is accompanied by additional interference that needs to be mitigated.
The enhanced Inter Cell Interference Coordination (eICIC) feature has been
introduced to address the interference problem. eICIC involves two parameters
which need to be optimized, namely the Cell Range Extension (CRE) of the small
cells and the ABS ratio (ABSr) which defines a mute ratio for the macro cell to
reduce the interference it produces. In this paper we propose self-optimizing
algorithms for the eICIC. The CRE is adjusted by means of load balancing
algorithm. The ABSr parameter is optimized by maximizing a proportional fair
utility of user throughputs. The convergence of the algorithms is proven using
stochastic approximation theorems. Numerical simulations illustrate the
important performance gain brought about by the different algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2376</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2376</id><created>2014-01-10</created><authors><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Iterative Dynamic Water-filling for Fading Multiple-Access Channels with
  Energy Harvesting</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop optimal energy scheduling algorithms for $N$-user
fading multiple-access channels with energy harvesting to maximize the channel
sum-rate, assuming that the side information of both the channel states and
energy harvesting states for $K$ time slots is known {\em a priori}, and the
battery capacity and the maximum energy consumption in each time slot are
bounded. The problem is formulated as a convex optimization problem with ${\cal
O}(NK)$ constraints making it hard to solve using a general convex solver since
the computational complexity of a generic convex solver is exponential in the
number of constraints. This paper gives an efficient energy scheduling
algorithm, called the iterative dynamic water-filling algorithm, that has a
computational complexity of ${\cal O}(NK^2)$ per iteration. For the single-user
case, a dynamic water-filling method is shown to be optimal. Unlike the
traditional water-filling algorithm, in dynamic water-filling, the water level
is not constant but changes when the battery overflows or depletes. An
iterative version of the dynamic water-filling algorithm is shown to be optimal
for the case of multiple users. Even though in principle the optimality is
achieved under large number of iterations, in practice convergence is reached
in only a few iterations. Moreover, a single iteration of the dynamic
water-filling algorithm achieves a sum-rate that is within $(N-1)K/2$ nats of
the optimal sum-rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2393</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2393</id><created>2014-01-10</created><authors><author><keyname>Kumar</keyname><forenames>Chiranjeev</forenames></author></authors><title>Approximation Algorithm Project</title><categories>cs.DS</categories><comments>Tiny project. arXiv admin note: text overlap with arXiv:1203.3097 by
  other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This application for learning APPROXIMATION ALGORITHM has been designed in
Java which will make user comfortable in learning the very complex subject
&quot;NP-Completeness&quot; and the solution to NP-Complete problem using approximation
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2398</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2398</id><created>2014-01-10</created><updated>2014-04-28</updated><authors><author><keyname>Dalai</keyname><forenames>Marco</forenames></author></authors><title>An Elias Bound on the Bhattacharyya Distance of Codes for Channels with
  a Zero-Error Capacity</title><categories>cs.IT math.CO math.IT</categories><comments>ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an upper bound on the minimum Bhattacharyya
distance of codes for channels with a zero-error capacity. The bound is
obtained by combining an extension of the Elias bound introduced by Blahut,
with an extension of a bound previously introduced by the author, which builds
upon ideas of Gallager, Lov\'asz and Marton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2405</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2405</id><created>2014-01-10</created><updated>2014-01-21</updated><authors><author><keyname>Samara</keyname><forenames>Ghassan</forenames></author><author><keyname>Alhmiedat</keyname><forenames>Tareq</forenames></author><author><keyname>Salem</keyname><forenames>Amer O. Abu</forenames></author></authors><title>Dynamic Safety Message Power Control in VANET Using PSO</title><categories>cs.NI</categories><comments>9 pages. arXiv admin note: substantial text overlap with
  arXiv:1311.2364</comments><journal-ref>The World of Computer Science and Information Technology Journal
  (WSCIT). 2013, Volume 3, Issue 10. pp. 176.184</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years Vehicular Ad hoc Networks (VANET) became one of the most
challenging research area in the field of Mobile Ad hoc Networks (MANET).
Vehicles in VANET send emergency and safety periodic messages through one
control channel having a limited bandwidth, which causes a growing collision to
the channel especially in dense traffic situations. In this paper a protocol
Particle swarm optimization Beacon Power Control (PBPC) is proposed, which
makes dynamic transmission power control to adjust the transmission power of
the safety periodic messages that have been aggressively sent by all vehicles
on the road 10 times per a second, the proposed protocol aims to decrease the
packet collision resulted from periodic safety messages, which leads to control
the load on the channel while ensuring a high probability of message reception
within the safety distance of the sender vehicle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2410</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2410</id><created>2014-01-10</created><authors><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Power Allocation for Energy Harvesting Transmitter with Causal
  Information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider power allocation for an access-controlled transmitter with energy
harvesting capability based on causal observations of the channel fading state.
We assume that the system operates in a time-slotted fashion and the channel
gain in each slot is a random variable which is independent across slots.
Further, we assume that the transmitter is solely powered by a renewable energy
source and the energy harvesting process can practically be predicted. With the
additional access control for the transmitter and the maximum power constraint,
we formulate the stochastic optimization problem of maximizing the achievable
rate as a Markov decision process (MDP) with continuous state. To efficiently
solve the problem, we define an approximate value function based on a piecewise
linear fit in terms of the battery state. We show that with the approximate
value function, the update in each iteration consists of a group of convex
problems with a continuous parameter. Moreover, we derive the optimal solution
to these convex problems in closed-form. Further, we propose power allocation
algorithms for both the finite- and infinite-horizon cases, whose computational
complexity is significantly lower than that of the standard discrete MDP method
but with improved performance. Extension to the case of a general payoff
function and imperfect energy prediction is also considered. Finally,
simulation results demonstrate that the proposed algorithms closely approach
the optimal performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2411</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2411</id><created>2014-01-10</created><authors><author><keyname>McCarty</keyname><forenames>L. Thorne</forenames></author></authors><title>Clustering, Coding, and the Concept of Similarity</title><categories>cs.LG</categories><comments>55 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a theory of clustering and coding which combines a
geometric model with a probabilistic model in a principled way. The geometric
model is a Riemannian manifold with a Riemannian metric, ${g}_{ij}({\bf x})$,
which we interpret as a measure of dissimilarity. The probabilistic model
consists of a stochastic process with an invariant probability measure which
matches the density of the sample input data. The link between the two models
is a potential function, $U({\bf x})$, and its gradient, $\nabla U({\bf x})$.
We use the gradient to define the dissimilarity metric, which guarantees that
our measure of dissimilarity will depend on the probability measure. Finally,
we use the dissimilarity metric to define a coordinate system on the embedded
Riemannian manifold, which gives us a low-dimensional encoding of our original
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2416</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2416</id><created>2014-01-10</created><authors><author><keyname>Assirati</keyname><forenames>Lucas</forenames></author><author><keyname>Martinez</keyname><forenames>Alexandre Souto</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir Martinez</forenames></author></authors><title>Satellite image classification and segmentation using non-additive
  entropy</title><categories>cs.CV</categories><comments>4 pages, 5 figures, ICMSquare 2013</comments><doi>10.1088/1742-6596/490/1/012086</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we compare the Boltzmann-Gibbs-Shannon (standard) with the Tsallis
entropy on the pattern recognition and segmentation of coloured images obtained
by satellites, via &quot;Google Earth&quot;. By segmentation we mean split an image to
locate regions of interest. Here, we discriminate and define an image partition
classes according to a training basis. This training basis consists of three
pattern classes: aquatic, urban and vegetation regions. Our numerical
experiments demonstrate that the Tsallis entropy, used as a feature vector
composed of distinct entropic indexes $q$ outperforms the standard entropy.
There are several applications of our proposed methodology, once satellite
images can be used to monitor migration form rural to urban regions,
agricultural activities, oil spreading on the ocean etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2417</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2417</id><created>2014-01-10</created><updated>2014-01-13</updated><authors><author><keyname>Armknecht</keyname><forenames>Frederik</forenames></author><author><keyname>Gagliardoni</keyname><forenames>Tommaso</forenames></author><author><keyname>Katzenbeisser</keyname><forenames>Stefan</forenames></author><author><keyname>Peter</keyname><forenames>Andreas</forenames></author></authors><title>General Impossibility of Group Homomorphic Encryption in the Quantum
  World</title><categories>cs.CR math.GR</categories><comments>20 pages, 2 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group homomorphic encryption represents one of the most important building
blocks in modern cryptography. It forms the basis of widely-used, more
sophisticated primitives, such as CCA2-secure encryption or secure multiparty
computation. Unfortunately, recent advances in quantum computation show that
many of the existing schemes completely break down once quantum computers reach
maturity (mainly due to Shor's algorithm). This leads to the challenge of
constructing quantum-resistant group homomorphic cryptosystems.
  In this work, we prove the general impossibility of (abelian) group
homomorphic encryption in the presence of quantum adversaries, when assuming
the IND-CPA security notion as the minimal security requirement. To this end,
we prove a new result on the probability of sampling generating sets of finite
(sub-)groups if sampling is done with respect to an arbitrary, unknown
distribution. Finally, we provide a sufficient condition on homomorphic
encryption schemes for our quantum attack to work and discuss its
satisfiability in non-group homomorphic cases. The impact of our results on
recent fully homomorphic encryption schemes poses itself as an open question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2422</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2422</id><created>2014-01-10</created><updated>2014-01-27</updated><authors><author><keyname>Prakash</keyname><forenames>N.</forenames></author><author><keyname>Lalitha</keyname><forenames>V.</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijay</forenames></author></authors><title>Codes with Locality for Two Erasures</title><categories>cs.IT math.IT</categories><comments>14 pages, 3 figures, Updated for improved readability</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study codes with locality that can recover from two
erasures via a sequence of two local, parity-check computations. By a local
parity-check computation, we mean recovery via a single parity-check equation
associated to small Hamming weight. Earlier approaches considered recovery in
parallel; the sequential approach allows us to potentially construct codes with
improved minimum distance. These codes, which we refer to as locally
2-reconstructible codes, are a natural generalization along one direction, of
codes with all-symbol locality introduced by Gopalan \textit{et al}, in which
recovery from a single erasure is considered. By studying the Generalized
Hamming Weights of the dual code, we derive upper bounds on the minimum
distance of locally 2-reconstructible codes and provide constructions for a
family of codes based on Tur\'an graphs, that are optimal with respect to this
bound. The minimum distance bound derived here is universal in the sense that
no code which permits all-symbol local recovery from $2$ erasures can have
larger minimum distance regardless of approach adopted. Our approach also leads
to a new bound on the minimum distance of codes with all-symbol locality for
the single-erasure case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2436</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2436</id><created>2014-01-10</created><authors><author><keyname>O'Donnell</keyname><forenames>Ryan</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author><author><keyname>Wu</keyname><forenames>Chenggang</forenames></author><author><keyname>Zhou</keyname><forenames>Yuan</forenames></author></authors><title>Hardness of robust graph isomorphism, Lasserre gaps, and asymmetry of
  random graphs</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building on work of Cai, F\&quot;urer, and Immerman \cite{CFI92}, we show two
hardness results for the Graph Isomorphism problem. First, we show that there
are pairs of nonisomorphic $n$-vertex graphs $G$ and $H$ such that any
sum-of-squares (SOS) proof of nonisomorphism requires degree $\Omega(n)$. In
other words, we show an $\Omega(n)$-round integrality gap for the Lasserre SDP
relaxation. In fact, we show this for pairs $G$ and $H$ which are not even
$(1-10^{-14})$-isomorphic. (Here we say that two $n$-vertex, $m$-edge graphs
$G$ and $H$ are $\alpha$-isomorphic if there is a bijection between their
vertices which preserves at least $\alpha m$ edges.) Our second result is that
under the {\sc R3XOR} Hypothesis \cite{Fei02} (and also any of a class of
hypotheses which generalize the {\sc R3XOR} Hypothesis), the \emph{robust}
Graph Isomorphism problem is hard. I.e.\ for every $\epsilon &gt; 0$, there is no
efficient algorithm which can distinguish graph pairs which are
$(1-\epsilon)$-isomorphic from pairs which are not even
$(1-\epsilon_0)$-isomorphic for some universal constant $\epsilon_0$. Along the
way we prove a robust asymmetry result for random graphs and hypergraphs which
may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2440</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2440</id><created>2014-01-10</created><authors><author><keyname>Mach</keyname><forenames>Werner</forenames></author><author><keyname>Pittl</keyname><forenames>Benedikt</forenames></author><author><keyname>Schikuta</keyname><forenames>Erich</forenames></author></authors><title>A Prediction Model for the Probability of SLA Matching in Consumer
  Provider Contracting of Web Services</title><categories>cs.DC</categories><comments>Conference submission, not published yet</comments><acm-class>H.3.5; K.4.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future e-business models will rely on electronic contracts which are agreed
dynamically and adaptively by web services. Thus, the automatic negotiation of
Service Level Agreements (SLAs) between consumers and providers is key for
enabling service-based value chains.
  The process of finding appropriate providers for web services seems to be
simple. Consumers contact several providers and take the provider which offers
the best matching SLA. However, currently consumers are not able forecasting
the probability of finding a matching provider for their requested SLA. So
consumers contact several providers and check if their offers are matching. In
case of continuing faults, on the one hand consumers may adapt their Service
Level Objects (SLOs) of the required SLA or on the other hand simply accept
offered SLAs of the contacted providers.
  By forecasting the probability of finding a matching provider, consumers
could assess their chances of finding a provider offering the requested SLA. If
a low probability is predicted, consumers can immediately adapt their SLOs or
increase the numbers of providers to be contacted.
  Thus, this paper proposes an analytical forecast model, which allows
consumers to get a realistic assessment of the probability to find matching
providers. Additionally, we present an optimization algorithm based on the
forecast results, which allows adapting the SLO parameter ranges in order to
find at least one matching provider. Not only consumers, but also providers can
use this forecast model to predict the prospective demand. So providers are
able to assess the number of potential consumers based on their offers too.
  Justification of our approach is done by simulation of practical examples
checking our theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2444</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2444</id><created>2014-01-10</created><authors><author><keyname>Williams</keyname><forenames>Ryan</forenames></author></authors><title>New algorithms and lower bounds for circuits with linear threshold gates</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $ACC \circ THR$ be the class of constant-depth circuits comprised of AND,
OR, and MOD$m$ gates (for some constant $m &gt; 1$), with a bottom layer of gates
computing arbitrary linear threshold functions. This class of circuits can be
seen as a &quot;midpoint&quot; between $ACC$ (where we know nontrivial lower bounds) and
depth-two linear threshold circuits (where nontrivial lower bounds remain
open).
  We give an algorithm for evaluating an arbitrary symmetric function of
$2^{n^{o(1)}}$ $ACC \circ THR$ circuits of size $2^{n^{o(1)}}$, on all possible
inputs, in $2^n \cdot poly(n)$ time. Several consequences are derived:
  $\bullet$ The number of satisfying assignments to an $ACC \circ THR$ circuit
of subexponential size can be computed in $2^{n-n^{\varepsilon}}$ time (where
$\varepsilon &gt; 0$ depends on the depth and modulus of the circuit).
  $\bullet$ $NEXP$ does not have quasi-polynomial size $ACC \circ THR$
circuits, nor does $NEXP$ have quasi-polynomial size $ACC \circ SYM$ circuits.
Nontrivial size lower bounds were not known even for $AND \circ OR \circ THR$
circuits.
  $\bullet$ Every 0-1 integer linear program with $n$ Boolean variables and $s$
linear constraints is solvable in $2^{n-\Omega(n/((\log M)(\log s)^{5}))}\cdot
poly(s,n,M)$ time with high probability, where $M$ upper bounds the bit
complexity of the coefficients. (For example, 0-1 integer programs with weights
in $[-2^{poly(n)},2^{poly(n)}]$ and $poly(n)$ constraints can be solved in
$2^{n-\Omega(n/\log^6 n)}$ time.)
  We also present an algorithm for evaluating depth-two linear threshold
circuits (a.k.a., $THR \circ THR$) with exponential weights and $2^{n/24}$ size
on all $2^n$ input assignments, running in $2^n \cdot poly(n)$ time. This is
evidence that non-uniform lower bounds for $THR \circ THR$ are within reach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2454</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2454</id><created>2014-01-10</created><updated>2014-02-05</updated><authors><author><keyname>Cohen</keyname><forenames>Michael B.</forenames></author><author><keyname>Miller</keyname><forenames>Gary L.</forenames></author><author><keyname>Pachocki</keyname><forenames>Jakub W.</forenames></author><author><keyname>Peng</keyname><forenames>Richard</forenames></author><author><keyname>Xu</keyname><forenames>Shen Chen</forenames></author></authors><title>Stretching Stretch</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a generalized definition of stretch that simplifies the efficient
construction of low-stretch embeddings suitable for graph algorithms. The
generalization, based on discounting highly stretched edges by taking their
$p$-th power for some $0 &lt; p &lt; 1$, is directly related to performances of
existing algorithms. This discounting of high-stretch edges allows us to treat
many classes of edges with coarser granularity. It leads to a two-pass approach
that combines bottom-up clustering and top-down decompositions to construct
these embeddings in $\mathcal{O}(m\log\log{n})$ time. Our algorithm
parallelizes readily and can also produce generalizations of low-stretch
subgraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2468</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2468</id><created>2014-01-10</created><authors><author><keyname>Schikuta</keyname><forenames>Erich</forenames></author><author><keyname>Mann</keyname><forenames>Erwin</forenames></author></authors><title>N2Sky - Neural Networks as Services in the Clouds</title><categories>cs.NE</categories><comments>extended version of paper published at IJCNN 2013</comments><acm-class>H.3.5; I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the N2Sky system, which provides a framework for the exchange of
neural network specific knowledge, as neural network paradigms and objects, by
a virtual organization environment. It follows the sky computing paradigm
delivering ample resources by the usage of federated Clouds. N2Sky is a novel
Cloud-based neural network simulation environment, which follows a pure service
oriented approach. The system implements a transparent environment aiming to
enable both novice and experienced users to do neural network research easily
and comfortably. N2Sky is built using the RAVO reference architecture of
virtual organizations which allows itself naturally integrating into the Cloud
service stack (SaaS, PaaS, and IaaS) of service oriented architectures.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="54000" completeListSize="102538">1122234|55001</resumptionToken>
</ListRecords>
</OAI-PMH>
